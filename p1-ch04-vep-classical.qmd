# Classical Variant Effect Prediction {#sec-vep-classical}

## The Variant Prioritization Challenge

A typical human genome contains approximately four to five million genetic variants relative to the reference assembly, encompassing single-nucleotide variants and short insertions and deletions. The vast majority of these are functionally neutral, representing the accumulated diversity of human evolution and population history. For any individual with a suspected genetic condition, the central interpretive challenge is to identify the handful of variants that plausibly contribute to disease from this enormous background of benign variation. Consider a child presenting with developmental delay and seizures whose exome sequencing reveals 25,000 variants from the reference genome: which ones, if any, explain the clinical presentation? This challenge of variant prioritization stands at the heart of clinical genomics.

The data resources surveyed in @sec-data provide multiple complementary views of variant function, each with distinct strengths and limitations. Population frequency databases such as gnomAD reveal which variants survive in large cohorts of ostensibly healthy individuals, offering a powerful filter for identifying rare, potentially deleterious alleles. Functional genomics consortia including ENCODE and the Roadmap Epigenomics Project indicate which genomic regions show evidence of biochemical activity across diverse cell types and developmental contexts. Clinical databases such as ClinVar and HGMD collect expert-curated variant classifications drawn from case reports and diagnostic laboratories, providing ground truth labels for known pathogenic and benign variants.

Each of these sources is partial in important ways. Population databases are dominated by common variants, which are mostly tolerated by virtue of their high frequency. Rare and de novo variants, which are often most relevant for Mendelian disease, have sparse or no direct labels. Functional genomics data is inherently noisy and often context-specific: a region active in liver hepatocytes may be quiescent in neurons, and vice versa. Clinical databases are sparse and heavily biased toward well-studied genes and variant types, leaving vast swaths of the genome without reliable clinical annotations. The annotation density varies dramatically across the genome, with protein-coding exons densely labeled relative to deep intronic and intergenic sequences. Non-coding regions remain especially under-annotated despite harboring the majority of disease-associated variants identified by genome-wide association studies.

This asymmetry between variant abundance and interpretive capacity defines the variant effect prediction problem. Before deep learning transformed the field, computational approaches to this problem relied on feature engineering: human experts identified biologically meaningful signals, computed them for each variant, and combined them into predictive scores. Conservation across species, amino acid physicochemistry, proximity to splice sites, overlap with regulatory elements: each signal captures some aspect of variant function, but none is sufficient alone. The field's response was to develop integrative methods that combine multiple weak signals into stronger predictions.

This chapter examines the pre-deep-learning approaches to variant effect prediction that established the conceptual foundations for modern methods. We trace the evolution from single-signal predictors (conservation scores, protein-level tools) through ensemble methods that integrate dozens of annotations into unified scores. Combined Annotation-Dependent Depletion (CADD) receives particular attention because it introduced design patterns that recur throughout genomic machine learning: proxy labels derived from evolutionary signals, large-scale training, integration of diverse features, and genome-wide precomputation for downstream reuse. Understanding these classical approaches illuminates both what they achieved and why the field ultimately moved toward learned representations.


## Conservation-Based Approaches

The longest-preserved sequences in the genome are those where mutations have been consistently removed by natural selection across millions of years. This simple observation underlies conservation-based variant scoring: if a genomic position has remained unchanged across species separated by hundreds of millions of years of evolution, mutations at that position are likely to be deleterious. Conservation scores quantify this evolutionary constraint and provide some of the strongest signals for variant prioritization, particularly in non-coding regions where other functional annotations are sparse.

### Measuring Evolutionary Constraint

Conservation scores derive from multiple sequence alignments spanning diverse species. The human genome is aligned against dozens to hundreds of other vertebrate, mammalian, and more distantly related genomes, creating a matrix where each column represents homologous positions across species. At positions under purifying selection, the aligned bases show little variation; at neutral positions, substitutions accumulate at rates determined by mutational processes and genetic drift.

PhyloP scores quantify the deviation of observed substitution rates from neutral expectation at individual positions [@siepel_phastcons_2005]. The score is computed by comparing the observed pattern of bases at each alignment column against a neutral evolutionary model, typically fit to ancestral repeat sequences that are assumed to evolve without selective constraint. Positive phyloP scores indicate conservation (slower evolution than expected under neutrality), while negative scores indicate acceleration (faster evolution, potentially reflecting positive selection). A phyloP score of 2 indicates that the observed base is approximately 100-fold more conserved than expected under neutrality.

Genomic Evolutionary Rate Profiling (GERP) takes a complementary approach by estimating "rejected substitutions" at each position: the number of substitutions that would have been expected under neutrality but are absent from the observed alignment [@davydov_gerp_2010]. Large positive GERP scores indicate strong constraint. For a position conserved across 30 mammalian species, a GERP score of 5 implies that approximately five substitutions were "rejected" by selection over mammalian evolution. This interpretation is intuitive but depends on accurate neutral rate estimation and alignment quality.

PhastCons provides element-level rather than position-level conservation by identifying contiguous stretches of constrained sequence [@siepel_phastcons_2005]. Using a hidden Markov model, phastCons classifies each position as belonging to a conserved or non-conserved state, then outputs the posterior probability of conservation. The resulting scores are smoother than position-level metrics, capturing functional elements that span multiple nucleotides even when individual positions show moderate conservation.

### Clinical Utility and Limitations

Conservation scores are particularly valuable for non-coding variant interpretation, where direct functional annotations are often incomplete or absent. A deeply conserved intronic position likely participates in splicing regulation, gene expression control, or other functional processes even if no explicit annotation overlaps it. In clinical variant interpretation, strong conservation provides computational evidence (PP3 criterion under ACMG-AMP guidelines) supporting pathogenicity.

The limitations of conservation-based approaches are equally important to recognize. First, conservation requires evolutionary time to accumulate signal. Recently evolved functional elements, including human-specific regulatory sequences and primate-specific genes, may show little conservation despite genuine function. Second, conservation reflects the aggregate of selective pressures across the species in the alignment, which may differ from the specific functional role in humans. A position conserved for its role in neural development across vertebrates may be less constrained for immune function, which evolves more rapidly. Third, lack of conservation does not prove neutrality; it may simply indicate rapid evolution under positive selection or lineage-specific function.

Conservation scores also face technical challenges from alignment quality. In repetitive regions, segmental duplications, and rapidly evolving gene families, reliable alignments may be impossible to construct, leaving conservation scores undefined or unreliable precisely where variant interpretation is most difficult. These technical limitations concentrate in genomically complex regions that are often clinically important, including the major histocompatibility complex and immunoglobulin loci.

Despite these limitations, conservation remains among the most robust signals for variant effect prediction. It provides information that is largely orthogonal to population frequency (which reflects recent human history rather than deep evolutionary constraint) and to functional genomics annotations (which capture biochemical activity rather than selective importance). The integrative methods discussed later in this chapter combine conservation with these other signals to achieve better performance than any single source.


## Protein-Level Predictors

For variants that alter protein sequence, specialized tools assess the likely impact on protein function. These protein-level predictors emerged before genome-wide methods and remain widely used in clinical practice, either alone or as features within ensemble scores. The underlying premise is that amino acid substitutions disrupting protein structure or function are more likely to cause disease than those preserving biochemical properties.

### SIFT: Sequence Homology as Functional Constraint

Sorting Intolerant From Tolerant (SIFT) predicts whether an amino acid substitution affects protein function based on sequence homology [@ng_sift_2003]. The method collects homologous protein sequences from diverse species, constructs a multiple sequence alignment, and examines which amino acids appear at each position across the alignment. Positions that are highly conserved (showing the same or similar amino acids across species) are predicted to be functionally important; substitutions introducing amino acids not observed at that position are predicted to be deleterious.

Mechanistically, SIFT computes a normalized probability for each possible amino acid at each position based on the diversity observed in the alignment. The SIFT score for a substitution is the probability of observing the mutant amino acid, scaled by the position's overall diversity. Scores range from 0 to 1, with low scores (typically below 0.05) indicating predicted damage. A SIFT score of 0.01 for a particular missense variant indicates that the mutant amino acid is rarely or never observed at that position across the sequence family, suggesting functional constraint.

SIFT's simplicity is both its strength and its limitation. The method requires only protein sequence information and a database of homologs; it makes no assumptions about protein structure, physicochemistry, or mechanism of damage. This generality allows application to any protein with sufficient homologs in sequence databases. However, SIFT captures only the evolutionary signal present in the alignment. For proteins with few homologs, young gene families, or positions with limited alignment depth, SIFT predictions may be unreliable.

### PolyPhen-2: Integrating Structure and Sequence

Polymorphism Phenotyping (PolyPhen-2) extends sequence-based prediction by incorporating protein structure features and amino acid physicochemistry [@adzhubei_polyphen_2010]. The method uses a naive Bayes classifier trained to distinguish disease-causing mutations from neutral polymorphisms based on a collection of sequence and structure-derived features.

The feature set includes sequence conservation (similar to SIFT), but adds several structural descriptors when three-dimensional structure data is available: solvent accessibility, secondary structure context, and proximity to known functional sites. Amino acid physicochemical properties (size, charge, hydrophobicity) inform predictions about whether substitutions are conservative or radical. The Grantham distance, a measure of biochemical dissimilarity between amino acid pairs, contributes to assessing substitution severity.

PolyPhen-2 provides two models trained on different datasets: HumDiv, trained on disease-causing and neutral variants from protein sequence databases, and HumVar, trained on Mendelian disease mutations versus common human polymorphisms. The choice of training set affects score interpretation; HumVar produces more conservative predictions appropriate for clinical Mendelian disease variant classification, while HumDiv is more sensitive and may be preferable for research applications.

PolyPhen-2 scores range from 0 to 1, with higher scores indicating greater predicted deleteriousness. The output includes qualitative classifications (benign, possibly damaging, probably damaging) based on score thresholds. A PolyPhen-2 score of 0.95 with a "probably damaging" classification indicates high confidence that the substitution disrupts protein function, though the clinical significance depends on additional evidence about the specific disease context.

### Limitations of Protein-Level Prediction

Several fundamental limitations constrain all protein-level predictors. First, they are restricted to missense variants; nonsense, frameshift, splice-altering, and non-coding variants lie outside their scope. Second, they predict impact on protein function without specifying the mechanism or clinical consequence. A variant predicted to damage function might impair enzymatic activity, disrupt protein folding, eliminate a binding interface, or alter stability. The clinical relevance depends on which function is affected and whether the phenotype results from loss or gain of function. Third, protein-level predictors provide no information about inheritance mode, penetrance, or expressivity. A strongly predicted-damaging variant in a gene with high tolerance to heterozygous loss may be clinically benign in carriers.

These tools also inherit the training data biases present in their underlying databases. Disease mutations in training sets are enriched for severe, early-onset Mendelian conditions with clear inheritance patterns. Variants causing subtle effects, incomplete penetrance, or complex phenotypes may be systematically mispredicted. The well-studied genes that dominate training data may not generalize to poorly characterized genes where variants are most difficult to interpret.

Despite these limitations, SIFT and PolyPhen-2 remain widely used in clinical practice and serve as features within more sophisticated ensemble methods. Their scores appear in diagnostic reports, contribute to ACMG-AMP classification criteria, and inform variant prioritization in research and clinical pipelines. Understanding their construction and limitations is essential for appropriate interpretation.


## The CADD Framework

Combined Annotation-Dependent Depletion (CADD) represented a fundamental shift in variant effect prediction [@kircher_general_2014; @rentzsch_cadd_2019]. Rather than focusing on a single signal or variant type, CADD defined a general framework for genome-wide variant prioritization that integrates dozens of heterogeneous annotations and uses evolutionary depletion as a proxy training label. The key insight was to avoid training directly on small sets of known pathogenic versus benign variants, which are scarce and biased toward certain genes and variant types. Instead, CADD contrasts variants that have survived purifying selection in the human lineage with matched simulated variants that could have occurred but did not. This evolutionary proxy strategy yields an enormous training set, enables genome-wide coverage, and produces scores that generalize across coding and non-coding regions alike.

### Evolutionary Proxy Training

The most important conceptual contribution of CADD was reframing variant effect prediction as a large-scale machine learning problem with labels derived from evolutionary signal rather than clinical curation. Rather than relying solely on curated clinical labels, CADD uses evolution as a weak supervisory signal, constructing two proxy classes of variants.

The proxy-neutral class consists of variants that have been tolerated by purifying selection. CADD draws these from sequence differences that arose on the human lineage since the split from chimpanzees and became fixed or nearly fixed in modern humans. These derived human alleles are present at very high frequency in human populations yet absent from the inferred human-chimp ancestral sequence. Because they have persisted over millions of years of evolution, most are presumed to be neutral or only weakly deleterious. This is not a perfect proxy: some observed alleles are genuinely pathogenic, particularly those with incomplete penetrance, late onset, or context-dependent effects. However, the proxy-neutral class is, on average, substantially enriched for tolerated alleles relative to a random sample of possible mutations.

The proxy-deleterious class is constructed by simulating mutations across the genome according to realistic mutational processes. The simulation matches local sequence context, typically using trinucleotide frequencies to capture the strong dependence of mutation rates on flanking bases. CpG dinucleotides, for example, have elevated mutation rates due to spontaneous deamination of methylated cytosines, and the simulation accounts for this by generating more CpG transitions. Regional variation in mutation rates, driven by factors including replication timing and chromatin state, is similarly incorporated.

The logic underlying this construction is subtle but powerful. Simulated variants represent changes that could plausibly occur under human mutational processes but are generally not observed at high frequency in population databases. The proxy-deleterious class as a whole is enriched for alleles disfavored by selection, because the set of possible mutations includes many that disrupt conserved elements, alter protein function, or perturb regulatory sequences. By contrasting this set with the proxy-neutral class, CADD learns to recognize the annotation signatures that distinguish variants under purifying selection from those that have been tolerated.

### Feature Integration

CADD's second conceptual pillar is the integration of many weak, noisy annotations into a single composite score. Where earlier variant effect predictors typically relied on one or a few signals, CADD combines more than 60 features. This integrative approach recognizes that no single annotation captures the full complexity of variant function.

Gene model annotations describe the local transcript and coding context of each variant. The most fundamental is the predicted sequence consequence: whether a variant is synonymous, missense, nonsense, frameshift, splice-site disrupting, or located in untranslated or intronic regions. Distance to exon-intron boundaries and proximity to canonical splice sites provide additional context. Gene-level attributes including constraint metrics (pLI, LOEUF from gnomAD) quantify how tolerant each gene is to damaging variation.

Conservation features from phyloP, GERP, and phastCons provide the evolutionary signals described earlier in this chapter. By incorporating multiple conservation metrics computed from different alignments and using different methodologies, CADD captures complementary aspects of evolutionary constraint.

Protein-level predictions from SIFT and PolyPhen-2 contribute assessments of amino acid substitution impact for coding variants. Amino acid physicochemical properties, Grantham distances, and domain annotations from databases like Pfam provide additional structural context.

Regulatory annotations derived from ENCODE and Roadmap Epigenomics data capture chromatin accessibility, histone modifications, and transcription factor binding. These features help prioritize non-coding variants that disrupt active regulatory regions.

Additional features capture local sequence context (GC content, CpG density), genomic architecture (segmental duplications, repetitive elements), and chromosomal position. The power of CADD lies in learning how to weight and combine these heterogeneous signals, upweighting annotations that distinguish proxy-deleterious from proxy-neutral variants and downweighting those that do not.

### Model Architecture and Scoring

CADD's classifier operates on the high-dimensional feature vector assembled for each variant. The original CADD model uses a linear support vector machine trained to discriminate proxy-neutral and proxy-deleterious variants based on approximately 30 million training examples. The choice of a linear model was deliberate and pragmatic: with tens of millions of training examples and dozens of features, a linear SVM is computationally tractable while capturing the main structure of the classification problem.

Raw CADD scores are not directly interpretable as probabilities or biological effect sizes. To provide a more intuitive scoring system, CADD defines PHRED-scaled scores based on the rank of each variant among all possible single-nucleotide substitutions in the reference genome. A scaled score of 10 indicates that a variant falls in the top 10% of predicted deleteriousness. A score of 20 indicates the top 1%, and a score of 30 indicates the top 0.1%. This rank-based transformation ensures comparability across CADD versions and provides immediate interpretability.

In rare disease pipelines, CADD scaled scores are commonly used as filters to enrich for potentially pathogenic variants. Typical thresholds range from 15 (top 3%) to 20 (top 1%) or higher. Variants with scores at or above 20 are considered moderately high deleteriousness candidates, while scores at or above 30 are frequently interpreted as strongly enriched for functional impact. These filters serve as prioritization tools that reduce the variant burden to a manageable number for expert review rather than as definitive pathogenicity calls.


## Other Ensemble Methods

CADD established the paradigm of integrative scoring, but several other ensemble methods have achieved widespread use in clinical and research settings. These approaches differ in their training data, feature sets, and target variant types, offering complementary perspectives on variant deleteriousness.

### REVEL

Rare Exome Variant Ensemble Learner (REVEL) represents a missense-specific ensemble predictor widely used in clinical laboratories [@ioannidis_revel_2016]. Rather than training on evolutionary proxy labels like CADD, REVEL directly discriminates pathogenic missense variants (curated from HGMD and other disease databases) from rare putatively neutral missense variants observed in population datasets.

REVEL integrates predictions from a panel of individual tools: SIFT, PolyPhen-2, PROVEAN, MutationAssessor, FATHMM, GERP++, phyloP, and phastCons, among others. A random forest model learns to combine these scores, weighting each according to its discriminative power for the pathogenic versus neutral classification. The training set is carefully constructed to avoid label contamination, excluding variants present in both disease and population databases.

REVEL scores range from 0 to 1, with higher values implying greater pathogenicity likelihood. Common interpretation thresholds treat scores above 0.5 as supporting evidence for pathogenicity, with scores above 0.75 providing stronger evidence. REVEL is restricted to missense single-nucleotide variants, making it more specialized than CADD but often more accurate within its scope.

### M-CAP

Mendelian Clinically Applicable Pathogenicity (M-CAP) addresses specifically the challenge of distinguishing pathogenic from benign rare missense variants in Mendelian disease contexts [@jagadeesh_m-cap_2016]. The method uses gradient boosting on a feature set including conservation scores, protein structure features, and amino acid properties.

M-CAP was explicitly designed to minimize false positives while maintaining reasonable sensitivity, recognizing that clinical variant interpretation requires high specificity. The authors tuned their classifier to achieve less than 5% false positive rate on known pathogenic variants, accepting some reduction in sensitivity as a tradeoff. This design philosophy differs from methods that balance sensitivity and specificity equally, reflecting M-CAP's intended use in diagnostic settings where false positive pathogenicity calls have serious consequences.

### Comparison and Selection

No single ensemble method dominates across all variant types and clinical contexts. CADD provides the broadest coverage (genome-wide, all variant types) but may sacrifice accuracy within specific variant classes to achieve this generality. REVEL often outperforms CADD on missense-only benchmarks, reflecting its focused training objective. M-CAP prioritizes specificity over sensitivity, appropriate for clinical settings where avoiding false positives is paramount.

Clinical variant interpretation typically incorporates multiple computational scores rather than relying on any single predictor. Different scores may agree (providing stronger evidence) or disagree (flagging variants requiring careful manual review). Understanding the construction, training data, and intended use case of each method is essential for appropriate interpretation.


## Circularity and Ascertainment Bias

Two pervasive problems affect variant effect predictors and their evaluation: circularity between scores and clinical databases, and ascertainment bias in the variants available for training and testing. These issues do not invalidate classical scores but require careful attention in both development and application.

### The Circularity Problem

ClinVar and similar clinical databases increasingly incorporate computational predictions as evidence supporting variant classification. When a clinical laboratory classifies a variant as pathogenic, the CADD score, PolyPhen-2 prediction, or other computational evidence may have contributed to that determination. If CADD is subsequently evaluated on ClinVar pathogenic variants, its performance is artificially inflated: the benchmark contains variants that were labeled partly because CADD assigned them high scores.

This circularity operates through multiple pathways. Direct use occurs when clinical laboratories explicitly cite computational scores in their classifications. Indirect influence arises when computational predictions shape clinical suspicion, affecting which variants receive functional testing or expert review. Selection bias in benchmark construction can compound the problem: benchmark creators may preferentially include variants with strong computational evidence, excluding ambiguous cases that would provide a more stringent test.

The consequence is that benchmark performance may overestimate real-world utility. A method that has been widely adopted will appear to perform well on benchmarks populated by variants classified using that method, even if its true discriminative power is more limited. This concern applies to all established computational tools, including conservation scores, protein-level predictors, and ensemble methods.

Addressing circularity requires careful benchmark construction. Temporal holdouts (using only classifications made before a method's widespread adoption) can reduce but not eliminate the problem. Functional assays that directly measure variant effects provide ground truth independent of computational predictions but are available for only a small fraction of variants. Prospective evaluation on newly classified variants offers the cleanest test but requires patience and ongoing data collection.

### Ascertainment Bias

The variants available for training and evaluating predictors represent a biased sample of all possible disease-causing mutations. Clinical databases are dominated by variants in well-studied genes, particularly those causing severe Mendelian phenotypes with clear inheritance patterns. Protein-coding variants are overrepresented because they are easier to interpret and more often tested. Variants in genes associated with common diagnostic panels appear frequently; those in rarely tested genes are sparse.

This ascertainment bias shapes what models learn and how they perform. A predictor trained predominantly on variants in constrained genes may learn that gene-level constraint is the primary signal for pathogenicity. When applied to variants in less constrained genes (where pathogenic variants also occur, but less frequently), the model may systematically underestimate risk. Similarly, models trained on European-ancestry samples may encode population-specific patterns that transfer poorly to other populations.

The consequences extend to evaluation. Benchmark variants inherit the ascertainment biases of their source databases. Strong performance on benchmark sets may not translate to the rare genes, unusual variant types, or underrepresented populations encountered in real clinical practice. Variants that are "easy" to classify (stop-gains in highly constrained genes, for example) are overrepresented in benchmarks, while diagnostically challenging variants (missense variants in moderate-constraint genes, non-coding variants) are underrepresented.

### Implications for Clinical Use

These limitations do not render classical scores useless, but they counsel appropriate humility in interpretation. Computational predictions provide one line of evidence among several in variant interpretation. Strong scores in expected directions support clinical suspicion; unexpected scores prompt careful review. No computational score should override clear clinical or functional evidence, and borderline scores in complex cases may warrant agnosticism rather than confident prediction.

The ACMG-AMP framework for variant classification appropriately treats computational predictions as supporting evidence (PP3 for predictions supporting pathogenicity, BP4 for predictions supporting benign status) rather than standalone criteria [@richards_standards_2015]. Multiple lines of computational evidence may be combined, but the weight assigned should reflect the limitations outlined here. Variants classified primarily on computational grounds should be flagged for potential reclassification as additional evidence emerges.


## Limitations of the Feature Engineering Paradigm

Classical variant effect prediction achieved substantial success in prioritizing potentially pathogenic variants and established conceptual foundations that persist in modern methods. The integration of diverse annotations, use of evolutionary signals as proxy labels, and genome-wide precomputation all anticipate contemporary practices. Nevertheless, the feature engineering paradigm faces fundamental limitations that motivated the shift toward learned representations.

### The Feature Ceiling

Manually designed features encode human knowledge about which genomic properties matter for variant function. Conservation scores capture evolutionary constraint; protein-level predictors encode structural intuitions; regulatory annotations mark biochemically active regions. This encoded knowledge is valuable but necessarily incomplete. Biologists cannot specify all relevant patterns in advance, and the interactions between features may be too complex for simple combination rules to capture.

The performance of feature-engineered methods is therefore bounded by the quality and completeness of the features themselves. Adding more features provides diminishing returns as the most informative signals are exhausted. Interactions between features (a variant in a conserved enhancer within a constrained gene) may require explicit specification or rely on simple combination rules that miss nonlinear relationships.

### Limited Context

Classical features typically describe variants in isolation or with minimal context. Conservation scores examine each position independently. Protein-level predictors consider amino acid substitutions without full protein context. Gene-level features apply uniformly across entire genes regardless of position-specific effects. This limited context prevents classical methods from learning the complex sequence patterns that determine variant effects.

A variant disrupting a critical transcription factor binding motif may escape detection if the motif is not annotated, even though the underlying sequence pattern is learnable from data. A missense variant at a protein-protein interface may be more damaging than one in a loop region, but capturing this distinction requires understanding protein structure and interactions that feature engineering incompletely represents.

### The Path Forward

These limitations motivated the deep learning approaches covered in subsequent chapters. Convolutional neural networks learn local sequence patterns directly from data, discovering regulatory motifs without requiring pre-specification. Transformers extend this learning to long-range dependencies spanning thousands of bases. Foundation models pretrained on massive sequence databases learn representations that capture protein structure, evolutionary relationships, and regulatory grammar from sequence alone.

The transition from CADD to these modern approaches represents a shift from encoding knowledge as features to learning representations from data. Yet classical methods remain valuable as baselines, as interpretable components in hybrid systems, and as reminders that the variant effect prediction problem is fundamentally difficult regardless of methodology. The circularity and ascertainment issues that affect CADD equally affect deep learning methods; learned representations do not automatically overcome training data limitations.

@sec-vep-fm examines how foundation models have advanced variant effect prediction, comparing their performance to classical methods and addressing the persistent challenges that affect all approaches. Understanding the achievements and limitations of classical methods provides essential context for evaluating these newer approaches and for the ongoing challenge of variant interpretation in clinical genomics.