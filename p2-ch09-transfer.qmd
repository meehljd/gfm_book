# Transfer Learning and Adaptation {#sec-transfer}

## The Promise of Universal Representations

Train once, deploy everywhere. This vision drives the foundation model paradigm: invest heavily in pretraining on massive unlabeled data, then adapt the resulting model cheaply to any downstream task. The appeal is obvious. Rather than training separate models for splice site prediction, variant effect scoring, and regulatory element identification, a single pretrained model provides a universal starting point that can be fine-tuned with minimal labeled data. The representations learned during pretraining capture general sequence patterns (motifs, compositional regularities, long-range dependencies) that transfer across tasks, reducing both the data and compute required for each new application.

The reality proves more nuanced. Transfer learning succeeds spectacularly in some settings: protein language models predict variant effects with near-zero task-specific training, achieving competitive performance through log-likelihood ratios alone. DNA foundation models enable chromatin accessibility prediction from a few hundred labeled examples. Yet transfer also fails in subtle and frustrating ways. Models trained on human sequences struggle with other species. Representations optimized for coding regions miss noncoding regulatory logic. Tasks that seem conceptually similar turn out to require incompatible features. **The central challenge of transfer learning is not whether to adapt pretrained models, but knowing when adaptation will help and choosing strategies that maximize benefit while minimizing risk.**

The adaptation strategies examined in this chapter span a spectrum from minimal intervention (frozen features with a linear probe) to aggressive modification (full fine-tuning of all parameters). Each point on this spectrum trades off preservation of pretrained knowledge against flexibility for task-specific learning. Frozen approaches guarantee that pretrained representations remain intact but cannot escape their limitations. Fine-tuning enables task-specific optimization but risks overwriting useful general knowledge. Parameter-efficient methods attempt to navigate this tension, enabling targeted adaptation while protecting the bulk of pretrained weights. Understanding when each strategy is appropriate, and developing intuition for diagnosing transfer failures, separates effective practitioners from those who apply foundation models as black boxes.

## Source and Target Domains

Transfer learning operates across two domains: the source domain where pretraining occurs and the target domain where the model will be deployed. Understanding what properties of each domain determine transfer success provides the conceptual foundation for choosing adaptation strategies.

The source domain in genomics typically consists of abundant unlabeled sequence data. For DNA models, this might span the human reference genome, pan-genomes from multiple species, or metagenomic collections from environmental samples. For protein models, UniRef and similar databases provide billions of sequences sampling the diversity of evolutionary history. Pretraining objectives like masked language modeling encourage models to learn regularities that help predict held-out tokens: local motifs, compositional patterns, long-range dependencies, and the statistical signatures that distinguish functional from random sequence. These learned regularities form the representations that transfer to downstream tasks.

The target domain differs fundamentally. Rather than abundant unlabeled data, the target domain offers sparse labeled examples of a specific task. A few thousand enhancer sequences with luciferase activity measurements. Several hundred variants with clinical pathogenicity classifications. Chromatin accessibility profiles across a handful of cell types. The target distribution may look nothing like the pretraining data. Pathogenic variants are rare outliers, not typical genomic sequence. Tissue-specific enhancers exhibit patterns that genome-wide pretraining may never emphasize. The gap between source and target distributions creates the core transfer learning challenge.

Several factors determine whether this gap can be bridged. Task relatedness matters profoundly. If the target task requires sequence patterns similar to those the model encountered during pretraining, the pretrained representations will prove useful. If the target task depends on patterns the pretraining objective ignored or actively suppressed, transfer may provide little benefit. Target data quantity constrains which adaptation strategies are feasible. With thousands of labeled examples, aggressive fine-tuning can reshape representations toward task requirements. With dozens of examples, only the lightest-touch approaches avoid overfitting. Model architecture and scale influence adaptation flexibility: larger models with more expressive internal representations can potentially adapt to more diverse tasks, but also risk memorizing small target datasets.

Not all transfer helps. Positive transfer accelerates learning or improves final performance beyond what training from scratch achieves. Negative transfer occurs when pretraining actively hurts performance, either because learned features conflict with task requirements or because the pretrained initialization creates optimization difficulties. Neutral transfer describes situations where pretraining neither helps nor hurts, often when target data is sufficient to learn effectively from scratch or when source and target domains share too little structure for meaningful knowledge sharing. Distinguishing these outcomes requires careful empirical validation, not assumptions about how transfer "should" work.

## Feature Extraction with Frozen Backbones

The simplest adaptation strategy treats the pretrained model as a fixed feature extractor. All backbone parameters remain frozen; only a lightweight classifier trained on top of the extracted representations learns from labeled data. This approach eliminates the risk of catastrophic forgetting (where fine-tuning overwrites useful pretrained knowledge) and requires minimal computation since gradients need not flow through the entire model.

Implementation is straightforward. Pass input sequences through the frozen backbone to obtain embeddings, typically from the final layer or from a designated [CLS] token that aggregates sequence information. These embeddings serve as fixed feature vectors capturing the model's learned understanding of sequence patterns. Train a supervised classifier (logistic regression, linear layer, or shallow neural network) to map embeddings to task labels. The backbone parameters never change.

Linear probing represents the minimal variant. A single linear layer maps embeddings directly to predictions, introducing only $d \times c$ parameters where $d$ is the embedding dimension and $c$ is the number of output classes. This approach trains in minutes and resists overfitting even with very limited labeled data. DNABERT embeddings paired with linear probes achieve competitive chromatin accessibility prediction from a few hundred positive and negative examples, matching CNN baselines that require far more labeled data. The success reflects alignment between the pretraining objective (predicting masked k-mers from context) and the target task (distinguishing sequences based on local motif patterns).

Shallow multilayer perceptrons extend linear probing by adding one or two hidden layers between embeddings and predictions. The additional nonlinearity enables more complex decision boundaries while maintaining computational efficiency and overfitting resistance. With a few thousand labeled examples, shallow MLPs on HyenaDNA embeddings improve splice site prediction over linear probes, capturing interactions between features that linear models cannot represent.

The advantages of frozen feature extraction are clear. Computational cost is minimal. Catastrophic forgetting is impossible since pretrained parameters never change. Training completes quickly, enabling rapid experimentation across tasks. The approach works reliably even with very small labeled datasets. But these advantages come with a fundamental limitation: the backbone cannot adapt to task-specific patterns. Performance is capped by how well pretrained representations align with target task requirements. If the pretraining objective emphasized patterns irrelevant to the downstream task, or if the target task requires features the pretraining objective actively discouraged, frozen features will underperform models trained from scratch regardless of how sophisticated the classifier head becomes.

## Parameter-Efficient Fine-Tuning

Parameter-efficient fine-tuning (PEFT) methods navigate the tension between frozen and full fine-tuning by updating a small subset of parameters while keeping the majority frozen. The model can adapt to task-specific patterns without the computational expense or overfitting risk of modifying all weights. Several PEFT techniques have emerged, with Low-Rank Adaptation (LoRA) proving particularly prominent in genomic applications.

LoRA modifies weight matrices by adding low-rank decompositions. Rather than updating a large weight matrix $W$ directly, LoRA introduces two smaller matrices $A$ and $B$ whose product approximates the desired weight change: $W' = W + BA$. During fine-tuning, $W$ remains frozen while only $A$ and $B$ receive gradient updates. The rank of these matrices (typically 8 to 64 for genomic models) controls the expressiveness of adaptation. Lower ranks introduce fewer parameters and stronger regularization; higher ranks enable more flexible task-specific modification at greater overfitting risk.

The memory and compute savings can be substantial. A transformer with hundreds of millions of parameters might require updating only a few million LoRA parameters, reducing memory requirements by 10 to 100 times compared to full fine-tuning. This efficiency enables training on consumer hardware for models that would otherwise require specialized infrastructure. LoRA adapters on Nucleotide Transformer have enabled tissue-specific expression prediction, where separate low-rank matrices capture tissue-specific regulatory patterns while the pretrained backbone encodes general sequence understanding.

Adapter layers take a different architectural approach. Rather than modifying existing weight matrices, adapters insert small bottleneck modules between transformer layers. Each adapter consists of a down-projection reducing dimensionality, a nonlinear activation, and an up-projection restoring the original dimension. The original transformer parameters remain frozen; only adapter parameters are updated. Different adapters can be trained for different tasks, enabling multi-task deployment from a single shared backbone. Enformer with tissue-specific adapters predicts chromatin states across cell types, with each adapter learning tissue-specific transformations of shared regulatory representations.

Prefix tuning prepends learnable embeddings to the input, effectively providing task-specific context that conditions model behavior. While less common in genomics due to the absence of natural "prompt" structure in biological sequence, prefix tuning has found limited application where task context can be encoded as additional input tokens. Other PEFT methods include BitFit (tuning only bias terms), prompt tuning (learning soft prompts), and compacter-style approaches combining low-rank decomposition with parameter sharing. These remain less explored in genomic contexts but may offer advantages for specific applications.

PEFT methods suit intermediate data regimes (thousands to tens of thousands of examples) where frozen features underperform but full fine-tuning risks overfitting. They enable managing multiple related tasks efficiently, training separate lightweight adapters rather than separate full models. The computational savings make experimentation feasible on modest hardware, accelerating development cycles.

## Full Fine-Tuning

Full fine-tuning updates all model parameters during adaptation, offering maximum flexibility to reshape representations for task-specific requirements. When target datasets are large enough and the target task diverges sufficiently from pretraining, full fine-tuning extracts more value from pretrained models than constrained approaches.

Implementation requires careful attention to optimization details. Learning rates must be substantially lower than during pretraining (often 10 to 100 times smaller) to avoid catastrophically disrupting learned representations in early training steps. Gradual unfreezing, where top layers are updated first and deeper layers are progressively brought into training, helps preserve low-level features while allowing high-level task-specific adjustment. Regularization through weight decay, dropout, and early stopping on validation data prevents overfitting to the target dataset.

Full fine-tuning is appropriate when labeled datasets are large (tens of thousands of examples or more), when the target task differs substantially from pretraining such that frozen or lightly adapted representations prove insufficient, or when performance requirements justify the computational investment. Enformer fine-tuned on new chromatin assays requires updating most parameters to capture assay-specific signal patterns distinct from original training conditions. The expense is justified when the adapted model will be deployed at scale or when the task is central to a research program.

The risks are real. Catastrophic forgetting occurs when fine-tuning overwrites general knowledge useful for related tasks or out-of-distribution inputs. A model fine-tuned aggressively on one tissue type may lose performance on others it previously handled well. Overfitting afflicts small target datasets, where the model memorizes training examples rather than learning generalizable patterns. Computational expense can be prohibitive for large models. These considerations make full fine-tuning a high-reward, high-risk strategy requiring careful validation that the investment yields genuine improvement over simpler approaches.

Best practices emphasize starting conservatively. Begin with frozen features to establish whether transfer provides any benefit. If frozen features improve over training from scratch, try PEFT methods before committing to full fine-tuning. Always compare full fine-tuning against simpler baselines; if the performance gap is small, the additional cost may not be justified.

## Probing Representations

Probing classifiers diagnose what information pretrained representations encode, guiding adaptation strategy by revealing alignment (or misalignment) between learned features and task requirements. Rather than immediately training for the target task, probing systematically interrogates representations to understand their structure.

The approach is simple: train lightweight classifiers to predict properties of interest from frozen embeddings, then examine how well different properties can be decoded. If chromatin accessibility can be predicted accurately from a linear probe, the representations already encode accessibility-relevant features and frozen feature extraction will likely succeed. If transcription factor binding requires a deep nonlinear classifier, the relevant information may be present but not linearly separable, suggesting that PEFT or fine-tuning might reorganize representations for easier extraction. If a property cannot be predicted even with flexible classifiers, the representations may lack the necessary information entirely.

Systematic probing reveals what models learn during pretraining. DNA language models encode local motif information recoverable by simple linear probes but show weaker encoding of long-range dependencies that require multi-layer networks to extract. Protein language models learn secondary structure so thoroughly that linear probes achieve near state-of-the-art prediction accuracy, while tertiary contact information requires nonlinear decoding. These probing results explain why certain transfer tasks succeed readily (they rely on well-encoded features) while others prove difficult (they require information the model encodes weakly or not at all).

Probing guides adaptation decisions. If the target task requires features that probing reveals as well-encoded, frozen feature extraction or minimal adaptation should suffice. If required features are present but deeply buried (requiring nonlinear probes to extract), PEFT methods may help by learning transformations that surface the relevant information. If probing suggests required features are absent or very weakly encoded, practitioners face a choice: either full fine-tuning to reshape representations (if target data is sufficient) or trying a different pretrained model whose pretraining objective better aligns with task requirements.

Layer-wise probing reveals how information flows through the model. Early layers often encode local compositional features (k-mer frequencies, simple motifs) while later layers capture more abstract patterns (long-range dependencies, functional signatures). Tasks depending on local features may benefit from representations extracted from early or middle layers rather than the final layer, which may have abstracted away relevant details. Layer selection becomes another hyperparameter to optimize when adapting pretrained models.

## Choosing an Adaptation Strategy

Selecting the appropriate adaptation strategy requires balancing three considerations: available labeled data, similarity between pretraining and target tasks, and computational budget. While no rigid rules cover all scenarios, several heuristics guide practical decisions.

Data quantity provides the primary decision axis. With fewer than 1,000 labeled examples, linear probing or shallow classifiers on frozen embeddings is often the only viable approach. More complex adaptation risks overfitting; the limited signal cannot support updating many parameters. With 1,000 to 10,000 examples, PEFT methods offer good tradeoffs between expressiveness and regularization. With more than 10,000 examples, full fine-tuning becomes feasible and may be necessary if the target task diverges substantially from pretraining.

Task similarity to pretraining provides the second axis. When target tasks closely resemble pretraining patterns (predicting transcription factor binding after sequence pretraining involves similar local motif recognition), frozen feature extraction often suffices. The pretrained representations already capture relevant patterns; a simple classifier separates positive from negative examples. For moderately different tasks, PEFT methods enable selective adaptation. For tasks fundamentally different from pretraining (where required features were not emphasized or were actively suppressed during pretraining), full fine-tuning may be necessary to reshape representations.

Computational budget imposes practical constraints. With minimal resources, only linear probing is feasible. With moderate resources, LoRA achieves much of fine-tuning's benefit at a fraction of the cost. With generous resources, full fine-tuning becomes an option, though comparing against simpler baselines remains essential to verify the investment is justified.

Empirical validation trumps heuristics. No rule perfectly predicts which strategy will succeed. Always compare multiple approaches. Validate on held-out data matching the intended deployment distribution. Monitor for overfitting versus underfitting and adjust strategy accordingly. The goal is developing intuition for which strategies merit trying, not blindly following rules.

## Domain Shift and Cross-Context Transfer

Transfer learning assumes that knowledge from the source domain applies to the target domain. When this assumption breaks, performance degrades in ways that may not be obvious from standard validation metrics. Three types of domain shift commonly afflict genomic transfer: cross-species, cross-tissue, and cross-assay transfer.

Cross-species transfer applies models trained on one organism to another. The challenge is that evolutionary divergence introduces sequence differences affecting regulatory patterns, motif syntax, and functional constraints. Strategies for cross-species success include pretraining on multi-species data to learn conservation patterns, fine-tuning with species-specific adapters, and focusing on highly conserved features that transfer more readily. Human-to-mouse regulatory prediction faces both sequence divergence and lineage-specific regulatory innovations; core promoter elements transfer better than species-specific enhancers. Transfer across greater phylogenetic distances becomes progressively more difficult.

Cross-tissue transfer addresses tissue-specific regulatory programs. Gene expression and chromatin accessibility vary dramatically across tissues, with thousands of tissue-specific enhancers and repressors. Models trained predominantly on one tissue may miss regulatory logic specific to others. Effective strategies include shared backbones with tissue-specific prediction heads, tissue-conditional models taking tissue identity as input, and meta-learning approaches training across many tissues to extract general principles. Broadly expressed housekeeping genes transfer more readily than tissue-restricted genes.

Cross-assay transfer handles different molecular readouts of related biology. ChIP-seq and ATAC-seq both measure chromatin state but with different biochemistry and signal characteristics. Bulk and single-cell RNA-seq quantify expression at vastly different scales. Successful cross-assay transfer requires understanding the mechanistic relationships between assays and either multi-task pretraining across assays or explicit modeling of assay-specific biases.

Detecting domain shift before deployment prevents silent failures. Statistical tests comparing source and target distributions flag potential problems. Embedding visualizations reveal whether target examples fall within the source distribution or in unfamiliar regions of representation space. Monitoring performance on "canary" examples (known easy cases that should always be predicted correctly) provides early warning of severe shift.

## Few-Shot and Zero-Shot Learning

Few-shot learning addresses scenarios with extremely scarce labels: 10 to 100 examples per class rather than thousands. This regime characterizes many genomic applications. Rare variant classes in ClinVar have few examples. Novel cell types in single-cell studies lack extensive annotation. Newly characterized functional elements have limited experimental validation. Standard adaptation approaches overfit catastrophically with so few examples.

Meta-learning trains models explicitly for rapid adaptation. Model-Agnostic Meta-Learning (MAML) finds parameter initializations that can be fine-tuned effectively from minimal data. The model learns across many few-shot tasks during meta-training, optimizing for fast adaptation rather than performance on any single task. At deployment, a few gradient steps on new labeled examples produce a task-specific model. Prototypical networks learn to classify based on distance to class prototypes in embedding space, enabling classification from a handful of examples per class. These approaches remain underexplored in genomics but offer promise for the many settings where labels are genuinely scarce.

Zero-shot transfer makes predictions without any task-specific adaptation. The pretrained model produces outputs that can be interpreted as task predictions without further training. For protein variant effect prediction, ESM log-likelihood ratios score variants by how much they reduce the model's probability of the observed sequence. Variants disrupting the model's learned expectations for natural proteins are flagged as potentially deleterious. This approach proves competitive with supervised methods because evolutionary constraint (captured by pretraining on natural sequences) correlates with functional importance.

Zero-shot methods require strong alignment between pretraining objectives and target tasks. In genomics, most practical applications still require at least some labeled data; few-shot learning represents a more realistic minimal-data regime than true zero-shot transfer.

## When Transfer Fails

Transfer learning can fail spectacularly, and understanding failure modes prevents wasted effort on adaptation strategies that cannot succeed. Negative transfer occurs when pretraining actively hurts performance, producing models worse than those trained from scratch on target data alone.

Common failure modes include misaligned pretraining objectives that emphasize features irrelevant or counterproductive for the target task. A model pretrained to predict masked nucleotides in coding sequence may learn features specific to protein-coding regions that mislead when applied to noncoding regulatory elements. Pretraining on data from one species may encode species-specific patterns that create false expectations when transferred to another organism.

Diagnostic steps help identify whether transfer is helping or hurting. First, compare adapted model performance against a from-scratch baseline trained on the same target data. If the pretrained model does not outperform from-scratch training, transfer provides no benefit. Second, try simpler adaptation strategies before complex ones. If linear probing fails, full fine-tuning is unlikely to help unless the target dataset is large. Third, visualize embeddings from the pretrained model using dimensionality reduction. If target task examples are not well-separated in embedding space, the pretrained representations may not encode useful features for this task. Fourth, ablate pretraining entirely by comparing against randomly initialized models. This isolates whether pretrained weights provide value or whether architectural choices alone drive performance.

When diagnostics reveal fundamental mismatches, several solutions may help. Task-specific pretraining on data more closely aligned with the target task can bridge the gap. Pretraining specifically on regulatory regions for regulatory prediction tasks, rather than genome-wide pretraining, may produce more suitable representations. Hybrid approaches combining pretrained and from-scratch components allow selective use of transfer where it helps. Trying different foundation models may reveal better alternatives. And accepting that transfer does not help, proceeding with from-scratch training, remains a valid option when pretrained representations genuinely misalign with task requirements.

## Case Studies in Transfer Learning

Examining concrete applications illustrates the principles developed throughout this chapter. Four case studies span different model architectures, adaptation strategies, and application domains.

DNABERT applied to chromatin accessibility prediction demonstrates successful feature extraction. The model was pretrained using masked language modeling on k-mer tokenized human genomic sequence. For ATAC-seq peak classification, a linear probe on [CLS] token embeddings achieved competitive performance with CNNs trained from scratch while requiring 10 times less labeled data. This success reflects strong alignment between pretraining (learning local sequence patterns) and the target task (identifying accessibility signals depending on motif composition). The lightweight adaptation was appropriate given limited labeled ATAC-seq data.

ESM for variant effect prediction illustrates zero-shot and minimal-supervision transfer. ESM was pretrained on UniRef protein sequences using masked language modeling. For ClinVar pathogenicity classification, zero-shot scoring based on how variants reduce sequence likelihood proved competitive with supervised methods. Adding a linear probe on ESM embeddings improved performance further. Pretraining captures the target objective implicitly: evolutionary constraint, which masked language modeling learns to predict, correlates with functional importance.

Enformer for cross-tissue gene expression shows benefits of full fine-tuning. Pretrained on thousands of chromatin and expression tracks, Enformer learned sequence-to-function mappings over long genomic contexts. Fine-tuning with tissue-specific prediction heads captured tissue-specific regulatory logic unavailable from frozen features, outperforming from-scratch models on individual tissues. The large scale of both pretraining and fine-tuning data justified computational expense.

HyenaDNA for regulatory element classification leverages long-range context. Pretrained on human genomic sequence with contexts up to one million base pairs, HyenaDNA embeddings capture distal regulatory relationships. LoRA adapters enabled efficient fine-tuning for enhancer and promoter classification, with long-range context improving accuracy on elements depending on distal interactions. This case demonstrates architecture-specific pretraining benefits: the long context that distinguishes HyenaDNA enables transfer to tasks where long-range dependencies matter.

## Validation and Common Pitfalls

Proper validation distinguishes genuine transfer success from artifacts of evaluation design. Several failure modes pervade genomic model evaluation, many arising from subtle data leakage or inappropriate evaluation protocols.

Test set overlap with pretraining data creates artificial inflation of transfer learning performance. Foundation models trained on massive corpora may inadvertently include sequences later used for evaluation. When evaluating models on "held-out" data that actually appeared during pretraining, the model has seen the answers. Verifying that test sequences were excluded from pretraining corpora requires careful provenance tracking.

Temporal leakage uses future information unavailable at prediction time. Evaluating a variant pathogenicity model on variants annotated after the training data was collected creates an unrealistically favorable setting. Temporal splits, where the model sees only variants discovered before a cutoff and is evaluated on variants discovered afterward, provide more realistic assessment.

Inappropriate baselines inflate apparent transfer learning benefits. Comparing adapted foundation models against weak or poorly-tuned from-scratch baselines makes transfer look more valuable than it is. Strong baselines, including properly hyperparameter-tuned models trained on the same target data, provide fair comparisons.

Reporting confidence intervals from multiple training runs reveals performance variability. A single training run may produce misleadingly good or bad results. Testing on multiple datasets rather than a single benchmark reveals whether gains generalize. Failure case analysis, examining where and why the model errs, often reveals more about model behavior than aggregate metrics.

## Emerging Directions

Transfer learning continues evolving, with several directions particularly relevant to genomic applications.

In-context learning, where models make predictions by conditioning on examples provided in the input context, has emerged as a powerful capability in very large language models. Early evidence suggests that sufficiently large genomic models may exhibit similar behavior, performing tasks by observing a few examples without explicit fine-tuning. This could transform deployment: rather than training adapters or fine-tuning, users would simply provide examples of desired behavior.

Test-time adaptation updates models during inference based on characteristics of test examples. Rather than freezing models after training, test-time adaptation allows limited parameter updates to match deployment conditions. This approach handles distribution shift without requiring labeled target data.

Federated learning enables collaborative training across institutions without sharing raw data, addressing privacy constraints that limit data pooling in clinical genomics. Institutions train local models on private data and share only aggregated updates. This could enable foundation models trained on far more diverse data than any single institution can access.

Better theory for predicting when transfer will succeed based on measurable source and target properties would reduce trial-and-error. Currently, practitioners must empirically test whether transfer helps; theoretical guidance could focus effort on promising combinations.

## Practical Guidelines

Transfer learning bridges pretrained models and specific applications, balancing adaptation flexibility against computational cost and overfitting risk. Several key principles emerge from this chapter.

Match adaptation strategy to available data. With fewer than 1,000 examples, feature extraction is safest. With 1,000 to 10,000 examples, PEFT methods balance expressiveness and regularization. With more than 10,000 examples, consider full fine-tuning but compare against simpler baselines.

Validate that transfer helps. Compare adapted models against from-scratch baselines trained on the same target data. Without this comparison, the value of pretrained models cannot be established. Sometimes training from scratch performs equivalently or better.

Consider domain shift. Models trained in one context may fail silently in another. Explicit domain adaptation and careful out-of-distribution evaluation help identify and mitigate these risks.

Start simple and increase complexity as needed. Linear probes train quickly and often prove surprisingly effective. Invest in more complex adaptation only when simpler approaches demonstrably fail.

Use probing to diagnose representations before committing to adaptation strategies. Understanding what features the pretrained model encodes guides choices about whether frozen features will suffice or whether more aggressive adaptation is needed.

These guidelines connect to later chapters examining specific foundation model families. The foundation model paradigm examined in @sec-fm-principles rests on the assumption that transfer succeeds: that pretraining on massive unlabeled data produces representations useful across diverse downstream tasks.  DNA language models (@sec-dna-lm), protein language models (@sec-protein-lm), and regulatory models (@sec-regulatory) each present characteristic transfer learning patterns. Variant effect prediction (@sec-vep-fm) synthesizes transfer across modalities. The evaluation principles in @sec-eval and confounding considerations in @sec-confounding further inform validation of transfer learning claims. Throughout, the core message persists: transfer learning is powerful but not automatic. Understanding when and how to adapt pretrained models separates effective practitioners from those who treat foundation models as black boxes.

