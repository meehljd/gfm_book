::: {.callout-warning .content-visible when-profile="draft"}
**TODO:**

- Add figure: Multi-omics integration strategies diagram showing early, intermediate, and late fusion approaches with representative models
- Add figure: CpGPT architecture schematic showing masked modeling objective, sample embeddings, and downstream task adaptation
- Add figure: GLUE framework diagram illustrating modality-specific VAEs, feature graph structure, and alignment objectives
- Add figure: GNN-based cancer subtyping comparison showing MoGCN patient-level graphs vs. CGMega gene-level modules
- Add figure: DeepRVAT set-based architecture showing variant aggregation into gene-level impairment scores
- Add figure: G2PT hierarchical structure from variants → genes → systems → phenotypes
- Add table: Comparison of multi-omics integration methods (GLUE, MoGCN, CGMega, etc.) with columns for input modalities, graph type, primary use case, and scalability
- Add table: Design patterns summary showing the five key patterns with representative methods and typical applications
- Consider adding conceptual diagram showing the trajectory from single-omic models toward whole-patient foundation models
:::


# Multi-omics & Systems Context  {#sec-multi}

Modern genomic foundation models excel at learning from sequences, structures, or single-omic profiles in isolation. Yet most complex traits arise from systems-level interactions: genetic variants perturb molecular networks, networks span multiple omics layers, and these layers interact with environment, development, and clinical context. A model that sees only one layer rarely captures the full story.

This chapter surveys how deep learning extends beyond single-omics to integrate methylation, chromatin, expression, protein, and clinical data into unified representations. As the final chapter of Part IV, it serves as a bridge from model-centric architecture design to systems-level, clinically grounded applications in Part VI. The methods introduced here illustrate emerging design patterns for systems-aware genomic foundation models that move from isolated sequences toward whole-patient representations.

We examine several archetypal systems that represent different facets of this integration challenge. CpGPT demonstrates how foundation modeling principles apply to DNA methylation, treating the methylome as a sequence-like object amenable to transformer-based pretraining. GLUE and its single-cell variant SCGLUE show how graph-linked embeddings can align cells across modalities when different omics are measured in different cells. Graph neural network approaches to cancer subtyping, including MoGCN and CGMega, illustrate how patient similarity networks and gene-level graphs can integrate genomics, transcriptomics, and proteomics for classification and biomarker discovery. DeepRVAT, NeEDL, and the Genotype-to-Phenotype Transformer address rare variants and epistasis, effects that linear PGS models largely miss. Finally, deep learning frameworks for polygenic risk and fine-mapping, including Delphi and MIFM, extend the PGS paradigm from @sec-pgs with nonlinear architectures and foundation model features.

Together, these approaches point toward a future where genomic models reason across biological scales, from single nucleotides through molecular networks to whole-patient phenotypes.

## Why Single-omics Models Are Not Enough

Earlier chapters emphasized how sequence-based models can predict variant effects from local DNA or protein context. These models already improve causal variant prioritization and polygenic risk scoring. However, they typically assume a narrow view of biology.

Most sequence models operate on a single molecular layer. A convolutional network or transformer may see only DNA sequence, or only expression values, without access to the other layers that mediate the flow of genetic information. Even when multiple outputs are predicted simultaneously, as in multi-task models like Enformer, the input remains a single modality.

Many downstream uses treat variant effects as additively summing across loci. The PGS framework from @sec-pgs exemplifies this assumption: effects of individual variants are estimated independently and combined through weighted sums. While linear models have well-understood statistical properties and interpretability, they cannot capture interactions between variants or between molecular layers.

Models rarely account for dynamic cellular state. The same sequence may have different regulatory consequences depending on cell type, developmental stage, or environmental exposure. Static sequence-to-function models provide context-averaged predictions that may not reflect biology in any particular condition.

Real diseases violate all three of these assumptions. Regulation is inherently multi-layered: genetic variants alter chromatin accessibility and DNA methylation, which modulate transcription, which affects splicing and translation, which determines protein levels and modifications. A variant's consequences propagate through this cascade in ways that single-layer models cannot fully capture.

Effects are context-dependent. A variant might be benign in one tissue and pathogenic in another, depending on which genes are expressed, which transcription factors are present, and how the local chromatin environment is configured. The same variant in different individuals may have different consequences depending on the genetic background and cellular states in which it operates.

Risk is combinatorial. Epistasis and pathway-level perturbations play significant roles in many complex traits. Two variants that individually have small effects might together strongly perturb a pathway, or might cancel each other out. Linear models assume effects are independent and additive, missing these interaction structures entirely.

@sec-pgs highlighted the "missing heritability" and limited cross-ancestry portability of traditional GWAS and linear PGS, motivating sequence-based deep learning. This chapter takes the next step: combining sequence-derived features with multi-omics and systems-level models that better reflect biological organization.

## Foundations of Multi-omics Integration

Multi-omics data come in several flavors that present different integration challenges. Bulk-level profiles such as GWAS variants, bulk RNA-seq, and bulk proteomics aggregate signals across millions of cells, providing population-level or tissue-averaged views. Single-cell modalities including scRNA-seq, scATAC-seq, multiome assays, and spatial omics resolve cellular heterogeneity but introduce sparsity and technical noise. Epigenetic readouts such as DNA methylation arrays, histone modification ChIP-seq, and chromatin conformation capture provide orthogonal views of regulatory state. Clinical and environmental covariates from electronic health records, laboratory measurements, and lifestyle questionnaires add non-molecular dimensions that influence phenotypes.

Integration strategies for combining these data types typically fall into three broad categories that represent different trade-offs between architectural complexity and the ability to capture cross-modal structure.

Early fusion, also called feature-level integration, concatenates normalized features from multiple omics and feeds them into a single model. This approach is straightforward to implement and allows the model to learn arbitrary interactions between features. However, early fusion is sensitive to differences in scale and dimensionality between modalities, handles missing data poorly since any sample lacking one modality must be imputed or excluded, and can be dominated by whichever modality has the most features or highest signal-to-noise ratio.

Intermediate fusion, also called shared latent space integration, learns modality-specific encoders that map each omic into a common embedding space. Alignment between modalities is encouraged through reconstruction losses that require each encoder's latent representation to support decoding back to its original features, contrastive terms that pull together representations of the same biological entity across modalities, or graph constraints that enforce consistency with known biological relationships. Intermediate fusion is the dominant design in modern multi-omics deep learning because it handles missing modalities gracefully (only the available encoder needs to fire), allows modality-specific preprocessing and architectures, and can incorporate biological prior knowledge through the alignment objectives.

Late fusion, also called prediction-level integration, trains separate models for each modality and combines their outputs through ensemble methods or a meta-model. This approach is robust to missing modalities since each sub-model operates independently, and it allows each modality to use whatever architecture works best for its data type. However, late fusion may underutilize cross-omic structure that could inform predictions, since interactions between modalities can only be captured at the final combination stage.

Modern frameworks like GLUE and multi-omics graph neural networks predominantly adopt intermediate fusion, often augmented with graphs that encode known or inferred biological relationships. Gene-peak edges in single-cell multi-omics link chromatin accessibility peaks to the genes they regulate. Gene-transcription factor edges connect genes to the factors that bind their promoters and enhancers. Protein-protein interaction edges capture physical and functional relationships. Sample similarity edges connect patients or cells with similar molecular profiles. The rest of this chapter traces how these design choices implement systems-level reasoning in practice.

## CpGPT: A Foundation Model for DNA Methylation

### Methylation as a Systems Hub

DNA methylation occupies a privileged position in the regulatory hierarchy, sitting at a junction between genotype, environment, and phenotype. Methylation patterns integrate genetic influences, since sequence context affects which CpG sites can be methylated and polymorphisms can create or destroy CpG dinucleotides. They also integrate developmental programs, since methylation landscapes are extensively remodeled during differentiation and establish cell-type-specific regulatory states. Environmental exposures including diet, smoking, toxins, and stress leave lasting methylation signatures that persist long after the exposure ends.

Beyond serving as an integrative readout, methylation encodes rich information about cellular identity and state. Cell types can be distinguished by their methylation profiles, and within a cell type, methylation captures information about age, health status, and disease risk. Epigenetic clocks built from methylation data predict chronological age with remarkable accuracy, and deviations from predicted age correlate with mortality risk and disease burden [@camillo_cpgpt_2024].

Traditional methylation models have been task-specific: one model for age prediction, another for mortality risk, another for tissue classification. Each model is trained from scratch on labeled data for its particular task, learning whatever methylation patterns happen to be predictive without necessarily capturing general structure. CpGPT reframes methylation as a foundation modeling problem, using large-scale pretraining to learn representations that transfer across tasks.

### Architecture and Pretraining

CpGPT, the Cytosine-phosphate-Guanine Pretrained Transformer, treats methylomes as sequences or sets of CpG sites and uses transformer-style self-attention to model their structure [@camillo_cpgpt_2024]. The model was pretrained on over 1,500 DNA methylation datasets encompassing more than 100,000 samples from diverse tissues and conditions.

Several aspects of methylation structure make it amenable to transformer modeling. Local CpG correlations arise because nearby CpG sites tend to share methylation status, particularly within CpG islands. Long-range coordination reflects the fact that methylation patterns at distant genomic regions can be correlated through shared regulatory programs or chromatin compartmentalization. Global sample-level variation captures the systematic differences between samples that reflect tissue identity, age, disease status, and other biological variables.

CpGPT uses masked modeling objectives analogous to BERT-style language model pretraining. During training, a subset of CpG methylation values is masked, and the model learns to reconstruct them from the surrounding context. This forces the model to learn relationships between CpG sites and to capture the statistical structure of methylation profiles.

Multi-task pretraining provides additional signal. Auxiliary objectives such as array platform conversion, where the model learns to translate between different methylation measurement technologies, and reference mapping, where the model learns to align samples to reference profiles, encourage the model to learn robust representations that generalize across technical and biological variation.

The result is a sample-level embedding, analogous to a CLS token representation in language models, that provides a compact, task-agnostic summary of each sample's methylome. This embedding can be used directly for downstream prediction or as input to more complex models.

### Zero-shot and Fine-tuned Tasks

Because CpGPT is trained on diverse cohorts spanning many tissues and conditions, it exhibits zero-shot or few-shot generalization to new tasks. For imputation and array conversion, the model can fill in missing CpG values or translate between different methylation array platforms, enabling harmonization of datasets collected with different technologies. For chronological age prediction, the pretrained model yields age estimates that match or exceed specialized epigenetic clocks, even without task-specific fine-tuning. For mortality risk prediction, CpGPT achieved state-of-the-art performance in the Biomarkers of Aging Challenge. Sample classification tasks such as distinguishing tissues, disease states, or exposure profiles also benefit from the learned representations.

In a multi-omics context, CpGPT-derived embeddings serve several roles. They can be inputs to downstream predictors, providing rich methylation features for risk scores, prognosis models, or treatment response prediction. They can function as one modality in a shared latent space that also includes expression, proteomics, and other data types. They can inject epigenetic state information into otherwise sequence-centric genomic foundation models, providing context about cellular identity and regulatory status.

Conceptually, CpGPT exemplifies a single-omic foundation model designed to plug into multi-omics architectures. The pretraining objective learns general methylation structure, and the resulting embeddings can be combined with other modalities for tasks that require systems-level reasoning.

## GLUE: Graph-linked Unified Embedding for Single-cell Multi-omics

### The Unpaired Single-cell Integration Problem

Single-cell experiments often profile different modalities in different cells. A typical study might include scRNA-seq data from one set of cells, scATAC-seq data from another set, and perhaps a small subset with both modalities measured simultaneously through multiome protocols. The central challenge is building a unified atlas that aligns these cells in a common space, recovers cell types and trajectories, and infers regulatory networks connecting chromatin to expression [@cao_glue_2022].

This problem is harder than standard data integration because the feature spaces are entirely different. RNA-seq measures gene expression across roughly 20,000 genes. ATAC-seq measures chromatin accessibility across hundreds of thousands of peaks. There is no direct correspondence between features: a gene is not the same object as a peak. Aligning cells across modalities requires reasoning about how features in one modality relate to features in another.

Previous approaches addressed this through explicit feature conversion, for example by assigning ATAC-seq peaks to nearby genes and treating the resulting gene-level accessibility as comparable to expression. This conversion is straightforward but loses information, since the detailed structure of chromatin accessibility within a gene's regulatory region is collapsed into a single number. It also introduces arbitrary choices about how to define gene-peak assignments.

GLUE, Graph-Linked Unified Embedding, addresses this problem by combining modality-specific encoders with a graph of biological prior knowledge linking features across omics [@cao_glue_2022].

### Architecture

GLUE consists of three key components that work together to align cells across modalities while respecting biological relationships between features.

Modality-specific variational autoencoders provide the foundation. Each omic has its own encoder-decoder pair. Encoders map cells to a low-dimensional latent embedding, and decoders reconstruct modality-specific features from these embeddings. The variational structure encourages smooth, interpretable latent spaces.

A feature graph encodes biological relationships between features across modalities. Genes, peaks, and motifs form nodes in this graph. Edges capture relationships: a peak linked to a gene's promoter or a distal enhancer predicted to regulate the gene, or a transcription factor binding motif whose presence in a peak suggests regulation of nearby genes. A graph neural network learns feature embeddings that are consistent with this graph structure, ensuring that biologically related features have similar representations.

Alignment objectives tie the components together. Loss terms encourage the cell latent spaces from different modalities to align, so that RNA-only cells and ATAC-only cells with similar biological states end up near each other in the shared embedding space. The feature embeddings from the graph neural network are tied to the cell embeddings through the generative decoders, enforcing consistency between the learned representations and the prior biological knowledge.

The result is a unified embedding in which cells from multiple modalities can be jointly clustered, visualized, and used for downstream analysis. Cell type labels transfer across modalities: once cell types are annotated in one modality, the alignment allows those annotations to propagate to cells from other modalities.

### Applications

The GLUE framework has demonstrated strong performance across several challenging applications. Multi-omics integration at single-cell resolution has been achieved for combinations of RNA, ATAC, methylation, and protein data. The graph structure provides a natural framework for regulatory network inference, linking chromatin features to gene expression through the learned feature relationships. Atlas construction over large cohorts has benefited from GLUE's ability to align datasets across laboratories, technologies, and biological conditions, in some cases correcting earlier annotation errors.

From the perspective of genomic foundation models, GLUE exemplifies graph-guided multi-modal pretraining. Modality-specific encoders learn representations of their respective data types, and the graph structure provides a principled way to align these representations across modalities. The framework is modular: new modalities can be added by training new encoders and connecting them to the feature graph, without retraining the entire system.

## GNN-based Multi-omics Cancer Subtyping

Cancer is inherently a multi-omic disease. Driver mutations, copy number alterations, epigenetic reprogramming, and transcriptional rewiring jointly define tumor subtypes with distinct prognosis and treatment response. Models that integrate these layers can capture biological structure that single-omic approaches miss.

### MoGCN: Patient Graphs from Multi-omics

MoGCN represents a graph-convolutional framework for cancer subtype classification that integrates genomics, transcriptomics, and proteomics [@li_mogcn_2022]. The key insight is that patients can be represented as nodes in a graph, with edges encoding similarity relationships derived from their multi-omic profiles.

The architecture proceeds in stages. First, each omic is processed by an autoencoder that reduces dimensionality and learns compressed representations. Second, a patient similarity network is constructed by measuring similarity between patients based on their multi-omic features. Third, graph convolutional layers operate on this patient graph, learning node embeddings that incorporate both the patient's own features and information from similar patients. Finally, a classifier operating on these graph-enhanced embeddings predicts cancer subtypes.

This design captures several important properties. The patient similarity graph allows information to flow between related samples, improving predictions for patients whose individual profiles might be ambiguous but whose neighbors provide context. The multi-view structure, with separate processing for each omic followed by combination, allows each data type to contribute according to its informativeness. The graph convolutional layers can learn which neighbors are most relevant for classification.

MoGCN achieved strong performance on breast cancer subtype classification using TCGA data, outperforming methods that treated each sample independently or that combined omics through simple concatenation. Feature extraction from the trained model highlighted biologically meaningful genes and pathways, including epidermal development, cell migration, Wnt signaling, and ErbB signaling pathways for different subtypes. The patient similarity network provided clinically intuitive structure, with clear separation between subtypes and interpretable relationships between patients.

### CGMega: Multi-omics Cancer Gene Modules

Where MoGCN focuses on patient-level graphs, CGMega operates on gene-level graphs that capture multi-omic relationships between genes [@li_cgmega_2024]. Nodes represent genes, and edges encode relationships from multiple data sources: co-expression from RNA-seq, correlation in copy number alterations, shared methylation patterns, and physical proximity in three-dimensional chromatin organization.

A graph attention network learns cancer gene modules, which are subsets of genes that co-vary across omics and associate with cancer phenotypes. This module-centric view aligns with systems biology intuitions: rather than seeking single-gene markers, CGMega identifies network-level signatures that reflect pathway dysregulation. The attention mechanism highlights which edges and neighbors are most important for each gene's module membership, providing interpretability.

Gene modules discovered by CGMega capture known cancer biology and reveal new associations. Modules enriched for cell cycle genes associate with proliferative subtypes. Modules involving immune signaling genes distinguish immunologically active from quiescent tumors. The multi-omic construction ensures that these modules reflect coordinated changes across molecular layers rather than artifacts of any single data type.

### Design Patterns and Alternatives

A growing ecosystem of multi-omics subtyping methods uses related architectural patterns. Contrastive learning approaches learn sample embeddings by encouraging similar samples to have similar representations while pushing dissimilar samples apart. Generative models including variational autoencoders and generative adversarial networks jointly model multiple omics for unsupervised clustering, learning latent spaces that capture shared and modality-specific variation. Transformer-based hybrids blend multi-layer perceptrons and attention mechanisms for high-dimensional omics, using self-attention to capture feature interactions.

Common themes emerge across these methods. Modality-specific encoders with shared latent spaces appear repeatedly, allowing flexible handling of missing modalities while enabling cross-modal interactions. Graphs capturing patient-patient or gene-gene relationships structure the learning problem and provide interpretability. Emphasis on biological interpretability through clusters, modules, or attention patterns helps translate model outputs into biological hypotheses.

These cancer subtyping models illustrate how multi-omics integration naturally leads to graph-structured genomic foundation models. Sequences, epigenetics, and expression become nodes in learned biological networks, and the models learn to reason over these networks rather than treating each measurement in isolation.

## Rare Variants and Epistasis in Systems Context

@sec-pgs discussed how standard PGS methods largely ignore rare variants and epistatic interactions, despite their importance for individual-level risk and disease mechanism. Rare variants, though individually uncommon, collectively explain substantial phenotypic variance and often have larger effect sizes than common variants. Epistasis, the non-additive interaction between variants, is theoretically expected from network biology and has been documented empirically for many traits. Multi-omics and systems models offer a framework to incorporate these effects more effectively than linear approaches.

### DeepRVAT: Set-based Rare Variant Burden Modeling

DeepRVAT, Deep Rare Variant Association Testing, addresses a fundamental statistical challenge: rare variants have too few carriers to achieve individual statistical significance, yet collectively they carry important phenotypic information [@clarke_deeprvat_2024]. Traditional burden tests collapse all rare variants in a gene into a single count, losing information about variant severity. DeepRVAT instead learns gene-level impairment scores from variant annotations using set neural networks.

The architecture treats each gene's rare variants as an unordered set, reflecting the biological reality that the order of variants along a gene is not informative for their combined effect. Each variant is characterized by a vector of annotations including predicted functional impact, conservation, and structural features. A permutation-invariant neural network aggregates these annotations into a gene-level impairment score.

Crucially, DeepRVAT learns trait-agnostic representations. The gene impairment scores are trained to be predictive across multiple phenotypes simultaneously, which provides regularization and enables transfer to new traits. This multi-task learning encourages the model to learn biologically meaningful notions of gene damage rather than overfitting to any single phenotype.

The result improves both gene discovery and risk prediction. For gene discovery, DeepRVAT identifies more significant gene-trait associations than linear burden tests, particularly for genes where variant effects are heterogeneous. For risk prediction, the learned impairment scores identify individuals with high rare variant burden across multiple genes, enabling personalized risk assessment that linear PGS cannot capture.

DeepRVAT bridges the gap between variant-level annotations and gene-level burden, making it naturally compatible with sequence-based variant effect models from earlier chapters. Annotations from models like SpliceAI, AlphaMissense, or DNA foundation models can serve as input features, and the set neural network learns how to combine them into predictive gene-level scores.

### NeEDL: Network-based Epistasis Detection

NeEDL, Network-based Epistasis Detection via Local search, addresses the complementary challenge of identifying epistatic interactions among variants [@kessler_needl_2023]. The search space for epistasis is enormous: even considering only pairwise interactions among a million variants yields approximately 500 billion pairs to test. NeEDL uses network structure and optimization algorithms to make this search tractable.

The approach builds on network medicine principles. Genes and variants are embedded in a network based on biological prior knowledge, including protein-protein interactions, pathway membership, and co-expression relationships, as well as GWAS signals that suggest which variants influence the trait. Local search strategies explore combinations of variants that are close in this network and that jointly influence disease.

The optimization uses quantum-inspired algorithms that efficiently explore the combinatorial space of variant combinations. Rather than exhaustively testing all pairs or higher-order combinations, the search focuses on biologically plausible interaction sets defined by network proximity.

NeEDL does not operate as a full genomic foundation model, but it points toward systems-level combinatorial reasoning that future GFMs will need to support. The network structure provides biological constraints that make the epistasis search feasible, and the discovered interactions map onto interpretable pathways and cellular processes.

### G2PT: Hierarchical Genotype-to-Phenotype Transformers

G2PT, Genotype-to-Phenotype Transformer, explicitly models the hierarchical structure connecting variants to phenotypes [@lee_g2pt_2025]. Rather than treating variants as independent features to be weighted and summed, G2PT organizes variants into genes, genes into systems such as pathways and tissues, and systems into phenotype predictions.

The architecture uses transformer blocks at each level of this hierarchy. Variant-level attention captures interactions between variants within a gene. Gene-level attention captures interactions between genes within a system. System-level attention captures how different pathways and tissues contribute to phenotype risk.

Prior biological knowledge structures these attention patterns. Gene-pathway membership from databases like KEGG and Reactome defines which genes belong to which systems. Tissue expression patterns from GTEx indicate where each gene is active. These priors constrain the attention patterns, ensuring that the model learns biologically plausible interaction structures rather than arbitrary statistical correlations.

The hierarchical structure provides interpretability. After training, attention weights can be examined to understand which variants, genes, and systems most strongly contribute to risk for a given individual. This enables explanations like "high risk is driven by variants in genes A and B that together perturb pathway X in tissue Y."

G2PT can be viewed as an early example of a systems-aware genomic foundation model for genotype data. It unifies additive and interaction effects within a single deep architecture, using prior knowledge to guide learning toward biologically meaningful structure.

## Deep Learning-enhanced Polygenic Risk and Fine-mapping

@sec-pgs framed polygenic scores as linear weighted sums of variant effects. This approach has attractive statistical properties including interpretability, efficiency, and well-characterized uncertainty. However, it misses nonlinear effects, cannot incorporate rich sequence-based features, and struggles with rare variants and cross-ancestry generalization. Deep learning extends the PGS paradigm along each of these dimensions.

### Deep-learning PGS Frameworks

Deep-learning PGS frameworks like Delphi replace the linear combination of variant effects with flexible neural networks that learn complex functions of genotype and covariates [@georgantas_delphi_2024].

The key technical contribution is enabling neural networks to handle genome-wide inputs. A typical GWAS includes hundreds of thousands to millions of variants, far more than can be naively input to a neural network. Delphi addresses this through efficient architectures that can process hundreds of thousands of variants while remaining computationally tractable.

The resulting models can capture dominance effects where heterozygotes differ from the midpoint of homozygotes, epistatic interactions where variant effects depend on genetic background, and gene-environment interactions where variant effects depend on non-genetic covariates. These effects are learned from data rather than specified a priori, allowing the model to discover whatever structure best predicts the phenotype.

Empirical evaluations demonstrate improved discrimination compared to linear PGS across several traits, with the gains being largest for traits where nonlinear effects are most important. Importantly, Delphi also shows improved cross-ancestry generalization: the learned representations transfer more effectively than linear weights to populations not well represented in training data.

From a systems perspective, deep-learning PGS frameworks represent a move toward whole-patient risk modeling. While still primarily based on genotype plus covariates without explicit multi-omics integration, they demonstrate that the linear PGS paradigm can be extended to capture more biological complexity.

### MIFM and Multi-ancestry Fine-mapping

Fine-mapping addresses a fundamental challenge in human genetics: GWAS identifies loci but cannot usually pinpoint causal variants. Within each associated locus, linkage disequilibrium means that many variants are correlated with the causal variant and show similar association signals. Fine-mapping methods attempt to distinguish causal variants from these correlated passengers.

Multiple-instance fine-mapping frameworks like MIFM address the key bottleneck that per-variant causal labels are rarely available [@rakowski_mifm_2025]. Instead, we typically know only that some variant or variants within a locus are causal. This is naturally framed as a multiple-instance learning problem: each locus is a "bag" of variants, loci with significant GWAS signals form positive bags, and a model learns to identify which variants within positive bags are responsible for the signal.

Deep sequence models provide per-variant features that inform fine-mapping. Predicted effects on chromatin accessibility, transcription factor binding, gene expression, and splicing from models described in earlier chapters create a rich characterization of each variant's functional potential. MIFM-type frameworks integrate these sequence-based priors with GWAS evidence to produce more accurate causal variant identification.

Multi-ancestry data provide additional resolution. Different populations have different LD patterns, so a variant that is correlated with many others in one population may be more isolated in another. Methods that jointly analyze multi-ancestry data can leverage these differences to refine fine-mapping, and deep learning provides flexible frameworks for combining signals across populations with different genetic backgrounds.

Connections to the rest of this book are direct. Variant effect predictors from @sec-reg, @sec-trans, @sec-splice, and @sec-veps supply per-variant features. Multi-omics models from this chapter provide functional priors about regulatory activity, methylation, and chromatin accessibility. MIFM-type frameworks integrate these priors with GWAS evidence to produce more accurate, ancestry-aware fine-mapping that identifies the variants most likely to be causal.

## Design Patterns for Multi-omics and Systems GFMs

Drawing these examples together reveals several design patterns that recur across systems-level genomic foundation models. These patterns provide a conceptual vocabulary for understanding existing methods and designing new ones.

Modality-specific encoders with shared latent spaces appear in CpGPT, GLUE, and many multi-omics subtyping models. Each omic has its own encoder architecture tailored to its data characteristics, whether that involves treating methylation as a sequence, using variational autoencoders for scRNA-seq, or applying graph convolutions to patient similarity networks. These modality-specific encoders map into a common embedding space where downstream tasks operate. This design supports flexible inference with missing modalities, since only the available encoders need to fire, and allows incremental addition of new data types by training new encoders without retraining existing components.

Graph-guided integration structures learning through biological prior knowledge. GLUE's feature graph links peaks to genes and transcription factors. CGMega's gene-level graphs encode multi-omic relationships. NeEDL's epistasis networks capture pathway structure and protein interactions. Graph neural networks, graph transformers, and attention mechanisms over graph edges provide natural tools for encoding these biological networks and learning representations that respect network structure.

Hierarchical modeling captures the organization of biological systems across scales. G2PT formalizes the hierarchy from variants to genes to systems to phenotypes. Similar hierarchies can be defined for omics layers: sequence gives rise to chromatin state, which influences methylation patterns, which affect transcription, which determines protein levels, which ultimately connect to clinical traits. Architectures that respect this hierarchy can learn more interpretable and generalizable representations than flat models that treat all features equivalently.

Set-based and bag-based learning handles collections of variants or features that lack natural ordering. DeepRVAT treats variants within a gene as an unordered set, using permutation-invariant architectures to aggregate them into gene-level scores. MIFM treats variants within a fine-mapping locus as a bag, learning to identify causal variants without explicit per-variant labels. This pattern is crucial when sample sizes are large, labels are sparse, and biological order is meaningless.

Foundation pretraining with task-specific adaptation follows the broader paradigm that defines foundation models. CpGPT is pretrained on massive methylation datasets covering diverse tissues and conditions, then adapted through fine-tuning or linear probing to specific tasks like age prediction or mortality risk. This pattern could extend to multi-omics pretraining, where models learn joint representations of sequence, chromatin, methylation, expression, and clinical data before specialization for particular applications.

These patterns collectively point toward general-purpose systems GFMs that can ingest heterogeneous biological data and output risk predictions, mechanistic hypotheses, or treatment recommendations. The field is not yet at this stage, but the methods surveyed in this chapter demonstrate the building blocks.

## Practical Pitfalls and Considerations

Despite impressive progress, multi-omics and systems GFMs face particular challenges that practitioners must navigate. The issues examined in depth in @sec-confound take on special importance when combining multiple data sources.

Batch effects and platform heterogeneity are endemic to multi-omics data. Different omics layers often come from different assays, laboratories, or time points. Sequencing depth varies between samples. Array platforms measure different subsets of features with different technical characteristics. Integration methods can inadvertently encode batch structure rather than biology if batch effects are not properly addressed. The problem is particularly acute when different modalities have different batch structures, since standard single-modality batch correction methods may not apply.

Sample size and missingness pose related challenges. Multi-omics datasets are typically smaller than single-omic datasets because the cost and complexity of generating multiple data types limit the number of samples that can be profiled. Many samples lack certain modalities entirely, requiring robust handling of missing data. Methods that require all modalities for every sample exclude large fractions of available data, while methods that impute missing modalities must avoid introducing artifacts.

Population diversity and fairness concerns that apply to PGS (@sec-pgs) are amplified in multi-omics settings. Most large multi-omics datasets come from European-ancestry populations in high-resource healthcare systems. Models trained on these data may perform poorly or behave differently in other populations. Multi-omics GFMs risk amplifying disparities if trained primarily on non-representative cohorts, since the richer feature sets provide more opportunity for overfitting to population-specific patterns.

Evaluation complexity increases with the number of modalities and the breadth of potential applications. Multi-omics models can be evaluated at many levels: predictive performance on held-out data, biological consistency of learned representations with known biology, plausibility of inferred networks compared to experimental validation, and clinical utility when deployed in real-world settings. Overfitting to proxy metrics that are easy to compute may not translate to performance on the metrics that ultimately matter.

Interpretability and causal inference remain challenging. Attention scores and feature importance values provide some insight into model behavior, but they are not guarantees of causal mechanism. A model might attend to a feature because that feature is causal, or because it is correlated with something causal, or for spurious reasons related to batch effects or data collection. Integrating deep models with perturbation data from CRISPR screens and gene knockouts, and with robust causal inference frameworks, remains an open frontier.

Careful experimental design, thoughtful validation, and transparent reporting are therefore especially crucial for multi-omics GFMs. The additional complexity of multi-modal data creates additional opportunities for both insight and error.

## Outlook: Toward Whole-patient Foundation Models

The methods in this chapter sketch an endgame for genomic deep learning that extends far beyond sequence-only models. The trajectory moves through several stages that the book has traced across its chapters.

Genome-wide variant and sequence representation through hybrid CNN, transformer, and state-space model architectures established the foundation in @sec-reg through @sec-hybrid. These models learn rich representations of sequence that capture regulatory grammar, variant effects, and long-range dependencies.

Multi-omics integration through graph-guided latent spaces adds new dimensions. CpGPT brings methylation into the foundation model paradigm. GLUE and related methods enable principled combination of modalities measured in different cells or samples. MoGCN and CGMega demonstrate how graph neural networks can integrate patient-level or gene-level multi-omic data for cancer subtyping and biomarker discovery.

Systems-level reasoning about rare variants and epistasis addresses effects that linear models miss. DeepRVAT learns gene-level impairment from rare variant sets. NeEDL searches for epistatic interactions guided by network structure. G2PT provides hierarchical models that explicitly represent the flow from variants through genes and pathways to phenotypes.

Clinically oriented risk modeling with deep PGS and fine-mapping connects genomic representations to patient outcomes. Delphi-like frameworks extend PGS to capture nonlinear effects and improve cross-ancestry generalization. MIFM-like methods integrate sequence-based variant features with GWAS evidence for more accurate fine-mapping.

A future whole-patient foundation model might unify all these threads. Such a model would jointly encode genotype, methylome, chromatin state, expression, proteomics, imaging, and electronic health record data. It would provide unified representations across tissues, cell types, and time points, capturing the dynamic nature of biological state. It would offer calibrated, equitable predictions of disease risk and treatment response across diverse populations. It would support mechanistic queries like "which pathways mediate this variant's effect in this tissue?" or "which interventions might counteract rare variant burden in this patient?"

Realizing this vision will require advances across multiple fronts. Data sharing and privacy-preserving learning must enable training on sensitive multi-omic and clinical data at scale. Scalable architecture design must handle the computational demands of truly multi-modal foundation models. Causal validation must distinguish correlative patterns from mechanistic understanding. Equity and fairness considerations must guide data collection and model development from the outset.

The methods surveyed here show that moving beyond single-omics is not merely incremental improvement but a qualitative change in what kinds of questions genomic models can address. The path from isolated sequence models to systems-level, clinically actionable genomics is becoming visible, even if substantial work remains to traverse it.

## Summary

This chapter has surveyed how deep learning extends beyond single-omics to integrate methylation, chromatin, expression, protein, and clinical data into unified representations. We examined CpGPT as a foundation model for DNA methylation that treats methylomes as sequences amenable to transformer-based pretraining. GLUE demonstrated how graph-linked embeddings can align single-cell measurements across modalities when different omics are profiled in different cells. Graph neural network approaches including MoGCN and CGMega showed how patient-level and gene-level graphs can integrate genomics, transcriptomics, and proteomics for cancer subtyping. DeepRVAT, NeEDL, and G2PT addressed rare variants and epistasis through set-based architectures and hierarchical modeling. Deep learning frameworks for polygenic risk and fine-mapping extended the PGS paradigm with nonlinear architectures and foundation model features.

Several design patterns emerged as common threads: modality-specific encoders with shared latent spaces, graph-guided integration, hierarchical modeling, set-based learning, and foundation pretraining with task-specific adaptation. These patterns provide conceptual vocabulary for understanding existing methods and designing new ones.

Practical challenges including batch effects, sample size limitations, population diversity, and evaluation complexity require careful attention. But the trajectory toward whole-patient foundation models that jointly encode multiple omics and clinical data is becoming clear.

The remaining chapters in Part V will address cross-cutting issues of evaluation, confounding, and interpretability that apply across all the models surveyed in this book. Part VI will then explore how genomic foundation models, including the multi-omics approaches from this chapter, translate into clinical practice.