# Part II: Sequence Architectures {.unnumbered}

Five foundational questions determine what any genomic model can learn. How should sequences be represented as neural network inputs? What mechanisms enable models to capture dependencies across thousands or millions of base pairs? What distinguishes foundation models from their task-specific predecessors? How do models extract useful representations from unlabeled sequence data? And how can pretrained representations be adapted to specific applications? The answers to these questions, made before any training begins, constrain everything that follows: what patterns the model can detect, what biological phenomena it can capture, and ultimately which questions it can answer.

This part addresses each question in turn, building from fundamental representation choices through architectural mechanisms to the training and adaptation strategies that make foundation models practical. @sec-representations examines tokenization from one-hot encoding through byte-pair encoding to biologically informed vocabularies, establishing how representation choices propagate through model design. @sec-cnn introduces convolutional neural networks, the architecture that first demonstrated deep learning could outperform handcrafted features for regulatory genomics, establishing the paradigm of learning sequence-to-function mappings from functional genomics data. @sec-attention unpacks the self-attention mechanism and transformer architecture, showing how these components enable both local pattern recognition and long-range dependency modeling in genomic sequences.

@sec-pretraining surveys the landscape of self-supervised objectives (masked language modeling, next-token prediction, denoising approaches), examining how each shapes learned representations and what biological patterns each encourages models to discover. @sec-transfer closes the loop by addressing how pretrained models are adapted to downstream tasks through fine-tuning, few-shot learning, and deployment strategies. Together, these chapters provide the conceptual foundation needed to understand the specific model families examined in Part III and the applications developed throughout the remainder of the book.