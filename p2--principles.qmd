# Part II: Sequence Architectures {.unnumbered}

Every neural network architecture encodes assumptions about biology. Convolutional networks assume that local patterns matter and that the same motifs are meaningful regardless of genomic position. Attention mechanisms assume that distant positions can interact directly without passing information through intermediate representations. Pretraining objectives assume that certain patterns in unlabeled sequence provide useful supervision in the absence of functional labels. These assumptions, embedded in architectural choices made before any training begins, determine which biological phenomena the model can capture and which remain invisible to it.

This part examines each architectural component in turn, building from fundamental representation choices through pattern-recognition mechanisms to the training and adaptation strategies that make foundation models practical. @sec-representations establishes how tokenization choices propagate through model design, from one-hot encoding through byte-pair encoding to biologically informed vocabularies. @sec-cnn introduces convolutional neural networks, the architecture that first demonstrated deep learning could outperform handcrafted features for regulatory genomics by learning sequence-to-function mappings directly from data. @sec-attention unpacks the self-attention mechanism and transformer architecture, showing how these components enable both local pattern recognition and long-range dependency modeling across genomic sequences.

@sec-pretraining surveys the landscape of self-supervised objectives, examining how masked language modeling, next-token prediction, and denoising approaches each shape learned representations and encourage models to discover different biological patterns. @sec-transfer closes the loop by addressing how pretrained models are adapted to downstream tasks through fine-tuning, few-shot learning, and deployment strategies. Together, these chapters provide the conceptual foundation needed to understand the specific model families examined in Part III and the applications developed throughout the remainder of the book.