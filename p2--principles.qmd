# Part II: Core Principles {.unnumbered}

The previous part surveyed the architectural landscape of deep learning for genomics, from convolutional sequence-to-function models through protein and DNA language models to hybrid architectures. Those chapters focused on how specific model families work and what capabilities they enable. This part steps back to examine the conceptual foundations that unite these approaches and distinguish foundation models from their task-specific predecessors.

Five questions organize this part. First, how should genomic sequences be represented as input to neural networks? The choice of tokenization scheme profoundly shapes what patterns a model can discover and how efficiently it processes long sequences. Second, what mechanisms allow transformers to capture long-range dependencies in genomic data? The self-attention architecture that revolutionized natural language processing has become equally central to computational genomics, but its application to biological sequences requires careful adaptation. Third, what defines a foundation model in genomics, and how do we navigate the emerging ecosystem of pretrained models? Understanding this landscape is essential for practitioners who must choose among competing approaches. Fourth, how do models learn useful representations from unlabeled genomic data? The shift from supervised learning on narrow tasks to self-supervised pretraining on broad sequence corpora enables the reusability that defines foundation models. Fifth, how can pretrained models be adapted to specific downstream applications? The gap between pretraining objectives and real-world problems requires principled strategies for transfer learning and deployment.

These five chapters build a progression from fundamental choices about representation through architectural mechanisms to the training and adaptation strategies that make foundation models practical. @sec-token examines tokenization from one-hot encoding through byte-pair encoding to biologically informed vocabularies, establishing how representation choices propagate through model design. @sec-transformers unpacks the self-attention mechanism and transformer architecture, showing how these components enable both local pattern recognition and long-range dependency modeling in genomic sequences. @sec-foundation develops a practical taxonomy of foundation models, distinguishing pretraining paradigms and establishing principles for model selection. @sec-pretrain surveys the landscape of self-supervised objectives, from masked language modeling to next-token prediction to denoising approaches, examining how each shapes learned representations. @sec-transfer closes the loop by addressing how pretrained models are adapted to downstream tasks through fine-tuning, few-shot learning, and deployment strategies.

Together, these chapters provide the conceptual foundation needed to understand both current genomic models and the emerging directions covered in later parts. The principles developed here apply whether working with DNA language models for regulatory prediction, protein models for structure and function, or hybrid architectures that combine multiple modalities.
