# Confounding and Data Leakage {#sec-ch12-confounding}

> Models learn shortcuts. Shortcuts work---until they don't.

::: {.callout-note}
## Chapter Overview

**Estimated reading time:** 40-50 minutes

**Prerequisites:** This chapter assumes familiarity with basic machine learning evaluation concepts (training/test splits, performance metrics) from @sec-ch09-evaluation, population structure concepts from @sec-ch03-population-structure, and variant annotation databases from @sec-ch02-clinical. Readers should also understand the distinction between correlation and causation.

**Learning Objectives:** After completing this chapter, you should be able to:

1. Distinguish between confounding, bias, and data leakage using precise definitions
2. Identify the major sources of confounding in genomic datasets (ancestry, batch effects, label bias, temporal drift)
3. Explain why population structure creates shortcuts that foundation models readily exploit
4. Design appropriate data splitting strategies for different evaluation goals
5. Apply diagnostic methods to detect confounding in model predictions
6. Select and implement mitigation strategies appropriate to specific confounding sources

**Key Insight:** A model can achieve excellent benchmark performance while learning nothing about biology. The same expressiveness that allows foundation models to capture complex biological patterns also allows them to discover subtle confounders invisible to simpler diagnostics. Rigorous evaluation design is not optional; it is the only way to distinguish genuine learning from sophisticated shortcut exploitation.
:::

A variant effect predictor trained on ClinVar achieves 0.92 auROC on held-out variants from the same database, yet performance drops to 0.71 when evaluated on a prospectively collected clinical cohort. A **polygenic risk score** for coronary artery disease stratifies European-ancestry individuals with impressive discrimination, then fails almost completely when applied to individuals of African ancestry. A gene expression model trained on GTEx data predicts tissue-specific patterns with apparent precision, until deployment reveals it learned to distinguish sequencing centers rather than biological states. Each model worked brilliantly in evaluation and failed quietly in practice.

These failures share a common cause: the models learned shortcuts rather than biology. Genomic datasets encode hidden structure from ancestry and family relatedness to sequencing center, capture kit, and label curation protocol. These factors correlate with both features and labels. When such **confounders** remain uncontrolled, models exploit them. The central challenge is that confounded models can appear to work, sometimes spectacularly well, until they encounter data where the shortcuts no longer apply.

This problem is not unique to deep learning. Linear regression and logistic models suffer from the same biases when fit on confounded data. What makes confounding particularly dangerous in the foundation model era is scale: larger datasets and more expressive architectures make it easier to discover subtle shortcuts that remain invisible in standard diagnostics but cause dramatic failures when distributions shift at deployment. A shallow model might miss the correlation between sequencing center and disease status; a transformer with hundreds of millions of parameters will find it if that correlation helps optimize the training objective.

::: {#fig-confounding-dag}
![Confounding structure in genomic prediction](../figs/part_2/ch12/01-fig-confounding-dag.svg)

Confounding structure in genomic prediction. (Left) Ancestry acts as a confounder, affecting both genomic features (through population-specific allele frequencies) and disease outcomes (through environmental, socioeconomic, and healthcare pathways). The spurious path (red) creates association between features and outcomes that models exploit as shortcuts. (Right) Conditioning on ancestry blocks the spurious path, isolating any genuine feature-outcome relationship. Concrete example: a rare disease clinic serving primarily European patients contributes most pathogenic variants to ClinVar, creating an ancestry-pathogenicity correlation that models learn instead of biological mechanisms. The central challenge: shortcuts appear to work until deployment shifts the confounder-outcome relationship.
:::


## Confounding, Bias, and Leakage {#sec-ch12-terminology}

The terminology of confounding, bias, and leakage describes distinct phenomena that often co-occur and reinforce each other. Precision in language helps clarify what has gone wrong when a model fails.

A confounder is a variable that influences both the input features and the label. Ancestry provides a canonical example: it affects allele frequencies across the genome (the features) and disease risk through environmental, socioeconomic, and healthcare pathways (the labels). If ancestry is not explicitly modeled or controlled, a model trained to predict disease may learn to identify ancestry rather than disease biology. The prediction appears accurate because ancestry correlates with outcome, but the model has captured correlation rather than mechanism.

**Bias** refers to systematic deviation from the quantity we intend to estimate or predict. Bias can result from confounding, but also arises from measurement error, label definitions, sampling procedures, or deployment differences. A case-control study with 50% disease prevalence will train models that systematically over-predict risk when deployed in populations where true prevalence is 5%. The model may be perfectly calibrated for the training distribution yet dangerously miscalibrated for clinical use.

**Data leakage** occurs when information about the test set inadvertently influences model training or selection. Leakage pathways include overlapping individuals or variants between training and evaluation, shared family members across splits, duplicated samples under different identifiers, and indirect channels such as pretraining on resources that later serve as benchmarks. The circularity between computational predictors and ClinVar annotations discussed in @sec-ch04-circularity exemplifies this last category: *CADD*-like scores influence which variants receive pathogenic annotations, and those annotations then become training labels for the next generation of predictors.

**Distribution shift** describes mismatch between training and deployment data distributions. Shift can be driven by changes in ancestry composition, sequencing technology, clinical coding practices, or temporal trends in care. A model that learns hospital-specific coding patterns will fail when deployed at a different institution, not because the biology differs but because the label generation process does.

::: {.callout-note title="Predict Before You Look"}
Before examining the table below, test your understanding: For each scenario, identify whether it represents confounding, bias, data leakage, or distribution shift:

1. A model trained on variants from 2018-2020 ClinVar is tested on variants added to ClinVar in 2023
2. A polygenic score calibrated on 50% case prevalence is deployed in a population with 5% prevalence
3. Ancestry affects both allele frequencies and disease risk through healthcare access pathways
4. The same individual's genome appears in both training and test sets under different identifiers

Check your predictions against the table definitions below.
:::

The following table clarifies the distinctions between these related but distinct concepts:

: Distinguishing confounding, bias, leakage, and distribution shift. Each phenomenon has different causes, manifestations, and solutions. {#tbl-terminology}

| Term | Definition | Example | Detection | Primary Solution |
|------|------------|---------|-----------|------------------|
| **Confounding** | Variable affects both features and labels | Ancestry affects genotype frequencies and disease risk | Confounder-only baselines match model performance | Matching, adjustment, or invariance learning |
| **Bias** | Systematic deviation from target | Training at 50% prevalence, deploying at 5% | Calibration analysis across settings | Design matching, recalibration |
| **Data leakage** | Test information influences training | Same variant in train and test sets | Performance collapse under strict splits | Rigorous deduplication, temporal splits |
| **Distribution shift** | Train/deploy distributions differ | Model trained on one hospital, deployed at another | Performance degradation on new cohorts | Domain adaptation, multi-site training |

::: {.callout-tip}
## Key Insight: The Confounder-Mechanism Distinction

The critical question for any genomic model is: *Does the association between features and labels flow through the biological mechanism I care about, or through a confounding pathway?* A model predicting variant pathogenicity from sequence might learn that certain haplotype backgrounds correlate with pathogenic labels, but if that correlation exists because of ancestry-biased ascertainment rather than biological causation, the model has learned a shortcut that will fail when applied to differently ascertained populations.
:::

For foundation models, these risks are magnified. Genomes encode ancestry, relatedness, and assay conditions in thousands of subtle features, even when those labels are never explicitly provided. Large transformers find shortcuts that smaller models would miss if those shortcuts improve the training objective. Complex training regimes involving pretraining on biobank-scale data, fine-tuning on curated labels, and evaluation on community benchmarks create many opportunities for direct and indirect leakage.


## Sources of Confounding in Genomic Data {#sec-ch12-sources}

Confounders in genomic modeling cluster into several categories, though the same underlying variable (such as recruitment site) may simultaneously induce ancestry differences, **batch effects**, and label bias. These categories are not mutually exclusive; batch effects in single-cell data (@sec-ch19-batch-effects) and multi-omic integration (@sec-ch22-batch-effects) represent domain-specific manifestations of the same underlying challenge.

::: {.callout-warning}
## Stop and Think

Before reading about specific confounding sources, consider: if you were designing a study to train a variant pathogenicity predictor, what variables might affect both the variants you observe (your features) and the pathogenicity labels you collect (your outcomes)? List three potential confounders and how they might create spurious associations.
:::

### Population Structure and Relatedness {#sec-ch12-ancestry-confounding}

Ancestry creates perhaps the most pervasive confounders. Continental and sub-continental **population structure** affects both genomic features and many phenotypes of interest, creating classic confounding. The portability failures of polygenic scores across ancestry groups (@sec-ch03-portability) represent one clinically consequential manifestation of this confounding. Family relationships (siblings, parent-offspring pairs, cryptic relatedness detectable only through genotype similarity) and founder effects that create local haplotype structure compound these issues. Relatedness creates a more subtle problem than population stratification: when close relatives appear in both training and test sets, models can memorize shared haplotype segments rather than learning generalizable patterns, producing inflated performance estimates that collapse for unrelated individuals.

### Technical Batch Effects {#sec-ch12-batch-effects}

Sequencing and analysis pipelines introduce their own systematic differences. Different instruments produce distinct error profiles. Library preparation protocols vary in GC bias, coverage uniformity, and adapter content. Capture kits determine which genomic regions receive adequate coverage. Alignment algorithms and variant callers make different decisions at ambiguous positions. When samples from a particular batch disproportionately represent a specific label class (cases sequenced at one center, controls at another), models learn to distinguish batches rather than biology.

### Institutional and Recruitment Confounding {#sec-ch12-institutional-confounding}

The institutions where patients receive care introduce additional confounding layers. Hospital systems use distinct coding practices, diagnostic thresholds, and follow-up schedules. The phenotype quality issues that result are examined in @sec-ch02-phenotypes, with implications for how models learn from systematically biased labels. Population-based biobanks differ from referral-center cohorts in disease severity, comorbidity patterns, and demographic composition. Individuals who receive genomic testing may be more severely affected, more affluent, or preferentially drawn from particular ancestry groups, introducing selection bias that distorts apparent variant-phenotype relationships.

These sources of confounding trace back to data collection and curation processes. Training data inherit the biases present in the databases from which they derive: ClinVar's overrepresentation of European ancestry variants (@sec-ch02-clinvar), gnomAD's population composition (@sec-ch02-gnomad), and the tissue coverage decisions of consortia like ENCODE and GTEx (@sec-ch02-encode). Understanding data provenance is prerequisite to anticipating which confounders a model may have learned.

### Label Generation Bias {#sec-ch12-label-bias}

The process of generating ground truth annotations itself creates biases. Clinical labels derived from billing codes or problem lists reflect documentation practices as much as underlying disease. Variant pathogenicity databases exhibit the systematic biases detailed in @sec-ch02-clinical: ClinVar annotations over-represent European ancestry, well-studied genes, and variants submitted by high-volume clinical laboratories [@landrum_clinvar_2018]. Expression, regulatory, or splicing labels derived from specific tissues or cell lines may not generalize to other biological contexts. The circularity problem identified in @sec-ch04-circularity persists into the foundation model era: when model predictions influence which variants receive expert review, and expert classifications become training labels, feedback loops amplify historical biases.

### Temporal Drift {#sec-ch12-temporal-drift}

Clinical practice, diagnostic criteria, and coding conventions evolve over time. Sequencing technologies and quality control pipelines also change. A model trained on 2015 data may fail on 2024 data not because biology changed but because documentation practices, coding standards, and available treatments all evolved. This temporal drift affects both the features models learn and the labels they predict.

### Resource Overlap and Indirect Leakage {#sec-ch12-resource-overlap}

Even the resources used for training and evaluation create leakage pathways. When databases like gnomAD or UK Biobank appear in both model training and evaluation, indirect information flows compromise apparent generalization. A foundation model pretrained on gnomAD allele frequencies, then evaluated on a benchmark that uses gnomAD for population filtering, faces indirect leakage even if specific variants do not overlap. Community benchmarks that reuse widely available variant sets across multiple publications create additional leakage pathways that accumulate over time as the field iterates.

The following table summarizes the major confounding sources, their mechanisms, and detection approaches:

: Major sources of confounding in genomic modeling. Each source creates distinct pathways from features to labels that bypass the biological mechanisms of interest. {#tbl-confounding-sources}

| Source | Affects Features Via | Affects Labels Via | Detection Signal | Mitigation Approach |
|--------|---------------------|-------------------|------------------|---------------------|
| Ancestry | Allele frequencies, haplotypes, LD patterns | Healthcare access, environmental exposure | Performance stratified by ancestry; PCA-only baseline | Matching, PCs as covariates, invariance training |
| Relatedness | Shared haplotype segments | Shared environmental factors, ascertainment | Kinship matrix analysis; family-aware split sensitivity | Family-aware splitting |
| Batch effects | Coverage, error profiles, variant calling | Case/control imbalance across batches | Batch predicts phenotype; embedding clusters by batch | Batch covariates, harmonization, domain adaptation |
| Institution | Sequencing protocols, capture kits | Coding practices, diagnostic criteria | Performance varies by site | Multi-site training, cohort holdouts |
| Label generation | Features used in curation decisions | Circular dependency with prior predictions | Ablating predictive features degrades performance | Temporal splits, independent validation |
| Temporal drift | Technology evolution | Practice guideline changes | Performance degrades on newer data | Time-based splits, continuous monitoring |


## Population Structure as a Shortcut {#sec-ch12-population-shortcut}

Population structure represents one of the most pervasive confounders in genomic modeling. The core issue is that ancestry simultaneously affects genomic features and many phenotypes through pathways that have nothing to do with direct genetic causation.

Human genetic variation is structured by ancestry: allele frequencies, haplotype blocks, and **linkage disequilibrium** patterns differ across populations in ways that reflect demographic history. Principal components computed from genome-wide genotypes provide a low-dimensional summary of this structure and have become standard in **genome-wide association studies (GWAS)** to correct for stratification [@patterson_population_2006; @price_pca_2006]. Yet ancestry is not merely a statistical nuisance. It is intertwined with geography, environment, socioeconomic status, and access to healthcare, factors that directly impact disease risk, likelihood of receiving genetic testing, and the quality of phenotyping when testing occurs.

The statistical genetics community developed these corrections precisely because early genome-wide association studies produced spurious signals driven by ancestry differences between cases and controls rather than causal variant effects (see @sec-ch03-population-structure for detailed treatment of population stratification in association testing). Foundation models face the same fundamental problem in a different guise: ancestry structure that confounded linear regression in GWAS now confounds neural network predictions, and the solutions require similar conceptual foundations even when the technical implementations differ.

::: {#fig-population-structure-shortcut layout-ncol=2}
![Population structure in genomic data](../figs/part_2/ch12/02-A-fig-population-structure-shortcut.svg)

![Ancestry encoded in local sequence composition](../figs/part_2/ch12/02-B-fig-population-structure-shortcut.svg)

![Ancestry creates shortcut pathways](../figs/part_2/ch12/02-C-fig-population-structure-shortcut.svg)

![Shortcuts fail when ancestry-label relationships change](../figs/part_2/ch12/02-D-fig-population-structure-shortcut.svg)

Population structure creates exploitable shortcuts in genomic prediction. (A) PCA reveals clear ancestry clustering in genetic data. (B) Even local k-mer frequencies differ by ancestry, meaning foundation models have access to ancestry signal from raw sequences. (C) Ancestry creates dual pathways to both features (through allele frequencies) and labels (through healthcare access and annotation practices), enabling models to learn ancestry as a proxy for the target outcome. (D) The consequence: polygenic scores show 40-75% performance reduction when applied to non-European populations because ancestry-based shortcuts do not generalize. Higher model capacity does not solve confounding—it makes it worse by enabling detection of ever more subtle ancestry-linked features.
:::

Consider a rare disease clinic serving primarily individuals of European ancestry. This clinic contributes most pathogenic variant submissions to ClinVar, while variants observed predominantly in other ancestries remain classified as variants of uncertain significance [@landrum_clinvar_2018].. A model trained on ClinVar may learn that European-enriched variants tend to have pathogenic labels and non-European-enriched variants tend to have uncertain or benign labels, not because of any biological difference in pathogenicity but because of differential clinical characterization. The model appears to predict pathogenicity while actually predicting ancestry-correlated ascertainment.

Foundation models trained on nucleotide sequences see ancestry information directly: the distribution of k-mers and haplotypes differs by population. When such models are fine-tuned to predict disease risk or variant effects, they may leverage ancestry as a shortcut. Increasing model capacity does not solve this problem; it often makes it worse by enabling detection of increasingly subtle ancestry-linked features. The polygenic score portability literature provides stark evidence: risk scores derived from European ancestry cohorts show 40-75% reductions in prediction accuracy when applied to African ancestry individuals [@duncan_analysis_2019]. Similar patterns emerge for variant effect predictors and regulatory models, though they are often less thoroughly documented due to limited cross-ancestry evaluation.

::: {.callout-tip}
## Key Insight: Capacity Amplifies Confounding

A common misconception is that larger, more powerful models will "see through" confounding to the underlying biology. In reality, the opposite often occurs. A linear model might capture only the strongest ancestry-outcome correlations; a transformer with billions of parameters will find every ancestry-linked feature that improves the training objective, no matter how subtle. *Model expressiveness is not a defense against confounding; it is an amplifier.*
:::

This mismatch between the populations used for model development and the populations that would benefit from genomic medicine creates a fundamental tension between current practice and equitable healthcare. Models that work primarily for European ancestry individuals perpetuate existing health disparities, regardless of their benchmark performance. The fairness implications are examined further in @sec-ch12-fairness.

::: {.callout-tip title="Knowledge Check: Recall Key Concepts"}
Before moving to technical artifacts, test your retention from the earlier sections:

1. What is the difference between a confounder and data leakage? Give a concrete example of each.
2. Why does ancestry act as a confounding variable in genomic prediction?
3. What diagnostic would reveal that your model's 0.85 auROC primarily reflects ancestry rather than biological mechanism?

:::{.callout-note collapse="true" title="Check Your Answer"}
(1) A confounder is a variable that affects both features and labels (e.g., ancestry affects allele frequencies and disease risk through healthcare access), while data leakage occurs when test information influences training (e.g., the same variant appearing in both train and test sets). Both inflate performance, but leakage involves information that shouldn't exist at prediction time, while confounding involves real but non-causal associations. (2) Ancestry affects genomic features through population-specific allele frequencies and haplotype structure, while simultaneously affecting disease labels through environmental factors, healthcare access, socioeconomic status, and clinical ascertainment practices—creating a spurious association pathway the model can exploit. (3) Train a confounder-only baseline using just ancestry principal components with no genomic features. If this baseline achieves performance close to your full model (e.g., 0.80 vs 0.85 auROC), ancestry confounding drives most of the signal.
:::
:::


## Technical Artifacts as Biological Signal {#sec-ch12-technical-artifacts}

Technical pipelines are complex, and each step from sample collection through final variant calls can introduce systematic differences that models may learn.

Sequencing centers differ in instruments, reagents, and quality control thresholds. Library preparation protocols produce distinct coverage profiles and GC bias patterns. Capture kits determine which genomic regions are well-covered and which have systematic dropout. Read length affects the ability to span repetitive regions and call structural variants. Alignment and variant calling algorithms make different decisions at ambiguous genomic positions.

::: {#fig-batch-effects layout-ncol=3}
![Embeddings cluster by batch rather than phenotype](../figs/part_2/ch12/03-A-fig-batch-effects.svg)

![Coverage patterns vary systematically by center](../figs/part_2/ch12/03-B-fig-batch-effects.svg)

![Batch-phenotype confounding enables trivial prediction](../figs/part_2/ch12/03-C-fig-batch-effects.svg)

Technical batch effects masquerade as biological signal. (A) Model embeddings cluster by sequencing center rather than by case/control status, revealing that learned representations capture technical rather than biological variation. (B) Coverage patterns at the same genomic region differ systematically by center, providing features models can exploit. (C) Batch-phenotype confounding occurs when cases and controls are imbalanced across centers—predicting batch becomes equivalent to predicting disease. Together, these panels illustrate why models can achieve high apparent performance while learning nothing about biology. Detection requires visualizing embeddings, examining coverage patterns, and testing whether batch identity predicts the outcome.
:::

When samples from a particular batch or platform are disproportionately drawn from a specific phenotype class, models learn to distinguish batches. Why does batch-phenotype correlation arise in real studies? Practical constraints drive the pattern: case samples are often collected at specialized disease centers with particular sequencing infrastructure, while controls come from population biobanks using different platforms. Temporal factors compound this—if cases were sequenced earlier when certain technologies dominated, and controls were added later with newer platforms, technology-phenotype correlation becomes embedded in the data. Studies rarely randomize case-control status across batches because retrospective collection is cheaper than prospective design, creating systematic confounding that standard quality control cannot detect.

In high-dimensional feature spaces, even subtle batch-specific artifacts (coverage dips at particular loci, variant density patterns reflecting caller behavior, residual adapter sequences) can become predictive. Foundation models that process raw reads, coverage tracks, or variant streams are particularly vulnerable because batch signatures may be encoded in features that preprocessing would typically remove.

::: {.callout-tip title="Knowledge Check"}
A model achieves 0.88 auROC predicting disease status from whole-genome sequences. You discover that case samples were sequenced at Center A using Illumina NovaSeq, while control samples were sequenced at Center B using HiSeq. What would you predict about performance on a new cohort where cases and controls are equally distributed across both centers? What diagnostic would you run to test for batch confounding?

:::{.callout-note collapse="true" title="Check Your Answer"}
Performance would likely collapse to near-chance levels (close to 0.50 auROC) because the model learned to distinguish sequencing centers rather than disease biology. When cases and controls are equally distributed across both centers, the batch-disease correlation disappears and the learned shortcut becomes useless. Diagnostics to run: (1) Train a classifier to predict sequencing center from the model's learned embeddings - high accuracy confirms batch encoding. (2) Visualize embeddings colored by center and by disease status - clustering by center rather than disease reveals the problem. (3) Evaluate performance stratified by center - if within-center performance is poor, the model relies on between-center differences.
:::
:::

Common patterns suggesting batch confounding include embedding spaces where samples cluster by sequencing center rather than phenotype, strong predictive performance that collapses when evaluated on data from a new platform, and models that can accurately predict batch identity (sequencing center, capture kit, processing date) from inputs that should be batch-independent. When a model designed to predict disease can also predict which laboratory processed the sample, something has gone wrong.


## Label Bias and Circularity {#sec-ch12-label-circularity}

Labels in genomic applications rarely represent ground truth in any absolute sense. They represent the outputs of complex processes involving clinical documentation, expert review, computational prediction, and database curation. These processes introduce biases that models absorb and may amplify.

Clinical phenotypes derived from electronic health records inherit the limitations of medical documentation. Billing codes capture what was reimbursable, not necessarily what was present. Problem lists reflect what clinicians chose to document, which varies by specialty, institution, and individual practice patterns. Diagnostic criteria change over time, creating apparent temporal trends in disease prevalence that reflect evolving definitions rather than changing biology.

::: {#fig-label-circularity}
![Label circularity inflates apparent validation](../figs/part_2/ch12/04-fig-label-circularity.svg)

Label circularity inflates apparent validation. Clinical laboratories submit variants to ClinVar using computational predictions (CADD, REVEL) as supporting evidence. ClinVar aggregates these submissions, creating labels that reflect historical computational predictions. New models trained on ClinVar learn to replicate these patterns, achieving high apparent accuracy through agreement with previous predictors rather than independent biological insight. When these new models are deployed, they influence the next generation of submissions, perpetuating the cycle. Breaking circularity requires prospective validation on newly discovered variants, temporal splits that train on historical data and test on recent annotations, or independent functional assay validation that bypasses computational predictions entirely.
:::


Variant pathogenicity labels illustrate the problem of circularity. ClinVar aggregates submissions from clinical laboratories, research groups, and expert panels [@landrum_clinvar_2018]. The evidence underlying these submissions often includes computational predictions: a laboratory may cite *CADD*, *REVEL*, or other predictors as supporting evidence for a pathogenic classification. When the next generation of predictors trains on ClinVar, it learns to replicate the computational predictions that contributed to those labels. Performance on ClinVar-derived benchmarks thus reflects, in part, agreement with previous predictors rather than independent biological insight.

Why does circularity inflate validation metrics specifically? The mechanism is statistical: the new model's task becomes predicting what previous models predicted, not predicting true pathogenicity. If *CADD* influenced 30% of pathogenic labels, and the new model learns to approximate *CADD*, it automatically achieves high agreement on those labels—regardless of whether the underlying biology was correctly captured. The inflation is proportional to the previous model's influence on labeling: more circularity means more inflated benchmarks. Critically, this inflation is invisible within the circular ecosystem; only prospective validation on genuinely novel variants or independent functional assays reveals the gap between apparent and true performance.

This circularity extends across the ecosystem of genomic resources. gnomAD allele frequencies inform variant filtering in clinical pipelines. UK Biobank genotype-phenotype associations shape which variants receive functional follow-up. Structural annotations from ENCODE and Roadmap Epigenomics influence which regulatory regions are considered biologically important. Foundation models pretrained on these resources, then evaluated against benchmarks derived from the same resources, may achieve impressive scores while learning to reproduce the assumptions and biases of existing annotations rather than discovering new biology.


## Data Splitting {#sec-ch12-data-splitting}

Data splitting is among the primary tools for assessing generalization, yet naive splits can silently permit leakage that inflates apparent performance.

::: {.callout-note}
## Mathematical Foundations

The following section introduces formal concepts about data splitting. The core intuition is simple: different splitting strategies test different types of generalization. Random splits test interpolation; structured splits test extrapolation to genuinely new contexts.
:::

### Random Individual-Level Splits {#sec-ch12-random-splits}

Random individual-level splits assign samples randomly to training, validation, and test sets. This approach fails when samples are not independent: family members may appear on both sides of a split, allowing models to memorize shared haplotypes. Rare variant analysis is particularly vulnerable because disease-causing variants may be private to specific families, and memorizing which families have which variants is far easier than learning generalizable sequence-function relationships.

### Family-Aware Splits {#sec-ch12-family-splits}

Family-aware splits address relatedness by ensuring that all members of a family appear in the same split. This prevents direct memorization of family-specific variants but does not address population structure (ancestry groups may remain imbalanced across splits) or other confounders.

### Locus-Level Splits {#sec-ch12-locus-splits}

Locus-level splits hold out entire genomic positions, ensuring that no variant at a test position appears during training. This stringent approach prevents models from memorizing site-specific patterns and is essential for **variant effect prediction** where the goal is to score novel variants at positions the model has never seen.

Why do models memorize positions when batch effects or ascertainment biases exist? Gradient descent discovers whatever pathway most efficiently reduces loss. When certain genomic positions systematically appear in training with particular labels—because well-studied genes are overrepresented, or because sequencing centers focused on specific regions—the model can reduce loss by learning "position X tends to be pathogenic" rather than "variants disrupting this motif tend to be pathogenic." In high-dimensional feature spaces, both pathways are viable; position memorization is often easier. Locus-level splits force the model to succeed on positions it has never seen, eliminating the memorization shortcut entirely.

Many published benchmarks fail to implement locus-level splitting, allowing models to achieve high scores by recognizing familiar positions rather than learning generalizable effects. The evaluation considerations in @sec-ch11-leakage-detection address these issues in detail.

### Region and Chromosome Splits {#sec-ch12-region-splits}

Region or chromosome splits hold out entire genomic regions, testing whether models learn biology that transfers across the genome rather than region-specific patterns. This is particularly relevant for regulatory prediction, where local chromatin context may differ between regions.

### Cohort and Site Splits {#sec-ch12-cohort-splits}

Cohort or site splits hold out entire institutions, sequencing centers, or biobanks, directly testing robustness to the batch and cohort effects discussed above. Models that perform well only within their training cohort but fail on held-out cohorts have learned institution-specific patterns.

### Temporal Splits {#sec-ch12-temporal-splits}

Time-based splits use temporal ordering, training on earlier data and evaluating on later data. This approach simulates prospective deployment and tests robustness to temporal drift. A model trained on 2018 data and evaluated on 2023 data faces realistic distribution shift that random splits would obscure.

### Indirect Leakage Across Resources {#sec-ch12-indirect-leakage}

Beyond explicit split design, indirect leakage remains a concern. A variant that appears in ClinVar may also appear in gnomAD (with population frequency information), in functional assay datasets (with splicing or expression effects), and in literature-derived databases (with disease associations). Pretraining on any of these resources while evaluating on another creates indirect information flow that standard deduplication would miss.

::: {.callout-note title="Predict Before You Look"}
Before examining the table, consider these scenarios and predict which splitting strategy is most appropriate:

1. You're building a variant effect predictor that must score novel variants at genomic positions never seen before
2. Your model showed excellent performance in development but you need to test if it will work at other hospitals
3. You're training on genotypes from families with rare diseases
4. You want to simulate how your model would perform if deployed next year

Match each scenario to the splitting strategy it requires. Then check the table to see if your predictions align with the "When to Use" column.
:::

The following table compares splitting strategies and their properties:

: Data splitting strategies for genomic models. Each strategy addresses different leakage risks and tests different generalization capabilities. The choice depends on the deployment context and evaluation goals. {#tbl-splitting-strategies}

| Strategy | What It Holds Out | Leakage Addressed | Generalization Tested | When to Use |
|----------|-------------------|-------------------|----------------------|-------------|
| Random | Random samples | None | Interpolation only | Never for final evaluation |
| Family-aware | Family groups | Relatedness memorization | Across unrelated individuals | When pedigree structure exists |
| Locus-level | Genomic positions | Position memorization | Novel genomic positions | Variant effect prediction |
| Chromosome | Entire chromosomes | Regional patterns | Cross-genome transfer | Regulatory prediction |
| Cohort/Site | Institutions | Batch effects, coding practices | Cross-institution deployment | Clinical deployment validation |
| Temporal | Time periods | Future information | Prospective performance | Simulating real deployment |


## Data Leakage as Confounding {#sec-ch12-leakage-confounding}

Data leakage can be understood as a special case of confounding where the confounder is information that should not exist at prediction time. This framing clarifies why leakage inflates performance estimates and why leaked models fail in deployment: they have learned associations with variables that are unavailable when predictions must actually be made.

The detailed taxonomy of leakage types (label, feature, temporal, and benchmark leakage) along with detection strategies is provided in @sec-ch11-leakage-detection. Here we examine how each leakage type creates confounding structures that distort model evaluation.

### Causal Structure of Leakage {#sec-ch12-leakage-causal}

In causal terms, leakage introduces a backdoor path between features and labels that does not represent the relationship we intend to model. Consider a pathogenicity predictor trained with conservation scores that were computed using alignments incorporating known pathogenic variants. The causal structure includes: (1) the intended path from sequence features through biological mechanism to pathogenicity, and (2) a leaked path from pathogenicity labels through their influence on conservation databases back to conservation features. The model cannot distinguish signal flowing through these two paths, and performance estimates reflect both.

Label leakage creates confounding when the process that generated labels also influenced feature construction. The confounder is the shared information source: ClinVar curators who used computational predictions created a dependency between those predictions and subsequent labels. Feature leakage creates confounding when features correlate with labels through non-causal pathways, such as batch effects that happen to align with case-control status. Temporal leakage creates confounding through time-dependent information flow: future knowledge that influenced either features or labels introduces associations that would not exist in prospective application.

### Compounding Effects {#sec-ch12-compounding}

These leakage types interact and compound. A model suffering from multiple forms may achieve extraordinary benchmark performance while learning nothing transferable to prospective clinical use. The apparent signal is real within the leaked evaluation framework but spurious for the intended application.

::: {.callout-warning}
## Stop and Think: Compound Leakage

Consider a variant effect predictor that: (1) uses conservation scores computed from databases that include known pathogenic variants, (2) was trained on ClinVar labels that were influenced by earlier predictors, and (3) is evaluated on a benchmark constructed using similar computational filtering methods.

How many distinct leakage pathways can you identify? For each pathway, what would the model learn that would inflate its apparent performance but fail in prospective deployment?
:::

Consider a variant effect predictor that uses conservation scores (feature leakage), was trained on ClinVar labels influenced by earlier predictors (label leakage), and is evaluated on a benchmark constructed using similar computational methods (benchmark leakage). Each leakage type independently inflates performance; together, they create an evaluation that measures something entirely different from prospective predictive ability.

### Implications for Confounding Analysis {#sec-ch12-leakage-implications}

The confounding framework suggests that leakage detection methods (described in @sec-ch11-leakage-detection) can be understood as strategies for identifying and blocking backdoor paths. Feature ablation removes variables that may carry leaked signal. Temporal validation eliminates paths that depend on future information. Baseline analysis reveals when simple confounders explain most of the apparent performance.

This perspective also clarifies why some apparent leakage may be acceptable. If conservation scores will always be available at prediction time, the path through conservation represents legitimate signal rather than confounding. The distinction depends on the deployment context: what information will actually be available when the model must make predictions? Leakage is confounding by information that exists in evaluation but not in application.


## Detecting Confounding {#sec-ch12-detection}

Confounding is often subtle, requiring systematic diagnostics rather than reliance on aggregate performance metrics.

::: {#fig-confounding-detection}
![Diagnostic toolkit for detecting confounding](../figs/part_2/ch12/05-fig-confounding-detection.svg)

Diagnostic toolkit for detecting confounding. (1) Confounder-only baselines: if ancestry PCs alone approach full model performance, ancestry confounding is likely. (2) Stratified analysis: large performance gaps between ancestry groups suggest the model exploits group-specific shortcuts. (3) Residual association: predictions should not correlate with ancestry after controlling for the true label; residual correlation indicates ancestry encoding beyond what the task requires. (4) Split sensitivity: performance should not depend dramatically on splitting strategy; large drops under locus-level or temporal splits indicate memorization. (5) Negative controls: models should not predict outcomes unrelated to genetics (insurance type, administrative codes); above-chance prediction indicates learned confounders. Apply all diagnostics; confounding may manifest in only some.
:::

### Confounder-Only Baselines {#sec-ch12-confounder-baselines}

The most direct diagnostic trains simple models using only potential confounders: ancestry principal components, batch indicators, sequencing center identifiers, recruitment site. If these confounder-only baselines approach the performance of complex genomic models, confounding likely drives a substantial portion of the signal. Reporting confounder-only baselines alongside genomic model results makes hidden shortcuts visible.

::: {.callout-tip}
## Key Insight: The Baseline Diagnostic

If a model using only ancestry principal components (no genomic features) achieves 0.75 auROC, and your full genomic model achieves 0.82 auROC, how much of that 0.82 reflects biology versus ancestry confounding? The baseline provides a floor: any performance attributable to your genomic features is the delta above this floor. Always compute and report confounder-only baselines.
:::

The choice of baseline fundamentally shapes conclusions about model performance. A particularly insidious form of baseline weakness occurs when polygenic prediction studies use only **clumping-and-thresholding (C+T)** methods as comparators rather than LD-aware Bayesian approaches. C+T aggressively discards genetic signal by pruning correlated variants, creating an artificially weak baseline that inflates apparent deep learning gains by 16-60% compared to properly tuned alternatives [@ge_polygenic_2019; @vilhjalmsson_modeling_2015].

Studies reporting neural network "improvements" over C+T baselines may be demonstrating only that neural networks implicitly model linkage disequilibrium—which LD-aware Bayesian methods like LDpred2 and PRS-CS already capture more efficiently. When compared against these stronger baselines, apparent neural network advantages often disappear or reverse. A 2025 Nature Communications analysis found that neural networks performed only 93-95% as well as properly tuned linear regression for polygenic prediction when appropriate baselines were used, with apparent "nonlinear advantages" reflecting joint-tagging effects rather than genuine epistasis detection.

::: {.callout-warning title="Baseline Selection as Hidden Confounder"}
Weak baselines function as hidden confounders in model evaluation: they create spurious associations between model complexity and performance improvement that do not reflect genuine capability gains. Always verify that published comparisons include LD-aware methods (LDpred2-auto, PRS-CS, SBayesR) rather than only C+T. Claims of substantial improvement over "state-of-the-art" warrant skepticism until baseline strength is confirmed.
:::

### Stratified Performance Analysis {#sec-ch12-stratified-performance}

Performance stratified by ancestry group, sequencing platform, institution, and time period reveals whether aggregate metrics mask heterogeneity. Both discrimination (auROC, **area under the precision-recall curve (auPRC)**) and **calibration** diagnostics should be computed for each subgroup. Models may achieve high overall auROC while being poorly calibrated or nearly useless for specific subpopulations. Performance that varies dramatically across subgroups suggests confounding or distribution shift even when overall metrics appear strong.

### Residual Confounder Associations {#sec-ch12-residual-associations}

Associations between model outputs and potential confounders can reveal encoding of ancestry or batch information beyond what the label requires. Plotting predictions against ancestry principal components, adjusting for true label status, shows residual confounding. Comparing mean predicted risk across batches or time periods within the same true label class identifies systematic biases. Formal association tests (regression, mutual information) between predictions and confounders that show strong residual associations indicate the model has learned confounder-related features that go beyond predicting the label itself.

### Split Sensitivity Analysis {#sec-ch12-split-sensitivity}

Varying the splitting strategy probes for leakage. Re-evaluating performance under locus-level splits, cohort holdouts, or temporal splits reveals whether initial results depended on memorization. A model that achieves 0.90 auROC with random splits but only 0.75 auROC with locus-level splits has likely memorized site-specific patterns. Large drops in performance under stricter splitting indicate inflated initial results.

### Negative Control Outcomes {#sec-ch12-negative-controls}

Using outcomes known to be unrelated to genomics as **negative controls** provides powerful confirmation of confounding. If a model trained to predict disease from genotypes can also predict administrative outcomes (insurance type, documentation completeness) with similar accuracy, it has learned confounders. Shuffling labels within batch or ancestry strata should eliminate predictive signal; if it does not, the model exploits structure that transcends any specific outcome.

::: {.callout-tip title="Knowledge Check: Apply Detection Methods"}
You've trained a disease prediction model that achieves 0.88 auROC. Apply what you've learned about detection:

1. Name three diagnostic tests you would run to detect confounding
2. For each test, describe what result would indicate a problem
3. If your confounder-only baseline achieves 0.82 auROC, what does this tell you about your model?

:::{.callout-note collapse="true" title="Check Your Answer"}
(1) Three key diagnostics: (a) Confounder-only baseline using ancestry PCs and batch indicators, (b) Stratified performance analysis across ancestry groups and sequencing centers, (c) Split sensitivity comparing random vs. locus-level vs. temporal splits. (2) Problem indicators: (a) Baseline approaches full model performance (e.g., 0.82 vs 0.88), indicating confounding drives most signal; (b) Performance varies dramatically across subgroups (e.g., 0.90 in European ancestry but 0.60 in African ancestry), suggesting shortcuts; (c) Performance drops substantially under stricter splits (e.g., from 0.88 to 0.70 with locus-level), indicating memorization. (3) If the confounder-only baseline achieves 0.82 auROC while your full model achieves 0.88, then 0.82 of your performance comes from ancestry/batch shortcuts, and only 0.06 (the delta) comes from actual genomic features—your model has learned primarily confounders, not biology.
:::
:::


## Mitigation Strategies {#sec-ch12-mitigation}

No mitigation strategy eliminates confounding entirely, and each involves trade-offs between bias, variance, and coverage. The approaches described here are complementary: design-based methods constrain confounding before modeling begins, statistical adjustments handle residual confounding, invariance learning provides protection when confounders are incompletely measured, and rigorous benchmark construction ensures that evaluation reflects generalization rather than shortcut learning.

::: {#fig-mitigation-strategies}
![Mitigation strategies address confounding at different stages](../figs/part_2/ch12/06-fig-mitigation-strategies.svg)

Mitigation strategies address confounding at different stages. Study design approaches (matching, balanced sampling) prevent confounding before data collection but reduce sample size. Covariate adjustment during training explicitly models known confounders but may inadvertently remove genuine biological signal. Residualization removes confounded variance in preprocessing but assumes linear relationships. Adversarial invariance learning trains representations that do not encode confounders but requires knowing which groups to enforce invariance over. Group DRO optimizes for worst-group performance at the cost of average performance. Multi-site training diversifies data sources. Temporal splits evaluate prospective performance. No single strategy eliminates all confounding; effective practice combines multiple approaches targeting different confounding sources.
:::

The following table provides a decision framework for selecting mitigation strategies based on your specific situation:

: Mitigation strategy selection guide. Choose based on when intervention is possible, what confounders are measured, and acceptable trade-offs. {#tbl-mitigation-strategies}

| Strategy | When Applied | Confounder Requirement | Main Trade-off | Best For |
|----------|--------------|----------------------|----------------|----------|
| **Matching** | Study design | Known, measurable | Reduced sample size | Known major confounders |
| **Covariate adjustment** | Training | Known, measurable | May remove real signal | Ancestry, batch correction |
| **Residualization** | Preprocessing | Known, measurable | Information loss | Strong linear confounding |
| **Adversarial invariance** | Training | Known groups | Reduced accuracy | Unknown within-group variation |
| **Group DRO** | Training | Known groups | Worse average performance | Fairness-critical applications |
| **Multi-site training** | Data collection | None | Logistical complexity | Institution effects |
| **Temporal splits** | Evaluation | Time stamps | Smaller test set | Prospective deployment |

### Study Design and Cohort Construction {#sec-ch12-study-design}

Design-based approaches provide the most robust protection against confounding because they prevent the problem rather than attempting to correct it statistically. When cases and controls are matched on potential confounders before data collection, those variables cannot drive spurious associations regardless of model complexity.

Matching strategies balance cases and controls on age, sex, ancestry, recruitment site, and sequencing platform. For ancestry, matching can use self-reported categories, genetic principal components, or fine-scale population assignments depending on the granularity required. Why does matching work when statistical adjustment could handle the same variables? Matching eliminates confounding by design rather than by assumption. Statistical adjustment assumes the functional form relating confounders to outcomes is correctly specified; if the true relationship is nonlinear or involves interactions the model does not include, residual confounding persists. Matching makes no such assumptions: when cases and controls have identical confounder distributions, no functional form is needed because there is no confounder-outcome variation to model. Exact matching (requiring identical values) provides the strongest protection but may be infeasible when confounders are continuous or when the pool of potential controls is limited. **Propensity score matching** or coarsened exact matching offer practical alternatives that achieve approximate balance across multiple confounders simultaneously.

Balanced sampling during training prevents models from optimizing primarily for majority patterns. When one ancestry group comprises 80% of training data, gradient updates predominantly reflect that group's patterns, and minority group performance suffers. Down-sampling the majority group or up-sampling minority groups within mini-batches ensures that all groups contribute meaningfully to parameter updates. The trade-off is reduced effective sample size: discarding majority group samples wastes information, while up-sampling minority groups risks overfitting to limited examples.

Prospective collection with diversity targets ensures that training data represent the populations where models will be deployed. Retrospective matching can balance existing cohorts but cannot address variants or patterns that are absent from the original collection. The All of Us Research Program, Million Veteran Program, and similar initiatives that prioritize ancestral diversity from inception provide data that enable genuinely generalizable models, though the genomic AI field has yet to fully leverage these resources.

The limitation of design-based approaches is that they must anticipate which variables will confound. Unknown or unmeasured confounders cannot be matched, and over-matching (matching on variables that are consequences of the exposure) can introduce bias rather than remove it. Design and analysis approaches work best in combination: match on known confounders, then adjust for residual imbalances that matching did not eliminate.

### Covariate Adjustment {#sec-ch12-covariate-adjustment}

Covariate adjustment explicitly models confounders rather than ignoring them, allowing estimation of outcome effects that account for confounding variables. The approach is familiar from genome-wide association studies, where including ancestry principal components as covariates in regression models reduces spurious associations driven by population structure.

For foundation models, covariate adjustment takes several forms. The simplest approach includes confounder variables (ancestry PCs, batch indicators, sequencing platform) as additional input features alongside genomic data. The model learns to use confounder information when predicting outcomes, and the genomic feature coefficients or attention weights reflect associations that remain after accounting for confounders. This approach assumes the model can learn the appropriate adjustment; for complex confounding patterns, explicit modeling may be preferable to implicit learning.

Residualization removes confounder-associated variance before training genomic models. Regressing features or phenotypes on confounders and retaining only the residuals ensures that subsequent models cannot exploit confounder-outcome associations. The risk is removing genuine biological signal when confounders correlate with causal variants. Ancestry principal components, for instance, capture population structure that includes both confounding (differential ascertainment) and biology (population-specific genetic architecture). Aggressive residualization may discard the latter along with the former.

**Mixed models** and hierarchical structures treat institution, batch, or ancestry group as random effects, estimating genomic associations while accounting for clustering within groups. This approach is standard in genetic epidemiology and translates naturally to deep learning through hierarchical Bayesian frameworks or explicit modeling of group-level parameters. The key advantage is borrowing strength across groups while allowing group-specific intercepts or slopes, though computational costs increase substantially for large datasets with many groups.

The fundamental limitation of covariate adjustment is that it requires measuring and correctly specifying confounders. Unmeasured confounders remain uncontrolled. Conditioning on colliders (variables caused by both exposure and outcome) introduces bias rather than removing it. Careful causal reasoning, often formalized through directed acyclic graphs, is essential for determining which variables should be adjusted and which should not.

### Domain Adaptation and Invariance Learning {#sec-ch12-domain-adaptation}

**Domain adaptation** methods aim to learn representations that do not encode confounders, achieving predictions that generalize across batches, institutions, or populations without explicitly modeling each source of variation. These approaches are particularly valuable when confounders are numerous, incompletely measured, or difficult to specify.

Adversarial training adds a discriminator network that attempts to predict batch identity, ancestry, or other confounders from learned representations. The feature extractor is trained with two competing objectives: maximize prediction accuracy for the primary task while minimizing the discriminator's ability to recover confounder labels. Why does this adversarial setup produce invariant representations? The gradient reversal trick provides the key insight: during backpropagation, gradients from the discriminator are negated before flowing to the feature extractor. Instead of learning features that help predict the confounder (which normal backprop would produce), the feature extractor learns features that actively hurt confounder prediction. The feature extractor faces a two-player game where the discriminator tries to extract confounder information and the feature extractor tries to hide it. At equilibrium, representations contain information useful for the primary task but encode confounders no better than random chance. When successful, the learned representations retain information useful for prediction while discarding information that distinguishes confounded groups. Domain adversarial neural networks and gradient reversal layers implement this approach efficiently within standard deep learning frameworks.

The theoretical limitation is that perfect invariance and maximum accuracy cannot be achieved simultaneously when confounders correlate with the outcome through both causal and non-causal pathways. Enforcing strict invariance to ancestry, for instance, may remove genuine population-specific genetic effects along with confounding. Practitioners must balance the degree of invariance against task performance, typically through hyperparameters controlling the adversarial loss weight.

**Group distributionally robust optimization (group DRO)** targets worst-group performance rather than average performance, encouraging models that work for all subgroups rather than optimizing for the majority. The training objective minimizes the maximum loss across predefined groups (ancestry categories, sequencing platforms, institutions), ensuring that no group is systematically disadvantaged. This approach requires group labels during training and may sacrifice some average performance to improve worst-case outcomes.

Importance weighting and distribution matching align feature distributions across domains without explicit adversarial training. Samples from underrepresented domains receive higher weights during training, or feature distributions are explicitly matched through maximum mean discrepancy or optimal transport objectives. These methods can be combined with other approaches and are particularly useful when the target deployment distribution is known but differs from training data.

### Data Curation and Benchmark Design {#sec-ch12-benchmark-design}

The signals available for learning depend entirely on how data are curated and how benchmarks are constructed. Careful attention to data provenance and evaluation design prevents many confounding problems that would otherwise require complex modeling solutions.

Deduplication across training and evaluation sets prevents direct memorization. For genomic data, deduplication must operate at multiple levels: individual samples (the same person appearing under different identifiers), family groups (relatives sharing haplotype segments), and genomic loci (the same variant position appearing in both training and test sets). Variant effect prediction requires particularly stringent locus-level deduplication; a model that has seen any variant at position chr1:12345 during training cannot be fairly evaluated on novel variants at that position.

Splitting strategies determine what generalization is actually tested. Random splits assess interpolation within the training distribution. Locus-level splits test generalization to novel genomic positions. Chromosome holdouts test transfer across genomic regions. Cohort splits test robustness to institutional and demographic differences. Temporal splits simulate prospective deployment. Each strategy answers a different question, and benchmark performance under one splitting regime does not guarantee performance under others. Reporting results across multiple splitting strategies reveals which aspects of generalization a model has achieved. The comprehensive treatment of benchmark design in @sec-ch11-benchmarks addresses these considerations in detail.

Benchmark diversity ensures that evaluation reflects the full range of deployment contexts. Benchmarks constructed from a single ancestry group, institution, or sequencing platform test only narrow generalization. Explicitly including diverse ancestries, multiple institutions, and varied technical platforms in evaluation sets reveals performance heterogeneity that homogeneous benchmarks would hide. The ProteinGym and CASP benchmarks in protein modeling demonstrate how thoughtfully constructed evaluation resources can drive genuine progress; genomic variant interpretation would benefit from similar community efforts.

Documentation of overlaps between training resources and benchmarks enables readers to assess potential leakage. When a foundation model is pretrained on gnomAD, fine-tuned on ClinVar, and evaluated on a benchmark that filters variants using gnomAD frequencies, the information flow is complex and potentially circular. Explicit documentation of which resources contributed to which stages of model development allows appropriate skepticism about performance claims. Benchmark papers should catalog known overlaps with major training resources; model papers should acknowledge which benchmarks may be compromised by their pretraining choices.

### Causal Inference Approaches {#sec-ch12-causal-inference}

When observational confounding cannot be eliminated through design or statistical adjustment, causal inference frameworks offer principled alternatives that leverage the structure of genetic inheritance itself.

The random assortment of alleles at meiosis creates natural experiments that **Mendelian randomization** exploits [@davey_smith_mendelian_2003]. Because genotypes are assigned before birth and cannot be influenced by most environmental confounders, genetic variants that affect an exposure (such as a biomarker level or gene expression) can serve as instrumental variables for estimating causal effects on downstream outcomes. Why does this random assortment matter for causal inference? Consider the confounding that plagues observational studies: people with high LDL cholesterol may also smoke, exercise less, and have poorer diets, confounding any association between LDL and heart disease. But the genetic variants affecting LDL levels were randomly assigned at conception, before any lifestyle choices occurred. These variants cannot be confounded by lifestyle because they were fixed before lifestyle existed. By using genetic variants as instruments, Mendelian randomization asks: "Do people who were randomly assigned higher LDL (through genetic lottery) have higher heart disease risk?" This isolates the causal effect of LDL itself. A foundation model trained to predict expression levels can be evaluated for causal relevance by testing whether its predictions, instrumented through genetic variants, associate with disease outcomes in ways that survive Mendelian randomization assumptions. This approach has revealed that many observational biomarker-disease associations reflect confounding rather than causation, and similar logic applies to model-derived predictions.

**Directed acyclic graphs (DAGs)** formalize assumptions about causal structure and clarify which variables should be adjusted, which should be left unadjusted, and which adjustments would introduce bias rather than remove it [@pearl_causality_2009]. Conditioning on a collider (a variable caused by both exposure and outcome) induces spurious associations; conditioning on a mediator blocks causal pathways of interest. Explicit DAG construction forces researchers to articulate their causal assumptions, making hidden confounding visible and enabling principled variable selection. For genomic models, DAGs clarify the relationships among ancestry, technical factors, biological mechanisms, and phenotypic outcomes, revealing which adjustment strategies address confounding versus which inadvertently condition on consequences of the outcome.

Outcomes and exposures known to be unrelated to the prediction target provide empirical tests of residual confounding without requiring complete causal knowledge [@lipsitch_negative_2010]. A negative control outcome is one that should not be causally affected by the exposure of interest; if the model predicts it anyway, confounding is present. A negative control exposure is one that should not causally affect the outcome; association with the outcome again indicates confounding. For a variant effect predictor, administrative outcomes (insurance status, documentation completeness) serve as negative control outcomes that genotypes should not predict. Synonymous variants in non-conserved regions can serve as negative control exposures that should not affect protein function. Strong predictions for negative controls reveal that the model has learned confounders rather than biology.

These causal approaches do not replace careful study design and rigorous splitting, but they provide additional tools for distinguishing genuine biological signal from confounded associations, particularly when the same observational data must serve both training and evaluation purposes.


## Fairness and External Validity {#sec-ch12-fairness}

Confounding connects directly to fairness and health equity. Models that achieve high average performance while failing for specific populations may appear successful while exacerbating existing disparities.

Polygenic risk scores illustrate this tension. European ancestry-derived scores predict cardiovascular disease, diabetes, and breast cancer risk reasonably well within European ancestry populations. Applied to African or Asian ancestry individuals, the same scores show substantially worse discrimination and calibration [@duncan_analysis_2019]. Healthcare systems that deploy these scores without ancestry-specific validation risk providing inferior risk stratification to already underserved populations. The portability analysis framework in @sec-ch03-portability quantifies these degradations, while clinical deployment frameworks (@sec-ch27-fairness) address operational responses.

Variant interpretation exhibits similar patterns. ClinVar contains many more pathogenic variant classifications for European ancestry individuals than for other populations [@landrum_clinvar_2018]. The data composition issues underlying this imbalance are examined in @sec-ch02-clinvar. Predictors trained on ClinVar inherit this imbalance, performing better for variants common in European populations and worse for variants enriched in other ancestries. Clinical deployment of such predictors may reduce diagnostic yield for non-European patients.

::: {.callout-tip title="Knowledge Check"}
A hospital system proposes deploying a polygenic risk score for breast cancer screening prioritization. The score was developed and validated in a European ancestry cohort. The hospital serves a population that is 40% African ancestry and 25% Hispanic/Latino.

1. What fairness concerns should be raised before deployment?
2. What validation studies would you require?
3. What monitoring should be implemented post-deployment?

:::{.callout-note collapse="true" title="Check Your Answer"}
(1) Major fairness concerns: the score may show 40-75% reduced accuracy in non-European populations, potentially providing inferior risk stratification to already underserved groups; differential performance could lead to missed diagnoses in minority populations or inappropriate screening recommendations. (2) Required validation: stratified performance analysis by ancestry group in the target population; calibration assessment for each ancestry group (not just discrimination); comparison to ancestry-matched baselines; sensitivity analysis to understand performance degradation mechanisms. (3) Post-deployment monitoring: track screening recommendations and cancer detection rates stratified by ancestry; monitor for disparities in false positive/negative rates; assess whether the tool improves or worsens existing outcome disparities; implement thresholds for stopping use if equity metrics deteriorate.
:::
:::

The **uncertainty quantification** approaches discussed in @sec-ch23-uncertainty provide partial mitigation: models that report high uncertainty for under-represented populations at least flag predictions that should not be trusted. Out-of-distribution detection methods (@sec-ch23-ood-detection) specifically address when inputs fall outside the training distribution. The **interpretability** methods in @sec-ch24-interpretability can reveal when models rely on ancestry-correlated features, with attribution analysis (@sec-ch24-attribution) identifying which input features drive ancestry-dependent predictions. Yet technical solutions alone are insufficient. Addressing fairness requires intentional data collection that prioritizes under-represented populations, evaluation protocols that mandate subgroup analysis, and deployment decisions that consider equity alongside aggregate accuracy.

External validity asks whether a model's performance in one setting predicts its performance in another. Confounding and distribution shift often cause dramatic external validity failures. A model that achieves excellent metrics in the development cohort may fail when deployed at a different institution, in a different healthcare system, or in a different country. The clinical risk prediction frameworks in @sec-ch27-clinical-integration emphasize multi-site validation precisely because single-site performance frequently fails to generalize.

The fairness implications of confounding extend beyond technical model performance into questions of justice in healthcare resource allocation, diagnostic equity, and the distribution of benefits from genomic medicine. Governance frameworks for addressing these structural challenges are examined in @sec-ch26-regulatory.


## A Practical Checklist {#sec-ch12-checklist}

The following checklist synthesizes the diagnostics and mitigations discussed above. Systematic application during model development and evaluation surfaces confounding that would otherwise remain hidden.

::: {.callout-note}
## Practical Guidance: Using This Checklist

This checklist should be applied at three stages: (1) during study design, to prevent confounding through matching and balanced sampling; (2) during model development, to detect and mitigate confounding through diagnostics and training modifications; and (3) during evaluation, to ensure that performance estimates reflect genuine generalization rather than shortcut learning. Document your responses to each item in your methods section.
:::

**Population structure and relatedness**: Quantify ancestry via principal components and relatedness via kinship coefficients. Decide explicitly whether to match, stratify, or adjust for these factors, and document the justification. Report performance stratified by ancestry group. When family structure exists in the data, verify that relatives do not appear across train-test boundaries.

**Data splits and leakage**: Ensure individuals, families, and genomic loci do not cross the train-validation-test boundaries for target tasks. Implement stricter splits (locus-level, chromosome-level, cohort-based, time-based) and report the performance differences. Check for overlap with external databases or benchmarks used in evaluation and document any shared resources.

**Batch, platform, and cohort effects**: Catalog technical variables (sequencing center, instrument, protocol, assay) and cohort identifiers. Test whether these variables predict labels or align with subgroups of interest. Use embedding visualizations, principal components, or simple classifiers to detect batch signatures. Apply mitigation (design matching, covariate adjustment, domain adaptation) when batch effects are detected.

**Label quality and curation bias**: Document how labels were defined and what processes (billing codes, expert review, computational prediction, registry inclusion) produced them. Quantify label noise where possible. Consider robust training strategies when labels are noisy. Assess how curated resources like ClinVar reflect historical biases and whether those biases affect evaluation validity.

**Cross-group performance and fairness**: Report metrics for each major subgroup (ancestry, sex, age, cohort, platform) rather than only aggregate performance. Examine calibration across groups, not just discrimination. Discuss clinical implications of residual performance gaps and whether deployment might worsen existing disparities.

**Reproducibility and transparency**: Document dataset construction, inclusion criteria, and splitting strategies completely. Release preprocessing, training, and evaluation code when feasible. Describe which confounders were measured, how they were handled, and what limitations remain.

Models that pass this checklist provide more reliable evidence of genuine biological learning. Models that fail at multiple points may achieve benchmark success while learning shortcuts that will not transfer to new settings.


## Rigor as Response {#sec-ch12-rigor}

These confounding and bias problems are not reasons for despair. They are reasons for rigor. The same expressive capacity that enables foundation models to discover subtle shortcuts also enables them to learn complex biological patterns when training data and evaluation protocols are designed appropriately. The goal is not to abandon powerful models but to create conditions under which their power serves biological discovery rather than benchmark gaming.

Several trends support progress. Multi-ancestry biobanks and international collaborations expand the diversity of available training data. Benchmark developers implement stricter splitting protocols and require subgroup analyses. Pretraining strategies that explicitly promote invariance to technical factors are emerging. Uncertainty quantification methods (@sec-ch23-uncertainty) provide mechanisms for models to express appropriate caution when inputs fall outside their training distribution. The problem of confounding is tractable with sustained attention to data provenance, evaluation design, and deployment monitoring. The benchmark catalog in @sec-ch11-benchmarks identifies which evaluation resources are most susceptible to particular confounders, while the evaluation methodology in @sec-ch11-eval provides protocols for detecting leakage before it inflates reported performance.

Yet vigilance remains essential. New datasets bring new confounders. Novel architectures create new opportunities for shortcut learning. Community benchmarks accumulate indirect leakage as resources are reused across studies. Treating confounding as a first-order concern throughout model development, rather than an afterthought addressed only when reviewers complain, distinguishes models that actually work from models that merely perform well on convenient benchmarks. The interpretability methods in @sec-ch24-interpretability provide tools for distinguishing genuine regulatory insight from sophisticated pattern matching, with mechanistic interpretability (@sec-ch24-mechanistic) offering the strongest evidence about what models have actually learned. The uncertainty quantification approaches in @sec-ch23-uncertainty enable models to communicate when their predictions should not be trusted, with selective prediction (@sec-ch23-selective-prediction) providing operational frameworks for routing uncertain cases to human review. Together with rigorous evaluation, these capabilities move the field toward models that reveal genuine biology and behave reliably across the diverse clinical and scientific settings where they will be deployed.

::: {.callout-tip title="Test Yourself"}
Before reviewing the summary, test your recall:

1. Distinguish between confounding, bias, data leakage, and distribution shift. Give a concrete genomic example of each.
2. How does population structure create shortcuts that foundation models exploit, and why does increasing model capacity amplify rather than mitigate this problem?
3. A confounder-only baseline using ancestry PCs achieves 0.75 auROC, while your full genomic model achieves 0.82 auROC. What does this reveal about your model's learned features?
4. Why does label circularity in ClinVar (where computational predictions influence pathogenicity annotations) inflate validation metrics for new predictors trained on ClinVar?
5. What splitting strategies address different types of leakage (individual overlap, family structure, position memorization, temporal drift)?

:::{.callout-note collapse="true" title="Check Your Answers"}
1. **Confounding, bias, leakage, and distribution shift**: Confounding occurs when a variable affects both features and labels (example: ancestry influences allele frequencies through population history and disease risk through healthcare access pathways). Bias is systematic deviation from the target quantity (example: training on 50% disease prevalence but deploying at 5% prevalence causes systematic over-prediction). Data leakage occurs when test information influences training (example: the same variant appearing in both training and test sets, or ClinVar labels being influenced by CADD scores that later become training features). Distribution shift is mismatch between training and deployment distributions (example: a model trained on one hospital's coding practices failing at a different institution with different documentation standards).

2. **Population structure as exploitable shortcut**: Population structure creates dual pathways where ancestry affects genetic features (through population-specific allele frequencies, haplotypes, and linkage disequilibrium patterns) and simultaneously affects disease labels through non-biological pathways (healthcare access, environmental exposures, clinical ascertainment practices). Foundation models can detect ancestry from local k-mer frequencies and haplotype patterns even in raw sequences, then exploit ancestry-label correlations as shortcuts. Increasing model capacity amplifies this problem because larger transformers with billions of parameters can discover increasingly subtle ancestry-linked features that smaller models would miss—model expressiveness is an amplifier of confounding, not a defense against it.

3. **Interpreting confounder-only baseline performance**: The confounder-only baseline achieving 0.75 auROC while the full model achieves 0.82 auROC reveals that the vast majority of predictive performance (0.75 out of 0.82) comes from ancestry/batch shortcuts rather than genomic biology. Only the 0.07 delta represents signal genuinely attributable to genomic features beyond what ancestry alone provides. This indicates the model has primarily learned to exploit confounders rather than biological mechanisms, and would likely fail when deployed in settings where ancestry-outcome relationships differ from the training distribution.

4. **Label circularity inflates validation metrics**: When clinical laboratories cite computational predictions like CADD or REVEL as supporting evidence for pathogenic classifications, and those classifications become ClinVar labels, new predictors trained on ClinVar face a circular task: they are learning to replicate what previous predictors said, not learning true pathogenicity. The inflation occurs because the new model achieves high agreement on labels that were influenced by computational predictions—essentially predicting what old models predicted rather than independent biological truth. This circularity is invisible within the circular ecosystem but becomes apparent during prospective validation on genuinely novel variants or independent functional assays where the feedback loop does not exist.

5. **Splitting strategies for different leakage types**: Individual overlap requires random individual-level splits at minimum, but this only works when samples are truly independent. Family structure requires family-aware splits that keep all relatives together in the same partition to prevent haplotype memorization. Position memorization requires locus-level splits that hold out entire genomic positions, ensuring the model has never seen any variant at test positions during training. Temporal drift requires time-based splits that train on earlier data and test on later data, simulating prospective deployment and capturing evolution in sequencing technology, coding practices, and diagnostic criteria. Each splitting strategy tests a different aspect of generalization, and models should be evaluated under all relevant strategies to demonstrate genuine robustness.
:::
:::

::: {.callout-note}
## Chapter Summary

**Core Concepts:**

- **Confounding** occurs when a variable affects both features and labels, creating spurious associations that models exploit as shortcuts
- **Population structure** is the most pervasive confounder in genomics, affecting both genetic features and phenotypes through non-biological pathways
- **Batch effects** from sequencing centers, capture kits, and analysis pipelines can become predictive signals that fail at deployment
- **Label circularity** occurs when computational predictions influence training labels, creating apparent validation that reflects agreement rather than insight
- **Data leakage** can be understood as confounding by information unavailable at prediction time

**Key Diagnostics:**

- Confounder-only baselines reveal how much signal comes from shortcuts versus biology
- Stratified performance analysis exposes hidden heterogeneity across subgroups
- Split sensitivity analysis (random vs. locus-level vs. temporal) tests for memorization
- Negative control outcomes confirm whether models learn confounders

**Mitigation Hierarchy:**

1. **Prevention through design**: Matching, balanced sampling, prospective diverse collection
2. **Adjustment during training**: Covariate inclusion, residualization, mixed models
3. **Invariance learning**: Adversarial training, group DRO
4. **Rigorous evaluation**: Locus-level splits, cohort holdouts, temporal validation

**Connection to Other Chapters:**

- Evaluation methodology (@sec-ch11-eval) provides detailed leakage detection protocols
- Uncertainty quantification (@sec-ch23-uncertainty) flags predictions on out-of-distribution inputs
- Interpretability (@sec-ch24-interpretability) reveals what features models actually use
- Clinical deployment (@sec-ch27-clinical-integration) addresses operational fairness monitoring

**Take-Home Message:** High benchmark performance is not evidence of biological learning. Only rigorous evaluation design, systematic confounding diagnostics, and stratified subgroup analysis can distinguish models that have learned biology from models that have learned shortcuts. Treat confounding as a first-order concern throughout model development.
:::
