# Part II: Sequence Architectures {#sec-part2-intro .unnumbered}

Every neural network architecture encodes assumptions about biology. Convolutional networks assume that local patterns matter and that the same motifs are meaningful regardless of genomic position. Attention mechanisms assume that distant positions can interact directly without passing information through intermediate representations. Pretraining objectives assume that certain patterns in unlabeled sequence provide useful supervision in the absence of functional labels. These assumptions, embedded in architectural choices made before any training begins, determine which biological phenomena the model can capture and which remain invisible to it.

Architectural choices made before training begins constrain everything a model can learn. Tokenization choices (@sec-representations) propagate through model design, from one-hot encoding through byte-pair encoding to biologically informed vocabularies. Convolutional neural networks (@sec-cnn) first demonstrated that deep learning could outperform handcrafted features for regulatory genomics by learning sequence-to-function mappings directly from data. Self-attention mechanisms and transformer architecture (@sec-attention) enable both local pattern recognition and long-range dependency modeling across genomic sequences.

Self-supervised objectives shape what models learn from unlabeled sequence (@sec-pretraining). Masked language modeling, next-token prediction, and denoising approaches each encourage models to discover different biological patterns and produce representations with distinct properties. Adapting pretrained models to downstream tasks (@sec-transfer) through fine-tuning, few-shot learning, and deployment strategies completes the path from raw sequence to useful prediction.