# Part II: Sequence Architectures {.unnumbered}

::: {.callout-note}
## Part II at a Glance

**Central question:** How do architectural choices made before training begins determine what a model can learn about biological sequences?

**Prerequisites:** Part I (genomic data context). For deep learning background, see Appendix A.

| Chapter | Topic | Key Concepts |
|---------|-------|--------------|
| @sec-ch05-representations | Sequence Representations | One-hot, k-mers, BPE, learned embeddings, position encodings |
| @sec-ch06-cnn | Convolutional Networks | Motif detection, regulatory prediction, receptive field limitations |
| @sec-ch07-attention | Attention & Transformers | Self-attention, position encodings, long-range dependencies |
| @sec-ch08-pretraining | Pretraining Objectives | MLM, next-token prediction, contrastive learning, multi-task |
| @sec-ch09-transfer | Transfer Learning | Fine-tuning, domain adaptation, few-shot learning |
| @sec-ch10-adaptation | Model Adaptation | Parameter-efficient fine-tuning, LoRA, prompt tuning |
| @sec-ch11-benchmarks | Benchmarks & Evaluation | Benchmark suites, evaluation methodology, metrics |
| @sec-ch12-confounding | Confounding & Leakage | Data leakage, batch effects, population stratification |

**After completing Part II, you will understand:**

- How tokenization and representation choices shape what models can learn
- Why CNNs revolutionized genomic deep learning and where they hit limits
- How attention mechanisms enable long-range dependency modeling
- What pretraining objectives teach models about biological sequence
- How to transfer learned representations to new tasks
- How to evaluate models rigorously and detect confounding
:::

Every neural network architecture encodes assumptions about biology. Convolutional networks assume that local patterns matter and that the same motifs are meaningful regardless of genomic position. Attention mechanisms assume that distant positions can interact directly without passing information through intermediate representations. Pretraining objectives assume that certain patterns in unlabeled sequence provide useful supervision in the absence of functional labels. These assumptions, embedded in architectural choices made before any training begins, determine which biological phenomena the model can capture and which remain invisible to it.

Architectural choices made before training begins constrain everything a model can learn. Tokenization choices (@sec-ch05-representations) propagate through model design, from one-hot encoding through byte-pair encoding to biologically informed vocabularies. Convolutional neural networks (@sec-ch06-cnn) first demonstrated that deep learning could outperform handcrafted features for regulatory genomics by learning sequence-to-function mappings directly from data. Self-attention mechanisms and transformer architecture (@sec-ch07-attention) enable both local pattern recognition and long-range dependency modeling across genomic sequences.

Self-supervised objectives shape what models learn from unlabeled sequence (@sec-ch08-pretraining). Masked language modeling, next-token prediction, and denoising approaches each encourage models to discover different biological patterns and produce representations with distinct properties. Adapting pretrained models to downstream tasks (@sec-ch09-transfer) through fine-tuning, few-shot learning, and deployment strategies completes the path from raw sequence to useful prediction. Parameter-efficient adaptation methods (@sec-ch10-adaptation) enable practical fine-tuning when computational resources or labeled data are limited.

Rigorous evaluation requires understanding both what benchmarks measure (@sec-ch11-benchmarks) and how confounding can inflate apparent performance (@sec-ch12-confounding). These evaluation principles apply throughout the book; mastering them here enables critical assessment of the foundation models surveyed in Parts III and IV.

::: {.callout-tip}
## Connections to Other Parts

- **Part I** provides the data context that makes architectural choices meaningful
- **Part III** applies these principles to specific foundation model families (DNA-LM, protein-LM, regulatory)
- **Part IV** extends these architectures to cellular context and systems-scale modeling
- **Part V** deepens the evaluation framework with uncertainty, interpretability, and causal reasoning
:::
