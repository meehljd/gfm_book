# Transfer Learning Foundations {#sec-ch09-transfer}

> Transfer learning fails as often as it succeeds. The failures are silent.

::: {.callout-note title="Chapter Overview"}
**Estimated reading time:** 30-40 minutes

**Prerequisites:** Understanding of neural network basics (@sec-apx-a-pretraining), familiarity with pretraining objectives (@sec-ch08-pretraining), and awareness of how genomic sequences are tokenized and embedded (@sec-ch05-representations).

**You will learn:**

- How to determine when transfer learning will help versus hurt your specific task
- The four factors that predict transfer success: task relatedness, data quantity, model expressiveness, and distribution overlap
- How to use linear probing as a diagnostic tool before committing to adaptation
- When frozen features suffice and when more aggressive adaptation is necessary
- How to detect silent transfer failures before they reach clinical applications

**Key insight:** Transfer learning fails silently. A model can produce confident predictions based on patterns completely irrelevant to your task, and nothing in the output signals this failure. The diagnostic tools in this chapter help detect these failures before they cause harm.
:::

A protein language model trained on human sequences may confidently score variants in mouse orthologs, producing predictions that look reasonable but reflect human-specific evolutionary pressures irrelevant to mouse biology. A foundation model pretrained on coding sequences may extract features actively misleading for noncoding regulatory elements. A classifier achieving 90% accuracy on common variants may collapse to chance performance on the rare variants that matter most clinically. Nothing in the model's outputs signals these failures. The predictions look the same whether transfer has succeeded or catastrophically failed. This asymmetry between confident outputs and actual reliability creates the central methodological challenge of applying pretrained models: detecting when transfer works and when it does not, before the predictions reach clinical applications where failures have consequences.

The promise of transfer learning is substantial. Foundation models trained on billions of evolutionary sequences learn representations that capture protein structure, functional constraints, and sequence grammar without task-specific supervision (see @sec-ch08-pretraining). When these representations are applied to downstream tasks with limited labeled data, they can achieve performance that would be impossible for models trained from scratch. A variant effect predictor fine-tuned from *ESM-2* can classify novel missense mutations using patterns learned from the entire protein universe, not just the handful of variants with clinical annotations. This capacity to generalize from abundant unlabeled data to rare clinical scenarios has driven much of the enthusiasm for genomic foundation models.

The reality requires careful navigation. Every adaptation decision involves tradeoffs: preserving pretrained knowledge versus enabling task-specific learning, computational efficiency versus model flexibility, rapid deployment versus careful validation. Like a medical student who studied general anatomy before specializing in cardiology, the question is how much to retain from broad training versus how deeply to reshape understanding for the specialty. Full **fine-tuning** updates all parameters, risking catastrophic forgetting of pretrained knowledge---akin to a specialist who forgets basic anatomy while mastering cardiac surgery. Feature extraction freezes all pretrained parameters, limiting adaptation to task-specific patterns---like applying general anatomical knowledge directly without specialty training. Parameter-efficient methods (adapters, LoRA, prompt tuning) navigate between these extremes, but each makes different assumptions about where adaptation should occur.

::: {.callout-important title="Key Insight"}
Transfer learning involves a fundamental tension: the pretrained model learned something useful, but not necessarily what you need. Your task is to determine whether what it learned helps, hurts, or is irrelevant to your specific problem---before making predictions that others will trust.
:::

## Source and Target Domains {#sec-ch09-source-target}

When a cardiologist requests variant interpretation for a patient with hypertrophic cardiomyopathy, the clinical need (classifying a specific *MYH7* variant) differs fundamentally from the data available during model development (millions of protein sequences sampled across all of evolution). Bridging this gap requires understanding what properties of pretraining determine whether transfer will succeed. When this bridge fails, patients receive confident predictions based on patterns irrelevant to their clinical context.

::: {#fig-domain-alignment}
![Domain shift in genomic transfer learning](../figs/part_2/ch09/01-fig-domain-alignment.svg)

Domain shift in genomic transfer learning. The source domain (left) contains billions of diverse genomic sequences from which pretrained models learn statistical regularities including local motifs, sequence composition, and conservation patterns. The target domain (right) presents sparse labeled examples for specific clinical tasks such as pathogenic variant classification or tissue-specific enhancer prediction. In the learned representation space (center), some features transfer effectively (solid arrows): local motif recognition and conservation patterns align between domains. Other features transfer poorly (dashed arrows): long-range regulatory logic and tissue-specific patterns present in targets may be absent or misleading in source representations. The challenge is that transfer failures are silent—models produce confident predictions regardless of whether underlying features are appropriate for the target task.
:::

### Gap Between Pretraining and Deployment {#sec-ch09-pretraining-deployment-gap}

The **source domain** encompasses the data and objectives used during pretraining. For DNA foundation models, source domains typically include reference genomes, pan-genomic collections spanning population diversity, or metagenomic assemblies sampling environmental sequence space [@ji_dnabert_2021; @dalla-torre_nucleotide_2023]. For protein models, databases like UniRef provide billions of sequences representing the diversity of evolutionary history [@suzek_uniref_2007]. Pretraining objectives (masked language modeling, next-token prediction, contrastive learning) encourage models to capture statistical regularities that help predict held-out tokens: local motifs, compositional patterns, and the signatures distinguishing functional from random sequence (see @sec-ch08-pretraining for detailed treatment of these objectives). These learned regularities become the representations that might transfer to downstream tasks.

The **target domain** presents a fundamentally different challenge. Rather than abundant unlabeled sequence, the target domain offers sparse labeled examples of a specific clinical or biological question: a few thousand enhancer sequences with luciferase measurements, several hundred variants with expert pathogenicity classifications, chromatin profiles across a handful of disease-relevant cell types. The target distribution often looks nothing like pretraining data. Pathogenic variants are rare outliers, not typical protein sequences. Tissue-specific enhancers exhibit patterns that genome-wide pretraining may never emphasize. Disease-associated regulatory elements may have been systematically underrepresented in reference data [@kircher_general_2014].

::: {.callout-tip title="Stop and Think"}
Consider a protein language model pretrained on UniRef sequences. You want to use it to predict pathogenicity of novel missense variants. What types of patterns learned during pretraining might help this task? What patterns might be irrelevant or misleading?

*Think about what the pretraining objective actually rewarded the model for learning, and whether those patterns correlate with what makes a variant pathogenic.*
:::

### Recognizing Transfer Outcomes {#sec-ch09-transfer-outcomes}

Not all transfer helps, and distinguishing outcomes requires explicit validation. **Positive transfer** accelerates learning or improves final performance beyond training from scratch. **Negative transfer** occurs when pretraining actively hurts, either because learned features conflict with task requirements or because pretrained initialization creates optimization difficulties [@wang_characterizing_2019]. Why would pretraining ever hurt? Similar to how learning British English spelling conventions can interfere with American English writing---"colour" feels right even when "color" is required---prior knowledge sometimes points in the wrong direction. Consider a model pretrained on protein-coding sequences that learns to recognize patterns like codon usage bias, amino acid composition, and reading frame consistency. When applied to noncoding regulatory sequences, these coding-specific patterns become noise that the model must unlearn before it can capture regulatory motif patterns. The pretrained initialization points the model in a direction that conflicts with the target task, and gradient descent must first undo this initialization before making progress---wasting optimization steps and potentially never fully escaping the misleading starting point. Neutral transfer describes situations where pretraining neither helps nor hurts, wasting computational resources on pretrained models without benefit. When a cardiology team adapts a DNA language model for *KCNQ1* long QT syndrome variant classification, they must empirically verify which outcome applies to their specific task rather than assuming transfer will help because it helped elsewhere.

| Transfer Outcome | Definition | Example | Detection Strategy |
|-----------------|------------|---------|-------------------|
| **Positive transfer** | Pretrained model improves task performance | ESM embeddings improve variant classification over one-hot encoding | Linear probe outperforms random features |
| **Negative transfer** | Pretraining hurts task performance | Coding-sequence model produces misleading features for noncoding regions | Fine-tuned model underperforms from-scratch training |
| **Neutral transfer** | Pretraining neither helps nor hurts | Model captures irrelevant patterns; adaptation simply overwrites them | Similar performance with and without pretraining |

: Possible outcomes when applying pretrained models to new tasks. The critical challenge is detecting negative transfer, which often manifests only on out-of-distribution examples. {#tbl-transfer-outcomes}

## Factors Determining Transfer Success {#sec-ch09-transfer-factors}

Four factors determine whether this distributional gap can be bridged. Task relatedness measures whether target predictions depend on patterns the model learned during pretraining; predicting transcription factor binding after sequence pretraining succeeds because both involve local motif recognition, while predicting three-dimensional chromatin contacts may require spatial relationships the pretraining objective never captured (see @sec-ch20-3d-genome for chromatin contact prediction approaches). Target data quantity constrains which adaptation strategies avoid overfitting; with thousands of labeled examples, aggressive fine-tuning can reshape representations, but with dozens, only the lightest approaches remain viable. Model expressiveness influences adaptation flexibility, as larger models encode richer internal representations that can potentially serve more diverse downstream tasks but also risk memorizing small target datasets. Distribution overlap between source and target determines how much learned knowledge applies; human regulatory elements share patterns with mouse elements (enabling cross-species transfer) but diverge in species-specific enhancers (limiting it).

Understanding why transfer succeeds or fails requires examining four interacting factors that collectively determine whether pretrained representations serve a new task. These factors are not independent: a highly related task may still fail with insufficient data, while abundant data cannot rescue transfer when source and target distributions fundamentally diverge. Practitioners must evaluate all four before committing to a transfer learning approach.

::: {.callout-warning title="Mathematical Content Ahead"}
The following sections discuss quantitative thresholds and factor interactions. The numerical guidance (e.g., "fewer than 500 examples") is approximate and context-dependent. Focus on the underlying logic: why each factor matters and how they interact.
:::

### Task Relatedness {#sec-ch09-task-relatedness}

Transfer succeeds when target predictions depend on patterns the model learned during pretraining. This dependency is not always obvious from surface-level task descriptions. A model pretrained on DNA sequence using masked language modeling learns to predict nucleotides from context, which implicitly requires learning motifs, sequence composition, and local dependencies. Predicting transcription factor binding sites succeeds because binding depends on sequence motifs that the pretraining objective directly rewarded the model for recognizing. Predicting three-dimensional chromatin contacts typically fails because spatial relationships between distant genomic loci depend on protein-mediated interactions, chromatin accessibility, and nuclear architecture that sequence statistics alone cannot capture (see @sec-ch20-3d-genome for approaches that explicitly model chromatin structure).

The key question is not whether source and target tasks share a domain (both involve genomics) but whether they share relevant features. Protein language models pretrained on evolutionary sequences learn representations that capture structural constraints, functional domains, and evolutionary conservation. Variant effect prediction succeeds because pathogenic variants often disrupt these same structural and functional properties. Protein-protein interaction prediction may succeed partially (interaction surfaces correlate with evolutionary conservation) but fail for interaction specificity (which residues determine *which* partners bind), because the pretraining objective never distinguished between interacting and non-interacting proteins.

Practitioners can estimate task relatedness before committing to transfer through three approaches. First, linear probing (see @sec-ch09-linear-probing) reveals whether frozen pretrained representations contain task-relevant information; if a simple classifier on frozen embeddings outperforms random features, the pretraining objective captured something useful. Second, examining what the pretraining objective explicitly rewards clarifies what patterns the model was incentivized to learn; masked language modeling rewards local context prediction, contrastive learning rewards distinguishing related from unrelated sequences, and next-token prediction rewards sequential dependencies. Third, consulting the literature for related transfer attempts provides empirical guidance; if similar transfers have failed for this model class, success is unlikely without architectural or data modifications.

::: {.callout-tip title="Knowledge Check"}
Before continuing, ensure you can answer: What is the difference between task relatedness based on domain (both are "genomics") versus feature alignment (both require the same learned patterns)? Why does the latter matter more for transfer success?

:::{.callout-note collapse="true" title="Check Your Answer"}
Domain relatedness refers to tasks being in the same field (both involving DNA or proteins), while feature alignment means tasks require similar learned patterns in their representations. Feature alignment matters more because transfer success depends on whether the pretrained features are actually useful for the target task, not just whether tasks share a domain label. For example, predicting gene expression and variant pathogenicity are both "genomics" but require different features, while splice site prediction and variant effect prediction both benefit from learning local sequence constraints despite different prediction targets.
:::
:::

When task relatedness is low, three strategies may salvage transfer. Intermediate fine-tuning on a related auxiliary task can build bridge representations: a model pretrained on general DNA sequence might be fine-tuned on chromatin accessibility prediction before the final adaptation to enhancer-gene linking, because chromatin accessibility provides intermediate features more relevant to regulatory relationships than raw sequence statistics. Multi-task fine-tuning that includes the target task alongside related tasks can encourage the model to extract shared features. Alternatively, practitioners may conclude that transfer is inappropriate for this task and proceed with from-scratch training, which remains a valid choice when pretrained representations offer no advantage.

### Target Data Quantity {#sec-ch09-target-data-quantity}

Available labeled data constrains which adaptation strategies avoid overfitting, creating a fundamental limit on adaptation complexity. The thresholds are approximate but provide useful guidance: with fewer than 500 labeled examples, only linear probing remains viable because any approach that updates pretrained parameters will overfit catastrophically. Between 500 and 5,000 examples, parameter-efficient methods like LoRA introduce enough flexibility to improve over frozen features while maintaining implicit regularization through low-rank constraints and frozen backbone parameters. Above 10,000 examples, full fine-tuning becomes feasible for adapting pretrained representations to fundamentally different target distributions.

| Data Quantity | Viable Strategies | Why | Risk |
|--------------|-------------------|-----|------|
| < 500 examples | Linear probing only | Too few examples to learn new parameters without overfitting | Underfitting if frozen features lack task-relevant information |
| 500 - 5,000 examples | PEFT (LoRA, adapters) | Low-rank constraints provide implicit regularization | Hyperparameter sensitivity; overfitting still possible |
| 5,000 - 10,000 examples | PEFT or careful full fine-tuning | Enough data for some parameter updates | Catastrophic forgetting if learning rate too high |
| > 10,000 examples | Full fine-tuning viable | Sufficient data to reshape representations without memorization | Computational cost; still validate on held-out data |

: Approximate data thresholds for different adaptation strategies. These boundaries are guidelines, not rules---effective thresholds depend on task complexity, class balance, and data quality. {#tbl-data-thresholds}

These thresholds interact with data quality in ways that complicate simple counting. Five thousand noisy labels from high-throughput screening contribute less information than five hundred expert-curated annotations. Class imbalance matters: a dataset with 10,000 examples split 9,900 negative and 100 positive effectively provides only hundreds of examples for learning positive class features. Redundancy in training data (multiple variants from the same gene, or cells from the same patient) reduces effective sample size because nominally independent examples share confounding factors. The relevant quantity is not raw example count but effective information content for the target task.

Data augmentation can stretch limited examples further, but augmentation strategies must preserve task-relevant properties. Reverse-complementing DNA sequences provides valid augmentation for tasks with strand-symmetric biology (transcription factor binding is typically strand-symmetric) but introduces noise for tasks with strand-specific signals (RNA secondary structure depends on transcript strand). Random nucleotide masking followed by model infilling can generate plausible sequence variants, but these variants may not span the relevant distribution of task-specific variation. The safest augmentation strategies involve domain knowledge about what transformations preserve task labels.

When data is severely limited (dozens of examples), practitioners face a choice between three imperfect options. Linear probing on frozen features provides the most stable approach but may miss task-specific patterns not captured in pretrained representations. Few-shot learning methods (see @sec-ch10-few-shot) attempt to adapt with minimal examples by leveraging structured prompts or metric learning, but success varies dramatically across tasks. Collecting more data, though often expensive, may be the only path to reliable adaptation.

### Model Expressiveness {#sec-ch09-model-expressiveness}

Larger models encode richer internal representations that can potentially serve more diverse downstream tasks, but this expressiveness creates a tension with overfitting risk. A 3-billion parameter protein language model captures subtle evolutionary signals invisible to smaller models, encoding relationships between distant residues, complex motif interactions, and nuanced conservation patterns. These rich representations enable zero-shot transfer to tasks the model was never explicitly trained for, because the pretraining objective forced the model to learn features that happen to correlate with task-relevant properties. *ESM-2* at 15 billion parameters predicts protein structure contact maps despite never seeing structure labels during training, because evolutionary constraints that determine which sequences survive (the pretraining signal) are the same constraints that determine which structures fold stably (the transfer target).

The same expressiveness that enables rich transfer creates memorization risk when adaptation data is limited. A highly expressive model can memorize thousands of training examples without learning generalizable patterns, achieving perfect training accuracy while failing entirely on held-out data. This risk scales with model capacity relative to dataset size: a 3-billion parameter model fine-tuned on 500 variants will almost certainly overfit, while the same model fine-tuned on 500,000 variants may generalize effectively.

::: {.callout-tip title="Stop and Think"}
You have two options: (1) a 150-million parameter model and 1,000 labeled examples, or (2) a 3-billion parameter model with the same 1,000 examples. Which would you expect to generalize better, and why? What adaptation strategy might make the larger model viable?
:::

Parameter-efficient methods mitigate this tension by constraining which model behaviors can change during adaptation. Why does restricting the adaptation space help? The core insight is that most of the pretrained model's capacity encodes generally useful features, while only a small subspace needs to change for task-specific adaptation. LoRA restricts updates to low-rank subspaces, limiting the effective capacity available for memorization while preserving the rich pretrained representations for transfer. If a model's behavior can be adapted with rank-8 updates (adding only 8 parameters per dimension to adapt), then the model cannot memorize thousands of unique examples through those 8 degrees of freedom---the low-rank bottleneck prevents it. Adapter layers introduce small trainable modules between frozen layers, enabling task-specific computation without overwriting general knowledge. The rank, placement, and number of adapted parameters become hyperparameters that balance adaptation flexibility against overfitting risk.

Model selection thus involves matching expressiveness to available data and task complexity. For tasks with abundant data and substantial divergence from pretraining, larger models provide more capacity to learn task-specific representations. For tasks with limited data that closely align with pretraining objectives, smaller models may transfer more reliably because their simpler representations leave less room for spurious memorization. The optimal model size depends on the interaction between all four transfer factors, not on model quality in isolation.

### Distribution Overlap {#sec-ch09-distribution-overlap}

The degree of overlap between source and target distributions determines how much learned knowledge applies directly versus requires adaptation. Human and mouse genomes share regulatory syntax for housekeeping genes whose expression patterns were established before the mammalian radiation, enabling direct transfer of core promoter recognition, splice site identification, and basic transcriptional logic. Human-specific enhancers that evolved after the human-mouse divergence (roughly 75 million years ago) have no mouse counterparts from which to transfer, creating blind spots for human enhancer prediction based on mouse training data.

Distribution overlap operates at multiple scales that practitioners must evaluate separately. At the sequence level, nucleotide composition, k-mer frequencies, and local motif distributions may diverge between source and target. Protein sequences from thermophilic organisms differ systematically in amino acid composition from mesophilic training data, potentially confusing models that implicitly learned composition-dependent features. At the feature level, the relationship between sequence patterns and biological function may shift: a motif that indicates enhancer activity in one cell type may be repressive in another due to cofactor availability. At the label level, the definition of positive and negative examples may differ: "pathogenic" variants in ClinVar reflect clinical ascertainment patterns that differ systematically from the evolutionary selection captured in pretraining.

Cross-species transfer illustrates distribution overlap challenges concretely. Models pretrained on human sequences and applied to non-human primates succeed for conserved elements (core promoters, splice sites, essential genes) because evolutionary proximity ensures feature preservation. Application to more distant species (zebrafish, *Drosophila*, plants) succeeds only for deeply conserved features and fails progressively for lineage-specific innovations. Kelley demonstrated that training simultaneously on human and mouse data improves regulatory prediction for both species compared to single-species training, because shared evolutionary history provides implicit labels about functional conservation while species-specific examples reveal where that conservation breaks down [@kelley_cross-species_2020].

::: {.callout-important title="Key Insight"}
Distribution shift can be subtle. A model trained on coding variants may fail on synonymous variants not because the sequences look different, but because the relationship between sequence features and pathogenicity differs. The model learned "this amino acid change is rare in evolution, therefore damaging"---a pattern that does not apply to synonymous changes.
:::

Detecting distribution shift requires comparing source and target distributions before deployment (see @sec-ch10-detecting-shift for methods). Statistical divergence measures quantify distribution differences numerically; embedding visualizations reveal whether target examples occupy familiar or novel regions of representation space; canary examples that should always be predicted correctly provide early warning of catastrophic shift. When shift is detected, practitioners must choose between domain adaptation techniques (which attempt to bridge the gap), acceptance that certain target subpopulations cannot be served by this model, or collection of target-distribution training data to enable proper adaptation.

### Factor Interactions

The four factors interact in ways that preclude simple rules. High task relatedness cannot rescue transfer when target data is too limited for any adaptation; abundant data cannot overcome fundamental distribution mismatch; an expressive model provides no advantage when pretrained representations lack task-relevant features. Practitioners must evaluate all four factors jointly, using the linear probing and validation approaches described in subsequent sections to empirically determine whether transfer succeeds for their specific combination of model, task, and data.

::: {#fig-four-factors layout-ncol=2}
![Task relatedness determines feature relevance](../figs/part_2/ch09/02-A-fig-four-factors.svg)

![Data quantity constrains adaptation strategy](../figs/part_2/ch09/02-B-fig-four-factors.svg)

![Model expressiveness vs. overfitting risk](../figs/part_2/ch09/02-C-fig-four-factors.svg)

![Distribution overlap enables knowledge transfer](../figs/part_2/ch09/02-D-fig-four-factors.svg)

Four interacting factors determine transfer success. (A) Task relatedness: when source and target tasks share relevant features (motif detection, conservation patterns), pretrained knowledge transfers; when they require different features (coding vs. noncoding), transfer fails or hurts. (B) Data quantity thresholds: fewer than 500 examples limits you to linear probing; 500-5,000 enables PEFT; above 10,000 enables full fine-tuning. (C) Model expressiveness creates a double-edged sword: larger models capture richer features but risk memorizing limited target data. (D) Distribution overlap: cross-species transfer succeeds for conserved elements (housekeeping promoters) but fails for lineage-specific innovations (human-specific enhancers).
:::

The most reliable path forward is conservative escalation: establish frozen feature baselines first to assess task relatedness and distribution overlap; try parameter-efficient methods next if frozen features show promise but leave room for improvement; reserve full fine-tuning for cases where simpler methods demonstrably fail and sufficient data exists to justify the risk; and maintain from-scratch training as a valid comparison throughout. Each escalation step provides information about which factors limit transfer, guiding both immediate decisions and future model development.

::: {.callout-note title="Practical Guidance: The Conservative Escalation Protocol"}
When approaching a new transfer learning problem, follow this sequence:

1. **Linear probe first.** Train a simple classifier on frozen embeddings. If this fails badly (near-random performance), the pretrained features may lack task-relevant information.
2. **Compare to random features.** If linear probe on pretrained embeddings barely beats random features, question whether transfer helps at all.
3. **Try PEFT if linear probe shows promise.** If frozen features provide reasonable accuracy but leave headroom, parameter-efficient methods can capture task-specific patterns.
4. **Reserve full fine-tuning for abundant data.** Only with 10,000+ examples and evidence that PEFT is insufficient should full parameter updates be considered.
5. **Always maintain a from-scratch baseline.** This reveals whether transfer actually helps or whether you are simply training on your target data.
:::

::: {#fig-conservative-escalation}
![Conservative escalation decision flowchart](../figs/part_2/ch09/03-fig-conservative-escalation.svg)

Conservative escalation protocol for transfer learning decisions. Start with linear probing on frozen embeddings (Step 1). If probe performance exceeds random baseline, pretrained features encode task-relevant information (Step 2a); proceed to PEFT if headroom remains. If probe performance matches random, question whether transfer helps (Step 2b). PEFT methods like LoRA add flexibility while constraining overfitting (Step 3). Reserve full fine-tuning for cases with abundant data (10,000+ examples) where PEFT is insufficient (Step 4). Throughout, maintain from-scratch baseline to verify transfer provides genuine benefit. Red paths indicate stopping points; green paths indicate progression.
:::

::: {.callout-note title="Worked Example: Applying the Conservative Escalation Protocol"}
A team wants to predict pathogenicity of *BRCA1* variants using ESM-2 embeddings. They have 800 labeled variants.

**Step 1 (Linear probe):** Train logistic regression on frozen ESM-2 embeddings.
- Result: 78% accuracy

**Step 2 (Random baseline):** Train the same classifier on random embeddings.
- Result: 52% accuracy
- Interpretation: The 26-percentage-point gap confirms pretrained embeddings encode pathogenicity-relevant information.

**Step 3 (PEFT consideration):** With 78% accuracy but room for improvement, they try LoRA (rank 8).
- Result: 84% accuracy
- Interpretation: Some task-specific reorganization helps.

**Step 4 (Decision):** With 800 examples, they stop here. Full fine-tuning risks overfitting, and 84% meets requirements.

**Step 5 (Baseline check):** A CNN trained from scratch achieves 71% accuracy.
- Interpretation: Transfer provides genuine 13-point benefit over from-scratch training.

| Approach | Accuracy | Trainable Params | Risk Level |
|----------|----------|------------------|------------|
| Random baseline | 52% | — | Reference |
| From-scratch CNN | 71% | 2M | Moderate |
| Linear probe (frozen ESM-2) | 78% | 1K | Minimal |
| LoRA (rank 8) | 84% | 50K | Low |
| Full fine-tuning | Not attempted | 650M | High with 800 examples |
:::


## Feature Extraction and Representation Analysis {#sec-ch09-feature-extraction}

Clinical laboratories processing hundreds of variants daily cannot afford to fine-tune models for each new gene or variant class. When a novel gene enters diagnostic panels, classifiers must be deployed rapidly using whatever labeled examples exist. A molecular diagnostics team with 200 annotated *RYR1* variants for malignant hyperthermia risk prediction cannot fine-tune a 500-million parameter model; they need an approach that works with minimal data while avoiding adaptation risk entirely.

Frozen feature extraction addresses this constraint by treating pretrained models as fixed representation engines. All backbone parameters remain frozen; only a lightweight classifier trained on the extracted representations learns from labeled data. The backbone never changes, eliminating catastrophic forgetting entirely and enabling deployment within hours rather than weeks. The fundamental tradeoff is clear: frozen features sacrifice adaptation flexibility for speed, safety, and efficiency.

### Linear Probing {#sec-ch09-linear-probing}

Why does the simplest possible classifier often suffice? If pretrained representations already encode task-relevant features in linearly separable form, adding complexity provides no benefit and risks overfitting. **Linear probing** tests this hypothesis by introducing only $d \times c$ parameters (where $d$ is the embedding dimension and $c$ is the number of output classes). Pass input sequences through the frozen model to obtain embeddings, typically from the final layer or from a designated [CLS] token aggregating sequence information, then train a linear classifier mapping embeddings to task labels.

::: {.callout-tip title="Worked Example: Linear Probing for Splice Site Classification"}
Suppose you want to classify whether a sequence contains a functional splice donor site using frozen *DNABERT* embeddings.

**Step 1:** Extract embeddings. For each 200-bp sequence centered on a potential splice site, obtain the 768-dimensional [CLS] embedding from the frozen model.

**Step 2:** Train a linear classifier. With 5,000 labeled examples (2,500 true splice sites, 2,500 negative controls), train a logistic regression: $p(\text{splice}) = \sigma(w^\top h + b)$ where $h$ is the embedding, $w$ is a 768-dimensional weight vector, and $b$ is a scalar bias.

**Step 3:** Evaluate on held-out data. If the linear probe achieves 92% accuracy while random features achieve 60%, the pretrained embeddings encode splice-relevant information that transferred successfully.

**Step 4:** Interpret the result. The 92% accuracy suggests motif patterns learned during pretraining (the GT dinucleotide, surrounding sequence context) are preserved in the embeddings. You now have evidence that transfer works for this task.
:::

::: {#fig-linear-probing-workflow}
![Linear probing workflow for transfer diagnostics](../figs/part_2/ch09/04-fig-linear-probing-workflow.svg)

Linear probing workflow for diagnosing transfer potential. Step 1: Pass input sequences through frozen pretrained model to extract embeddings (typically [CLS] token or final layer). Step 2: Train lightweight classifier (logistic regression, small MLP) on frozen embeddings using labeled target data. Step 3: Compare probe accuracy against random embedding baseline and from-scratch training. Interpretation guide: If probe >> random, pretrained features encode task-relevant information (positive transfer likely). If probe ≈ random, pretrained features lack relevance (consider different model or from-scratch training). If probe < from-scratch, pretraining may hurt (negative transfer risk).
:::

Ji et al. demonstrated that *DNABERT* embeddings paired with linear probes achieve competitive chromatin accessibility prediction from a few hundred positive and negative examples, matching convolutional neural network baselines requiring far more labeled data [@ji_dnabert_2021]. Dalla-Torre et al. showed similar results with *Nucleotide Transformer*, where linear probes on frozen embeddings approached fine-tuned performance for promoter detection and splice site recognition [@dalla-torre_nucleotide_2023]. These successes reflect alignment between pretraining objectives (predicting masked tokens from local context) and target tasks (distinguishing sequences based on motif patterns the model already learned to recognize).

### When Linear Probing Fails {#sec-ch09-linear-probing-limits}

Linear probes fail when relevant information exists in embeddings but requires nonlinear transformation to extract. Shallow multilayer perceptrons (one or two hidden layers) extend linear probing by enabling more complex decision boundaries while maintaining computational efficiency. With several thousand labeled examples, shallow MLPs on *HyenaDNA* embeddings improve splice site prediction over linear probes by capturing interactions between features that linear models cannot represent [@nguyen_hyenadna_2023]. The additional expressiveness helps when task-relevant patterns are distributed across embedding dimensions in ways that linear combination cannot capture.

The more fundamental limitation cannot be addressed by classifier complexity: performance caps at how well pretrained representations already encode task-relevant features. If the pretraining objective emphasized patterns irrelevant to the downstream task, or if required features were actively suppressed during pretraining, frozen features will underperform models trained from scratch regardless of classifier sophistication. A model pretrained exclusively on coding sequence may encode features misleading for noncoding regulatory prediction; no linear probe can overcome representations that point in the wrong direction.

::: {.callout-tip title="Knowledge Check"}
A linear probe on frozen protein language model embeddings achieves 85% accuracy for predicting whether variants are pathogenic. Adding a two-layer MLP increases accuracy to 86%. Adding a five-layer MLP increases accuracy to 86.5%. What do these results suggest about:

1. Whether the pretrained embeddings contain pathogenicity-relevant information?
2. Whether more complex classifiers are likely to help?
3. What the ceiling on frozen-feature performance might be?

:::{.callout-note collapse="true" title="Check Your Answer"}
(1) Yes, the 85% accuracy with a linear probe indicates the frozen embeddings contain substantial pathogenicity-relevant information that is linearly separable. (2) No, the minimal gains from adding complexity (only 1.5% improvement with a five-layer MLP) suggest more complex classifiers won't help much - the useful information is already captured linearly. (3) The ceiling appears to be around 86-87%, indicating that further improvements likely require updating the pretrained model itself rather than just adding more classifier capacity. This pattern suggests linear probing is sufficient and parameter-efficient fine-tuning or full fine-tuning would be needed to go beyond this ceiling.
:::
:::

### Probing Representations {#sec-ch09-probing-representations}

A variant effect predictor built on *ESM* embeddings achieves 85% accuracy in initial testing, but the team deploying it needs to understand why. Does the model genuinely capture evolutionary constraint relevant to pathogenicity, or has it learned spurious correlations that will fail on out-of-distribution variants? Before committing computational resources to adaptation, practitioners benefit from understanding what the pretrained model actually learned.

**Probing classifiers** answer these diagnostic questions by systematically interrogating representations before deployment. The methodology converts the abstract question "will transfer help?" into concrete evidence about representation content: train lightweight classifiers to predict properties of interest from frozen embeddings, then examine how accurately different properties can be decoded. If chromatin accessibility can be predicted with 85% accuracy from a linear probe, the representations already encode accessibility-relevant features and frozen feature extraction will likely succeed. If transcription factor binding requires a deep nonlinear classifier to reach the same accuracy, relevant information exists but is not linearly separable, suggesting PEFT might help by reorganizing representations for easier extraction. If a property cannot be predicted above chance even with flexible classifiers, the representations may lack necessary information entirely, and transfer to this task may fail regardless of adaptation strategy.

### What Probing Reveals About Pretrained Models {#sec-ch09-probing-results}

Systematic probing reveals what models learn during pretraining. Rives et al. demonstrated that *ESM* protein embeddings encode secondary structure so thoroughly that linear probes achieve near state-of-the-art helix/sheet/coil prediction accuracy [@rives_biological_2021]. Contact prediction (which residues are spatially close in folded structure) requires nonlinear probes but still achieves strong performance, indicating that tertiary structure information is present but requires transformation to extract. DNA language models show similar patterns: local motif information is recoverable by linear probes while long-range dependencies require multi-layer networks [@ji_dnabert_2021]. The *ESM* family and its learned structural knowledge are examined in @sec-ch15-protein-lm, while DNA language model probing appears in @sec-ch14-dna-lm.

Layer-wise probing reveals how information transforms through the model. Early layers typically encode local compositional features ($k$-mer frequencies, simple motifs, sequence statistics) while later layers capture more abstract patterns (regulatory signatures, evolutionary constraints, functional classifications) [@jawahar_what_2019]. Why does this layer-wise organization emerge? The structure reflects how neural networks compose features hierarchically: early layers detect simple patterns (individual motifs, dinucleotide frequencies) because they operate on raw input with limited receptive field; later layers combine these detections into higher-order features (motif combinations, spacing patterns, evolutionary signatures) that summarize broader context. The implication for practitioners is that optimal layer selection depends on task complexity: tasks requiring raw motif detection may benefit from early layers, while tasks requiring integration of multiple signals benefit from later layers. Layer selection becomes another hyperparameter to optimize during adaptation.

::: {.callout-important title="Key Insight"}
Probing is diagnostic, not just evaluative. The goal is not just to measure performance but to understand *what* the model knows. This understanding guides adaptation strategy: if probing reveals that secondary structure is encoded but contact information requires nonlinear extraction, you know that contact prediction will benefit from PEFT more than secondary structure prediction.
:::

### Probe-Guided Adaptation {#sec-ch09-probe-guided-adaptation}

The diagnostic value extends beyond predicting which adaptation strategy to use. When probing reveals that required features are absent from pretrained representations, practitioners face a choice: commit to full fine-tuning with sufficient target data (hoping the model can learn missing features), switch to a different foundation model whose pretraining objective better aligns with task requirements, or proceed with from-scratch training that does not inherit inappropriate inductive biases. The investment in probing before adaptation often saves months of wasted effort on transfer that was doomed from the start.

## Summary {#sec-ch09-summary}

Transfer learning succeeds when pretrained representations encode features relevant to downstream tasks, and fails when they do not. The four factors examined in this chapter---task relatedness, target data quantity, model expressiveness, and distribution overlap---collectively determine whether pretrained knowledge transfers productively to new applications.

Feature extraction and linear probing provide the essential diagnostic tools for assessing transfer potential before committing to more complex adaptation. When linear probes on frozen representations outperform random baselines, the pretrained model has captured task-relevant structure. When probing accuracy approaches fine-tuned performance, simpler adaptation strategies may suffice. When probing fails entirely, more aggressive adaptation or alternative models may be necessary.

::: {.callout-tip title="Test Yourself"}
Before reviewing the summary, test your recall:

1. Explain the four factors that determine transfer success (task relatedness, data quantity, model expressiveness, distribution overlap) and how they interact.
2. Why can transfer learning fail silently, producing confident predictions despite learning nothing useful? What makes this particularly dangerous for clinical applications?
3. A linear probe on frozen ESM-2 embeddings achieves 78% accuracy for pathogenicity prediction, while random embeddings achieve 52%. What does this 26-point gap reveal about the pretrained representations?
4. When does negative transfer occur, and why would pretraining sometimes hurt performance compared to training from scratch?

:::{.callout-note collapse="true" title="Check Your Answers"}
1. **Four factors and interactions**: Task relatedness measures whether the target task requires patterns learned during pretraining (e.g., motif recognition transfers well from masked language modeling). Data quantity constrains which adaptation strategies avoid overfitting (fewer than 500 examples limits you to linear probing). Model expressiveness determines how rich the pretrained representations are, but larger models risk overfitting with limited data. Distribution overlap quantifies similarity between source and target data (human-mouse regulatory elements share patterns, enabling cross-species transfer). These factors interact: high task relatedness cannot rescue transfer with insufficient data, abundant data cannot overcome fundamental distribution mismatch, and expressive models provide no advantage when pretrained representations lack task-relevant features.

2. **Silent failures in transfer learning**: Transfer learning fails silently because models produce confident predictions regardless of whether they learned relevant patterns or spurious correlations. A protein language model trained on human sequences may confidently score mouse variants based on human-specific evolutionary pressures completely irrelevant to mouse biology, but the output format looks identical to successful predictions. For clinical applications, this is particularly dangerous because confident but wrong predictions can lead to misdiagnosis, inappropriate treatment decisions, or missed pathogenic variants, with no internal signal that the model's learned features are misaligned with the clinical task.

3. **Interpreting the 26-point gap**: The 26-percentage-point gap between ESM-2 embeddings (78%) and random embeddings (52%) confirms that the pretrained representations encode substantial pathogenicity-relevant information in a form that is already linearly separable. This suggests the pretraining objective (masked language modeling on evolutionary sequences) successfully captured patterns correlated with variant pathogenicity, such as evolutionary constraints, structural preferences, and functional domain information. The gap provides strong evidence that transfer learning will help this task and justifies exploring parameter-efficient methods to capture additional task-specific patterns beyond what frozen features provide.

4. **Negative transfer mechanisms**: Negative transfer occurs when pretraining actively hurts performance because learned features conflict with task requirements or create optimization difficulties. For example, a model pretrained on protein-coding sequences learns patterns like codon usage bias, reading frame consistency, and amino acid composition constraints. When applied to noncoding regulatory sequences, these coding-specific patterns become noise that misleads the model or must be unlearned during fine-tuning. The pretrained initialization points gradient descent in a direction that conflicts with the target task, wasting optimization steps and potentially never fully escaping the misleading starting point, resulting in worse performance than training from scratch without these inappropriate biases.
:::
:::

::: {.callout-note title="Chapter Summary"}
**Core concepts:**

- **Source and target domains**: Pretraining data differs systematically from deployment data; understanding this gap is essential
- **Transfer outcomes**: Positive, negative, and neutral transfer are all possible---only validation distinguishes them
- **Four factors**: Task relatedness, data quantity, model expressiveness, and distribution overlap jointly determine transfer success
- **Linear probing**: The essential first diagnostic---reveals what pretrained representations encode before committing to adaptation
- **Conservative escalation**: Start with frozen features, escalate to PEFT, reserve full fine-tuning for abundant data

**Main takeaways:**

1. Transfer failures are silent. Models produce confident predictions whether transfer has succeeded or failed catastrophically.
2. Task relatedness depends on shared features, not shared domain. "Both are genomics" does not guarantee transfer will help.
3. Data quantity constrains adaptation complexity. With limited data, simpler methods (linear probing, PEFT) avoid overfitting.
4. Probing before adaptation saves wasted effort. Understanding what the model knows guides strategy selection.
5. The conservative escalation protocol provides a systematic path from diagnosis to deployment.

**Looking ahead:** @sec-ch10-adaptation examines the practical adaptation strategies that operationalize these principles: parameter-efficient fine-tuning (LoRA, adapters), layer selection for embedding extraction, full fine-tuning, and the emerging paradigms that extend transfer to minimal-data scenarios.
:::

::: {.callout-tip title="Self-Assessment"}
Before moving to @sec-ch10-adaptation, ensure you can:

1. Explain why transfer learning fails silently and what makes this dangerous for clinical applications
2. Describe the four factors that determine transfer success and how they interact
3. Outline the linear probing procedure and interpret its results
4. Articulate when frozen features suffice versus when more aggressive adaptation is necessary
5. Apply the conservative escalation protocol to a new transfer learning problem
:::
