# Scaling Sequence Context Beyond Attention

## The Quadratic Bottleneck

- Standard Transformer attention scales quadratically (O(L²)) with sequence length (L).
- Limits input context to around 4k tokens, which is insufficient for long-range genomic regulation.

## Alternatives (SSMs and Hyena)

- New architectures leverage:
  - State Space Models (SSMs)
  - Hierarchical convolutions
- Achieve quasi-linear scaling, enabling contexts up to 1 million nucleotides.

Examples:

- **HyenaDNA**:
  - Decoder-only model optimized for runtime scalability.
  - Handles ultra-long genomic sequences.
- **Caduceus (MambaDNA)**:
  - Built on the Mamba SSM block.
  - Designed explicitly for genomics, incorporating:
    - Bi-directionality.
    - Reverse Complement (RC) equivariance.
  - Outperforms 10× larger Transformer models on long-range VEP tasks.
