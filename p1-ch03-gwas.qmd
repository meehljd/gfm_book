# GWAS and Polygenic Scores {#sec-gwas}

A human genome contains roughly five million common variants that differ from the reference sequence. Genome-wide association studies have now linked thousands of these variants to hundreds of diseases and traits, from height and blood pressure to schizophrenia and coronary artery disease. We can compute polygenic scores that aggregate these associations into a single number per person, rank individuals by genetic susceptibility, and identify those in the top percentiles of risk. Yet for most associated variants, we cannot explain how they influence biology. The statistical machinery is mature; the mechanistic understanding lags far behind.

This asymmetry between predictive power and explanatory depth defines the central tension in statistical genetics. GWAS excel at finding genomic regions that harbor trait-relevant variation, but association alone cannot distinguish causal variants from their correlated neighbors, cannot explain which genes or pathways are affected, and cannot predict whether a score trained in one population will transfer to another. Polygenic scores can stratify risk, but they are fundamentally correlational summaries rather than mechanistic models. When a patient asks why their score is high, or a clinician asks which intervention might help, GWAS and PGS provide only partial answers.

This chapter establishes the statistical foundations that underpin much of modern human genetics. We examine how GWAS identify associated regions through millions of parallel hypothesis tests, why linkage disequilibrium makes it difficult to pinpoint causal variants, how fine-mapping attempts to resolve this ambiguity, and how polygenic scores aggregate many small effects into clinically relevant predictions. Throughout, we confront the limitations that motivate the mechanistic approaches developed in later parts of this book: the failure of scores to transfer across ancestries, the difficulty of interpreting noncoding associations, and the fundamental gap between statistical association and biological mechanism.

## The GWAS Framework

Consider a clinician counseling a patient about cardiovascular disease risk. Traditional risk factors (age, smoking, cholesterol, blood pressure) explain perhaps half the variation in who develops disease. Family history suggests that genetics contributes substantially to the remainder, but which genetic variants matter, and how much does each contribute? GWAS provide a systematic approach to answering these questions by testing each of millions of variants for association with the trait of interest.

The core logic is straightforward. For each variant in turn, we ask whether individuals carrying more copies of a particular allele tend to have higher or lower values of the phenotype (for quantitative traits) or higher or lower probability of disease (for binary outcomes). We estimate an effect size, compute a test statistic under the null hypothesis of no association, and record a p-value. After testing millions of variants, we identify those that exceed a stringent significance threshold, report the associated loci, and attempt to interpret which genes and pathways might be involved.

This apparently simple procedure requires careful attention to study design, quality control, and statistical modeling. The phenotype must be measured consistently across individuals. The genotypes must be accurate and the variants well-defined. Confounders that correlate with both genotype and phenotype (most notably population structure) must be controlled. Multiple testing across millions of variants demands stringent significance thresholds. Only after addressing these challenges can GWAS results be trusted and translated into downstream applications.

### Association Models for Quantitative Traits

For continuous phenotypes such as height, LDL cholesterol, or blood pressure, the standard approach is linear regression. Let $y_i$ denote the phenotype for individual $i$, and let $g_{ij}$ denote the genotype dosage at variant $j$, encoded as 0, 1, or 2 copies of the alternative allele (or as a fractional value for imputed genotypes). The model is:

$$
y_i = \alpha + \beta_j g_{ij} + \gamma^\top c_i + \varepsilon_i
$$

The coefficient $\beta_j$ represents the expected change in phenotype per additional copy of the alternative allele, holding covariates $c_i$ fixed. When phenotypes are standardized to zero mean and unit variance, $\beta_j$ is expressed in standard deviation units per allele. The vector $c_i$ typically includes age, sex, genotyping batch, and principal components capturing ancestry (discussed below). The residual $\varepsilon_i$ captures unexplained variation, assumed to be independent and identically distributed across individuals.

For each variant, we compute a test statistic for the null hypothesis $H_0: \beta_j = 0$. In large samples, the t-statistic follows approximately a standard normal distribution under the null, yielding a two-sided p-value. With $M$ variants tested (typically $10^6$ to $10^7$ after imputation), we must correct for multiple comparisons. The conventional genome-wide significance threshold of $5 \times 10^{-8}$ approximates a Bonferroni correction for roughly one million effectively independent tests, accounting for correlation among variants due to linkage disequilibrium [@peer_estimation_2008].

### Association Models for Disease Outcomes

For binary phenotypes (disease present or absent), logistic regression replaces linear regression. The model relates genotype to the log-odds of disease:

$$
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \beta_j g_{ij} + \gamma^\top c_i
$$

Here $\beta_j$ is the log-odds ratio per allele, and $\exp(\beta_j)$ gives the odds ratio (OR). An odds ratio of 1.2 means that each additional copy of the alternative allele increases the odds of disease by 20%. For rare diseases, odds ratios approximate relative risks, but the distinction matters for common conditions and when communicating absolute risk.

Case-control sampling, in which cases are enriched relative to their population frequency, distorts absolute risk estimates but preserves the validity of odds ratio estimation. This is why GWAS conducted in case-control designs can still produce effect sizes useful for polygenic scores, provided downstream applications account for baseline disease incidence.

### Controlling for Population Structure

Population structure poses a fundamental challenge to GWAS interpretation. If allele frequencies differ systematically across subpopulations and the phenotype also varies across these groups for non-genetic reasons (differences in environment, diet, healthcare access, socioeconomic status), naive association testing will detect variants that mark ancestry rather than causal biology.

Principal component analysis on the genotype matrix captures the major axes of genetic variation across individuals [@price_pca_2006; @patterson_population_2006]. The leading principal components often correspond to continental ancestry gradients or finer-scale population structure within a study. Including these PCs as covariates in the regression model attenuates spurious associations driven by ancestry stratification.

This correction is imperfect. Subtle structure not captured by the included PCs, cryptic relatedness among individuals, and the interweaving of genetic ancestry with environmental exposures all complicate interpretation. The challenges extend far beyond technical statistical adjustment: ancestry is entangled with healthcare access, environmental exposures, and socioeconomic factors in ways that simple covariate correction cannot fully resolve. These issues become critical when translating GWAS results to clinical applications and when evaluating whether polygenic scores perform equitably across populations. We return to the full complexity of ancestry as a confounder in @sec-confounding.

## Linkage Disequilibrium and the Association-Causation Gap

GWAS test variants one at a time, but the genome is not inherited one variant at a time. Nearby variants travel together on haplotypes and are co-inherited across generations except when recombination separates them. This correlation structure, known as linkage disequilibrium (LD), is both essential to GWAS power and the source of their fundamental interpretive limitation.

When a GWAS identifies a significant association at variant $j$, three possibilities exist. The variant itself may be causal, directly influencing the phenotype through some molecular mechanism. Alternatively, variant $j$ may simply be correlated with a nearby causal variant $k$ due to LD, with the association signal reflecting this correlation rather than direct causation. In complex regions, multiple causal variants may exist, and the observed association pattern reflects their joint effects filtered through the local LD structure. Distinguishing these scenarios from GWAS summary statistics alone is often impossible.

### The Structure of Linkage Disequilibrium

Recombination during meiosis shuffles genetic material between parental chromosomes. Over many generations, recombination breaks down long-range correlations between variants while preserving short-range structure. The result is a mosaic pattern: regions of high LD (haplotype blocks) where many variants are strongly correlated, interspersed with recombination hotspots where LD decays rapidly.

The squared correlation coefficient $r^2$ quantifies LD between pairs of variants. When $r^2$ approaches 1, the two variants are nearly always observed together on the same haplotypes; when $r^2$ approaches 0, they segregate independently. From a GWAS perspective, if a causal variant $k$ has strong association with the phenotype and variant $j$ is in high LD with $k$ (high $r^2$), then variant $j$ will also show strong association even if it has no direct causal role.

LD patterns vary across populations because demographic history (founder effects, bottlenecks, admixture, population expansion) shapes which haplotypes persist and at what frequencies. A variant that tags a causal allele effectively in one population may be a poor proxy in another where different recombination history has decoupled the correlation. This population-specificity of LD structure is one reason why polygenic scores fail to transfer across ancestries.

### Causal Variants, Tag Variants, and GWAS Catalogs

A causal variant directly influences the phenotype, whether by altering protein sequence, disrupting transcription factor binding, affecting splicing, or modifying chromatin state. A tag variant is merely correlated with a causal variant through LD, serving as a statistical proxy without direct functional consequence.

GWAS catalogs therefore report associated loci, not causal variants. The "lead SNP" at each locus (the variant with the smallest p-value) is often a tag rather than the causal variant, particularly when the causal variant is rare, poorly genotyped, or not present on the array. Even when a locus is robustly associated, dozens or hundreds of correlated variants may be statistically indistinguishable from the lead SNP.

This limitation has practical consequences. Drug development requires identifying causal genes and mechanisms, not just associated regions. Clinical variant interpretation needs to distinguish functional mutations from neutral passengers. Polygenic scores built on tag SNPs may lose power when applied to populations with different LD patterns. The gap between association and causation motivates the fine-mapping approaches we consider next.

## Fine-Mapping: From Loci to Causal Variants

Fine-mapping attempts to resolve the ambiguity created by LD, moving from "this region is associated" to "these specific variants are most likely causal." The core insight is that while multiple variants may show similar marginal association statistics, their joint behavior under a model that accounts for LD can discriminate among them.

### The Statistical Framework

Bayesian fine-mapping methods approach the problem by specifying a prior distribution over which variants in a region might be causal, then computing posterior probabilities given the observed association statistics and local LD structure. The key outputs are posterior inclusion probabilities (PIPs), which estimate the probability that each variant is among the causal set, and credible sets, which are minimal sets of variants that contain the true causal variant(s) with specified probability (commonly 95%).

The procedure typically proceeds as follows. First, define a region around an index SNP, often all variants within 1 megabase. Second, specify a prior: perhaps at most $K$ variants in the region are causal, and causal effect sizes follow some distribution. Third, use the observed marginal association statistics (effect sizes and standard errors) together with an LD matrix (correlations among variants) to compute the likelihood of the data under each possible configuration of causal variants. Fourth, sum over configurations to obtain the marginal PIP for each variant.

Variants with high PIPs (say, above 0.5 or 0.9) are strong candidates for functional follow-up. Credible sets that contain few variants are more actionable than those containing dozens. The width of credible sets reflects both the strength of the association signal and the local LD structure: tight LD means many variants remain plausible even with strong statistical evidence.

### Leveraging Functional Annotations

Pure statistical fine-mapping uses only association statistics and LD. Annotation-informed approaches incorporate functional information to update priors. Variants in coding regions, promoters, enhancers, or regions of evolutionary constraint may be assigned higher prior probability of causality. Integration with chromatin accessibility, transcription factor binding, or expression quantitative trait loci (eQTL) data can further prioritize variants with plausible regulatory mechanisms.

Large-scale resources now link GWAS summary statistics, fine-mapping results, and functional genomic annotations across hundreds of traits and thousands of loci [@mountjoy_open_2021]. These datasets enable systematic identification of variants that are both statistically prioritized and functionally plausible.

### Multi-Ancestry Fine-Mapping

Because LD patterns differ across populations, fine-mapping in a single ancestry leaves substantial ambiguity. A variant in tight LD with many neighbors in Europeans may have fewer proxies in African-ancestry populations, where shorter LD blocks (reflecting larger historical effective population size) provide greater resolution.

Joint fine-mapping across ancestries exploits these differences to narrow credible sets [@pasaniuc_dissecting_2016]. When a variant remains strongly associated across populations despite different local LD structure, confidence in its causal role increases. Multi-ancestry approaches are increasingly important as large biobanks expand to include diverse populations, though they require careful attention to population-specific effect sizes and potential gene-environment interactions.

## Polygenic Score Construction

Polygenic scores aggregate variant effects across the genome into a single number per individual:

$$
\text{PGS}_i = \sum_{j} w_j g_{ij}
$$

The weight $w_j$ reflects the estimated effect of variant $j$, and $g_{ij}$ is the genotype dosage for individual $i$. The simplest approach uses GWAS effect size estimates directly as weights; more sophisticated methods adjust for LD, apply shrinkage, or incorporate fine-mapping information.

The clinical promise is substantial. For diseases with significant genetic components, PGS can identify individuals at elevated risk years or decades before disease onset, potentially enabling targeted screening or prevention. For quantitative traits, PGS explain a portion of individual variation that complements environmental factors. Yet the translation from statistical aggregation to clinical utility requires careful attention to how scores are constructed, validated, and interpreted.

::: {.callout-note}
### Terminology: PGS versus PRS

The literature uses overlapping terminology. **Polygenic risk score (PRS)** is common in clinical contexts, emphasizing disease risk prediction. **Polygenic score (PGS)** is more general, encompassing both disease and quantitative trait prediction. **Genomic risk score** and related terms also appear, often interchangeably. This book uses PGS as the default, adding "risk" when specifically discussing disease outcomes. Methodological overviews provide detailed guidance on construction and evaluation [@choi_prs_2020].
:::

### Clumping and Thresholding

The simplest and historically most common approach is clumping and thresholding (C+T). The procedure involves three steps. First, clumping: rank variants by p-value, then iteratively select the most significant variant and remove all variants within a specified window (say, 250 kb) that are in LD above a threshold (say, $r^2 > 0.1$). This yields a set of approximately independent index variants. Second, thresholding: apply a p-value cutoff and retain only variants below this threshold. Third, weighting: set $w_j$ equal to the GWAS effect size estimate for retained variants, and zero otherwise.

The hyperparameters (LD window, $r^2$ threshold, p-value threshold) are typically chosen by grid search to maximize predictive performance in a held-out validation set. This tuning introduces overfitting risk, particularly in small samples or when the validation population differs from the eventual deployment population.

C+T is transparent and computationally simple, but it discards substantial information. Most variants are excluded, LD is handled only through coarse pruning, and variants with modest p-values that collectively explain meaningful variance may be entirely omitted. These limitations motivate more sophisticated approaches.

### LD-Aware Bayesian Methods

A more principled framework models the joint distribution of effect sizes explicitly, treating the true effects $\beta = (\beta_1, \ldots, \beta_M)$ as random variables drawn from a prior distribution. Given GWAS summary statistics and an LD reference panel, these methods infer posterior mean effect sizes that serve as PGS weights.

LDpred assumes that a fraction $p$ of variants have nonzero effects drawn from a Gaussian distribution, while the remainder have zero effect [@vilhjalmsson_modeling_2015]. The method uses GWAS summary statistics and LD from a reference panel to compute approximate posterior effect sizes. These posteriors shrink noisy estimates toward zero, borrow strength across correlated variants, and generally outperform C+T when properly tuned.

PRS-CS extends this framework by placing a continuous shrinkage prior on effect sizes, which better accommodates the highly polygenic architecture of complex traits and reduces sensitivity to the sparsity hyperparameter [@ge_prs-cs_2019]. The method has shown strong performance across a range of traits and ancestries, though like all methods it requires an LD reference that reasonably matches the target population.

Related approaches (lassosum, SBayesR, and others) use different priors or optimization strategies but share the core insight: jointly modeling effect sizes under LD yields better predictions than pruning LD away. Performance differences among methods are often modest when each is well-tuned, and the choice may depend on computational resources, availability of validation data, and specific trait architecture.

### Fine-Mapping-Informed Scores

Fine-mapping outputs, particularly posterior inclusion probabilities, provide another basis for PGS construction. Variants with high PIPs are more likely to be causal and may therefore be more robust to LD differences across populations. Two strategies are common: selection, where only variants above a PIP threshold contribute to the score, and weighting, where PIPs modulate the contribution of each variant.

Fine-mapping-informed approaches aim to concentrate weight on variants that are biologically meaningful rather than merely statistically associated. In principle, this should improve cross-ancestry transferability since causal variants remain causal regardless of population-specific LD patterns. In practice, gains depend on fine-mapping resolution, and the approaches remain an active area of methodological development.

## Interpreting Polygenic Scores

A polygenic score is a number. Converting that number into actionable information requires understanding what it represents, how it relates to risk or trait values, and where its interpretation breaks down.

### Relative Risk and Percentiles

PGS are most naturally interpreted in relative terms. For a disease outcome, we might fit a logistic regression in a validation cohort:

$$
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \theta \cdot \text{PGS}_i + \eta^\top z_i
$$

where $z_i$ contains covariates and $\theta$ captures the effect of the PGS. After standardizing the score to unit variance, $\exp(\theta)$ gives the odds ratio per standard deviation of the PGS. This metric allows statements like "individuals one standard deviation above the mean have 1.5-fold higher odds of disease."

Percentile-based communication is common in clinical contexts. The risk for individuals in the top 1% or 5% of the PGS distribution can be compared to those near the median or in the bottom percentiles. For some conditions (coronary artery disease, breast cancer, type 2 diabetes), individuals in the top percentiles have risk comparable to or exceeding that conferred by single high-penetrance mutations, making PGS potentially relevant for clinical risk stratification.

### Absolute Risk

Relative risk statements can mislead when baseline risk varies. A 1.5-fold increase in odds for a disease with 1% baseline risk differs substantially in absolute terms from the same relative increase for a disease with 20% baseline risk. Clinical decision-making typically requires absolute risk estimates: the probability that this individual will develop disease over a specified time horizon.

Converting PGS to absolute risk requires combining the score with baseline incidence rates, which vary by age, sex, and other factors. The hazard ratio per SD of PGS, combined with age-specific incidence curves, can yield personalized risk trajectories. Such calculations demand careful attention to calibration: the model must produce well-calibrated probabilities in the population where it will be deployed, not just the population where it was trained. Clinical deployment of PGS is addressed in @sec-clinical-risk.

### Explained Variance and Discrimination

How much of trait variation does a PGS explain? For quantitative traits, the squared correlation between PGS and phenotype ($R^2$) provides a direct answer. For binary traits, the $R^2$ on the liability scale (the underlying continuous risk) is more interpretable than the observed-scale $R^2$, which depends on disease prevalence.

Area under the receiver operating characteristic curve (AUC) measures discrimination: the probability that a randomly selected case has a higher PGS than a randomly selected control. AUC values of 0.5 indicate no discrimination (random guessing); values approaching 1.0 indicate near-perfect separation. For most complex diseases, PGS achieve AUC values in the 0.55 to 0.70 range when used alone, with incremental gains when combined with traditional risk factors.

These metrics characterize population-level performance but say little about individual prediction. Even a PGS that explains 10% of trait variance leaves 90% unexplained by factors genetic and environmental. High-risk individuals by PGS may never develop disease; low-risk individuals may be affected. PGS provide probabilistic risk stratification, not deterministic prediction.

## Ancestry, Portability, and Fairness

The vast majority of GWAS participants have been of European ancestry. This historical imbalance has profound consequences for who benefits from polygenic scores and who may be harmed by their limitations.

### The Portability Problem

Polygenic scores derived from European-ancestry GWAS show markedly reduced performance in other populations. African-ancestry individuals typically experience 40% to 75% reductions in prediction accuracy compared to European-ancestry individuals, even for the same trait measured in the same study [@duncan_analysis_2019]. The pattern holds across traits and across methods, though the magnitude varies.

Several factors contribute to this portability failure. LD structure differs across populations: tag SNPs that effectively proxy causal variants in Europeans may be poor proxies in populations with different recombination history. Allele frequencies differ: variants common in one population may be rare or absent in another. Effect sizes may genuinely differ across populations due to gene-environment interactions or genetic background effects. And GWAS in smaller non-European samples have less power to detect associations, yielding noisier effect estimates.

Multi-ancestry GWAS and methods designed to leverage diverse training data offer partial solutions. Including multiple ancestries in discovery improves transferability, and methods that explicitly model ancestry-specific LD or effect sizes can enhance performance [@marquez-luna_incorporating_2021]. Yet even state-of-the-art approaches do not fully close the gap, and substantial research is needed before PGS perform equitably across populations.

### Fairness and Health Equity

Portability failure is not merely a technical nuisance; it raises fundamental questions about fairness in precision medicine. If genomic models work primarily for individuals of European ancestry, deploying these models in diverse clinical populations risks exacerbating existing health disparities.

Consider a scenario where PGS are used for risk-stratified screening. If the score identifies high-risk individuals more accurately in Europeans than in other groups, Europeans receive more targeted and efficient screening while others receive either under-screening (if falsely classified as low risk) or over-screening (if falsely classified as high risk). The benefits of precision medicine accrue disproportionately to those already overrepresented in research.

These challenges extend beyond PGS to every genomic model in this book. Foundation models can learn to exploit ancestry signals as shortcuts, achieving high benchmark performance while performing poorly on underrepresented groups. Aggregate performance metrics mask inequities across populations. Deployment in diverse clinical settings requires explicit evaluation of performance stratified by ancestry, along with transparent reporting of limitations. We examine these issues comprehensively in @sec-confounding.

## From Association to Mechanism

GWAS and PGS have delivered remarkable achievements: thousands of robust trait associations, clinically useful risk scores for some conditions, and insights into the highly polygenic architecture of complex traits. Yet they have also exposed persistent gaps between statistical association and biological understanding.

Most GWAS hits lie in noncoding regions, often within regulatory elements such as enhancers and promoters. The variant is associated; the mechanism is obscure. Fine-mapping can narrow the list of candidate variants but rarely identifies a single causal nucleotide. Even when a variant is confidently prioritized, the path from sequence change to molecular consequence to cellular phenotype to disease remains opaque.

This mechanistic gap limits translation. Drug development requires actionable targets, not just associated regions. Clinical variant interpretation needs to explain why a variant matters, not just that it correlates with disease. Polygenic scores provide population-level risk stratification but offer little guidance on individual intervention.

The models developed in subsequent parts of this book address this gap through different strategies. Regulatory sequence models (Chapter @sec-regulatory) predict how variants alter transcription factor binding, chromatin accessibility, and gene expression, providing mechanistic hypotheses for noncoding associations. Variant effect prediction methods (Chapter @sec-vep-fm) combine foundation model representations with evolutionary information to assess likely functional impact. Multi-omics integration (Chapter @sec-multi-omics) connects genetic variation to intermediate molecular phenotypes and ultimately to disease outcomes.

The goal is not to replace GWAS and PGS but to build on them. Statistical association provides a map of where trait-relevant variation resides; mechanistic models attempt to explain how that variation produces its effects. The combination of statistical genetics and mechanistic modeling offers the most promising path toward precision medicine that is both predictive and interpretable.