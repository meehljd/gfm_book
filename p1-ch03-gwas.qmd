# GWAS and Polygenic Scores {#sec-gwas}

Genome-wide association studies do not identify causal variants; they identify signposts. When a GWAS reports that a particular SNP associates with coronary artery disease, that SNP is almost certainly not the variant that alters cardiac biology. It is correlated with the causal variant through linkage disequilibrium, the non-random association of nearby alleles that persists across generations. The statistical machinery of GWAS is exquisitely sensitive to these correlations but fundamentally agnostic about mechanism. It can identify a region of the genome that harbors trait-relevant variation without distinguishing the causal variant from its correlated neighbors, without explaining which genes or pathways are affected, and without revealing whether the same associations hold in populations with different linkage patterns.

This distinction between association and causation defines the central intellectual challenge of statistical genetics. GWAS have identified thousands of genomic regions associated with hundreds of complex traits, from height and blood pressure to schizophrenia and type 2 diabetes. These associations replicate across studies with remarkable consistency, confirming that the signals are real. Yet the path from associated region to biological mechanism remains obscure for most loci. The majority of GWAS signals fall in non-coding regions where there is no obvious gene to implicate. Even when a signal overlaps a gene, the mechanism of action (altered expression? splicing? protein function?) is rarely apparent from the association alone.

Polygenic scores aggregate these associations into predictions, summing risk alleles across thousands of loci to estimate an individual's genetic predisposition. For some traits, these scores achieve clinically meaningful discrimination: individuals in the top percentile of coronary artery disease risk have odds ratios comparable to monogenic familial hypercholesterolemia. Yet polygenic scores inherit all the limitations of the associations they aggregate. They predict without explaining, correlate without identifying mechanism, and transfer poorly across populations with different allele frequencies and linkage patterns. Understanding both their power and their limitations is essential for the mechanistic approaches developed throughout this book, where regulatory sequence models and variant effect predictors attempt to move from statistical association to biological explanation.

## The GWAS Framework

Consider a clinician counseling a patient about cardiovascular disease risk. Traditional risk factors (age, smoking, cholesterol, blood pressure) explain roughly 50% of the variation in who develops disease [@khera_genetics_2017]. Family history suggests that genetics contributes substantially to the remainder, but which genetic variants matter, and how much does each contribute? **Genome-wide association studies (GWAS)** provide a systematic approach to answering these questions by testing each of millions of variants for association with the trait of interest.

The scale required for well-powered GWAS explains why large-scale biobanks (@sec-data) have become essential infrastructure for statistical genetics. UK Biobank, with its 500,000 participants genotyped across hundreds of thousands of variants and linked to extensive phenotypic data, has enabled GWAS for thousands of traits at sample sizes that were unimaginable a decade ago. Similar resources, including the Million Veteran Program, FinnGen, and All of Us, continue to expand the scope of discoverable associations. The biobank paradigm of combining dense genotyping with rich phenotyping at population scale has transformed GWAS from underpowered fishing expeditions into reliable discovery engines.

The core logic is straightforward. For each variant in turn, researchers ask whether individuals carrying more copies of a particular allele tend to have higher or lower values of the phenotype (for quantitative traits) or higher or lower probability of disease (for binary outcomes). They estimate an effect size, compute a test statistic under the null hypothesis of no association, and record a p-value. After testing millions of variants, those exceeding a stringent significance threshold are identified, the associated loci reported, and interpretation begins regarding which genes and pathways might be involved.

This apparently simple procedure requires careful attention to study design, quality control, and statistical modeling. The phenotype must be measured consistently across individuals. The genotypes must be accurate and the variants well-defined. Confounders that correlate with both genotype and phenotype (most notably population structure) must be controlled. Multiple testing across millions of variants demands stringent significance thresholds. Only after addressing these challenges can GWAS results be trusted and translated into downstream applications.

### Association Models for Quantitative Traits

Choosing the wrong statistical model for a GWAS does not merely introduce imprecision; it distorts effect size estimates in ways that propagate through every downstream analysis, from fine-mapping to polygenic scores to drug target prioritization. A height GWAS and a schizophrenia GWAS require fundamentally different approaches because one outcome is continuous and the other binary. Applying linear regression to a binary outcome produces fitted values outside the 0-1 probability range and effect estimates that misrepresent biological reality.

For continuous phenotypes such as height, LDL cholesterol, or blood pressure, the standard approach is linear regression. Let $y_i$ denote the phenotype for individual $i$, and let $g_{ij}$ denote the genotype dosage at variant $j$, encoded as 0, 1, or 2 copies of the alternative allele (or as a fractional value for imputed genotypes). The model is:

$$
y_i = \alpha + \beta_j g_{ij} + \gamma^\top c_i + \varepsilon_i
$$

The coefficient $\beta_j$ represents the expected change in phenotype per additional copy of the alternative allele, holding covariates $c_i$ fixed. When phenotypes are standardized to zero mean and unit variance, $\beta_j$ is expressed in standard deviation units per allele. The vector $c_i$ typically includes age, sex, genotyping batch, and principal components capturing ancestry (discussed below). The residual $\varepsilon_i$ captures unexplained variation, assumed to be independent and identically distributed across individuals.

For each variant, a test statistic is computed for the null hypothesis $H_0: \beta_j = 0$. In large samples, the t-statistic follows approximately a standard normal distribution under the null, yielding a two-sided p-value. With $M$ variants tested (typically $10^6$ to $10^7$ after imputation), multiple comparison correction is essential. The conventional **genome-wide significance threshold** of $5 \times 10^{-8}$ approximates a Bonferroni correction for roughly one million effectively independent tests, accounting for correlation among variants due to linkage disequilibrium [@risch_future_1996; @pe'er_estimation_2008].

### Association Models for Disease Outcomes

Binary outcomes create a specific statistical problem that, if ignored, systematically distorts effect size estimates in ways that compound through downstream applications. When the phenotype is disease status (affected or unaffected), linear regression produces nonsensical predictions: fitted values outside the 0-1 probability range and residuals that violate normality assumptions. The consequence extends beyond statistical inelegance. Effect sizes estimated under the wrong model propagate into polygenic scores and risk prediction, potentially misclassifying patients who sit near clinical decision thresholds where intervention recommendations change.

For binary phenotypes, **logistic regression** replaces linear regression. The model relates genotype to the log-odds of disease:

$$
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \beta_j g_{ij} + \gamma^\top c_i
$$

Here $\beta_j$ is the log-odds ratio per allele, and $\exp(\beta_j)$ gives the **odds ratio (OR)**. An odds ratio of 1.2 means that each additional copy of the alternative allele increases the odds of disease by 20%. For rare diseases (prevalence below approximately 10%), odds ratios approximate relative risks, but the distinction matters for common conditions and when communicating absolute risk to patients.

Case-control sampling, in which cases are enriched relative to their population frequency, distorts absolute risk estimates but preserves the validity of odds ratio estimation. This mathematical property explains why GWAS conducted in case-control designs can still produce effect sizes useful for polygenic scores, provided downstream applications account for baseline disease incidence. The likelihood function conditions on disease status, making the odds ratio identifiable regardless of sampling scheme.

### Visualizing Genome-Wide Results

The **Manhattan plot** has become the iconic visualization of GWAS results, named for its resemblance to the New York City skyline. Each point represents a tested variant, with genomic position along the x-axis (ordered by chromosome) and negative log-transformed p-value on the y-axis. Variants with stronger associations rise higher; those exceeding the genome-wide significance threshold of $5 \times 10^{-8}$ (typically drawn as a horizontal line at $-\log_{10}(5 \times 10^{-8}) \approx 7.3$) are considered significant hits.

The Manhattan plot reveals both the successes and limitations of GWAS at a glance. Prominent peaks indicate genomic regions harboring trait-associated variants, but each peak typically contains dozens or hundreds of correlated variants rather than a single causal nucleotide. The width of peaks reflects local linkage disequilibrium structure: broader peaks indicate regions where many variants are correlated with the lead signal. The height reflects statistical strength, which depends on effect size, allele frequency, and sample size. Tall, narrow peaks suggest strong, well-localized signals; broad peaks spanning megabases indicate that fine-mapping will be challenging.

Quantile-quantile (Q-Q) plots complement Manhattan plots by assessing whether the observed p-value distribution matches theoretical expectations under the null hypothesis. Systematic deviation from the diagonal (genomic inflation) suggests either true polygenic signal or residual confounding from population structure. The genomic inflation factor $\lambda$ quantifies this deviation, with values substantially above 1.0 warranting investigation of potential confounders.

### Controlling for Population Structure

Population structure poses a fundamental challenge to GWAS interpretation because it can generate association signals indistinguishable from true biological effects. If allele frequencies differ systematically across subpopulations and the phenotype also varies across these groups for non-genetic reasons (differences in environment, diet, healthcare access, socioeconomic status), naive association testing will detect variants that mark ancestry rather than causal biology. A variant that is simply more common in one population will appear associated with any trait that differs between populations, regardless of biological mechanism. The resulting false positives waste resources on follow-up studies and, more insidiously, can embed ancestry-related confounding into polygenic scores that are then deployed as if they measured pure genetic risk.

**Principal component analysis (PCA)** on the genotype matrix captures the major axes of genetic variation across individuals [@price_principal_2006; @patterson_population_2006]. The leading principal components often correspond to continental ancestry gradients or finer-scale population structure within a study. Including these PCs as covariates in the regression model attenuates spurious associations driven by ancestry stratification.

This correction is imperfect. Subtle structure not captured by the included PCs, cryptic relatedness among individuals, and the interweaving of genetic ancestry with environmental exposures all complicate interpretation. The challenges extend far beyond technical statistical adjustment: ancestry is entangled with healthcare access, environmental exposures, and socioeconomic factors in ways that simple covariate correction cannot fully resolve. These issues become critical when translating GWAS results to clinical applications and when evaluating whether polygenic scores perform equitably across populations. We return to the full complexity of ancestry as a confounder in @sec-confounding.

## Heritability: What Genetics Can Explain

Before GWAS can identify specific variants, a more fundamental question must be answered: how much of the variation in a trait is attributable to genetics at all? A trait entirely determined by environment would yield no GWAS hits regardless of sample size. A trait entirely determined by genetics would, in principle, be fully predictable from genotype. **Heritability** quantifies where traits fall along this spectrum, but the concept is more subtle than it first appears, and different estimation methods yield systematically different answers.

### Pedigree Heritability

Classical genetics estimated heritability by comparing phenotypic similarity among relatives. Identical twins share all their genetic variation; fraternal twins share on average half; full siblings also share half; parents and offspring share half; cousins share one-eighth. If genetic variation influences a trait, closer relatives should be more similar. The correlation structure across relationship types allows partitioning of phenotypic variance into genetic and environmental components.

**Narrow-sense heritability** ($h^2$) represents the proportion of phenotypic variance attributable to additive genetic effects. For height, pedigree studies consistently estimate $h^2$ around 0.80, meaning that 80% of the variation in height across individuals in the studied population can be attributed to genetic differences [@visscher_heritability_2008]. For schizophrenia, twin studies estimate $h^2$ around 0.80 as well. For body mass index, estimates cluster around 0.40 to 0.70 depending on the population and study design.

These high heritability estimates established that genetics substantially influences most traits of biomedical interest, motivating the search for specific causal variants. If 80% of height variation is genetic, then genetic variants collectively must explain most of that variation. Finding those variants became the goal of GWAS.

### SNP-Heritability and the Missing Heritability Problem

GWAS delivered a puzzle. For height, even the largest studies with hundreds of significant hits explained only a fraction of the heritability estimated from family studies. Early GWAS collectively explained perhaps 5% of height variance when pedigree studies suggested 80% should be genetic. This gap, termed **missing heritability**, sparked intense debate about where the remaining genetic variance might hide [@manolio_finding_2009].

The concept of **SNP-heritability** ($h^2_{SNP}$) emerged to address this puzzle. Rather than asking how much variance is explained by genome-wide significant variants, researchers asked how much variance is explained by all common SNPs on genotyping arrays, including those that fail to reach significance. Methods such as GCTA-GREML estimate this quantity by modeling phenotypic similarity as a function of genetic similarity computed across all SNPs [@yang_common_2010]. For height, SNP-heritability estimates reach approximately 0.50 to 0.60, substantially higher than variance explained by significant hits alone but still below pedigree estimates.

The gap between pedigree heritability (0.80 for height) and SNP-heritability (0.50 to 0.60) reflects genetic variation not captured by common SNPs on genotyping arrays. Rare variants, structural variants, and variants not in linkage disequilibrium with array content all contribute to the difference. The gap between SNP-heritability and variance explained by significant hits reflects the polygenic architecture of complex traits: thousands of variants each contribute effects too small to reach genome-wide significance individually, yet they collectively explain substantial variance.

### Implications for GWAS and Polygenic Scores

The heritability landscape carries practical implications for what GWAS and polygenic scores can achieve. SNP-heritability sets an upper bound on the predictive accuracy of polygenic scores built from common variants: a PGS cannot explain more variance than is captured by the SNPs it uses. For height, with SNP-heritability around 0.50, the best possible common-variant PGS could explain at most half of phenotypic variance. Current PGS for height in European-ancestry populations approach this bound, explaining roughly 25% of variance with continued gains as sample sizes grow [@yengo_meta-analysis_2022].

For diseases, the relationship between heritability and predictive accuracy is more complex. A highly heritable disease might have low predictive accuracy if the causal variants are rare, if gene-environment interactions dominate, or if the heritability is distributed across thousands of variants each with tiny effects. Conversely, a moderately heritable disease with a few common variants of large effect might be more predictable. The architecture of genetic effects matters as much as total heritability.

Missing heritability also motivates the integration of rare variant analysis with GWAS of common variants. Whole-genome sequencing studies can capture rare variants invisible to genotyping arrays, potentially recovering some of the genetic variance missing from common-variant analyses. Foundation models trained on sequence data, rather than genotype arrays, may ultimately capture genetic effects across the full allele frequency spectrum, a possibility we explore in @sec-vep-fm.

## Linkage Disequilibrium and the Association-Causation Gap

GWAS test variants one at a time, but the genome is not inherited one variant at a time. Nearby variants travel together on haplotypes and are co-inherited across generations except when recombination separates them. This correlation structure, known as **linkage disequilibrium (LD)**, is both essential to GWAS power and the source of their fundamental interpretive limitation. Without LD, GWAS would need to genotype every variant in the genome directly; with LD, statistical association cannot distinguish cause from correlation.

When a GWAS identifies a significant association at variant $j$, three possibilities exist. The variant itself may be causal, directly influencing the phenotype through some molecular mechanism. Alternatively, variant $j$ may simply be correlated with a nearby causal variant $k$ due to LD, with the association signal reflecting this correlation rather than direct causation. In complex regions, multiple causal variants may exist, and the observed association pattern reflects their joint effects filtered through the local LD structure. Distinguishing these scenarios from GWAS summary statistics alone is often impossible. The causal variant and its tag look identical in the association data, yet only the causal variant represents a valid drug target or mechanistic insight.

### The Structure of Linkage Disequilibrium

Understanding why LD creates interpretive ambiguity requires understanding how LD arises and decays. Recombination during meiosis shuffles genetic material between parental chromosomes. Over many generations, recombination breaks down long-range correlations between variants while preserving short-range structure. The result is a mosaic pattern: regions of high LD (haplotype blocks) where many variants are strongly correlated, interspersed with recombination hotspots where LD decays rapidly.

The squared correlation coefficient $r^2$ quantifies LD between pairs of variants. When $r^2$ approaches 1, the two variants are nearly always observed together on the same haplotypes; when $r^2$ approaches 0, they segregate independently. From a GWAS perspective, if a causal variant $k$ has strong association with the phenotype and variant $j$ is in high LD with $k$ (high $r^2$), then variant $j$ will also show strong association even if it has no direct causal role. The statistical signal propagates through LD, creating ambiguity about which variant is actually functional.

LD patterns vary across populations because demographic history shapes which haplotypes persist and at what frequencies. Founder effects concentrate certain haplotypes; bottlenecks reduce diversity; admixture creates novel combinations; population expansion allows rare haplotypes to drift to higher frequency. A variant that tags a causal allele effectively in one population may be a poor proxy in another where different recombination history has decoupled the correlation. This population-specificity of LD structure is one reason polygenic scores fail to transfer across ancestries, a problem we examine in detail below.

### Causal Variants, Tag Variants, and GWAS Catalogs

The distinction between causal and tag variants determines whether GWAS results can translate into biological insight or clinical action. A **causal variant** directly influences the phenotype, whether by altering protein sequence, disrupting transcription factor binding, affecting splicing, or modifying chromatin state. A **tag variant** is merely correlated with a causal variant through LD, serving as a statistical proxy without direct functional consequence. The distinction is invisible to GWAS: both produce association signals, and in the presence of strong LD, those signals are statistically indistinguishable.

GWAS catalogs therefore report associated loci, not causal variants. The "lead SNP" at each locus (the variant with the smallest p-value) is often a tag rather than the causal variant, particularly when the causal variant is rare, poorly genotyped, or not present on the array. Even when a locus is robustly associated, dozens or hundreds of correlated variants may be statistically indistinguishable from the lead SNP.

This limitation has concrete practical consequences. Drug development requires identifying causal genes and mechanisms, not just associated regions; targeting a tag variant or the wrong gene wastes years of development effort. Clinical variant interpretation needs to distinguish functional mutations from neutral passengers; reporting a tag as pathogenic misleads patients and clinicians. Polygenic scores built on tag SNPs may lose power when applied to populations with different LD patterns, since the tag-causal correlation that made the tag useful may not hold. The gap between association and causation motivates the fine-mapping approaches we consider next.

## Fine-Mapping: From Loci to Causal Variants

A pharmaceutical company evaluating a GWAS hit for drug development faces a concrete problem: the associated locus spans 500 kilobases, contains 200 correlated variants, and overlaps three genes. Which gene should they target? Which variant drives the association? Investing hundreds of millions of dollars in a program targeting the wrong gene would be catastrophic, yet GWAS summary statistics alone cannot resolve the ambiguity. **Fine-mapping** attempts to address this gap, moving from "this region is associated" to "these specific variants are most likely causal" by exploiting the joint behavior of correlated variants under explicit statistical models.

### The Statistical Framework

The core insight of fine-mapping is that while multiple variants may show similar marginal association statistics, their joint behavior under a model that accounts for LD can discriminate among them. A causal variant should show association beyond what can be explained by LD with its neighbors; a tag variant should not. This distinction, invisible when variants are tested one at a time, becomes apparent when their correlations are modeled jointly.

Bayesian fine-mapping methods approach the problem by specifying a prior distribution over which variants in a region might be causal, then computing posterior probabilities given the observed association statistics and local LD structure. The key outputs are **posterior inclusion probabilities (PIPs)**, which estimate the probability that each variant is among the causal set, and **credible sets**, which are minimal sets of variants that contain the true causal variant(s) with specified probability (commonly 95%).

The procedure typically proceeds as follows. First, researchers define a region around an index SNP, often all variants within 1 megabase. Second, they specify a prior: perhaps at most $K$ variants in the region are causal, and causal effect sizes follow some distribution (often Gaussian). Third, the observed marginal association statistics (effect sizes and standard errors) together with an LD matrix (correlations among variants) inform the likelihood of the data under each possible configuration of causal variants. Fourth, summation over configurations yields the marginal PIP for each variant.

Variants with high PIPs (above 0.5 or 0.9) are strong candidates for functional follow-up. Credible sets that contain few variants are more actionable than those containing dozens. The width of credible sets reflects both the strength of the association signal and the local LD structure: tight LD means many variants remain plausible even with strong statistical evidence. In some regions, fine-mapping narrows thousands of candidates to a handful; in others, the ambiguity remains irreducible given available data.

### Leveraging Functional Annotations

Statistical fine-mapping alone cannot resolve regions where multiple variants are in near-perfect LD; the data simply cannot distinguish variants that are always co-inherited. Functional annotations offer a path forward by incorporating biological plausibility: not all genomic positions are equally likely to harbor causal variants. Variants disrupting coding sequences, altering transcription factor binding sites, or falling within active enhancers carry higher prior probability of functional relevance than variants in unannotated intergenic regions.

Annotation-informed approaches update fine-mapping priors based on these external data sources. Variants in coding regions, promoters, enhancers, or regions of evolutionary constraint may be assigned higher prior probability of causality. Integration with chromatin accessibility data (from ATAC-seq or DNase-seq), transcription factor binding maps (from ChIP-seq), or expression quantitative trait loci (eQTL) can further prioritize variants with plausible regulatory mechanisms.

The functional scores introduced in @sec-data provide systematic frameworks for quantifying variant-level annotations. Scores such as CADD, DANN, and Eigen integrate diverse genomic features into single numbers that can inform fine-mapping priors. More recently, foundation models trained on genomic sequence have produced variant effect predictions that capture functional information beyond what traditional annotations provide (@sec-vep-fm). These scores transform fine-mapping from a purely statistical exercise into an integrative analysis that combines association evidence with mechanistic plausibility.

Large-scale resources now link GWAS summary statistics, fine-mapping results, and functional genomic annotations across hundreds of traits and thousands of loci [@mountjoy_open_2021]. These datasets enable systematic identification of variants that are both statistically prioritized and functionally plausible, though the biological validation required to confirm causal mechanisms remains laborious and is completed for only a small fraction of associated loci.

### Multi-Ancestry Fine-Mapping

Single-ancestry fine-mapping encounters a fundamental resolution limit: when variants are in tight LD within the study population, no amount of statistical sophistication can distinguish them. Multi-ancestry approaches break through this limit by exploiting the population-specificity of LD structure. A variant in tight LD with twenty neighbors in Europeans may have only three correlated variants in African-ancestry populations, where shorter LD blocks (reflecting larger historical effective population size) provide greater resolution.

Joint fine-mapping across ancestries leverages these differences systematically [@kichaev_improved_2017]. When a variant remains strongly associated across populations despite different local LD structure, confidence in its causal role increases. The logic is straightforward: a true causal variant should show consistent association regardless of which other variants happen to be correlated with it in any particular population. A tag variant, by contrast, may appear associated in one population (where it correlates with the causal variant) but not in another (where that correlation is absent).

Multi-ancestry approaches are increasingly important as large biobanks expand to include diverse populations, though they require careful attention to potential effect size heterogeneity across populations. Gene-environment interactions or genetic background effects could cause genuine differences in variant effects, complicating the assumption that causal variants have consistent effects worldwide.

## Polygenic Score Construction

A 35-year-old woman with a family history of breast cancer asks her physician whether she should begin mammography screening earlier than guidelines recommend. Traditional risk models incorporate family history, age, and reproductive factors, but cannot capture the cumulative effect of thousands of common variants, each conferring small increases in risk, that together may substantially elevate her probability of disease. **Polygenic scores** address this gap by aggregating variant effects across the genome into a single number:

$$
\text{PGS}_i = \sum_{j} w_j g_{ij}
$$

The weight $w_j$ reflects the estimated effect of variant $j$, and $g_{ij}$ is the genotype dosage for individual $i$. The simplest approach uses GWAS effect size estimates directly as weights; more sophisticated methods adjust for LD, apply shrinkage, or incorporate fine-mapping information. The clinical promise is substantial: for diseases with significant genetic components, polygenic scores can identify individuals at elevated risk years or decades before disease onset, potentially enabling targeted screening or prevention.

::: {.callout-note}
### Terminology: PGS versus PRS
The literature uses overlapping terminology. **Polygenic risk score (PRS)** is common in clinical contexts, emphasizing disease risk prediction. **Polygenic score (PGS)** is more general, encompassing both disease and quantitative trait prediction. **Genomic risk score** and related terms also appear, often interchangeably. This book uses PGS as the default, adding "risk" when specifically discussing disease outcomes. Methodological overviews provide detailed guidance on construction and evaluation [@choi_tutorial_2020].
:::

### Clumping and Thresholding

The challenge of constructing a useful polygenic score is not mathematical but statistical: GWAS provide noisy estimates of millions of effects, many of which are correlated through LD, and naive summation produces scores dominated by noise rather than signal. **Clumping and thresholding (C+T)** represents the simplest solution: reduce both the noise and the correlation by aggressive filtering, accepting substantial information loss in exchange for robustness.

The procedure involves three steps. First, clumping: rank variants by p-value, then iteratively select the most significant variant and remove all variants within a specified window (typically 250 kb) that are in LD above a threshold (typically $r^2 > 0.1$). This yields a set of approximately independent index variants. Second, thresholding: apply a p-value cutoff and retain only variants below this threshold. Third, weighting: set $w_j$ equal to the GWAS effect size estimate for retained variants, and zero otherwise.

The hyperparameters (LD window, $r^2$ threshold, p-value threshold) are typically chosen by grid search to maximize predictive performance in a held-out validation set. This tuning introduces overfitting risk, particularly in small samples or when the validation population differs from the eventual deployment population.

C+T is transparent and computationally simple, but it discards substantial information. Most variants are excluded, LD is handled only through coarse pruning, and variants with modest p-values that collectively explain meaningful variance may be entirely omitted. For highly polygenic traits where thousands of variants each contribute small effects, this information loss substantially degrades prediction accuracy. The method treats LD as a problem to be eliminated rather than a correlation structure to be modeled, an approach that sacrifices power for simplicity.

### LD-Aware Bayesian Methods

The information discarded by C+T is not random noise; it contains genuine signal about genetic effects distributed across correlated variants. Rather than pruning away this structure, a more principled approach models the joint distribution of effect sizes explicitly, treating the true effects $\beta = (\beta_1, \ldots, \beta_M)$ as random variables drawn from a prior distribution. Given GWAS summary statistics and an LD reference panel, these methods infer posterior mean effect sizes that serve as PGS weights. The key insight is that LD becomes information rather than nuisance: correlated variants constrain each other's likely effects, improving estimation for all.

LDpred assumes that a fraction $p$ of variants have nonzero effects drawn from a Gaussian distribution, while the remainder have zero effect [@vilhjalmsson_modeling_2015]. The method uses GWAS summary statistics and LD from a reference panel (computed from a subset of individuals or external dataset matching the target ancestry) to compute approximate posterior effect sizes. These posteriors shrink noisy estimates toward zero, borrow strength across correlated variants, and generally outperform C+T when properly tuned.

PRS-CS extends this framework by placing a continuous shrinkage prior on effect sizes, which better accommodates the highly polygenic architecture of complex traits and reduces sensitivity to the sparsity hyperparameter [@ge_polygenic_2019]. The continuous prior assigns most variants small but nonzero effects rather than forcing a binary causal/non-causal distinction. The method has shown strong performance across a range of traits and ancestries, though like all methods it requires an LD reference that reasonably matches the target population.

Related approaches (lassosum, SBayesR, and others) use different priors or optimization strategies but share the core insight: jointly modeling effect sizes under LD yields better predictions than pruning LD away. Performance differences among methods are often modest when each is well-tuned, and the choice may depend on computational resources, availability of validation data, and specific trait architecture.

### Fine-Mapping-Informed Scores

Polygenic scores built on tag SNPs face a fundamental portability problem: the tag-causal correlation that justified including a variant may not hold in populations with different LD structure. Fine-mapping outputs, particularly posterior inclusion probabilities, offer a potential solution by identifying variants more likely to be causal. Causal variants should remain predictive regardless of population-specific LD patterns, since their effects are direct rather than mediated through correlation.

Two strategies incorporate fine-mapping information into PGS construction. Selection approaches retain only variants above a PIP threshold (typically 0.1 or 0.5), focusing the score on high-confidence causal candidates. Weighting approaches modulate each variant's contribution by its PIP, downweighting likely tags while preserving information from variants with intermediate evidence.

Fine-mapping-informed approaches aim to concentrate weight on variants that are biologically meaningful rather than merely statistically associated. In principle, this should improve cross-ancestry transferability since causal variants remain causal regardless of population-specific LD patterns. In practice, gains depend on fine-mapping resolution, which is limited in regions of tight LD. The approaches remain an active area of methodological development, with potential for substantial improvement as multi-ancestry fine-mapping resources expand.

## Interpreting Polygenic Scores

A polygenic score is a number, but numbers do not make clinical decisions. A patient told they are in the 95th percentile of genetic risk may interpret this as near-certain disease development, while a physician may recognize it as modest risk elevation insufficient to change management. Converting a score into actionable information requires understanding what it represents, how it relates to disease risk or trait values, and where its interpretation breaks down. Miscommunication at this stage can transform a useful risk stratification tool into a source of inappropriate anxiety or false reassurance.

### Relative Risk and Percentiles

The most immediate clinical question about a high polygenic score is: how much does it increase risk? Polygenic scores are most naturally interpreted in relative terms by fitting a logistic regression in a validation cohort:

$$
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \theta \cdot \text{PGS}_i + \eta^\top z_i
$$

where $z_i$ contains covariates and $\theta$ captures the effect of the PGS. After standardizing the score to unit variance, $\exp(\theta)$ gives the odds ratio per standard deviation of the PGS. This metric allows statements such as "individuals one standard deviation above the mean have 1.5-fold higher odds of disease."

Percentile-based communication is common in clinical contexts. The risk for individuals in the top 1% or 5% of the PGS distribution can be compared to those near the median or in the bottom percentiles. For some conditions, individuals in the top percentiles have risk comparable to or exceeding that conferred by single high-penetrance mutations: the top 8% of the coronary artery disease PGS distribution has risk equivalent to familial hypercholesterolemia carriers, and the top 1% of the breast cancer PGS distribution has lifetime risk approaching that of *BRCA2* mutation carriers [@khera_genome-wide_2018; @mavaddat_polygenic_2019]. This finding makes polygenic scores potentially relevant for clinical risk stratification, though the appropriate thresholds and clinical actions remain subjects of ongoing research and debate.

### Absolute Risk

A physician cannot act on relative risk alone; clinical decisions require knowing the probability that this specific patient will develop disease over a specified time horizon. Relative risk statements can mislead when baseline risk varies substantially. A 1.5-fold increase in odds for a disease with 1% baseline risk means absolute risk rises from 1% to roughly 1.5%; the same relative increase for a disease with 20% baseline risk means absolute risk rises from 20% to roughly 26%. A patient told they have "50% higher risk" may react very differently depending on whether baseline risk is low or high.

Converting PGS to absolute risk requires combining the score with baseline incidence rates, which vary by age, sex, and other factors. The hazard ratio per standard deviation of PGS, combined with age-specific incidence curves from population registries, can yield personalized risk trajectories. Such calculations demand careful attention to calibration: the model must produce well-calibrated probabilities in the population where it will be deployed, not just the population where it was trained. A model calibrated in UK Biobank may systematically over- or under-estimate risk when applied to a U.S. clinical population with different baseline incidence rates or healthcare practices. Clinical deployment of PGS is addressed in detail in @sec-clinical-risk.

### Explained Variance and Discrimination

Population-level performance metrics determine whether a polygenic score has any utility, but they can mask the substantial uncertainty that remains for any individual patient. For quantitative traits, the squared correlation between PGS and phenotype ($R^2$) provides a direct measure of explanatory power. Height PGS now explain roughly 25% of phenotypic variance in European-ancestry populations, approaching the theoretical maximum given current sample sizes and the heritability of the trait [@yengo_meta-analysis_2022]. For binary traits, the $R^2$ on the liability scale (the underlying continuous risk) is more interpretable than the observed-scale $R^2$, which depends on disease prevalence.

Area under the receiver operating characteristic curve (AUC) measures discrimination: the probability that a randomly selected case has a higher PGS than a randomly selected control. AUC values of 0.5 indicate no discrimination (random guessing); values approaching 1.0 indicate near-perfect separation. For most complex diseases, PGS achieve AUC values in the 0.55 to 0.70 range when used alone, with incremental gains when combined with traditional risk factors [@torkamani_polygenic_2018; @lambert_polygenic_2019]. These values reflect meaningful stratification at the population level but limited utility for individual prediction.

Even a PGS that explains 10% of trait variance leaves 90% unexplained by factors genetic and environmental. High-risk individuals by PGS may never develop disease; low-risk individuals may be affected. Polygenic scores provide probabilistic risk stratification, not deterministic prediction. This distinction is critical for clinical communication and for setting appropriate expectations about what genomic risk information can and cannot offer.

## Ancestry, Portability, and Fairness

The vast majority of GWAS participants have been of European ancestry: as of 2019, approximately 78% of participants were European despite Europeans comprising roughly 16% of the global population [@martin_clinical_2019]. This historical imbalance has profound consequences for who benefits from polygenic scores and who may be harmed by their limitations. A technology that works well for some populations and poorly for others is not merely incomplete; deployed without appropriate caution, it risks widening existing health disparities rather than narrowing them.

### The Portability Problem

Polygenic scores derived from European-ancestry GWAS show markedly reduced performance in other populations. African-ancestry individuals typically experience 40% to 75% reductions in prediction accuracy compared to European-ancestry individuals, even for the same trait measured in the same study [@duncan_analysis_2019; @martin_clinical_2019]. The pattern holds across traits and across methods, though the magnitude varies with genetic architecture and the degree of shared causal variants.

Several factors contribute to this portability failure. LD structure differs across populations: tag SNPs that effectively proxy causal variants in Europeans may be poor proxies in populations with different recombination history. Allele frequencies differ: variants common in one population may be rare or absent in another. Effect sizes may genuinely differ across populations due to gene-environment interactions or genetic background effects. And GWAS in smaller non-European samples have less power to detect associations, yielding noisier effect estimates that further degrade prediction.

Multi-ancestry GWAS and methods designed to leverage diverse training data offer partial solutions. Including multiple ancestries in discovery improves transferability, and methods that explicitly model ancestry-specific LD or effect sizes can enhance performance [@marquez-luna_multiethnic_2017]. Yet even state-of-the-art approaches do not fully close the gap, and substantial research is needed before PGS perform equitably across populations.

### Fairness and Health Equity

The performance gap across ancestries is not merely a technical nuisance; it raises fundamental questions about fairness in precision medicine. If genomic models work primarily for individuals of European ancestry, deploying these models in diverse clinical populations risks exacerbating existing health disparities rather than ameliorating them. The communities historically excluded from genetic research would continue to receive inferior genomic medicine, now encoded in algorithmic form.

Consider a scenario where PGS are used for risk-stratified screening. If the score identifies high-risk individuals more accurately in Europeans than in other groups, Europeans receive more targeted and efficient screening while others receive either under-screening (if falsely classified as low risk) or over-screening (if falsely classified as high risk). The benefits of precision medicine accrue disproportionately to those already overrepresented in research, while the costs of miscalibration fall on those historically excluded.

These challenges extend beyond PGS to every genomic model in this book. Foundation models can learn to exploit ancestry signals as shortcuts, achieving high benchmark performance while performing poorly on underrepresented groups. Aggregate performance metrics mask inequities across populations. Deployment in diverse clinical settings requires explicit evaluation of performance stratified by ancestry, along with transparent reporting of limitations and appropriate caution in populations where validation is limited. We examine these issues comprehensively in @sec-confounding.

## From Association to Mechanism

GWAS and polygenic scores have delivered thousands of robust trait associations, clinically useful risk stratification for some conditions, and fundamental insights into the polygenic architecture of complex phenotypes. They have also exposed a persistent gap between statistical association and biological understanding. Most GWAS hits lie in noncoding regions, often within enhancers, promoters, or other regulatory elements. The variant is associated; the mechanism is obscure. Fine-mapping narrows the list of candidates but rarely identifies a single causal nucleotide with confidence. Even when a variant is prioritized, the path from sequence change to molecular consequence to cellular phenotype to disease remains opaque.

This mechanistic gap limits translation in concrete ways. Drug development requires actionable targets, not associated regions. Clinical variant interpretation needs to explain why a variant matters, not just that it correlates with disease. Polygenic scores stratify population risk but offer little guidance on individual intervention. The models developed in subsequent chapters address this gap through complementary strategies: regulatory sequence models that predict how variants alter transcription factor binding and chromatin accessibility (@sec-regulatory), variant effect predictors that assess functional impact at nucleotide resolution (@sec-vep-fm), and multi-omics integration approaches that connect genetic variation to intermediate molecular phenotypes (@sec-multi-omics).

The goal is not to replace statistical genetics but to build on it. Association provides the map of where trait-relevant variation resides; mechanistic modeling attempts to explain how that variation produces its effects. The combination of statistical association and mechanistic interpretation offers the most promising path toward genomic medicine that is both predictive and understood.