# Pathogenic Variant Discovery  {#sec-variants}

Clinical genetics ultimately cares about specific variants and genes: which change in this patient, tumor, or cohort is plausibly causal, and what should we do about it? Earlier chapters focused on learning rich sequence representations and predicting molecular readouts such as chromatin accessibility, splicing, and protein stability (@sec-dna; @sec-rna; @sec-vep). This chapter shifts the emphasis from prediction to discovery workflows: how do we move from raw variant calls to ranked lists of variants and genes for follow-up, and where do genomic foundation models (GFMs) make that process more powerful?

The central question is: given a huge space of possible variants, which ones should we believe and which should we act on? In practice, the answer emerges from layered pipelines that combine sequencing, population genetics, variant effect prediction, aggregation, network context, and experimental validation. GFMs appear at multiple stages: as variant effect predictors, turning raw sequence variants into functional scores (@sec-vep); as feature providers for gene- and pathway-level models, including deep set architectures for rare variant aggregation [@clarke_deeprvat_2024]; as node and edge embeddings in molecular and clinical knowledge graphs [@chandak_primekg_2023]; and as oracles in closed-loop discovery, guiding which variants to perturb or validate next.

::: {.callout-note icon=false}
## Figure: Variant Prioritization Pipeline

A flowchart showing the progression from sequencing through variant calling and quality control, followed by population and technical filters (allele frequency, coverage, artifacts), then GFM-based variant effect prediction for both coding and regulatory variants. The pipeline continues through aggregation to gene/element level (with a DeepRVAT-style deep set block shown), graph-based integration using knowledge graphs like PrimeKG, and finally produces a ranked variant/gene list for expert review.
:::

This chapter walks through these roles, moving from locus-level variant prioritization to association testing, rare disease diagnosis, graph-based gene ranking, and closed-loop discovery systems.

---

## From Variant Effect Prediction to Prioritization

Variant effect predictors (VEPs) map a candidate variant to one or more predicted consequences: changes in transcription factor binding, chromatin accessibility, RNA abundance, splicing, protein stability, and so on (@sec-vep). GFMs greatly expand the coverage and resolution of these predictions. AlphaMissense provides proteome-wide missense effect estimates [@cheng_alphamissense_2023], while long-range regulatory models like Enformer and Borzoi predict complex cis-regulatory outputs from sequence alone [@avsec_enformer_2021; @linder_borzoi_2025].

However, clinical and discovery tasks rarely operate on individual variant-assay pairs. Instead, they ask questions like: Which variants in this gene are likely pathogenic under a dominant model? Which regulatory elements near this locus harbor functional variants that could explain the GWAS signal? Which genes or pathways are most likely to drive this tumor subtype? Answering these questions requires several steps that transform raw VEP scores into actionable rankings.

### Contextualizing Variant Scores

A raw variant effect score has very different implications depending on its genetic and clinical context. For allele frequency and constraint, rare variants (especially ultra-rare singletons in gnomAD) with high-effect predictions are much more suspicious in severe early-onset disease than common variants with similar VEP scores. Gene-level constraint metrics such as intolerance to loss-of-function or missense variation modulate how much weight we assign to damaging predictions in that gene. For inheritance models, under a dominant model (such as de novo pathogenic variants in trios), a single predicted-damaging allele may be enough to warrant attention. Under a recessive model, we seek two hits (compound heterozygous or homozygous) with consistent predicted effects. For X-linked or mitochondrial inheritance, dosage and sex-specific considerations further influence interpretation.

Clinical phenotype and penetrance also matter. The same variant effect score is interpreted very differently in a patient with a classic, highly specific phenotype versus a heterogeneous, nonspecific presentation. In complex traits, penetrance is partial and polygenic; VEP scores become one contributor among many rather than a binary decision rule. GFMs do not solve these contextual issues by themselves. Instead, they provide better candidate scores that must be embedded in downstream models explicitly accounting for frequency, inheritance, and phenotype.

### Aggregating to Genes and Elements

Most discovery tasks operate at the gene, element, or locus level rather than on individual variants. This requires aggregation. Simple aggregation schemes include max pooling (taking the maximum predicted effect among variants in a gene or element, representing the single worst variant), sum or average pooling (combining effects additively, sometimes weighting by allele frequency or zygosity), and threshold-based counts (counting how many variants exceed a pathogenicity or functional effect threshold).

::: {.callout-note icon=false}
## Figure: Deep Set Architecture for Rare Variant Aggregation

A schematic showing a DeepRVAT-style architecture where multiple rare variants in one gene, each with VEP and GFM-derived features, are processed through a permutation-invariant pooling layer (deep set or attention mechanism) to produce a single gene-level risk score.
:::

More sophisticated approaches use deep set and attention pooling. DeepRVAT [@clarke_deeprvat_2024] and related methods treat the set of variants in a gene or region as a permutation-invariant set. Each variant is encoded using VEP and GFM-derived features, a shared network transforms each variant representation, pooling (sum, mean, or attention) aggregates to a gene-level embedding, and a final network outputs a gene-level risk or association score. This architecture naturally handles variable numbers of variants and allows the model to learn which variants in the set matter most.

For element-centric aggregation focused on regulatory elements such as enhancers, promoters, and splice junctions, we may first aggregate variants within each element, then propagate scores to target genes using contact maps, enhancer-promoter links, or model-predicted regulatory influence (@sec-epi; @sec-networks). Long-range GFMs such as Enformer and Borzoi can directly predict the change in gene expression induced by combinations of variants in cis [@avsec_enformer_2021; @linder_borzoi_2025], effectively learning aggregation and regulatory wiring jointly.

### Combining VEP with Orthogonal Evidence

Variant effect prediction is rarely used in isolation in modern discovery pipelines. Instead, VEP and GFM scores are combined with orthogonal evidence. Population and familial information includes segregation in pedigrees (de novo status, co-segregation with disease), co-occurrence with other pathogenic variants, and ancestry-matched frequency spectra. Functional genomics and expression data provide co-localization with expression or splicing QTLs, overlap with chromatin accessibility, histone modifications, or 3D genome interactions (@sec-epi), and perturbation data such as CRISPR screens and massively parallel reporter assays. Clinical and literature priors draw from known disease genes and gene-phenotype associations (such as ClinVar and OMIM) and evidence from model organisms or prior case reports.

GFMs enhance this step by providing richer features such as expression changes across cell types and protein embeddings capturing domain-level context that can be fed into joint models. Knowledge graphs such as PrimeKG [@chandak_primekg_2023] provide a natural scaffold for integrating these heterogeneous signals.

### Calibration and Interpretability

In prioritization tasks, ranking performance often matters more than perfectly calibrated probabilities: we mostly care that truly pathogenic variants or genes are near the top. Nevertheless, clinicians need approximate probabilities or evidence levels to communicate risk and decide on testing or treatment. Miscalibration can lead to overconfidence in variants with unstable predictions across ancestries or genetic backgrounds (@sec-confound).

Calibration techniques such as isotonic regression and temperature scaling can be applied to gene- or variant-level scores, often stratified by variant class (missense versus splice versus regulatory). Interpretability matters too. Deep set models can provide attention weights over variants within a gene, highlighting which variants drive the gene-level score. Graph-based models can expose path-level importance, showing which disease-gene-pathway edges contributed most strongly.

In short, GFMs provide high-resolution features and predictions; prioritization frameworks turn those into calibrated, interpretable rankings that fit within clinical and experimental constraints.

---

## Rare Variant Association and Complex Trait Discovery

In the GWAS paradigm discussed in @sec-pgs, common variants are tested one at a time, often yielding loci where the actual causal variant and gene are ambiguous. Rare variant association studies (RVAS) flip the perspective: consider many rare variants within a gene or region, aggregate them into a single test statistic per gene or region, and test whether carriers of "damaging" variants are enriched among cases versus controls. Foundation models naturally plug into RVAS because they provide nuanced estimates of how damaging each variant might be.

### Burden and Kernel Tests with GFM-Derived Weights

Classical rare variant methods include collapsing or burden tests, which collapse all "qualifying" variants in a gene into a single indicator or count (for example, does the individual carry at least one damaging variant?). This requires hand-crafted criteria for what counts as damaging (such as predicted loss-of-function or highly deleterious missense). SKAT and related kernel tests model the combined effect of multiple variants using a kernel over genotypes and allow variants to have different directions and magnitudes of effect.

::: {.callout-note icon=false}
## Table: GFM-Based Variant Effect Prediction Tools

A comparison table with columns for Method, Variant Type(s), Training Signals, Output, and Key Strengths. Example rows include AlphaMissense [@cheng_alphamissense_2023] for missense variants, GPN-MSA [@benegas_gpn-msa_2024] for genome-wide variants, Evo 2 [@brixi_evo_2025] for long-range genomic contexts, and AlphaGenome [@avsec_alphagenome_2025] for multi-omic regulatory predictions.
:::

GFMs can supply data-driven weights and qualifiers. They enable using missense or regulatory effect scores as continuous weights, rather than hard thresholds. They allow downweighting variants whose predicted effects are modest or uncertain. They help focus burden tests on variants predicted to perturb regulatory programs in relevant tissues or cell types (such as enhancers active in a disease-relevant cell population). This improves power when many observed variants are benign or mildly deleterious, and it helps harmonize analyses across cohorts with different sequencing depths.

### Deep Set and Multiple-Instance Models

DeepRVAT [@clarke_deeprvat_2024] extends traditional RVAS by learning a trait-agnostic gene impairment score from variant annotations using deep set networks. GFMs are a natural provider of such annotations. The input for each individual and gene is a set of variants with GFM-derived features (such as predicted expression changes, splicing disruptions, and protein stability shifts). The model applies a permutation-invariant network to the set, pooling to a gene-level embedding. The output is a gene-level liability or risk score that can be tested against case-control status.

Multiple-instance learning frameworks such as MIFM [@rakowski_mifm_2025] push this further by grouping variants into loci or instances and learning to predict which groups harbor causal regulatory variants across many GWAS signals. Here again, regulatory GFMs supply the base features that represent each candidate variant's mechanistic plausibility.

### Biobank-Scale Studies and Ancestry Considerations

At biobank scale, rare variant association must grapple with heterogeneous ancestries and allele frequencies. Pathogenic variants may be common in one ancestry but absent in another. GFMs trained predominantly on one ancestry may misestimate effect sizes in others (@sec-confound). Coverage and technical artifacts also matter: exome versus genome sequencing, differences in capture kits, and batch effects can induce spurious associations if not carefully modeled.

GFMs can help and hurt. They enable more precise filtering and weighting, increasing power. But if trained on biased data, they can systematically under- or over-score variants in underrepresented groups. Robust RVAS pipelines therefore treat GFM-derived scores as powerful, but fallible, covariates, and they rely on careful quality control, replication, and sensitivity analyses to ensure that discovered genes withstand scrutiny (@sec-eval; @sec-confound).

---

## Mendelian Disease Gene and Variant Discovery

For Mendelian disorders, the goal is often to explain the phenotype of one family or a small number of patients using rare, high-effect variants. Typical pipelines involve sequencing and variant calling (trio whole-exome or whole-genome sequencing with high-quality variant calls and strict filters for depth, quality, and artifacts), frequency and consequence filtering (removing variants too common in population datasets given the disease prevalence and mode of inheritance, focusing on predicted loss-of-function, damaging missense, canonical splice, and high-impact regulatory variants), and gene and phenotype filters (prioritizing genes with known or plausible links to the phenotype through expert-curated gene lists and HPO-based phenotype matching, cross-referencing ClinVar and other knowledge bases for previously reported variants).

GFMs integrate primarily at the variant and gene scoring stages. Missense and protein-level GFMs such as AlphaMissense [@cheng_alphamissense_2023] and related GFM-based VEPs provide proteome-wide estimates of missense pathogenicity. Incorporating these scores into Mendelian pipelines can de-prioritize previously misclassified variants of uncertain significance (VUS) and highlight novel missense changes with strong predicted impact in constrained genes.

Regulatory and splicing GFMs can flag variants likely to disrupt critical regulatory elements or splice junctions in noncoding or intronic regions. In disorders with tissue-specific manifestations, cell-type-resolved regulatory predictions are especially informative. For joint scoring of multi-hit genes, some Mendelian disorders involve multiple variants in the same gene (such as compound heterozygotes). Deep set-style aggregation can combine missense and regulatory predictions across hits to estimate the overall probability of gene disruption.

### Practical Considerations in Mendelian Settings

Despite the promise, several caveats are important. Sample size is small: individual families provide limited data, and overfitting to idiosyncratic variants is a risk. GFMs are mostly used as priors, not as stand-alone evidence of causality. Phenotypic heterogeneity is common: many Mendelian genes produce overlapping phenotypes, and relying on phenotype matching alone can mislead. Interpretations should integrate both phenotype and GFM-derived variant scores. Manual review remains essential: expert curation and multidisciplinary case conferences remain the gold standard. GFMs can triage large variant lists to a manageable subset for expert review rather than replacing human judgment.

---

## Graph-Based Prioritization of Disease Genes

Beyond individual genes, many discovery tasks require reasoning over networks and knowledge graphs: genes interact in pathways; diseases share molecular mechanisms; drugs target proteins that reside in these networks.

### Knowledge Graphs and GFM-Derived Features

Resources like PrimeKG [@chandak_primekg_2023] assemble multimodal knowledge graphs connecting genes and proteins, diseases and phenotypes, drugs, pathways, and molecular functions, and multi-omics signals (expression, methylation, and others).

::: {.callout-note icon=false}
## Figure: Knowledge Graph Visualization

A stylized representation of a PrimeKG-like graph structure showing gene nodes, disease associations, pathway membership, and drug-target interactions connected by edges representing protein-protein interactions, co-expression, regulatory relationships, and literature-derived associations. Highlights indicate where GFM-derived features populate node embeddings.
:::

GFMs enrich these graphs by providing node embeddings for genes and proteins derived from sequence, structure, or expression patterns; edge features representing interaction strengths or regulatory impacts predicted from sequence (such as variant-to-gene regulatory scores from Enformer-like models [@avsec_enformer_2021; @linder_borzoi_2025]); and text-derived embeddings for literature and clinical notes (@sec-foundation). Graph neural networks (GNNs) operating on these enriched graphs can then learn to propagate disease risk, predict novel disease-gene links, or identify candidate drug targets.

### Multi-Omics and Single-Cell Graph Models

Several methods illustrate how graph-based approaches and GFMs can be combined. MoGCN integrates multi-omics data into a graph convolutional network to classify cancer subtypes [@li_mogcn_2022]. CGMega uses an explainable GNN framework with attention to dissect cancer gene modules [@li_cgmega_2024]. GLUE performs multi-omics single-cell data integration and regulatory inference with a graph-linked embedding [@cao_glue_2022].

::: {.callout-note icon=false}
## Table: Graph-Based Gene Prioritization Methods

A comparison table with columns for Method, Graph Type, Input Features (including whether GFM-derived), Primary Task, and Example Applications. Rows include MoGCN [@li_mogcn_2022], CGMega [@li_cgmega_2024], GLUE [@cao_glue_2022], and classical network propagation methods (without GFMs).
:::

In each case, foundation models can provide initial embeddings for genes and cells (from protein or RNA GFMs) and supply variant- or region-level functional scores that feed into graph construction (such as edges between variants and genes, or between enhancers and promoters). For pathogenic variant discovery, these models allow us to move from "this variant looks damaging" to "this variant is embedded in a network of genes and pathways already implicated in this disease and points toward a specific mechanism or druggable module."

---

## Closed-Loop Discovery: Foundation Models, Perturbation, and Iteration

Traditional discovery pipelines are mostly one-shot: collect data, fit models, test hypotheses, publish results. Closed-loop systems instead iterate. They use existing data to train GFMs and downstream prioritization models, use these models to propose new variants, genes, or regions to investigate, perform targeted experiments (CRISPR perturbations, saturation mutagenesis, reporter assays) to validate or refute predictions, and feed new data back into the models, refining predictions.

::: {.callout-note icon=false}
## Figure: Closed-Loop Discovery Workflow

A cycle diagram showing: (1) data flowing into GFM training and fine-tuning, (2) GFM-based variant prioritization generating hypotheses, (3) CRISPR screens, MPRA assays, and tiling screens providing experimental validation, and (4) updated training data feeding back into model refinement, creating a continuous improvement cycle.
:::

GFMs are particularly well-suited to this loop because they can generalize from limited experimental data to large variant spaces, incorporate new labeled examples efficiently via fine-tuning (@sec-foundation), and provide uncertainty estimates or ensemble variability that can guide which variants are most informative to test next. For example, a regulatory GFM trained on MPRA data might propose noncoding variants with high predicted impact in a disease-relevant enhancer. A CRISPR tiling screen around that enhancer validates a subset of these variants. The new measurements are used to fine-tune the GFM, improving predictions near that locus and for similar regulatory architectures elsewhere in the genome. Over time, such closed loops can create self-improving discovery systems where model predictions and experimental design reinforce each other.

---

## Case Studies and Practical Considerations

### Case Study A: GFM-Enhanced Rare Disease Diagnosis

Consider a child with a severe neurodevelopmental disorder and no clear diagnosis after standard testing. Trio whole-genome sequencing identifies tens of thousands of variants. Filtering by quality, allele frequency, inheritance model, and gene panels reduces this to hundreds. Missense variants are scored by a proteome-wide GFM VEP [@cheng_alphamissense_2023]; noncoding variants in brain-specific enhancers are scored by a regulatory GFM [@avsec_enformer_2021]. A deep set model aggregates these variant scores at the gene level, emphasizing de novo and highly damaging variants in constrained genes [@clarke_deeprvat_2024].

The top-ranked gene has biallelic predicted-loss-of-function variants, strong expression in developing cortex, and prior evidence from model organisms and weakly similar human cases. This candidate moves into manual review, targeted functional assays, and potentially clinical reclassification of the variants from VUS to likely pathogenic. GFMs here serve as force multipliers: they compress a large candidate list into a small, interpretable set where human expertise is most effective.

### Case Study B: Noncoding Driver Discovery in Cancer

In cancer genomics, the goal is often to distinguish driver mutations (those conferring a selective advantage) from passenger mutations in noncoding regions. Whole-genome sequencing of tumors identifies many somatic variants in promoters, enhancers, and other regulatory elements. A regulatory GFM predicts the effect of each variant on chromatin accessibility and gene expression in the relevant cancer cell type. Variants are aggregated to elements and genes, weighted by predicted regulatory impact. Graph-based models such as CGMega [@li_cgmega_2024] integrate these scores into multi-omics modules linked to tumor subtypes or outcomes. Elements with high predicted impact and module centrality become candidates for CRISPR perturbation screens.

This approach prioritizes noncoding mutations not just by local effect, but by their predicted position in disease-relevant regulatory programs.

### Practical Pitfalls and Best Practices

Across these settings, several practical issues recur. Confounding and batch effects represent a persistent challenge: correlations between technical variables and phenotypes can bias association and prioritization (@sec-confound). GFMs trained on confounded data may internalize and propagate these biases. Ancestry and data shift matter because performance can differ markedly across ancestries or sequencing platforms. Models should always be evaluated and, where possible, recalibrated in the target population (@sec-eval).

Overreliance on single scores is dangerous. No single GFM score should be treated as definitive. Robust pipelines triangulate across multiple predictors, orthogonal functional data, and biological plausibility. Reproducibility and validation require that variant- and gene-level discoveries be replicated in independent cohorts when possible. Experimental validation, even at small scale (such as focused MPRA or CRISPR screens), dramatically increases confidence.

A pragmatic view is that GFMs are tools for triage and hypothesis generation, not automatic truth machines. Used wisely, they can save enormous time and highlight non-obvious candidates; used uncritically, they can amplify biases and generate false leads.

---

## Outlook: Towards End-to-End Discovery Systems

Biomedical discovery of pathogenic variants is gradually moving from manual, stepwise pipelines toward more integrated, end-to-end systems that combine GFMs for sequence, structure, and multi-omics data; aggregation and graph models for variant, gene, and pathway-level reasoning; causal inference tools for distinguishing correlation from mechanism; and active learning loops with high-throughput perturbation experiments.

In the long term, one can imagine systems that constantly ingest new genomic, phenotypic, and functional data, update GFM representations and prioritization models on a rolling basis, propose candidate variants and genes for follow-up in specific diseases, and integrate experimental results into improved models, closing the loop. Yet several challenges remain.

Robustness and generalization require that models handle data shifts, rare ancestries, and out-of-distribution phenotypes without brittle failure. Calibration and uncertainty matter because overconfident but wrong predictions can be more harmful than modest but honest uncertainty. Interpretability and oversight are essential: clinicians, genetic counselors, and experimentalists need mechanisms to interrogate why a variant was prioritized. Ethical and regulatory considerations become central as automated systems play a larger role in diagnostic and therapeutic decisions, raising questions of accountability, fairness, and transparency.

Subsequent chapters zoom out to drug discovery and target identification (@sec-drugs) and then to sequence and protein design (@sec-design), where many of the same principles reappear. Pathogenic variant discovery is an early and especially high-impact testbed for genomic foundation models. Success here will shape how these models are trusted and deployed across clinical medicine and biology.