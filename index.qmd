# Introduction {.unnumbered}

Genomics is in the middle of a quiet phase change.  
On one side, sequencing has become routine: biobanks now contain hundreds of thousands to millions of genomes, exomes, and transcriptomes, cataloging billions of variants across diverse populations. On the other side, deep learning and large-scale sequence modeling have transformed how we represent language, proteins, and now DNA itself [@zhou_deepsea_2015; @avsec_enformer_2021; @he_nucleic_2023; @benegas_gpn_2023; @benegas_genomic_2024; @zhang_scientific_2024].

This book is about the intersection: **genomic foundation models (GFMs)**—large, reusable models trained on genomic and related data that can be adapted to many downstream tasks, from variant interpretation to clinical risk prediction.

Rather than offering a general introduction to genomics or machine learning, the goal here is narrower and more opinionated:

> To give you a *conceptual and practical map* of how modern deep models for DNA, RNA, and proteins are built, what they actually learn, and how they can be used responsibly in research and clinical workflows.

The chapters that follow connect classic genomics pipelines, early deep regulatory models, sequence language models, and multi-omic GFMs into a single narrative arc.

---

## Why Genomic Foundation Models?

Traditional genomic modeling has typically been **task-specific**:

- A variant caller designed only to distinguish sequencing errors from real variants.
- A convolutional network trained to predict chromatin marks in a fixed panel of cell types.
- A risk score tuned for coronary artery disease in one ancestry group.

These models can work well within their narrow domains, but they do not usually offer a **single, reusable representation** of genomic variation.

By contrast, the “foundation model” paradigm—popularized in natural language and protein modeling—rests on three ideas:

1. **Scale**  
   Train large models on massive, heterogeneous datasets (e.g., whole genomes across species, genome-wide chromatin maps, population variation).

2. **Self-supervision**  
   Use objectives such as masked-token prediction, next-token modeling, or multi-task sequence-to-function training that do not require dense human labels [@benegas_gpn_2023; @zvyagin_genslms_2022; @nguyen_hyenadna_2023; @schiff_caduceus_2024].

3. **Reusability**  
   Treat the model as a *backbone*: for any new task, you probe or fine-tune it, rather than training from scratch. Variant effect prediction, enhancer classification, GWAS fine-mapping priors, and clinical risk models all become “adapters” on top of a shared representation [@benegas_genomic_2024; @fishman_gena-lm_2025].

In genomics, this paradigm is still evolving and remains empirically contested: some studies report large gains from pretraining, while others argue that simple baselines remain competitive [@vishniakov_genomic_2024]. This book deliberately leans into that tension. Throughout the chapters we will ask:

- **When** does large-scale pretraining actually help?
- **What** do these models learn about regulatory logic, evolution, and molecular mechanisms?
- **How** can they be plugged into real decision-making pipelines without overclaiming?

---

## What This Book Is (and Is Not)

This is **not**:

- A full genomics textbook.
- A comprehensive review of every published DNA or protein model.
- A mathematical introduction to deep learning from first principles.

Instead, it aims to be:

- A **roadmap** to the main families of models and data that matter for genomic foundation modeling today.
- A **bridge** between classic statistical genetics (GWAS, PRS, deleteriousness scores) and modern sequence models.
- A **practical guide** to how these models are built, trained, evaluated, and deployed—emphasizing caveats, confounders, and failure modes.

The assumed background is a working familiarity with basic genomics (variants, genes, regulatory elements) and some exposure to machine learning. Where needed, earlier chapters provide primers before moving into more advanced models.

---

## How the Book Is Organized

The book is organized into five parts. Each part can be read on its own, but they are designed to build on each other.

### Part I — Data & Pre-DL Methods (Chapters 1–4)

Part I lays the **genomic and statistical foundation** that later models rest on.

- **Chapter 1 — NGS & Variant Calling**  
  Introduces next-generation sequencing, alignment, and variant calling. It traces the evolution from hand-crafted statistical pipelines (e.g., GATK) to deep learning-based callers such as DeepVariant, highlighting how even “upstream” tools have transitioned to learned models.

- **Chapter 2 — PRS & GWAS Basics**  
  Covers the genome-wide association study (GWAS) paradigm, summary statistics, linkage disequilibrium, and polygenic risk scores (PRS). This chapter sets up the statistical framing of “variant-to-trait” associations that later GFMs must interface with.

- **Chapter 3 — Deleteriousness Scores**  
  Surveys conservation-based and machine learning-based variant pathogenicity scores (e.g., CADD and related tools), emphasizing how hand-crafted features and population constraint metrics are combined into early genome-wide predictors [@rentzsch_cadd_2019; @schubach_cadd_2024].

- **Chapter 4 — Foundational Genomics Data**  
  Reviews the major data resources that fuel modern models: ENCODE and Roadmap epigenomics for chromatin and accessibility, large expression atlases, population resources like gnomAD, and biobanks linking genomes to phenotypes. These datasets become the training targets and evaluation benchmarks for the deep models in later parts.

Together, Part I answers: *What are the core data, statistical tools, and baseline methods that any genomic foundation model must respect, integrate with, or improve upon?*

---

### Part II — CNN Seq-to-Function Models (Chapters 5–7)

Part II turns to the first wave of **deep sequence-to-function models**, largely built on convolutional neural networks (CNNs).

- **Chapter 5 — Regulatory Prediction**  
  Focuses on models that predict chromatin accessibility and transcription factor binding from DNA sequence (e.g., DeepSEA and successors) [@zhou_deepsea_2015; @chen_deepsea_2022; @avsec_enformer_2021]. It introduces one-hot sequence encodings, convolutional filters as motif detectors, and multi-task training over many assays.

- **Chapter 6 — Transcriptional Effects**  
  Moves from local chromatin states to gene-level transcriptional readouts, including models such as ExPecto and related architectures. It emphasizes the challenge of mapping sequences to expression in specific cell types and conditions, and introduces ideas like distance-dependent kernels.

- **Chapter 7 — Splicing Prediction**  
  Examines specialized sequence models for splicing, such as SpliceAI, which use deep receptive fields to capture intronic and exonic context. This chapter shows how tailored architectures can achieve state-of-the-art performance on particular mechanisms, foreshadowing the mechanism-aware GFMs of Part IV.

Part II illustrates how **task-specific supervised models** can uncover rich regulatory logic, but also how their narrow training objectives limit reuse.

---

### Part III — Transformer Models (Chapters 8–11)

Part III explores the **sequence modeling architectures** that underpin many GFMs, with an emphasis on transformers and long-range variants.

- **Chapter 8 — Sequence Representation & Tokens**  
  Discusses how to tokenize DNA, RNA, and protein sequences (nucleotides vs k-mers vs learned tokens) and how those choices affect model capacity, context length, and interpretability. It also introduces positional encodings and other tricks needed for very long sequences.

- **Chapter 9 — Protein Language Models**  
  Provides a primer on protein language models—ESM, AlphaMissense-style models, and related architectures—that learn from large multiple sequence alignments or unaligned protein corpora [@cheng_alphamissense_2023]. These models serve as a conceptual and technical template for DNA-based LMs and illustrate how representations can support both structure prediction and variant effect prediction.

- **Chapter 10 — DNA Foundation Models**  
  Introduces DNA language models and related self-supervised architectures (e.g., GPN, GenSLMs, HyenaDNA) [@benegas_gpn_2023; @zvyagin_genslms_2022; @nguyen_hyenadna_2023; @benegas_genomic_2024]. It contrasts them with the supervised CNNs of Part II, focusing on objectives, pretraining corpora, and usage modes (frozen probes, fine-tuning, zero-shot).

- **Chapter 11 — Long-range Hybrid Models**  
  Surveys architectures that integrate long-range context and multi-task supervision, including models like Enformer, Caduceus, and hybrid convolution–transformer–SSM designs [@avsec_enformer_2021; @schiff_caduceus_2024]. These models blur the line between “language model” and “sequence-to-function” network and motivate the broader GFM framing in Part IV.

Part III answers: *What architectural tools are available for modeling genomic sequences at kilobase to megabase scale, and how do they differ from classic CNNs?*

---

### Part IV — GFMs & Multi-omics (Chapters 12–16)

Part IV is the conceptual heart of the book, focusing explicitly on **genomic foundation models** and their multi-omic extensions.

- **Chapter 12 — Genomic FMs: Principles & Practice**  
  Provides a working definition of GFMs, tracing the progression from deleteriousness scores and CNNs to self-supervised DNA and protein LMs [@he_nucleic_2023; @benegas_genomic_2024; @dalla-torre_nucleotide_2023; @nguyen_hyenadna_2023]. It distills design principles around pretraining objectives, context length, conditioning (cell type, species), and deployment patterns.

- **Chapter 13 — Variant Effect Prediction**  
  Recasts variant effect prediction in the GFM era, spanning conservation-based scores, protein LMs, sequence-to-function models, and multi-modal systems like AlphaMissense and MSA-enhanced models [@cheng_alphamissense_2023; @benegas_gpn-msa_2024; @brixi_evo_2025]. It emphasizes how GFMs act as feature generators for downstream prioritization and association analyses.

- **Chapter 14 — Confounders in Model Training**  
  Details the many sources of confounding—batch effects, ancestry and population structure, data leakage between train and test, label bias—that can inflate reported performance or create spurious signals. This chapter provides a checklist for stress-testing GFMs and their derivatives.

- **Chapter 15 — Interpretability & Mechanisms**  
  Explores how to probe GFMs and related models for mechanistic insight: motif discovery, saliency maps, in silico mutagenesis, causal perturbation experiments, and connections to biophysical or evolutionary models. The emphasis is on turning black-box predictions into hypotheses about regulatory grammar and molecular mechanisms.

- **Chapter 16 — Multi-omics and Systems Context**  
  Broadens the view from isolated sequences to **multi-omic and network-level models**, integrating chromatin, expression, protein–protein interaction networks, and other modalities. It discusses graph-based models, multi-modal encoders, and how GFMs can provide a common representation layer for multi-omic integration [@li_mogcn_2022; @chandak_primekg_2023; @cornman_glm2_2024].

Part IV pulls together themes from every earlier chapter, focusing on how to design, train, and interrogate models that aspire to be genuinely *foundational* within genomics.

---

### Part V — Applications (Chapters 17–19)

Part V turns outward, focusing on **applications and deployment** in clinical and translational settings.

- **Chapter 17 — Clinical Risk Prediction**  
  Describes how GFMs and related models can be used to build clinical risk scores and decision support tools, often combining genomic features with electronic health record (EHR) and other data sources [@cao_glue_2022; @georgantas_delphi_2024; @clarke_deeprvat_2024; @rakowski_mifm_2025]. It covers calibration, uncertainty, fairness, and regulatory considerations for high-stakes predictions.

- **Chapter 18 — Pathogenic Variant Discovery**  
  Focuses on discovery workflows: ranking variants and genes for follow-up, integrating GFMs into fine-mapping, network-based gene prioritization, and the design of CRISPR and MPRA experiments for functional validation [@benegas_gpn-msa_2024; @avsec_enformer_2021; @linder_borzoi_2025; @li_cgmega_2024; @wu_genome-wide_2024].

- **Chapter 19 — Drug Discovery & Biotech**  
  Zooms out to drug discovery and biotech pipelines: target identification, genetic validation, functional genomics screens, and early-stage safety and efficacy prediction. It highlights where GFMs are already useful, where they remain speculative, and how they might integrate with broader scientific LLM ecosystems [@ma_bingo-large_2023; @zhang_scientific_2024].

These chapters emphasize **end-to-end workflows**: how a model trained on sequences ends up influencing a clinical report, a gene nomination list, or a preclinical portfolio.

---

## How to Read This Book

Different readers will want to enter at different points:

- If you are primarily a **computational biologist or statistical geneticist**, you may want to skim Part I (to align terminology) and then focus on Parts II–IV to understand how deep models are constructed and evaluated.
- If you are a **machine learning researcher**, Chapters 5–11 and 12–16 provide the most relevant architectural and modeling details, with Part I serving as biological context and Part V illustrating application constraints.
- If you are a **clinician, translational scientist, or industry practitioner**, you may wish to read Chapters 1–4 and 12–19, treating the technical architectural chapters as optional references.

The chapters are intentionally cross-referenced. When a later chapter relies heavily on earlier material—for example, Chapter 13 on variant effect prediction drawing from Chapters 3, 5–7, 9, and 10—this is called out explicitly.

---

## A Moving Target

Genomic foundation models are a **moving target**. New architectures, training corpora, and evaluation benchmarks continue to appear at a rapid pace [@he_nucleic_2023; @benegas_genomic_2024; @zhang_scientific_2024; @vishniakov_genomic_2024; @fishman_gena-lm_2025]. This book does not aim to be the final word on any particular model; instead, it offers a framework for understanding and comparing them.

If the book succeeds, you should finish it able to:

- Read a new GFM paper and place it in the landscape of data, architecture, objective, and application.
- Design experiments that use GFMs as components—features, priors, or simulators—without overclaiming.
- Recognize common pitfalls in training, evaluation, and deployment, especially in clinical and translational contexts.
- Form your own views about where genomic foundation models are genuinely transformative, and where simpler baselines or traditional methods may suffice.

The rest of the book now turns to the foundations: how we get from raw sequencing reads to high-confidence variants and the statistical tools that predate deep learning, setting the stage for the models to come.
