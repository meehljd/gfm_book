# Architectural Shift to Attention and Foundation Models

## The Transformer Core

- Transformer architecture (from NLP) uses self-attention to capture long-range dependencies efficiently.
- Contrasts with the local receptive fields of traditional CNNs.

## Genomic Language Models (gLMs)

- Apply the principles of language models (like BERT, using the Masked Language Modeling objective) to DNA sequences.
- Pre-trained on massive amounts of unlabeled data to learn complex features and dependencies.

Examples:

- **DNABERT**  
  - One of the earliest examples, applying BERT with k-mer tokenization to the human genome.
- **Nucleotide Transformer (NT)**  
  - A family of encoder-only transformer foundation models (up to 2.5B parameters).  
  - Pretrained on diverse human and multispecies genomes.  
  - Embeddings and derived scores have been used to prioritize functional variants such as eQTLs and meQTLs.
