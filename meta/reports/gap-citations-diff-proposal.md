# Gap Citations: Proposed Book Modifications

**Document Type:** Diff Proposal (PRE-EDIT REVIEW)
**Generated:** 2026-01-18
**Status:** DRAFT - Awaiting multi-agent review

---

## Overview

This document proposes specific text modifications to incorporate 62 high-priority papers identified in the gap analysis. Changes are organized by chapter with exact locations and proposed text.

**Notation:**
- `[+]` = New text to add
- `[-]` = Text to remove (if any)
- `[~]` = Text to modify
- `>>>` = Insertion point marker

---

## Part 4, Chapter 14: Foundation Model Paradigm

### Change 1: Add Bommasani et al. Definition Reference

**Location:** Section 14.2 "Defining Genomic Foundation Models" (around line 67)

**Current text:**
```
The term "foundation model" appears frequently in genomics literature,
sometimes applied to any large neural network trained on biological sequences.
```

**Proposed modification:**
```diff
The term "foundation model" appears frequently in genomics literature,
sometimes applied to any large neural network trained on biological sequences.
+ The Stanford HAI report formally defined foundation models as "models trained
+ on broad data at scale such that they can be adapted to a wide range of
+ downstream tasks" [@bommasani_on_2022]. This definition, while originating
+ from general AI discourse, captures essential properties that distinguish
+ true genomic foundation models from task-specific deep learning approaches.
```

**Rationale:** Bommasani et al. (2022) is the seminal paper defining "foundation model" - essential for grounding the chapter's conceptual framework.

---

### Change 2: Add Genomics FM Landscape Reference

**Location:** Section 14.2 after essential properties (around line 95)

**Proposed addition:**
```diff
+ ::: {.callout-note title="Field Overview"}
+ For a comprehensive survey of genomic foundation models as of 2024, including
+ taxonomy, benchmarks, and applications, see @trop_genomics_2024. This review
+ provides valuable context for understanding the rapid evolution of the field
+ and the relationships between model families discussed in subsequent chapters.
+ :::
```

**Rationale:** Trop et al. (2024) provides essential field overview that readers need for context.

---

## Part 4, Chapter 16: Protein Language Models

### Change 3: Add ESM3 Section

**Location:** After ESM-2 section (Section 16.2.2), before variant effect prediction

**Proposed addition:**
```diff
+ ### ESM3: Multimodal Protein Understanding {#sec-ch16-esm3}
+
+ The *ESM3* model represents a generational leap in protein language modeling,
+ expanding from sequence-only to multimodal reasoning across sequence, structure,
+ and function [@hayes_esm_2025]. Where *ESM-2* learned from sequence statistics
+ alone, *ESM3* jointly models the relationships between amino acid sequences,
+ 3D coordinates, secondary structure annotations, and functional tokens.
+
+ The model introduces several architectural innovations:
+
+ - **Multimodal tokenization:** Protein structure is discretized into learned
+   tokens that can be predicted alongside sequence, enabling joint generation
+ - **Function conditioning:** Annotations from InterPro, Gene Ontology, and
+   other databases are encoded as conditioning tokens
+ - **esmGFP:** A novel GFP variant generated by ESM3, demonstrating functional
+   protein design from language model outputs
+
+ The esmGFP result is particularly striking: the model generated a fluorescent
+ protein with only ~60% sequence identity to any known GFP, yet it folded
+ correctly and exhibited fluorescence. This demonstrates that protein language
+ models can extrapolate beyond their training distribution to discover novel
+ functional solutions, a capability discussed further in @sec-ch31-generative.
+
+ ::: {.callout-important title="Implications for Genomic FMs"}
+ ESM3's success with multimodal protein learning raises questions about whether
+ analogous approaches could benefit genomic language models. Could DNA models
+ benefit from joint training on sequence, chromatin state, and expression data?
+ The protein results suggest this direction merits exploration.
+ :::
```

**Rationale:** ESM3/Hayes et al. (2025) is a major advance that extends the ESM narrative and introduces functional protein design.

---

### Change 4: Add RoseTTAFold Reference

**Location:** Section on structure prediction methods

**Proposed addition:**
```diff
+ While *ESMFold* demonstrates single-sequence structure prediction,
+ *RoseTTAFold* [@baek_accurate_2021] provides an alternative approach that
+ combines multiple sequence alignment information with deep learning. The
+ model achieved accuracy competitive with *AlphaFold2* while offering
+ different trade-offs: it processes MSA information explicitly rather than
+ learning to extract it from sequence alone. For proteins with good homolog
+ coverage, *RoseTTAFold* may offer advantages; for orphan sequences,
+ single-sequence methods like *ESMFold* become essential.
```

---

### Change 5: Add OpenFold Reference

**Location:** After RoseTTAFold discussion

**Proposed addition:**
```diff
+ The *OpenFold* project [@ahdritz_openfold_2024] provides an open-source
+ reimplementation of *AlphaFold2* that enables fine-tuning and modification
+ not possible with the original weights. This accessibility has proven
+ valuable for domain-specific applications and for understanding the
+ internal mechanisms of structure prediction models.
```

---

### Change 6: Add Ingraham Protein Generation Reference

**Location:** End of chapter, before limitations section

**Proposed addition:**
```diff
+ ## Generative Protein Design {#sec-ch16-generative-intro}
+
+ The knowledge encoded in protein language models can be leveraged not just
+ for analysis but for generation. *Illuminating protein space*
+ [@ingraham_illuminating_2023] demonstrated that models trained to predict
+ masked amino acids can be inverted to generate novel proteins conditioned
+ on desired properties. This work established key principles:
+
+ 1. **Controllable generation:** Conditioning on functional annotations
+    enables targeted design
+ 2. **Diversity-function trade-off:** Models can generate diverse sequences
+    while maintaining predicted function
+ 3. **Experimental validation:** Generated sequences show high success rates
+    in experimental testing
+
+ This generative paradigm connects protein language models to the broader
+ design applications discussed in @sec-ch31-generative.
```

---

## Part 4, Chapter 17: Regulatory Models

### Change 7: Add Maurano Motivation

**Location:** Opening section, after the enhancer distance discussion (around line 25)

**Proposed addition:**
```diff
+ The biological reality of long-range regulation was established by systematic
+ studies mapping GWAS variants to functional elements. @maurano_systematic_2012
+ demonstrated that disease-associated variants are overwhelmingly enriched in
+ regulatory regions rather than coding sequences, with enrichment patterns
+ specific to cell types relevant to each disease. This foundational observation
+ motivates the entire enterprise of regulatory modeling: if most disease
+ variants act through regulatory mechanisms, understanding those mechanisms
+ requires models that can capture the regulatory grammar of the genome.
```

**Rationale:** Maurano et al. (2012) provides essential motivation for why regulatory models matter.

---

## Part 4, Chapter 18: Variant Effect Prediction

### Change 8: Add Non-Coding Statistics

**Location:** Section on non-coding variant interpretation

**Proposed addition:**
```diff
+ The challenge of non-coding variant interpretation is quantified by landmark
+ studies of GWAS variant localization. @farh_genetic_2015 established that
+ approximately 90% of disease-associated variants fall in non-coding regions,
+ with roughly 60% in enhancer-like chromatin states. Of these, only 10-20%
+ directly disrupt recognizable transcription factor binding motifs. These
+ statistics frame the VEP challenge: most variants of clinical interest act
+ through mechanisms that classical approaches cannot capture.
```

---

### Change 9: Add VEP Limitations Perspective

**Location:** Limitations section

**Proposed addition:**
```diff
+ ### Critical Perspectives on PLM-VEP {#sec-ch18-plm-limitations}
+
+ Despite impressive benchmark performance, protein language model approaches
+ to variant effect prediction face fundamental limitations that practitioners
+ must understand. @karollus_current_2023 systematically evaluated PLM-VEP
+ methods and identified several failure modes:
+
+ - **Conservation bias:** Models preferentially score positions that are
+   evolutionarily constrained, potentially missing functionally important
+   but variable positions
+ - **Epistatic blindness:** Single-variant scoring ignores combinatorial
+   effects that may be clinically relevant
+ - **Calibration gaps:** Scores do not translate directly to pathogenicity
+   probabilities
+
+ @swanson_predicting_2022 provides complementary analysis showing that PLMs
+ systematically underperform on proteins with limited evolutionary data or
+ unusual functions. @notin_tranception_2022 addresses some limitations through
+ retrieval-augmented approaches, but fundamental gaps remain between model
+ predictions and clinical interpretation needs.
```

---

## Part 3, Chapter 12: Benchmarks

### Change 10: Add Benchmark Circularity Section

**Location:** NEW section after standard benchmarks discussion

**Proposed addition:**
```diff
+ ## Benchmark Circularity {#sec-ch12-circularity}
+
+ A pervasive problem in variant effect prediction benchmarks is circularity:
+ the labels used for evaluation were often derived using methods that share
+ information with the models being evaluated. @grimm_evaluation_2015
+ demonstrated this problem systematically, showing that apparent performance
+ advantages of some methods disappeared when evaluation accounted for data
+ contamination.
+
+ The circularity problem manifests in several forms:
+
+ 1. **Label circularity:** Pathogenicity labels derived from SIFT/PolyPhen
+    scores advantage methods using similar features
+ 2. **Temporal leakage:** Training on variants classified after model
+    development inflates apparent performance
+ 3. **Gene-level leakage:** Variants in the same gene as training examples
+    may share confounding features
+
+ ::: {.callout-warning title="Practical Implication"}
+ When evaluating VEP methods, always ask: how were the labels generated?
+ Do any label sources overlap with model features? Are training and test
+ variants from the same genes? Apparent benchmark leaders may reflect
+ circularity rather than genuine predictive improvement.
+ :::
```

---

### Change 11: Add Genomic Touchstone Reference

**Location:** After existing benchmark descriptions

**Proposed addition:**
```diff
+ ### Emerging Comprehensive Benchmarks
+
+ The limitations of existing benchmarks have motivated development of more
+ rigorous evaluation frameworks. @wang_genomic_2025 introduce *Genomic
+ Touchstone*, a comprehensive benchmark designed specifically for genomic
+ foundation models. Key features include:
+
+ - **Systematic holdout:** Gene-level splits prevent information leakage
+ - **Multi-task evaluation:** Performance across diverse tasks reveals
+   generalization rather than task-specific optimization
+ - **Temporal splits:** Training cutoffs ensure evaluation on truly novel
+   variants
+
+ @tanigawa_significant_2022 provides complementary resources for comparing
+ foundation model approaches against traditional PRS methods, enabling
+ principled assessment of when deep learning adds value over classical
+ approaches.
```

---

## Part 3, Chapter 13: Confounding

### Change 12: Add Ancestry-Aware Methods

**Location:** Section on ancestry confounding

**Proposed addition:**
```diff
+ ### Addressing Ancestry Bias in Genomic Models
+
+ Several approaches have emerged to address ancestry confounding in genomic
+ prediction. @amariuta_improving_2020 demonstrated that incorporating
+ functional genomic annotations can improve PRS transferability across
+ populations, with larger gains for traits with well-characterized
+ regulatory mechanisms.
+
+ The importance of diverse representation extends beyond European-focused
+ cohorts. @sohail_mexican_2023 provides critical analysis of PRS performance
+ in Latin American populations, revealing systematic biases that cannot be
+ addressed by simple recalibration. The work highlights the need for
+ foundation models to be evaluated explicitly on diverse populations rather
+ than assuming that performance on European-ancestry cohorts will transfer.
```

---

## Part 6, Chapter 25: Interpretability

### Change 13: Add BERTology Reference

**Location:** Section on attention analysis

**Proposed addition:**
```diff
+ The techniques for interpreting attention patterns in genomic models draw
+ heavily from work in natural language processing. @vig_bertology_2021
+ established systematic approaches for analyzing what BERT-style models
+ learn, including:
+
+ - **Attention head specialization:** Different heads capture different
+   linguistic (or in genomics, biological) phenomena
+ - **Layer-wise representation analysis:** Early layers capture surface
+   patterns; later layers encode more abstract relationships
+ - **Probing classifiers:** Simple classifiers trained on internal
+   representations reveal encoded knowledge
+
+ These "BERTology" techniques translate directly to genomic models, where
+ attention patterns may reveal learned motif relationships, chromatin
+ domain structure, or evolutionary constraints.
```

---

## Part 6, Chapter 26: Causality

### Change 14: Add Mendelian Randomization Foundation

**Location:** Opening section

**Proposed addition:**
```diff
+ Establishing causal relationships from observational genomic data requires
+ careful methodology. @daveysmith_mendelian_2003 introduced **Mendelian
+ randomization** as a framework for using genetic variants as instrumental
+ variables, enabling causal inference even without experimental manipulation.
+ The key insight is that genotypes are assigned randomly at conception
+ (conditional on parental genotypes), providing a natural randomization that
+ can be exploited to estimate causal effects.
+
+ This framework has profound implications for genomic foundation models:
+ predictions about variant effects should be evaluated not just for
+ predictive accuracy but for causal validity. A model that predicts
+ associations without causal grounding may perform well on benchmarks while
+ failing to generalize to intervention settings.
```

---

## Part 6, Chapter 27: Regulation

### Change 15: Add FDA AI/ML Guidance

**Location:** Section on regulatory frameworks

**Proposed addition:**
```diff
+ ### FDA Guidance on AI/ML in Medicine
+
+ The U.S. Food and Drug Administration has issued specific guidance on
+ AI/ML-based medical devices that affects genomic prediction tools intended
+ for clinical use [@fda_artificial_2021]. Key requirements include:
+
+ - **Predetermined change control plans:** Methods for updating models
+   without requiring new regulatory clearance
+ - **Real-world performance monitoring:** Requirements for ongoing
+   performance assessment after deployment
+ - **Transparency requirements:** Documentation of training data,
+   validation procedures, and known limitations
+
+ Genomic foundation models intended for clinical variant interpretation must
+ consider these requirements from the design phase, not as an afterthought.
```

---

## Part 7, Chapter 28: Clinical Risk

### Change 16: Add Non-Linear PRS Methods

**Location:** Section on ML-enhanced PRS

**Proposed addition:**
```diff
+ ### Deep Learning Approaches to PRS
+
+ Traditional PRS assume linear and additive genetic effects, but non-linear
+ methods may capture additional predictive signal. @elgart_non_2022
+ systematically compared linear PRS against deep learning approaches,
+ finding that non-linear models provide modest but consistent improvements
+ for some phenotypes, particularly those with complex genetic architectures.
+
+ @dibaeinia_prsformer_2025 extends this work with transformer-based
+ architectures specifically designed for PRS, demonstrating that attention
+ mechanisms can capture genetic interactions missed by linear approaches.
+ However, the gains come with increased computational requirements and
+ reduced interpretability.
+
+ @yun_regle_2023 introduces an alternative paradigm using phenotype
+ embeddings, suggesting that learning representations of disease phenotypes
+ may improve genetic prediction through better characterization of the
+ outcome space.
```

---

## Part 7, Chapter 30: Drug Discovery

### Change 17: Add DiffDock Reference

**Location:** Section on molecular docking

**Proposed addition:**
```diff
+ ### Diffusion Models for Molecular Docking
+
+ *DiffDock* [@corso_diffdock_2022] introduced diffusion-based approaches to
+ molecular docking, achieving state-of-the-art performance on binding pose
+ prediction. The method treats docking as a generative problem, iteratively
+ denoising random poses toward the true binding configuration. This
+ represents a departure from traditional scoring-function-based approaches
+ and connects molecular docking to the broader foundation model paradigm
+ through learned priors over molecular interactions.
```

---

## Part 5, Chapter 21: Spatial Transcriptomics

### Change 18: Add SCENIC Reference

**Location:** Section on gene regulatory networks

**Proposed addition:**
```diff
+ Classical approaches to gene regulatory network inference from single-cell
+ data, exemplified by *SCENIC* [@aibar_scenic_2017], establish baseline
+ methods against which foundation model approaches should be compared.
+ *SCENIC* combines motif enrichment with co-expression analysis to infer
+ transcription factor regulons, providing interpretable network structures
+ that can be validated against ChIP-seq binding data.
```

---

### Change 19: Add HEIST Reference

**Location:** Section on spatial foundation models

**Proposed addition:**
```diff
+ ### Spatial Foundation Models
+
+ Foundation models are beginning to incorporate spatial information in
+ single-cell analysis. *HEIST* [@madhu_heist_2025] demonstrates that
+ transformer architectures can learn meaningful representations from
+ spatial transcriptomics data, capturing tissue organization and
+ cell-cell communication patterns that bulk or dissociated approaches miss.
```

---

## Part 1, Chapter 2: Datasets

### Change 20: Add TOPMed Reference

**Location:** Section on large-scale sequencing resources

**Proposed addition:**
```diff
+ The Trans-Omics for Precision Medicine (TOPMed) program provides one of
+ the largest deeply sequenced cohorts available for genomic research.
+ @taliun_sequencing_2021 describes the resource: over 150,000 genomes
+ at high coverage (>30Ã—), with diverse ancestry representation and deep
+ phenotyping. TOPMed has become a crucial resource for rare variant
+ analysis and for developing methods that generalize across populations.
```

---

## Additional Tool/Resource Citations

### Chapter 2: Add PLINK Reference
```diff
+ *PLINK* [@purcell_plink_2007] remains the standard tool for genome-wide
+ association analysis and data manipulation, providing essential
+ functionality for quality control, format conversion, and basic statistical
+ analysis that most genomic workflows depend upon.
```

### Chapter 28: Add PharmGKB Reference
```diff
+ The Pharmacogenomics Knowledge Base (PharmGKB) [@whirlcarrillo_pharmacogenomics_2012]
+ curates variant-drug associations with clinical annotations, providing
+ essential resources for pharmacogenomic applications of foundation models.
```

---

## Summary of Proposed Changes

| Chapter | Changes | Key Papers Added |
|---------|---------|------------------|
| ch14 | 2 | bommasani_on_2022, trop_genomics_2024 |
| ch16 | 4 | hayes_esm_2025, baek_accurate_2021, ahdritz_openfold_2024, ingraham_illuminating_2023 |
| ch17 | 1 | maurano_systematic_2012 |
| ch18 | 2 | farh_genetic_2015, karollus_current_2023, swanson_predicting_2022, notin_tranception_2022 |
| ch12 | 2 | grimm_evaluation_2015, wang_genomic_2025, tanigawa_significant_2022 |
| ch13 | 1 | amariuta_improving_2020, sohail_mexican_2023 |
| ch25 | 1 | vig_bertology_2021 |
| ch26 | 1 | daveysmith_mendelian_2003 |
| ch27 | 1 | fda_artificial_2021 |
| ch28 | 2 | elgart_non_2022, dibaeinia_prsformer_2025, yun_regle_2023 |
| ch30 | 1 | corso_diffdock_2022 |
| ch21 | 2 | aibar_scenic_2017, madhu_heist_2025 |
| ch02 | 2 | taliun_sequencing_2021, purcell_plink_2007 |

**Total:** ~20 major changes adding ~45 papers

---

## Review Requested

Please review for:
1. **Pedagogy:** Do additions support learning objectives?
2. **Flow:** Do additions integrate smoothly with existing text?
3. **Accuracy:** Are paper characterizations correct?
4. **Accessibility:** Is jargon appropriately explained?
5. **Consistency:** Do additions match book style?
6. **Necessity:** Are all additions truly needed?

---

*This is a DRAFT document. Do not implement changes until multi-agent review is complete.*
