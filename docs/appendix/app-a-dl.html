<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Appendix A — Deep Learning Primer – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../appendix/app-b-compute.html" rel="next">
<link href="../bib/references.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../appendix/app-a-dl.html">Appendices</a></li><li class="breadcrumb-item"><a href="../appendix/app-a-dl.html"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Learning &amp; Evaluation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmark Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch12-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Evaluation Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--fm-families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch14-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch15-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch16-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch17-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--cellular-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Cellular Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch19-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch20-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch21-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch22-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Responsible Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch24-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch25-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch26-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Causality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_7/p7--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VII: Applications &amp; Frontiers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch28-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch29-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch30-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch31-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch32-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-g-learning-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Statistical Learning Theory Primer</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-apx-a-nn-basics" id="toc-sec-apx-a-nn-basics" class="nav-link active" data-scroll-target="#sec-apx-a-nn-basics"><span class="header-section-number">A.1</span> Neural Networks as Function Approximators</a>
  <ul class="collapse">
  <li><a href="#the-perceptron-and-linear-layers" id="toc-the-perceptron-and-linear-layers" class="nav-link" data-scroll-target="#the-perceptron-and-linear-layers"><span class="header-section-number">A.1.1</span> The Perceptron and Linear Layers</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">A.1.2</span> Activation Functions</a></li>
  <li><a href="#depth-and-width" id="toc-depth-and-width" class="nav-link" data-scroll-target="#depth-and-width"><span class="header-section-number">A.1.3</span> Depth and Width</a></li>
  </ul></li>
  <li><a href="#sec-apx-a-training" id="toc-sec-apx-a-training" class="nav-link" data-scroll-target="#sec-apx-a-training"><span class="header-section-number">A.2</span> Training Neural Networks</a>
  <ul class="collapse">
  <li><a href="#loss-functions" id="toc-loss-functions" class="nav-link" data-scroll-target="#loss-functions"><span class="header-section-number">A.2.1</span> Loss Functions</a></li>
  <li><a href="#gradient-descent-and-backpropagation" id="toc-gradient-descent-and-backpropagation" class="nav-link" data-scroll-target="#gradient-descent-and-backpropagation"><span class="header-section-number">A.2.2</span> Gradient Descent and Backpropagation</a></li>
  <li><a href="#stochastic-gradient-descent-and-minibatches" id="toc-stochastic-gradient-descent-and-minibatches" class="nav-link" data-scroll-target="#stochastic-gradient-descent-and-minibatches"><span class="header-section-number">A.2.3</span> Stochastic Gradient Descent and Minibatches</a></li>
  <li><a href="#optimizers" id="toc-optimizers" class="nav-link" data-scroll-target="#optimizers"><span class="header-section-number">A.2.4</span> Optimizers</a></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="header-section-number">A.2.5</span> Regularization</a></li>
  </ul></li>
  <li><a href="#sec-apx-a-cnn" id="toc-sec-apx-a-cnn" class="nav-link" data-scroll-target="#sec-apx-a-cnn"><span class="header-section-number">A.3</span> Convolutional Neural Networks</a>
  <ul class="collapse">
  <li><a href="#convolution-operation" id="toc-convolution-operation" class="nav-link" data-scroll-target="#convolution-operation"><span class="header-section-number">A.3.1</span> Convolution Operation</a></li>
  <li><a href="#key-cnn-components" id="toc-key-cnn-components" class="nav-link" data-scroll-target="#key-cnn-components"><span class="header-section-number">A.3.2</span> Key CNN Components</a></li>
  <li><a href="#cnns-for-genomics" id="toc-cnns-for-genomics" class="nav-link" data-scroll-target="#cnns-for-genomics"><span class="header-section-number">A.3.3</span> CNNs for Genomics</a></li>
  </ul></li>
  <li><a href="#sec-apx-a-rnn" id="toc-sec-apx-a-rnn" class="nav-link" data-scroll-target="#sec-apx-a-rnn"><span class="header-section-number">A.4</span> Recurrent Neural Networks</a>
  <ul class="collapse">
  <li><a href="#lstm-and-gru" id="toc-lstm-and-gru" class="nav-link" data-scroll-target="#lstm-and-gru"><span class="header-section-number">A.4.1</span> LSTM and GRU</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations"><span class="header-section-number">A.4.2</span> Limitations</a></li>
  </ul></li>
  <li><a href="#sec-apx-a-attention" id="toc-sec-apx-a-attention" class="nav-link" data-scroll-target="#sec-apx-a-attention"><span class="header-section-number">A.5</span> Attention and Transformers</a>
  <ul class="collapse">
  <li><a href="#self-attention" id="toc-self-attention" class="nav-link" data-scroll-target="#self-attention"><span class="header-section-number">A.5.1</span> Self-Attention</a></li>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention"><span class="header-section-number">A.5.2</span> Multi-Head Attention</a></li>
  <li><a href="#transformer-architecture" id="toc-transformer-architecture" class="nav-link" data-scroll-target="#transformer-architecture"><span class="header-section-number">A.5.3</span> Transformer Architecture</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding"><span class="header-section-number">A.5.4</span> Positional Encoding</a></li>
  <li><a href="#encoder-vs.-decoder" id="toc-encoder-vs.-decoder" class="nav-link" data-scroll-target="#encoder-vs.-decoder"><span class="header-section-number">A.5.5</span> Encoder vs.&nbsp;Decoder</a></li>
  <li><a href="#computational-complexity" id="toc-computational-complexity" class="nav-link" data-scroll-target="#computational-complexity"><span class="header-section-number">A.5.6</span> Computational Complexity</a></li>
  </ul></li>
  <li><a href="#sec-apx-a-embeddings" id="toc-sec-apx-a-embeddings" class="nav-link" data-scroll-target="#sec-apx-a-embeddings"><span class="header-section-number">A.6</span> Embeddings and Representations</a>
  <ul class="collapse">
  <li><a href="#token-embeddings" id="toc-token-embeddings" class="nav-link" data-scroll-target="#token-embeddings"><span class="header-section-number">A.6.1</span> Token Embeddings</a></li>
  <li><a href="#contextual-embeddings" id="toc-contextual-embeddings" class="nav-link" data-scroll-target="#contextual-embeddings"><span class="header-section-number">A.6.2</span> Contextual Embeddings</a></li>
  </ul></li>
  <li><a href="#sec-apx-a-pretraining" id="toc-sec-apx-a-pretraining" class="nav-link" data-scroll-target="#sec-apx-a-pretraining"><span class="header-section-number">A.7</span> Pretraining and Transfer Learning</a>
  <ul class="collapse">
  <li><a href="#self-supervised-objectives" id="toc-self-supervised-objectives" class="nav-link" data-scroll-target="#self-supervised-objectives"><span class="header-section-number">A.7.1</span> Self-Supervised Objectives</a></li>
  <li><a href="#transfer-learning" id="toc-transfer-learning" class="nav-link" data-scroll-target="#transfer-learning"><span class="header-section-number">A.7.2</span> Transfer Learning</a></li>
  </ul></li>
  <li><a href="#sec-apx-a-practical" id="toc-sec-apx-a-practical" class="nav-link" data-scroll-target="#sec-apx-a-practical"><span class="header-section-number">A.8</span> Practical Considerations</a>
  <ul class="collapse">
  <li><a href="#hardware-requirements" id="toc-hardware-requirements" class="nav-link" data-scroll-target="#hardware-requirements"><span class="header-section-number">A.8.1</span> Hardware Requirements</a></li>
  <li><a href="#software-frameworks" id="toc-software-frameworks" class="nav-link" data-scroll-target="#software-frameworks"><span class="header-section-number">A.8.2</span> Software Frameworks</a></li>
  <li><a href="#common-pitfalls" id="toc-common-pitfalls" class="nav-link" data-scroll-target="#common-pitfalls"><span class="header-section-number">A.8.3</span> Common Pitfalls</a></li>
  </ul></li>
  <li><a href="#sec-apx-a-further" id="toc-sec-apx-a-further" class="nav-link" data-scroll-target="#sec-apx-a-further"><span class="header-section-number">A.9</span> Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../appendix/app-a-dl.html">Appendices</a></li><li class="breadcrumb-item"><a href="../appendix/app-a-dl.html"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-apx-a-dl" class="quarto-section-identifier">Appendix A — Deep Learning Primer</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This appendix provides a concise introduction to deep learning concepts for readers with limited machine learning background. It covers the foundational ideas necessary to understand genomic foundation models without requiring prior exposure to neural networks. Readers already familiar with deep learning can skip this appendix; those seeking deeper treatment should consult the resources in <a href="app-e-resources.html" class="quarto-xref"><span>Appendix E</span></a>.</p>
<section id="sec-apx-a-nn-basics" class="level2" data-number="A.1">
<h2 data-number="A.1" class="anchored" data-anchor-id="sec-apx-a-nn-basics"><span class="header-section-number">A.1</span> Neural Networks as Function Approximators</h2>
<p>A neural network is a parameterized function that maps inputs to outputs through a series of transformations. For genomic applications, inputs might be DNA sequences, protein sequences, or variant annotations; outputs might be pathogenicity scores, expression predictions, or functional class probabilities.</p>
<section id="the-perceptron-and-linear-layers" class="level3" data-number="A.1.1">
<h3 data-number="A.1.1" class="anchored" data-anchor-id="the-perceptron-and-linear-layers"><span class="header-section-number">A.1.1</span> The Perceptron and Linear Layers</h3>
<p>The simplest neural network component, the <strong>perceptron</strong>, computes a weighted sum of inputs plus a bias term:</p>
<p><span class="math display">\[y = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right) = \sigma(\mathbf{w}^T \mathbf{x} + b)\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}\)</span> is the input vector, <span class="math inline">\(\mathbf{w}\)</span> are learnable weights, <span class="math inline">\(b\)</span> is a learnable bias, and <span class="math inline">\(\sigma\)</span> is an activation function. A <strong>linear layer</strong> (also called a fully connected or dense layer) extends this to multiple outputs by using a weight matrix <span class="math inline">\(\mathbf{W}\)</span> instead of a vector:</p>
<p><span class="math display">\[\mathbf{y} = \sigma(\mathbf{W}\mathbf{x} + \mathbf{b})\]</span></p>
</section>
<section id="activation-functions" class="level3" data-number="A.1.2">
<h3 data-number="A.1.2" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">A.1.2</span> Activation Functions</h3>
<p>Without nonlinear activation functions, stacking linear layers produces only linear transformations (the composition of linear functions is linear). <strong>Activation functions</strong> introduce nonlinearity, enabling networks to learn complex mappings.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 29%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Function</th>
<th>Formula</th>
<th>Properties</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ReLU</td>
<td><span class="math inline">\(\max(0, x)\)</span></td>
<td>Simple, fast; standard default</td>
</tr>
<tr class="even">
<td>GELU</td>
<td><span class="math inline">\(x \cdot \Phi(x)\)</span></td>
<td>Smooth; used in transformers</td>
</tr>
<tr class="odd">
<td>Sigmoid</td>
<td><span class="math inline">\(1/(1 + e^{-x})\)</span></td>
<td>Output in (0, 1); used for probabilities</td>
</tr>
<tr class="even">
<td>Softmax</td>
<td><span class="math inline">\(e^{x_i}/\sum_j e^{x_j}\)</span></td>
<td>Output sums to 1; used for classification</td>
</tr>
<tr class="odd">
<td>Tanh</td>
<td><span class="math inline">\((e^x - e^{-x})/(e^x + e^{-x})\)</span></td>
<td>Output in (-1, 1); centered</td>
</tr>
</tbody>
</table>
<p><strong>ReLU</strong> (Rectified Linear Unit) is the most common choice for hidden layers due to computational efficiency and good gradient properties. <strong>GELU</strong> (Gaussian Error Linear Unit) has become standard in transformer architectures. <strong>Softmax</strong> converts a vector of scores into a probability distribution and is typically used in the final layer for classification tasks.</p>
</section>
<section id="depth-and-width" class="level3" data-number="A.1.3">
<h3 data-number="A.1.3" class="anchored" data-anchor-id="depth-and-width"><span class="header-section-number">A.1.3</span> Depth and Width</h3>
<p>A network’s <strong>depth</strong> refers to the number of layers; its <strong>width</strong> refers to the number of units per layer. Deeper networks can represent more complex hierarchical features but are harder to train. Wider networks have more capacity per layer but may require more data to avoid overfitting.</p>
<p>Modern genomic foundation models are both deep (dozens to hundreds of layers) and wide (thousands of units per layer), requiring specialized training techniques and substantial computational resources.</p>
</section>
</section>
<section id="sec-apx-a-training" class="level2" data-number="A.2">
<h2 data-number="A.2" class="anchored" data-anchor-id="sec-apx-a-training"><span class="header-section-number">A.2</span> Training Neural Networks</h2>
<p>Training a neural network means finding parameter values that minimize a <strong>loss function</strong> measuring the discrepancy between predictions and targets.</p>
<section id="loss-functions" class="level3" data-number="A.2.1">
<h3 data-number="A.2.1" class="anchored" data-anchor-id="loss-functions"><span class="header-section-number">A.2.1</span> Loss Functions</h3>
<p>The loss function quantifies prediction error. Common choices:</p>
<p><strong>Cross-entropy loss</strong> for classification measures the divergence between predicted probabilities and true labels:</p>
<p><span class="math display">\[\mathcal{L} = -\sum_{i} y_i \log(\hat{y}_i)\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the true label (1 for correct class, 0 otherwise) and <span class="math inline">\(\hat{y}_i\)</span> is the predicted probability.</p>
<p><strong>Mean squared error</strong> for regression measures average squared difference:</p>
<p><span class="math display">\[\mathcal{L} = \frac{1}{n}\sum_{i}(y_i - \hat{y}_i)^2\]</span></p>
</section>
<section id="gradient-descent-and-backpropagation" class="level3" data-number="A.2.2">
<h3 data-number="A.2.2" class="anchored" data-anchor-id="gradient-descent-and-backpropagation"><span class="header-section-number">A.2.2</span> Gradient Descent and Backpropagation</h3>
<p>Neural networks are trained using <strong>gradient descent</strong>: iteratively adjusting parameters in the direction that reduces the loss. The gradient (partial derivatives of the loss with respect to each parameter) indicates the direction of steepest increase; moving opposite to the gradient decreases the loss.</p>
<p><strong>Backpropagation</strong> efficiently computes gradients by applying the chain rule layer by layer, propagating error signals backward from the output to the input. This algorithm makes training deep networks computationally tractable.</p>
<p>The <strong>learning rate</strong> <span class="math inline">\(\eta\)</span> controls step size:</p>
<p><span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}\]</span></p>
<p>Too large a learning rate causes unstable training; too small a rate causes slow convergence.</p>
</section>
<section id="stochastic-gradient-descent-and-minibatches" class="level3" data-number="A.2.3">
<h3 data-number="A.2.3" class="anchored" data-anchor-id="stochastic-gradient-descent-and-minibatches"><span class="header-section-number">A.2.3</span> Stochastic Gradient Descent and Minibatches</h3>
<p>Computing gradients over the entire dataset is expensive. <strong>Stochastic gradient descent (SGD)</strong> approximates the full gradient using random subsets (<strong>minibatches</strong>) of training examples. This introduces noise but enables efficient training on large datasets and can help escape local minima.</p>
<p><strong>Batch size</strong> affects training dynamics: larger batches provide more stable gradient estimates but may converge to sharper minima that generalize worse; smaller batches introduce more noise but often find flatter minima with better generalization.</p>
</section>
<section id="optimizers" class="level3" data-number="A.2.4">
<h3 data-number="A.2.4" class="anchored" data-anchor-id="optimizers"><span class="header-section-number">A.2.4</span> Optimizers</h3>
<p>Modern optimizers improve on basic SGD:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 45%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th>Optimizer</th>
<th>Key Feature</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>SGD with momentum</strong></td>
<td>Accumulates gradient history for smoother updates</td>
</tr>
<tr class="even">
<td><strong>Adam</strong></td>
<td>Adapts learning rate per-parameter; default choice</td>
</tr>
<tr class="odd">
<td><strong>AdamW</strong></td>
<td>Adam with decoupled weight decay; standard for transformers</td>
</tr>
<tr class="even">
<td><strong>LAMB</strong></td>
<td>Layer-wise adaptive rates; enables large batch training</td>
</tr>
</tbody>
</table>
<p><strong>Adam</strong> (Adaptive Moment Estimation) maintains running averages of gradients and squared gradients, adapting the learning rate for each parameter. It is the default optimizer for most deep learning applications. <strong>AdamW</strong> adds proper weight decay regularization and is standard for transformer training.</p>
</section>
<section id="regularization" class="level3" data-number="A.2.5">
<h3 data-number="A.2.5" class="anchored" data-anchor-id="regularization"><span class="header-section-number">A.2.5</span> Regularization</h3>
<p><strong>Regularization</strong> techniques prevent overfitting by constraining model complexity:</p>
<p><strong>Weight decay</strong> (L2 regularization) penalizes large weights by adding <span class="math inline">\(\lambda \|\theta\|^2\)</span> to the loss, encouraging simpler solutions.</p>
<p><strong>Dropout</strong> randomly sets a fraction of activations to zero during training, preventing co-adaptation of features. At inference, all units are active but scaled appropriately.</p>
<p><strong>Early stopping</strong> monitors validation loss during training and stops when it begins increasing, preventing the model from memorizing training data.</p>
<p><strong>Data augmentation</strong> artificially expands training data by applying label-preserving transformations. For sequences, this might include reverse complementation (for strand-symmetric tasks) or random masking.</p>
</section>
</section>
<section id="sec-apx-a-cnn" class="level2" data-number="A.3">
<h2 data-number="A.3" class="anchored" data-anchor-id="sec-apx-a-cnn"><span class="header-section-number">A.3</span> Convolutional Neural Networks</h2>
<p><strong>Convolutional neural networks (CNNs)</strong> are designed for data with spatial or sequential structure. They were the dominant architecture for genomic sequence analysis before transformers and remain important for certain applications.</p>
<section id="convolution-operation" class="level3" data-number="A.3.1">
<h3 data-number="A.3.1" class="anchored" data-anchor-id="convolution-operation"><span class="header-section-number">A.3.1</span> Convolution Operation</h3>
<p>A <strong>convolutional layer</strong> applies learnable filters (kernels) that slide across the input, computing dot products at each position. For a 1D sequence (like DNA), a filter of width <span class="math inline">\(k\)</span> detects patterns of length <span class="math inline">\(k\)</span> nucleotides:</p>
<p><span class="math display">\[y_i = \sigma\left(\sum_{j=0}^{k-1} w_j \cdot x_{i+j} + b\right)\]</span></p>
<p>The same filter is applied at every position, so the network learns position-invariant patterns. A filter trained to recognize a TATA box will detect it regardless of its location in the sequence.</p>
</section>
<section id="key-cnn-components" class="level3" data-number="A.3.2">
<h3 data-number="A.3.2" class="anchored" data-anchor-id="key-cnn-components"><span class="header-section-number">A.3.2</span> Key CNN Components</h3>
<p><strong>Multiple filters</strong> learn different patterns. A layer with 64 filters of width 8 learns 64 different 8-bp motifs.</p>
<p><strong>Pooling</strong> reduces spatial dimensions by taking the maximum or average over local regions, providing translation invariance and reducing computational cost.</p>
<p><strong>Dilation</strong> inserts gaps in the filter, allowing detection of patterns spanning larger regions without increasing parameters. A dilated convolution with dilation rate 2 and filter width 3 spans 5 positions.</p>
<p><strong>Stride</strong> controls how far the filter moves between applications. Stride &gt; 1 downsamples the output.</p>
</section>
<section id="cnns-for-genomics" class="level3" data-number="A.3.3">
<h3 data-number="A.3.3" class="anchored" data-anchor-id="cnns-for-genomics"><span class="header-section-number">A.3.3</span> CNNs for Genomics</h3>
<p>Early genomic deep learning models (<em>DeepSEA</em>, <em>Basset</em>, <em>DeepBind</em>) used CNNs to predict regulatory function from DNA sequence. The architecture naturally captures motifs: first-layer filters learn individual transcription factor binding motifs; deeper layers combine these into higher-order regulatory logic.</p>
<p>CNNs remain useful for:</p>
<ul>
<li><strong>Short-range patterns</strong>: Splice sites, promoter elements, binding sites</li>
<li><strong>Computational efficiency</strong>: Faster training than transformers for local tasks</li>
<li><strong>Interpretability</strong>: First-layer filters directly correspond to sequence motifs</li>
</ul>
<p>Limitations include difficulty capturing long-range dependencies (addressed by dilated convolutions in <em>Basenji</em>) and lack of position-specific processing (every position is treated identically).</p>
</section>
</section>
<section id="sec-apx-a-rnn" class="level2" data-number="A.4">
<h2 data-number="A.4" class="anchored" data-anchor-id="sec-apx-a-rnn"><span class="header-section-number">A.4</span> Recurrent Neural Networks</h2>
<p><strong>Recurrent neural networks (RNNs)</strong> process sequences by maintaining hidden state that accumulates information across positions. At each position, the network updates its hidden state based on the current input and previous state:</p>
<p><span class="math display">\[h_t = f(h_{t-1}, x_t)\]</span></p>
<p>This allows modeling dependencies across arbitrary distances, in principle.</p>
<section id="lstm-and-gru" class="level3" data-number="A.4.1">
<h3 data-number="A.4.1" class="anchored" data-anchor-id="lstm-and-gru"><span class="header-section-number">A.4.1</span> LSTM and GRU</h3>
<p>Basic RNNs suffer from <strong>vanishing gradients</strong>: signals from distant positions decay exponentially, preventing learning of long-range dependencies.</p>
<p><strong>Long Short-Term Memory (LSTM)</strong> addresses this with gated units that control information flow, allowing the network to selectively remember or forget information across many steps.</p>
<p><strong>Gated Recurrent Unit (GRU)</strong> is a simplified variant with fewer parameters that often performs comparably.</p>
</section>
<section id="limitations" class="level3" data-number="A.4.2">
<h3 data-number="A.4.2" class="anchored" data-anchor-id="limitations"><span class="header-section-number">A.4.2</span> Limitations</h3>
<p>RNNs process sequences sequentially, preventing parallelization and making training slow for long sequences. They also struggle with very long-range dependencies despite architectural improvements. These limitations motivated the development of attention mechanisms and transformers, which have largely replaced RNNs in genomic applications.</p>
</section>
</section>
<section id="sec-apx-a-attention" class="level2" data-number="A.5">
<h2 data-number="A.5" class="anchored" data-anchor-id="sec-apx-a-attention"><span class="header-section-number">A.5</span> Attention and Transformers</h2>
<p>The <strong>transformer</strong> architecture, introduced by Vaswani et al. <span class="citation" data-cites="vaswani_attention_2023">(<a href="../bib/references.html#ref-vaswani_attention_2023" role="doc-biblioref">Vaswani et al. 2023</a>)</span>, has become the foundation for modern language models and genomic foundation models. Its key innovation is the <strong>attention mechanism</strong>, which allows direct interaction between any two positions in a sequence.</p>
<section id="self-attention" class="level3" data-number="A.5.1">
<h3 data-number="A.5.1" class="anchored" data-anchor-id="self-attention"><span class="header-section-number">A.5.1</span> Self-Attention</h3>
<p><strong>Self-attention</strong> computes, for each position, a weighted combination of all positions based on their relevance. Given input representations <span class="math inline">\(\mathbf{X}\)</span>, the mechanism computes:</p>
<ol type="1">
<li><strong>Queries</strong> <span class="math inline">\(\mathbf{Q} = \mathbf{X}\mathbf{W}_Q\)</span>: What information is this position looking for?</li>
<li><strong>Keys</strong> <span class="math inline">\(\mathbf{K} = \mathbf{X}\mathbf{W}_K\)</span>: What information does this position contain?</li>
<li><strong>Values</strong> <span class="math inline">\(\mathbf{V} = \mathbf{X}\mathbf{W}_V\)</span>: What information should be retrieved?</li>
</ol>
<p>Attention weights are computed as:</p>
<p><span class="math display">\[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}\]</span></p>
<p>The softmax ensures weights sum to 1; the <span class="math inline">\(\sqrt{d_k}\)</span> scaling prevents dot products from growing too large for high-dimensional representations.</p>
</section>
<section id="multi-head-attention" class="level3" data-number="A.5.2">
<h3 data-number="A.5.2" class="anchored" data-anchor-id="multi-head-attention"><span class="header-section-number">A.5.2</span> Multi-Head Attention</h3>
<p><strong>Multi-head attention</strong> runs several attention operations in parallel with different learned projections, allowing the model to attend to different types of relationships simultaneously:</p>
<p><span class="math display">\[\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}_O\]</span></p>
<p>Each head might capture different patterns: one head might attend to nearby positions for local context, another to conserved positions across the sequence, another to structurally related positions.</p>
</section>
<section id="transformer-architecture" class="level3" data-number="A.5.3">
<h3 data-number="A.5.3" class="anchored" data-anchor-id="transformer-architecture"><span class="header-section-number">A.5.3</span> Transformer Architecture</h3>
<p>A transformer layer combines multi-head attention with a feed-forward network and residual connections:</p>
<pre><code>Input → LayerNorm → Multi-Head Attention → + → LayerNorm → Feed-Forward → + → Output
         ↑_____________________________|         ↑_____________________|
                (residual connection)                (residual connection)</code></pre>
<p><strong>Layer normalization</strong> stabilizes training by normalizing activations.</p>
<p><strong>Residual connections</strong> add the input directly to the output, allowing gradients to flow unchanged and enabling training of very deep networks.</p>
<p><strong>Feed-forward networks</strong> (typically two linear layers with GELU activation) process each position independently, providing additional transformation capacity.</p>
</section>
<section id="positional-encoding" class="level3" data-number="A.5.4">
<h3 data-number="A.5.4" class="anchored" data-anchor-id="positional-encoding"><span class="header-section-number">A.5.4</span> Positional Encoding</h3>
<p>Self-attention is permutation-invariant: it treats positions as a set, not a sequence. <strong>Positional encodings</strong> inject position information, either through learned embeddings or fixed sinusoidal patterns. For genomic sequences, positional encoding enables the model to learn position-dependent patterns (like distance from transcription start sites).</p>
</section>
<section id="encoder-vs.-decoder" class="level3" data-number="A.5.5">
<h3 data-number="A.5.5" class="anchored" data-anchor-id="encoder-vs.-decoder"><span class="header-section-number">A.5.5</span> Encoder vs.&nbsp;Decoder</h3>
<p><strong>Encoder</strong> transformers (like <em>BERT</em>, <em>DNABERT</em>) use bidirectional attention: each position attends to all positions. They excel at classification and embedding tasks.</p>
<p><strong>Decoder</strong> transformers (like GPT, <em>HyenaDNA</em> in autoregressive mode) use <strong>causal attention</strong>: each position attends only to preceding positions. They excel at generation tasks.</p>
<p><strong>Encoder-decoder</strong> transformers use both, with the decoder attending to encoder outputs. Less common in genomics.</p>
</section>
<section id="computational-complexity" class="level3" data-number="A.5.6">
<h3 data-number="A.5.6" class="anchored" data-anchor-id="computational-complexity"><span class="header-section-number">A.5.6</span> Computational Complexity</h3>
<p>Standard attention scales quadratically with sequence length (<span class="math inline">\(O(n^2)\)</span>), limiting context length. A 200 kb genomic sequence would require attention over 200,000 positions, demanding enormous memory.</p>
<p>Efficient attention variants address this:</p>
<ul>
<li><strong>Sparse attention</strong>: Attend only to local windows plus global tokens</li>
<li><strong>Linear attention</strong>: Approximate attention with linear complexity</li>
<li><strong>Flash attention</strong>: Exact attention with optimized memory access patterns</li>
</ul>
<p>Models like <em>HyenaDNA</em> use alternative architectures (state space models) to achieve sub-quadratic scaling while maintaining long-range modeling.</p>
</section>
</section>
<section id="sec-apx-a-embeddings" class="level2" data-number="A.6">
<h2 data-number="A.6" class="anchored" data-anchor-id="sec-apx-a-embeddings"><span class="header-section-number">A.6</span> Embeddings and Representations</h2>
<p><strong>Embeddings</strong> are dense vector representations of discrete inputs. Rather than representing a nucleotide as a one-hot vector (A = [1,0,0,0]), an embedding maps it to a learned vector in continuous space (A = [0.2, -0.5, 0.8, …]).</p>
<section id="token-embeddings" class="level3" data-number="A.6.1">
<h3 data-number="A.6.1" class="anchored" data-anchor-id="token-embeddings"><span class="header-section-number">A.6.1</span> Token Embeddings</h3>
<p>The <strong>embedding layer</strong> is a lookup table mapping each token (nucleotide, k-mer, amino acid) to a vector. These embeddings are learned during training, with similar tokens (functionally similar amino acids, for instance) often ending up with similar vectors.</p>
</section>
<section id="contextual-embeddings" class="level3" data-number="A.6.2">
<h3 data-number="A.6.2" class="anchored" data-anchor-id="contextual-embeddings"><span class="header-section-number">A.6.2</span> Contextual Embeddings</h3>
<p>Unlike static embeddings (where each token always has the same representation), transformer outputs are <strong>contextual</strong>: the same token has different representations depending on its context. An alanine in a buried hydrophobic core has a different representation than an alanine on a solvent-exposed surface, because the surrounding context is different.</p>
<p>These contextual embeddings capture rich information about each position’s functional role and can be extracted for downstream tasks.</p>
</section>
</section>
<section id="sec-apx-a-pretraining" class="level2" data-number="A.7">
<h2 data-number="A.7" class="anchored" data-anchor-id="sec-apx-a-pretraining"><span class="header-section-number">A.7</span> Pretraining and Transfer Learning</h2>
<p><strong>Pretraining</strong> trains a model on a large dataset with a self-supervised objective (one that does not require human labels), then <strong>fine-tunes</strong> or adapts the model for specific downstream tasks. This approach leverages abundant unlabeled data to learn general representations.</p>
<section id="self-supervised-objectives" class="level3" data-number="A.7.1">
<h3 data-number="A.7.1" class="anchored" data-anchor-id="self-supervised-objectives"><span class="header-section-number">A.7.1</span> Self-Supervised Objectives</h3>
<p><strong>Masked language modeling (MLM)</strong>: Randomly mask tokens and train the model to predict them from context. Used by <em>BERT</em>, <em>DNABERT</em>, <em>ESM</em>. Captures bidirectional context.</p>
<p><strong>Next-token prediction</strong>: Train the model to predict the next token given all preceding tokens. Used by GPT, <em>HyenaDNA</em>. Enables generation.</p>
<p><strong>Contrastive learning</strong>: Train the model to distinguish related from unrelated examples. Useful for learning representations without reconstruction.</p>
</section>
<section id="transfer-learning" class="level3" data-number="A.7.2">
<h3 data-number="A.7.2" class="anchored" data-anchor-id="transfer-learning"><span class="header-section-number">A.7.2</span> Transfer Learning</h3>
<p>After pretraining, the model can be adapted to downstream tasks:</p>
<ul>
<li><strong>Linear probing</strong>: Freeze pretrained weights, train only a new output layer</li>
<li><strong>Fine-tuning</strong>: Update all or some pretrained weights on labeled data</li>
<li><strong>Parameter-efficient fine-tuning</strong>: Update only small adapter modules</li>
</ul>
<p>See <a href="../part_3/p3-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a> for detailed treatment of transfer learning strategies.</p>
</section>
</section>
<section id="sec-apx-a-practical" class="level2" data-number="A.8">
<h2 data-number="A.8" class="anchored" data-anchor-id="sec-apx-a-practical"><span class="header-section-number">A.8</span> Practical Considerations</h2>
<section id="hardware-requirements" class="level3" data-number="A.8.1">
<h3 data-number="A.8.1" class="anchored" data-anchor-id="hardware-requirements"><span class="header-section-number">A.8.1</span> Hardware Requirements</h3>
<p>Deep learning requires specialized hardware:</p>
<ul>
<li><strong>GPUs</strong>: Graphics processing units optimized for parallel matrix operations</li>
<li><strong>TPUs</strong>: Tensor processing units designed specifically for neural networks</li>
<li><strong>Memory</strong>: Large models require substantial GPU memory (VRAM)</li>
</ul>
<p>Foundation models with billions of parameters require multiple high-end GPUs or TPUs for training; smaller models can run on single consumer GPUs for inference.</p>
</section>
<section id="software-frameworks" class="level3" data-number="A.8.2">
<h3 data-number="A.8.2" class="anchored" data-anchor-id="software-frameworks"><span class="header-section-number">A.8.2</span> Software Frameworks</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Framework</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>PyTorch</strong></td>
<td>Dominant framework; flexible, research-friendly</td>
</tr>
<tr class="even">
<td><strong>TensorFlow</strong></td>
<td>Production-focused; strong deployment tools</td>
</tr>
<tr class="odd">
<td><strong>JAX</strong></td>
<td>Functional approach; used by DeepMind</td>
</tr>
<tr class="even">
<td><strong>HuggingFace</strong></td>
<td>Model hub and high-level training utilities</td>
</tr>
</tbody>
</table>
<p>Most genomic foundation models are implemented in PyTorch and distributed through HuggingFace.</p>
</section>
<section id="common-pitfalls" class="level3" data-number="A.8.3">
<h3 data-number="A.8.3" class="anchored" data-anchor-id="common-pitfalls"><span class="header-section-number">A.8.3</span> Common Pitfalls</h3>
<p><strong>Overfitting</strong>: Model memorizes training data instead of learning generalizable patterns. Detect via validation loss diverging from training loss. Address with regularization, more data, or simpler models.</p>
<p><strong>Underfitting</strong>: Model fails to capture data patterns. Detect via high training loss. Address with larger models, longer training, or better architectures.</p>
<p><strong>Vanishing/exploding gradients</strong>: Gradients become too small or large for stable training. Address with proper initialization, normalization, and residual connections.</p>
<p><strong>Data leakage</strong>: Test data information inadvertently appears in training, inflating performance estimates. Ensure strict separation of training, validation, and test sets.</p>
</section>
</section>
<section id="sec-apx-a-further" class="level2" data-number="A.9">
<h2 data-number="A.9" class="anchored" data-anchor-id="sec-apx-a-further"><span class="header-section-number">A.9</span> Further Reading</h2>
<p>This primer covers only the essentials. For deeper understanding:</p>
<ul>
<li><strong>Fundamentals</strong>: Goodfellow et al., <em>Deep Learning</em> (<a href="app-e-resources.html#sec-apx-e-textbooks" class="quarto-xref"><span>Section E.1</span></a>)</li>
<li><strong>Transformers</strong>: Vaswani et al.&nbsp;(2017), “Attention Is All You Need”</li>
<li><strong>Genomic applications</strong>: Main text chapters, especially <a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a> through <a href="../part_3/p3-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a></li>
<li><strong>Practical tutorials</strong>: fast.ai course, D2L.ai (<a href="app-e-resources.html#sec-apx-e-courses" class="quarto-xref"><span>Section E.2</span></a>)</li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-vaswani_attention_2023" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention <span>Is</span> <span>All</span> <span>You</span> <span>Need</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../bib/references.html" class="pagination-link" aria-label="References">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">References</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../appendix/app-b-compute.html" class="pagination-link" aria-label="Deployment and Compute">
        <span class="nav-page-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>