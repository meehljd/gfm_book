<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>21&nbsp; Graph and Network Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_4/p4-ch22-multi-omics.html" rel="next">
<link href="../part_4/p4-ch20-3d-genome.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_4/p4--multi-modal.html">Part IV: Systems and Scale</a></li><li class="breadcrumb-item"><a href="../part_4/p4-ch21-networks.html"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch12-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch14-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch15-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch16-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch17-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--multi-modal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Systems and Scale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch19-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch20-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch21-networks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch22-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch25-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Causal Inference with Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch26-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch28-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch29-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch30-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch31-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch21-biological-networks" id="toc-sec-ch21-biological-networks" class="nav-link active" data-scroll-target="#sec-ch21-biological-networks"><span class="header-section-number">21.1</span> Biological Networks and Data Resources</a>
  <ul class="collapse">
  <li><a href="#sec-ch21-landscape" id="toc-sec-ch21-landscape" class="nav-link" data-scroll-target="#sec-ch21-landscape"><span class="header-section-number">21.1.1</span> Landscape of Biological Graphs</a></li>
  <li><a href="#sec-ch21-network-biases" id="toc-sec-ch21-network-biases" class="nav-link" data-scroll-target="#sec-ch21-network-biases"><span class="header-section-number">21.1.2</span> Biases and Limitations</a></li>
  </ul></li>
  <li><a href="#sec-ch21-gnn-fundamentals" id="toc-sec-ch21-gnn-fundamentals" class="nav-link" data-scroll-target="#sec-ch21-gnn-fundamentals"><span class="header-section-number">21.2</span> Graph Neural Network Fundamentals</a>
  <ul class="collapse">
  <li><a href="#sec-ch21-message-passing" id="toc-sec-ch21-message-passing" class="nav-link" data-scroll-target="#sec-ch21-message-passing"><span class="header-section-number">21.2.1</span> Message Passing Principles</a></li>
  <li><a href="#sec-ch21-canonical-architectures" id="toc-sec-ch21-canonical-architectures" class="nav-link" data-scroll-target="#sec-ch21-canonical-architectures"><span class="header-section-number">21.2.2</span> Canonical Architectures</a></li>
  </ul></li>
  <li><a href="#sec-ch21-fm-embeddings" id="toc-sec-ch21-fm-embeddings" class="nav-link" data-scroll-target="#sec-ch21-fm-embeddings"><span class="header-section-number">21.3</span> Foundation Model Embeddings as Node Features</a>
  <ul class="collapse">
  <li><a href="#sec-ch21-integration-principle" id="toc-sec-ch21-integration-principle" class="nav-link" data-scroll-target="#sec-ch21-integration-principle"><span class="header-section-number">21.3.1</span> Integration Principle</a></li>
  <li><a href="#sec-ch21-practical-patterns" id="toc-sec-ch21-practical-patterns" class="nav-link" data-scroll-target="#sec-ch21-practical-patterns"><span class="header-section-number">21.3.2</span> Practical Integration Patterns</a></li>
  <li><a href="#sec-ch21-integration-evidence" id="toc-sec-ch21-integration-evidence" class="nav-link" data-scroll-target="#sec-ch21-integration-evidence"><span class="header-section-number">21.3.3</span> Evidence for the Integration Benefit</a></li>
  </ul></li>
  <li><a href="#sec-ch21-applications" id="toc-sec-ch21-applications" class="nav-link" data-scroll-target="#sec-ch21-applications"><span class="header-section-number">21.4</span> Applications</a>
  <ul class="collapse">
  <li><a href="#sec-ch21-disease-gene" id="toc-sec-ch21-disease-gene" class="nav-link" data-scroll-target="#sec-ch21-disease-gene"><span class="header-section-number">21.4.1</span> Disease Gene Prioritization</a></li>
  <li><a href="#sec-ch21-drug-target" id="toc-sec-ch21-drug-target" class="nav-link" data-scroll-target="#sec-ch21-drug-target"><span class="header-section-number">21.4.2</span> Drug-Target Interaction Prediction</a></li>
  <li><a href="#sec-ch21-kg-reasoning" id="toc-sec-ch21-kg-reasoning" class="nav-link" data-scroll-target="#sec-ch21-kg-reasoning"><span class="header-section-number">21.4.3</span> Knowledge Graph Reasoning and Drug Repurposing</a></li>
  <li><a href="#sec-ch21-pathway-analysis" id="toc-sec-ch21-pathway-analysis" class="nav-link" data-scroll-target="#sec-ch21-pathway-analysis"><span class="header-section-number">21.4.4</span> Pathway and Module Analysis</a></li>
  <li><a href="#sec-ch21-cell-annotation" id="toc-sec-ch21-cell-annotation" class="nav-link" data-scroll-target="#sec-ch21-cell-annotation"><span class="header-section-number">21.4.5</span> Cell Type and State Annotation</a></li>
  </ul></li>
  <li><a href="#sec-ch21-practical" id="toc-sec-ch21-practical" class="nav-link" data-scroll-target="#sec-ch21-practical"><span class="header-section-number">21.5</span> Practical Considerations</a>
  <ul class="collapse">
  <li><a href="#sec-ch21-graph-construction" id="toc-sec-ch21-graph-construction" class="nav-link" data-scroll-target="#sec-ch21-graph-construction"><span class="header-section-number">21.5.1</span> Graph Construction Quality</a></li>
  <li><a href="#sec-ch21-scalability" id="toc-sec-ch21-scalability" class="nav-link" data-scroll-target="#sec-ch21-scalability"><span class="header-section-number">21.5.2</span> Scalability and Mini-Batching</a></li>
  <li><a href="#sec-ch21-robustness" id="toc-sec-ch21-robustness" class="nav-link" data-scroll-target="#sec-ch21-robustness"><span class="header-section-number">21.5.3</span> Robustness to Noise and Missingness</a></li>
  <li><a href="#sec-ch21-interpretation" id="toc-sec-ch21-interpretation" class="nav-link" data-scroll-target="#sec-ch21-interpretation"><span class="header-section-number">21.5.4</span> Interpretation and Validation</a></li>
  </ul></li>
  <li><a href="#sec-ch21-limitations" id="toc-sec-ch21-limitations" class="nav-link" data-scroll-target="#sec-ch21-limitations"><span class="header-section-number">21.6</span> Limitations and Open Challenges</a>
  <ul class="collapse">
  <li><a href="#sec-ch21-study-bias" id="toc-sec-ch21-study-bias" class="nav-link" data-scroll-target="#sec-ch21-study-bias"><span class="header-section-number">21.6.1</span> Study Bias Problem</a></li>
  <li><a href="#sec-ch21-causality" id="toc-sec-ch21-causality" class="nav-link" data-scroll-target="#sec-ch21-causality"><span class="header-section-number">21.6.2</span> Causality Versus Association</a></li>
  <li><a href="#sec-ch21-negative-data" id="toc-sec-ch21-negative-data" class="nav-link" data-scroll-target="#sec-ch21-negative-data"><span class="header-section-number">21.6.3</span> Negative Data and Class Imbalance</a></li>
  <li><a href="#sec-ch21-distribution-shift" id="toc-sec-ch21-distribution-shift" class="nav-link" data-scroll-target="#sec-ch21-distribution-shift"><span class="header-section-number">21.6.4</span> Distribution Shift</a></li>
  </ul></li>
  <li><a href="#sec-ch21-conclusion" id="toc-sec-ch21-conclusion" class="nav-link" data-scroll-target="#sec-ch21-conclusion"><span class="header-section-number">21.7</span> Sequence Encodes, Structure Connects</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_4/p4--multi-modal.html">Part IV: Systems and Scale</a></li><li class="breadcrumb-item"><a href="../part_4/p4-ch21-networks.html"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch21-networks" class="quarto-section-identifier"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Prerequisites:</strong> This chapter builds on foundation model concepts from Part 3 (especially protein language models in <a href="../part_3/p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a> and DNA models in <a href="../part_3/p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>) and attention mechanisms from <a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>. Familiarity with single-cell representations (<a href="p4-ch19-single-cell.html" class="quarto-xref"><span>Chapter 19</span></a>) is helpful but not required.</p>
<p><strong>Learning Objectives:</strong> After completing this chapter, you should be able to:</p>
<ol type="1">
<li>Explain why graph neural networks complement rather than replace sequence-based foundation models</li>
<li>Describe the message passing framework and how it propagates information across network neighborhoods</li>
<li>Compare the design tradeoffs among GCN, GraphSAGE, GAT, and graph transformer architectures</li>
<li>Design an integration strategy combining foundation model embeddings with GNN layers for a given biological task</li>
<li>Evaluate the limitations of network-based predictions, including ascertainment bias and causality concerns</li>
</ol>
<p><strong>Estimated Reading Time:</strong> 45-55 minutes</p>
</div>
</div>
<p>Graph neural networks are not alternatives to foundation models; they are consumers of them. Foundation models produce rich representations of individual biological entities: protein language models encode evolutionary constraint and structural propensity (<a href="../part_3/p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a>), DNA models capture regulatory grammar (<a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>), RNA models represent transcript-level features (<a href="p4-ch18-rna.html" class="quarto-xref"><span>Chapter 18</span></a>). These representations are powerful but operate on isolated sequences. A protein embedding captures what <em>ESM</em> learned about that protein’s sequence; it says nothing about which other proteins it binds, which pathways it participates in, or which disease phenotypes result from its disruption. Graph neural networks operate at a higher level of abstraction, taking foundation model representations as node features and learning to propagate information across relational structure. The combination yields capabilities that neither approach achieves alone.</p>
<p>This architectural relationship reflects a biological reality: organisms are not collections of independent molecules but systems of interacting components. A <strong>transcription factor</strong> affects its target genes through regulatory edges. Proteins assemble into functional complexes through physical binding. Signaling cascades propagate perturbations across cellular networks. These relationships exist at a level of abstraction above sequence, requiring a different mathematical framework to represent. Graphs provide precisely this framework. In a protein-protein interaction network, proteins become nodes and physical binding creates edges. In a <strong>gene regulatory network</strong>, directed edges connect transcription factors to their targets. In spatial transcriptomics data, cells become nodes with edges capturing physical proximity. Each graph encodes relational structure that sequence models cannot directly capture.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: The Division of Labor
</div>
</div>
<div class="callout-body-container callout-body">
<p>Think of the relationship between foundation models and GNNs as a division of labor: foundation models answer “What can this protein do?” based on its sequence, while GNNs answer “What does this protein do in context?” based on its network neighborhood. Neither question subsumes the other. A transcription factor’s DNA-binding properties (captured by sequence models) matter, but so does which genes it actually regulates in a given cell type (captured by network structure).</p>
</div>
</div>
<p>The practical implications are substantial. Disease gene prioritization leverages the observation that genes causing similar diseases cluster in network neighborhoods. A GNN can learn to propagate disease signals across protein interaction networks, but effectiveness depends critically on node feature quality. When those features come from protein language models encoding evolutionary constraint and structural propensity, the GNN gains access to sequence-level biological knowledge unavailable from simpler features like expression levels alone. Drug-target interaction prediction similarly benefits: <em>ESM</em> <strong>embeddings</strong> capture what makes a protein druggable, while network context reveals which targets sit in therapeutically relevant pathways.</p>
<section id="sec-ch21-biological-networks" class="level2" data-number="21.1">
<h2 data-number="21.1" class="anchored" data-anchor-id="sec-ch21-biological-networks"><span class="header-section-number">21.1</span> Biological Networks and Data Resources</h2>
<p>Graph neural networks can only learn from relationships encoded in their input graphs. The choice of network, its source, and its inherent biases determine what a model can discover and what it will miss. Understanding the landscape of available biological networks, their construction methods, and their systematic limitations is therefore prerequisite to effective graph-based modeling.</p>
<section id="sec-ch21-landscape" class="level3" data-number="21.1.1">
<h3 data-number="21.1.1" class="anchored" data-anchor-id="sec-ch21-landscape"><span class="header-section-number">21.1.1</span> Landscape of Biological Graphs</h3>
<p>Before examining <strong>graph neural network</strong> architectures, it is essential to understand what biological networks exist and where they come from. The choice of network fundamentally shapes what a model can learn, and the biases inherent in network construction propagate through all downstream analyses.</p>
<p>Physical associations between proteins constitute perhaps the most widely used network type for GNN applications. Major databases include STRING, which integrates experimental data with computational predictions and text mining to assign confidence scores to interactions; BioGRID, which focuses on curated experimental interactions; and IntAct, which provides detailed interaction metadata from direct molecular experiments. These <strong>protein-protein interaction networks</strong> are incomplete (current estimates suggest only 20-30% of human PPIs are catalogued) and biased toward well-studied proteins in well-characterized pathways <span class="citation" data-cites="szklarczyk2023string oughtred2021biogrid orchard2014intact venkatesan2009protein hart2006completeness">(<a href="../bib/references.html#ref-szklarczyk2023string" role="doc-biblioref"><strong>szklarczyk2023string?</strong></a>; <a href="../bib/references.html#ref-oughtred2021biogrid" role="doc-biblioref"><strong>oughtred2021biogrid?</strong></a>; <a href="../bib/references.html#ref-orchard2014intact" role="doc-biblioref"><strong>orchard2014intact?</strong></a>; <a href="../bib/references.html#ref-venkatesan2009protein" role="doc-biblioref"><strong>venkatesan2009protein?</strong></a>; <a href="../bib/references.html#ref-hart2006completeness" role="doc-biblioref"><strong>hart2006completeness?</strong></a>)</span>. A gene involved in cancer or a common disease may have hundreds of documented interactions, while an uncharacterized protein in a specialized tissue may have none, not because it lacks interactions but because no one has looked.</p>
<p>Transcriptional control relationships require a different network structure. Unlike PPIs, gene regulatory networks are inherently directed: a transcription factor activates or represses its targets, not vice versa. Sources include <strong>chromatin immunoprecipitation sequencing (ChIP-seq)</strong> experiments that identify transcription factor binding sites, <strong>chromatin accessibility</strong> data (assay for transposase-accessible chromatin sequencing (ATAC-seq), DNase-seq) that reveals active regulatory regions, and chromosome conformation capture (Hi-C) that maps enhancer-promoter contacts (<a href="p4-ch20-3d-genome.html" class="quarto-xref"><span>Chapter 20</span></a>). Databases like ENCODE and the Roadmap Epigenomics Project provide <strong>regulatory annotations</strong> across cell types, though coverage varies dramatically by tissue (<a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a>). Computational methods infer regulatory edges from expression correlations or sequence motifs, but such predictions contain substantial false positives and miss context-specific interactions.</p>
<p>Organized biochemical knowledge takes yet another form. KEGG, Reactome, and WikiPathways curate reactions, enzymatic steps, and signaling cascades into hierarchical <strong>pathway and metabolic networks</strong> where nodes can represent genes, proteins, metabolites, or abstract pathway concepts. These networks encode decades of molecular biology knowledge but reflect historical research priorities: metabolism and signal transduction are well-characterized, while more recently discovered processes like autophagy or RNA modification have sparser coverage.</p>
<p>Beyond molecular interactions, relationships among genes, diseases, drugs, phenotypes, and other biomedical entities require heterogeneous representations. Unlike protein interaction networks, which contain a single node type and edge type, <strong>knowledge graphs</strong> are inherently heterogeneous: nodes represent diverse entity classes, and edges capture semantically distinct relationship types. This heterogeneity enables richer reasoning but demands architectures capable of handling multiple node and edge embeddings.</p>
<p>Several large-scale biomedical knowledge graphs have become standard resources. Hetionet integrates 47,031 nodes across 11 types (genes, diseases, compounds, anatomies, and others) with 2.25 million edges spanning 24 relationship types, providing a comprehensive substrate for computational drug repurposing <span class="citation" data-cites="himmelstein2017systematic">(<a href="../bib/references.html#ref-himmelstein2017systematic" role="doc-biblioref"><strong>himmelstein2017systematic?</strong></a>)</span>. The Unified Medical Language System (UMLS) aggregates over 200 biomedical vocabularies into a metathesaurus linking millions of concepts through hierarchical and associative relationships. PrimeKG consolidates 17 biological databases into a precision medicine knowledge graph with over 4 million relationships connecting diseases, drugs, genes, pathways, and phenotypes, explicitly designed for machine learning applications <span class="citation" data-cites="chandak2023primekg">(<a href="../bib/references.html#ref-chandak2023primekg" role="doc-biblioref"><strong>chandak2023primekg?</strong></a>)</span>.</p>
<p>Disease-gene association databases provide critical edges for clinical applications. DisGeNET curates over one million gene-disease associations from expert-reviewed sources, GWAS catalogs (<a href="../part_1/p1-ch03-gwas.html" class="quarto-xref"><span>Chapter 3</span></a>), and text mining, assigning evidence scores that enable confidence-based filtering <span class="citation" data-cites="pinero2020disgenet">(<a href="../bib/references.html#ref-pinero2020disgenet" role="doc-biblioref"><strong>pinero2020disgenet?</strong></a>)</span>. OMIM (Online Mendelian Inheritance in Man) provides authoritative curation of Mendelian disease genes, while OrphaNet focuses on rare diseases with detailed phenotypic annotations (<a href="../part_6/p6-ch28-rare-disease.html" class="quarto-xref"><span>Chapter 28</span></a>). The Clinical Genome Resource (ClinGen) adds expert-reviewed <strong>gene-disease validity</strong> assessments using standardized evidence frameworks.</p>
<p>Drug-centric resources complete the translational picture. DrugBank provides comprehensive drug-target annotations with mechanism and pharmacology details. ChEMBL aggregates bioactivity data from medicinal chemistry literature, linking compounds to protein targets through binding affinity measurements. The Drug Gene Interaction Database (DGIdb) consolidates druggable gene categories and known interactions to support target prioritization (<a href="../part_6/p6-ch29-drug-discovery.html" class="quarto-xref"><span>Chapter 30</span></a>).</p>
<div id="tbl-network-types" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-network-types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;21.1: Major biological network types and their characteristics. The choice of network determines what a GNN can learn and what biases it inherits.
</figcaption>
<div aria-describedby="tbl-network-types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 25%">
<col style="width: 15%">
<col style="width: 21%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Network Type</th>
<th>Example Databases</th>
<th>Node Types</th>
<th>Edge Semantics</th>
<th>Key Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Protein-Protein Interaction</td>
<td>STRING, BioGRID, IntAct</td>
<td>Proteins</td>
<td>Physical binding, co-complex</td>
<td>20-30% coverage; study bias</td>
</tr>
<tr class="even">
<td>Gene Regulatory</td>
<td>ENCODE, Roadmap, JASPAR</td>
<td>TFs, genes, enhancers</td>
<td>Activation/repression (directed)</td>
<td>Cell-type specificity</td>
</tr>
<tr class="odd">
<td>Pathway/Metabolic</td>
<td>KEGG, Reactome, WikiPathways</td>
<td>Genes, metabolites, reactions</td>
<td>Enzymatic, signaling</td>
<td>Historical research bias</td>
</tr>
<tr class="even">
<td>Knowledge Graph</td>
<td>Hetionet, PrimeKG, UMLS</td>
<td>Multiple entity types</td>
<td>Multiple relationship types</td>
<td>Integration quality varies</td>
</tr>
<tr class="odd">
<td>Spatial/Cell-Cell</td>
<td>Spatial transcriptomics data</td>
<td>Cells, spots</td>
<td>Proximity, communication</td>
<td>Emerging; sparse coverage</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-biological-networks" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-biological-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/01-A-fig-biological-networks.svg" class="img-fluid figure-img"></p>
<figcaption>PPI networks: undirected physical binding</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/01-B-fig-biological-networks.svg" class="img-fluid figure-img"></p>
<figcaption>Gene regulatory networks: directed TF-target relationships</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/01-C-fig-biological-networks.svg" class="img-fluid figure-img"></p>
<figcaption>Knowledge graphs: heterogeneous multi-type nodes and edges</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/01-D-fig-biological-networks.svg" class="img-fluid figure-img"></p>
<figcaption>Spatial graphs: cell proximity from spatial transcriptomics</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-biological-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.1: Landscape of biological networks. (A) Protein-protein interaction networks: undirected edges represent physical binding, with only 20-30% of interactions currently catalogued. (B) Gene regulatory networks: directed edges from transcription factors to targets, derived from ChIP-seq and accessibility data. (C) Knowledge graphs: heterogeneous graphs with multiple node types (genes, diseases, drugs, pathways) and relationship types enabling multi-hop reasoning. (D) Spatial and cell-cell interaction graphs: emerging from spatial transcriptomics, encoding tissue architecture and cell communication.
</figcaption>
</figure>
</div>
<p>The power of knowledge graphs lies in their support for multi-hop reasoning. A query asking whether a drug might treat a disease can traverse multiple edge types: drug inhibits protein A, protein A interacts with protein B, protein B is implicated in disease. Each hop contributes evidence, and the combination of paths provides signal that no single edge contains. Graph neural networks learn to aggregate across such paths, weighting different relationship types and path lengths according to their predictive value for specific tasks.</p>
<p>Spatially resolved transcriptomics and imaging data give rise to graphs capturing tissue organization invisible to bulk or even single-cell measurements (<a href="p4-ch19-single-cell.html" class="quarto-xref"><span>Chapter 19</span></a>). In these <strong>spatial and cell-cell interaction graphs</strong>, nodes represent cells or spatial locations, while edges encode physical proximity or inferred ligand-receptor communication. Such graphs enable questions about how spatial context influences cell behavior.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a protein that has no documented interactions in STRING or BioGRID. Does this mean it truly has no binding partners? What factors might explain why some proteins have hundreds of documented interactions while others have none? How might this asymmetry affect a GNN trained on such networks?</p>
</div>
</div>
</section>
<section id="sec-ch21-network-biases" class="level3" data-number="21.1.2">
<h3 data-number="21.1.2" class="anchored" data-anchor-id="sec-ch21-network-biases"><span class="header-section-number">21.1.2</span> Biases and Limitations</h3>
<p>All biological networks share systematic biases that affect downstream modeling. Well-studied genes appear as highly connected hubs not necessarily because they have more interactions but because researchers have investigated them more thoroughly. This <strong>ascertainment bias</strong> means that GNNs trained on network structure may primarily learn to propagate signals toward well-characterized genes, potentially missing novel biology in peripheral network regions.</p>
<p>Network incompleteness creates particular challenges for message passing algorithms. If a critical interaction is missing, information cannot flow across that gap. If a spurious interaction is present, noise propagates where it should not. These issues are especially acute for less-studied organisms, tissues, or disease contexts where network coverage is sparse.</p>
<p>The distinction between physical and functional associations matters for interpretation. A protein-protein interaction might represent stable complex membership, transient signaling, or indirect association through shared binding partners. Different edge types may warrant different treatment by graph models, but many databases conflate these categories or provide insufficient metadata to distinguish them.</p>
</section>
</section>
<section id="sec-ch21-gnn-fundamentals" class="level2" data-number="21.2">
<h2 data-number="21.2" class="anchored" data-anchor-id="sec-ch21-gnn-fundamentals"><span class="header-section-number">21.2</span> Graph Neural Network Fundamentals</h2>
<p>The mathematical machinery underlying graph neural networks differs fundamentally from the architectures examined in previous chapters. Where convolutional and transformer models operate on regular structures (sequences, grids), GNNs must handle irregular topology with variable-degree nodes, no inherent ordering, and arbitrary connectivity (<a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>, <a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>). This section develops the message passing framework that addresses these challenges, then surveys the canonical architectures that have become standard tools for biological applications.</p>
<section id="sec-ch21-message-passing" class="level3" data-number="21.2.1">
<h3 data-number="21.2.1" class="anchored" data-anchor-id="sec-ch21-message-passing"><span class="header-section-number">21.2.1</span> Message Passing Principles</h3>
<p>The challenge of learning from graph-structured data lies in the irregular topology: unlike images (regular grids) or sequences (linear chains), graphs have variable-degree nodes, no inherent ordering, and complex connectivity patterns. Classical approaches computed hand-crafted features such as degree centrality, clustering coefficients, or shortest path statistics, then fed these to standard machine learning models. Such features capture useful properties but cannot adapt to task-specific patterns.</p>
<p><strong>Message passing</strong> provides a learnable alternative. The core intuition is local information exchange: each node should update its representation based on what its neighbors know. By iterating this process across multiple layers, information propagates across the graph, allowing nodes to incorporate signals from increasingly distant parts of the network.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Message Passing as Neural Diffusion
</div>
</div>
<div class="callout-body-container callout-body">
<p>Think of message passing as a controlled diffusion process. Just as heat diffuses from hot regions to cold ones, information in a GNN flows from nodes to their neighbors. After one layer, each node knows about its immediate neighbors. After two layers, it knows about neighbors of neighbors. After L layers, information has spread across L-hop neighborhoods. The learned weights control <em>how</em> information mixes, not just <em>that</em> it spreads.</p>
</div>
</div>
<p>Formally, at layer <span class="math inline">\(\ell\)</span>, each node <span class="math inline">\(i\)</span> maintains a hidden state <span class="math inline">\(\mathbf{h}_i^{(\ell)}\)</span>. A message passing layer computes, for each edge from neighbor <span class="math inline">\(j\)</span> to node <span class="math inline">\(i\)</span>, a message:</p>
<p><span class="math display">\[
\mathbf{m}_{ij}^{(\ell)} = \phi_m\left(\mathbf{h}_i^{(\ell)}, \mathbf{h}_j^{(\ell)}, \mathbf{e}_{ij}\right)
\]</span></p>
<p>where <span class="math inline">\(\phi_m\)</span> is a learned function and <span class="math inline">\(\mathbf{e}_{ij}\)</span> represents edge features. The node then aggregates messages from all neighbors and updates its state:</p>
<p><span class="math display">\[
\mathbf{h}_i^{(\ell+1)} = \phi_h\left(\mathbf{h}_i^{(\ell)}, \bigoplus_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}^{(\ell)}\right)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{N}(i)\)</span> denotes neighbors of node <span class="math inline">\(i\)</span> and <span class="math inline">\(\bigoplus\)</span> is a permutation-invariant aggregation (sum, mean, max, or attention-weighted combination). The aggregation must be permutation-invariant because neighbors have no inherent ordering.</p>
<div id="fig-message-passing" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-message-passing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/02-A-fig-message-passing.svg" class="img-fluid figure-img"></p>
<figcaption>Initial node features from foundation model embeddings</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/02-B-fig-message-passing.svg" class="img-fluid figure-img"></p>
<figcaption>Message computation along each edge</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/02-C-fig-message-passing.svg" class="img-fluid figure-img"></p>
<figcaption>Aggregation combines neighbor information</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/02-D-fig-message-passing.svg" class="img-fluid figure-img"></p>
<figcaption>After L layers, nodes capture L-hop neighborhood context</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-message-passing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.2: Message passing in graph neural networks. (A) Initial state: each node has feature vectors from foundation model embeddings. (B) Message computation: for each edge, compute message m_ij = φ(h_i, h_j, e_ij). (C) Aggregation: each node aggregates incoming messages using permutation-invariant operations (sum, mean, max, or attention). (D) After L layers: each node’s embedding incorporates information from its L-hop neighborhood, capturing pathway-level context.
</figcaption>
</figure>
</div>
<p>After <span class="math inline">\(L\)</span> layers, a node’s representation incorporates information from all nodes within <span class="math inline">\(L\)</span> hops. For biological networks, this means a gene’s learned embedding can reflect not only its own features but signals from interaction partners, their partners, and so on, capturing pathway-level and module-level context.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading further, test your understanding of message passing:</p>
<ol type="1">
<li>If a GNN has 3 message passing layers, how many hops away can information travel from any given node?</li>
<li>Why must the aggregation function be permutation-invariant?</li>
<li>What happens to node representations if you stack many message passing layers without any mechanism to prevent it?</li>
</ol>
<p><em>Answers: (1) 3 hops; (2) Because graph neighbors have no inherent ordering, unlike sequence positions; (3) They converge (over-smoothing), losing discriminative signal.</em></p>
</div>
</div>
</section>
<section id="sec-ch21-canonical-architectures" class="level3" data-number="21.2.2">
<h3 data-number="21.2.2" class="anchored" data-anchor-id="sec-ch21-canonical-architectures"><span class="header-section-number">21.2.2</span> Canonical Architectures</h3>
<p>Several GNN architectures have become standard tools for biological applications, each with distinct design choices that reflect different tradeoffs between computational efficiency, expressive power, and scalability.</p>
<p>The simplest approach performs normalized neighborhood averaging followed by linear transformation and nonlinearity. <strong>Graph convolutional networks (GCN)</strong> are computationally efficient and conceptually straightforward but suffer from <strong>over-smoothing</strong> when stacked deeply: repeated averaging causes node representations to converge, losing the discriminative signal that distinguishes different network positions <span class="citation" data-cites="li2018deeper oono2019graph">(<a href="../bib/references.html#ref-li2018deeper" role="doc-biblioref"><strong>li2018deeper?</strong></a>; <a href="../bib/references.html#ref-oono2019graph" role="doc-biblioref"><strong>oono2019graph?</strong></a>)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Difficulty Warning: Over-Smoothing
</div>
</div>
<div class="callout-body-container callout-body">
<p>The over-smoothing problem is subtle but critical. Intuitively, if you repeatedly average a node’s representation with its neighbors, eventually all nodes in a connected component converge to similar representations. This means deeper GNNs are not always better. In practice, most GNNs use only 2-4 layers. Understanding when and why to limit depth is essential for effective GNN design.</p>
</div>
</div>
<p>Scalability to large graphs requires a different strategy. <em>GraphSAGE</em> learns aggregation functions that operate on sampled neighborhoods rather than the full neighbor set <span class="citation" data-cites="hamilton2017inductive">(<a href="../bib/references.html#ref-hamilton2017inductive" role="doc-biblioref"><strong>hamilton2017inductive?</strong></a>)</span>. This enables mini-batch training on large graphs and provides inductive capability: the model can generate embeddings for nodes not seen during training by applying learned aggregators to their neighborhoods. For biological networks that grow as new genes are characterized, this generalization is valuable.</p>
<p>When some neighbors matter more than others, attention-weighted aggregation provides a learnable solution. <strong>Graph attention networks (GAT)</strong> compute attention scores between each node and its neighbors, allowing the model to focus on the most informative interactions <span class="citation" data-cites="velickovic2018graph">(<a href="../bib/references.html#ref-velickovic2018graph" role="doc-biblioref"><strong>velickovic2018graph?</strong></a>)</span>. This is analogous to attention in transformers but operates over graph neighborhoods rather than sequence positions (<a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>).</p>
<p>Finally, the boundary between sequence and graph models blurs when transformer architectures extend to graphs. <strong>Graph transformers</strong> replace local message passing with structured or global attention. Some variants attend over all node pairs with <strong>positional encodings</strong> derived from graph structure (shortest paths, Laplacian eigenvectors); others restrict attention to k-hop neighborhoods <span class="citation" data-cites="ying2021graphormer dwivedi2021graph">(<a href="../bib/references.html#ref-ying2021graphormer" role="doc-biblioref"><strong>ying2021graphormer?</strong></a>; <a href="../bib/references.html#ref-dwivedi2021graph" role="doc-biblioref"><strong>dwivedi2021graph?</strong></a>)</span>. These architectures potentially capture long-range dependencies that multi-layer message passing struggles to propagate.</p>
<div id="tbl-gnn-architectures" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-gnn-architectures-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;21.2: Comparison of canonical GNN architectures. The choice depends on graph size, whether inductive inference is needed, and computational budget.
</figcaption>
<div aria-describedby="tbl-gnn-architectures-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 14%">
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Architecture</th>
<th>Aggregation Method</th>
<th>Scalability</th>
<th>Inductive?</th>
<th>Key Strength</th>
<th>Key Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GCN</td>
<td>Normalized mean</td>
<td>Limited (full-batch)</td>
<td>No</td>
<td>Simple, efficient</td>
<td>Over-smoothing with depth</td>
</tr>
<tr class="even">
<td>GraphSAGE</td>
<td>Sampled aggregators</td>
<td>High (mini-batch)</td>
<td>Yes</td>
<td>Scales to large graphs</td>
<td>Sampling variance</td>
</tr>
<tr class="odd">
<td>GAT</td>
<td>Attention-weighted</td>
<td>Moderate</td>
<td>Yes</td>
<td>Learns edge importance</td>
<td>Quadratic in neighbors</td>
</tr>
<tr class="even">
<td>Graph Transformer</td>
<td>Global/structured attention</td>
<td>Variable</td>
<td>Yes</td>
<td>Long-range dependencies</td>
<td>Computational cost</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The expressiveness of GNNs is bounded by their ability to distinguish different graph structures. Theoretical analysis connects standard message passing to the Weisfeiler-Lehman graph isomorphism test, revealing that certain graph structures remain indistinguishable regardless of the number of layers <span class="citation" data-cites="xu2019how morris2019weisfeiler">(<a href="../bib/references.html#ref-xu2019how" role="doc-biblioref"><strong>xu2019how?</strong></a>; <a href="../bib/references.html#ref-morris2019weisfeiler" role="doc-biblioref"><strong>morris2019weisfeiler?</strong></a>)</span>. For most biological applications, this theoretical limitation is less constraining than practical issues of data quality, training efficiency, and <strong>interpretability</strong> (<a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 25</span></a>).</p>
</section>
</section>
<section id="sec-ch21-fm-embeddings" class="level2" data-number="21.3">
<h2 data-number="21.3" class="anchored" data-anchor-id="sec-ch21-fm-embeddings"><span class="header-section-number">21.3</span> Foundation Model Embeddings as Node Features</h2>
<p>The power of combining foundation models with graph neural networks lies in their complementary strengths. Foundation models extract rich biological information from sequence, but they operate on isolated entities without relational context. Graph neural networks reason over relationships, but they require informative node features to propagate meaningful signal. This section examines how to integrate these approaches effectively, from the architectural principle underlying the combination to practical patterns for implementation.</p>
<section id="sec-ch21-integration-principle" class="level3" data-number="21.3.1">
<h3 data-number="21.3.1" class="anchored" data-anchor-id="sec-ch21-integration-principle"><span class="header-section-number">21.3.1</span> Integration Principle</h3>
<p>The central architectural insight for genomic graph learning is that foundation models and graph neural networks operate at complementary levels of abstraction. Sequence-based foundation models excel at extracting biological information from linear sequences: <em>ESM-2</em> learns <strong>evolutionary constraints</strong> and structural propensities from protein sequences (<a href="../part_3/p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a>); <em>DNABERT</em> and its successors capture regulatory motifs and sequence grammar (<a href="../part_3/p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>); single-cell foundation models like <em>scGPT</em> learn cell state representations from expression profiles (<a href="p4-ch19-single-cell.html" class="quarto-xref"><span>Chapter 19</span></a>). These representations encode rich biological knowledge but operate on individual entities without explicit relational information. The principles of feature extraction from pretrained models, which underpin this integration pattern, are developed in <a href="../part_2/p2-ch09-transfer.html#sec-ch09-feature-extraction" class="quarto-xref"><span>Section 9.3</span></a>.</p>
<div id="fig-gnn-integration" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gnn-integration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/03-A-fig-gnn-integration.svg" class="img-fluid figure-img"></p>
<figcaption>Foundation models produce rich entity representations</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/03-B-fig-gnn-integration.svg" class="img-fluid figure-img"></p>
<figcaption>Biological networks encode relational structure</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/03-C-fig-gnn-integration.svg" class="img-fluid figure-img"></p>
<figcaption>GNNs integrate embeddings with network structure</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/03-D-fig-gnn-integration.svg" class="img-fluid figure-img"></p>
<figcaption>Combined capabilities exceed either approach alone</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gnn-integration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.3: The foundation model + GNN integration paradigm. (A) Foundation models produce rich representations of individual entities: protein embeddings from ESM, DNA embeddings from DNABERT, cell embeddings from scGPT. (B) Biological networks encode relational structure that sequence models cannot capture. (C) GNNs integrate foundation model embeddings with network structure through message passing. (D) This combination enables capabilities neither achieves alone: disease gene prioritization, drug-target prediction, and perturbation response modeling.
</figcaption>
</figure>
</div>
<p>Graph neural networks excel at learning from relational structure but require informative node features to propagate. When node features are uninformative (simple one-hot encodings or scalar expression values), message passing can only learn from network topology. When node features carry substantial biological signal, message passing can refine and contextualize that information based on network position.</p>
<p>Combining these approaches follows a natural two-stage pattern. First, apply a foundation model to each entity in the graph to generate initial node embeddings. For a protein-protein interaction network, run <em>ESM-2</em> on each protein sequence; for a gene regulatory network, use DNA embeddings for regulatory elements and protein embeddings for transcription factors; for a cell graph, apply <em>scGPT</em> to generate cell state representations. Second, train a GNN on these embeddings using the biological graph structure, allowing message passing to integrate entity-level representations with relational context.</p>
<p>This combination yields capabilities that neither component achieves alone. The foundation model provides rich, transferable features that would require massive labeled datasets to learn from scratch. The GNN provides relational reasoning that sequence models cannot perform. A protein’s druggability depends both on intrinsic properties (binding pocket geometry, expression pattern) that <em>ESM</em> captures and on network context (pathway position, interaction partners) that the GNN integrates.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine you are building a model to predict which genes are essential for cancer cell survival. You have: - <em>ESM-2</em> embeddings for each protein - A protein-protein interaction network from STRING - CRISPR knockout screens identifying essential genes in 10 cancer cell lines</p>
<p>Sketch out your approach. Would you freeze the <em>ESM-2</em> embeddings or fine-tune them? How many GNN layers would you use? What might go wrong if you only used the network structure without the sequence embeddings?</p>
</div>
</div>
</section>
<section id="sec-ch21-practical-patterns" class="level3" data-number="21.3.2">
<h3 data-number="21.3.2" class="anchored" data-anchor-id="sec-ch21-practical-patterns"><span class="header-section-number">21.3.2</span> Practical Integration Patterns</h3>
<p>Several integration patterns have emerged in practice, each suited to different constraints and objectives. The simplest approach freezes foundation model weights and treats embeddings as fixed features, training only the GNN layers. This is computationally efficient and prevents <strong>catastrophic forgetting</strong> of pretrained knowledge but limits the model’s ability to adapt representations to the specific task (<a href="../part_2/p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>).</p>
<p>When sufficient task-specific data is available, allowing gradients to flow through both the GNN and (parts of) the foundation model enables end-to-end optimization. This <strong>joint fine-tuning</strong> typically requires careful learning rate scheduling, with smaller updates to foundation model parameters and larger updates to GNN layers. The approach can improve performance but risks <strong>overfitting</strong> and requires substantially more computation.</p>
<p>A middle ground inserts small trainable modules between foundation model layers or at the interface between foundation model outputs and GNN inputs. <strong>Adapter-based integration</strong> provides task adaptation with modest parameter overhead, avoiding <strong>full fine-tuning</strong> costs while retaining flexibility (<a href="../part_2/p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>).</p>
<p>The granularity of representations also offers flexibility. For proteins, one might extract both per-residue embeddings (capturing local structure) and sequence-level embeddings (capturing global properties), concatenating these as node features. For regulatory networks, one might combine nucleotide-level DNA embeddings with region-level chromatin accessibility predictions. This <strong>multi-scale integration</strong> uses foundation model representations at multiple granularities to capture different aspects of biological function.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Choosing an Integration Strategy
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Start simple:</strong> Begin with frozen embeddings and a 2-layer GNN. This establishes a strong baseline with minimal hyperparameter tuning.</p>
<p><strong>Add complexity when justified:</strong> If frozen embeddings underperform and you have substantial labeled data (thousands of examples), try adapter-based integration before full fine-tuning.</p>
<p><strong>Monitor for overfitting:</strong> Watch the gap between training and validation performance. Joint fine-tuning on small datasets often memorizes rather than generalizes.</p>
<p><strong>Consider compute budget:</strong> Pre-computing embeddings for all nodes once is much cheaper than computing them on-the-fly during training. For large graphs, this can reduce training time by 10-100x.</p>
</div>
</div>
<p>The choice of integration pattern depends on data availability, computational resources, and the degree of <strong>distribution shift</strong> between foundation model <strong>pretraining</strong> and the target application (<a href="../part_2/p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 12</span></a>). For well-characterized systems with substantial labeled data, joint fine-tuning may be warranted. For novel organisms or rare diseases with limited labels, frozen embeddings with simple GNN layers often generalize better.</p>
</section>
<section id="sec-ch21-integration-evidence" class="level3" data-number="21.3.3">
<h3 data-number="21.3.3" class="anchored" data-anchor-id="sec-ch21-integration-evidence"><span class="header-section-number">21.3.3</span> Evidence for the Integration Benefit</h3>
<p>Empirical studies consistently demonstrate that foundation model embeddings improve GNN performance on biological tasks. In protein function prediction, <em>ESM</em> embeddings combined with PPI network GNNs substantially outperform either sequence-only or network-only baselines <span class="citation" data-cites="lin2025gobeacon">(<a href="../bib/references.html#ref-lin2025gobeacon" role="doc-biblioref"><strong>lin2025gobeacon?</strong></a>)</span>. The improvement is particularly pronounced for proteins with few characterized interaction partners, where network structure alone provides limited signal but sequence features carry evolutionary information.</p>
<p>For disease gene prioritization, combining DNA and protein foundation model embeddings with multi-relational GNNs over heterogeneous biological networks improves ranking of causal genes from GWAS loci <span class="citation" data-cites="saadat2024dna decarlo2023xgdag">(<a href="../bib/references.html#ref-saadat2024dna" role="doc-biblioref"><strong>saadat2024dna?</strong></a>; <a href="../bib/references.html#ref-decarlo2023xgdag" role="doc-biblioref"><strong>decarlo2023xgdag?</strong></a>)</span>. The foundation model features help distinguish genes with similar network positions based on sequence-level functional signals.</p>
<p>In single-cell analysis, <em>scGPT</em> embeddings combined with cell-cell communication graphs enable more accurate prediction of perturbation effects than either component alone <span class="citation" data-cites="cui2023scgpt segceco2024 cgcom2023">(<a href="../bib/references.html#ref-cui2023scgpt" role="doc-biblioref"><strong>cui2023scgpt?</strong></a>; <a href="../bib/references.html#ref-segceco2024" role="doc-biblioref"><strong>segceco2024?</strong></a>; <a href="../bib/references.html#ref-cgcom2023" role="doc-biblioref"><strong>cgcom2023?</strong></a>)</span>. The cell embeddings capture transcriptional state, while the graph structure encodes spatial and molecular interaction context.</p>
<p>These results suggest that the integration principle generalizes across biological domains. The specific foundation models and graph types vary, but the architectural pattern (rich entity embeddings + relational structure + message passing) consistently outperforms simpler alternatives.</p>
</section>
</section>
<section id="sec-ch21-applications" class="level2" data-number="21.4">
<h2 data-number="21.4" class="anchored" data-anchor-id="sec-ch21-applications"><span class="header-section-number">21.4</span> Applications</h2>
<p>The integration of foundation model embeddings with graph neural networks enables applications across the translational pipeline. Disease gene prioritization leverages network context to identify causal genes from GWAS loci. Drug-target prediction exploits both sequence-derived druggability features and pathway positioning. Knowledge graph reasoning supports multi-hop inference for drug repurposing. Pathway analysis identifies dysregulated modules in patient-specific contexts. Each application follows the same architectural pattern (rich node features from foundation models, relational structure from biological networks, refinement through message passing) while addressing distinct biological questions.</p>
<section id="sec-ch21-disease-gene" class="level3" data-number="21.4.1">
<h3 data-number="21.4.1" class="anchored" data-anchor-id="sec-ch21-disease-gene"><span class="header-section-number">21.4.1</span> Disease Gene Prioritization</h3>
<p>Genome-wide association studies identify genomic loci associated with disease risk but rarely pinpoint causal genes (<a href="../part_1/p1-ch03-gwas.html" class="quarto-xref"><span>Chapter 3</span></a>). A typical GWAS locus contains dozens of genes, most of which are passengers linked to the true <strong>causal variant</strong> through <strong>linkage disequilibrium</strong>. Identifying which gene(s) mediate the association requires integrating functional evidence with genetic signal.</p>
<p>Network-based prioritization leverages the observation that disease genes cluster in biological networks. If a GWAS locus contains genes A, B, and C, and gene B interacts with five known disease genes while A and C interact with none, gene B becomes a stronger causal candidate. Graph neural networks formalize and extend this intuition, learning to propagate disease labels through networks and score candidate genes based on their network context.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Beyond Guilt by Association
</div>
</div>
<div class="callout-body-container callout-body">
<p>Classical network analysis uses “guilt by association”—genes near known disease genes are likely disease genes. GNNs go further by learning <em>which</em> associations matter. Not all neighbors are equally informative. A GNN trained with foundation model embeddings can learn that a neighbor sharing functional domains (detected through sequence similarity) is more informative than a neighbor connected only through high-throughput screening artifacts.</p>
</div>
</div>
<p>The integration with foundation models strengthens this approach. Rather than relying solely on network topology, which favors well-studied hub genes, the model can assess each candidate’s intrinsic functional properties through sequence embeddings. A gene with protein features characteristic of disease-relevant functions (membrane localization, DNA binding, signaling domains) receives higher scores even if its network position is peripheral. This helps mitigate the ascertainment bias toward well-characterized genes that plagues purely topological methods.</p>
<div id="fig-disease-gene-prioritization" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-disease-gene-prioritization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/04-A-fig-disease-gene-prioritization.svg" class="img-fluid figure-img"></p>
<figcaption>GWAS locus: multiple genes in LD with lead SNP</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/04-B-fig-disease-gene-prioritization.svg" class="img-fluid figure-img"></p>
<figcaption>Network context: one gene connects to disease genes</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/04-C-fig-disease-gene-prioritization.svg" class="img-fluid figure-img"></p>
<figcaption>GNN scoring prioritizes network-connected candidates</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/04-D-fig-disease-gene-prioritization.svg" class="img-fluid figure-img"></p>
<figcaption>Integration of sequence and network features</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-disease-gene-prioritization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.4: Disease gene prioritization for GWAS follow-up. (A) The GWAS challenge: a lead SNP is in LD with multiple genes—which is causal? (B) Network context: Gene B connects to five known disease genes while A, C, D are peripheral. (C) GNN scoring: foundation model embeddings as node features combined with network propagation prioritize gene B. (D) Sequence-network integration: Gene C has strong protein features while Gene B has network context—combined scoring identifies both as candidates for follow-up.
</figcaption>
</figure>
</div>
<p>Clinical applications include rare disease diagnosis, where patient exome sequencing identifies hundreds of candidate variants and network-based scoring helps prioritize which genes to investigate further (<a href="../part_6/p6-ch28-rare-disease.html" class="quarto-xref"><span>Chapter 28</span></a>). The approach also supports drug target identification by highlighting genes whose network position and functional properties make them amenable to therapeutic modulation (<a href="../part_6/p6-ch29-drug-discovery.html" class="quarto-xref"><span>Chapter 30</span></a>). For rare disease diagnosis, network-based prioritization integrates with the variant filtering pipelines in <span class="quarto-unresolved-ref">?sec-ch26-prioritization-funnel</span>, where foundation model embeddings and network context jointly inform gene ranking.</p>
</section>
<section id="sec-ch21-drug-target" class="level3" data-number="21.4.2">
<h3 data-number="21.4.2" class="anchored" data-anchor-id="sec-ch21-drug-target"><span class="header-section-number">21.4.2</span> Drug-Target Interaction Prediction</h3>
<p>Identifying which proteins a drug binds is fundamental to understanding mechanism and predicting side effects. Experimental screening of drug-target pairs is expensive and incomplete; computational prediction can prioritize candidates for validation.</p>
<p>Drug-target interaction prediction naturally fits a graph framework. Construct a heterogeneous graph with drug nodes, protein nodes, and edges representing known interactions. Node features for proteins come from sequence foundation models; node features for drugs come from molecular encodings (fingerprints, learned representations from molecular graphs). Train a GNN to predict missing edges, learning which drug and protein features, combined with network context, indicate likely binding.</p>
<p>The foundation model integration is critical here. Protein embeddings from <em>ESM</em> capture binding pocket characteristics, domain structure, and evolutionary constraint that influence druggability. The graph structure provides context: if a drug binds protein A, and protein A participates in complex with protein B, then the drug may also affect protein B’s function. Multi-relational GNNs can learn different propagation patterns for different edge types (physical binding versus pathway membership versus sequence similarity), improving prediction accuracy.</p>
<p>This application connects to broader drug discovery workflows (<a href="../part_6/p6-ch29-drug-discovery.html" class="quarto-xref"><span>Chapter 30</span></a>), where target identification is one component of a multi-stage pipeline. GNN-based predictions provide hypotheses for experimental validation, accelerating the search for novel therapeutic targets.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>A pharmaceutical company wants to predict off-target effects of a new kinase inhibitor. They have: - The drug’s binding affinity to 50 kinases (experimentally measured) - A kinase family tree based on sequence similarity - <em>ESM-2</em> embeddings for all human kinases</p>
<p>How would you structure this as a graph learning problem? What would be your nodes, edges, and prediction target? What might the model learn that simple sequence similarity would miss?</p>
</div>
</div>
</section>
<section id="sec-ch21-kg-reasoning" class="level3" data-number="21.4.3">
<h3 data-number="21.4.3" class="anchored" data-anchor-id="sec-ch21-kg-reasoning"><span class="header-section-number">21.4.3</span> Knowledge Graph Reasoning and Drug Repurposing</h3>
<p>Drug repurposing seeks new therapeutic applications for existing compounds, exploiting the observation that drugs often affect multiple targets and pathways beyond their original indication. Knowledge graphs provide a natural framework for repurposing by encoding the relationships through which a drug’s effects might propagate to new disease contexts.</p>
<p>The repurposing problem can be framed as link prediction in a heterogeneous graph: given a knowledge graph with drugs, diseases, genes, and pathways as nodes, predict missing drug-treats-disease edges. Unlike direct drug-target prediction, this task requires reasoning across multiple relationship types. A candidate repurposing hypothesis might involve a chain such as: drug D binds protein P1, P1 regulates pathway W, pathway W is dysregulated in disease X, therefore D may treat X. Graph neural networks designed for heterogeneous graphs learn to aggregate evidence across such chains, weighting different metapaths (sequences of edge types) according to their predictive reliability. The drug repurposing applications that exploit this reasoning are detailed in <span class="quarto-unresolved-ref">?sec-ch27-repurposing</span>.</p>
<p>Foundation model embeddings strengthen knowledge graph reasoning in several ways. For gene and protein nodes, <em>ESM</em> embeddings encode functional properties that influence druggability and pathway membership. For disease nodes, embeddings derived from clinical text or phenotype ontologies capture symptom patterns and comorbidity relationships. For drug nodes, molecular representations from chemical language models or graph neural networks over molecular structure encode binding properties and pharmacokinetics. These rich node features allow the GNN to assess not just whether a path exists but whether the entities along that path have compatible functional characteristics.</p>
<p>Empirical results demonstrate the value of this integration. Models combining knowledge graph structure with foundation model embeddings outperform both topology-only approaches (which ignore node semantics) and embedding-only approaches (which ignore relational structure) on standard drug repurposing benchmarks <span class="citation" data-cites="biomedkg2025 dreamgnn2025">(<a href="../bib/references.html#ref-biomedkg2025" role="doc-biblioref"><strong>biomedkg2025?</strong></a>; <a href="../bib/references.html#ref-dreamgnn2025" role="doc-biblioref"><strong>dreamgnn2025?</strong></a>)</span>. The improvement is particularly pronounced for drugs and diseases with sparse direct evidence, where multi-hop reasoning through well-characterized intermediate entities provides the primary signal.</p>
<p>Clinical translation of knowledge graph predictions requires careful interpretation. A high-scoring drug-disease prediction indicates that multiple lines of computational evidence converge, not that efficacy has been established. The paths contributing to predictions provide mechanistic hypotheses that can guide experimental validation: if the model relies heavily on a drug-protein-pathway-disease chain, that pathway becomes a candidate biomarker for patient selection or treatment response monitoring. Several repurposing candidates identified through knowledge graph methods have entered clinical trials, though the approach remains most valuable for hypothesis generation rather than definitive target validation <span class="citation" data-cites="stebbing2020mechanism richardson2020baricitinib">(<a href="../bib/references.html#ref-stebbing2020mechanism" role="doc-biblioref"><strong>stebbing2020mechanism?</strong></a>; <a href="../bib/references.html#ref-richardson2020baricitinib" role="doc-biblioref"><strong>richardson2020baricitinib?</strong></a>)</span>.</p>
<div id="fig-kg-drug-repurposing" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kg-drug-repurposing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/05-A-fig-kg-drug-repurposing.svg" class="img-fluid figure-img"></p>
<figcaption>Heterogeneous knowledge graph with multiple node types</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/05-B-fig-kg-drug-repurposing.svg" class="img-fluid figure-img"></p>
<figcaption>Multi-hop reasoning provides mechanistic hypotheses</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/05-C-fig-kg-drug-repurposing.svg" class="img-fluid figure-img"></p>
<figcaption>Link prediction scores potential drug-disease associations</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch21/05-D-fig-kg-drug-repurposing.svg" class="img-fluid figure-img"></p>
<figcaption>FM embeddings enhance node representations</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kg-drug-repurposing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.5: Knowledge graph reasoning for drug repurposing. (A) Heterogeneous knowledge graph structure with multiple node types (drugs, proteins, diseases, pathways) and relationship types. (B) Multi-hop reasoning: a drug might treat a disease through a path like Drug → binds → Protein → pathway → Disease. (C) Link prediction: GNNs learn embeddings to score potential new drug-disease associations. (D) Foundation model enhancement: FM embeddings capture functional similarity that database annotations miss.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch21-pathway-analysis" class="level3" data-number="21.4.4">
<h3 data-number="21.4.4" class="anchored" data-anchor-id="sec-ch21-pathway-analysis"><span class="header-section-number">21.4.4</span> Pathway and Module Analysis</h3>
<p>Individual genes rarely act alone; biological function emerges from coordinated activity of gene sets organized into pathways and functional modules. Patient-specific pathway analysis identifies which modules show coordinated dysregulation, providing mechanistic insight beyond single-gene associations.</p>
<p>GNNs enable pathway analysis that respects network structure rather than treating gene sets as independent members. By propagating patient-specific expression or mutation signals through pathway graphs, models can identify which subnetworks show coherent perturbation. This differs from classical gene set enrichment, which tests for overrepresentation without considering internal pathway topology.</p>
<p>Foundation model features strengthen pathway analysis by providing functional context for each gene. A gene with features indicating chromatin regulation may contribute to pathway dysfunction through different mechanisms than one with features indicating membrane signaling. The GNN learns to weight these contributions based on network position and functional annotation, identifying pathway perturbations that purely topological or purely gene-set methods miss.</p>
</section>
<section id="sec-ch21-cell-annotation" class="level3" data-number="21.4.5">
<h3 data-number="21.4.5" class="anchored" data-anchor-id="sec-ch21-cell-annotation"><span class="header-section-number">21.4.5</span> Cell Type and State Annotation</h3>
<p>Single-cell foundation models generate rich representations of individual cells (<a href="p4-ch19-single-cell.html" class="quarto-xref"><span>Chapter 19</span></a>), but many biological questions involve relationships between cells: which cells communicate, how spatial neighborhoods influence behavior, which cell types co-occur in disease states.</p>
<p>Graph neural networks over cell-cell interaction graphs enable several applications. Cell type annotation propagates labels from well-characterized cells to ambiguous ones based on expression similarity and spatial proximity. Perturbation response prediction models how signals from perturbed cells propagate to neighbors. Tissue region classification identifies coherent spatial domains (tumor, stroma, immune infiltrate) based on local cell compositions.</p>
<p>The foundation model integration follows the standard pattern: <em>scGPT</em> or similar models generate cell embeddings, spatial proximity or inferred ligand-receptor interactions define edges, and GNN message passing refines cell representations based on neighborhood context. The resulting embeddings capture both intrinsic cell state and extrinsic spatial/communicative context, enabling predictions that purely expression-based or purely spatial models cannot make.</p>
</section>
</section>
<section id="sec-ch21-practical" class="level2" data-number="21.5">
<h2 data-number="21.5" class="anchored" data-anchor-id="sec-ch21-practical"><span class="header-section-number">21.5</span> Practical Considerations</h2>
<p>Deploying graph neural networks on biological data requires navigating choices that profoundly affect model behavior. Graph construction determines what relationships the model can exploit. Scalability strategies determine whether training is feasible on large networks. Robustness techniques determine whether predictions generalize beyond well-characterized network regions. Interpretation methods determine whether outputs provide actionable biological insight. The following subsections address each consideration in turn.</p>
<section id="sec-ch21-graph-construction" class="level3" data-number="21.5.1">
<h3 data-number="21.5.1" class="anchored" data-anchor-id="sec-ch21-graph-construction"><span class="header-section-number">21.5.1</span> Graph Construction Quality</h3>
<p>The impact of graph construction choices cannot be overstated. A GNN can only learn from relationships encoded in its input graph; missing edges prevent information flow, spurious edges introduce noise, and biased edge sets propagate ascertainment artifacts.</p>
<p>Source selection involves tradeoffs between precision and completeness. Curated databases like BioGRID provide high-confidence interactions but miss most true relationships. Computational predictions from STRING or co-expression analysis are more comprehensive but noisier. The appropriate choice depends on the downstream task: high-precision networks may be preferable when false positives are costly, while high-recall networks enable discovery of novel biology at the risk of chasing artifacts.</p>
<p>Thresholding decisions determine network density. Confidence scores or distance metrics allow continuous edge weights, but many GNN implementations require discrete edges or work better with relatively sparse graphs. Cross-validation over threshold values or principled selection criteria (target edge density, ensure graph connectivity) help navigate this choice.</p>
<p>For heterogeneous graphs, schema design (which node types exist, which edge types connect them) encodes strong assumptions about relevant biology. A knowledge graph that separates genes, transcripts, and proteins as distinct node types enables fine-grained reasoning but requires more training data than a simpler gene-only representation.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Graph Construction Checklist
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before training a GNN, verify your graph construction:</p>
<ul class="task-list">
<li><label><input type="checkbox"><strong>Edge source quality:</strong> What is the expected false positive rate? False negative rate?</label></li>
<li><label><input type="checkbox"><strong>Threshold selection:</strong> Did you validate the confidence threshold on held-out data?</label></li>
<li><label><input type="checkbox"><strong>Connectivity:</strong> Are there disconnected components? Isolated nodes?</label></li>
<li><label><input type="checkbox"><strong>Degree distribution:</strong> Are there extreme hubs that might dominate message passing?</label></li>
<li><label><input type="checkbox"><strong>Temporal consistency:</strong> Were edges assembled from data available at the time of your labels?</label></li>
<li><label><input type="checkbox"><strong>Schema alignment:</strong> Do your node/edge types match your biological question?</label></li>
</ul>
<p>Mistakes in graph construction often matter more than model architecture choices.</p>
</div>
</div>
</section>
<section id="sec-ch21-scalability" class="level3" data-number="21.5.2">
<h3 data-number="21.5.2" class="anchored" data-anchor-id="sec-ch21-scalability"><span class="header-section-number">21.5.2</span> Scalability and Mini-Batching</h3>
<p>Biological graphs range from thousands of nodes (a single-patient cell graph) to millions (a comprehensive knowledge graph or large spatial transcriptomics dataset). Full-batch training, where the entire graph is processed simultaneously, becomes infeasible at scale due to memory constraints.</p>
<p>Mini-batching strategies partition computation into manageable pieces. Neighborhood sampling (<em>GraphSAGE</em>-style) restricts message passing to a fixed sample of neighbors per node, enabling node-level mini-batches. Subgraph sampling trains on induced subgraphs corresponding to meaningful units (individual pathways, tissue regions, patient subsets). Cluster-based training partitions the graph into communities, processes each independently, and handles cross-cluster edges in a second pass.</p>
<p>For foundation model integration, computational cost compounds: generating embeddings for millions of proteins or cells may itself be expensive. Pre-computing and caching embeddings is often practical, decoupling the foundation model forward pass from GNN training. When embeddings must be computed on-the-fly (for dynamic features or joint fine-tuning), careful batching and <strong>gradient checkpointing</strong> become essential.</p>
</section>
<section id="sec-ch21-robustness" class="level3" data-number="21.5.3">
<h3 data-number="21.5.3" class="anchored" data-anchor-id="sec-ch21-robustness"><span class="header-section-number">21.5.3</span> Robustness to Noise and Missingness</h3>
<p>All biological networks contain errors. Experimental methods for detecting interactions have false positive and false negative rates; computational predictions rely on imperfect proxies; even curated databases contain mistakes. GNNs must tolerate this noise to be practically useful.</p>
<p>Randomly masking edges during training forces the model to avoid relying on any single interaction. This <strong>edge dropout</strong> improves robustness to missing or incorrect edges and serves as a form of regularization. Similarly, masking node features or entire nodes through <strong>node dropout</strong> prevents overfitting to well-connected hubs.</p>
<p>Ensemble methods train multiple GNNs on different network subsamples or with different random initializations, aggregating predictions to reduce variance from network noise. Bayesian GNNs provide uncertainty estimates that flag low-confidence predictions for manual review (<a href="../part_5/p5-ch23-uncertainty.html" class="quarto-xref"><span>Chapter 23</span></a>).</p>
<p>Evaluation should explicitly assess robustness by testing on held-out edges, nodes from poorly characterized network regions, or networks constructed from different data sources than training. A model that performs well only on hub genes or well-characterized interactions may fail in precisely the scenarios where computational prediction is most needed (<a href="../part_2/p2-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 12</span></a>).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>You have trained a GNN for disease gene prioritization and achieved 0.85 AUC on your test set. Before celebrating, what additional evaluations should you perform to assess whether this performance is meaningful?</p>
<p>Consider: 1. How would you check if the model is simply learning node degree? 2. How would you assess performance on understudied genes? 3. How would you test generalization to a new disease not in training?</p>
<p><em>Take a moment to think through each question before reading on.</em></p>
</div>
</div>
</section>
<section id="sec-ch21-interpretation" class="level3" data-number="21.5.4">
<h3 data-number="21.5.4" class="anchored" data-anchor-id="sec-ch21-interpretation"><span class="header-section-number">21.5.4</span> Interpretation and Validation</h3>
<p>A key advantage of graph models is interpretability: the graph structure itself provides a scaffold for understanding predictions (<a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 25</span></a>). Several techniques extract biological insight from trained GNNs.</p>
<p>Attention weights in GAT and graph transformer models indicate which neighbors most influenced each node’s prediction. Aggregating attention across predictions can highlight critical edges or subgraphs, suggesting which interactions drive model behavior. For cases where some neighbors matter more than others, this <strong>attention weight analysis</strong> reveals learned priorities.</p>
<p>Computing how predictions change with respect to node or edge features identifies which parts of the input most affect outputs. <strong>Gradient-based attribution</strong> methods such as integrated gradients provide smoother, more reliable attributions than raw gradients.</p>
<p>Systematically removing edges, masking nodes, or perturbing features and observing prediction changes reveals which graph elements are necessary for specific predictions. This <strong>counterfactual analysis</strong> can identify model vulnerabilities and generate testable hypotheses about which interactions are essential.</p>
<p>Projecting learned node representations into two dimensions using Uniform Manifold Approximation and Projection (UMAP) or t-distributed stochastic neighbor embedding (t-SNE) reveals clusters that may correspond to functional categories, cell types, or disease subtypes. Comparing <strong>embedding visualizations</strong> across conditions identifies network regions that show context-specific changes.</p>
<p>Interpretation is not an afterthought but a central goal. The most impactful applications are those where GNN predictions generate testable hypotheses about biological mechanism, ultimately validated by experiment. Attention weights highlighting a regulatory edge or gradient attribution implicating a signaling pathway should prompt follow-up experiments, not immediate clinical action.</p>
</section>
</section>
<section id="sec-ch21-limitations" class="level2" data-number="21.6">
<h2 data-number="21.6" class="anchored" data-anchor-id="sec-ch21-limitations"><span class="header-section-number">21.6</span> Limitations and Open Challenges</h2>
<p>Graph neural networks inherit the biases and limitations of their input networks. Network incompleteness means critical relationships may be absent. Ascertainment bias means well-studied genes dominate predictions. Correlational structure may not reflect causal mechanisms. These limitations do not invalidate the approach but constrain its appropriate use and interpretation.</p>
<section id="sec-ch21-study-bias" class="level3" data-number="21.6.1">
<h3 data-number="21.6.1" class="anchored" data-anchor-id="sec-ch21-study-bias"><span class="header-section-number">21.6.1</span> Study Bias Problem</h3>
<p>Network-based methods inherit the biases of their input networks. Well-studied genes appear as hubs; poorly characterized genes are peripheral or disconnected. GNNs trained on such networks learn to propagate signals toward well-characterized genes, effectively recapitulating rather than extending existing knowledge.</p>
<p>This creates particular problems for disease gene discovery, where the goal is often to identify previously unrecognized genes. A model that consistently ranks known disease genes highly may simply be exploiting their network prominence rather than learning generalizable disease biology. Careful evaluation on temporal holdouts (genes characterized after training data was assembled) or stratified by network degree can reveal whether models truly generalize (<a href="../part_2/p2-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 12</span></a>). The systematic approaches for detecting and quantifying such confounding patterns are detailed in <span class="quarto-unresolved-ref">?sec-ch22-detection</span>.</p>
<p>Mitigation strategies include degree-corrected training objectives, explicit modeling of ascertainment bias, or alternative network constructions that reduce dependence on historical research focus. None fully solves the problem, which reflects fundamental data limitations rather than algorithmic shortcomings.</p>
<div id="fig-network-bias" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-network-bias-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch18/06-fig-network-bias.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER</strong></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-network-bias-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.6: [Enhancing] Study bias visualization. Node degree vs.&nbsp;publication count showing strong correlation; well-studied genes (<em>TP53</em>, <em>BRCA1</em>) highly connected; understudied genes peripheral; risk: GNN prioritizes well-studied genes; mitigation strategies (degree normalization, attention to edge confidence).
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch21-causality" class="level3" data-number="21.6.2">
<h3 data-number="21.6.2" class="anchored" data-anchor-id="sec-ch21-causality"><span class="header-section-number">21.6.2</span> Causality Versus Association</h3>
<p>Network edges typically represent associations (two proteins bind, two genes correlate) rather than causal relationships (perturbing gene A changes gene B). GNNs learn to exploit correlational patterns, which may not correspond to causal mechanisms.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Critical Limitation
</div>
</div>
<div class="callout-body-container callout-body">
<p>For applications like drug target identification, the causality limitation matters enormously. A gene that correlates with disease through confounding may be a poor target despite high network-based prioritization scores. A GNN might learn that genes in the “inflammation” module are associated with autoimmune disease, but this does not mean that targeting any gene in that module will be therapeutic. Experimental validation remains essential before clinical translation.</p>
</div>
</div>
<p>Integrating causal inference methods with graph learning is an active research area, but current GNN applications should be interpreted as identifying associations worthy of experimental follow-up rather than establishing causal relationships.</p>
</section>
<section id="sec-ch21-negative-data" class="level3" data-number="21.6.3">
<h3 data-number="21.6.3" class="anchored" data-anchor-id="sec-ch21-negative-data"><span class="header-section-number">21.6.3</span> Negative Data and Class Imbalance</h3>
<p>Most biological network datasets encode only positive relationships: known interactions, confirmed regulatory edges, documented associations. The absence of an edge may indicate true non-interaction or simply lack of evidence. This creates severe class imbalance for edge prediction tasks and makes negative sampling strategies critical (<a href="../part_2/p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 12</span></a>).</p>
<p>Random negative sampling (assuming absent edges represent non-interactions) is common but biologically unrealistic. More sophisticated approaches sample negatives with matched properties (same degree distribution, similar node features) to create harder and more meaningful contrasts. Evaluation should report performance separately on different negative sampling schemes to assess whether models generalize beyond easily discriminated negatives (<a href="../part_2/p2-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a>).</p>
</section>
<section id="sec-ch21-distribution-shift" class="level3" data-number="21.6.4">
<h3 data-number="21.6.4" class="anchored" data-anchor-id="sec-ch21-distribution-shift"><span class="header-section-number">21.6.4</span> Distribution Shift</h3>
<p>A GNN trained on one biological network (human PPI from STRING) may not transfer to another (mouse regulatory network, patient-specific spatial graph). Foundation model embeddings help by providing transferable features, but network structure differences can still break performance.</p>
<p>Applying models across species is particularly challenging: network topology, edge type distributions, and gene function may all differ between organisms. Cross-tissue or cross-disease transfer poses similar challenges. Explicit domain adaptation methods, multi-task training across related networks, or foundation model fine-tuning on target domains can help but add complexity (<a href="../part_2/p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>).</p>
</section>
</section>
<section id="sec-ch21-conclusion" class="level2" data-number="21.7">
<h2 data-number="21.7" class="anchored" data-anchor-id="sec-ch21-conclusion"><span class="header-section-number">21.7</span> Sequence Encodes, Structure Connects</h2>
<p>Graph neural networks operate at a complementary level of abstraction to sequence-based foundation models. Foundation models learn rich representations of biological entities from sequence data; graph neural networks learn to reason about relationships between those entities. Combining them follows a natural pattern: generate embeddings with foundation models, then refine them through message passing over graph structure. This integration yields capabilities that neither component achieves alone, propagating information across protein interaction networks, regulatory pathways, and spatial neighborhoods in ways that sequence models cannot represent.</p>
<p>The central insight is that biological knowledge exists at multiple scales. Sequence encodes what individual genes and proteins can do; networks encode how they interact to produce cellular function. GNNs translate the relational structure of biological networks into learnable inductive biases, enabling disease gene prioritization through network propagation, drug target prediction through pathway context, and spatial analysis through tissue graphs. The improvements over sequence-only approaches are consistent across applications, demonstrating that relational context adds genuine information beyond what sequence representations capture.</p>
<p>Yet network structure carries its own biases. Protein interaction databases are enriched for well-studied genes and disease-relevant pathways; less-characterized genes have fewer annotated interactions regardless of their biological importance. Correlation between genes does not imply regulatory relationship. Class imbalance between known disease genes and the genome-wide background reflects research history as much as biology. These biases propagate through GNN predictions, creating systematic patterns in what the models emphasize and what they miss. The <strong>multi-omics</strong> integration examined in <a href="p4-ch22-multi-omics.html" class="quarto-xref"><span>Chapter 22</span></a> extends graph-based reasoning to additional modalities. Clinical applications in <a href="../part_6/p6-ch27-clinical-risk.html" class="quarto-xref"><span>Chapter 27</span></a> leverage network-derived features for risk stratification, while <a href="../part_6/p6-ch28-rare-disease.html" class="quarto-xref"><span>Chapter 28</span></a> applies network propagation to gene prioritization workflows. Both depend on understanding where network-derived predictions are trustworthy and where they inherit the limitations of their inputs, challenges that the uncertainty quantification methods in <a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-uq-methods" class="quarto-xref"><span>Section 23.4</span></a> help address.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Core Concepts:</strong></p>
<ul>
<li>Graph neural networks consume foundation model embeddings, not replace them. FMs provide rich node features; GNNs add relational reasoning.</li>
<li>Message passing iteratively updates node representations based on neighbor information, with each layer extending the receptive field by one hop.</li>
<li>Over-smoothing limits GNN depth; most practical models use 2-4 layers.</li>
</ul>
<p><strong>Architecture Choices:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Need</th>
<th>Choose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple baseline</td>
<td>GCN with frozen FM embeddings</td>
</tr>
<tr class="even">
<td>Large graphs</td>
<td>GraphSAGE with neighborhood sampling</td>
</tr>
<tr class="odd">
<td>Variable edge importance</td>
<td>GAT for attention-weighted aggregation</td>
</tr>
<tr class="even">
<td>Long-range dependencies</td>
<td>Graph transformers (but higher compute)</td>
</tr>
</tbody>
</table>
<p><strong>Key Applications:</strong></p>
<ul>
<li>Disease gene prioritization: Propagate disease labels through PPI networks</li>
<li>Drug-target prediction: Heterogeneous graphs with drug and protein nodes</li>
<li>Drug repurposing: Multi-hop reasoning through knowledge graphs</li>
<li>Spatial analysis: Cell graphs with FM embeddings as node features</li>
</ul>
<p><strong>Critical Limitations to Remember:</strong></p>
<ol type="1">
<li>Networks inherit study bias (well-studied genes are over-connected)</li>
<li>Edges represent association, not causation</li>
<li>Missing edges block information flow</li>
<li>Performance on hub genes may not generalize to understudied genes</li>
</ol>
<p><strong>Connection to Later Chapters:</strong></p>
<ul>
<li><a href="p4-ch22-multi-omics.html" class="quarto-xref"><span>Chapter 22</span></a>: Extending GNNs to multi-modal data</li>
<li><a href="../part_5/p5-ch23-uncertainty.html" class="quarto-xref"><span>Chapter 23</span></a>: Uncertainty quantification for network predictions</li>
<li><a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 25</span></a>: Interpreting what GNNs learn</li>
<li><a href="../part_6/p6-ch27-clinical-risk.html" class="quarto-xref"><span>Chapter 27</span></a>: Clinical applications of network features</li>
<li><a href="../part_6/p6-ch28-rare-disease.html" class="quarto-xref"><span>Chapter 28</span></a>: Network-based gene prioritization for diagnosis</li>
</ul>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_4/p4-ch20-3d-genome.html" class="pagination-link" aria-label="3D Genome Organization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_4/p4-ch22-multi-omics.html" class="pagination-link" aria-label="Multi-Omics Integration">
        <span class="nav-page-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>