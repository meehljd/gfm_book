<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; DNA Language Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_4/p4-ch16-protein-lm.html" rel="next">
<link href="../part_4/p4-ch14-fm-principles.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_4/p4--fm-families.html">Part IV: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_4/p4-ch15-dna-lm.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">DNA Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Learning &amp; Evaluation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmark Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch12-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Evaluation Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--fm-families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch14-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch15-dna-lm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch16-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch17-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--cellular-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Cellular Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch19-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch20-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch21-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch22-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Responsible Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch24-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch25-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch26-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Causality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_7/p7--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VII: Applications &amp; Frontiers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch28-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch29-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch30-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch31-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch32-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch15-task-specific-to-general" id="toc-sec-ch15-task-specific-to-general" class="nav-link active" data-scroll-target="#sec-ch15-task-specific-to-general"><span class="header-section-number">15.1</span> From Task-Specific CNNs to General-Purpose Language Models</a></li>
  <li><a href="#sec-ch15-dnabert" id="toc-sec-ch15-dnabert" class="nav-link" data-scroll-target="#sec-ch15-dnabert"><span class="header-section-number">15.2</span> <em>DNABERT</em>: The First DNA Language Model</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-tokenization-alternatives" id="toc-sec-ch15-tokenization-alternatives" class="nav-link" data-scroll-target="#sec-ch15-tokenization-alternatives"><span class="header-section-number">15.2.1</span> Tokenization Beyond k-mers</a></li>
  </ul></li>
  <li><a href="#sec-ch15-nucleotide-transformer" id="toc-sec-ch15-nucleotide-transformer" class="nav-link" data-scroll-target="#sec-ch15-nucleotide-transformer"><span class="header-section-number">15.3</span> <em>Nucleotide Transformer</em>: Scaling Data and Model Diversity</a></li>
  <li><a href="#sec-ch15-gpn" id="toc-sec-ch15-gpn" class="nav-link" data-scroll-target="#sec-ch15-gpn"><span class="header-section-number">15.4</span> <em>GPN</em>: Cross-Species Pretraining for Variant Effect Prediction</a></li>
  <li><a href="#sec-ch15-long-context" id="toc-sec-ch15-long-context" class="nav-link" data-scroll-target="#sec-ch15-long-context"><span class="header-section-number">15.5</span> Long-Context Revolution</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-hyenadna" id="toc-sec-ch15-hyenadna" class="nav-link" data-scroll-target="#sec-ch15-hyenadna"><span class="header-section-number">15.5.1</span> <em>HyenaDNA</em>: Megabase Context via Implicit Convolutions</a></li>
  <li><a href="#sec-ch15-caduceus" id="toc-sec-ch15-caduceus" class="nav-link" data-scroll-target="#sec-ch15-caduceus"><span class="header-section-number">15.5.2</span> <em>Caduceus</em>: Bidirectional Processing with Reverse-Complement Equivariance</a></li>
  <li><a href="#sec-ch15-evo2" id="toc-sec-ch15-evo2" class="nav-link" data-scroll-target="#sec-ch15-evo2"><span class="header-section-number">15.5.3</span> <em>Evo 2</em>: Genome-Scale Modeling Across the Tree of Life</a></li>
  </ul></li>
  <li><a href="#sec-ch15-training-data" id="toc-sec-ch15-training-data" class="nav-link" data-scroll-target="#sec-ch15-training-data"><span class="header-section-number">15.6</span> Training Data and What Models Learn</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-corpus-composition" id="toc-sec-ch15-corpus-composition" class="nav-link" data-scroll-target="#sec-ch15-corpus-composition"><span class="header-section-number">15.6.1</span> Training Corpus Composition</a></li>
  <li><a href="#sec-ch15-probing" id="toc-sec-ch15-probing" class="nav-link" data-scroll-target="#sec-ch15-probing"><span class="header-section-number">15.6.2</span> Probing What Models Learn</a></li>
  <li><a href="#sec-ch15-limitations-learned" id="toc-sec-ch15-limitations-learned" class="nav-link" data-scroll-target="#sec-ch15-limitations-learned"><span class="header-section-number">15.6.3</span> What Models Do Not Learn</a></li>
  </ul></li>
  <li><a href="#sec-ch15-benchmarks" id="toc-sec-ch15-benchmarks" class="nav-link" data-scroll-target="#sec-ch15-benchmarks"><span class="header-section-number">15.7</span> Benchmark Performance and Evaluation</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-benchmark-suites" id="toc-sec-ch15-benchmark-suites" class="nav-link" data-scroll-target="#sec-ch15-benchmark-suites"><span class="header-section-number">15.7.1</span> Major Benchmark Suites</a></li>
  <li><a href="#sec-ch15-benchmark-limitations" id="toc-sec-ch15-benchmark-limitations" class="nav-link" data-scroll-target="#sec-ch15-benchmark-limitations"><span class="header-section-number">15.7.2</span> Benchmark Limitations</a></li>
  </ul></li>
  <li><a href="#sec-ch15-annotation-aware" id="toc-sec-ch15-annotation-aware" class="nav-link" data-scroll-target="#sec-ch15-annotation-aware"><span class="header-section-number">15.8</span> Annotation-Aware Extensions</a></li>
  <li><a href="#sec-ch15-practical-use" id="toc-sec-ch15-practical-use" class="nav-link" data-scroll-target="#sec-ch15-practical-use"><span class="header-section-number">15.9</span> Using DNA Language Models in Practice</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-embeddings" id="toc-sec-ch15-embeddings" class="nav-link" data-scroll-target="#sec-ch15-embeddings"><span class="header-section-number">15.9.1</span> Embeddings as Universal Features</a></li>
  <li><a href="#sec-ch15-fine-tuning" id="toc-sec-ch15-fine-tuning" class="nav-link" data-scroll-target="#sec-ch15-fine-tuning"><span class="header-section-number">15.9.2</span> Fine-Tuning and Adaptation</a></li>
  <li><a href="#sec-ch15-zero-shot" id="toc-sec-ch15-zero-shot" class="nav-link" data-scroll-target="#sec-ch15-zero-shot"><span class="header-section-number">15.9.3</span> Zero-Shot and Few-Shot Scoring</a></li>
  </ul></li>
  <li><a href="#sec-ch15-open-challenges" id="toc-sec-ch15-open-challenges" class="nav-link" data-scroll-target="#sec-ch15-open-challenges"><span class="header-section-number">15.10</span> Limitations and Open Challenges</a></li>
  <li><a href="#sec-ch15-soft-landing" id="toc-sec-ch15-soft-landing" class="nav-link" data-scroll-target="#sec-ch15-soft-landing"><span class="header-section-number">15.11</span> Representations Without Predictions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_4/p4--fm-families.html">Part IV: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_4/p4-ch15-dna-lm.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">DNA Language Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch15-dna-lm" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>What if a model could discover the genome’s regulatory grammar automatically, simply by reading it?</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Estimated reading time:</strong> 35-45 minutes</p>
<p><strong>Prerequisites:</strong> Before reading this chapter, you should be familiar with:</p>
<ul>
<li>Sequence representations and tokenization strategies (<a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>)</li>
<li>Convolutional neural networks for genomics (<a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>)</li>
<li>Attention mechanisms and transformers (<a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>)</li>
<li>Pretraining objectives (masked language modeling, autoregressive) (<a href="../part_3/p3-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>)</li>
</ul>
<p><strong>Learning Objectives:</strong> After completing this chapter, you will be able to:</p>
<ol type="1">
<li>Explain why self-supervised learning on DNA sequences can discover regulatory grammar</li>
<li>Compare the architectural innovations of major DNA language models (DNABERT, Nucleotide Transformer, HyenaDNA, Caduceus, Evo 2)</li>
<li>Evaluate the tradeoffs between context length, computational complexity, and biological inductive biases</li>
<li>Apply DNA language models for embedding extraction, fine-tuning, and zero-shot variant scoring</li>
<li>Identify the fundamental limitations of sequence-only models</li>
</ol>
<p><strong>Key Insight:</strong> DNA language models learn <em>representations</em>, not <em>predictions</em>. They capture what patterns exist in genomic sequence but not what those patterns do in cellular context. Understanding this distinction is essential for knowing when and how to use these models.</p>
</div>
</div>
<p>A regulatory element in the genome looks like random sequence to the untrained eye: ACGTACGTACGT… indistinguishable from noise. Yet hidden within these letters is a grammar: rules governing which proteins bind where, how signals propagate across kilobases, why some mutations devastate while neighbors remain silent. For decades, researchers cataloged fragments of this grammar one experiment at a time: a binding site here, a splice signal there, each discovery hard-won and narrow in scope.</p>
<p>What if a model could discover this regulatory grammar automatically, simply by reading the genome?</p>
<p>This question might seem fanciful (how could a computer program learn biology without being taught?). But an unexpected answer emerged from an entirely different field. The transformer revolution in natural language processing demonstrated that statistical patterns in unlabeled text contain information about grammar, semantics, and even world knowledge. Train a model to predict masked words from context, and it learns not just vocabulary but the structure of language itself. <em>BERT</em>, <em>GPT</em>, and their successors demonstrated that self-supervised learning on raw text yields representations useful for tasks the model was never explicitly trained to perform. Proteins proved amenable to the same approach: models trained to predict masked amino acids learned evolutionary constraints, structural properties, and functional relationships without explicit supervision (<a href="p4-ch16-protein-lm.html" class="quarto-xref"><span>Chapter 16</span></a>). DNA presents the analogous opportunity. If genomes encode a regulatory language, perhaps self-supervised learning on raw nucleotide sequence could discover its grammar.</p>
<p>DNA language models import this paradigm to nucleotide sequences. Rather than training separate models for each genomic prediction task, as the <strong>convolutional neural network (CNN)</strong> era required (<a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>), these approaches learn general-purpose representations from unlabeled genomes that transfer across applications. A single pretrained backbone can support regulatory element classification, variant effect prediction, cross-species analysis, and sequence generation through different downstream heads or adaptation strategies. After <strong>fine-tuning</strong> (<a href="../part_3/p3-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>), the same model that learns to predict masked nucleotides can predict chromatin accessibility in cell types it never saw during <strong>pretraining</strong> (<a href="../part_3/p3-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>). It can also identify splice sites without splice-specific training data and score variant effects using evolutionary patterns learned from billions of nucleotides.</p>
<p>The opportunity is substantial but not guaranteed to succeed. Protein sequences have clear functional units (domains, secondary structures, binding sites) that language model representations can capture. DNA sequences present a different challenge: regulatory grammar operates at multiple scales simultaneously, from six-nucleotide transcription factor binding sites through kilobase-scale enhancers to megabase chromatin domains. Whether self-supervised learning can discover this multi-scale grammar remains an empirical question.</p>
<section id="sec-ch15-task-specific-to-general" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="sec-ch15-task-specific-to-general"><span class="header-section-number">15.1</span> From Task-Specific CNNs to General-Purpose Language Models</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading about the limitations of task-specific CNNs, consider: if you trained a model specifically to predict chromatin accessibility in one cell type, what challenges might you face when applying it to a new cell type or a different prediction task?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>You would face three key challenges:</p>
<ol type="1">
<li><p><strong>Data dependency</strong> - you’d need new labeled chromatin accessibility data for the new cell type, which is expensive and may not be available;</p></li>
<li><p><strong>Architecture mismatch</strong> - the model’s architecture was optimized for chromatin accessibility, so adapting it to a different task (like splice prediction) would require substantial re-engineering;</p></li>
<li><p><strong>Feature transfer failure</strong> - patterns learned for one task (chromatin marks) do not directly transfer to unrelated tasks without retraining.</p></li>
</ol>
<p>This is precisely why the foundation model paradigm emerged: learn general representations once, then adapt to many tasks.</p>
</div>
</div>
</div>
</div>
</div>
<p>The convolutional neural networks examined in <a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> achieved strong performance on specific genomic prediction tasks. They faced, however, the feature ceiling limitation discussed in <a href="../part_1/p1-ch04-vep-classical.html#sec-ch04-features-to-representations" class="quarto-xref"><span>Section 4.6.4</span></a>: performance bounded by what architectural choices and training data could capture. <em>DeepSEA</em> predicted chromatin marks from sequence; <em>SpliceAI</em> identified splice junctions with clinical utility; <em>ExPecto</em> estimated expression effects of variants. Each model was engineered for its particular application, with architectural choices (filter sizes, dilation patterns, pooling strategies) optimized for the task at hand.</p>
<div id="fig-dna-lm-timeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dna-lm-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/01-fig-dna-lm-timeline.svg" class="img-fluid figure-img"></p>
<figcaption>Evolution of DNA language models from 2021-2025</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dna-lm-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.1: Evolution of DNA language models from 2021-2025. The timeline traces key milestones in architectural capability. DNABERT (2021) demonstrated proof-of-concept with 512-token contexts using k-mer tokenization. Nucleotide Transformer (2023) scaled to 2.5 billion parameters with 6kb context and multi-species pretraining. HyenaDNA (2023) broke through the quadratic attention barrier, achieving 1 megabase context through sub-quadratic Hyena operators. Caduceus (2024) introduced reverse-complement equivariance through bidirectional Mamba architectures. Evo 2 (2024-2025) scaled to 40 billion parameters with million-base contexts, enabling pan-genomic understanding and sequence generation. Upper track shows exponential growth in context length; lower track highlights architectural innovations enabling each advance.
</figcaption>
</figure>
</div>
<p>This paradigm succeeded but imposed three constraints that limited scalability. Every new assay, cell type, or phenotype required fresh labeled data; a model trained on ENCODE chromatin data could not predict histone modifications in a new cell type without additional labeled examples. Model architecture was bound to specific prediction problems: <em>SpliceAI’s</em> dilated convolutions were tailored for splice junction detection, and <em>ExPecto’s</em> spatial transformation was designed for the distance-dependent relationship between regulatory elements and transcription start sites. These architectural choices, while effective, did not transfer naturally to other problems. Features learned for one task could not easily support others; a model that learned to recognize transcription factor binding sites during chromatin accessibility training could not directly apply those representations to variant effect prediction without substantial re-engineering.</p>
<p>Protein language models demonstrated an alternative. <em>ESM</em> and related models trained on massive corpora of protein sequences using <strong>masked language modeling</strong> (predicting held-out amino acids from context) or <strong>autoregressive</strong> objectives (predicting the next amino acid). The resulting representations transferred to structure prediction, function annotation, and variant effect scoring without architecture changes (<a href="p4-ch16-protein-lm.html" class="quarto-xref"><span>Chapter 16</span></a>). DNA language models import this recipe: pretrain on large collections of genomic sequences using self-supervised objectives, then adapt the learned representations to downstream tasks through probing, fine-tuning, or zero-shot scoring.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Deep Dive: Self-Supervised Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deep Dive: Self-Supervised Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>For biology readers:</strong> Self-supervised learning creates training labels from the data itself, without manual annotation:</p>
<p><strong>The insight:</strong> Instead of requiring expensive labeled data (e.g., “this variant is pathogenic”), self-supervised learning generates labels automatically from raw data.</p>
<p><strong>Two main strategies for sequence models:</strong></p>
<p><strong>Masked Language Modeling (MLM):</strong> Hide some tokens, predict them from context.</p>
<ul>
<li>Input: “The CTCF motif [MASK] gene expression”</li>
<li>Target: predict the masked word</li>
<li>Genomic version: mask nucleotides, predict from surrounding sequence</li>
</ul>
<p><strong>Autoregressive (Next-Token Prediction):</strong> Predict each token from all previous tokens.</p>
<ul>
<li>Given: “ACGT”</li>
<li>Predict: the next nucleotide</li>
<li>Used by GPT-style models</li>
</ul>
<p><strong>Why it works:</strong></p>
<ol type="1">
<li>Creates unlimited training data from unlabeled sequences</li>
<li>Forces model to learn statistical patterns that capture biological structure</li>
<li>Positions with strong predictions = evolutionarily constrained positions</li>
<li>Patterns useful for masked prediction transfer to other tasks</li>
</ol>
<p><strong>The key insight:</strong> Predicting masked nucleotides requires understanding what patterns are “allowed” in genomic sequence, which is exactly what determines variant effects.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: The Foundation Model Paradigm Shift
</div>
</div>
<div class="callout-body-container callout-body">
<p>The core shift from task-specific CNNs to DNA language models is this: instead of building specialized architectures for each task, train a single model to understand DNA sequence through self-supervision, then adapt that understanding to any downstream application. This inverts the traditional workflow from <em>task-first</em> (design architecture for task, train from scratch) to <em>representation-first</em> (learn general representations, adapt to tasks).</p>
</div>
</div>
<p>The rapid proliferation of DNA foundation models has prompted systematic reviews cataloguing architectural choices, pretraining strategies, and benchmark performance across model families <span class="citation" data-cites="boshar_foundational_nodate consens_transformers_2025">(<a href="../bib/references.html#ref-boshar_foundational_nodate" role="doc-biblioref">Boshar et al., n.d.</a>; <a href="../bib/references.html#ref-consens_transformers_2025" role="doc-biblioref">Consens et al. 2025</a>)</span>. These surveys reveal that while transformer architectures dominate, significant diversity exists in tokenization schemes, context lengths, and training objectives.</p>
<p>The practical workflow begins with training a language model on unlabeled genomic sequences to predict masked or subsequent nucleotides. From the trained model, <strong>embeddings</strong> are extracted for sequences of interest (windows around variants, regulatory elements, or entire genes). These embeddings then support downstream tasks through probing with lightweight classifiers, fine-tuning for specific applications, or zero-shot scoring via probability comparisons. Once a sufficiently powerful backbone exists, it becomes the default starting point for nearly any DNA-level prediction problem.</p>
</section>
<section id="sec-ch15-dnabert" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="sec-ch15-dnabert"><span class="header-section-number">15.2</span> <em>DNABERT</em>: The First DNA Language Model</h2>
<p><em>DNABERT</em> applied the <em>BERT</em> masked language modeling framework to genomic sequences, establishing proof of concept for DNA self-supervision <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. The model used overlapping k-mers (typically 6-mers) as tokens, creating a vocabulary of 4,096 tokens from the <span class="math inline">\(4^6\)</span> possible hexamers. This tokenization strategy, detailed in <a href="../part_2/p2-ch05-representations.html#sec-ch05-kmer" class="quarto-xref"><span>Section 5.2</span></a>, provided computational efficiency at the cost of positional ambiguity for variants. Training on the human reference genome, <em>DNABERT</em> learned to predict masked tokens from surrounding context using the standard <em>BERT</em> architecture.</p>
<p>The design choices reflected computational constraints of the time. The <span class="math inline">\(k\)</span>-mer <strong>tokenization</strong> provided some sequence compression compared to single-nucleotide representations, but the overlapping nature (each nucleotide participates in multiple adjacent k-mers) meant the compression was modest and created ambiguity about precise variant positions. Context windows were limited to 512 tokens, corresponding to a few hundred base pairs of genomic sequence. The standard <strong>transformer architecture</strong> with quadratic attention complexity made longer contexts computationally prohibitive, a limitation examined in <a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a> and resolved by the architectural innovations in <a href="#sec-ch15-hyenadna" class="quarto-xref"><span>Section 15.5.1</span></a> and <a href="#sec-ch15-caduceus" class="quarto-xref"><span>Section 15.5.2</span></a>.</p>
<p>Despite these limitations, <em>DNABERT</em> demonstrated several important principles. Fine-tuning on downstream tasks (promoter classification, splice site prediction, transcription factor binding site identification) achieved competitive performance with task-specific models trained from scratch. Learned embeddings captured biologically meaningful patterns, with similar sequences clustering together in embedding space even when trained only on the reference genome. The <em>BERT</em>-style architecture could be reused across multiple tasks with modest adaptation.</p>
<p><em>DNABERT-2</em> addressed the tokenization limitations through improved approaches including BPE-style token merging that better compressed repetitive sequences <span class="citation" data-cites="zhou_dnabert-2_2024">(<a href="../bib/references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Zhou et al. 2024</a>)</span>. The resulting model could represent longer genomic contexts within the same number of tokens, improving computational efficiency. On standardized benchmarks spanning sequence classification, regulatory element prediction, and variant effect scoring (<a href="../part_3/p3-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a>), <em>DNABERT-2</em> achieved consistent gains over both the original <em>DNABERT</em> and non-pretrained baselines. These improvements validated the importance of thoughtful tokenization design for genomic applications (see <a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a> for detailed discussion of tokenization strategies).</p>
<p>The <em>DNABERT</em> family collectively established that self-supervision on DNA works, that tokenization choices substantially affect performance, and that masked language model training produces reusable representations for diverse sequence tasks. The foundation model paradigm transfers effectively from natural language to genomic sequence.</p>
<section id="sec-ch15-tokenization-alternatives" class="level3" data-number="15.2.1">
<h3 data-number="15.2.1" class="anchored" data-anchor-id="sec-ch15-tokenization-alternatives"><span class="header-section-number">15.2.1</span> Tokenization Beyond k-mers</h3>
<p>While k-mer and BPE tokenization dominate DNA language models, biologically-motivated alternatives offer compelling advantages. Codon-level tokenization aligns token boundaries with the genetic code’s natural reading frame, providing an inductive bias well-suited to protein-coding regions <span class="citation" data-cites="outeiral_codon_2024">(<a href="../bib/references.html#ref-outeiral_codon_2024" role="doc-biblioref">Outeiral and Deane 2024</a>)</span>. This approach demonstrates that domain knowledge can inform tokenization design, potentially improving performance on tasks where codon identity matters, such as synonymous variant effect prediction or codon optimization for expression.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tokenization Trade-offs
</div>
</div>
<div class="callout-body-container callout-body">
<p>Codon tokenization excels for coding sequences but may underperform in non-coding regions where the reading frame concept does not apply. Model selection should consider the target application’s sequence composition.</p>
</div>
</div>
</section>
</section>
<section id="sec-ch15-nucleotide-transformer" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="sec-ch15-nucleotide-transformer"><span class="header-section-number">15.3</span> <em>Nucleotide Transformer</em>: Scaling Data and Model Diversity</h2>
<p><em>DNABERT</em> demonstrated feasibility but operated at modest scale relative to the size of genomes. The <em>Nucleotide Transformer</em> family pushed substantially further, emphasizing diversity in both training data and model architecture <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>.</p>
<p>The training corpus spanned genomic data from multiple species and human populations, exposing models to diverse sequence patterns, different regulatory architectures, and evolutionary constraints recurring across lineages. This cross-species pretraining mirrors the use of large multi-species alignments in protein language models but operates directly on raw DNA without explicit alignment. Context length expanded to approximately 6 kb per input sequence, representing an order-of-magnitude increase over <em>DNABERT</em> while still using dense transformer attention. The training objective remained masked language modeling on subsequences sampled from genomes.</p>
<p>The <em>Nucleotide Transformer</em> project introduced a benchmark panel that has become a standard yardstick for evaluating DNA language models. Tasks include promoter and enhancer classification, histone mark and chromatin accessibility prediction, splice site identification, and regulatory element type classification. Models are evaluated through linear probes or light fine-tuning on standardized train/validation/test splits. This benchmark infrastructure enabled systematic comparison across models and established the evaluation protocols now used throughout the field (see <a href="../part_3/p3-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a> for comprehensive discussion of genomic benchmarks).</p>
<p>Scaling experiments revealed predictable relationships between model size, training data, and performance. Larger models with more pretraining data and longer context windows achieved better downstream performance, following patterns observed in natural language and protein modeling. These scaling trends suggest that continued investment in larger genomic language models will yield further improvements, though the optimal allocation between parameters, data, and compute remains an active research question (<a href="p4-ch14-fm-principles.html" class="quarto-xref"><span>Chapter 14</span></a>).</p>
<p>The <em>Nucleotide Transformer</em> family continues to evolve through subsequent versions with architectural improvements, including expanded context windows and refined tokenization strategies <span class="citation" data-cites="boshar_foundational_nodate">(<a href="../bib/references.html#ref-boshar_foundational_nodate" role="doc-biblioref">Boshar et al., n.d.</a>)</span>. These updates demonstrate how DNA language models mature through iterative refinement, with each generation addressing limitations identified in prior versions.</p>
</section>
<section id="sec-ch15-gpn" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="sec-ch15-gpn"><span class="header-section-number">15.4</span> <em>GPN</em>: Cross-Species Pretraining for Variant Effect Prediction</h2>
<p>While the <em>Nucleotide Transformer</em> demonstrated the value of scaling, the Genomic Pretrained Network (<em>GPN</em>) explored a complementary direction: what can be learned from cross-species pretraining on relatively small, well-annotated genomes <span class="citation" data-cites="benegas_gpn_2023">(<a href="../bib/references.html#ref-benegas_gpn_2023" role="doc-biblioref">Benegas, Batra, and Song 2023</a>)</span>. Rather than scaling to maximum size, <em>GPN</em> asked whether self-supervision could yield useful variant effect predictors even in constrained settings.</p>
<p><em>GPN</em> was trained on unaligned reference genomes from <em>Arabidopsis thaliana</em> and seven related species within the Brassicales order using masked language modeling. Despite this modest training corpus, analysis revealed emergent encoding of gene structure (exon-intron boundaries, splice sites) and DNA sequence motifs (transcription factor binding patterns) without explicit supervision. The model discovered these patterns purely from statistical regularities of genomic sequence across related species.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>GPN</em> learns from only eight plant genomes, yet it outperforms conservation scores derived from alignments across dozens of species. What might explain this surprising result? Consider what information is preserved versus lost in multiple sequence alignment.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Multiple sequence alignment preserves only positions where sequences can be reliably aligned, discarding information from insertion/deletion-rich regions and structural variants. <em>GPN</em> learns from unaligned sequences, capturing patterns in these regions that alignment-based methods miss. Additionally, <em>GPN</em> learns higher-order sequence patterns (motif combinations, spacing constraints, regulatory grammar) that phyloP and phastCons cannot represent; they only measure single-position conservation. The self-supervised objective forces <em>GPN</em> to predict nucleotides from context, requiring it to learn the sequence rules that determine “allowed” versus “disallowed” patterns. This is precisely the information needed for variant effect prediction.</p>
</div>
</div>
</div>
</div>
</div>
<p>For variant effect prediction, <em>GPN</em> used a <strong>likelihood ratio</strong> approach. Given reference and alternate alleles at a position, the model computes the log-likelihood of each under the learned sequence distribution. Variants that substantially reduce sequence likelihood (relative to the reference) are inferred to be more disruptive. This scoring strategy exploits the fact that constrained positions should have confident predictions for the reference allele, while unconstrained positions allow more flexibility.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Worked Example: Zero-Shot Variant Scoring
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider scoring a variant at position 1000 in a gene. The workflow:</p>
<ol type="1">
<li><strong>Extract context window:</strong> Take the sequence from positions 500-1500 (1 kb centered on variant)</li>
<li><strong>Compute reference likelihood:</strong> Feed sequence with reference allele (e.g., A) to model; record log-probability <span class="math inline">\(\log P(\text{seq}|\text{A})\)</span></li>
<li><strong>Compute alternate likelihood:</strong> Feed sequence with alternate allele (e.g., G) to model; record log-probability <span class="math inline">\(\log P(\text{seq}|\text{G})\)</span></li>
<li><strong>Calculate likelihood ratio:</strong> <span class="math inline">\(\Delta = \log P(\text{seq}|\text{ref}) - \log P(\text{seq}|\text{alt})\)</span></li>
<li><strong>Interpret:</strong> Positive <span class="math inline">\(\Delta\)</span> means alternate reduces sequence likelihood, suggesting disruption. Larger values indicate stronger constraint violation.</li>
</ol>
<p>This approach requires no variant-specific training data and works for any position the model can process.</p>
</div>
</div>
<p>Evaluated on <em>A. thaliana</em> variants using allele frequencies from the 1001 Genomes Project, <em>GPN</em> outperformed traditional conservation scores including phyloP and phastCons <span class="citation" data-cites="benegas_gpn_2023">(<a href="../bib/references.html#ref-benegas_gpn_2023" role="doc-biblioref">Benegas, Batra, and Song 2023</a>)</span>. This was notable because phyloP and phastCons require explicit multiple sequence alignments and evolutionary models, while <em>GPN</em> learned its representations from unaligned sequences through self-supervision alone. The later <em>GPN-MSA</em> extended this approach to mammalian genomes by incorporating multi-species alignments, achieving strong performance on human variant benchmarks (<a href="p4-ch18-vep-fm.html#sec-ch18-dna-lm-vep" class="quarto-xref"><span>Section 18.3.3</span></a>). The success of this approach informed subsequent development of zero-shot variant scoring methods for clinical applications (<a href="../part_7/p7-ch29-rare-disease.html#sec-ch29-fm-scoring" class="quarto-xref"><span>Section 29.1.4</span></a>).</p>
<p><em>GPN</em> established several important principles. Cross-species pretraining captures evolutionary constraints transferable to variant effect prediction. Relatively small models trained on focused phylogenetic groups can outperform larger generic conservation measures within that group. The masked language modeling objective naturally produces representations suitable for variant scoring via likelihood comparisons.</p>
</section>
<section id="sec-ch15-long-context" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="sec-ch15-long-context"><span class="header-section-number">15.5</span> Long-Context Revolution</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Content Ahead
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section discusses computational complexity notation (O notation) for comparing algorithms. If you are not familiar with this notation: <span class="math inline">\(O(L^2)\)</span> means computational cost grows with the square of sequence length <span class="math inline">\(L\)</span>, while <span class="math inline">\(O(L)\)</span> or <span class="math inline">\(O(L \log L)\)</span> means cost grows linearly or near-linearly. The key takeaway is that quadratic scaling makes long sequences impractical, while linear scaling enables megabase processing.</p>
</div>
</div>
<p>Quadratic attention complexity limits transformer context to tens of kilobases at best. Processing a 100 kb sequence with dense attention requires on the order of <span class="math inline">\(10^{10}\)</span> computations per layer. Yet regulatory phenomena routinely span larger distances: enhancer-promoter interactions extend 50-200 kb, topologically associating domains organize chromatin at the megabase scale, and some gene regulation involves even longer-range dependencies. The three-dimensional organization of chromatin that enables these long-range contacts is examined in <a href="../part_5/p5-ch21-3d-genome.html" class="quarto-xref"><span>Chapter 21</span></a>; here we focus on how linear sequence models can capture information about these interactions. The mismatch between biological context and computational context represented a fundamental architectural limitation.</p>
<div id="fig-long-context-revolution" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-long-context-revolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/02-A-fig-long-context-revolution.svg" class="img-fluid figure-img"></p>
<figcaption>The quadratic attention bottleneck limits standard transformers</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/02-B-fig-long-context-revolution.svg" class="img-fluid figure-img"></p>
<figcaption>Sub-quadratic architectures enable million-base contexts</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-long-context-revolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.2: Breaking the attention bottleneck for genomic modeling. (A) Standard self-attention requires computing all pairwise interactions between positions, creating quadratic O(L²) memory and compute requirements. This limits practical contexts to approximately 10-50 kilobases, falling short of enhancer-promoter distances (50-100kb) and topologically associating domain scales (~1Mb). (B) Sub-quadratic architectures achieve longer contexts through different strategies: Hyena uses learnable long convolutions (O(L log L)); Mamba employs selective state-space models with linear scaling (O(L)); linear attention approximates the attention mechanism with kernel methods (O(L)). These innovations enabled DNA language models to process million-base contexts, accessing biological relationships invisible to shorter-context models.
</figcaption>
</figure>
</div>
<section id="sec-ch15-hyenadna" class="level3" data-number="15.5.1">
<h3 data-number="15.5.1" class="anchored" data-anchor-id="sec-ch15-hyenadna"><span class="header-section-number">15.5.1</span> <em>HyenaDNA</em>: Megabase Context via Implicit Convolutions</h3>
<p><em>HyenaDNA</em> addressed this limitation by replacing attention with implicit convolutions that scale sub-quadratically <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. The Hyena architecture parameterizes long convolutional filters through neural networks rather than storing explicit filter weights, achieving <span class="math inline">\(O(L \log L)\)</span> complexity compared to <span class="math inline">\(O(L^2)\)</span> for standard attention. The result was a 500-fold increase in context length: <em>HyenaDNA</em> processes sequences up to 1 Mb while maintaining single-nucleotide resolution.</p>
<p>Why does this architectural change achieve sub-quadratic complexity? Standard attention requires computing and storing an <span class="math inline">\(L \times L\)</span> matrix because every position’s output explicitly depends on pairwise interactions with every other position. Hyena sidesteps this requirement entirely. Instead of computing explicit pairwise interactions, Hyena learns a continuous filter function parameterized by a small neural network that can be evaluated at any relative position. This filter is applied to the sequence via convolution, which can be computed efficiently using Fast Fourier Transform (FFT) in <span class="math inline">\(O(L \log L)\)</span> time, the same mathematical trick that enables efficient signal processing and image filtering. The key insight is that convolution in the spatial domain equals multiplication in the frequency domain, so we transform to frequency space, multiply, and transform back, avoiding the quadratic pairwise computation altogether. The tradeoff is expressiveness: attention’s content-dependent routing (where which positions interact depends on what the sequence contains) is replaced by position-dependent filtering (where interactions depend on relative distance but not content). For many genomic applications where positional relationships matter as much as content similarity, this tradeoff proves acceptable.</p>
<p>Processing megabase-scale windows allows the model to capture entire gene bodies plus flanking regulatory regions, long-range enhancer-promoter interactions, and topologically associating domain structure. Despite the long context, single-nucleotide tokens preserve maximum resolution for variant effect prediction. Each nucleotide is independently represented without the ambiguity introduced by <span class="math inline">\(k\)</span>-mer tokenization.</p>
<p>On <em>Nucleotide Transformer</em> benchmarks, <em>HyenaDNA</em> achieved state-of-the-art results on the majority of tasks with orders of magnitude fewer parameters. On GenomicBenchmarks, it surpassed prior state-of-the-art on seven of eight datasets <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. <em>HyenaDNA</em> also showed improved performance with longer contexts containing task examples, consistent with <strong>in-context learning</strong>: performance improved when examples were included in the input context without updating model weights. Whether this represents true task adaptation (as observed in large language models) or a more general benefit of additional context requires further investigation, but the results suggest that sufficient context length combined with appropriate architecture may enable new forms of genomic reasoning.</p>
</section>
<section id="sec-ch15-caduceus" class="level3" data-number="15.5.2">
<h3 data-number="15.5.2" class="anchored" data-anchor-id="sec-ch15-caduceus"><span class="header-section-number">15.5.2</span> <em>Caduceus</em>: Bidirectional Processing with Reverse-Complement Equivariance</h3>
<p>DNA is double-stranded, and any sequence can be read from either strand. The reverse complement of a sequence encodes the same information from the opposite strand’s perspective. For many biological processes, predictions should be identical or related consistently regardless of which strand is presented. Standard neural networks can produce divergent predictions for a sequence and its reverse complement, even with data augmentation during training.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a model predicting transcription factor binding. A binding site on the forward strand (5’-GAATTC-3’) has reverse complement (5’-GAATTC-3’ on the reverse strand). Should a model’s prediction differ based on which strand you query? Why or why not? What about for genes, which have defined orientations?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For transcription factor binding sites, predictions should be identical for forward and reverse complement sequences because the binding site represents the same functional element on either strand. However, for genes with defined orientations (coding sequences, promoters), strand matters biologically, so predictions can differ. The key insight is that strand symmetry applies to motifs without inherent directionality, but not to oriented features like genes where 5’ to 3’ direction carries functional meaning.</p>
</div>
</div>
</div>
</div>
</div>
<p><em>Caduceus</em> addressed this challenge by building <strong>reverse-complement equivariance</strong> directly into the architecture <span class="citation" data-cites="schiff_caduceus_2024">(<a href="../bib/references.html#ref-schiff_caduceus_2024" role="doc-biblioref">Schiff et al. 2024</a>)</span>. The model extends the <em>Mamba</em> state space architecture (which achieves O(L) complexity) to support both bidirectional processing and strand equivariance. The BiMamba component enables information flow in both directions along the sequence, while the MambaDNA block ensures mathematically related predictions for sequences and their reverse complements.</p>
<p>On downstream benchmarks, <em>Caduceus</em> outperformed previous long-range models. On challenging long-range variant effect prediction tasks, it exceeded models with ten times as many parameters that lacked bidirectionality or equivariance <span class="citation" data-cites="schiff_caduceus_2024">(<a href="../bib/references.html#ref-schiff_caduceus_2024" role="doc-biblioref">Schiff et al. 2024</a>)</span>. The key insight was that incorporating appropriate biological inductive biases can substitute for raw scale. Strand symmetry is a known property of DNA; building it into the architecture avoids wasting model capacity learning what could be specified directly.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Inductive Biases vs.&nbsp;Scale
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Caduceus</em> demonstrates a fundamental principle: when you know something about your domain, encode it in the architecture rather than hoping the model learns it from data. Strand symmetry is mathematically specifiable; building it in yields better performance with fewer parameters than training a larger model to approximate it. This principle applies broadly: whenever you have domain knowledge that can be expressed architecturally, doing so typically improves efficiency and generalization.</p>
</div>
</div>
</section>
<section id="sec-ch15-evo2" class="level3" data-number="15.5.3">
<h3 data-number="15.5.3" class="anchored" data-anchor-id="sec-ch15-evo2"><span class="header-section-number">15.5.3</span> <em>Evo 2</em>: Genome-Scale Modeling Across the Tree of Life</h3>
<p>The original <em>Evo</em> model demonstrated that DNA language models could operate at unprecedented scale <span class="citation" data-cites="nguyen_sequence_2024">(<a href="../bib/references.html#ref-nguyen_sequence_2024" role="doc-biblioref">Nguyen et al. 2024</a>)</span>. Trained on 2.7 million prokaryotic and phage genomes comprising 300 billion nucleotides, <em>Evo</em> processed sequences up to 131 kilobases using the StripedHyena architecture, a hybrid design combining state-space models with attention mechanisms. The 7 billion parameter model exhibited emergent biological understanding: predicting gene essentiality, identifying functional elements, and generating synthetic sequences with plausible biological properties. <em>Evo</em> demonstrated that training on raw DNA sequence alone, without annotation, could yield models that captured fundamental aspects of genome organization.</p>
<p><em>Evo 2</em> extends this foundation to the entire tree of life <span class="citation" data-cites="brixi_evo_2025">(<a href="../bib/references.html#ref-brixi_evo_2025" role="doc-biblioref">Brixi et al. 2025</a>)</span>. Where <em>Evo</em> focused primarily on prokaryotes and phages, <em>Evo 2</em> incorporates eukaryotic genomes with their dramatically different organization: extensive noncoding regions, complex regulatory architectures, and intronic sequences that comprise the majority of gene length in many species. This expansion required both larger models and longer context windows to capture the sprawling regulatory landscapes characteristic of eukaryotic genomes.</p>
<div id="fig-caduceus-equivariance" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-caduceus-equivariance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/03-A-fig-caduceus-equivariance.svg" class="img-fluid figure-img"></p>
<figcaption>DNA reverse complement equivalence</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/03-B-fig-caduceus-equivariance.svg" class="img-fluid figure-img"></p>
<figcaption>Standard models break RC equivalence</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/03-C-fig-caduceus-equivariance.svg" class="img-fluid figure-img"></p>
<figcaption>Caduceus architecture guarantees equivariance</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-caduceus-equivariance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.3: Reverse complement equivariance as architectural constraint. (A) DNA’s double-stranded nature creates biological equivalence: a regulatory element on one strand has the same function when read as its reverse complement from the other strand. (B) Standard models violate this equivalence, producing different predictions for forward and reverse complement sequences despite their biological identity. (C) Caduceus uses bidirectional Mamba state-space models with equivariant architecture to guarantee identical predictions for equivalent sequences. This architectural constraint eliminates strand bias without data augmentation, improving both biological correctness and data efficiency.
</figcaption>
</figure>
</div>
<p>The training corpus draws from the OpenGenome2 dataset comprising 9.3 trillion DNA base pairs across all domains of life, a 30-fold increase over the original <em>Evo</em> training data. This massive scale exposes the model to the full spectrum of genomic organization: compact prokaryotic gene arrangements, sprawling eukaryotic regulatory landscapes with extensive noncoding sequence, viral genomes with overlapping reading frames, and the diversity of regulatory architectures across evolution. The model comes in 7 billion and 40 billion parameter variants, with the larger model extending well beyond the original <em>Evo</em>’s scale.</p>
<p>The architecture builds on StripedHyena 2, refining the hybrid design that proved effective in the original <em>Evo</em>. The combination of convolutional operations with selective attention mechanisms enables processing of sequences up to 1 million nucleotides, nearly an order of magnitude beyond <em>Evo</em>’s 131 kilobase context. Like its predecessor, <em>Evo 2</em> uses an autoregressive training objective (predicting the next base given all previous bases), which differs from the masked language modeling used in <em>DNABERT</em> and related models. Autoregressive training may provide complementary strengths for sequence generation and likelihood-based scoring, since the model learns to generate plausible sequences in addition to discriminating between them (<a href="../part_3/p3-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>).</p>
<p>Analysis reveals that <em>Evo 2</em>’s representations encode information corresponding to several biological features, extending the capabilities first observed in <em>Evo</em>. Probing and analysis show that embeddings encode exon-intron boundary information without explicit annotation, contain patterns matching known transcription factor binding site motifs, and capture aspects of protein secondary and tertiary structure when processing coding sequences. Whether these encodings drive downstream task performance versus represent artifacts of predicting genome-scale patterns remains uncertain; mechanistic interpretation is required to determine which properties the model actively uses. Where <em>Evo</em> demonstrated these capabilities primarily in prokaryotic contexts, <em>Evo 2</em> extends them across eukaryotic genomes with their more complex gene structures.</p>
<p>For variant effect prediction, <em>Evo 2</em> enables zero-shot scoring through likelihood ratios. Variants can be scored for consistency with learned genomic patterns by comparing model probabilities for reference versus alternate sequences. On benchmarks of pathogenic versus benign variants, zero-shot scores achieve competitive performance with specialized supervised methods, though calibration remains necessary before clinical application (<a href="p4-ch18-vep-fm.html#sec-ch18-dna-lm-vep" class="quarto-xref"><span>Section 18.3.3</span></a>). The calibration methods required for clinical deployment are examined in <a href="../part_6/p6-ch24-uncertainty.html#sec-ch24-calibration" class="quarto-xref"><span>Section 24.2</span></a>, and integration into diagnostic workflows in <a href="../part_7/p7-ch29-rare-disease.html#sec-ch29-fm-scoring" class="quarto-xref"><span>Section 29.1.4</span></a>. The model also supports classification of variants of uncertain significance through simple classifiers trained on its embeddings.</p>
<p>The pan-species training enables cross-species applications that extend <em>Evo</em>’s prokaryotic focus to the full breadth of biology. Variant interpretation extends naturally to non-model organisms, supporting conservation genomics and agricultural breeding where labeled training data is scarce. Model representations cluster sequences by phylogenetic relationships even without explicit evolutionary modeling. Beyond discriminative tasks, <em>Evo 2</em> demonstrates generative capabilities building on <em>Evo</em>’s initial demonstrations: synthesizing plausible mitochondrial genomes, prokaryotic operons, and eukaryotic regulatory regions with coherence across kilobase to megabase scales.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check: Predict Before You Look
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before viewing <a href="#tbl-dna-lm-comparison" class="quarto-xref">Table&nbsp;<span>15.1</span></a> below, test your understanding of the architectural tradeoffs:</p>
<ol type="1">
<li>Which models do you predict handle the longest context lengths?</li>
<li>What architectural features enable processing beyond the quadratic attention bottleneck?</li>
<li>Which training objective (masked LM vs.&nbsp;next-token prediction) do you think is more common in long-context models?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Expected predictions based on earlier discussion:</strong></p>
<ol type="1">
<li><p><strong>Longest contexts:</strong> HyenaDNA and Evo 2 should handle 1 Mb contexts, since they use sub-quadratic architectures (Hyena operators and StripedHyena). Caduceus achieves 131 kb with linear-complexity Mamba. Standard transformers (DNABERT, Nucleotide Transformer) are limited to ≤6 kb due to quadratic attention.</p></li>
<li><p><strong>Enabling features:</strong> Sub-quadratic complexity is key. HyenaDNA uses implicit convolutions with O(L log L) complexity via FFT. Caduceus uses Mamba state-space models with O(L) complexity. Evo 2 uses hybrid StripedHyena combining both approaches.</p></li>
<li><p><strong>Training objective:</strong> Long-context models tend to use next-token (autoregressive) prediction rather than masked LM. This is because autoregressive training naturally supports generation and may provide better gradients for learning long-range dependencies. Notice in the table: all sub-quadratic models use next-token prediction.</p></li>
</ol>
</div>
</div>
</div>
</div>
</div>
<p>The following table highlights five landmark DNA language models representing distinct paradigms. The chapter discusses additional models including <em>DNABERT-2</em> (BPE tokenization), <em>GPN</em> (cross-species VEP), and <em>Evo</em> (prokaryotic scale).</p>
<div id="tbl-dna-lm-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dna-lm-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.1: Landmark DNA language models representing distinct architectural paradigms.
</figcaption>
<div aria-describedby="tbl-dna-lm-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 14%">
<col style="width: 18%">
<col style="width: 21%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Year</th>
<th>Context</th>
<th>Parameters</th>
<th>Architecture</th>
<th>Key Innovation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>DNABERT</em></td>
<td>2021</td>
<td>~500 bp</td>
<td>110M</td>
<td>Transformer</td>
<td>First DNA LM, masked pretraining</td>
</tr>
<tr class="even">
<td><em>Nucleotide Transformer</em></td>
<td>2023</td>
<td>6 kb</td>
<td>500M-2.5B</td>
<td>Transformer</td>
<td>Multi-species scaling</td>
</tr>
<tr class="odd">
<td><em>HyenaDNA</em></td>
<td>2023</td>
<td>1 Mb</td>
<td>1.6M-6.6M</td>
<td>Hyena</td>
<td>Sub-quadratic long context</td>
</tr>
<tr class="even">
<td><em>Caduceus</em></td>
<td>2024</td>
<td>131 kb</td>
<td>1.6M-6.6M</td>
<td>Mamba</td>
<td>RC-equivariance</td>
</tr>
<tr class="odd">
<td><em>Evo 2</em></td>
<td>2025</td>
<td>1 Mb</td>
<td>7B-40B</td>
<td>StripedHyena 2</td>
<td>Pan-genomic, largest scale</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Beyond the models detailed above, the Genomic Language Model (gLM) demonstrates that architectural innovations beyond standard BERT encoders (including mixture-of-experts and improved positional encodings) can yield competitive performance while enabling joint DNA-protein modeling <span class="citation" data-cites="cornman_glm_2024">(<a href="../bib/references.html#ref-cornman_glm_2024" role="doc-biblioref">Cornman et al. 2024</a>)</span>. This multimodal capability suggests future convergence between DNA and protein foundation models.</p>
</section>
</section>
<section id="sec-ch15-training-data" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="sec-ch15-training-data"><span class="header-section-number">15.6</span> Training Data and What Models Learn</h2>
<p>DNA language models are trained on diverse corpora ranging from single reference genomes to pan-genomic collections spanning the tree of life. Understanding what training data is used and what models learn from it is essential for anticipating model capabilities and limitations.</p>
<section id="sec-ch15-corpus-composition" class="level3" data-number="15.6.1">
<h3 data-number="15.6.1" class="anchored" data-anchor-id="sec-ch15-corpus-composition"><span class="header-section-number">15.6.1</span> Training Corpus Composition</h3>
<p>Early models like <em>DNABERT</em> trained primarily on the human reference genome (GRCh38), providing exposure to approximately 3 billion nucleotides from a single individual. The <em>Nucleotide Transformer</em> expanded to include multiple species and human population variation from resources like the 1000 Genomes Project (<a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a>). <em>Evo 2</em> scaled to 9.3 trillion base pairs spanning all domains of life, including complete bacterial chromosomes, eukaryotic genomes, viral sequences, and metagenomic assemblies.</p>
<div id="fig-evo2-training" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-evo2-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/04-A-fig-evo2-training.svg" class="img-fluid figure-img"></p>
<figcaption>Pan-genomic training data for Evo 2</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/04-B-fig-evo2-training.svg" class="img-fluid figure-img"></p>
<figcaption>Context length curriculum for stable training</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/04-C-fig-evo2-training.svg" class="img-fluid figure-img"></p>
<figcaption>Evo 2 generates biologically coherent sequences</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-evo2-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.4: Evo 2 training regime for pan-genomic understanding and generation. (A) Training data spans 2.7 trillion nucleotides across all domains of life, enabling learning of universal and lineage-specific sequence patterns. (B) Context length curriculum progressively extends from 8k to 1M tokens, maintaining stable optimization through each transition. (C) The resulting model generates novel sequences with emergent biological coherence: appropriate gene structure, regulatory elements, and statistical properties matching natural genomes. This combination of massive scale, architectural innovation, and careful training regime enables both understanding and generation of genomic sequence.
</figcaption>
</figure>
</div>
<p>The composition of training data shapes what models learn. Reference-only training captures the genome’s architecture but not population variation. Multi-individual training exposes models to common polymorphisms but may underrepresent rare variants. Cross-species training provides evolutionary context (constrained regions are conserved, variable regions diverge) but may not capture species-specific regulatory patterns. Training on functional genomics data (<em>GROVER</em>-style approaches) teaches regulatory activity patterns but ties models to specific assays and cell types.</p>
<p>A tension exists between generality and specificity. Models trained on broader corpora learn more general representations that transfer across species and contexts, but may underperform narrower models on specific applications. Models trained on focused datasets may capture task-relevant patterns more effectively but transfer less well. The optimal training strategy depends on intended applications.</p>
</section>
<section id="sec-ch15-probing" class="level3" data-number="15.6.2">
<h3 data-number="15.6.2" class="anchored" data-anchor-id="sec-ch15-probing"><span class="header-section-number">15.6.2</span> Probing What Models Learn</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>If a model was trained only to predict masked nucleotides (never seeing any labels), how could we determine whether it learned to recognize splice sites, transcription factor binding sites, or gene boundaries? What experiments would reveal this?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Use <strong>linear probing</strong>: freeze the pretrained model and extract embeddings for sequences with known annotations (e.g., windows around splice sites versus random genomic positions). Train a simple linear classifier (logistic regression) on these frozen embeddings to predict the annotation. If the classifier achieves high accuracy, the embeddings must encode information distinguishing that feature—meaning the model learned to recognize it during pretraining. This works because if splice sites have different statistical patterns than non-splice sites, and the model learned to predict nucleotides accurately, it must have internalized what makes splice sites distinctive. The probing methodology is detailed in <a href="../part_3/p3-ch09-transfer.html#sec-ch09-probing-representations" class="quarto-xref"><span>Section 9.3.3</span></a>.</p>
</div>
</div>
</div>
</div>
</div>
<p><strong>Linear probing</strong> experiments reveal what information is encoded in model representations without task-specific fine-tuning. By training simple classifiers (logistic regression, single-layer perceptrons) on frozen embeddings to predict known annotations, researchers can assess whether models have learned biologically meaningful patterns. The methodology for such probing experiments is detailed in <a href="../part_3/p3-ch09-transfer.html#sec-ch09-probing-representations" class="quarto-xref"><span>Section 9.3.3</span></a>, with implications for interpretability examined in <a href="../part_6/p6-ch25-interpretability.html#sec-ch25-probing" class="quarto-xref"><span>Section 25.4</span></a>.</p>
<p>Probing studies reveal that DNA language model representations encode information corresponding to several categories of genomic features. Model embeddings contain patterns corresponding to known transcription factor binding sites, splice signals, and other sequence motifs; probing for specific motif presence shows that embeddings can distinguish sequences containing binding sites from those lacking them. Representations also encode gene structure: probes can distinguish coding from noncoding regions, identify exon-intron boundaries, and recognize splice donor and acceptor sites. Whether models actively <em>use</em> this information for downstream predictions (rather than exploiting correlated features) remains an open question requiring mechanistic interpretation. This information emerges from sequence statistics alone, suggesting that the compositional and structural differences between genomic region types are detectable from DNA sequence.</p>
<p>Evolutionary constraints are implicitly captured, particularly in models trained on multi-species data. Positions under purifying selection (constrained across evolution) show different embedding patterns than neutral positions. This provides a self-supervised analog to traditional conservation scoring, though the relationship between model-learned and alignment-based conservation measures varies across genomic contexts.</p>
<p>More complex patterns like regulatory grammar (the syntax governing how transcription factors combine to specify expression) show mixed evidence. Models capture some aspects of regulatory logic, such as the spacing preferences between binding sites, but may not fully represent the combinatorial complexity of enhancer function. Similarly, long-range dependencies (enhancer-promoter interactions across tens of kilobases) are accessible to long-context models but require extensive probing to assess whether they are actually used.</p>
</section>
<section id="sec-ch15-limitations-learned" class="level3" data-number="15.6.3">
<h3 data-number="15.6.3" class="anchored" data-anchor-id="sec-ch15-limitations-learned"><span class="header-section-number">15.6.3</span> What Models Do Not Learn</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check: Connect to Earlier Material
</div>
</div>
<div class="callout-body-container callout-body">
<p>Recall the feature ceiling discussed in <a href="../part_1/p1-ch04-vep-classical.html#sec-ch04-features-to-representations" class="quarto-xref"><span>Section 4.6.4</span></a>. How do the limitations of sequence-only DNA language models relate to that concept? Think about what information is available versus what is needed for complete predictions.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The feature ceiling occurs when model performance is bounded by what the input features can represent, not by model architecture or training data. DNA language models face a fundamental feature ceiling: they can only learn from sequence, but gene regulation depends on epigenetic state, 3D chromatin structure, and cellular context that sequence does not encode. Even with infinite parameters and perfect optimization, a sequence-only model cannot predict cell-type-specific chromatin accessibility because the information simply is not present in DNA sequence alone. This is analogous to trying to predict splice patterns without knowing which tissues a gene is expressed in—the necessary information is missing. Breaking this ceiling requires multi-modal integration (Part IV) that incorporates functional genomics data beyond sequence.</p>
</div>
</div>
</div>
</div>
</div>
<p>Equally important is recognizing what current DNA language models struggle to represent. Sequence-only models cannot capture epigenetic context: DNA methylation, histone modifications, and chromatin accessibility all affect gene regulation but are not encoded in primary sequence. Some models (like <em>GROVER</em>) address this by incorporating functional genomics data, but this ties them to specific cell types and experimental conditions.</p>
<p>The three-dimensional structure of chromatin affects which regulatory elements can physically interact, but linear sequence models cannot represent folding (<a href="../part_5/p5-ch21-3d-genome.html" class="quarto-xref"><span>Chapter 21</span></a>). Cell-type specificity of gene regulation depends on transcription factor expression levels and chromatin state, not just sequence; models trained on sequence alone can predict potential regulatory activity but not its realization in specific contexts.</p>
<p>Complex variant patterns beyond single nucleotide changes remain challenging. Indels, structural variants, repeat expansions, and epistatic interactions between distant loci are either not representable (depending on tokenization) or poorly predicted. Most benchmark tasks focus on SNVs, leaving multi-nucleotide effects underexplored.</p>
<p>These are not data limitations but architectural ones. Expanding the training corpus or increasing model scale cannot provide information not present in DNA sequence itself. No amount of pretraining data will allow a sequence-only model to predict cell-type-specific chromatin accessibility, because that information requires knowing the cellular context, not more sequence. Breaking this ceiling requires multi-modal inputs that incorporate functional genomics data beyond sequence, as addressed in Part V.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Predict Before You Look
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before viewing <a href="#tbl-what-models-learn" class="quarto-xref">Table&nbsp;<span>15.2</span></a> below, make predictions based on what you have learned:</p>
<ol type="1">
<li>What categories of biological features can sequence-only models learn?</li>
<li>What requires information beyond DNA sequence?</li>
<li>Where do you think the fundamental boundary lies between “learnable from sequence” and “requires additional context”?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Learnable from sequence:</strong> Features encoded in DNA sequence itself: motifs (TF binding sites, splice signals), gene structure (exon boundaries, coding regions), and evolutionary constraint (patterns conserved across species). These emerge from statistical regularities in genomic sequence.</p>
<p><strong>Requires additional context:</strong> Features dependent on cellular state: epigenetic modifications (methylation, histone marks), 3D chromatin organization (which enhancers contact which promoters), and cell-type-specific activity (whether a motif is actually bound by TFs present in that cell).</p>
<p><strong>Fundamental boundary:</strong> Sequence encodes <em>potential</em> (what <em>could</em> happen), but realization depends on <em>context</em> (what <em>does</em> happen in specific cells). A promoter sequence contains information about where transcription could initiate, but whether it actually does depends on transcription factor availability, chromatin accessibility, and regulatory signals, none of which are in the DNA sequence alone.</p>
</div>
</div>
</div>
</div>
</div>
<p>The following table summarizes what DNA language models can and cannot learn from sequence alone:</p>
<div id="tbl-what-models-learn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-what-models-learn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.2: What DNA language models can and cannot learn from sequence alone. The fundamental limitation is that sequence encodes <em>potential</em>, not <em>realization</em> in specific cellular contexts.
</figcaption>
<div aria-describedby="tbl-what-models-learn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 31%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Category</th>
<th>Can Learn</th>
<th>Cannot Learn</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Sequence motifs</strong></td>
<td>TF binding sites, splice signals, promoter elements</td>
<td>Cell-type-specific activity of motifs</td>
</tr>
<tr class="even">
<td><strong>Gene structure</strong></td>
<td>Exon-intron boundaries, coding vs.&nbsp;noncoding</td>
<td>Alternative splicing patterns in specific tissues</td>
</tr>
<tr class="odd">
<td><strong>Evolutionary constraint</strong></td>
<td>Conservation patterns from cross-species training</td>
<td>Recent selection not captured in training data</td>
</tr>
<tr class="even">
<td><strong>Regulatory grammar</strong></td>
<td>Spacing preferences between motifs</td>
<td>Full combinatorial logic of enhancers</td>
</tr>
<tr class="odd">
<td><strong>Epigenetic state</strong></td>
<td>—</td>
<td>DNA methylation, histone modifications, chromatin accessibility</td>
</tr>
<tr class="even">
<td><strong>3D structure</strong></td>
<td>—</td>
<td>Chromatin folding, enhancer-promoter contacts</td>
</tr>
<tr class="odd">
<td><strong>Complex variants</strong></td>
<td>SNVs (with limitations)</td>
<td>Indels, structural variants, repeat expansions</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-dna-lm-probing" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dna-lm-probing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/05-A-fig-dna-lm-probing.svg" class="img-fluid figure-img"></p>
<figcaption>DNA LMs encode genomic element identity</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/05-B-fig-dna-lm-probing.svg" class="img-fluid figure-img"></p>
<figcaption>Model attention correlates with conservation</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/05-C-fig-dna-lm-probing.svg" class="img-fluid figure-img"></p>
<figcaption>DNA LMs learn regulatory grammar</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/05-D-fig-dna-lm-probing.svg" class="img-fluid figure-img"></p>
<figcaption>Probing reveals fundamental limitations</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dna-lm-probing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.5: Probing analysis reveals what DNA language models learn, and what they cannot. (A) Frozen embeddings support high-accuracy linear classification of genomic element types including promoters, enhancers, and splice sites. (B) Model attention correlates with evolutionary conservation, suggesting learned focus on functionally important positions. (C) Attention patterns capture long-range regulatory relationships between enhancers and promoters. (D) However, probing fails for context-dependent properties: tissue-specific activity, environmental responses, and 3D chromatin organization cannot be predicted from sequence representations alone. DNA language models learn sequence potential; realizing that potential requires cellular context they cannot access.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ch15-benchmarks" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="sec-ch15-benchmarks"><span class="header-section-number">15.7</span> Benchmark Performance and Evaluation</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Spaced Retrieval: Test Your Memory
</div>
</div>
<div class="callout-body-container callout-body">
<p>Without looking back, try to recall from <a href="#sec-ch15-task-specific-to-general" class="quarto-xref"><span>Section 15.1</span></a>:</p>
<ol type="1">
<li>What were the three key limitations of task-specific CNN models that motivated the shift to foundation models?</li>
<li>What is the core difference between the old “task-first” workflow and the new “representation-first” paradigm?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Three limitations of task-specific CNNs:</strong></p>
<ol type="1">
<li><strong>Data dependency:</strong> Every new task required fresh labeled data; models could not transfer knowledge</li>
<li><strong>Architecture rigidity:</strong> Model architectures were tailored to specific tasks and did not generalize</li>
<li><strong>Feature isolation:</strong> Learned features did not transfer across tasks without re-engineering</li>
</ol>
<p><strong>Workflow shift:</strong></p>
<ul>
<li><strong>Old (task-first):</strong> Design specialized architecture for task → collect labeled data → train from scratch → repeat for each new task</li>
<li><strong>New (representation-first):</strong> Pretrain general-purpose model on unlabeled sequences → extract reusable representations → adapt to any downstream task with minimal data</li>
</ul>
<p>This paradigm shift is why DNA language models can support diverse applications (regulatory classification, variant scoring, splice prediction) without task-specific retraining.</p>
</div>
</div>
</div>
</div>
</div>
<p>Standardized benchmarks enable systematic comparison across DNA language models, though each benchmark captures only part of what we care about. Understanding benchmark construction and limitations is essential for interpreting performance claims.</p>
<section id="sec-ch15-benchmark-suites" class="level3" data-number="15.7.1">
<h3 data-number="15.7.1" class="anchored" data-anchor-id="sec-ch15-benchmark-suites"><span class="header-section-number">15.7.1</span> Major Benchmark Suites</h3>
<p>The BEND (Benchmark for Nucleotide Deep learning) suite provides a unified framework with tasks including gene finding, enhancer annotation, chromatin state prediction, and variant effect scoring <span class="citation" data-cites="marin_bend_2024">(<a href="../bib/references.html#ref-marin_bend_2024" role="doc-biblioref">Marin et al. 2024</a>)</span>. Standardized splits and metrics enable fair comparison. BEND specifically evaluates whether models capture biologically meaningful features at different resolution scales.</p>
<p>Genomic Benchmarks focus on regulatory element classification tasks: distinguishing promoters from nonpromoters, identifying active enhancers, predicting histone mark presence <span class="citation" data-cites="gresova_genomic_2023">(<a href="../bib/references.html#ref-gresova_genomic_2023" role="doc-biblioref">Grešová et al. 2023</a>)</span>. These tasks test whether model representations encode basic genomic annotations. Most current DNA language models achieve high accuracy on these tasks, suggesting benchmark saturation for simpler classification problems.</p>
<p>The Long Range Benchmark (LRB) and DNALongBench evaluate long-context modeling capabilities <span class="citation" data-cites="cheng_dnalongbench_2024">(<a href="../bib/references.html#ref-cheng_dnalongbench_2024" role="doc-biblioref">Cheng et al. 2024</a>)</span>. Tasks include predicting distal enhancer-promoter interactions, modeling chromatin structure across hundreds of kilobases, and integrating information over extended genomic windows. These benchmarks specifically test whether long-context architectures provide meaningful advantages over shorter-context models.</p>
<p>Comparative evaluations across model families reveal that no single architecture dominates all tasks <span class="citation" data-cites="manzo_comparative_2025">(<a href="../bib/references.html#ref-manzo_comparative_2025" role="doc-biblioref">Manzo, Borkowski, and Ovcharenko 2025</a>)</span>. Performance varies substantially depending on task characteristics (local motif recognition versus long-range integration), training data composition, and architectural choices. <em>HyenaDNA</em> and <em>Caduceus</em> excel on long-range tasks where their architectural innovations matter; <em>DNABERT-2</em> and <em>Nucleotide Transformer</em> perform well on shorter-range regulatory classification; <em>Evo 2</em> shows advantages on cross-species tasks and variant effect prediction.</p>
</section>
<section id="sec-ch15-benchmark-limitations" class="level3" data-number="15.7.2">
<h3 data-number="15.7.2" class="anchored" data-anchor-id="sec-ch15-benchmark-limitations"><span class="header-section-number">15.7.2</span> Benchmark Limitations</h3>
<p>Several systematic issues affect benchmark interpretation. Many benchmarks have reached saturation, where multiple models achieve near-perfect performance and discriminative power disappears. When benchmarks saturate (with multiple models exceeding 95% accuracy), performance rankings become uninformative: the remaining 5% may reflect noise, implementation details, or evaluation variance rather than meaningful capability differences. Continued use of saturated benchmarks for model ranking is not merely unhelpful but actively misleading, suggesting meaningful distinctions where none can reliably be detected. This has happened for simpler classification tasks in Genomic Benchmarks, requiring the field to develop harder evaluation tasks or adopt alternative evaluation paradigms. Data leakage arises when training and test sequences share homology, allowing models to succeed through memorization rather than generalization; the homology-aware splitting strategies required to prevent this are detailed in <a href="../part_3/p3-ch12-evaluation.html#sec-ch12-homology-aware-splitting" class="quarto-xref"><span>Section 12.2</span></a>. Careful sequence clustering (using tools like MMseqs2 or CD-HIT) is required, but many older benchmarks lack rigorous split design. The comprehensive treatment of benchmark construction and evaluation methodology appears in <a href="../part_3/p3-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a> and <a href="../part_3/p3-ch12-evaluation.html" class="quarto-xref"><span>Chapter 12</span></a>.</p>
<p>Distribution shift between benchmark data and real-world applications means strong benchmark performance may not predict deployment success. Most benchmarks derive from well-studied regions of well-characterized genomes; performance on understudied regions, rare variants, or non-European populations may differ substantially. The systematic treatment of such confounding factors appears in <a href="../part_3/p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>, with specific attention to ancestry-related performance disparities in <a href="../part_3/p3-ch13-confounding.html#sec-ch13-ancestry-confounding" class="quarto-xref"><span>Section 13.2.1</span></a>.</p>
<p>The choice of evaluation metric affects what gets optimized. auROC favors discrimination regardless of calibration; Spearman correlation measures rank ordering but not absolute effect size prediction. Clinical applications may require well-calibrated probability estimates or accurate quantitative predictions, neither of which standard metrics directly assess (<a href="../part_6/p6-ch24-uncertainty.html" class="quarto-xref"><span>Chapter 24</span></a>). The gap between benchmark performance and deployment utility remains substantial for most genomic applications.</p>
<div id="fig-dna-lm-benchmarks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dna-lm-benchmarks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch15/06-fig-dna-lm-benchmarks.svg" class="img-fluid figure-img"></p>
<figcaption>Benchmark comparison across DNA language models</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dna-lm-benchmarks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.6: Benchmark comparison across DNA language models. Heatmap shows relative performance on standardized genomic prediction tasks. Larger, newer models generally achieve higher performance, with particularly strong gains on tasks requiring long-range context (enhancer activity, gene expression) where architectural innovations enabling million-base contexts provide clear advantages. Performance on local tasks (splice prediction, promoter classification) is more saturated, with smaller models approaching larger model performance. These benchmarks primarily test frozen embedding quality; fine-tuning would yield different relative rankings.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ch15-annotation-aware" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="sec-ch15-annotation-aware"><span class="header-section-number">15.8</span> Annotation-Aware Extensions</h2>
<p>Recent work explores enriching DNA language models with explicit biological structure beyond raw sequence. These approaches represent early steps toward multi-modal genomic foundation models.</p>
<p><em>Life-Code</em> proposes central-dogma-informed tokenization, treating coding and noncoding regions differently <span class="citation" data-cites="liu_life-code_2025">(<a href="../bib/references.html#ref-liu_life-code_2025" role="doc-biblioref">Liu et al. 2025</a>)</span>. Coding regions use codon tokens (three-nucleotide units specifying amino acids), respecting the genetic code’s fundamental structure. Noncoding regions use learned subword units optimized during training. Knowledge distillation from protein language models imports protein-level structural knowledge into DNA representations. <em>Life-Code</em> achieves competitive results across DNA, RNA, and protein tasks, suggesting that encoding biological structure into tokenization provides useful inductive bias (<a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>).</p>
<p><em>BioToken</em> extends tokenization to include explicit genomic annotations <span class="citation" data-cites="medvedev_biotoken_2025">(<a href="../bib/references.html#ref-medvedev_biotoken_2025" role="doc-biblioref">Medvedev et al. 2025</a>)</span>. Rather than representing regions purely as nucleotide strings, <em>BioToken</em> creates composite tokens encoding sequence content, variant presence, structural annotations (exon, intron, UTR), and functional context. The associated <em>BioFM</em> model achieves state-of-the-art performance across genomic benchmarks with substantially fewer parameters (265M), suggesting that annotation-aware representations improve parameter efficiency.</p>
<p>These approaches foreshadow the multi-modal foundation models discussed in Part IV (<a href="../part_5/p5-ch23-multi-omics.html" class="quarto-xref"><span>Chapter 23</span></a>), where sequence is only one of many integrated information streams.</p>
</section>
<section id="sec-ch15-practical-use" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="sec-ch15-practical-use"><span class="header-section-number">15.9</span> Using DNA Language Models in Practice</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Choosing a Usage Pattern
</div>
</div>
<div class="callout-body-container callout-body">
<p>The right approach depends on your data and computational constraints:</p>
<ul>
<li><strong>Little/no labeled data:</strong> Use zero-shot scoring or few-shot in-context learning</li>
<li><strong>Moderate labeled data (hundreds to thousands of examples):</strong> Extract embeddings, train lightweight downstream classifier</li>
<li><strong>Substantial labeled data (thousands+):</strong> Fine-tune with LoRA or full fine-tuning</li>
<li><strong>Computational constraints:</strong> Prefer embedding extraction (frozen model, one forward pass per sequence)</li>
<li><strong>Maximum performance:</strong> Full fine-tuning with sufficient data</li>
</ul>
</div>
</div>
<p>DNA language models support multiple usage patterns for different applications.</p>
<section id="sec-ch15-embeddings" class="level3" data-number="15.9.1">
<h3 data-number="15.9.1" class="anchored" data-anchor-id="sec-ch15-embeddings"><span class="header-section-number">15.9.1</span> Embeddings as Universal Features</h3>
<p>The simplest approach extracts embeddings from a pretrained model and uses them as features for downstream classifiers. The workflow involves extracting embeddings for windows around loci of interest, pooling or selecting positions relevant to the task, and training lightweight downstream models (linear layers, shallow MLPs, gradient boosting) on the extracted features.</p>
<p>This approach supports diverse applications. Regulatory element classification distinguishes promoters, enhancers, silencers, and insulators based on learned representations. Chromatin state prediction uses sequence embeddings to predict ATAC-seq or histone mark presence. Variant effect scoring replaces or augments hand-crafted features in frameworks like <em>CADD</em> with language model features (analogous to <em>CADD</em> v1.7’s incorporation of protein language model features, as discussed in <a href="../part_1/p1-ch04-vep-classical.html" class="quarto-xref"><span>Chapter 4</span></a>). The integration of these features into comprehensive variant effect prediction workflows is detailed in <a href="p4-ch18-vep-fm.html#sec-ch18-combining-evidence" class="quarto-xref"><span>Section 18.4</span></a>. Splicing analysis combines embeddings with specialized architectures.</p>
<p>Because the language model remains frozen, this approach is computationally efficient and avoids catastrophic forgetting when new tasks are added. The pretrained model serves as a general-purpose feature extractor supporting many downstream applications.</p>
</section>
<section id="sec-ch15-fine-tuning" class="level3" data-number="15.9.2">
<h3 data-number="15.9.2" class="anchored" data-anchor-id="sec-ch15-fine-tuning"><span class="header-section-number">15.9.2</span> Fine-Tuning and Adaptation</h3>
<p>When sufficient labeled data exists, fine-tuning typically outperforms frozen embedding approaches (<a href="../part_3/p3-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>). Updating all language model parameters for a specific task allows representations to specialize, achieving highest performance but requiring more compute and risking catastrophic forgetting of general knowledge.</p>
<p>Parameter-efficient methods like LoRA (Low-Rank Adaptation) offer a middle path, inserting small trainable modules into each layer while keeping the backbone mostly frozen <span class="citation" data-cites="hu_lora_2021">(<a href="../bib/references.html#ref-hu_lora_2021" role="doc-biblioref">Hu et al. 2021</a>)</span>. These approaches achieve most of the performance gains of full fine-tuning while maintaining computational efficiency and preserving general capabilities. Adapter-based methods similarly add small bottleneck modules tuned for specific tasks.</p>
</section>
<section id="sec-ch15-zero-shot" class="level3" data-number="15.9.3">
<h3 data-number="15.9.3" class="anchored" data-anchor-id="sec-ch15-zero-shot"><span class="header-section-number">15.9.3</span> Zero-Shot and Few-Shot Scoring</h3>
<p>For variant interpretation, language models enable zero-shot scoring based on sequence likelihood. Compute the model’s probability for a sequence containing the reference allele, compare to probability for the sequence with the alternative allele, and interpret variants reducing probability as more disruptive. This approach requires no variant-specific training and can score any single-nucleotide variant the model can represent.</p>
<p>Zero-shot scoring quality depends on how well the model’s learned distribution captures biological constraints. Performance tends to improve with model scale and training data diversity (<a href="p4-ch14-fm-principles.html" class="quarto-xref"><span>Chapter 14</span></a>). Few-shot approaches include task examples in the input context, allowing in-context learning without parameter updates. <em>HyenaDNA</em> demonstrated this capability for genomic tasks, suggesting that sufficiently large models with long context can adapt through prompts rather than training.</p>
</section>
</section>
<section id="sec-ch15-open-challenges" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="sec-ch15-open-challenges"><span class="header-section-number">15.10</span> Limitations and Open Challenges</h2>
<p>Despite substantial progress, DNA language models face several fundamental limitations.</p>
<p>The tradeoff between context length and representational fidelity persists. Long-context models like <em>HyenaDNA</em> and <em>Evo 2</em> can process megabase sequences but require efficient architectures that may not capture all the relationships dense attention would learn. Whether these architectural tradeoffs matter for specific applications remains task-dependent.</p>
<p>Most tokenization schemes represent insertions and deletions awkwardly or not at all. Structural variants spanning kilobases, repeat expansions, and complex rearrangements fall outside what current models can process (<a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>). Epistatic interactions between variants at distant loci are not captured even by long-context models trained solely on single sequences.</p>
<p>Training data composition shapes model capabilities in underexplored ways. Models trained primarily on European-ancestry genomes may perform poorly on variants common in other populations (<a href="../part_3/p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>). Ascertainment bias in training databases (enrichment for coding regions, well-studied genes, specific diseases) propagates to learned representations. The field lacks systematic evaluation of performance disparities across populations.</p>
<p>Interpretability remains limited (<a href="../part_6/p6-ch25-interpretability.html" class="quarto-xref"><span>Chapter 25</span></a>). While probing studies reveal what models encode, explaining why a specific variant receives a particular score in terms connecting to biological mechanism is difficult. Attention patterns and gradient-based attribution provide some insight but often fail to identify the specific sequence features driving predictions.</p>
<p>Integration with other modalities is nascent. DNA sequence provides necessary but insufficient information for predicting gene regulation. Epigenomic state, three-dimensional chromatin structure, transcription factor concentrations, and cellular context all matter. Current DNA language models cannot represent these factors; multi-modal approaches (discussed in Part IV) aim to address this limitation.</p>
</section>
<section id="sec-ch15-soft-landing" class="level2" data-number="15.11">
<h2 data-number="15.11" class="anchored" data-anchor-id="sec-ch15-soft-landing"><span class="header-section-number">15.11</span> Representations Without Predictions</h2>
<p>DNA language models capture sequence patterns, regulatory motifs, and evolutionary constraints through self-supervised pretraining on genomic sequence. The progression from early proof-of-concept models through architectural innovations enabling megabase context demonstrates that the paradigm works: models trained to predict masked nucleotides learn representations that transfer across diverse downstream tasks. Biological inductive biases (strand symmetry, codon structure, cross-species training) can substitute for raw scale on appropriate tasks, creating opportunities for efficient models that encode domain knowledge.</p>
<p>Yet DNA language models have inherent limitations. They produce representations, not predictions. A language model can embed a sequence in a space where similar regulatory elements cluster together, but it cannot directly output the expression level that sequence will produce or the chromatin accessibility it will exhibit. The models capture what patterns exist in genomic sequence but not what those patterns do in cellular context. They cannot represent epigenomic state, three-dimensional chromatin organization, or cell-type-specific regulation without additional inputs beyond sequence.</p>
<p>These limitations define the complementary relationship between language models and sequence-to-function models. Where DNA language models learn representations from sequence statistics, regulatory models like <em>Enformer</em> and <em>Borzoi</em> predict molecular phenotypes from sequence context (<a href="p4-ch17-regulatory.html" class="quarto-xref"><span>Chapter 17</span></a>). The regulatory models provide quantitative outputs (expression levels, chromatin tracks, splice probabilities) that language models alone cannot produce. For variant effect prediction (<a href="p4-ch18-vep-fm.html" class="quarto-xref"><span>Chapter 18</span></a>), both representation quality and phenotypic prediction matter: language model embeddings capture evolutionary constraint while regulatory models predict functional consequences. Understanding what each model family provides is prerequisite to combining them effectively.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<div class="callout callout-style-default callout-tip callout-titled" title="Test Yourself">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Test Yourself
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reviewing the summary, test your recall:</p>
<ol type="1">
<li><p>What was the key innovation of DNABERT that proved self-supervised pretraining could work for DNA sequences? What were its main limitations regarding tokenization and context length?</p></li>
<li><p>Explain why standard transformer attention has quadratic complexity O(L²) and how this limits biological applications. How do HyenaDNA and Caduceus overcome this limitation?</p></li>
<li><p>What is reverse-complement equivariance and why does it matter for DNA language models? Give an example of when this property should hold versus when it should not.</p></li>
<li><p>Describe how DNA language models perform zero-shot variant scoring using likelihood ratios. What makes this approach work without pathogenicity training data?</p></li>
<li><p>DNA language models learn representations from sequence alone. What can they capture (provide three examples) and what fundamental limitations prevent them from learning (provide three examples)?</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>DNABERT innovation and limitations:</strong>
<ul>
<li><strong>Innovation:</strong> Applied BERT masked language modeling to DNA using k-mer tokenization (6-mers), demonstrating that self-supervised pretraining on genomic sequence learns transferable representations for downstream tasks</li>
<li><strong>Tokenization limitation:</strong> Overlapping k-mers create positional ambiguity for variants (each nucleotide appears in multiple adjacent tokens)</li>
<li><strong>Context limitation:</strong> Limited to 512 tokens (~few hundred bp) due to quadratic attention complexity</li>
</ul></li>
<li><strong>Quadratic complexity and solutions:</strong>
<ul>
<li><strong>Why quadratic:</strong> Standard attention computes all pairwise interactions between positions, requiring an L×L matrix. For 100kb sequence: ~10¹⁰ computations per layer</li>
<li><strong>Biological limitation:</strong> Most regulatory phenomena span &gt;10kb (enhancer-promoter interactions: 50-200kb; TADs: ~1Mb), exceeding what quadratic attention can process</li>
<li><strong>HyenaDNA solution:</strong> Implicit convolutions with O(L log L) complexity via FFT, achieving 1 Mb context</li>
<li><strong>Caduceus solution:</strong> Mamba state-space models with O(L) linear complexity, plus reverse-complement equivariance</li>
</ul></li>
<li><strong>Reverse-complement equivariance:</strong>
<ul>
<li><strong>Definition:</strong> Model predictions should be mathematically related for a sequence and its reverse complement</li>
<li><strong>Why it matters:</strong> DNA is double-stranded; many features (TF binding sites, motifs) are functionally identical regardless of which strand is read</li>
<li><strong>Should hold:</strong> Transcription factor binding sites (e.g., GAATTC has same binding activity on either strand)</li>
<li><strong>Should not hold:</strong> Genes with defined orientation (5’→3’ direction matters for transcription; strand determines sense vs.&nbsp;antisense)</li>
<li><strong>Impact:</strong> Building this constraint into architecture (Caduceus) improves efficiency and correctness</li>
</ul></li>
<li><strong>Zero-shot variant scoring:</strong>
<ul>
<li><p><strong>Method:</strong></p>
<ol type="1">
<li><p>Compute model’s log-likelihood for sequence with reference allele: log P(seq|ref);</p></li>
<li><p>Compute log-likelihood for sequence with alternate allele: log P(seq|alt);</p></li>
<li><p>Calculate likelihood ratio: Δ = log P(seq|ref) - log P(seq|alt);</p></li>
<li><p>Positive Δ means alternate reduces likelihood → inferred to be disruptive</p></li>
</ol></li>
<li><p><strong>Why it works:</strong> Pretraining forces the model to learn what sequence patterns are “allowed” in genomes. Constrained positions have confident predictions; variants violating learned constraints get low probability. This captures evolutionary constraint without explicit pathogenicity labels</p></li>
</ul></li>
<li><strong>What DNA-LMs can and cannot learn:</strong>
<ul>
<li><strong>Can learn from sequence:</strong> (a) Sequence motifs (TF binding sites, splice signals, promoter elements); (b) Gene structure (exon-intron boundaries, coding vs.&nbsp;noncoding regions); (c) Evolutionary constraints (conservation patterns from cross-species training)</li>
<li><strong>Cannot learn (missing from sequence):</strong> (a) Epigenetic state (DNA methylation, histone modifications, chromatin accessibility); (b) 3D chromatin structure (which enhancers contact which promoters); (c) Cell-type specificity (whether a regulatory element is active in specific tissues)</li>
<li><strong>Fundamental limitation:</strong> Sequence encodes potential, not realization in cellular context</li>
</ul></li>
</ol>
</div>
</div>
</div>
</div>
</div>
<p><strong>What we covered:</strong></p>
<ol type="1">
<li><p><strong>The paradigm shift</strong> from task-specific CNNs to general-purpose DNA language models that learn reusable representations from self-supervised pretraining</p></li>
<li><p><strong>Model evolution</strong> from DNABERT (512 tokens, proof of concept) through Evo 2 (1 Mb context, 40B parameters, pan-genomic)</p></li>
<li><p><strong>Architectural innovations</strong> that enable long contexts: implicit convolutions (HyenaDNA), state-space models (Caduceus), and hybrid designs (Evo 2)</p></li>
<li><p><strong>Biological inductive biases</strong> like reverse-complement equivariance that can substitute for raw scale</p></li>
<li><p><strong>What models learn</strong> (motifs, gene structure, evolutionary constraint) and <strong>what they cannot learn</strong> (epigenetic state, 3D structure, cell-type specificity)</p></li>
<li><p><strong>Practical usage patterns:</strong> embeddings as features, fine-tuning, and zero-shot variant scoring</p></li>
</ol>
<p><strong>Key takeaways:</strong></p>
<ul>
<li>DNA language models produce <em>representations</em>, not <em>predictions</em>. They capture sequence patterns but not cellular context.</li>
<li>No single model dominates all tasks; model choice depends on context length requirements and available training data.</li>
<li>Benchmark performance may not predict real-world deployment success due to distribution shift and metric limitations.</li>
</ul>
<p><strong>Looking ahead:</strong></p>
<ul>
<li>Protein language models (<a href="p4-ch16-protein-lm.html" class="quarto-xref"><span>Chapter 16</span></a>) apply similar principles to amino acid sequences</li>
<li>Regulatory models (<a href="p4-ch17-regulatory.html" class="quarto-xref"><span>Chapter 17</span></a>) predict molecular phenotypes that DNA-LMs cannot</li>
<li>Variant effect prediction (<a href="p4-ch18-vep-fm.html" class="quarto-xref"><span>Chapter 18</span></a>) combines both representation and prediction</li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-benegas_gpn_2023" class="csl-entry" role="listitem">
Benegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. <span>“[<span>GPN</span>] <span>DNA</span> Language Models Are Powerful Predictors of Genome-Wide Variant Effects.”</span> <em>Proceedings of the National Academy of Sciences</em> 120 (44): e2311219120. <a href="https://doi.org/10.1073/pnas.2311219120">https://doi.org/10.1073/pnas.2311219120</a>.
</div>
<div id="ref-boshar_foundational_nodate" class="csl-entry" role="listitem">
Boshar, Sam, Benjamin Evans, Ziqi Tang, Armand Picard, Yanis Adel, Franziska K Lorbeer, Chandana Rajesh, et al. n.d. <span>“A Foundational Model for Joint Sequence-Function Multi-Species Modeling at Scale for Long-Range Genomic Prediction.”</span>
</div>
<div id="ref-brixi_evo_2025" class="csl-entry" role="listitem">
Brixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. <span>“[<span>Evo</span> 2] <span>Genome</span> Modeling and Design Across All Domains of Life with <span>Evo</span> 2.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.02.18.638918">https://doi.org/10.1101/2025.02.18.638918</a>.
</div>
<div id="ref-cheng_dnalongbench_2024" class="csl-entry" role="listitem">
Cheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu Yang, Lei Li, and Jian Ma. 2024. <span>“<span>DNALONGBENCH</span>: <span>A</span> <span>Benchmark</span> <span>Suite</span> <span>For</span> <span>Long</span>-<span>Range</span> <span>DNA</span> <span>Prediction</span> <span>Tasks</span>,”</span> October.
</div>
<div id="ref-consens_transformers_2025" class="csl-entry" role="listitem">
Consens, Micaela E., Cameron Dufault, Michael Wainberg, Duncan Forster, Mehran Karimzadeh, Hani Goodarzi, Fabian J. Theis, Alan Moses, and Bo Wang. 2025. <span>“Transformers and Genome Language Models.”</span> <em>Nature Machine Intelligence</em> 7 (3): 346–62. <a href="https://doi.org/10.1038/s42256-025-01007-9">https://doi.org/10.1038/s42256-025-01007-9</a>.
</div>
<div id="ref-cornman_glm_2024" class="csl-entry" role="listitem">
Cornman, Andre, Jacob West-Roberts, Antonio Pedro Camargo, Simon Roux, Martin Beracochea, Milot Mirdita, Sergey Ovchinnikov, and Yunha Hwang. 2024. <span>“The <span>OMG</span> Dataset: <span>An</span> <span>Open</span> <span>MetaGenomic</span> Corpus for Mixed-Modality Genomic Language Modeling.”</span> bioRxiv. <a href="https://doi.org/10.1101/2024.08.14.607850">https://doi.org/10.1101/2024.08.14.607850</a>.
</div>
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-gresova_genomic_2023" class="csl-entry" role="listitem">
Grešová, Katarína, Vlastimil Martinek, David Čechák, Petr Šimeček, and Panagiotis Alexiou. 2023. <span>“Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification.”</span> <em>BMC Genomic Data</em> 24 (1): 25. <a href="https://doi.org/10.1186/s12863-023-01123-8">https://doi.org/10.1186/s12863-023-01123-8</a>.
</div>
<div id="ref-hu_lora_2021" class="csl-entry" role="listitem">
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. <span>“<span>LoRA</span>: <span>Low</span>-<span>Rank</span> <span>Adaptation</span> of <span>Large</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2106.09685">https://doi.org/10.48550/arXiv.2106.09685</a>.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-liu_life-code_2025" class="csl-entry" role="listitem">
Liu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. <span>“Life-<span>Code</span>: <span>Central</span> <span>Dogma</span> <span>Modeling</span> with <span>Multi</span>-<span>Omics</span> <span>Sequence</span> <span>Unification</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2502.07299">https://doi.org/10.48550/arXiv.2502.07299</a>.
</div>
<div id="ref-manzo_comparative_2025" class="csl-entry" role="listitem">
Manzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025. <span>“Comparative <span>Analysis</span> of <span>Deep</span> <span>Learning</span> <span>Models</span> for <span>Predicting</span> <span>Causative</span> <span>Regulatory</span> <span>Variants</span>.”</span> <em>bioRxiv: The Preprint Server for Biology</em>, June, 2025.05.19.654920. <a href="https://doi.org/10.1101/2025.05.19.654920">https://doi.org/10.1101/2025.05.19.654920</a>.
</div>
<div id="ref-marin_bend_2024" class="csl-entry" role="listitem">
Marin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. <span>“<span>BEND</span>: <span>Benchmarking</span> <span>DNA</span> <span>Language</span> <span>Models</span> on Biologically Meaningful Tasks.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2311.12570">https://doi.org/10.48550/arXiv.2311.12570</a>.
</div>
<div id="ref-medvedev_biotoken_2025" class="csl-entry" role="listitem">
Medvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. <span>“<span>BioToken</span> and <span>BioFM</span> – <span>Biologically</span>-<span>Informed</span> <span>Tokenization</span> <span>Enables</span> <span>Accurate</span> and <span>Efficient</span> <span>Genomic</span> <span>Foundation</span> <span>Models</span>.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.03.27.645711">https://doi.org/10.1101/2025.03.27.645711</a>.
</div>
<div id="ref-nguyen_sequence_2024" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. <span>“Sequence Modeling and Design from Molecular to Genome Scale with <span>Evo</span>.”</span> <em>Science</em> 386 (6723): eado9336. <a href="https://doi.org/10.1126/science.ado9336">https://doi.org/10.1126/science.ado9336</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-outeiral_codon_2024" class="csl-entry" role="listitem">
Outeiral, Carlos, and Charlotte M. Deane. 2024. <span>“Codon Language Embeddings Provide Strong Signals for Use in Protein Engineering.”</span> <em>Nature Machine Intelligence</em> 6 (2): 170–79. <a href="https://doi.org/10.1038/s42256-024-00791-0">https://doi.org/10.1038/s42256-024-00791-0</a>.
</div>
<div id="ref-schiff_caduceus_2024" class="csl-entry" role="listitem">
Schiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. <span>“Caduceus: <span>Bi</span>-<span>Directional</span> <span>Equivariant</span> <span>Long</span>-<span>Range</span> <span>DNA</span> <span>Sequence</span> <span>Modeling</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2403.03234">https://doi.org/10.48550/arXiv.2403.03234</a>.
</div>
<div id="ref-zhou_dnabert-2_2024" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_4/p4-ch14-fm-principles.html" class="pagination-link" aria-label="Foundation Model Paradigm">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_4/p4-ch16-protein-lm.html" class="pagination-link" aria-label="Protein Language Models">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>