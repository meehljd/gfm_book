<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>14&nbsp; Foundation Model Paradigm – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_4/p4-ch15-dna-lm.html" rel="next">
<link href="../part_4/p4--fm-families.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_4/p4--fm-families.html">Part IV: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_4/p4-ch14-fm-principles.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Learning &amp; Evaluation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmark Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch12-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Evaluation Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--fm-families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch14-fm-principles.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch15-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch16-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch17-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--cellular-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Cellular Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch19-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch20-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch21-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch22-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Responsible Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch24-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch25-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch26-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Causality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_7/p7--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VII: Applications &amp; Frontiers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch28-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch29-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch30-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch31-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch32-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch14-task-specific" id="toc-sec-ch14-task-specific" class="nav-link active" data-scroll-target="#sec-ch14-task-specific"><span class="header-section-number">14.1</span> From Task-Specific Models to Foundation Models</a></li>
  <li><a href="#sec-ch14-defining" id="toc-sec-ch14-defining" class="nav-link" data-scroll-target="#sec-ch14-defining"><span class="header-section-number">14.2</span> Defining Genomic Foundation Models</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-essential-properties" id="toc-sec-ch14-essential-properties" class="nav-link" data-scroll-target="#sec-ch14-essential-properties"><span class="header-section-number">14.2.1</span> Essential Properties</a></li>
  <li><a href="#sec-ch14-what-doesnt-count" id="toc-sec-ch14-what-doesnt-count" class="nav-link" data-scroll-target="#sec-ch14-what-doesnt-count"><span class="header-section-number">14.2.2</span> What does not Count</a></li>
  <li><a href="#sec-ch14-concept-limitations" id="toc-sec-ch14-concept-limitations" class="nav-link" data-scroll-target="#sec-ch14-concept-limitations"><span class="header-section-number">14.2.3</span> Limitations of the Foundation Model Concept</a></li>
  </ul></li>
  <li><a href="#sec-ch14-scaling" id="toc-sec-ch14-scaling" class="nav-link" data-scroll-target="#sec-ch14-scaling"><span class="header-section-number">14.3</span> Scaling Laws and Compute-Optimal Training</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-scaling-framework" id="toc-sec-ch14-scaling-framework" class="nav-link" data-scroll-target="#sec-ch14-scaling-framework"><span class="header-section-number">14.3.1</span> Chinchilla Framework and Genomic Constraints</a></li>
  <li><a href="#sec-ch14-empirical-scaling" id="toc-sec-ch14-empirical-scaling" class="nav-link" data-scroll-target="#sec-ch14-empirical-scaling"><span class="header-section-number">14.3.2</span> Empirical Scaling in Genomic Models</a></li>
  <li><a href="#sec-ch14-emergence" id="toc-sec-ch14-emergence" class="nav-link" data-scroll-target="#sec-ch14-emergence"><span class="header-section-number">14.3.3</span> Emergent Capabilities</a></li>
  </ul></li>
  <li><a href="#sec-ch14-taxonomy" id="toc-sec-ch14-taxonomy" class="nav-link" data-scroll-target="#sec-ch14-taxonomy"><span class="header-section-number">14.4</span> A Taxonomy of Genomic Foundation Models</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-dna-lm" id="toc-sec-ch14-dna-lm" class="nav-link" data-scroll-target="#sec-ch14-dna-lm"><span class="header-section-number">14.4.1</span> DNA Language Models</a></li>
  <li><a href="#sec-ch14-seq-to-func" id="toc-sec-ch14-seq-to-func" class="nav-link" data-scroll-target="#sec-ch14-seq-to-func"><span class="header-section-number">14.4.2</span> Sequence-to-Function Foundation Models</a></li>
  <li><a href="#sec-ch14-vep-models" id="toc-sec-ch14-vep-models" class="nav-link" data-scroll-target="#sec-ch14-vep-models"><span class="header-section-number">14.4.3</span> Variant Effect Prediction Models</a></li>
  <li><a href="#sec-ch14-multi-omic" id="toc-sec-ch14-multi-omic" class="nav-link" data-scroll-target="#sec-ch14-multi-omic"><span class="header-section-number">14.4.4</span> Multi-Omic Foundation Models</a></li>
  </ul></li>
  <li><a href="#sec-ch14-design-dimensions" id="toc-sec-ch14-design-dimensions" class="nav-link" data-scroll-target="#sec-ch14-design-dimensions"><span class="header-section-number">14.5</span> Design Dimensions</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-data-composition" id="toc-sec-ch14-data-composition" class="nav-link" data-scroll-target="#sec-ch14-data-composition"><span class="header-section-number">14.5.1</span> Data Composition</a></li>
  <li><a href="#sec-ch14-architecture" id="toc-sec-ch14-architecture" class="nav-link" data-scroll-target="#sec-ch14-architecture"><span class="header-section-number">14.5.2</span> Architecture Choices</a></li>
  <li><a href="#sec-ch14-context-length" id="toc-sec-ch14-context-length" class="nav-link" data-scroll-target="#sec-ch14-context-length"><span class="header-section-number">14.5.3</span> Context Length</a></li>
  <li><a href="#sec-ch14-tokenization" id="toc-sec-ch14-tokenization" class="nav-link" data-scroll-target="#sec-ch14-tokenization"><span class="header-section-number">14.5.4</span> Tokenization</a></li>
  </ul></li>
  <li><a href="#sec-ch14-build-vs-use" id="toc-sec-ch14-build-vs-use" class="nav-link" data-scroll-target="#sec-ch14-build-vs-use"><span class="header-section-number">14.6</span> Build Versus Use Decisions</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-use-existing" id="toc-sec-ch14-use-existing" class="nav-link" data-scroll-target="#sec-ch14-use-existing"><span class="header-section-number">14.6.1</span> When to Use Existing Models</a></li>
  <li><a href="#sec-ch14-adapt-existing" id="toc-sec-ch14-adapt-existing" class="nav-link" data-scroll-target="#sec-ch14-adapt-existing"><span class="header-section-number">14.6.2</span> When to Adapt Existing Models</a></li>
  <li><a href="#sec-ch14-train-scratch" id="toc-sec-ch14-train-scratch" class="nav-link" data-scroll-target="#sec-ch14-train-scratch"><span class="header-section-number">14.6.3</span> When to Train from Scratch</a></li>
  <li><a href="#sec-ch14-cost-benefit" id="toc-sec-ch14-cost-benefit" class="nav-link" data-scroll-target="#sec-ch14-cost-benefit"><span class="header-section-number">14.6.4</span> Cost-Benefit Analysis</a></li>
  </ul></li>
  <li><a href="#sec-ch14-evaluation" id="toc-sec-ch14-evaluation" class="nav-link" data-scroll-target="#sec-ch14-evaluation"><span class="header-section-number">14.7</span> Evaluation Principles</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-multi-task" id="toc-sec-ch14-multi-task" class="nav-link" data-scroll-target="#sec-ch14-multi-task"><span class="header-section-number">14.7.1</span> Multi-Task Assessment</a></li>
  <li><a href="#sec-ch14-transfer-eval" id="toc-sec-ch14-transfer-eval" class="nav-link" data-scroll-target="#sec-ch14-transfer-eval"><span class="header-section-number">14.7.2</span> Transfer Versus Pretraining Performance</a></li>
  </ul></li>
  <li><a href="#sec-ch14-ecosystem" id="toc-sec-ch14-ecosystem" class="nav-link" data-scroll-target="#sec-ch14-ecosystem"><span class="header-section-number">14.8</span> Foundation Model Ecosystem</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-distribution" id="toc-sec-ch14-distribution" class="nav-link" data-scroll-target="#sec-ch14-distribution"><span class="header-section-number">14.8.1</span> Model Distribution</a></li>
  <li><a href="#sec-ch14-documentation" id="toc-sec-ch14-documentation" class="nav-link" data-scroll-target="#sec-ch14-documentation"><span class="header-section-number">14.8.2</span> Documentation Requirements</a></li>
  <li><a href="#sec-ch14-contributions" id="toc-sec-ch14-contributions" class="nav-link" data-scroll-target="#sec-ch14-contributions"><span class="header-section-number">14.8.3</span> Industry and Academic Contributions</a></li>
  </ul></li>
  <li><a href="#sec-ch14-open-questions" id="toc-sec-ch14-open-questions" class="nav-link" data-scroll-target="#sec-ch14-open-questions"><span class="header-section-number">14.9</span> Open Questions</a></li>
  <li><a href="#sec-ch14-convergence" id="toc-sec-ch14-convergence" class="nav-link" data-scroll-target="#sec-ch14-convergence"><span class="header-section-number">14.10</span> Convergence Without Consolidation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_4/p4--fm-families.html">Part IV: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_4/p4-ch14-fm-principles.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch14-fm-principles" class="quarto-section-identifier"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>Four questions about one variant. Four separate models. Months of waiting.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Estimated reading time:</strong> 40-50 minutes</p>
<p><strong>Prerequisites:</strong> This chapter builds on concepts from <a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> (convolutional architectures), <a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a> (attention mechanisms), and <a href="../part_3/p3-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a> (self-supervised learning). Familiarity with basic neural network training and the distinction between supervised and unsupervised learning will help you follow the discussion.</p>
<p><strong>Learning Objectives:</strong> After completing this chapter, you should be able to:</p>
<ol type="1">
<li>Define what distinguishes a genomic foundation model from a task-specific deep learning model</li>
<li>Explain the scaling law framework and its implications for model development decisions</li>
<li>Compare the four major families of genomic foundation models and their respective strengths</li>
<li>Apply the build-versus-use decision framework to choose appropriate models for specific applications</li>
<li>Evaluate foundation models across multiple tasks rather than single benchmarks</li>
</ol>
<p><strong>Key Insight:</strong> Foundation models represent a paradigm shift from training separate models for each task to pretraining a single general-purpose model that can be adapted to many downstream applications. This changes not just how models are built, but how researchers interact with them, shifting from training practitioners to adaptation specialists.</p>
</div>
</div>
<p>In 2019, a family arrived at a genetics clinic after their infant was diagnosed with a novel cardiac arrhythmia syndrome. Whole-genome sequencing identified a candidate variant, but determining whether it was pathogenic required answering three regulatory questions: Would this variant disrupt a transcription factor binding site? Would it alter chromatin accessibility in cardiomyocytes? Could it create a cryptic splice site? The research team trained three separate models, each requiring its own data curation, hyperparameter search, and validation pipeline. When the fourth question arose (would the variant affect 3D chromatin looping?), they had to start from scratch again. Months passed. The family waited. Every new biological question about the <em>same variant</em> demanded a new model. Knowledge learned for one task provided no benefit for another, even though all four questions probed different consequences of the same few nucleotides.</p>
<p>This scenario captures the fragmentation that defined genomics’ early deep learning era. One convolutional network predicted transcription factor binding; another predicted chromatin accessibility; a third classified splice sites. Each model required its own training data, its own hyperparameter tuning, its own validation strategy. The field accumulated tools without accumulating shared knowledge.</p>
<p><strong>Foundation models</strong> promise a different approach: train once on vast genomic data, then adapt to many downstream tasks. A single model pretrained on billions of nucleotides might provide representations useful for regulatory prediction, variant interpretation, sequence design, and cross-species analysis simultaneously. Rather than curating labeled datasets for each new question, researchers could fine-tune existing models on modest task-specific data, leveraging knowledge the model acquired during <strong>pretraining</strong>. The efficiency gains would be substantial; the conceptual shift would be larger still. Where specialized models treat each genomic question as independent, foundation models assume that shared patterns underlie diverse biological phenomena and that representations capturing those patterns should transfer.</p>
<p>Why should patterns transfer across such different tasks? The key insight is that many genomic prediction tasks share underlying structure. Consider how learning to read opens access to novels, newspapers, scientific papers, and poetry: a single skill unlocks diverse applications because all written text shares common structures (letters, words, grammar). Similarly, diverse genomic tasks share common structures: motifs, regulatory grammar, and evolutionary signatures. Predicting whether a variant disrupts a transcription factor binding site requires understanding sequence motifs; predicting whether that same variant affects chromatin accessibility also requires understanding sequence motifs. A model that learns robust motif representations during pretraining can apply those representations to both tasks. More fundamentally, evolution has shaped genomic sequence through shared mechanisms: natural selection acts on function, and functional sequences share statistical properties regardless of which specific function is measured. A model that learns these shared properties during pretraining has learned something relevant to many downstream questions.</p>
<p>This paradigm, which transformed natural language processing and protein structure prediction, carries both promise and peril for genomics. Pretraining at scale requires computational resources beyond most academic budgets. Adaptation to specific tasks demands expertise in <strong>transfer learning</strong> techniques that remain poorly understood (<a href="../part_3/p3-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>). Predictions from general-purpose models may lack the precision of specialized alternatives trained directly on task-specific data. The decision to use, adapt, or build foundation models involves tradeoffs that depend on available resources, target applications, and acceptable uncertainty.</p>
<section id="sec-ch14-task-specific" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="sec-ch14-task-specific"><span class="header-section-number">14.1</span> From Task-Specific Models to Foundation Models</h2>
<p>The history of computational genomics reveals a consistent pattern: models become more general while maintaining or improving task-specific performance. Hand-crafted scores such as <em>CADD</em> and <em>SIFT</em> established that integration of diverse genomic annotations could improve variant pathogenicity prediction <span class="citation" data-cites="rentzsch_cadd_2019 schubach_cadd_2024">(<a href="../bib/references.html#ref-rentzsch_cadd_2019" role="doc-biblioref">Rentzsch et al. 2019</a>; <a href="../bib/references.html#ref-schubach_cadd_2024" role="doc-biblioref">Schubach et al. 2024</a>)</span> (<a href="../part_1/p1-ch04-vep-classical.html" class="quarto-xref"><span>Chapter 4</span></a>). These approaches faced a ceiling imposed by the features available for engineering, a limitation examined in <a href="../part_1/p1-ch04-vep-classical.html#sec-ch04-features-to-representations" class="quarto-xref"><span>Section 4.6.4</span></a>. These approaches relied on expert feature engineering, combining conservation scores, functional annotations, and population frequency data through ensemble methods or logistic regression.</p>
<p>Task-specific deep learning models demonstrated that neural networks could learn relevant features directly from sequence. <em>DeepSEA</em> predicted chromatin accessibility and transcription factor binding from 1 kb sequences using convolutional architectures <span class="citation" data-cites="zhou_deepsea_2015">(<a href="../bib/references.html#ref-zhou_deepsea_2015" role="doc-biblioref">J. Zhou and Troyanskaya 2015</a>)</span>. <em>ExPecto</em> extended this approach to gene expression prediction by modeling regulatory elements across multiple cell types <span class="citation" data-cites="zhou_expecto_2018">(<a href="../bib/references.html#ref-zhou_expecto_2018" role="doc-biblioref">J. Zhou et al. 2018</a>)</span>. <em>Sei</em> organized regulatory predictions into interpretable sequence classes through unsupervised clustering <span class="citation" data-cites="chen_deepsea_2022">(<a href="../bib/references.html#ref-chen_deepsea_2022" role="doc-biblioref">Chen et al. 2022</a>)</span>. <em>SpliceAI</em> achieved near-perfect splice site prediction through dilated convolutions over 10 kb contexts, though its architecture was purpose-built for this specific task and could not generalize to other regulatory prediction problems (<a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>). <em>Enformer</em> scaled sequence-to-function modeling to 200 kb windows and thousands of chromatin tracks through transformer architectures <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>.</p>
<p>These models succeeded within their specific domains but remained difficult to repurpose. Training a <em>DeepSEA</em> model required chromatin accessibility data. Using <em>SpliceAI</em> for regulatory prediction would require complete retraining on different labels. Each application domain needed its own model, trained from scratch on task-specific data. The fundamental limitation was not model capacity but training paradigm: supervised learning on narrow tasks produced narrow capabilities.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Predict Before You Look">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Predict Before You Look
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before viewing the figure below, make a prediction: In the task-specific paradigm, if a researcher needs to solve five different genomic prediction problems (e.g., splice site prediction, enhancer identification, transcription factor binding, chromatin accessibility, and gene expression), how many separate models would they need to train? In the foundation model paradigm, how many models would be trained from scratch? What is the key difference in how knowledge is reused between these two approaches?</p>
</div>
</div>
<div id="fig-paradigm-shift" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paradigm-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch14/01-A-fig-paradigm-shift.svg" class="img-fluid figure-img"></p>
<figcaption>Task-specific paradigm: isolated models for isolated tasks</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch14/01-B-fig-paradigm-shift.svg" class="img-fluid figure-img"></p>
<figcaption>Foundation model paradigm: shared representations, efficient adaptation</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paradigm-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: The paradigm shift from task-specific to foundation models. (A) The task-specific paradigm trains separate models from scratch for each application. Knowledge about sequence patterns cannot transfer between tasks, requiring substantial labeled data for each new application. (B) The foundation model paradigm pretrains a single large model on diverse unlabeled sequences, capturing general biological knowledge in reusable representations. Small task-specific adapters enable efficient transfer to diverse downstream tasks. This paradigm shift mirrors developments in natural language processing, where pretrained language models revolutionized the efficiency and capability of text-based AI systems.
</figcaption>
</figure>
</div>
<p>Sequence language models introduced the self-supervised pretraining paradigm (<a href="../part_3/p3-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>) to genomics. <em>DNABERT</em> applied <strong>masked language modeling</strong> to DNA sequences, demonstrating that general representations could be learned without task-specific labels <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span> (<a href="p4-ch15-dna-lm.html#sec-ch15-dnabert" class="quarto-xref"><span>Section 15.2</span></a>). <em>ESM</em> and <em>ESM-2</em> showed that protein language models pretrained on sequence alone could transfer effectively to structure prediction, variant effect prediction, and protein design <span class="citation" data-cites="rives_esm-1b_2021 lin_esm-2_2022">(<a href="../bib/references.html#ref-rives_esm-1b_2021" role="doc-biblioref">Rives et al. 2021</a>; <a href="../bib/references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span> (<a href="p4-ch16-protein-lm.html#sec-ch16-esm-family" class="quarto-xref"><span>Section 16.1</span></a>). The <em>Nucleotide Transformer</em> family scaled DNA language modeling to cross-species training corpora <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span> (<a href="p4-ch15-dna-lm.html#sec-ch15-nucleotide-transformer" class="quarto-xref"><span>Section 15.3</span></a>). <em>HyenaDNA</em> used implicit convolutions to reach million-token contexts at single-nucleotide resolution <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span> (<a href="p4-ch15-dna-lm.html#sec-ch15-hyenadna" class="quarto-xref"><span>Section 15.5.1</span></a>).</p>
<p>The transition from task-specific to foundation models changes the relationship between model developers and users. Task-specific models deliver predictions as their primary product. Foundation models deliver representations that users adapt to their own tasks through the transfer learning techniques examined in <a href="../part_3/p3-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>. This distinction affects everything from model architecture design to evaluation strategies to deployment infrastructure.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading about the formal definition of foundation models, consider: what properties would you require for a model to qualify as a “foundation” for multiple downstream tasks? Think about training data, output types, and how users would interact with the model.</p>
</div>
</div>
</section>
<section id="sec-ch14-defining" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="sec-ch14-defining"><span class="header-section-number">14.2</span> Defining Genomic Foundation Models</h2>
<p>The term “foundation model” appears frequently in genomics literature, sometimes applied to any large neural network trained on biological sequences. The Stanford HAI report formally defined foundation models as “models trained on broad data at scale such that they can be adapted to a wide range of downstream tasks” <span class="citation" data-cites="bommasani_opportunities_2022">(<a href="../bib/references.html#ref-bommasani_opportunities_2022" role="doc-biblioref">Bommasani et al. 2022</a>)</span>. This definition, while originating from general AI discourse, captures essential properties that distinguish true genomic foundation models from task-specific deep learning approaches. Establishing clear criteria helps separate these categories.</p>
<section id="sec-ch14-essential-properties" class="level3" data-number="14.2.1">
<h3 data-number="14.2.1" class="anchored" data-anchor-id="sec-ch14-essential-properties"><span class="header-section-number">14.2.1</span> Essential Properties</h3>
<p>The defining characteristic of genomic foundation models is their capacity to serve purposes far beyond their original training objectives. This generality emerges from several interconnected properties.</p>
<p>Foundation models train on entire genomes, pan-genomic sequence collections, or large assay compendia with minimal supervision. Their pretraining objectives include masked language modeling, next-token prediction, denoising, or multi-task sequence-to-function prediction. Critically, these objectives do not require dense task-specific labels for every training example. A model that requires annotated enhancers or curated pathogenic variants for every training instance does not qualify as a foundation model under this criterion.</p>
<p>The representations these models produce must prove useful across many downstream tasks. <strong>Embeddings</strong> can be extracted through forward passes and reused with simple linear probes or lightweight adapter modules rather than requiring full model retraining. The representations should encode biological information at multiple scales, from local sequence motifs to long-range regulatory grammar.</p>
<p>Transfer capability extends across multiple dimensions: different assays (from chromatin accessibility to gene expression), different tissues and cell types, different species, and different variant types (from SNVs to structural variants). Evidence of broad transfer requires evaluation across multiple benchmarks rather than demonstration of performance on a single task (<a href="../part_3/p3-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a>).</p>
<p>Foundation models operate at a scale that would be impractical for task-specific training. Some scale context length, as <em>HyenaDNA</em> scales to million-token windows at single-nucleotide resolution. Others scale parameter count, as the <em>ESM</em> and <em>Nucleotide Transformer</em> families reach billions of parameters. Still others scale data diversity through pan-genomic pretraining across hundreds of species or integration of many assays and cell types. The scaling dimension chosen reflects the model’s intended applications and architectural constraints.</p>
<p>Finally, foundation models typically expose consistent APIs for common operations. These include embedding extraction for sequences or variants, sequence probability scoring, and mask-based <em>in silico</em> mutagenesis for variant effect prediction. Models distributed through repositories such as Hugging Face often include documented recipes for downstream <strong>fine-tuning</strong> and example notebooks demonstrating common use cases.</p>
<p>The following table summarizes the essential properties that distinguish foundation models from task-specific alternatives.</p>
<div id="tbl-fm-vs-taskspecific" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-fm-vs-taskspecific-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14.1: Distinguishing properties of foundation models versus task-specific models.
</figcaption>
<div aria-describedby="tbl-fm-vs-taskspecific-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 35%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Foundation Model</th>
<th>Task-Specific Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Training data</strong></td>
<td>Minimal/no labels; self-supervised</td>
<td>Dense task-specific labels required</td>
</tr>
<tr class="even">
<td><strong>Transfer capability</strong></td>
<td>Many downstream tasks</td>
<td>Single task</td>
</tr>
<tr class="odd">
<td><strong>Scale dimension</strong></td>
<td>Parameters, context, or data diversity</td>
<td>Optimized for specific task</td>
</tr>
<tr class="even">
<td><strong>User interaction</strong></td>
<td>Embeddings + adaptation</td>
<td>End-to-end predictions</td>
</tr>
<tr class="odd">
<td><strong>Evaluation</strong></td>
<td>Multi-task benchmarks</td>
<td>Single-task metrics</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Why does self-supervised pretraining enable transfer while supervised training does not? The difference lies in what the training objective encourages the model to learn. A model trained to predict splice sites learns features specifically useful for splice site prediction: the GT-AG consensus, branch point motifs, exonic splicing enhancers. These features may be irrelevant or even misleading for other tasks. In contrast, a model trained to predict masked nucleotides across the entire genome must learn features useful for reconstructing any genomic context. To succeed at this broad objective, the model must discover general principles: how motifs combine, how sequence composition varies across regions, what patterns distinguish functional from non-functional sequence. These general features transfer because they capture the underlying organization of the genome rather than task-specific shortcuts.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Field Overview">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Field Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a comprehensive survey of genomic foundation models as of 2024, including taxonomy, benchmarks, and applications, see <span class="citation" data-cites="trop_genomics_2024">Trop et al. (<a href="../bib/references.html#ref-trop_genomics_2024" role="doc-biblioref">2024</a>)</span>.</p>
<p><strong>Reading Extension:</strong> Compare Trop et al.’s taxonomy to the four families introduced in this chapter. Where do their categories align with ours? Where do they diverge, and what might account for different organizational choices?</p>
</div>
</div>
</section>
<section id="sec-ch14-what-doesnt-count" class="level3" data-number="14.2.2">
<h3 data-number="14.2.2" class="anchored" data-anchor-id="sec-ch14-what-doesnt-count"><span class="header-section-number">14.2.2</span> What does not Count</h3>
<p>Many excellent genomic models fail one or more of these criteria and should not be classified as foundation models. Early versions of <em>DeepSEA</em> trained specifically on chromatin accessibility data from a limited set of cell types lack the generality and standardized interface of foundation models, though later iterations that integrate many assays begin to approach foundation model territory <span class="citation" data-cites="zhou_deepsea_2015">(<a href="../bib/references.html#ref-zhou_deepsea_2015" role="doc-biblioref">J. Zhou and Troyanskaya 2015</a>)</span>. <em>SpliceAI</em> predicts splicing outcomes exceptionally well but was designed for that specific task and provides neither general-purpose embeddings nor easy transfer to other genomic prediction problems <span class="citation" data-cites="jaganathan_predicting_2019">(<a href="../bib/references.html#ref-jaganathan_predicting_2019" role="doc-biblioref">Jaganathan et al. 2019</a>)</span>. Even a very large <em>Enformer</em>-like model trained solely on human chromatin tracks remains bound to its specific prediction interface despite its scale and sophistication <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>.</p>
<p>The distinction matters for several reasons. It affects evaluation strategy, since foundation models must be assessed across families of tasks rather than single benchmarks (<a href="../part_3/p3-ch12-evaluation.html" class="quarto-xref"><span>Chapter 12</span></a>). It affects integration into existing pipelines, since foundation models serve as feature extractors while task-specific models typically provide end-to-end predictions. It affects how we think about model development, since foundation model training requires different infrastructure and data curation than task-specific supervised learning.</p>
</section>
<section id="sec-ch14-concept-limitations" class="level3" data-number="14.2.3">
<h3 data-number="14.2.3" class="anchored" data-anchor-id="sec-ch14-concept-limitations"><span class="header-section-number">14.2.3</span> Limitations of the Foundation Model Concept</h3>
<p>The term “foundation” carries implications worth examining. Architectural foundations are static, load-bearing, and invisible once construction proceeds. Genomic foundation models share only the load-bearing property: they support downstream applications that would otherwise require independent construction. Yet unlike architectural foundations, these models remain visible and modifiable throughout their use. Fine-tuning adjusts the foundation itself rather than building atop an immutable base. The metaphor also implies that foundations precede and enable all subsequent work, but genomic foundation models often coexist with task-specific alternatives that outperform them on narrow benchmarks.</p>
<p>A more accurate metaphor might be “foundation” in the educational sense: a broad base of knowledge that enables specialized learning but continues to develop alongside it. The pretraining phase establishes general competence; adaptation refines that competence for specific purposes without abandoning the original learning. This framing better captures the dynamic relationship between pretrained representations and downstream tasks, though the architectural metaphor has become standard terminology.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: The Foundation Model Criterion
</div>
</div>
<div class="callout-body-container callout-body">
<p>A model qualifies as a foundation model not by its size or training cost, but by its demonstrated ability to transfer to diverse downstream tasks without full retraining. The critical test is: can users extract embeddings or apply lightweight adaptation to solve problems the original developers never anticipated? If the answer is yes across multiple task families, you have a foundation model. If the model only produces predictions for its original task, it remains task-specific regardless of scale.</p>
</div>
</div>
</section>
</section>
<section id="sec-ch14-scaling" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="sec-ch14-scaling"><span class="header-section-number">14.3</span> Scaling Laws and Compute-Optimal Training</h2>
<p>The success of foundation models in natural language processing rests partly on empirical <strong>scaling laws</strong>: predictable relationships between model size, training data, computational budget, and performance. Understanding these relationships guides resource allocation and model development decisions.</p>
<section id="sec-ch14-scaling-framework" class="level3" data-number="14.3.1">
<h3 data-number="14.3.1" class="anchored" data-anchor-id="sec-ch14-scaling-framework"><span class="header-section-number">14.3.1</span> Chinchilla Framework and Genomic Constraints</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Content Ahead
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following subsection presents the mathematical formulation of scaling laws. The key intuition is that performance improves predictably with more parameters and more data, but the rate of improvement follows specific power laws. If you find the equations challenging, focus on the practical implications summarized after the mathematical derivation.</p>
</div>
</div>
<p>Hoffmann et al.&nbsp;formalized the relationship between model performance and scaling factors through a power law decomposition <span class="citation" data-cites="hoffmann_training_2022">(<a href="../bib/references.html#ref-hoffmann_training_2022" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>:</p>
<p><span id="eq-14-01"><span class="math display">\[
L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}
\tag{14.1}\]</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(L\)</span> is the cross-entropy loss on held-out data (in nats or bits)</li>
<li><span class="math inline">\(N\)</span> is the number of model parameters</li>
<li><span class="math inline">\(D\)</span> is the number of training tokens</li>
<li><span class="math inline">\(E\)</span> is the irreducible loss (entropy remaining even with infinite resources)</li>
<li><span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> are empirically fitted scale constants</li>
<li><span class="math inline">\(\alpha \approx \beta \approx 0.3\)</span> are the scaling exponents (fitted to data) For language models, the exponents <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> both approximate 0.3, meaning doubling parameters or data reduces loss by roughly 20%. The constant <span class="math inline">\(E\)</span> represents irreducible loss (the entropy remaining even with infinite resources), while the parameter term <span class="math inline">\(A/N^\alpha\)</span> quantifies gains from greater model capacity and the data term <span class="math inline">\(B/D^\beta\)</span> captures gains from additional training examples.</li>
</ul>
<p>Why does this decomposition take this particular form? The power law structure reflects the diminishing returns inherent in learning: the first million parameters capture the most common patterns, while each subsequent million captures progressively rarer regularities. The exponents near 0.3 indicate that performance improvements slow considerably as scale increases; you need roughly eight times the resources to halve the gap to optimal performance. The additive structure separates distinct bottlenecks: insufficient model capacity to represent learned patterns (the <span class="math inline">\(N\)</span> term) versus insufficient data diversity to learn all relevant patterns (the <span class="math inline">\(D\)</span> term). This decomposition explains why over-parameterized models trained on limited data, or under-parameterized models trained on vast data, both underperform balanced allocations.</p>
<p>Training cost constrains both parameters and data simultaneously through the compute budget <span class="math inline">\(C\)</span> (measured in FLOPs):</p>
<p><span id="eq-14-02"><span class="math display">\[
C \approx 6ND
\tag{14.2}\]</span></span></p>
<p>where <span class="math inline">\(C\)</span> is measured in FLOPs (floating-point operations). This approximation holds because each training token requires roughly 6 FLOPs per parameter (one forward pass and one backward pass through the network). Optimizing this tradeoff by minimizing loss subject to the budget constraint yields:</p>
<p><span id="eq-14-03"><span class="math display">\[
N_{\text{opt}} \propto C^{0.49}, \quad D_{\text{opt}} \propto C^{0.51}
\tag{14.3}\]</span></span></p>
<p>These exponents, both near 0.5, encode the Chinchilla insight: model size and training data should scale approximately equally. Practical implementations target roughly 20 tokens per parameter for compute-optimal training.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Worked Example: Applying the Chinchilla Framework
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Scenario:</strong> Your lab has a compute budget of <span class="math inline">\(10^{20}\)</span> FLOPs for training a DNA language model. How should you allocate between model size and training data?</p>
<p><strong>Step 1:</strong> Apply the 20-tokens-per-parameter heuristic. Using <span class="math inline">\(C \approx 6ND\)</span> and <span class="math inline">\(D \approx 20N\)</span>:</p>
<p><span class="math display">\[10^{20} \approx 6 \times N \times 20N = 120N^2\]</span></p>
<p><span class="math display">\[N \approx \sqrt{10^{20}/120} \approx 2.9 \times 10^8 \text{ parameters}\]</span></p>
<p><strong>Step 2:</strong> Calculate corresponding data requirement:</p>
<p><span class="math display">\[D \approx 20 \times 2.9 \times 10^8 \approx 5.8 \times 10^9 \text{ tokens}\]</span></p>
<p><strong>Interpretation:</strong> With this budget, you should target approximately 300 million parameters trained on roughly 6 billion tokens. Training a 3 billion parameter model on the same budget would leave it severely undertrained (only 600 million tokens), likely underperforming the smaller compute-optimal model.</p>
<p><strong>Caveat for genomics:</strong> These ratios were derived for natural language. Genomic sequence may require different ratios due to its simpler alphabet and different statistical structure. When in doubt, empirically validate on held-out data.</p>
</div>
</div>
<p>The constants fitted for language models should not be assumed to hold for genomic tasks. DNA lacks the hierarchical compositional structure of natural language; regulatory grammar does not build meaning through recursive phrase structure the way sentences do. More critically, the 20-tokens-per-parameter guidance reflects optimization for next-token prediction loss on held-out text. Genomic foundation models often aim to learn representations that transfer to diverse downstream tasks (variant effect prediction, chromatin state inference, evolutionary constraint estimation), none of which were objectives during language model pretraining. The optimal balance between parameters and data may shift when the goal is representation learning rather than task-specific loss minimization.</p>
</section>
<section id="sec-ch14-empirical-scaling" class="level3" data-number="14.3.2">
<h3 data-number="14.3.2" class="anchored" data-anchor-id="sec-ch14-empirical-scaling"><span class="header-section-number">14.3.2</span> Empirical Scaling in Genomic Models</h3>
<p>Several genomic foundation model families have reported scaling experiments, though systematic scaling laws comparable to NLP remain elusive. The <em>Nucleotide Transformer</em> family provides perhaps the clearest genomic scaling data <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. Performance on downstream benchmarks improves consistently with parameter count across models from 50 million to 2.5 billion parameters. The largest models (trained on multi-species data) outperform smaller models trained on human sequences alone, suggesting that cross-species data provides effective scaling even when human-specific performance is the target. Training compute scaled from approximately <span class="math inline">\(10^{19}\)</span> to <span class="math inline">\(10^{21}\)</span> FLOPs across the model family.</p>
<div id="fig-scaling-laws" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scaling-laws-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch14/02-A-fig-scaling-laws.svg" class="img-fluid figure-img"></p>
<figcaption>Loss vs.&nbsp;model size</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch14/02-B-fig-scaling-laws.svg" class="img-fluid figure-img"></p>
<figcaption>Downstream performance vs.&nbsp;model size</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch14/02-C-fig-scaling-laws.svg" class="img-fluid figure-img"></p>
<figcaption>Optimal allocation of compute</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaling-laws-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.2: Scaling laws for genomic foundation models. (A) Pretraining loss decreases predictably with model parameters following a power law, enabling informed decisions about resource allocation. (B) Downstream task performance scales consistently across diverse tasks including contact prediction, secondary structure, and variant effects, demonstrating that larger models capture more transferable biological knowledge. (C) Compute-optimal scaling reveals the trade-off between model size and training data: for fixed compute budget, optimal performance requires balancing parameter count with training tokens. These scaling relationships, first established in natural language processing, extend to biological sequence models and guide foundation model development.
</figcaption>
</figure>
</div>
<p><em>ESM-2</em> demonstrated similar scaling for protein language models, with performance on structure prediction and variant effect tasks improving smoothly from 8 million to 15 billion parameters <span class="citation" data-cites="lin_esm-2_2022">(<a href="../bib/references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. The largest <em>ESM-2</em> models approach the structure prediction accuracy of <em>AlphaFold2</em> using only single-sequence input, a capability entirely absent in smaller models. <em>HyenaDNA</em> focused on context length scaling rather than parameter scaling, demonstrating that million-token contexts at single-nucleotide resolution could be achieved through sub-quadratic architectures <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>.</p>
<p>The scaling law framework has direct implications for model development decisions in genomics, though the constraints differ fundamentally from natural language processing. Unlike natural language, where text data is effectively unlimited, genomic sequence data faces hard constraints. Reference genomes for well-studied species total perhaps <span class="math inline">\(10^{11}\)</span> to <span class="math inline">\(10^{12}\)</span> nucleotides. Population-level variant data can expand this somewhat, but the effective diversity may be lower than raw counts suggest. In such data-constrained regimes, smaller models trained to convergence may outperform larger models that are undertrained.</p>
<p>Academic groups typically face stricter compute constraints than industry labs. Given fixed compute budgets, the Chinchilla framework suggests allocating resources toward longer training of smaller models rather than abbreviated training of larger models. A 500 million parameter model trained for 10 epochs on diverse genomic data may outperform a 5 billion parameter model trained for 1 epoch on the same data. Cross-species data offers a potential path around genomic data limitations. The <em>Nucleotide Transformer</em> and <em>Evo</em> families exploit this strategy, learning evolutionary patterns from diverse genomes that improve human-specific predictions.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider the genomic data constraint: reference genomes contain roughly <span class="math inline">\(10^{11}\)</span> to <span class="math inline">\(10^{12}\)</span> nucleotides total. How does this compare to the trillions of tokens used to train large language models? What strategies might help overcome this data limitation for genomic foundation models?</p>
</div>
</div>
</section>
<section id="sec-ch14-emergence" class="level3" data-number="14.3.3">
<h3 data-number="14.3.3" class="anchored" data-anchor-id="sec-ch14-emergence"><span class="header-section-number">14.3.3</span> Emergent Capabilities</h3>
<p>Perhaps the most intriguing aspect of foundation model scaling is the emergence of qualitatively new capabilities at sufficient scale. Emergence refers to abilities that are absent or negligible in smaller models but appear discontinuously as models grow. Think of it like learning to ride a bicycle: you cannot ride “a little bit”—you wobble and fall until suddenly, with enough practice, you can balance. The capability emerges all at once rather than improving gradually. Similarly, certain model capabilities remain effectively zero until the model crosses a threshold, then appear seemingly from nowhere.</p>
<p>In large language models, emergent capabilities include multi-step reasoning, code generation, and in-context learning. These capabilities appear at model scales of roughly <span class="math inline">\(10^{10}\)</span> parameters and above, with no clear precursor in smaller models <span class="citation" data-cites="wei_emergent_2022">(<a href="../bib/references.html#ref-wei_emergent_2022" role="doc-biblioref">Wei et al. 2022</a>)</span>.</p>
<p>Genomic foundation models exhibit analogous emergence, though the capability thresholds are less well characterized. The most striking example involves structural understanding from sequence: <em>ESM-2</em> at sufficient scale produces contact maps and secondary structure predictions from single sequences with accuracy approaching multiple sequence alignment methods like <em>trRosetta</em> <span class="citation" data-cites="lin_esm-2_2022">(<a href="../bib/references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. Smaller <em>ESM</em> models show no meaningful structural understanding. This capability emerges at approximately 650 million parameters and continues improving with scale.</p>
<p>Larger <em>Nucleotide Transformer</em> models transfer more effectively to novel species not seen during training <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. The ability to generalize beyond training species appears to require sufficient model capacity to learn abstract regulatory principles rather than memorizing species-specific patterns. Similarly, foundation models at sufficient scale can predict variant effects without task-specific fine-tuning, using only the difference in likelihood between reference and alternative sequences. For example, <em>ESM-1v</em> computes log-likelihood ratios to predict protein variant pathogenicity in a zero-shot manner. This zero-shot capability requires models large enough to capture subtle sequence dependencies</p>
<p>Few-shot approaches include task examples in the input context, allowing in-context learning without parameter updates. <em>HyenaDNA</em> demonstrated this capability for genomic tasks, suggesting that sufficiently large models with long context can adapt through prompts rather than training <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>.</p>
<p>The practical implication is that capability thresholds exist: models below certain scales may be fundamentally incapable of certain tasks regardless of fine-tuning. Identifying these thresholds helps guide model selection and prevents wasted effort fine-tuning models that lack necessary capacity.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Emergence Creates Capability Thresholds
</div>
</div>
<div class="callout-body-container callout-body">
<p>Certain capabilities only appear above specific scale thresholds. A 100 million parameter protein language model cannot perform zero-shot structure prediction regardless of how it is fine-tuned; the capability simply does not exist at that scale. Before attempting to adapt a foundation model for a challenging task, verify that models of similar scale have demonstrated the required capability. Attempting to fine-tune a model below the capability threshold is wasted effort.</p>
</div>
</div>
</section>
</section>
<section id="sec-ch14-taxonomy" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="sec-ch14-taxonomy"><span class="header-section-number">14.4</span> A Taxonomy of Genomic Foundation Models</h2>
<p>The landscape of genomic foundation models can be organized into four broad families. Each family exhibits distinct characteristics, strengths, limitations, and typical application domains.</p>
<div id="fig-model-taxonomy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model-taxonomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch14/03-fig-model-taxonomy.svg" class="img-fluid figure-img"></p>
<figcaption>Taxonomy of genomic foundation models</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-taxonomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.3: Taxonomy of genomic foundation models organized by modality and approach. DNA language models (blue) process nucleotide sequences with emphasis on long context and single-nucleotide resolution. Protein language models (green) encode evolutionary knowledge from protein sequences with increasing integration of structural information. Regulatory sequence models (orange) combine sequence processing with multi-task prediction of chromatin and expression tracks. Multi-modal and emerging models (purple) integrate across modalities, combining sequence with structure (AlphaFold2) or leveraging multiple information sources simultaneously. Arrows indicate connections between families where models build on each other’s capabilities.
</figcaption>
</figure>
</div>
<p>The following table provides a quick reference for comparing the four foundation model families across key dimensions.</p>
<div id="tbl-model-families" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-families-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14.2: Comparison of genomic foundation model families.
</figcaption>
<div aria-describedby="tbl-model-families-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 22%">
<col style="width: 17%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Family</th>
<th>Input</th>
<th>Output</th>
<th>Pretraining</th>
<th>Strength</th>
<th>Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>DNA LMs</strong></td>
<td>Nucleotide sequence</td>
<td>Embeddings, probabilities</td>
<td>MLM, autoregressive</td>
<td>General, scalable</td>
<td>No functional grounding</td>
</tr>
<tr class="even">
<td><strong>Seq-to-Function</strong></td>
<td>Sequence windows</td>
<td>Assay predictions</td>
<td>Supervised multi-task</td>
<td>Mechanistic</td>
<td>Tied to training assays</td>
</tr>
<tr class="odd">
<td><strong>VEP Models</strong></td>
<td>Variant + context</td>
<td>Effect scores</td>
<td>Mixed supervision</td>
<td>Clinical relevance</td>
<td>Narrow task focus</td>
</tr>
<tr class="even">
<td><strong>Multi-Omic</strong></td>
<td>Multiple modalities</td>
<td>Cross-modal embeddings</td>
<td>Contrastive, joint</td>
<td>Holistic</td>
<td>Data engineering complexity</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="sec-ch14-dna-lm" class="level3" data-number="14.4.1">
<h3 data-number="14.4.1" class="anchored" data-anchor-id="sec-ch14-dna-lm"><span class="header-section-number">14.4.1</span> DNA Language Models</h3>
<p>DNA language models treat genomic sequence as a language to be modeled, learning representations from raw nucleotide strings through self-supervised objectives. Without explicit functional labels, these models discover patterns through statistical regularities in genomic sequence.</p>
<p>The pretraining objectives typically involve masked language modeling or <strong>autoregressive</strong> next-token prediction. Training draws from reference genomes or pan-genomic sequence collections spanning multiple species. The resulting models produce per-position or pooled sequence embeddings that can be extracted and used for downstream tasks. Critically, these embeddings are not tied to specific assays or cell types, making them applicable to any task that benefits from general sequence context.</p>
<p><em>DNABERT</em> and <em>DNABERT-2</em> apply BERT-style masked language modeling to DNA sequences, using overlapping k-mers as tokens <span class="citation" data-cites="ji_dnabert_2021 zhou_dnabert-2_2024">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>; <a href="../bib/references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Z. Zhou et al. 2024</a>)</span> (<a href="p4-ch15-dna-lm.html#sec-ch15-dnabert" class="quarto-xref"><span>Section 15.2</span></a>). The <em>Nucleotide Transformer</em> family scales this approach to larger parameter counts and cross-species training <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span> (<a href="p4-ch15-dna-lm.html#sec-ch15-nucleotide-transformer" class="quarto-xref"><span>Section 15.3</span></a>). <em>HyenaDNA</em> achieves subquadratic complexity through implicit convolutions, enabling context lengths up to one million nucleotides <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span> (<a href="p4-ch15-dna-lm.html#sec-ch15-hyenadna" class="quarto-xref"><span>Section 15.5.1</span></a>). <em>Caduceus</em> incorporates bidirectional processing and reverse-complement equivariance as architectural inductive biases (<a href="p4-ch15-dna-lm.html#sec-ch15-caduceus" class="quarto-xref"><span>Section 15.5.2</span></a>). <em>Evo 2</em> combines long-range attention with biological <strong>tokenization</strong> strategies (<a href="p4-ch15-dna-lm.html#sec-ch15-evo2" class="quarto-xref"><span>Section 15.5.3</span></a>). <em>GROVER</em> integrates learned BPE-style tokenization with training on regulatory tracks in addition to raw sequence <span class="citation" data-cites="sanabria_grover_2024">(<a href="../bib/references.html#ref-sanabria_grover_2024" role="doc-biblioref">Sanabria et al. 2024</a>)</span>. These models and their architectural innovations are examined in detail in <a href="p4-ch15-dna-lm.html" class="quarto-xref"><span>Chapter 15</span></a>.</p>
<p>The primary strength of DNA language models lies in their generality: representations not bound to specific assays, cell types, or experimental conditions, capable of processing novel sequences absent from reference genomes. Their self-supervised training requires only genome sequences, making them scalable to massive corpora. The corresponding limitation is that without explicit functional grounding, they may not capture subtle regulatory patterns that manifest only under specific cellular conditions. Performance on tasks requiring fine-grained functional discrimination may lag models trained with functional supervision.</p>
<p>Applications span sequence classification (promoters, enhancers, transposons), motif discovery, variant effect prediction through embedding perturbation, sequence generation for synthetic biology, and transfer learning to new species with limited labeled data.</p>
</section>
<section id="sec-ch14-seq-to-func" class="level3" data-number="14.4.2">
<h3 data-number="14.4.2" class="anchored" data-anchor-id="sec-ch14-seq-to-func"><span class="header-section-number">14.4.2</span> Sequence-to-Function Foundation Models</h3>
<p>Sequence-to-function models predict molecular readouts directly from sequence through supervised or semi-supervised training on assay compendia. These models blur into foundation model territory when their output space is sufficiently broad and their internal representations prove useful for tasks beyond the original assay set.</p>
<p>These models map DNA sequences to high-dimensional vectors of molecular measurements, including chromatin accessibility, histone modifications, transcription factor binding, and gene expression levels. Training uses large collections of functional genomics assays spanning many cell types, enabling the models to learn regulatory grammar through supervised prediction of molecular phenotypes.</p>
<p><em>Enformer</em> predicts thousands of chromatin and expression tracks from 200 kb sequence windows through transformer attention <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span> (<a href="p4-ch17-regulatory.html#sec-ch17-enformer" class="quarto-xref"><span>Section 17.2</span></a>). <em>Borzoi</em> extends this with refined architectures and expanded RNA-seq coverage (<a href="p4-ch17-regulatory.html#sec-ch17-borzoi" class="quarto-xref"><span>Section 17.3</span></a>). <em>Sei</em> organizes predictions into interpretable sequence classes through unsupervised clustering <span class="citation" data-cites="chen_deepsea_2022">(<a href="../bib/references.html#ref-chen_deepsea_2022" role="doc-biblioref">Chen et al. 2022</a>)</span> (<a href="p4-ch17-regulatory.html#sec-ch17-sei" class="quarto-xref"><span>Section 17.4</span></a>). Earlier models including <em>DeepSEA</em> and <em>Basset</em> established the paradigm at smaller scales (<a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>).</p>
<p>The explicit functional supervision in these models provides mechanistic grounding that pure language models lack. Predictions can be interpreted through comparison to experiments. The models naturally support variant effect prediction by computing reference-alternative differences. The tradeoff is that models remain tied to training assays and cell types; extension to new contexts typically requires retraining or new data collection.</p>
<p>Applications center on regulatory variant interpretation in well-studied cell types, eQTL fine-mapping, enhancer identification, transcription factor binding prediction, and regulatory mechanism discovery.</p>
</section>
<section id="sec-ch14-vep-models" class="level3" data-number="14.4.3">
<h3 data-number="14.4.3" class="anchored" data-anchor-id="sec-ch14-vep-models"><span class="header-section-number">14.4.3</span> Variant Effect Prediction Models</h3>
<p>The clinical need to interpret genetic variants has driven development of models optimized specifically for predicting functional or clinical consequences. These take a variant and predict its effect on molecular phenotypes, organismal fitness, or disease risk.</p>
<p>Variant effect prediction models integrate sequence context with evolutionary information, population genetics signals, and sometimes structural or functional annotations. They output pathogenicity scores, effect size estimates, or functional consequence predictions. Training combines multiple data sources: clinical labels from ClinVar, population frequency from gnomAD, functional assays such as deep mutational scanning, and evolutionary constraint metrics.</p>
<p><em>AlphaMissense</em> applies protein language models to predict pathogenicity of missense variants <span class="citation" data-cites="cheng_alphamissense_2023">(<a href="../bib/references.html#ref-cheng_alphamissense_2023" role="doc-biblioref">Cheng et al. 2023</a>)</span> (<a href="p4-ch18-vep-fm.html#sec-ch18-alphamissense" class="quarto-xref"><span>Section 18.2.3</span></a>). <em>ESM-1v</em> uses evolutionary context for protein variant effect prediction (<a href="p4-ch18-vep-fm.html#sec-ch18-zeroshot-plm" class="quarto-xref"><span>Section 18.2.1</span></a>). <em>EVE</em> combines evolutionary and structural information (<a href="p4-ch18-vep-fm.html#sec-ch18-alignment-models" class="quarto-xref"><span>Section 18.2.2</span></a>). Genomic foundation models like <em>DNABERT</em> and <em>Enformer</em> provide variant effect predictions through <em>in silico</em> mutagenesis (<a href="p4-ch18-vep-fm.html#sec-ch18-dna-vep" class="quarto-xref"><span>Section 18.3</span></a>). The architecture, training, evaluation, and clinical deployment of variant effect predictors are covered comprehensively in <a href="p4-ch18-vep-fm.html" class="quarto-xref"><span>Chapter 18</span></a>, with integration into clinical workflows detailed in <a href="../part_7/p7-ch29-rare-disease.html" class="quarto-xref"><span>Chapter 29</span></a>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>At this point, you should be able to distinguish between the three model families covered so far. Without looking back, try to answer: What is the key difference between DNA language models and sequence-to-function models in terms of their training objectives? Which family would you choose if you needed to predict enhancer activity in a novel cell type not represented in existing training data?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>DNA language models use self-supervised objectives (masked language modeling or next-token prediction) on raw sequence without functional labels, while sequence-to-function models train with supervised multi-task prediction on thousands of chromatin and expression assays. For a novel cell type, DNA language models would be preferable because they learn general sequence patterns that transfer across contexts, whereas sequence-to-function models are tied to the specific cell types and assays in their training data.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-ch14-multi-omic" class="level3" data-number="14.4.4">
<h3 data-number="14.4.4" class="anchored" data-anchor-id="sec-ch14-multi-omic"><span class="header-section-number">14.4.4</span> Multi-Omic Foundation Models</h3>
<p>The most ambitious foundation models natively integrate multiple molecular modalities, jointly processing DNA sequence, chromatin state, gene expression, protein abundance, 3D genome structure, or phenotypic descriptions.</p>
<p>Multi-omic models employ architectures designed for heterogeneous input types: transformer variants with <strong>cross-attention</strong>, graph neural networks, or modality-specific encoders with fusion layers (<a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>, <a href="../part_5/p5-ch22-networks.html" class="quarto-xref"><span>Chapter 22</span></a>). Training objectives encourage cross-modal alignment through contrastive learning, joint prediction, or generative modeling of multiple data types.</p>
<p><em>Omni-DNA</em> uses transformer-based autoregressive models with vocabulary expansion and multi-task finetuning, unifying diverse genomic tasks under an instruction-response paradigm <span class="citation" data-cites="li_omni-dna_2025">(<a href="../bib/references.html#ref-li_omni-dna_2025" role="doc-biblioref">Li et al. 2025</a>)</span>. Models integrating Hi-C data capture 3D genome organization (<a href="../part_5/p5-ch21-3d-genome.html" class="quarto-xref"><span>Chapter 21</span></a>). Cross-modal architectures align DNA embeddings with chromatin or expression predictions (<a href="../part_5/p5-ch23-multi-omics.html" class="quarto-xref"><span>Chapter 23</span></a>).</p>
<p>The unified representations these models produce enable cross-modal queries, and joint training can improve performance through multi-task effects. Data engineering becomes substantially more complex, however, with different modalities requiring different measurement technologies and quality control. The field is early, with few models reaching production maturity.</p>
</section>
</section>
<section id="sec-ch14-design-dimensions" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="sec-ch14-design-dimensions"><span class="header-section-number">14.5</span> Design Dimensions</h2>
<p>Within and across families, individual models differ along orthogonal design dimensions that affect suitability for specific tasks.</p>
<section id="sec-ch14-data-composition" class="level3" data-number="14.5.1">
<h3 data-number="14.5.1" class="anchored" data-anchor-id="sec-ch14-data-composition"><span class="header-section-number">14.5.1</span> Data Composition</h3>
<p>The choice of training data shapes what patterns a model can learn. Training on human sequences alone focuses on clinically relevant patterns but limits exposure to evolutionary diversity. Cross-species training encourages learning of conserved elements and evolutionary constraints, potentially improving generalization but risking dilution of human-specific signals.</p>
<p>Sequence diversity presents a similar tradeoff. Training on reference genomes alone provides clean sequences but limited exposure to population variation. Incorporating variant data improves robustness but requires careful design to avoid learning spurious associations. Models may also train on raw sequence alone or incorporate functional annotations, trading generality against functional grounding. The implications of training data choices for model bias are examined in <a href="../part_3/p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>.</p>
</section>
<section id="sec-ch14-architecture" class="level3" data-number="14.5.2">
<h3 data-number="14.5.2" class="anchored" data-anchor-id="sec-ch14-architecture"><span class="header-section-number">14.5.2</span> Architecture Choices</h3>
<p>Architectural decisions determine both computational characteristics and inductive biases. Among transformer variants, encoder-only models (<em>DNABERT</em>, <em>Nucleotide Transformer</em>) excel at classification and embedding tasks, while decoder-only models (<em>GROVER</em>) support generative applications (<a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>). Full and sparse attention patterns, linear approximations, and Flash attention implementations affect computational efficiency.</p>
<p>Hyena-based models and state space models achieve subquadratic scaling, enabling longer contexts than standard transformers with comparable parameters. Hybrid approaches combine local convolutions with global attention, as in <em>Enformer</em>, processing sequences at multiple resolutions.</p>
</section>
<section id="sec-ch14-context-length" class="level3" data-number="14.5.3">
<h3 data-number="14.5.3" class="anchored" data-anchor-id="sec-ch14-context-length"><span class="header-section-number">14.5.3</span> Context Length</h3>
<p>The context window determines what genomic relationships a model can capture. Short context (under 1 kb) captures local patterns: motifs, splice sites, promoter elements. Medium context (1 to 10 kb) spans complete genes with proximal regulatory regions. Long context (10 to 200 kb) represents enhancer-promoter interactions and TAD-scale organization. Ultra-long context (over 200 kb) enables chromosomal domain modeling and complex structural variant interpretation. The effective use of long context requires appropriate tokenization and positional encoding strategies discussed in <a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>, with specific implementations examined in <a href="p4-ch15-dna-lm.html#sec-ch15-dnabert" class="quarto-xref"><span>Section 15.2</span></a> (k-mer tokenization), <a href="p4-ch15-dna-lm.html#sec-ch15-nucleotide-transformer" class="quarto-xref"><span>Section 15.3</span></a> (BPE variants), and <a href="../part_2/p2-ch07-attention.html#sec-ch07-positional-encoding" class="quarto-xref"><span>Section 7.2</span></a> (position embeddings).</p>
<p>The following table summarizes the relationship between context length and the biological phenomena that can be captured.</p>
<div id="tbl-context-length" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-context-length-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14.3: Context length determines what genomic relationships a model can capture.
</figcaption>
<div aria-describedby="tbl-context-length-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 11%">
<col style="width: 29%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Context Length</th>
<th>Range</th>
<th>Biological Scope</th>
<th>Example Applications</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Short</td>
<td>&lt; 1 kb</td>
<td>Motifs, splice sites</td>
<td>TF binding, splice prediction</td>
</tr>
<tr class="even">
<td>Medium</td>
<td>1-10 kb</td>
<td>Genes, proximal regulation</td>
<td>Promoter analysis, UTR effects</td>
</tr>
<tr class="odd">
<td>Long</td>
<td>10-200 kb</td>
<td>Enhancer-promoter, TADs</td>
<td>Regulatory variants, eQTL</td>
</tr>
<tr class="even">
<td>Ultra-long</td>
<td>&gt; 200 kb</td>
<td>Chromosomal domains</td>
<td>Structural variants, 3D genome</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-ch14-tokenization" class="level3" data-number="14.5.4">
<h3 data-number="14.5.4" class="anchored" data-anchor-id="sec-ch14-tokenization"><span class="header-section-number">14.5.4</span> Tokenization</h3>
<p>The representation of nucleotides as model inputs affects both computational efficiency and biological resolution. Character-level tokenization maintains single-base resolution but imposes longest sequence lengths. <em>K</em>-mer tokenization reduces length by a factor approaching <span class="math inline">\(k\)</span>, with vocabulary reaching 4,096 for 6-mers. Learned tokenization (BPE-style) discovers schemes from data, potentially allocating vocabulary more efficiently <span class="citation" data-cites="medvedev_biotoken_2025">(<a href="../bib/references.html#ref-medvedev_biotoken_2025" role="doc-biblioref">Medvedev et al. 2025</a>)</span>. The choice should align with both computational constraints and biological resolution requirements. Detailed discussion of tokenization strategies appears in <a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>.</p>
<div id="fig-design-dimensions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-design-dimensions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch14/04-fig-design-dimensions.svg" class="img-fluid figure-img"></p>
<figcaption>Design dimensions for genomic foundation models</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-design-dimensions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.4: Design dimensions for genomic foundation models. Radar chart positions representative models across six key dimensions: context length (how much sequence the model processes), parameter count (model capacity), training compute (resources required), architecture type (encoder vs.&nbsp;decoder), tokenization strategy (k-mer vs.&nbsp;single-nucleotide), and pretraining objective (masked vs.&nbsp;autoregressive). Different models make different trade-offs: ESM-2 emphasizes parameter scale within protein-length contexts; Enformer balances long context with multi-task supervision; HyenaDNA pushes context length to megabases using sub-quadratic architectures; Evo combines massive scale with autoregressive generation. These trade-offs determine which applications each model best serves.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ch14-build-vs-use" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="sec-ch14-build-vs-use"><span class="header-section-number">14.6</span> Build Versus Use Decisions</h2>
<p>The availability of pretrained foundation models creates strategic choices about when to use existing models, when to adapt them, and when to train from scratch.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading the detailed guidance, consider your own research context: Do you have unique proprietary data? What computational resources are available? How specific is your target task? Based on these factors, would you expect to use, adapt, or build a foundation model?</p>
</div>
</div>
<section id="sec-ch14-use-existing" class="level3" data-number="14.6.1">
<h3 data-number="14.6.1" class="anchored" data-anchor-id="sec-ch14-use-existing"><span class="header-section-number">14.6.1</span> When to Use Existing Models</h3>
<p>Existing foundation models provide immediate utility when the target application aligns with model capabilities, labeled data is limited, and computational resources are constrained.</p>
<p>For tasks where general sequence representations suffice, frozen foundation model embeddings with simple downstream classifiers often perform competitively with fine-tuned alternatives. This approach requires minimal compute (single forward passes), no gradient computation through large models, and modest labeled data (hundreds to thousands of examples). Applications include sequence classification, clustering, and similarity search.</p>
<p>Some foundation models support zero-shot variant effect prediction through likelihood ratio scoring. This requires no task-specific training and produces calibrated scores for novel variants immediately. Zero-shot approaches work well when the pretraining objective aligns with the target task and when fine-tuning data is unavailable or unreliable.</p>
<p>Foundation model APIs also enable rapid prototyping, allowing quick assessment of whether a modeling approach is viable before committing resources to custom development. Testing variant effect prediction with <em>ESM-1v</em> takes hours rather than the weeks required to train a custom model.</p>
</section>
<section id="sec-ch14-adapt-existing" class="level3" data-number="14.6.2">
<h3 data-number="14.6.2" class="anchored" data-anchor-id="sec-ch14-adapt-existing"><span class="header-section-number">14.6.2</span> When to Adapt Existing Models</h3>
<p>Adaptation through fine-tuning or lightweight methods (LoRA, adapters, prefix tuning) makes sense when downstream tasks require specialized behavior beyond what frozen embeddings provide, sufficient labeled data exists (typically thousands to tens of thousands of examples), and the target domain falls within the pretraining distribution.</p>
<p>Parameter-efficient methods like LoRA update a small fraction of model parameters (often under 1%) while keeping the foundation model frozen <span class="citation" data-cites="hu_lora_2021">(<a href="../bib/references.html#ref-hu_lora_2021" role="doc-biblioref">Hu et al. 2021</a>)</span>. This preserves general knowledge while allowing task-specific adaptation. Compute requirements are modest: a few GPU-hours for most genomic tasks. The approach works well when the foundation model’s representations are largely appropriate but need refinement for specific applications. Details on parameter-efficient adaptation appear in <a href="../part_3/p3-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>.</p>
<p>Updating all parameters typically achieves the best single-task performance but requires more data (tens of thousands of examples), more compute (GPU-days to weeks), and careful regularization to prevent <strong>overfitting</strong>. Full fine-tuning makes sense for high-stakes applications where maximum accuracy justifies the investment.</p>
</section>
<section id="sec-ch14-train-scratch" class="level3" data-number="14.6.3">
<h3 data-number="14.6.3" class="anchored" data-anchor-id="sec-ch14-train-scratch"><span class="header-section-number">14.6.3</span> When to Train from Scratch</h3>
<p>Building custom foundation models requires substantial justification given the resources involved.</p>
<p>Novel domains present the clearest case for custom pretraining. When target sequences differ fundamentally from existing model pretraining data (novel species, synthetic sequences, non-standard nucleotides), existing models may provide poor transfer. Applications requiring architectural features absent from existing models (specific attention patterns, custom tokenization, multi-modal inputs) similarly demand building from scratch.</p>
<p>Organizations with unique large-scale datasets (clinical biobanks, pharmaceutical screening data) may achieve better performance through custom pretraining than public models allow, though the data advantage must be substantial to justify training costs. Applications requiring larger models or longer contexts than available options face similar calculus.</p>
</section>
<section id="sec-ch14-cost-benefit" class="level3" data-number="14.6.4">
<h3 data-number="14.6.4" class="anchored" data-anchor-id="sec-ch14-cost-benefit"><span class="header-section-number">14.6.4</span> Cost-Benefit Analysis</h3>
<p>The decision framework involves comparing expected performance against resource requirements.</p>
<p>Training a foundation model from scratch requires <span class="math inline">\(10^{20}\)</span> to <span class="math inline">\(10^{22}\)</span> FLOPs, translating to thousands of GPU-hours and tens of thousands of dollars at current cloud prices. Fine-tuning requires <span class="math inline">\(10^{16}\)</span> to <span class="math inline">\(10^{18}\)</span> FLOPs, often achievable in hours on single GPUs. Inference with frozen embeddings requires only forward passes.</p>
<p>Foundation model pretraining requires billions of tokens. Fine-tuning requires thousands to tens of thousands of labeled examples. Zero-shot and embedding approaches require only evaluation data.</p>
<p>For well-studied tasks with abundant labeled data, fine-tuned models typically outperform frozen embeddings by 5 to 15% on standard metrics. Zero-shot approaches often achieve 70 to 90% of fine-tuned performance. Custom foundation models rarely outperform existing options by large margins unless the application involves genuinely novel domains.</p>
<p>The following table summarizes the resource requirements and expected performance for each approach.</p>
<div id="tbl-build-vs-use" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-build-vs-use-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14.4: Resource requirements and expected performance for different foundation model approaches.
</figcaption>
<div aria-describedby="tbl-build-vs-use-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 14%">
<col style="width: 24%">
<col style="width: 9%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>Compute</th>
<th>Data Required</th>
<th>Time</th>
<th>Expected Performance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Frozen embeddings</strong></td>
<td><span class="math inline">\(10^{14}\)</span> FLOPs</td>
<td>100s-1000s labels</td>
<td>Hours</td>
<td>70-90% of fine-tuned</td>
</tr>
<tr class="even">
<td><strong>LoRA/Adapters</strong></td>
<td><span class="math inline">\(10^{16}\)</span> FLOPs</td>
<td>1000s labels</td>
<td>Hours-Days</td>
<td>95% of full fine-tuning</td>
</tr>
<tr class="odd">
<td><strong>Full fine-tuning</strong></td>
<td><span class="math inline">\(10^{18}\)</span> FLOPs</td>
<td>10Ks labels</td>
<td>Days-Weeks</td>
<td>Best single-task</td>
</tr>
<tr class="even">
<td><strong>Train from scratch</strong></td>
<td><span class="math inline">\(10^{20}\)</span>+ FLOPs</td>
<td>Billions tokens</td>
<td>Weeks-Months</td>
<td>Best if novel domain</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-build-vs-use" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-build-vs-use-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_4/ch14/05-fig-build-vs-use.svg" class="img-fluid figure-img"></p>
<figcaption>Decision framework for using vs.&nbsp;building foundation models</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-build-vs-use-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.5: Decision framework for using vs.&nbsp;building foundation models. Entry point: a new genomic prediction task. First decision: does a suitable pretrained model exist? If yes, assess task alignment. For high alignment, USE frozen embeddings (hours of work, ~$10 compute, achieving 70-90% of fine-tuned performance); this serves most applications. For moderate alignment, ADAPT using LoRA or light fine-tuning (days of work, $100-1000 compute, ~95% of full fine-tuning). Only when existing models fundamentally lack required capabilities should practitioners BUILD custom foundation models (months of work, $100K+ compute). The vast majority of applications are best served by using or adapting existing models rather than building from scratch.
</figcaption>
</figure>
</div>
<p>Time costs often dominate: using existing models takes hours to days, fine-tuning takes days to weeks, training from scratch takes weeks to months. For time-sensitive applications, using existing models often dominates even if custom training would eventually yield better results.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: The Build-vs-Use Decision
</div>
</div>
<div class="callout-body-container callout-body">
<p>For most genomic applications, follow this decision sequence:</p>
<ol type="1">
<li><p><strong>Start with frozen embeddings</strong> from the most appropriate existing foundation model. Evaluate on held-out data before investing more resources.</p></li>
<li><p><strong>Try parameter-efficient fine-tuning</strong> (LoRA or adapters) if frozen embeddings underperform by more than 10% versus published baselines.</p></li>
<li><p><strong>Consider full fine-tuning</strong> only for high-stakes applications where the 5% improvement over LoRA justifies GPU-days of compute.</p></li>
<li><p><strong>Train from scratch</strong> only when all of the following hold: (a) target domain differs fundamentally from existing pretraining data, (b) you have access to unique large-scale data, (c) timeline permits months of development, and (d) budget permits $100K+ in compute.</p></li>
</ol>
<p>Most researchers will never need to train a foundation model from scratch. The efficiency of using pretrained models is precisely their value proposition.</p>
</div>
</div>
</section>
</section>
<section id="sec-ch14-evaluation" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="sec-ch14-evaluation"><span class="header-section-number">14.7</span> Evaluation Principles</h2>
<p>Foundation models resist evaluation on single tasks. Their value lies in transfer across many applications, making comprehensive evaluation substantially more complex than benchmarking task-specific models.</p>
<section id="sec-ch14-multi-task" class="level3" data-number="14.7.1">
<h3 data-number="14.7.1" class="anchored" data-anchor-id="sec-ch14-multi-task"><span class="header-section-number">14.7.1</span> Multi-Task Assessment</h3>
<p>A genomic foundation model should be evaluated across families of related tasks rather than isolated benchmarks. For DNA language models, this includes sequence classification tasks, variant effect prediction across multiple variant types, motif discovery, and cross-species transfer. For sequence-to-function models, evaluation should span prediction of held-out assays, transfer to novel cell types, and consistency with experimental measurements.</p>
<p>The diversity of evaluation tasks complicates comparison across models. A model excelling at promoter classification may underperform on eQTL fine-mapping. Direct comparisons require controlling for differences in training data, model scale, and evaluation protocols. Standardized benchmark suites are examined in <a href="../part_3/p3-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a>.</p>
</section>
<section id="sec-ch14-transfer-eval" class="level3" data-number="14.7.2">
<h3 data-number="14.7.2" class="anchored" data-anchor-id="sec-ch14-transfer-eval"><span class="header-section-number">14.7.2</span> Transfer Versus Pretraining Performance</h3>
<p>Foundation models are intended for transfer, making pretraining loss only moderately predictive of downstream utility. A model with slightly worse masked language modeling loss may produce better embeddings if its training objective better aligns with useful representations. Evaluation should explicitly test transfer through zero-shot performance, few-shot learning, cross-domain transfer, and robustness to distribution shift.</p>
<p>Detailed discussion of benchmark suites, evaluation protocols, and methodological best practices appears in <a href="../part_3/p3-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a> and <a href="../part_3/p3-ch12-evaluation.html" class="quarto-xref"><span>Chapter 12</span></a>.</p>
</section>
</section>
<section id="sec-ch14-ecosystem" class="level2" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="sec-ch14-ecosystem"><span class="header-section-number">14.8</span> Foundation Model Ecosystem</h2>
<p>Genomic foundation models exist within a broader ecosystem of infrastructure, community resources, and shared practices.</p>
<section id="sec-ch14-distribution" class="level3" data-number="14.8.1">
<h3 data-number="14.8.1" class="anchored" data-anchor-id="sec-ch14-distribution"><span class="header-section-number">14.8.1</span> Model Distribution</h3>
<p>Most models are distributed through centralized repositories. Hugging Face hosts many DNA and protein language models with documented APIs. GitHub repositories accompany publications with weights, code, and examples. Standardized formats reduce friction in adoption, enabling rapid benchmarking and experimentation.</p>
</section>
<section id="sec-ch14-documentation" class="level3" data-number="14.8.2">
<h3 data-number="14.8.2" class="anchored" data-anchor-id="sec-ch14-documentation"><span class="header-section-number">14.8.2</span> Documentation Requirements</h3>
<p>Responsible distribution requires comprehensive documentation: training data provenance, preprocessing procedures, architecture details, hyperparameters, evaluation protocols, and known limitations. Data provenance is particularly important given population-specific biases and use restrictions in genomic datasets (<a href="../part_3/p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>).</p>
</section>
<section id="sec-ch14-contributions" class="level3" data-number="14.8.3">
<h3 data-number="14.8.3" class="anchored" data-anchor-id="sec-ch14-contributions"><span class="header-section-number">14.8.3</span> Industry and Academic Contributions</h3>
<p>Both academic and industry groups develop genomic foundation models. Academic models emphasize reproducibility and open access. Industry models may offer superior performance through proprietary data or compute but with limited transparency. Notable industry contributions include NVIDIA’s BioNeMo platform and Microsoft’s Azure genomics integration. Users should review license terms before clinical or commercial deployment.</p>
</section>
</section>
<section id="sec-ch14-open-questions" class="level2" data-number="14.9">
<h2 data-number="14.9" class="anchored" data-anchor-id="sec-ch14-open-questions"><span class="header-section-number">14.9</span> Open Questions</h2>
<p>Despite rapid progress, fundamental challenges remain unsolved, and the field’s trajectory remains uncertain.</p>
<p>Whether genomic foundation models converge toward unified architectures or maintain specialized families is unclear. The diversity of genomic scales, resolution requirements, and functional contexts may preclude the convergence seen in NLP, where transformers now dominate across most tasks.</p>
<p>Existing models learn correlations without distinguishing causal from spurious relationships. Integrating causal structure could improve robustness and enable counterfactual reasoning, but current architectures provide no principled mechanism for causal inference (<a href="../part_3/p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>).</p>
<p>Models trained on reference genomes and common variants may not calibrate well for ultra-rare or <em>de novo</em> variants, precisely the variants most likely to be clinically actionable (<a href="../part_7/p7-ch29-rare-disease.html" class="quarto-xref"><span>Chapter 29</span></a>). Improved integration of structural and evolutionary constraints could strengthen rare variant interpretation.</p>
<p>Translation to clinical use requires robust cross-population performance, calibrated uncertainty (<a href="../part_6/p6-ch24-uncertainty.html" class="quarto-xref"><span>Chapter 24</span></a>), interpretability for clinicians (<a href="../part_6/p6-ch25-interpretability.html" class="quarto-xref"><span>Chapter 25</span></a>), prospective validation, and regulatory approval. These requirements extend well beyond benchmark performance, and the path from research model to clinical deployment remains poorly charted.</p>
</section>
<section id="sec-ch14-convergence" class="level2" data-number="14.10">
<h2 data-number="14.10" class="anchored" data-anchor-id="sec-ch14-convergence"><span class="header-section-number">14.10</span> Convergence Without Consolidation</h2>
<p>Foundation models for genomics divide into families serving different needs. DNA language models learn general sequence representations from self-supervised pretraining, capturing evolutionary constraints and regulatory patterns without explicit functional labels (<a href="p4-ch15-dna-lm.html" class="quarto-xref"><span>Chapter 15</span></a>). Sequence-to-function models predict molecular phenotypes from sequence, providing quantitative outputs (expression levels, chromatin states, splice probabilities) that DNA language models alone cannot produce (<a href="p4-ch17-regulatory.html" class="quarto-xref"><span>Chapter 17</span></a>). Variant effect models integrate sequence representations with evolutionary information to score the functional impact of genetic variants (<a href="p4-ch18-vep-fm.html" class="quarto-xref"><span>Chapter 18</span></a>). Multi-omic models combine sequence with additional data modalities to capture regulatory relationships that sequence alone cannot resolve (<a href="../part_5/p5-ch23-multi-omics.html" class="quarto-xref"><span>Chapter 23</span></a>). No single family dominates; effective genomic AI requires matching model capabilities to application requirements.</p>
<p>Scale introduces both opportunities and constraints. Scaling laws describe predictable relationships between parameters, data, compute, and performance, enabling principled resource allocation. Some capabilities appear only at sufficient scale, creating thresholds that cannot be crossed through fine-tuning alone. The practical implication is that certain applications require institutional-scale investment, while others can leverage existing pretrained models with modest adaptation. The build-versus-use framework guides this decision: use existing models when they suffice, adapt through fine-tuning or feature extraction when needed, train from scratch only when unique data or requirements justify the investment.</p>
<p>This framework instantiates across specific domains. DNA language models (<a href="p4-ch15-dna-lm.html" class="quarto-xref"><span>Chapter 15</span></a>) and protein language models (<a href="p4-ch16-protein-lm.html" class="quarto-xref"><span>Chapter 16</span></a>) exemplify self-supervised pretraining on biological sequence. Regulatory models (<a href="p4-ch17-regulatory.html" class="quarto-xref"><span>Chapter 17</span></a>) demonstrate sequence-to-function prediction at long-range scales. Variant effect prediction (<a href="p4-ch18-vep-fm.html" class="quarto-xref"><span>Chapter 18</span></a>) integrates multiple model families for clinical interpretation. Throughout, these principles guide model selection: what does this application require, which model family provides it, and what scale is necessary to achieve it?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<div class="callout callout-style-default callout-tip callout-titled" title="Test Yourself">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Test Yourself
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reviewing the summary, test your recall:</p>
<ol type="1">
<li><p>What distinguishes a foundation model from a task-specific deep learning model? Why does self-supervised pretraining enable transfer to multiple downstream tasks while supervised training does not?</p></li>
<li><p>According to the Chinchilla scaling laws, what is the relationship between model parameters, training data, and compute budget? If you have a fixed compute budget, should you train a larger model on less data or a smaller model on more data?</p></li>
<li><p>What are the four major families of genomic foundation models and what is the key strength and limitation of each family?</p></li>
<li><p>Explain the concept of emergent capabilities in foundation models. Why does this matter when selecting a model for adaptation to a new task?</p></li>
<li><p>When should you build a foundation model from scratch versus adapting an existing one? What are the key decision factors in the build-versus-use hierarchy?</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>Foundation vs.&nbsp;task-specific models</strong>: Foundation models are distinguished by their ability to transfer to diverse downstream tasks through embeddings or lightweight adaptation, not by their size alone. Self-supervised pretraining enables transfer because the model must learn general features useful for reconstructing any genomic context (e.g., how motifs combine, sequence composition patterns, functional vs.&nbsp;non-functional sequence). In contrast, supervised training on narrow tasks produces features specifically optimized for that task (e.g., splice site prediction learns GT-AG consensus and branch points), which may be irrelevant or misleading for other applications.</p></li>
<li><p><strong>Chinchilla scaling laws</strong>: The framework shows that loss <span class="math inline">\(L(N, D) = E + A/N^\alpha + B/D^\beta\)</span> where <span class="math inline">\(N\)</span> is parameters, <span class="math inline">\(D\)</span> is training tokens, and compute <span class="math inline">\(C \approx 6ND\)</span>. The key insight is that model parameters and training data should scale approximately equally; the optimal ratio is roughly 20 tokens per parameter. For a fixed compute budget, you should train a smaller model on more data rather than a larger model on less data, as undertrained large models typically underperform smaller compute-optimal models.</p></li>
<li><p><strong>Four foundation model families</strong>:</p>
<ol type="1">
<li><p>DNA language models use self-supervised pretraining for general sequence representations but lack functional grounding for subtle regulatory patterns.</p></li>
<li><p>Sequence-to-function models predict molecular phenotypes with mechanistic grounding but are tied to training assays and cell types.</p></li>
<li><p>Variant effect prediction models integrate multiple information sources for clinical relevance but focus narrowly on variant interpretation.</p></li>
<li><p>Multi-omic models integrate across modalities for holistic understanding but face data engineering complexity and are still early in development.</p></li>
</ol></li>
<li><p><strong>Emergent capabilities</strong>: These are abilities that appear discontinuously at certain scale thresholds and are absent in smaller models (e.g., ESM-2’s structural understanding emerges at ~650M parameters, zero-shot variant effect prediction requires sufficient scale). This matters because attempting to fine-tune a model below the capability threshold is wasted effort—if a 100M parameter model fundamentally cannot perform a task, no amount of fine-tuning will enable it. You must verify that models of similar scale have demonstrated the required capability before attempting adaptation.</p></li>
<li><p><strong>Build vs.&nbsp;use hierarchy</strong>: Start with frozen embeddings from existing models (hours of work, ~$10 compute, 70-90% of fine-tuned performance). Escalate to parameter-efficient adaptation like LoRA if embeddings underperform (days of work, $100-1000 compute, ~95% of full fine-tuning). Consider full fine-tuning only for high-stakes applications (weeks, $1000+). Train from scratch only when: (a) target domain differs fundamentally from existing pretraining data, (b) you have unique large-scale data, (c) timeline permits months, and (d) budget permits $100K+ compute. Most researchers never need to train from scratch.</p></li>
</ol>
</div>
</div>
</div>
</div>
</div>
<p><strong>Core Concepts:</strong></p>
<ul>
<li><p><strong>Foundation models</strong> are distinguished from task-specific models by their ability to transfer to diverse downstream tasks through embeddings or lightweight adaptation, not by size alone.</p></li>
<li><p><strong>Scaling laws</strong> (Chinchilla framework) describe predictable relationships between parameters, data, compute, and performance. For genomics, data constraints often matter more than compute constraints.</p></li>
<li><p><strong>Emergent capabilities</strong> appear at scale thresholds; models below these thresholds cannot achieve certain capabilities regardless of fine-tuning.</p></li>
<li><p><strong>Four model families</strong> serve different needs: DNA language models (general embeddings), sequence-to-function models (assay predictions), variant effect models (clinical interpretation), and multi-omic models (cross-modal integration).</p></li>
<li><p><strong>Build-vs-use decisions</strong> follow a clear hierarchy: start with frozen embeddings, escalate to adaptation if needed, train from scratch only for genuinely novel domains.</p></li>
</ul>
<p><strong>Key Takeaways:</strong></p>
<ol type="1">
<li><p>The paradigm shift from task-specific to foundation models changes how researchers interact with models, shifting from training practitioners to adaptation specialists.</p></li>
<li><p>For most applications, using or adapting existing foundation models is more efficient than training from scratch.</p></li>
<li><p>Evaluation must span multiple tasks; single-benchmark performance does not capture foundation model value.</p></li>
<li><p>The path from research to clinical deployment requires addressing uncertainty, interpretability, and regulatory requirements beyond benchmark performance.</p></li>
</ol>
<p><strong>Looking Ahead:</strong> The next chapters examine each foundation model family in depth: DNA language models (<a href="p4-ch15-dna-lm.html" class="quarto-xref"><span>Chapter 15</span></a>), protein language models (<a href="p4-ch16-protein-lm.html" class="quarto-xref"><span>Chapter 16</span></a>), regulatory sequence-to-function models (<a href="p4-ch17-regulatory.html" class="quarto-xref"><span>Chapter 17</span></a>), and variant effect prediction (<a href="p4-ch18-vep-fm.html" class="quarto-xref"><span>Chapter 18</span></a>).</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-avsec_enformer_2021" class="csl-entry" role="listitem">
Avsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. <span>“[<span>Enformer</span>] <span>Effective</span> Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.”</span> <em>Nature Methods</em> 18 (October): 1196–1203. <a href="https://doi.org/10.1038/s41592-021-01252-x">https://doi.org/10.1038/s41592-021-01252-x</a>.
</div>
<div id="ref-bommasani_opportunities_2022" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2022. <span>“On the <span>Opportunities</span> and <span>Risks</span> of <span>Foundation</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2108.07258">https://doi.org/10.48550/arXiv.2108.07258</a>.
</div>
<div id="ref-chen_deepsea_2022" class="csl-entry" role="listitem">
Chen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. <span>“[<span>DeepSEA</span> <span>Sei</span>] <span>A</span> Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.”</span> <em>Nature Genetics</em> 54 (7): 940–49. <a href="https://doi.org/10.1038/s41588-022-01102-2">https://doi.org/10.1038/s41588-022-01102-2</a>.
</div>
<div id="ref-cheng_alphamissense_2023" class="csl-entry" role="listitem">
Cheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. <span>“[<span>AlphaMissense</span>] <span>Accurate</span> Proteome-Wide Missense Variant Effect Prediction with <span>AlphaMissense</span>.”</span> <em>Science</em> 381 (6664): eadg7492. <a href="https://doi.org/10.1126/science.adg7492">https://doi.org/10.1126/science.adg7492</a>.
</div>
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-hoffmann_training_2022" class="csl-entry" role="listitem">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>“Training <span>Compute</span>-<span>Optimal</span> <span>Large</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2203.15556">https://doi.org/10.48550/arXiv.2203.15556</a>.
</div>
<div id="ref-hu_lora_2021" class="csl-entry" role="listitem">
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. <span>“<span>LoRA</span>: <span>Low</span>-<span>Rank</span> <span>Adaptation</span> of <span>Large</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2106.09685">https://doi.org/10.48550/arXiv.2106.09685</a>.
</div>
<div id="ref-jaganathan_predicting_2019" class="csl-entry" role="listitem">
Jaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. <span>“[<span>SpliceAI</span>] <span>Predicting</span> <span>Splicing</span> from <span>Primary</span> <span>Sequence</span> with <span>Deep</span> <span>Learning</span>.”</span> <em>Cell</em> 176 (3): 535–548.e24. <a href="https://doi.org/10.1016/j.cell.2018.12.015">https://doi.org/10.1016/j.cell.2018.12.015</a>.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-li_omni-dna_2025" class="csl-entry" role="listitem">
Li, Zehui, Vallijah Subasri, Yifei Shen, Dongsheng Li, Yiren Zhao, Guy-Bart Stan, and Caihua Shan. 2025. <span>“Omni-<span>DNA</span>: <span>A</span> <span>Unified</span> <span>Genomic</span> <span>Foundation</span> <span>Model</span> for <span>Cross</span>-<span>Modal</span> and <span>Multi</span>-<span>Task</span> <span>Learning</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2502.03499">https://doi.org/10.48550/arXiv.2502.03499</a>.
</div>
<div id="ref-lin_esm-2_2022" class="csl-entry" role="listitem">
Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. <span>“[<span>ESM</span>-2] <span>Language</span> Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.07.20.500902">https://doi.org/10.1101/2022.07.20.500902</a>.
</div>
<div id="ref-medvedev_biotoken_2025" class="csl-entry" role="listitem">
Medvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. <span>“<span>BioToken</span> and <span>BioFM</span> – <span>Biologically</span>-<span>Informed</span> <span>Tokenization</span> <span>Enables</span> <span>Accurate</span> and <span>Efficient</span> <span>Genomic</span> <span>Foundation</span> <span>Models</span>.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.03.27.645711">https://doi.org/10.1101/2025.03.27.645711</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-rentzsch_cadd_2019" class="csl-entry" role="listitem">
Rentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. <span>“<span>CADD</span>: Predicting the Deleteriousness of Variants Throughout the Human Genome.”</span> <em>Nucleic Acids Research</em> 47 (D1): D886–94. <a href="https://doi.org/10.1093/nar/gky1016">https://doi.org/10.1093/nar/gky1016</a>.
</div>
<div id="ref-rives_esm-1b_2021" class="csl-entry" role="listitem">
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. <span>“[<span>ESM</span>-1b] <span>Biological</span> Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.”</span> <em>Proceedings of the National Academy of Sciences of the United States of America</em> 118 (15): e2016239118. <a href="https://doi.org/10.1073/pnas.2016239118">https://doi.org/10.1073/pnas.2016239118</a>.
</div>
<div id="ref-sanabria_grover_2024" class="csl-entry" role="listitem">
Sanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. <span>“[<span>GROVER</span>] <span>DNA</span> Language Model <span>GROVER</span> Learns Sequence Context in the Human Genome.”</span> <em>Nature Machine Intelligence</em> 6 (8): 911–23. <a href="https://doi.org/10.1038/s42256-024-00872-0">https://doi.org/10.1038/s42256-024-00872-0</a>.
</div>
<div id="ref-schubach_cadd_2024" class="csl-entry" role="listitem">
Schubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. <span>“<span>CADD</span> V1.7: Using Protein Language Models, Regulatory <span>CNNs</span> and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.”</span> <em>Nucleic Acids Research</em> 52 (D1): D1143–54. <a href="https://doi.org/10.1093/nar/gkad989">https://doi.org/10.1093/nar/gkad989</a>.
</div>
<div id="ref-trop_genomics_2024" class="csl-entry" role="listitem">
Trop, Evan, Yair Schiff, Edgar Mariano Marroquin, Chia Hsiang Kao, Aaron Gokaslan, McKinley Polen, Mingyi Shao, et al. 2024. <span>“The <span>Genomics</span> <span>Long</span>-<span>Range</span> <span>Benchmark</span>: <span>Advancing</span> <span>DNA</span> <span>Language</span> <span>Models</span>,”</span> October.
</div>
<div id="ref-wei_emergent_2022" class="csl-entry" role="listitem">
Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, et al. 2022. <span>“Emergent <span>Abilities</span> of <span>Large</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2206.07682">https://doi.org/10.48550/arXiv.2206.07682</a>.
</div>
<div id="ref-zhou_expecto_2018" class="csl-entry" role="listitem">
Zhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. <span>“[<span>Expecto</span>] <span>Deep</span> Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.”</span> <em>Nature Genetics</em> 50 (8): 1171–79. <a href="https://doi.org/10.1038/s41588-018-0160-6">https://doi.org/10.1038/s41588-018-0160-6</a>.
</div>
<div id="ref-zhou_deepsea_2015" class="csl-entry" role="listitem">
Zhou, Jian, and Olga G. Troyanskaya. 2015. <span>“[<span>DeepSEA</span>] <span>Predicting</span> Effects of Noncoding Variants with Deep Learning–Based Sequence Model.”</span> <em>Nature Methods</em> 12 (10): 931–34. <a href="https://doi.org/10.1038/nmeth.3547">https://doi.org/10.1038/nmeth.3547</a>.
</div>
<div id="ref-zhou_dnabert-2_2024" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_4/p4--fm-families.html" class="pagination-link" aria-label="Part IV: Foundation Model Families">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Part IV: Foundation Model Families</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_4/p4-ch15-dna-lm.html" class="pagination-link" aria-label="DNA Language Models">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>