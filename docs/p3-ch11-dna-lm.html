<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; DNA Language Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p3-ch12-protein-lm.html" rel="next">
<link href="./p3-ch10-fm-principles.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p3--architectures.html">Part III: Deep Learning Architectures</a></li><li class="breadcrumb-item"><a href="./p3-ch11-dna-lm.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation and Tokenization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Models for Genomic Sequence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Attention and Transformers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives and Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning and Adaptation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">The Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna-lm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Long-Context Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Variant Effect Prediction with Foundation Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p4--multi-scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Multi-Scale Modeling</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RNA Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Single-Cell and Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">3D Genome and Spatial Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch18-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch19-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Evaluation Methodology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability and Mechanism</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Rare Disease and Variant Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Drug Discovery and Target Identification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch28-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Sequence Design and Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch29-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Regulatory, Ethical, and Future Considerations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Training Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#from-task-specific-cnns-to-general-purpose-language-models" id="toc-from-task-specific-cnns-to-general-purpose-language-models" class="nav-link active" data-scroll-target="#from-task-specific-cnns-to-general-purpose-language-models"><span class="header-section-number">11.1</span> From Task-Specific CNNs to General-Purpose Language Models</a></li>
  <li><a href="#dnabert-the-first-dna-language-model" id="toc-dnabert-the-first-dna-language-model" class="nav-link" data-scroll-target="#dnabert-the-first-dna-language-model"><span class="header-section-number">11.2</span> DNABERT: The First DNA Language Model</a></li>
  <li><a href="#nucleotide-transformer-scaling-data-and-model-diversity" id="toc-nucleotide-transformer-scaling-data-and-model-diversity" class="nav-link" data-scroll-target="#nucleotide-transformer-scaling-data-and-model-diversity"><span class="header-section-number">11.3</span> Nucleotide Transformer: Scaling Data and Model Diversity</a></li>
  <li><a href="#gpn-cross-species-pretraining-for-variant-effect-prediction" id="toc-gpn-cross-species-pretraining-for-variant-effect-prediction" class="nav-link" data-scroll-target="#gpn-cross-species-pretraining-for-variant-effect-prediction"><span class="header-section-number">11.4</span> GPN: Cross-Species Pretraining for Variant Effect Prediction</a></li>
  <li><a href="#the-long-context-revolution" id="toc-the-long-context-revolution" class="nav-link" data-scroll-target="#the-long-context-revolution"><span class="header-section-number">11.5</span> The Long-Context Revolution</a>
  <ul class="collapse">
  <li><a href="#hyenadna-megabase-context-via-implicit-convolutions" id="toc-hyenadna-megabase-context-via-implicit-convolutions" class="nav-link" data-scroll-target="#hyenadna-megabase-context-via-implicit-convolutions"><span class="header-section-number">11.5.1</span> HyenaDNA: Megabase Context via Implicit Convolutions</a></li>
  <li><a href="#caduceus-bidirectional-processing-with-reverse-complement-equivariance" id="toc-caduceus-bidirectional-processing-with-reverse-complement-equivariance" class="nav-link" data-scroll-target="#caduceus-bidirectional-processing-with-reverse-complement-equivariance"><span class="header-section-number">11.5.2</span> Caduceus: Bidirectional Processing with Reverse-Complement Equivariance</a></li>
  <li><a href="#evo-2-genome-scale-modeling-across-the-tree-of-life" id="toc-evo-2-genome-scale-modeling-across-the-tree-of-life" class="nav-link" data-scroll-target="#evo-2-genome-scale-modeling-across-the-tree-of-life"><span class="header-section-number">11.5.3</span> Evo 2: Genome-Scale Modeling Across the Tree of Life</a></li>
  </ul></li>
  <li><a href="#training-data-and-what-models-learn" id="toc-training-data-and-what-models-learn" class="nav-link" data-scroll-target="#training-data-and-what-models-learn"><span class="header-section-number">11.6</span> Training Data and What Models Learn</a>
  <ul class="collapse">
  <li><a href="#training-corpus-composition" id="toc-training-corpus-composition" class="nav-link" data-scroll-target="#training-corpus-composition"><span class="header-section-number">11.6.1</span> Training Corpus Composition</a></li>
  <li><a href="#probing-what-models-learn" id="toc-probing-what-models-learn" class="nav-link" data-scroll-target="#probing-what-models-learn"><span class="header-section-number">11.6.2</span> Probing What Models Learn</a></li>
  <li><a href="#what-models-do-not-learn" id="toc-what-models-do-not-learn" class="nav-link" data-scroll-target="#what-models-do-not-learn"><span class="header-section-number">11.6.3</span> What Models Do Not Learn</a></li>
  </ul></li>
  <li><a href="#benchmark-performance-and-evaluation" id="toc-benchmark-performance-and-evaluation" class="nav-link" data-scroll-target="#benchmark-performance-and-evaluation"><span class="header-section-number">11.7</span> Benchmark Performance and Evaluation</a>
  <ul class="collapse">
  <li><a href="#major-benchmark-suites" id="toc-major-benchmark-suites" class="nav-link" data-scroll-target="#major-benchmark-suites"><span class="header-section-number">11.7.1</span> Major Benchmark Suites</a></li>
  <li><a href="#benchmark-limitations" id="toc-benchmark-limitations" class="nav-link" data-scroll-target="#benchmark-limitations"><span class="header-section-number">11.7.2</span> Benchmark Limitations</a></li>
  </ul></li>
  <li><a href="#annotation-aware-extensions" id="toc-annotation-aware-extensions" class="nav-link" data-scroll-target="#annotation-aware-extensions"><span class="header-section-number">11.8</span> Annotation-Aware Extensions</a></li>
  <li><a href="#using-dna-language-models-in-practice" id="toc-using-dna-language-models-in-practice" class="nav-link" data-scroll-target="#using-dna-language-models-in-practice"><span class="header-section-number">11.9</span> Using DNA Language Models in Practice</a>
  <ul class="collapse">
  <li><a href="#embeddings-as-universal-features" id="toc-embeddings-as-universal-features" class="nav-link" data-scroll-target="#embeddings-as-universal-features"><span class="header-section-number">11.9.1</span> Embeddings as Universal Features</a></li>
  <li><a href="#fine-tuning-and-adaptation" id="toc-fine-tuning-and-adaptation" class="nav-link" data-scroll-target="#fine-tuning-and-adaptation"><span class="header-section-number">11.9.2</span> Fine-Tuning and Adaptation</a></li>
  <li><a href="#zero-shot-and-few-shot-scoring" id="toc-zero-shot-and-few-shot-scoring" class="nav-link" data-scroll-target="#zero-shot-and-few-shot-scoring"><span class="header-section-number">11.9.3</span> Zero-Shot and Few-Shot Scoring</a></li>
  </ul></li>
  <li><a href="#limitations-and-open-challenges" id="toc-limitations-and-open-challenges" class="nav-link" data-scroll-target="#limitations-and-open-challenges"><span class="header-section-number">11.10</span> Limitations and Open Challenges</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">11.11</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p3--architectures.html">Part III: Deep Learning Architectures</a></li><li class="breadcrumb-item"><a href="./p3-ch11-dna-lm.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-dna-lm" class="quarto-section-identifier"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The human genome contains roughly three billion base pairs of DNA, yet we lack the ability to read most of it. Protein-coding sequences constitute less than two percent of the total, and decades of research have characterized only a fraction of the regulatory elements that orchestrate when and where genes are expressed. The remaining sequence was once dismissed as “junk,” but we now recognize it contains a sophisticated regulatory grammar whose rules we can barely articulate. Every genome-wide association study identifies variants in noncoding regions whose mechanisms remain opaque; every clinical exome returns dozens of variants of uncertain significance that resist interpretation. <strong>The fundamental challenge is not sequencing the genome but understanding the language in which it is written.</strong></p>
<p>The success of language models in natural language processing suggested a potential path forward. BERT, GPT, and their successors demonstrated that statistical patterns in unlabeled text could yield representations encoding grammar, semantics, and even world knowledge. Proteins, too, proved amenable to this approach: models trained to predict masked amino acids learned representations that captured evolutionary constraints, structural properties, and functional relationships without explicit supervision (<a href="p3-ch12-protein-lm.html" class="quarto-xref"><span>Chapter 12</span></a>). DNA presents an analogous opportunity. If genomes encode a regulatory language, perhaps self-supervised learning on raw sequence could discover its grammar.</p>
<p>DNA language models import the self-supervised paradigm to nucleotide sequences. Rather than training separate models for each genomic prediction task (as the CNN era required; see <a href="p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>), these approaches learn general-purpose representations from unlabeled genomes that transfer across applications. A single pretrained backbone can support regulatory element classification, variant effect prediction, cross-species analysis, and sequence generation through different downstream heads or scoring strategies. The transition mirrors the broader shift in machine learning from task-specific architectures to foundation models (<a href="p3-ch10-fm-principles.html" class="quarto-xref"><span>Chapter 10</span></a>), with implications for how genomic AI systems are developed and deployed.</p>
<p>This chapter traces the development of DNA language models from early proof-of-concept systems through current state-of-the-art architectures. We examine the design choices that distinguish different approaches (tokenization strategies, context lengths, training objectives, and architectural innovations), analyze what these models actually learn through probing studies, and assess their performance across standardized benchmarks. The goal is to equip readers with the conceptual framework and practical knowledge needed to apply these models effectively while understanding their limitations.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Figure suggestion:</strong> Timeline visualization showing the evolution of DNA language models from 2021 to 2025, with key milestones including DNABERT (2021), Nucleotide Transformer (2023), HyenaDNA (2023), Caduceus (2024), and Evo 2 (2025). Annotate with context length progression (512 bp → 6 kb → 1 Mb) and key architectural innovations at each stage.</p>
</div>
</div>
<section id="from-task-specific-cnns-to-general-purpose-language-models" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="from-task-specific-cnns-to-general-purpose-language-models"><span class="header-section-number">11.1</span> From Task-Specific CNNs to General-Purpose Language Models</h2>
<p>The convolutional neural networks examined in <a href="p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> achieved remarkable performance on specific genomic prediction tasks. DeepSEA predicted chromatin marks from sequence; SpliceAI identified splice junctions with clinical utility; ExPecto estimated expression effects of variants. Each model was engineered for its particular application, with architectural choices (filter sizes, dilation patterns, pooling strategies) optimized for the task at hand.</p>
<p>This paradigm succeeded but imposed three constraints that limited scalability. <strong>Label dependence</strong> meant that every new assay, cell type, or phenotype required fresh labeled data. A model trained on ENCODE chromatin data could not predict histone modifications in a new cell type without additional labeled examples. <strong>Task coupling</strong> bound model architecture to specific prediction problems. SpliceAI’s dilated convolutions were tailored for splice junction detection; ExPecto’s spatial transformation was designed for the distance-dependent relationship between regulatory elements and transcription start sites. These choices, while effective, did not transfer naturally to other problems. <strong>Limited reuse</strong> meant features learned for one task could not easily support others. A model that learned to recognize transcription factor binding sites during chromatin accessibility training could not directly apply those representations to variant effect prediction without substantial re-engineering.</p>
<p>Protein language models demonstrated an alternative. ESM and related models trained on massive corpora of protein sequences using <strong>masked language modeling</strong> (predicting held-out amino acids from context) or autoregressive objectives (predicting the next amino acid). The resulting representations transferred to structure prediction, function annotation, and variant effect scoring without architecture changes. DNA language models import this recipe: pretrain on large collections of genomic sequences using self-supervised objectives, then adapt the learned representations to downstream tasks through probing, fine-tuning, or zero-shot scoring.</p>
<p>The practical workflow involves several steps. First, train a language model on unlabeled genomic sequences, where the model learns to predict masked or subsequent nucleotides from context. Second, extract embeddings from the trained model for sequences of interest (windows around variants, regulatory elements, or entire genes). Third, apply these embeddings to downstream tasks through one of several strategies: train lightweight classifiers on frozen embeddings (probing), update model parameters for specific applications (fine-tuning), or score sequence variants by comparing model probabilities (zero-shot evaluation). The promise is that once a sufficiently powerful backbone exists, it becomes the default starting point for nearly any DNA-level prediction problem.</p>
</section>
<section id="dnabert-the-first-dna-language-model" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="dnabert-the-first-dna-language-model"><span class="header-section-number">11.2</span> DNABERT: The First DNA Language Model</h2>
<p><em>DNABERT</em> applied the BERT masked language modeling framework to genomic sequences, establishing proof of concept for DNA self-supervision <span class="citation" data-cites="ji_dnabert_2021">(<a href="references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. The model used overlapping k-mers (typically 6-mers) as tokens, creating a vocabulary of 4,096 tokens from the 4^6 possible hexamers. Training on the human reference genome, <em>DNABERT</em> learned to predict masked tokens from surrounding context using the standard BERT architecture.</p>
<p>The design choices reflected computational constraints of the time. The k-mer tokenization provided some sequence compression compared to single-nucleotide representations, but the overlapping nature (each nucleotide participates in multiple adjacent k-mers) meant the compression was modest and created ambiguity about precise variant positions. Context windows were limited to 512 tokens, corresponding to a few hundred base pairs of genomic sequence. The standard transformer architecture with quadratic attention complexity made longer contexts computationally prohibitive.</p>
<p>Despite these limitations, <em>DNABERT</em> demonstrated several important principles. Fine-tuning on downstream tasks (promoter classification, splice site prediction, transcription factor binding site identification) achieved competitive performance with task-specific models trained from scratch. Learned embeddings captured biologically meaningful patterns, with similar sequences clustering together in embedding space even when trained only on the reference genome. The BERT-style architecture could be reused across multiple tasks with modest adaptation.</p>
<p><em>DNABERT-2</em> addressed the tokenization limitations through improved approaches including BPE-style token merging that better compressed repetitive sequences <span class="citation" data-cites="zhou_dnabert-2_2024">(<a href="references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Zhou et al. 2024</a>)</span>. The resulting model could represent longer genomic contexts within the same number of tokens, improving computational efficiency. On standardized benchmarks spanning sequence classification, regulatory element prediction, and variant effect scoring, <em>DNABERT-2</em> achieved consistent gains over both the original <em>DNABERT</em> and non-pretrained baselines. These improvements validated the importance of thoughtful tokenization design for genomic applications (see <a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a> for detailed discussion of tokenization strategies).</p>
<p>The DNABERT family collectively established that self-supervision on DNA works, that tokenization choices substantially affect performance, and that masked language model training produces reusable representations for diverse sequence tasks. The foundation model paradigm transfers effectively from natural language to genomic sequence.</p>
</section>
<section id="nucleotide-transformer-scaling-data-and-model-diversity" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="nucleotide-transformer-scaling-data-and-model-diversity"><span class="header-section-number">11.3</span> Nucleotide Transformer: Scaling Data and Model Diversity</h2>
<p><em>DNABERT</em> demonstrated feasibility but operated at modest scale relative to the size of genomes. The <strong>Nucleotide Transformer</strong> family pushed substantially further, emphasizing diversity in both training data and model architecture <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>.</p>
<p>The training corpus spanned genomic data from multiple species and human populations, exposing models to diverse sequence patterns, different regulatory architectures, and evolutionary constraints recurring across lineages. This cross-species pretraining mirrors the use of large multi-species alignments in protein language models but operates directly on raw DNA without explicit alignment. Context length expanded to approximately 6 kb per input sequence, representing an order-of-magnitude increase over <em>DNABERT</em> while still using dense transformer attention. The training objective remained masked language modeling on subsequences sampled from genomes.</p>
<p>The Nucleotide Transformer project introduced a benchmark panel that has become a standard yardstick for evaluating DNA language models. Tasks include promoter and enhancer classification, histone mark and chromatin accessibility prediction, splice site identification, and regulatory element type classification. Models are evaluated through linear probes or light fine-tuning on standardized train/validation/test splits. This benchmark infrastructure enabled systematic comparison across models and established the evaluation protocols now used throughout the field (see <a href="p5-ch20-benchmarks.html" class="quarto-xref"><span>Chapter 20</span></a> for comprehensive discussion of genomic benchmarks).</p>
<p>Scaling experiments revealed predictable relationships between model size, training data, and performance. Larger models with more pretraining data and longer context windows achieved better downstream performance, following patterns observed in natural language and protein modeling. These scaling trends suggest that continued investment in larger genomic language models will yield further improvements, though the optimal allocation between parameters, data, and compute remains an active research question (<a href="p3-ch10-fm-principles.html" class="quarto-xref"><span>Chapter 10</span></a>).</p>
</section>
<section id="gpn-cross-species-pretraining-for-variant-effect-prediction" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="gpn-cross-species-pretraining-for-variant-effect-prediction"><span class="header-section-number">11.4</span> GPN: Cross-Species Pretraining for Variant Effect Prediction</h2>
<p>While the Nucleotide Transformer demonstrated the value of scaling, the <strong>Genomic Pre-trained Network (GPN)</strong> explored a complementary direction: what can be learned from cross-species pretraining on relatively small, well-annotated genomes <span class="citation" data-cites="benegas_gpn_2023">(<a href="references.html#ref-benegas_gpn_2023" role="doc-biblioref">Benegas, Batra, and Song 2023</a>)</span>. Rather than scaling to maximum size, GPN asked whether self-supervision could yield useful variant effect predictors even in constrained settings.</p>
<p>GPN was trained on unaligned reference genomes from <em>Arabidopsis thaliana</em> and seven related species within the Brassicales order using masked language modeling. Despite this modest training corpus, analysis revealed emergent encoding of gene structure (exon-intron boundaries, splice sites) and DNA sequence motifs (transcription factor binding patterns) without explicit supervision. The model discovered these patterns purely from statistical regularities of genomic sequence across related species.</p>
<p>For variant effect prediction, GPN used a <strong>likelihood ratio</strong> approach. Given reference and alternate alleles at a position, the model computes the log-likelihood of each under the learned sequence distribution. Variants that substantially reduce sequence likelihood (relative to the reference) are inferred to be more disruptive. This scoring strategy exploits the fact that constrained positions should have confident predictions for the reference allele, while unconstrained positions allow more flexibility.</p>
<p>Evaluated on <em>A. thaliana</em> variants using allele frequencies from the 1001 Genomes Project, GPN outperformed traditional conservation scores including phyloP and phastCons. This was notable because phyloP and phastCons require explicit multiple sequence alignments and evolutionary models, while GPN learned its representations from unaligned sequences through self-supervision alone. The later GPN-MSA extended this approach to mammalian genomes by incorporating multi-species alignments, achieving strong performance on human variant benchmarks as discussed in <a href="p3-ch14-vep-fm.html" class="quarto-xref"><span>Chapter 14</span></a>.</p>
<p>GPN established that cross-species pretraining captures evolutionary constraints transferable to variant effect prediction, that relatively small models trained on focused phylogenetic groups can outperform larger generic conservation measures within that group, and that the masked language modeling objective naturally produces representations suitable for variant scoring via likelihood comparisons.</p>
</section>
<section id="the-long-context-revolution" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="the-long-context-revolution"><span class="header-section-number">11.5</span> The Long-Context Revolution</h2>
<p>Quadratic attention complexity limits transformer context to tens of kilobases at best. Processing a 100 kb sequence with dense attention requires on the order of 10^10 computations per layer. Yet regulatory phenomena routinely span larger distances: enhancer-promoter interactions extend 50-200 kb, topologically associating domains organize chromatin at the megabase scale, and some gene regulation involves even longer-range dependencies. <strong>The mismatch between biological context and computational context represented a fundamental architectural limitation.</strong></p>
<section id="hyenadna-megabase-context-via-implicit-convolutions" class="level3" data-number="11.5.1">
<h3 data-number="11.5.1" class="anchored" data-anchor-id="hyenadna-megabase-context-via-implicit-convolutions"><span class="header-section-number">11.5.1</span> HyenaDNA: Megabase Context via Implicit Convolutions</h3>
<p><em>HyenaDNA</em> addressed this limitation by replacing attention with implicit convolutions that scale sub-quadratically <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. The <strong>Hyena architecture</strong> parameterizes long convolutional filters through neural networks rather than storing explicit filter weights, achieving O(L log L) complexity through efficient FFT-based convolution compared to O(L^2) for standard attention. The result was a 500-fold increase in context length: <em>HyenaDNA</em> processes sequences up to 1 Mb while maintaining single-nucleotide resolution.</p>
<p>Processing megabase-scale windows allows the model to capture entire gene bodies plus flanking regulatory regions, long-range enhancer-promoter interactions, and topologically associating domain structure. Despite the long context, single-nucleotide tokens preserve maximum resolution for variant effect prediction. Each nucleotide is independently represented without the ambiguity introduced by k-mer tokenization.</p>
<p>On Nucleotide Transformer benchmarks, <em>HyenaDNA</em> achieved state-of-the-art results on the majority of tasks with orders of magnitude fewer parameters. On GenomicBenchmarks, it surpassed prior state-of-the-art on seven of eight datasets. Perhaps most notably, <em>HyenaDNA</em> demonstrated <strong>in-context learning</strong> in genomics: performance improved when examples were included in the input context without updating model weights. This capability, familiar from large language models, had not previously been observed for genomic sequences and suggests that sufficient context length combined with appropriate architecture enables qualitatively new forms of biological reasoning.</p>
</section>
<section id="caduceus-bidirectional-processing-with-reverse-complement-equivariance" class="level3" data-number="11.5.2">
<h3 data-number="11.5.2" class="anchored" data-anchor-id="caduceus-bidirectional-processing-with-reverse-complement-equivariance"><span class="header-section-number">11.5.2</span> Caduceus: Bidirectional Processing with Reverse-Complement Equivariance</h3>
<p>DNA is double-stranded, and any sequence can be read from either strand. The reverse complement of a sequence encodes the same information from the opposite strand’s perspective. For many biological processes, predictions should be identical or related consistently regardless of which strand is presented. Standard neural networks can produce divergent predictions for a sequence and its reverse complement, even with data augmentation during training.</p>
<p><em>Caduceus</em> addressed this challenge by building <strong>reverse-complement equivariance</strong> directly into the architecture <span class="citation" data-cites="schiff_caduceus_2024">(<a href="references.html#ref-schiff_caduceus_2024" role="doc-biblioref">Schiff et al. 2024</a>)</span>. The model extends the Mamba state space architecture (which achieves O(L) complexity) to support both bidirectional processing and strand equivariance. The BiMamba component enables information flow in both directions along the sequence, while the MambaDNA block ensures mathematically related predictions for sequences and their reverse complements.</p>
<p>On downstream benchmarks, <em>Caduceus</em> outperformed previous long-range models. On challenging long-range variant effect prediction tasks, it exceeded models with ten times as many parameters that lacked bidirectionality or equivariance. <strong>The key insight was that incorporating appropriate biological inductive biases can substitute for raw scale.</strong> Strand symmetry is a known property of DNA; building it into the architecture avoids wasting model capacity learning what could be specified directly.</p>
</section>
<section id="evo-2-genome-scale-modeling-across-the-tree-of-life" class="level3" data-number="11.5.3">
<h3 data-number="11.5.3" class="anchored" data-anchor-id="evo-2-genome-scale-modeling-across-the-tree-of-life"><span class="header-section-number">11.5.3</span> Evo 2: Genome-Scale Modeling Across the Tree of Life</h3>
<p><em>Evo 2</em> represents the current frontier: training at genome scale across all domains of life <span class="citation" data-cites="brixi_evo_2025">(<a href="references.html#ref-brixi_evo_2025" role="doc-biblioref">Brixi et al. 2025</a>)</span>. While previous models focused on specific organisms (<em>DNABERT</em> on human, GPN on plants) or trained on multi-species corpora at limited scale (Nucleotide Transformer), <em>Evo 2</em> aims to learn universal genomic patterns spanning bacteria, archaea, eukaryotes, and phages.</p>
<p>The training corpus draws from the OpenGenome2 dataset comprising 9.3 trillion DNA tokens across all domains of life. This massive scale exposes the model to the full spectrum of genomic organization: compact prokaryotic gene arrangements, sprawling eukaryotic regulatory landscapes with extensive noncoding sequence, viral genomes with overlapping reading frames, and the diversity of regulatory architectures across evolution. The model comes in 7 billion and 40 billion parameter variants.</p>
<p>The architecture builds on StripedHyena 2, a hybrid design combining convolutional operations with selective attention mechanisms. This enables processing of sequences up to 1 million nucleotides while maintaining computational tractability. The <strong>autoregressive training objective</strong> (predicting the next base given all previous bases) differs from the masked language modeling used in DNABERT and related models. Autoregressive training may provide complementary strengths for sequence generation and likelihood-based scoring, since the model learns to generate plausible sequences in addition to discriminating between them.</p>
<p><em>Evo 2</em> exhibits several forms of emergent biological knowledge despite training only on raw sequence. The model learns to identify exon-intron boundaries without explicit annotation, discovers transcription factor binding site patterns matching known motifs, captures aspects of protein secondary and tertiary structure when processing coding sequences, and identifies prophage insertion regions in bacterial genomes. These capabilities emerge from pure sequence statistics, demonstrating that genome-scale pretraining captures fundamental biological organization.</p>
<p>For variant effect prediction, <em>Evo 2</em> enables zero-shot scoring through likelihood ratios. Variants can be scored for consistency with learned genomic patterns by comparing model probabilities for reference versus alternate sequences. On benchmarks of pathogenic versus benign variants, zero-shot scores achieve competitive performance with specialized supervised methods, though calibration remains necessary before clinical application. The model also supports classification of variants of uncertain significance through simple classifiers trained on its embeddings.</p>
<p>The pan-species training enables cross-species applications. Variant interpretation extends naturally to non-model organisms, supporting conservation genomics and agricultural breeding where labeled training data is scarce. Model representations cluster sequences by phylogenetic relationships even without explicit evolutionary modeling. Beyond discriminative tasks, <em>Evo 2</em> demonstrates generative capabilities: synthesizing plausible mitochondrial genomes, prokaryotic operons, and eukaryotic regulatory regions with coherence across kilobase to megabase scales.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Figure suggestion:</strong> Multi-panel figure showing (A) OpenGenome2 training corpus composition across the tree of life with taxonomic breakdown, (B) StripedHyena 2 architecture schematic highlighting hybrid attention-convolution blocks, (C) t-SNE projection of sequence embeddings colored by taxonomic group showing phylogenetic clustering, and (D) zero-shot variant effect scores compared to experimental pathogenicity labels demonstrating calibration.</p>
</div>
</div>
</section>
</section>
<section id="training-data-and-what-models-learn" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="training-data-and-what-models-learn"><span class="header-section-number">11.6</span> Training Data and What Models Learn</h2>
<p>DNA language models are trained on diverse corpora ranging from single reference genomes to pan-genomic collections spanning the tree of life. Understanding what training data is used and what models learn from it is essential for anticipating model capabilities and limitations.</p>
<section id="training-corpus-composition" class="level3" data-number="11.6.1">
<h3 data-number="11.6.1" class="anchored" data-anchor-id="training-corpus-composition"><span class="header-section-number">11.6.1</span> Training Corpus Composition</h3>
<p>Early models like <em>DNABERT</em> trained primarily on the human reference genome (GRCh38), providing exposure to approximately 3 billion nucleotides from a single individual. The Nucleotide Transformer expanded to include multiple species and human population variation from resources like the 1000 Genomes Project. <em>Evo 2</em> scaled to 9.3 trillion tokens spanning all domains of life, including complete bacterial chromosomes, eukaryotic genomes, viral sequences, and metagenomic assemblies.</p>
<p>The composition of training data shapes what models learn. Reference-only training captures the genome’s architecture but not population variation. Multi-individual training exposes models to common polymorphisms but may underrepresent rare variants. Cross-species training provides evolutionary context (constrained regions are conserved, variable regions diverge) but may not capture species-specific regulatory patterns. Training on functional genomics data (GROVER-style approaches) teaches regulatory activity patterns but ties models to specific assays and cell types.</p>
<p>A tension exists between generality and specificity. Models trained on broader corpora learn more general representations that transfer across species and contexts, but may underperform narrower models on specific applications. Models trained on focused datasets may capture task-relevant patterns more effectively but transfer less well. The optimal training strategy depends on intended applications.</p>
</section>
<section id="probing-what-models-learn" class="level3" data-number="11.6.2">
<h3 data-number="11.6.2" class="anchored" data-anchor-id="probing-what-models-learn"><span class="header-section-number">11.6.2</span> Probing What Models Learn</h3>
<p>Linear probing experiments reveal what information is encoded in model representations without task-specific fine-tuning. By training simple classifiers (logistic regression, single-layer perceptrons) on frozen embeddings to predict known annotations, researchers can assess whether models have learned biologically meaningful patterns.</p>
<p>DNA language models consistently learn to recognize several categories of genomic features. <strong>Motif recognition</strong> emerges naturally: models learn patterns corresponding to known transcription factor binding sites, splice signals, and other sequence motifs without explicit supervision. Probing for specific motif presence shows that model embeddings can distinguish sequences containing binding sites from those lacking them. <strong>Gene structure</strong> is encoded in representations: models distinguish coding from noncoding regions, identify exon-intron boundaries, and recognize splice donor and acceptor sites. This knowledge emerges from sequence statistics alone, suggesting that the compositional and structural differences between genomic region types are learnable from DNA sequence.</p>
<p><strong>Evolutionary constraints</strong> are implicitly captured, particularly in models trained on multi-species data. Positions under purifying selection (constrained across evolution) show different embedding patterns than neutral positions. This provides a self-supervised analog to traditional conservation scoring, though the relationship between model-learned and alignment-based conservation measures varies across genomic contexts.</p>
<p>More complex patterns like <strong>regulatory grammar</strong> (the syntax governing how transcription factors combine to specify expression) show mixed evidence. Models capture some aspects of regulatory logic, such as the spacing preferences between binding sites, but may not fully represent the combinatorial complexity of enhancer function. Similarly, long-range dependencies (enhancer-promoter interactions across tens of kilobases) are accessible to long-context models but require extensive probing to assess whether they are actually leveraged.</p>
</section>
<section id="what-models-do-not-learn" class="level3" data-number="11.6.3">
<h3 data-number="11.6.3" class="anchored" data-anchor-id="what-models-do-not-learn"><span class="header-section-number">11.6.3</span> What Models Do Not Learn</h3>
<p>Equally important is recognizing what current DNA language models struggle to represent. <strong>Epigenetic context</strong> is not captured by sequence-only models: DNA methylation, histone modifications, and chromatin accessibility all affect gene regulation but are not encoded in primary sequence. Some models (like GROVER) address this by incorporating functional genomics data, but this ties them to specific cell types and experimental conditions.</p>
<p><strong>Three-dimensional structure</strong> of chromatin affects which regulatory elements can physically interact, but linear sequence models cannot represent folding (see <a href="p4-ch17-3d-genome.html" class="quarto-xref"><span>Chapter 17</span></a>). <strong>Cell-type specificity</strong> of gene regulation depends on transcription factor expression levels and chromatin state, not just sequence; models trained on sequence alone can predict potential regulatory activity but not its realization in specific contexts.</p>
<p><strong>Complex variant patterns</strong> beyond single nucleotide changes remain challenging. Indels, structural variants, repeat expansions, and epistatic interactions between distant loci are either not representable (depending on tokenization) or poorly predicted. Most benchmark tasks focus on SNVs, leaving multi-nucleotide effects underexplored.</p>
</section>
</section>
<section id="benchmark-performance-and-evaluation" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="benchmark-performance-and-evaluation"><span class="header-section-number">11.7</span> Benchmark Performance and Evaluation</h2>
<p>Standardized benchmarks enable systematic comparison across DNA language models, though each benchmark captures only part of what we care about. Understanding benchmark construction and limitations is essential for interpreting performance claims.</p>
<section id="major-benchmark-suites" class="level3" data-number="11.7.1">
<h3 data-number="11.7.1" class="anchored" data-anchor-id="major-benchmark-suites"><span class="header-section-number">11.7.1</span> Major Benchmark Suites</h3>
<p><strong>BEND (Benchmark for Nucleotide Deep learning)</strong> provides a unified framework with tasks including gene finding, enhancer annotation, chromatin state prediction, and variant effect scoring <span class="citation" data-cites="marin_bend_2024">(<a href="references.html#ref-marin_bend_2024" role="doc-biblioref">Marin et al. 2024</a>)</span>. Standardized splits and metrics enable fair comparison. BEND specifically evaluates whether models capture biologically meaningful features at different resolution scales.</p>
<p><strong>Genomic Benchmarks</strong> focus on regulatory element classification tasks: distinguishing promoters from nonpromoters, identifying active enhancers, predicting histone mark presence. These tasks test whether model representations encode basic genomic annotations. Most current DNA language models achieve high accuracy on these tasks, suggesting benchmark saturation for simpler classification problems.</p>
<p><strong>Long Range Benchmark (LRB)</strong> and <strong>DNALongBench</strong> evaluate long-context modeling capabilities <span class="citation" data-cites="cheng_dnalongbench_2024">(<a href="references.html#ref-cheng_dnalongbench_2024" role="doc-biblioref">Cheng et al. 2024</a>)</span>. Tasks include predicting distal enhancer-promoter interactions, modeling chromatin structure across hundreds of kilobases, and integrating information over extended genomic windows. These benchmarks specifically test whether long-context architectures provide meaningful advantages over shorter-context models.</p>
<p>Comparative evaluations across model families reveal that no single architecture dominates all tasks <span class="citation" data-cites="manzo_comparative_2025">(<a href="references.html#ref-manzo_comparative_2025" role="doc-biblioref">Manzo, Borkowski, and Ovcharenko 2025</a>)</span>. Performance varies substantially depending on task characteristics (local motif recognition versus long-range integration), training data composition, and architectural choices. <em>HyenaDNA</em> and <em>Caduceus</em> excel on long-range tasks where their architectural innovations matter; <em>DNABERT-2</em> and Nucleotide Transformer perform well on shorter-range regulatory classification; <em>Evo 2</em> shows advantages on cross-species tasks and variant effect prediction.</p>
</section>
<section id="benchmark-limitations" class="level3" data-number="11.7.2">
<h3 data-number="11.7.2" class="anchored" data-anchor-id="benchmark-limitations"><span class="header-section-number">11.7.2</span> Benchmark Limitations</h3>
<p>Several systematic issues affect benchmark interpretation. <strong>Saturation</strong> occurs when multiple models achieve near-perfect performance, eliminating discriminative power. This has happened for simpler classification tasks in Genomic Benchmarks. <strong>Leakage</strong> arises when training and test sequences share homology, allowing models to succeed through memorization rather than generalization. Careful sequence clustering (using tools like MMseqs2 or CD-HIT) is required to prevent this, but many older benchmarks lack rigorous split design.</p>
<p><strong>Distribution shift</strong> between benchmark data and real-world applications means strong benchmark performance may not predict deployment success. Most benchmarks derive from well-studied regions of well-characterized genomes; performance on understudied regions, rare variants, or non-European populations may differ substantially (see <a href="p5-ch22-confounding.html" class="quarto-xref"><span>Chapter 22</span></a> for discussion of ancestry bias).</p>
<p><strong>Metric selection</strong> affects what gets optimized. AUROC favors discrimination regardless of calibration; Spearman correlation measures rank ordering but not absolute effect size prediction. Clinical applications may require well-calibrated probability estimates or accurate quantitative predictions, neither of which standard metrics directly assess. The gap between benchmark performance and deployment utility remains substantial for most genomic applications.</p>
</section>
</section>
<section id="annotation-aware-extensions" class="level2" data-number="11.8">
<h2 data-number="11.8" class="anchored" data-anchor-id="annotation-aware-extensions"><span class="header-section-number">11.8</span> Annotation-Aware Extensions</h2>
<p>Recent work explores enriching DNA language models with explicit biological structure beyond raw sequence. These approaches represent early steps toward multi-modal genomic foundation models.</p>
<p><strong>Life-Code</strong> proposes central-dogma-informed tokenization, treating coding and noncoding regions differently <span class="citation" data-cites="liu_life-code_2025">(<a href="references.html#ref-liu_life-code_2025" role="doc-biblioref">Liu et al. 2025</a>)</span>. Coding regions use codon tokens (three-nucleotide units specifying amino acids), respecting the genetic code’s fundamental structure. Noncoding regions use learned subword units optimized during training. Knowledge distillation from protein language models imports protein-level structural knowledge into DNA representations. Life-Code achieves competitive results across DNA, RNA, and protein tasks, suggesting that encoding biological structure into tokenization provides useful inductive bias.</p>
<p><strong>BioToken</strong> extends tokenization to include explicit genomic annotations <span class="citation" data-cites="medvedev_biotoken_2025">(<a href="references.html#ref-medvedev_biotoken_2025" role="doc-biblioref">Medvedev et al. 2025</a>)</span>. Rather than representing regions purely as nucleotide strings, BioToken creates composite tokens encoding sequence content, variant presence, structural annotations (exon, intron, UTR), and functional context. The associated BioFM model achieves state-of-the-art performance across genomic benchmarks with substantially fewer parameters (265M), suggesting that annotation-aware representations improve parameter efficiency.</p>
<p>These approaches foreshadow the multi-modal foundation models discussed in Part IV, where sequence is only one of many integrated information streams.</p>
</section>
<section id="using-dna-language-models-in-practice" class="level2" data-number="11.9">
<h2 data-number="11.9" class="anchored" data-anchor-id="using-dna-language-models-in-practice"><span class="header-section-number">11.9</span> Using DNA Language Models in Practice</h2>
<p>DNA language models support multiple usage patterns for different applications.</p>
<section id="embeddings-as-universal-features" class="level3" data-number="11.9.1">
<h3 data-number="11.9.1" class="anchored" data-anchor-id="embeddings-as-universal-features"><span class="header-section-number">11.9.1</span> Embeddings as Universal Features</h3>
<p>The simplest approach extracts embeddings from a pretrained model and uses them as features for downstream classifiers. The workflow involves extracting embeddings for windows around loci of interest, pooling or selecting positions relevant to the task, and training lightweight downstream models (linear layers, shallow MLPs, gradient boosting) on the extracted features.</p>
<p>This approach supports diverse applications. Regulatory element classification distinguishes promoters, enhancers, silencers, and insulators based on learned representations. Chromatin state prediction uses sequence embeddings to predict ATAC-seq or histone mark presence. Variant effect scoring replaces or augments hand-crafted features in frameworks like CADD with language model features (analogous to CADD v1.7’s incorporation of protein language model features). Splicing analysis combines embeddings with specialized architectures.</p>
<p>Because the language model remains frozen, this approach is computationally efficient and avoids catastrophic forgetting when new tasks are added. The pretrained model serves as a general-purpose feature extractor supporting many downstream applications.</p>
</section>
<section id="fine-tuning-and-adaptation" class="level3" data-number="11.9.2">
<h3 data-number="11.9.2" class="anchored" data-anchor-id="fine-tuning-and-adaptation"><span class="header-section-number">11.9.2</span> Fine-Tuning and Adaptation</h3>
<p>When sufficient labeled data exists, fine-tuning typically outperforms frozen embedding approaches. Full fine-tuning updates all language model parameters for a specific task, allowing representations to specialize. This achieves highest performance but requires more compute and risks catastrophic forgetting of general knowledge.</p>
<p><strong>Parameter-efficient fine-tuning</strong> methods like LoRA (Low-Rank Adaptation) insert small trainable modules into each layer while keeping the backbone mostly frozen. These approaches achieve most of the performance gains of full fine-tuning while maintaining computational efficiency and preserving general capabilities. Adapter-based methods similarly add small bottleneck modules tuned for specific tasks.</p>
</section>
<section id="zero-shot-and-few-shot-scoring" class="level3" data-number="11.9.3">
<h3 data-number="11.9.3" class="anchored" data-anchor-id="zero-shot-and-few-shot-scoring"><span class="header-section-number">11.9.3</span> Zero-Shot and Few-Shot Scoring</h3>
<p>For variant interpretation, language models enable zero-shot scoring based on sequence likelihood. Compute the model’s probability for a sequence containing the reference allele, compare to probability for the sequence with the alternative allele, and interpret variants reducing probability as more disruptive. This approach requires no variant-specific training and can score any single-nucleotide variant the model can represent.</p>
<p>Zero-shot scoring quality depends on how well the model’s learned distribution captures biological constraints. Performance tends to improve with model scale and training data diversity. Few-shot approaches include task examples in the input context, allowing in-context learning without parameter updates. <em>HyenaDNA</em> demonstrated this capability for genomic tasks, suggesting that sufficiently large models with long context can adapt through prompts rather than training.</p>
</section>
</section>
<section id="limitations-and-open-challenges" class="level2" data-number="11.10">
<h2 data-number="11.10" class="anchored" data-anchor-id="limitations-and-open-challenges"><span class="header-section-number">11.10</span> Limitations and Open Challenges</h2>
<p>Despite substantial progress, DNA language models face several fundamental limitations.</p>
<p><strong>Context length versus resolution tradeoffs</strong> persist. Long-context models like <em>HyenaDNA</em> and <em>Evo 2</em> can process megabase sequences but require efficient architectures that may not capture all the relationships dense attention would learn. Whether these architectural tradeoffs matter for specific applications remains task-dependent.</p>
<p><strong>Complex variant patterns</strong> beyond SNVs are poorly handled. Most tokenization schemes represent insertions and deletions awkwardly or not at all. Structural variants spanning kilobases, repeat expansions, and complex rearrangements fall outside what current models can process. Epistatic interactions between variants at distant loci are not captured even by long-context models.</p>
<p><strong>Training data bias</strong> shapes model capabilities in underexplored ways. Models trained primarily on European-ancestry genomes may perform poorly on variants common in other populations. Ascertainment bias in training databases (enrichment for coding regions, well-studied genes, specific diseases) propagates to learned representations. The field lacks systematic evaluation of performance disparities across populations.</p>
<p><strong>Interpretability</strong> remains limited. While probing studies reveal what models encode, explaining why a specific variant receives a particular score in terms connecting to biological mechanism is difficult. Attention patterns and gradient-based attribution provide some insight but often fail to identify the specific sequence features driving predictions.</p>
<p><strong>Integration with other modalities</strong> is nascent. DNA sequence provides necessary but insufficient information for predicting gene regulation. Epigenomic state, three-dimensional chromatin structure, transcription factor concentrations, and cellular context all matter. Current DNA language models cannot represent these factors; multi-modal approaches (discussed in Part IV) aim to address this limitation.</p>
</section>
<section id="summary" class="level2" data-number="11.11">
<h2 data-number="11.11" class="anchored" data-anchor-id="summary"><span class="header-section-number">11.11</span> Summary</h2>
<p>This chapter traced the development of DNA language models from early proof-of-concept systems like <em>DNABERT</em> through scaled approaches like Nucleotide Transformer to architectural innovations enabling megabase context in <em>HyenaDNA</em>, <em>Caduceus</em>, and <em>Evo 2</em>. We examined how cross-species training captures evolutionary constraints (GPN), how biological inductive biases like reverse-complement equivariance improve efficiency (<em>Caduceus</em>), and how annotation-aware tokenization enriches sequence representations (Life-Code, BioToken).</p>
<p>Several principles emerged. Self-supervised pretraining on DNA works and produces representations competitive with task-specific models across diverse applications. Tokenization and architectural choices substantially affect performance, with single-nucleotide resolution and long context enabling applications inaccessible to earlier models. Biological inductive biases (strand symmetry, codon structure) can substitute for raw scale on appropriate tasks. Training data composition shapes what models learn; cross-species pretraining captures evolutionary constraints while multi-population training improves variant interpretation.</p>
<p>Current DNA language models capture sequence patterns, regulatory motifs, and evolutionary constraints but cannot predict quantitative molecular readouts, represent epigenomic or three-dimensional context, or provide mechanistic explanations of their predictions. In <a href="p3-ch13-regulatory.html" class="quarto-xref"><span>Chapter 13</span></a>, we examine how long-range sequence-to-function models like <em>Enformer</em> address some of these limitations by explicitly predicting molecular phenotypes from sequence. These models complement pure language models by providing quantitative predictions of regulatory activity, expression levels, and variant effects that DNA language models alone cannot provide.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-benegas_gpn_2023" class="csl-entry" role="listitem">
Benegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. <span>“[<span>GPN</span>] <span>DNA</span> Language Models Are Powerful Predictors of Genome-Wide Variant Effects.”</span> <em>Proceedings of the National Academy of Sciences</em> 120 (44): e2311219120. <a href="https://doi.org/10.1073/pnas.2311219120">https://doi.org/10.1073/pnas.2311219120</a>.
</div>
<div id="ref-brixi_evo_2025" class="csl-entry" role="listitem">
Brixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. <span>“[<span>Evo</span> 2] <span>Genome</span> Modeling and Design Across All Domains of Life with <span>Evo</span> 2.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.02.18.638918">https://doi.org/10.1101/2025.02.18.638918</a>.
</div>
<div id="ref-cheng_dnalongbench_2024" class="csl-entry" role="listitem">
Cheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu Yang, Lei Li, and Jian Ma. 2024. <span>“<span>DNALONGBENCH</span>: <span>A</span> <span>Benchmark</span> <span>Suite</span> <span>For</span> <span>Long</span>-<span>Range</span> <span>DNA</span> <span>Prediction</span> <span>Tasks</span>,”</span> October. <a href="https://openreview.net/forum?id=opv67PpqLS">https://openreview.net/forum?id=opv67PpqLS</a>.
</div>
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-liu_life-code_2025" class="csl-entry" role="listitem">
Liu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. <span>“Life-<span>Code</span>: <span>Central</span> <span>Dogma</span> <span>Modeling</span> with <span>Multi</span>-<span>Omics</span> <span>Sequence</span> <span>Unification</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2502.07299">https://doi.org/10.48550/arXiv.2502.07299</a>.
</div>
<div id="ref-manzo_comparative_2025" class="csl-entry" role="listitem">
Manzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025. <span>“Comparative <span>Analysis</span> of <span>Deep</span> <span>Learning</span> <span>Models</span> for <span>Predicting</span> <span>Causative</span> <span>Regulatory</span> <span>Variants</span>.”</span> <em>bioRxiv: The Preprint Server for Biology</em>, June, 2025.05.19.654920. <a href="https://doi.org/10.1101/2025.05.19.654920">https://doi.org/10.1101/2025.05.19.654920</a>.
</div>
<div id="ref-marin_bend_2024" class="csl-entry" role="listitem">
Marin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. <span>“<span>BEND</span>: <span>Benchmarking</span> <span>DNA</span> <span>Language</span> <span>Models</span> on Biologically Meaningful Tasks.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2311.12570">https://doi.org/10.48550/arXiv.2311.12570</a>.
</div>
<div id="ref-medvedev_biotoken_2025" class="csl-entry" role="listitem">
Medvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. <span>“<span>BioToken</span> and <span>BioFM</span> – <span>Biologically</span>-<span>Informed</span> <span>Tokenization</span> <span>Enables</span> <span>Accurate</span> and <span>Efficient</span> <span>Genomic</span> <span>Foundation</span> <span>Models</span>.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.03.27.645711">https://doi.org/10.1101/2025.03.27.645711</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-schiff_caduceus_2024" class="csl-entry" role="listitem">
Schiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. <span>“Caduceus: <span>Bi</span>-<span>Directional</span> <span>Equivariant</span> <span>Long</span>-<span>Range</span> <span>DNA</span> <span>Sequence</span> <span>Modeling</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2403.03234">https://doi.org/10.48550/arXiv.2403.03234</a>.
</div>
<div id="ref-zhou_dnabert-2_2024" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p3-ch10-fm-principles.html" class="pagination-link" aria-label="The Foundation Model Paradigm">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">The Foundation Model Paradigm</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p3-ch12-protein-lm.html" class="pagination-link" aria-label="Protein Language Models">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>