<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>20&nbsp; Benchmarks for Genomic AI – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p5-ch21-eval.html" rel="next">
<link href="./p5--eval-interp.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p5--eval-interp.html">Part V: Evaluation and Reliability</a></li><li class="breadcrumb-item"><a href="./p5-ch20-benchmarks.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic AI</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation and Tokenization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Models for Genomic Sequence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Attention and Transformers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives and Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning and Adaptation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">The Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Long-Context Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Variant Effect Prediction with Foundation Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p4--multi-scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Multi-Scale Modeling</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RNA Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Single-Cell and Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">3D Genome and Spatial Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch18-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch19-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-benchmarks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Evaluation Methodology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability and Mechanism</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Rare Disease and Variant Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Drug Discovery and Target Identification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch28-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Sequence Design and Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch29-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Regulatory, Ethical, and Future Considerations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Training Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#protein-language-model-benchmarks" id="toc-protein-language-model-benchmarks" class="nav-link active" data-scroll-target="#protein-language-model-benchmarks"><span class="header-section-number">20.1</span> Protein Language Model Benchmarks</a>
  <ul class="collapse">
  <li><a href="#tape-tasks-assessing-protein-embeddings" id="toc-tape-tasks-assessing-protein-embeddings" class="nav-link" data-scroll-target="#tape-tasks-assessing-protein-embeddings"><span class="header-section-number">20.1.1</span> TAPE: Tasks Assessing Protein Embeddings</a></li>
  <li><a href="#flip-function-linked-protein-benchmark" id="toc-flip-function-linked-protein-benchmark" class="nav-link" data-scroll-target="#flip-function-linked-protein-benchmark"><span class="header-section-number">20.1.2</span> FLIP: Function-Linked Protein Benchmark</a></li>
  <li><a href="#proteingym-comprehensive-variant-effect-evaluation" id="toc-proteingym-comprehensive-variant-effect-evaluation" class="nav-link" data-scroll-target="#proteingym-comprehensive-variant-effect-evaluation"><span class="header-section-number">20.1.3</span> ProteinGym: Comprehensive Variant Effect Evaluation</a></li>
  <li><a href="#structure-prediction-benchmarks" id="toc-structure-prediction-benchmarks" class="nav-link" data-scroll-target="#structure-prediction-benchmarks"><span class="header-section-number">20.1.4</span> Structure Prediction Benchmarks</a></li>
  </ul></li>
  <li><a href="#dna-and-regulatory-benchmarks" id="toc-dna-and-regulatory-benchmarks" class="nav-link" data-scroll-target="#dna-and-regulatory-benchmarks"><span class="header-section-number">20.2</span> DNA and Regulatory Benchmarks</a>
  <ul class="collapse">
  <li><a href="#classical-regulatory-prediction-tasks" id="toc-classical-regulatory-prediction-tasks" class="nav-link" data-scroll-target="#classical-regulatory-prediction-tasks"><span class="header-section-number">20.2.1</span> Classical Regulatory Prediction Tasks</a></li>
  <li><a href="#quantitative-regulatory-prediction" id="toc-quantitative-regulatory-prediction" class="nav-link" data-scroll-target="#quantitative-regulatory-prediction"><span class="header-section-number">20.2.2</span> Quantitative Regulatory Prediction</a></li>
  <li><a href="#genomic-benchmarks" id="toc-genomic-benchmarks" class="nav-link" data-scroll-target="#genomic-benchmarks"><span class="header-section-number">20.2.3</span> Genomic Benchmarks</a></li>
  <li><a href="#bend-benchmark-for-dna-language-models" id="toc-bend-benchmark-for-dna-language-models" class="nav-link" data-scroll-target="#bend-benchmark-for-dna-language-models"><span class="header-section-number">20.2.4</span> BEND: Benchmark for DNA Language Models</a></li>
  <li><a href="#long-range-benchmarks" id="toc-long-range-benchmarks" class="nav-link" data-scroll-target="#long-range-benchmarks"><span class="header-section-number">20.2.5</span> Long-Range Benchmarks</a></li>
  <li><a href="#cross-species-evaluation" id="toc-cross-species-evaluation" class="nav-link" data-scroll-target="#cross-species-evaluation"><span class="header-section-number">20.2.6</span> Cross-Species Evaluation</a></li>
  </ul></li>
  <li><a href="#variant-effect-prediction-benchmarks" id="toc-variant-effect-prediction-benchmarks" class="nav-link" data-scroll-target="#variant-effect-prediction-benchmarks"><span class="header-section-number">20.3</span> Variant Effect Prediction Benchmarks</a>
  <ul class="collapse">
  <li><a href="#clinical-variant-databases" id="toc-clinical-variant-databases" class="nav-link" data-scroll-target="#clinical-variant-databases"><span class="header-section-number">20.3.1</span> Clinical Variant Databases</a></li>
  <li><a href="#cagi-critical-assessment-of-genome-interpretation" id="toc-cagi-critical-assessment-of-genome-interpretation" class="nav-link" data-scroll-target="#cagi-critical-assessment-of-genome-interpretation"><span class="header-section-number">20.3.2</span> CAGI: Critical Assessment of Genome Interpretation</a></li>
  <li><a href="#deep-mutational-scanning-benchmarks" id="toc-deep-mutational-scanning-benchmarks" class="nav-link" data-scroll-target="#deep-mutational-scanning-benchmarks"><span class="header-section-number">20.3.3</span> Deep Mutational Scanning Benchmarks</a></li>
  <li><a href="#regulatory-and-non-coding-variant-benchmarks" id="toc-regulatory-and-non-coding-variant-benchmarks" class="nav-link" data-scroll-target="#regulatory-and-non-coding-variant-benchmarks"><span class="header-section-number">20.3.4</span> Regulatory and Non-Coding Variant Benchmarks</a></li>
  </ul></li>
  <li><a href="#trait-and-population-level-benchmarks" id="toc-trait-and-population-level-benchmarks" class="nav-link" data-scroll-target="#trait-and-population-level-benchmarks"><span class="header-section-number">20.4</span> Trait and Population-Level Benchmarks</a>
  <ul class="collapse">
  <li><a href="#polygenic-score-evaluation" id="toc-polygenic-score-evaluation" class="nav-link" data-scroll-target="#polygenic-score-evaluation"><span class="header-section-number">20.4.1</span> Polygenic Score Evaluation</a></li>
  <li><a href="#traitgym" id="toc-traitgym" class="nav-link" data-scroll-target="#traitgym"><span class="header-section-number">20.4.2</span> TraitGym</a></li>
  <li><a href="#embedgem-framework" id="toc-embedgem-framework" class="nav-link" data-scroll-target="#embedgem-framework"><span class="header-section-number">20.4.3</span> EmbedGEM Framework</a></li>
  </ul></li>
  <li><a href="#benchmark-construction-and-hidden-assumptions" id="toc-benchmark-construction-and-hidden-assumptions" class="nav-link" data-scroll-target="#benchmark-construction-and-hidden-assumptions"><span class="header-section-number">20.5</span> Benchmark Construction and Hidden Assumptions</a>
  <ul class="collapse">
  <li><a href="#data-sources-and-label-provenance" id="toc-data-sources-and-label-provenance" class="nav-link" data-scroll-target="#data-sources-and-label-provenance"><span class="header-section-number">20.5.1</span> Data Sources and Label Provenance</a></li>
  <li><a href="#splitting-strategies-and-leakage" id="toc-splitting-strategies-and-leakage" class="nav-link" data-scroll-target="#splitting-strategies-and-leakage"><span class="header-section-number">20.5.2</span> Splitting Strategies and Leakage</a></li>
  <li><a href="#metric-selection-and-aggregation" id="toc-metric-selection-and-aggregation" class="nav-link" data-scroll-target="#metric-selection-and-aggregation"><span class="header-section-number">20.5.3</span> Metric Selection and Aggregation</a></li>
  <li><a href="#goodharts-law-and-benchmark-gaming" id="toc-goodharts-law-and-benchmark-gaming" class="nav-link" data-scroll-target="#goodharts-law-and-benchmark-gaming"><span class="header-section-number">20.5.4</span> Goodhart’s Law and Benchmark Gaming</a></li>
  </ul></li>
  <li><a href="#benchmark-saturation-and-staleness" id="toc-benchmark-saturation-and-staleness" class="nav-link" data-scroll-target="#benchmark-saturation-and-staleness"><span class="header-section-number">20.6</span> Benchmark Saturation and Staleness</a>
  <ul class="collapse">
  <li><a href="#saturation-when-benchmarks-stop-discriminating" id="toc-saturation-when-benchmarks-stop-discriminating" class="nav-link" data-scroll-target="#saturation-when-benchmarks-stop-discriminating"><span class="header-section-number">20.6.1</span> Saturation: When Benchmarks Stop Discriminating</a></li>
  <li><a href="#staleness-when-benchmarks-diverge-from-practice" id="toc-staleness-when-benchmarks-diverge-from-practice" class="nav-link" data-scroll-target="#staleness-when-benchmarks-diverge-from-practice"><span class="header-section-number">20.6.2</span> Staleness: When Benchmarks Diverge from Practice</a></li>
  <li><a href="#leakage-from-scale" id="toc-leakage-from-scale" class="nav-link" data-scroll-target="#leakage-from-scale"><span class="header-section-number">20.6.3</span> Leakage from Scale</a></li>
  </ul></li>
  <li><a href="#the-benchmark-deployment-gap" id="toc-the-benchmark-deployment-gap" class="nav-link" data-scroll-target="#the-benchmark-deployment-gap"><span class="header-section-number">20.7</span> The Benchmark-Deployment Gap</a>
  <ul class="collapse">
  <li><a href="#distribution-shift" id="toc-distribution-shift" class="nav-link" data-scroll-target="#distribution-shift"><span class="header-section-number">20.7.1</span> Distribution Shift</a></li>
  <li><a href="#calibration-requirements" id="toc-calibration-requirements" class="nav-link" data-scroll-target="#calibration-requirements"><span class="header-section-number">20.7.2</span> Calibration Requirements</a></li>
  <li><a href="#metric-mismatch" id="toc-metric-mismatch" class="nav-link" data-scroll-target="#metric-mismatch"><span class="header-section-number">20.7.3</span> Metric Mismatch</a></li>
  <li><a href="#practical-constraints" id="toc-practical-constraints" class="nav-link" data-scroll-target="#practical-constraints"><span class="header-section-number">20.7.4</span> Practical Constraints</a></li>
  </ul></li>
  <li><a href="#systematic-gaps-in-current-benchmarks" id="toc-systematic-gaps-in-current-benchmarks" class="nav-link" data-scroll-target="#systematic-gaps-in-current-benchmarks"><span class="header-section-number">20.8</span> Systematic Gaps in Current Benchmarks</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">20.9</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p5--eval-interp.html">Part V: Evaluation and Reliability</a></li><li class="breadcrumb-item"><a href="./p5-ch20-benchmarks.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic AI</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-benchmarks" class="quarto-section-identifier"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic AI</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>A foundation model that achieves state-of-the-art performance on published benchmarks may fail catastrophically when deployed on patient data from underrepresented populations. Conversely, a model dismissed as mediocre on standard leaderboards may prove invaluable for a specialized clinical application the benchmarks never measured. The genomic AI field has accumulated dozens of benchmark suites, hundreds of individual tasks, and thousands of leaderboard entries, yet fundamental questions remain unanswered: Do these benchmarks measure what matters? When does benchmark success predict deployment value? How do we know when a benchmark has outlived its usefulness?</p>
<p>These questions are not merely academic. Clinical laboratories considering whether to adopt a new variant classifier, pharmaceutical companies evaluating foundation models for target discovery, and research groups selecting architectures for regulatory prediction all face the same challenge: translating benchmark metrics into deployment decisions. The gap between benchmark performance and real-world utility represents one of the most consequential yet underexplored problems in genomic AI. A model optimized for AUROC on ClinVar may systematically miscalibrate predictions for the rare variants that matter most clinically. A DNA language model that excels at enhancer classification may learn superficial sequence features rather than regulatory grammar. Understanding what benchmarks actually measure, and what they miss, is prerequisite to responsible model development and deployment.</p>
<p>This chapter surveys the benchmark landscape for genomic foundation models, organized by the biological modality each benchmark family targets. We examine protein benchmarks first, as the most mature evaluation ecosystem, then DNA and regulatory benchmarks, variant effect prediction benchmarks, and finally trait-level benchmarks. Throughout, we attend not only to what these benchmarks measure but to their construction, their biases, and their limitations. The methodological principles for using benchmarks properly (designing experiments, choosing metrics, avoiding pitfalls) are covered in <a href="p5-ch21-eval.html" class="quarto-xref"><span>Chapter 21</span></a>. Here we focus on cataloging what exists and developing the critical perspective necessary to interpret benchmark claims.</p>
<section id="protein-language-model-benchmarks" class="level2" data-number="20.1">
<h2 data-number="20.1" class="anchored" data-anchor-id="protein-language-model-benchmarks"><span class="header-section-number">20.1</span> Protein Language Model Benchmarks</h2>
<p>Protein language models (<a href="p3-ch12-protein-lm.html" class="quarto-xref"><span>Chapter 12</span></a>) benefit from the longest-established and most systematic evaluation ecosystem in genomic AI. The maturity of protein benchmarks reflects both the longer history of computational protein science and the relative tractability of protein structure and function prediction compared to regulatory genomics.</p>
<section id="tape-tasks-assessing-protein-embeddings" class="level3" data-number="20.1.1">
<h3 data-number="20.1.1" class="anchored" data-anchor-id="tape-tasks-assessing-protein-embeddings"><span class="header-section-number">20.1.1</span> TAPE: Tasks Assessing Protein Embeddings</h3>
<p>The Tasks Assessing Protein Embeddings (TAPE) benchmark, introduced in 2019, established the template for systematic protein representation evaluation <span class="citation" data-cites="rao_evaluating_2019">(<a href="references.html#ref-rao_evaluating_2019" role="doc-biblioref"><strong>rao_evaluating_2019?</strong></a>)</span>. TAPE frames protein language model assessment as transfer learning evaluation: pretrained models generate embeddings, which are then used as features for supervised prediction on downstream tasks. This framework decouples representation quality from task-specific modeling, enabling comparison across architectures that may have very different inductive biases.</p>
<p>TAPE comprises five tasks spanning different aspects of protein biology. Secondary structure prediction requires classifying each residue as helix, sheet, or coil, testing whether embeddings capture local structural preferences. Contact prediction asks whether residue pairs are spatially proximate in the folded structure, probing the representation’s ability to encode tertiary structure information from sequence alone. Remote homology detection requires classifying proteins into structural superfamilies, testing whether embeddings capture evolutionary relationships that transcend sequence similarity. Fluorescence prediction and stability prediction use data from deep mutational scanning experiments to assess whether embeddings encode fitness landscapes.</p>
<p>The benchmark’s design reflects deliberate methodological choices. Train, validation, and test splits enforce sequence identity thresholds to prevent homology-based leakage. Evaluation uses simple linear or shallow neural network heads rather than complex task-specific architectures, isolating representation quality from modeling capacity. Standardized preprocessing and data loading eliminate confounds from inconsistent implementation.</p>
<p>TAPE’s influence extended beyond its specific tasks. The benchmark established norms for protein representation evaluation: systematic coverage of diverse prediction targets, controlled transfer learning protocols, and explicit attention to data splitting. Subsequent benchmarks adopted and extended this framework.</p>
</section>
<section id="flip-function-linked-protein-benchmark" class="level3" data-number="20.1.2">
<h3 data-number="20.1.2" class="anchored" data-anchor-id="flip-function-linked-protein-benchmark"><span class="header-section-number">20.1.2</span> FLIP: Function-Linked Protein Benchmark</h3>
<p>The FLIP (Function-Linked Integrated Protein) benchmark addresses gaps in TAPE’s coverage by focusing on experimentally measured functional properties <span class="citation" data-cites="dallago_flip_2021">(<a href="references.html#ref-dallago_flip_2021" role="doc-biblioref"><strong>dallago_flip_2021?</strong></a>)</span>. Where TAPE includes structurally derived labels and computational annotations, FLIP emphasizes high-throughput experimental assays that directly measure protein fitness.</p>
<p>FLIP aggregates deep mutational scanning datasets across diverse proteins and functional readouts. The benchmark includes assays measuring enzymatic activity, binding affinity, thermostability, and expression level. Each dataset provides quantitative measurements for thousands of single-point mutations, enabling evaluation of fine-grained variant effect prediction.</p>
<p>The benchmark’s value lies in its experimental grounding. Computational structure predictions and evolutionary conservation scores, while useful, are indirect proxies for function. Deep mutational scanning provides direct measurements of how sequence changes affect the property of interest. Models that perform well on FLIP demonstrate the ability to predict experimentally validated functional consequences rather than computationally inferred annotations.</p>
<p>FLIP also introduced systematic evaluation of different splitting strategies. Random splits, where training and test variants are sampled uniformly from the same protein, represent the easiest setting. Contiguous splits, where training and test variants occupy different sequence regions, test spatial generalization. Modulo splits, which interleave training and test positions along the sequence, provide intermediate difficulty. Performance typically degrades from random to contiguous splits, revealing how much models rely on local sequence context versus genuine functional understanding.</p>
</section>
<section id="proteingym-comprehensive-variant-effect-evaluation" class="level3" data-number="20.1.3">
<h3 data-number="20.1.3" class="anchored" data-anchor-id="proteingym-comprehensive-variant-effect-evaluation"><span class="header-section-number">20.1.3</span> ProteinGym: Comprehensive Variant Effect Evaluation</h3>
<p>ProteinGym has emerged as the most comprehensive benchmark for protein variant effect prediction, compiling 217 deep mutational scanning assays across diverse protein families <span class="citation" data-cites="notin_proteingym_2024">(<a href="references.html#ref-notin_proteingym_2024" role="doc-biblioref"><strong>notin_proteingym_2024?</strong></a>)</span>. The benchmark’s scale enables statistically robust comparison across modeling approaches while its diversity reveals where different methods excel or struggle.</p>
<p>The primary evaluation metric is Spearman correlation between predicted and experimentally measured fitness effects. This rank-based metric is appropriate for deep mutational scanning data, where absolute fitness values depend on assay-specific calibration but relative rankings are more comparable across experiments. ProteinGym reports correlations for each assay individually and aggregated across the full benchmark, enabling both global comparison and identification of task-specific strengths.</p>
<p>ProteinGym distinguishes between zero-shot and supervised evaluation regimes. In zero-shot evaluation, models predict variant effects without any task-specific training, relying entirely on representations learned during pretraining. Models like ESM-1v (<a href="p3-ch12-protein-lm.html" class="quarto-xref"><span>Chapter 12</span></a>) compute effects as log-likelihood ratios under the pretrained language model, while structure-based methods like AlphaMissense (<span class="quarto-unresolved-ref">?sec-variant-effect</span>) incorporate predicted structural consequences. In supervised evaluation, models are fine-tuned on a subset of measured variants before predicting held-out effects. The gap between zero-shot and supervised performance indicates how much task-specific information improves over general-purpose representations.</p>
<p>The benchmark reveals systematic patterns in model performance. Protein language models generally outperform conservation-based methods, particularly for variants in regions with sparse evolutionary sampling. Structure-aware models show advantages for variants affecting protein stability or buried residues. Ensemble methods that combine multiple predictors often achieve the highest correlations, suggesting that different approaches capture complementary information.</p>
<p>ProteinGym’s limitations mirror those of its constituent datasets. Deep mutational scanning experiments are biased toward well-studied proteins amenable to high-throughput screening. Assay-specific selection pressures affect which variants appear deleterious: a variant may strongly affect enzymatic activity while leaving thermostability unchanged, or vice versa. The benchmark measures correlation with specific experimental readouts rather than clinical pathogenicity, which integrates multiple functional consequences in complex ways.</p>
</section>
<section id="structure-prediction-benchmarks" class="level3" data-number="20.1.4">
<h3 data-number="20.1.4" class="anchored" data-anchor-id="structure-prediction-benchmarks"><span class="header-section-number">20.1.4</span> Structure Prediction Benchmarks</h3>
<p>Protein structure prediction benchmarks derive from the Critical Assessment of protein Structure Prediction (CASP) tradition, which has evaluated computational methods against experimentally determined structures since 1994. The dramatic success of AlphaFold2 at CASP14 in 2020 transformed the field, but structure prediction benchmarks remain relevant for evaluating single-sequence methods and assessing whether language model pretraining improves structural accuracy.</p>
<p>Structure prediction quality is typically assessed using the Global Distance Test (GDT-TS) and Template Modeling score (TM-score). GDT-TS measures the percentage of residues that can be superimposed within various distance thresholds, providing a single number between 0 and 100 that correlates well with visual assessment of structural similarity. TM-score normalizes by protein length, enabling comparison across proteins of different sizes.</p>
<p>For protein language models, the relevant evaluation setting is single-sequence structure prediction, where the model receives only the target sequence without multiple sequence alignments. This tests whether pretraining on evolutionary sequence databases enables structure prediction without explicit evolutionary analysis at inference time. ESMFold (<a href="p3-ch12-protein-lm.html" class="quarto-xref"><span>Chapter 12</span></a>) demonstrated that single-sequence prediction can approach MSA-based methods for many proteins, though performance gaps remain for sequences with sparse evolutionary coverage.</p>
<p>Structure prediction benchmarks complement sequence-based evaluations by testing whether learned representations encode biophysical constraints. A model that achieves high accuracy on contact prediction or secondary structure classification may still fail to integrate these local predictions into globally consistent structures. The emergence of accurate single-sequence structure prediction from language model embeddings suggests that pretraining captures substantial structural information, even without explicit structural supervision.</p>
</section>
</section>
<section id="dna-and-regulatory-benchmarks" class="level2" data-number="20.2">
<h2 data-number="20.2" class="anchored" data-anchor-id="dna-and-regulatory-benchmarks"><span class="header-section-number">20.2</span> DNA and Regulatory Benchmarks</h2>
<p>DNA foundation models (<a href="p3-ch11-dna-lm.html" class="quarto-xref"><span>Chapter 11</span></a>) and regulatory models (<a href="p3-ch13-regulatory.html" class="quarto-xref"><span>Chapter 13</span></a>) face a less mature but rapidly developing benchmark landscape. Early deep learning work in genomics focused on individual tasks derived from ENCODE-style assays. Recent efforts have introduced benchmark suites that attempt to standardize evaluation across multiple tasks, tissues, and species.</p>
<section id="classical-regulatory-prediction-tasks" class="level3" data-number="20.2.1">
<h3 data-number="20.2.1" class="anchored" data-anchor-id="classical-regulatory-prediction-tasks"><span class="header-section-number">20.2.1</span> Classical Regulatory Prediction Tasks</h3>
<p>The earliest deep learning benchmarks for genomics framed regulatory prediction as classification over short sequence windows. Transcription factor binding prediction asks whether a specific TF ChIP-seq peak overlaps a given sequence window, typically around 1 kilobase centered on the binding site. Open chromatin prediction requires classifying regions as accessible or inaccessible based on DNase-seq or ATAC-seq signal. Histone mark prediction asks whether a chromatin modification peak (H3K27ac, H3K4me3, etc.) is present at each position.</p>
<p>These tasks derive from consortia like ENCODE and Roadmap Epigenomics, which systematically profiled chromatin states across cell types. Benchmark construction typically involves defining positive regions from called peaks and sampling negative regions from elsewhere in the genome, extracting fixed-length sequences centered on each region, and evaluating binary classification using AUROC or average precision.</p>
<p>Models such as DeepSEA, Basset, and DanQ established baseline performance on these tasks (see <a href="p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> for architectural details). Their success demonstrated that convolutional networks could learn sequence features predictive of regulatory state without hand-crafted motifs. Modern foundation models still report performance on similar tasks as sanity checks, though these classical benchmarks have significant limitations.</p>
<p>The primary limitation is that binary classification over short windows fails to capture the quantitative, cell-type-specific, and long-range nature of transcriptional regulation. A region may be weakly accessible in some cell types and strongly accessible in others; binary labels collapse this continuous variation. Short windows cannot assess whether models capture distal regulatory interactions that span tens to hundreds of kilobases. Evaluation on curated peak regions may overestimate performance relative to genome-wide prediction, where the vast majority of positions are regulatory “background.”</p>
</section>
<section id="quantitative-regulatory-prediction" class="level3" data-number="20.2.2">
<h3 data-number="20.2.2" class="anchored" data-anchor-id="quantitative-regulatory-prediction"><span class="header-section-number">20.2.2</span> Quantitative Regulatory Prediction</h3>
<p>Beyond binary classification, benchmarks increasingly require prediction of quantitative regulatory readouts. Signal regression asks models to predict per-base or per-bin signal intensity from ChIP-seq, ATAC-seq, or related assays. Gene expression prediction requires predicting transcript abundance (TPM, counts) from promoter sequences or larger genomic contexts. Massively parallel reporter assays (MPRAs) provide systematic measurements of enhancer or promoter activity for thousands of sequences, enabling evaluation of quantitative activity prediction.</p>
<p>Hybrid architectures like Enformer (<a href="p3-ch13-regulatory.html" class="quarto-xref"><span>Chapter 13</span></a>) popularized benchmarks combining large receptive fields with dense quantitative targets across many assays and cell types. Evaluation metrics shift from AUROC to Pearson or Spearman correlation between predicted and observed profiles. Some benchmarks report correlation relative to replicate concordance, establishing an upper bound set by experimental reproducibility.</p>
<p>Quantitative benchmarks better reflect the continuous nature of regulatory activity but introduce new challenges. Heterogeneous noise across assays and laboratories complicates aggregation: should a model be penalized equally for poor performance on a low-quality assay versus a high-quality one? Cell-type diversity raises questions about how to weight performance across tissues: is accurate prediction in a rare cell type more or less important than in a common one? The relationship between predicted and observed signal depends on assay-specific calibration that may not transfer across experimental batches.</p>
</section>
<section id="genomic-benchmarks" class="level3" data-number="20.2.3">
<h3 data-number="20.2.3" class="anchored" data-anchor-id="genomic-benchmarks"><span class="header-section-number">20.2.3</span> Genomic Benchmarks</h3>
<p>The Genomic Benchmarks resource provides standardized classification datasets for DNA sequence models <span class="citation" data-cites="gresova_genomic_2023">(<a href="references.html#ref-gresova_genomic_2023" role="doc-biblioref">Grešová et al. 2023</a>)</span>. The benchmark compiles tasks including enhancer identification, promoter recognition, splice site detection, and coding sequence classification across multiple species. Standardized train, validation, and test splits enable direct comparison of different architectures without confounds from inconsistent data processing.</p>
<p>Genomic Benchmarks emphasizes accessibility and reproducibility. Datasets are available in a unified format with documented preprocessing. Baseline results for multiple architectures provide reference points for new models. The benchmark includes tasks of varying difficulty, from relatively easy (distinguishing coding from non-coding sequence) to challenging (identifying tissue-specific enhancers).</p>
<p>The benchmark’s limitations reflect its design priorities. Focus on classification rather than regression excludes quantitative prediction tasks. Task difficulty varies substantially, with some tasks approaching saturation where gains become difficult to measure. Species coverage, while broader than many benchmarks, remains biased toward well-studied model organisms.</p>
</section>
<section id="bend-benchmark-for-dna-language-models" class="level3" data-number="20.2.4">
<h3 data-number="20.2.4" class="anchored" data-anchor-id="bend-benchmark-for-dna-language-models"><span class="header-section-number">20.2.4</span> BEND: Benchmark for DNA Language Models</h3>
<p>BEND (Benchmark for Evaluating DNA Models) provides a unified framework for evaluating genomic foundation models across diverse tasks <span class="citation" data-cites="de_almeida_bend_2024">(<a href="references.html#ref-de_almeida_bend_2024" role="doc-biblioref"><strong>de_almeida_bend_2024?</strong></a>)</span>. The benchmark includes regulatory element classification, chromatin accessibility prediction, variant effect scoring, and gene expression prediction. Standardized splits and evaluation protocols enable fair comparison across model families.</p>
<p>BEND’s design reflects lessons learned from earlier benchmarks. Tasks span multiple biological scales, from nucleotide-level variant effects to kilobase-scale regulatory elements. Evaluation includes both zero-shot settings (using pretrained representations directly) and fine-tuned settings (adapting models to specific tasks). Performance is reported separately for each task rather than aggregated into a single score, acknowledging that different models may excel at different aspects of genomic prediction.</p>
<p>Comparative evaluations using BEND reveal that no single model dominates across all tasks. Architecture choices (CNN versus transformer versus state space model), tokenization schemes (single nucleotide versus k-mer versus BPE), and pretraining corpora all influence task-specific performance. These patterns inform model selection for specific applications while highlighting the limitations of aggregate benchmarks that obscure such variation.</p>
</section>
<section id="long-range-benchmarks" class="level3" data-number="20.2.5">
<h3 data-number="20.2.5" class="anchored" data-anchor-id="long-range-benchmarks"><span class="header-section-number">20.2.5</span> Long-Range Benchmarks</h3>
<p>Long-range regulatory interactions, where enhancers tens to hundreds of kilobases from their target genes influence expression, require benchmarks that specifically test extended context modeling. The Long Range Benchmark (LRB) evaluates models’ ability to integrate information across large genomic distances, with tasks including predicting distal enhancer-promoter interactions, modeling TAD boundary effects, and identifying long-range regulatory dependencies.</p>
<p>DNALongBench extends evaluation to ultra-long contexts spanning up to millions of base pairs. Tasks at this scale test whether models can leverage chromosome-level context for regulatory prediction, potentially capturing effects from 3D chromatin organization and large-scale chromatin domains.</p>
<p>These benchmarks are particularly relevant for evaluating efficient attention mechanisms, state space models, and other architectures designed to extend effective context length. Performance on long-range benchmarks does not necessarily correlate with short-range task performance, indicating that different architectural choices optimize for different aspects of sequence modeling.</p>
</section>
<section id="cross-species-evaluation" class="level3" data-number="20.2.6">
<h3 data-number="20.2.6" class="anchored" data-anchor-id="cross-species-evaluation"><span class="header-section-number">20.2.6</span> Cross-Species Evaluation</h3>
<p>GenBench and related resources test whether models trained on one organism generalize to related species. Cross-species evaluation is important for several reasons. Many applications require predictions in non-human organisms (agricultural genomics, model organism research, comparative genomics). Multi-species training may improve within-species performance by providing additional evolutionary signal. The ability to transfer across species indicates that models have learned general principles of genome organization rather than species-specific artifacts.</p>
<p>Cross-species benchmarks typically evaluate models on held-out species not seen during training. Performance degradation from training to held-out species indicates the degree to which learned representations depend on species-specific features. Some architectures show better cross-species transfer than others, suggesting differences in how well they capture conserved regulatory principles.</p>
</section>
</section>
<section id="variant-effect-prediction-benchmarks" class="level2" data-number="20.3">
<h2 data-number="20.3" class="anchored" data-anchor-id="variant-effect-prediction-benchmarks"><span class="header-section-number">20.3</span> Variant Effect Prediction Benchmarks</h2>
<p>Variant effect prediction (VEP) benchmarks connect sequence changes to molecular or phenotypic consequences, addressing the clinically central question of which variants matter. These benchmarks span multiple biological levels, from molecular function to clinical pathogenicity.</p>
<section id="clinical-variant-databases" class="level3" data-number="20.3.1">
<h3 data-number="20.3.1" class="anchored" data-anchor-id="clinical-variant-databases"><span class="header-section-number">20.3.1</span> Clinical Variant Databases</h3>
<p>ClinVar provides the most widely used labels for clinical variant effect prediction, aggregating pathogenicity assertions from clinical laboratories and researchers worldwide. Benchmarks derived from ClinVar frame variant interpretation as classification: given a variant, predict whether it is pathogenic, likely pathogenic, benign, or likely benign.</p>
<p>ClinVar’s value as a benchmark stems from its clinical relevance. Variants classified in ClinVar represent the actual population of variants encountered in clinical testing. Performance on ClinVar directly addresses whether a model can assist variant interpretation workflows. The database’s scale (over 2 million variant submissions as of 2024) enables statistically robust evaluation.</p>
<p>ClinVar’s limitations as a benchmark are equally important. Submission heterogeneity means that label quality varies dramatically: expert-curated panels provide high-confidence classifications while single-laboratory submissions may reflect limited evidence. Version sensitivity means that benchmark composition changes over time as new submissions arrive and old classifications are updated. Most consequentially, circularity with computational predictors creates feedback loops: variants may have been classified using the very tools being evaluated, inflating apparent performance.</p>
<p>Ancestry and gene coverage biases profoundly shape what ClinVar benchmarks measure. Variants from European ancestry individuals and well-studied disease genes are heavily overrepresented. High performance on ClinVar demonstrates accuracy for this specific population rather than robust generalization across human genetic diversity. Benchmarks stratified by ancestry reveal substantial performance gaps, with models typically performing worse on variants from underrepresented populations.</p>
<p>Best practices for using ClinVar as a benchmark include specifying the exact database version and download date, excluding variants with conflicting assertions, stratifying performance by evidence level and ancestry, and comparing to baselines using only allele frequency to detect circularity. These practices are detailed in <a href="p5-ch21-eval.html" class="quarto-xref"><span>Chapter 21</span></a>.</p>
</section>
<section id="cagi-critical-assessment-of-genome-interpretation" class="level3" data-number="20.3.2">
<h3 data-number="20.3.2" class="anchored" data-anchor-id="cagi-critical-assessment-of-genome-interpretation"><span class="header-section-number">20.3.2</span> CAGI: Critical Assessment of Genome Interpretation</h3>
<p>The Critical Assessment of Genome Interpretation (CAGI) challenges provide prospective evaluation of variant effect predictors on unpublished datasets. Unlike retrospective benchmarks that evaluate models on historical data, CAGI distributes prediction targets before ground truth is available, preventing any possibility of overfitting to known labels.</p>
<p>CAGI challenges cover diverse prediction targets. Some challenges focus on molecular phenotypes: predicting the effect of variants on protein stability, binding affinity, or enzymatic activity. Others target clinical phenotypes: predicting disease risk, drug response, or clinical severity from individual genomes. The diversity of challenges tests whether models generalize across different types of variant effects.</p>
<p>The prospective design provides several advantages over retrospective benchmarks. Predictions must be made before labels are known, eliminating leakage from any source. The timeline forces models to commit to predictions rather than post-hoc optimization. Community participation enables comparison across many approaches under identical conditions.</p>
<p>CAGI’s limitation is scale: challenges include hundreds to thousands of variants rather than the millions available in databases like ClinVar. Statistical power to detect small performance differences is correspondingly limited. The challenges also depend on experimental collaborators willing to withhold data until after the prediction deadline, limiting the range of phenotypes that can be assessed.</p>
</section>
<section id="deep-mutational-scanning-benchmarks" class="level3" data-number="20.3.3">
<h3 data-number="20.3.3" class="anchored" data-anchor-id="deep-mutational-scanning-benchmarks"><span class="header-section-number">20.3.3</span> Deep Mutational Scanning Benchmarks</h3>
<p>Deep mutational scanning (DMS) provides systematic experimental measurement of variant effects across entire proteins or regulatory elements. DMS benchmarks test whether models can predict these experimentally determined effects, providing direct validation against measured functional consequences rather than inferred clinical classifications.</p>
<p>MaveDB aggregates DMS datasets in a standardized format, enabling systematic benchmarking across diverse proteins and assays. ProteinGym’s DMS component (discussed above) represents the most comprehensive benchmark in this space. For non-coding variants, MPRA datasets provide analogous systematic measurements of regulatory activity.</p>
<p>DMS benchmarks have distinct strengths and limitations compared to clinical databases. The experimental grounding means that labels reflect actual measured effects rather than clinical inference that may involve multiple assumptions. However, the relationship between DMS fitness and clinical pathogenicity is complex: a variant may substantially affect enzymatic activity without causing disease if the residual activity suffices for normal physiology. DMS benchmarks measure one component of the variant interpretation puzzle rather than the full clinical picture.</p>
</section>
<section id="regulatory-and-non-coding-variant-benchmarks" class="level3" data-number="20.3.4">
<h3 data-number="20.3.4" class="anchored" data-anchor-id="regulatory-and-non-coding-variant-benchmarks"><span class="header-section-number">20.3.4</span> Regulatory and Non-Coding Variant Benchmarks</h3>
<p>Non-coding variants require specialized benchmarks because their effects operate through different mechanisms than coding variants. MPRA-based benchmarks test whether models can predict the quantitative effect of variants on enhancer or promoter activity measured in reporter assays. eQTL-based benchmarks use naturally occurring variants associated with expression changes, treating the statistical evidence for eQTL status as a proxy for regulatory impact.</p>
<p>The challenge for non-coding benchmarks is connecting molecular effects to phenotypic consequences. A variant may alter chromatin accessibility without affecting any gene’s expression. A variant may affect expression without influencing disease risk. This gap between molecular and clinical effects complicates interpretation: high performance on MPRA prediction does not necessarily translate to accurate regulatory disease variant interpretation.</p>
<p>Fine-mapped GWAS variants provide another benchmark source for non-coding VEP. Statistical fine-mapping identifies putatively causal variants within associated loci, and models can be evaluated on their ability to prioritize these variants over nearby non-causal variants. Performance on fine-mapping tasks more directly assesses clinical relevance than molecular phenotype prediction, though fine-mapping itself has substantial uncertainty.</p>
</section>
</section>
<section id="trait-and-population-level-benchmarks" class="level2" data-number="20.4">
<h2 data-number="20.4" class="anchored" data-anchor-id="trait-and-population-level-benchmarks"><span class="header-section-number">20.4</span> Trait and Population-Level Benchmarks</h2>
<p>At the individual and population level, benchmarks assess whether models improve prediction of complex traits and disease risk.</p>
<section id="polygenic-score-evaluation" class="level3" data-number="20.4.1">
<h3 data-number="20.4.1" class="anchored" data-anchor-id="polygenic-score-evaluation"><span class="header-section-number">20.4.1</span> Polygenic Score Evaluation</h3>
<p>Polygenic score (PGS) benchmarks evaluate how well genotype-derived scores predict disease risk or quantitative traits. Common evaluation settings include within-biobank evaluation, where a single large cohort is partitioned into training and test sets, and cross-biobank evaluation, where models trained in one population are tested in another.</p>
<p>Metrics depend on the phenotype. For quantitative traits, benchmarks report the coefficient of determination (R²) or incremental R² over non-genetic covariates. For binary disease outcomes, AUROC and AUPRC quantify discrimination. Calibration metrics assess whether predicted risks match observed event rates. The clinical utility of PGS, discussed in <a href="p6-ch25-clinical-risk.html" class="quarto-xref"><span>Chapter 25</span></a>, depends on all these properties: a score may discriminate well (high AUROC) while being poorly calibrated (predicted risks don’t match actual event rates).</p>
<p>Cross-population evaluation is particularly important because PGS portability is a major limitation of current methods (<a href="p1-ch03-gwas.html" class="quarto-xref"><span>Chapter 3</span></a>). Benchmarks stratified by ancestry typically reveal substantial performance degradation from European ancestry (where most GWAS have been conducted) to other populations. This degradation stems from multiple sources: different linkage disequilibrium patterns mean that tag SNPs identify different causal variants, population-specific variants are absent from training data, and effect sizes may differ across populations due to gene-environment interactions.</p>
</section>
<section id="traitgym" class="level3" data-number="20.4.2">
<h3 data-number="20.4.2" class="anchored" data-anchor-id="traitgym"><span class="header-section-number">20.4.2</span> TraitGym</h3>
<p>TraitGym provides a framework specifically designed to assess complex trait prediction using genomic foundation models. The benchmark evaluates whether foundation model embeddings or variant scores improve prediction beyond traditional polygenic score methods.</p>
<p>TraitGym’s design addresses several limitations of standard PGS benchmarks. Ancestry stratification is built into the evaluation protocol, requiring models to report performance separately for different population groups. Multiple phenotypes spanning different genetic architectures (highly polygenic versus more oligogenic) test generalization across trait types. Comparison to appropriate baselines (standard PGS methods, clinical covariates alone) isolates the contribution of foundation model features.</p>
<p>The benchmark is particularly relevant for assessing claims that genomic foundation models add predictive value beyond classical statistical genetics. Foundation models incur substantial computational costs compared to linear PGS models; TraitGym helps determine whether these costs are justified by improved prediction.</p>
</section>
<section id="embedgem-framework" class="level3" data-number="20.4.3">
<h3 data-number="20.4.3" class="anchored" data-anchor-id="embedgem-framework"><span class="header-section-number">20.4.3</span> EmbedGEM Framework</h3>
<p>The EmbedGEM framework evaluates whether foundation model embeddings capture biologically meaningful genetic signal, as opposed to technical artifacts or confounders <span class="citation" data-cites="mukherjee_embedgem_2024">(<a href="references.html#ref-mukherjee_embedgem_2024" role="doc-biblioref">Mukherjee et al. 2024</a>)</span>. The framework assesses embeddings along two axes: heritability and disease relevance.</p>
<p>The heritability axis measures how much genetic signal an embedding captures. EmbedGEM counts the number of genome-wide significant loci associated with embedding components and quantifies the strength of association through mean chi-squared statistics. Higher values indicate that the embedding reflects heritable biology rather than noise.</p>
<p>The disease relevance axis measures whether embedding-associated variants predict clinically meaningful outcomes. Polygenic scores constructed from embedding GWAS hits are evaluated for their ability to predict disease in independent cohorts. Incremental predictive value over standard clinical models indicates that the embedding captures disease-relevant genetic information.</p>
<p>This two-axis evaluation addresses a critical question for foundation model deployment: do learned representations discover novel biology or merely recapitulate known associations with additional computational overhead? Embeddings that show high heritability but low disease relevance may capture biological signal that is not clinically actionable. Embeddings that show disease relevance without novel genetic discoveries may not add value beyond existing PGS methods.</p>
</section>
</section>
<section id="benchmark-construction-and-hidden-assumptions" class="level2" data-number="20.5">
<h2 data-number="20.5" class="anchored" data-anchor-id="benchmark-construction-and-hidden-assumptions"><span class="header-section-number">20.5</span> Benchmark Construction and Hidden Assumptions</h2>
<p>Beyond cataloging benchmark suites, understanding how benchmarks are constructed reveals assumptions that shape what they measure and what they miss.</p>
<section id="data-sources-and-label-provenance" class="level3" data-number="20.5.1">
<h3 data-number="20.5.1" class="anchored" data-anchor-id="data-sources-and-label-provenance"><span class="header-section-number">20.5.1</span> Data Sources and Label Provenance</h3>
<p>Benchmark labels derive from diverse sources with different properties. Experimental assays (ChIP-seq, DMS, MPRA) provide direct measurements but are limited by assay-specific artifacts and selection pressures. Computational annotations (gene calls, functional predictions, conservation scores) provide broader coverage but introduce circular dependencies if models are trained and evaluated on overlapping sources. Clinical classifications aggregate expert judgment but reflect the evidence available at classification time, which may include the very predictors being benchmarked.</p>
<p>The provenance of benchmark labels determines what success on that benchmark actually means. High performance on experimentally derived labels suggests the model captures the specific molecular process assayed. High performance on clinical labels may indicate genuine clinical utility or may reflect circularity with existing prediction tools. Understanding label provenance is prerequisite to interpreting benchmark results.</p>
</section>
<section id="splitting-strategies-and-leakage" class="level3" data-number="20.5.2">
<h3 data-number="20.5.2" class="anchored" data-anchor-id="splitting-strategies-and-leakage"><span class="header-section-number">20.5.2</span> Splitting Strategies and Leakage</h3>
<p>How benchmarks partition data into training and test sets determines whether evaluation measures generalization or memorization. Random splitting, where examples are assigned to splits uniformly at random, represents the weakest form of evaluation. In genomics, random splits often permit homology-based leakage: training and test sequences may share sufficient similarity that memorization suffices for good performance.</p>
<p>Homology-aware splitting clusters sequences by similarity before assigning clusters to splits, ensuring that test sequences are evolutionarily distant from training sequences. This approach is standard for protein benchmarks (using tools like CD-HIT or MMseqs2) but less consistently applied for DNA benchmarks.</p>
<p>Chromosome-based splitting holds out entire chromosomes for testing, preventing any position-based leakage within chromosomes. This approach is common for regulatory benchmarks but does not account for homologous sequences on different chromosomes. Temporal splitting reserves recent data for testing, appropriate when benchmarks derive from databases with submission timestamps. Each splitting strategy tests different aspects of generalization; the choice should match the intended deployment scenario.</p>
</section>
<section id="metric-selection-and-aggregation" class="level3" data-number="20.5.3">
<h3 data-number="20.5.3" class="anchored" data-anchor-id="metric-selection-and-aggregation"><span class="header-section-number">20.5.3</span> Metric Selection and Aggregation</h3>
<p>Benchmark metrics determine what aspects of model performance are measured. Discrimination metrics (AUROC, AUPRC, correlation) assess whether models rank predictions correctly. Calibration metrics (ECE, reliability diagrams) assess whether predicted probabilities match observed frequencies. Clinical utility metrics (net benefit, decision curves) assess whether predictions improve decisions compared to treating all patients the same.</p>
<p>Different metrics can yield different rankings of models. A model with superior discrimination may have poor calibration, predicting the right relative order but wrong absolute probabilities. Choosing which metric to optimize, and how to aggregate across multiple tasks or datasets, involves implicit decisions about what matters for downstream use.</p>
<p>Aggregation across tasks raises additional issues. Mean performance across many tasks weights each task equally, regardless of clinical importance or dataset quality. Median performance is robust to outliers but obscures variation. Reporting full distributions of task-level performance provides more information but complicates comparison. The choice of aggregation method can substantially affect which model appears best.</p>
</section>
<section id="goodharts-law-and-benchmark-gaming" class="level3" data-number="20.5.4">
<h3 data-number="20.5.4" class="anchored" data-anchor-id="goodharts-law-and-benchmark-gaming"><span class="header-section-number">20.5.4</span> Goodhart’s Law and Benchmark Gaming</h3>
<p>Benchmarks create incentive structures, and incentive structures invite optimization. Goodhart’s Law, that a measure ceases to be a good measure once it becomes a target, applies with particular force to machine learning evaluation. When model development prioritizes leaderboard position, the benchmark becomes the optimization target rather than a proxy for the underlying capability it was designed to measure.</p>
<p>Gaming takes multiple forms in genomic AI. Architectural choices may be tuned specifically to benchmark characteristics: receptive fields sized to match benchmark sequence lengths, output heads designed for benchmark label distributions, hyperparameters selected through extensive benchmark-specific search. Such tuning improves benchmark performance without necessarily improving generalization to deployment scenarios that differ from benchmark conditions.</p>
<p>More subtle gaming arises from selective reporting. Models may be evaluated on many benchmarks with only favorable results published. Benchmark versions may be chosen to maximize apparent performance. Evaluation protocols may deviate from published standards in ways that inflate metrics. The cumulative effect is a literature where reported performance systematically overestimates deployment capability.</p>
<p>The circularity between predictors and databases creates particularly insidious gaming dynamics. When ClinVar classifications incorporate computational predictions, and those predictions are then benchmarked against ClinVar, the benchmark rewards models that resemble their predecessors rather than models that provide independent information. This circularity is rarely acknowledged in benchmark reporting, yet it fundamentally compromises the validity of performance claims.</p>
<p>Mitigating gaming requires structural changes to evaluation practice: prospective benchmarks like CAGI where predictions precede labels, held-out evaluation consortia that resist optimization pressure, and reporting standards that require disclosure of all benchmarks attempted rather than only those where performance was favorable. The field’s maturation depends on developing evaluation cultures that reward honest assessment over leaderboard position.</p>
</section>
</section>
<section id="benchmark-saturation-and-staleness" class="level2" data-number="20.6">
<h2 data-number="20.6" class="anchored" data-anchor-id="benchmark-saturation-and-staleness"><span class="header-section-number">20.6</span> Benchmark Saturation and Staleness</h2>
<p>Benchmarks have finite useful lifetimes. As models improve, benchmarks saturate; as data and methods evolve, benchmarks become stale.</p>
<section id="saturation-when-benchmarks-stop-discriminating" class="level3" data-number="20.6.1">
<h3 data-number="20.6.1" class="anchored" data-anchor-id="saturation-when-benchmarks-stop-discriminating"><span class="header-section-number">20.6.1</span> Saturation: When Benchmarks Stop Discriminating</h3>
<p>A benchmark saturates when the best models achieve performance that cannot be meaningfully improved. Saturation may reflect fundamental limits (the benchmark approaches the Bayes error rate), measurement noise (the benchmark’s labels are too noisy to support finer discrimination), or ceiling effects (the metric itself cannot distinguish between excellent and perfect performance).</p>
<p>Saturation is problematic because it removes the benchmark’s value for model selection. When all reasonable models achieve 0.97 AUROC, differences between 0.970 and 0.975 are unlikely to reflect meaningful capability differences. Yet benchmark reporting conventions often emphasize such decimal places, creating an illusion of progress.</p>
<p>Detecting saturation requires estimating the irreducible error. For benchmarks with replicate measurements, comparing model performance to replicate concordance provides an upper bound: models cannot systematically outperform the reproducibility of the underlying assay. For benchmarks without replicates, saturation is harder to diagnose. One heuristic is tracking the rate of improvement: when new methods provide diminishing gains despite substantial architectural innovations, saturation is likely.</p>
<p>The response to saturation should be moving to harder benchmarks that still discriminate between methods, developing new benchmarks that capture aspects of performance that existing benchmarks miss, and retiring saturated benchmarks from active leaderboard competition while retaining them as sanity checks.</p>
</section>
<section id="staleness-when-benchmarks-diverge-from-practice" class="level3" data-number="20.6.2">
<h3 data-number="20.6.2" class="anchored" data-anchor-id="staleness-when-benchmarks-diverge-from-practice"><span class="header-section-number">20.6.2</span> Staleness: When Benchmarks Diverge from Practice</h3>
<p>Benchmarks become stale when they no longer reflect current data, methods, or clinical practice. Assays evolve: a benchmark constructed from early ENCODE data may not represent current experimental protocols. Annotations improve: gene models, variant classifications, and functional element maps are continuously updated. Clinical practice shifts: treatment guidelines and diagnostic criteria change the meaning of historical labels.</p>
<p>Staleness is insidious because it erodes benchmark validity gradually rather than abruptly. A benchmark that accurately represented regulatory prediction in 2015 may systematically misrepresent it in 2025, yet the benchmark’s continued use perpetuates optimization for an outdated target.</p>
<p>Addressing staleness requires periodic benchmark refresh with updated data and annotations, version control that documents exactly what each benchmark version contains, and awareness that performance on historical benchmarks may not predict performance on current data.</p>
</section>
<section id="leakage-from-scale" class="level3" data-number="20.6.3">
<h3 data-number="20.6.3" class="anchored" data-anchor-id="leakage-from-scale"><span class="header-section-number">20.6.3</span> Leakage from Scale</h3>
<p>Modern foundation models are pretrained on corpora that may include most publicly available genomic data. This creates novel leakage risks distinct from classical train-test overlap. A model pretrained on all ENCODE data may effectively have seen the exact experiments used in many regulatory benchmarks. A model pretrained on all UniRef may have seen sequences highly similar to protein benchmark test sets. This pretraining-benchmark overlap inflates performance in ways that are difficult to detect and even more difficult to correct.</p>
<p>Leakage from scale is particularly problematic because it is often undocumented. Model papers rarely enumerate exactly which datasets were included in pretraining corpora, and benchmark papers rarely specify which datasets should be excluded. The result is ambiguity about whether benchmark success reflects genuine generalization or memorization from pretraining.</p>
<p>Mitigating leakage from scale requires explicit documentation of pretraining corpora, tools or hashes that help identify overlap between pretraining data and benchmark test sets, and held-out evaluation consortia that reserve data specifically for assessment without any use in pretraining.</p>
</section>
</section>
<section id="the-benchmark-deployment-gap" class="level2" data-number="20.7">
<h2 data-number="20.7" class="anchored" data-anchor-id="the-benchmark-deployment-gap"><span class="header-section-number">20.7</span> The Benchmark-Deployment Gap</h2>
<p>High benchmark performance does not guarantee deployment success. Understanding why requires examining the systematic differences between benchmark settings and real-world applications.</p>
<section id="distribution-shift" class="level3" data-number="20.7.1">
<h3 data-number="20.7.1" class="anchored" data-anchor-id="distribution-shift"><span class="header-section-number">20.7.1</span> Distribution Shift</h3>
<p>Benchmark test sets sample from the same distribution as training sets. Deployment populations may differ systematically. For variant effect prediction, benchmark variants are typically common enough to appear in multiple databases, while deployment often targets rare variants seen in single individuals. For regulatory prediction, benchmarks derive from well-studied cell types and tissues, while deployment may require prediction in understudied contexts.</p>
<p>Distribution shift manifests as degraded performance, but the pattern of degradation varies. Some models degrade gracefully, maintaining reasonable accuracy across the distribution shift. Others degrade catastrophically, with confident predictions that prove systematically wrong. Benchmarks that include held-out subpopulations or out-of-distribution test sets provide some information about robustness, but cannot anticipate every deployment scenario.</p>
</section>
<section id="calibration-requirements" class="level3" data-number="20.7.2">
<h3 data-number="20.7.2" class="anchored" data-anchor-id="calibration-requirements"><span class="header-section-number">20.7.2</span> Calibration Requirements</h3>
<p>Clinical deployment requires not just accurate rankings but accurate probability estimates. A variant classifier that achieves 0.95 AUROC by assigning probability 0.9 to all pathogenic variants and 0.3 to all benign variants discriminates well but provides miscalibrated uncertainty. Clinical decisions that depend on thresholded predictions (reporting variants above a certain probability) will perform poorly if those probabilities don’t reflect actual pathogenicity rates.</p>
<p>Most benchmark metrics emphasize discrimination over calibration. AUROC is invariant to monotonic transformations of predicted probabilities. Correlation measures rank preservation. As a result, models may be optimized for benchmark success through strategies that damage calibration. The benchmark-deployment gap for calibration can be large even when discrimination metrics are excellent.</p>
</section>
<section id="metric-mismatch" class="level3" data-number="20.7.3">
<h3 data-number="20.7.3" class="anchored" data-anchor-id="metric-mismatch"><span class="header-section-number">20.7.3</span> Metric Mismatch</h3>
<p>Benchmarks optimize specific metrics that may not align with deployment objectives. AUROC weights errors equally regardless of where they occur on the score distribution, but clinical utility may depend primarily on performance at specific operating points. Correlation rewards getting the overall pattern right but may not penalize systematic errors in clinically important regions.</p>
<p>The gap between optimized metrics and deployment objectives creates misaligned incentives. Model developers optimize for benchmark success, which rewards specific metric improvements. Deployment success may require different tradeoffs: prioritizing calibration over discrimination, minimizing false negatives over false positives, or performing well on specific subpopulations rather than overall.</p>
</section>
<section id="practical-constraints" class="level3" data-number="20.7.4">
<h3 data-number="20.7.4" class="anchored" data-anchor-id="practical-constraints"><span class="header-section-number">20.7.4</span> Practical Constraints</h3>
<p>Deployment environments impose constraints that benchmarks typically ignore. Inference speed matters when predictions must be returned in clinical timescales. Model size matters when deployment hardware has limited memory. Interpretability matters when predictions must be explained to clinicians or patients. Benchmarks that evaluate only accuracy miss these dimensions of deployment fitness.</p>
<p>The benchmark-deployment gap is not merely a technical inconvenience. It represents a fundamental tension between evaluation tractability and deployment validity. Benchmarks are valuable precisely because they are standardized, reproducible, and comparable across methods. Deployment is valuable precisely because it addresses the specific needs of real-world applications. Bridging this gap requires benchmark designs that better approximate deployment conditions and deployment evaluations that provide feedback to benchmark development.</p>
</section>
</section>
<section id="systematic-gaps-in-current-benchmarks" class="level2" data-number="20.8">
<h2 data-number="20.8" class="anchored" data-anchor-id="systematic-gaps-in-current-benchmarks"><span class="header-section-number">20.8</span> Systematic Gaps in Current Benchmarks</h2>
<p>Despite the proliferation of benchmark suites, systematic gaps remain in the genomic evaluation landscape.</p>
<p><strong>Variant types</strong>: Structural variants, inversions, copy number variants, and complex rearrangements are rarely evaluated despite accounting for substantial genomic variation and disease burden. Repeat regions are often excluded or masked. Multi-variant effects and haplotype-specific phenomena receive minimal attention.</p>
<p><strong>Populations</strong>: Non-European ancestry groups remain severely underrepresented. Performance stratified by ancestry reveals gaps that aggregate metrics conceal. Environmental diversity (lifestyle, exposures, treatments) that shapes phenotypic expression is rarely incorporated.</p>
<p><strong>Modalities</strong>: Long-read sequencing data is scarce in benchmarks despite its advantages for structural variants and phasing. Single-cell benchmarks are emerging but remain limited compared to bulk assay benchmarks. Spatial transcriptomics and other emerging modalities have minimal coverage.</p>
<p><strong>Clinical endpoints</strong>: Most benchmarks use molecular surrogates rather than hard clinical endpoints. Disease incidence, progression, treatment response, and patient-reported outcomes are rarely the direct prediction target. The gap between molecular proxy accuracy and clinical utility remains poorly characterized.</p>
<p>These gaps mean that strong benchmark performance may not predict utility for underserved populations, understudied variant classes, or clinical applications that depend on endpoints the benchmarks don’t measure.</p>
</section>
<section id="summary" class="level2" data-number="20.9">
<h2 data-number="20.9" class="anchored" data-anchor-id="summary"><span class="header-section-number">20.9</span> Summary</h2>
<p>Benchmarks structure the incentives of genomic AI development. The specific tasks, metrics, and leaderboards that the community adopts determine what models are optimized for and what claims of progress are evaluated against. This chapter surveyed the benchmark landscape across biological modalities:</p>
<p>Protein benchmarks, including TAPE, FLIP, and ProteinGym, provide the most mature evaluation ecosystem. These benchmarks test whether language model representations capture structural, functional, and evolutionary constraints on protein sequences. DNA and regulatory benchmarks span classical classification tasks, quantitative signal prediction, and recent standardized suites like Genomic Benchmarks, BEND, and long-range evaluation frameworks. Variant effect benchmarks connect sequence changes to functional and clinical consequences, using both clinical databases (ClinVar, CAGI) and high-throughput experimental measurements (DMS, MPRA). Trait-level benchmarks evaluate complex phenotype prediction and genetic discovery, with frameworks like TraitGym and EmbedGEM specifically designed for foundation model assessment.</p>
<p>Across all categories, we identified persistent challenges: benchmark saturation that reduces discriminative power, staleness that erodes validity over time, leakage risks that inflate apparent performance, and systematic gaps in population, variant type, and clinical endpoint coverage. The benchmark-deployment gap, where strong benchmark performance fails to predict deployment success, represents perhaps the most consequential limitation.</p>
<p>The methodological principles for using benchmarks properly, including experiment design, metric selection, and avoiding common pitfalls, are the subject of <a href="p5-ch21-eval.html" class="quarto-xref"><span>Chapter 21</span></a>. The confounding issues that plague both benchmark construction and model training are examined in <a href="p5-ch22-confounding.html" class="quarto-xref"><span>Chapter 22</span></a>. Together with this catalog of the benchmark landscape, these chapters provide the foundation for rigorous evaluation of genomic foundation models.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-gresova_genomic_2023" class="csl-entry" role="listitem">
Grešová, Katarína, Vlastimil Martinek, David Čechák, Petr Šimeček, and Panagiotis Alexiou. 2023. <span>“Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification.”</span> <em>BMC Genomic Data</em> 24 (1): 25. <a href="https://doi.org/10.1186/s12863-023-01123-8">https://doi.org/10.1186/s12863-023-01123-8</a>.
</div>
<div id="ref-mukherjee_embedgem_2024" class="csl-entry" role="listitem">
Mukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom Soare, Raghav Tandon, David Amar, et al. 2024. <span>“<span>EmbedGEM</span>: A Framework to Evaluate the Utility of Embeddings for Genetic Discovery.”</span> <em>Bioinformatics Advances</em> 4 (1). <a href="https://doi.org/10.1093/bioadv/vbae135">https://doi.org/10.1093/bioadv/vbae135</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p5--eval-interp.html" class="pagination-link" aria-label="Part V: Evaluation and Reliability">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Part V: Evaluation and Reliability</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p5-ch21-eval.html" class="pagination-link" aria-label="Evaluation Methodology">
        <span class="nav-page-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Evaluation Methodology</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>