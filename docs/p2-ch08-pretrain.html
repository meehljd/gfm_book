<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>8&nbsp; Pretraining Objectives &amp; Strategies – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p2-ch09-transfer.html" rel="next">
<link href="./p2-ch07-foundation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part III: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch08-pretrain.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-pgs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS &amp; Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-cadd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Deleteriousness Scores</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-tokens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-foundation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretrain.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning &amp; Deployment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">CNN Sequence-to-Function Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RNA &amp; Transcript-Level Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-plm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-hybrid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Long-range Hybrid Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">p4–multi-modal_multi-scale.qmd</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-sc-epi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Single-Cell &amp; Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Graphs, Networks, and Biology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Multi-Omics &amp; Systems Biology</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch18-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch19-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-vep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-confound.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Interpretability &amp; Mechanisms</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch23-clinical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch24-variants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Pathogenic Variant Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-drugs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Drug Discovery &amp; Biotech</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Future Work &amp; Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-model-list.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-from-unlabeled-genomes" id="toc-learning-from-unlabeled-genomes" class="nav-link active" data-scroll-target="#learning-from-unlabeled-genomes"><span class="header-section-number">8.1</span> Learning from Unlabeled Genomes</a></li>
  <li><a href="#the-pretraining-paradigm" id="toc-the-pretraining-paradigm" class="nav-link" data-scroll-target="#the-pretraining-paradigm"><span class="header-section-number">8.2</span> The Pretraining Paradigm</a></li>
  <li><a href="#masked-language-modeling" id="toc-masked-language-modeling" class="nav-link" data-scroll-target="#masked-language-modeling"><span class="header-section-number">8.3</span> Masked Language Modeling</a>
  <ul class="collapse">
  <li><a href="#genomic-adaptations-of-mlm" id="toc-genomic-adaptations-of-mlm" class="nav-link" data-scroll-target="#genomic-adaptations-of-mlm"><span class="header-section-number">8.3.1</span> Genomic Adaptations of MLM</a></li>
  <li><a href="#what-mlm-learns" id="toc-what-mlm-learns" class="nav-link" data-scroll-target="#what-mlm-learns"><span class="header-section-number">8.3.2</span> What MLM Learns</a></li>
  </ul></li>
  <li><a href="#next-token-prediction-and-autoregressive-models" id="toc-next-token-prediction-and-autoregressive-models" class="nav-link" data-scroll-target="#next-token-prediction-and-autoregressive-models"><span class="header-section-number">8.4</span> Next-Token Prediction and Autoregressive Models</a>
  <ul class="collapse">
  <li><a href="#genomic-applications-of-autoregressive-pretraining" id="toc-genomic-applications-of-autoregressive-pretraining" class="nav-link" data-scroll-target="#genomic-applications-of-autoregressive-pretraining"><span class="header-section-number">8.4.1</span> Genomic Applications of Autoregressive Pretraining</a></li>
  <li><a href="#trade-offs-between-mlm-and-autoregressive-objectives" id="toc-trade-offs-between-mlm-and-autoregressive-objectives" class="nav-link" data-scroll-target="#trade-offs-between-mlm-and-autoregressive-objectives"><span class="header-section-number">8.4.2</span> Trade-offs Between MLM and Autoregressive Objectives</a></li>
  </ul></li>
  <li><a href="#denoising-objectives" id="toc-denoising-objectives" class="nav-link" data-scroll-target="#denoising-objectives"><span class="header-section-number">8.5</span> Denoising Objectives</a>
  <ul class="collapse">
  <li><a href="#genomic-denoising-strategies" id="toc-genomic-denoising-strategies" class="nav-link" data-scroll-target="#genomic-denoising-strategies"><span class="header-section-number">8.5.1</span> Genomic Denoising Strategies</a></li>
  </ul></li>
  <li><a href="#contrastive-learning" id="toc-contrastive-learning" class="nav-link" data-scroll-target="#contrastive-learning"><span class="header-section-number">8.6</span> Contrastive Learning</a>
  <ul class="collapse">
  <li><a href="#contrastive-strategies-for-genomic-sequences" id="toc-contrastive-strategies-for-genomic-sequences" class="nav-link" data-scroll-target="#contrastive-strategies-for-genomic-sequences"><span class="header-section-number">8.6.1</span> Contrastive Strategies for Genomic Sequences</a></li>
  <li><a href="#applications-of-contrastive-genomic-representations" id="toc-applications-of-contrastive-genomic-representations" class="nav-link" data-scroll-target="#applications-of-contrastive-genomic-representations"><span class="header-section-number">8.6.2</span> Applications of Contrastive Genomic Representations</a></li>
  </ul></li>
  <li><a href="#multi-task-pretraining" id="toc-multi-task-pretraining" class="nav-link" data-scroll-target="#multi-task-pretraining"><span class="header-section-number">8.7</span> Multi-Task Pretraining</a>
  <ul class="collapse">
  <li><a href="#genomic-multi-task-examples" id="toc-genomic-multi-task-examples" class="nav-link" data-scroll-target="#genomic-multi-task-examples"><span class="header-section-number">8.7.1</span> Genomic Multi-Task Examples</a></li>
  <li><a href="#when-multi-task-helps-and-when-it-hurts" id="toc-when-multi-task-helps-and-when-it-hurts" class="nav-link" data-scroll-target="#when-multi-task-helps-and-when-it-hurts"><span class="header-section-number">8.7.2</span> When Multi-Task Helps and When It Hurts</a></li>
  </ul></li>
  <li><a href="#data-strategies-for-pretraining" id="toc-data-strategies-for-pretraining" class="nav-link" data-scroll-target="#data-strategies-for-pretraining"><span class="header-section-number">8.8</span> Data Strategies for Pretraining</a>
  <ul class="collapse">
  <li><a href="#data-augmentation-and-sampling" id="toc-data-augmentation-and-sampling" class="nav-link" data-scroll-target="#data-augmentation-and-sampling"><span class="header-section-number">8.8.1</span> Data Augmentation and Sampling</a></li>
  </ul></li>
  <li><a href="#curriculum-learning-and-training-schedules" id="toc-curriculum-learning-and-training-schedules" class="nav-link" data-scroll-target="#curriculum-learning-and-training-schedules"><span class="header-section-number">8.9</span> Curriculum Learning and Training Schedules</a></li>
  <li><a href="#loss-functions-and-optimization" id="toc-loss-functions-and-optimization" class="nav-link" data-scroll-target="#loss-functions-and-optimization"><span class="header-section-number">8.10</span> Loss Functions and Optimization</a>
  <ul class="collapse">
  <li><a href="#loss-scaling-and-multi-task-balancing" id="toc-loss-scaling-and-multi-task-balancing" class="nav-link" data-scroll-target="#loss-scaling-and-multi-task-balancing"><span class="header-section-number">8.10.1</span> Loss Scaling and Multi-Task Balancing</a></li>
  </ul></li>
  <li><a href="#pretraining-at-scale" id="toc-pretraining-at-scale" class="nav-link" data-scroll-target="#pretraining-at-scale"><span class="header-section-number">8.11</span> Pretraining at Scale</a>
  <ul class="collapse">
  <li><a href="#data-throughput-and-infrastructure" id="toc-data-throughput-and-infrastructure" class="nav-link" data-scroll-target="#data-throughput-and-infrastructure"><span class="header-section-number">8.11.1</span> Data Throughput and Infrastructure</a></li>
  </ul></li>
  <li><a href="#monitoring-and-debugging-pretraining" id="toc-monitoring-and-debugging-pretraining" class="nav-link" data-scroll-target="#monitoring-and-debugging-pretraining"><span class="header-section-number">8.12</span> Monitoring and Debugging Pretraining</a>
  <ul class="collapse">
  <li><a href="#debugging-strategies" id="toc-debugging-strategies" class="nav-link" data-scroll-target="#debugging-strategies"><span class="header-section-number">8.12.1</span> Debugging Strategies</a></li>
  </ul></li>
  <li><a href="#choosing-the-right-pretraining-strategy" id="toc-choosing-the-right-pretraining-strategy" class="nav-link" data-scroll-target="#choosing-the-right-pretraining-strategy"><span class="header-section-number">8.13</span> Choosing the Right Pretraining Strategy</a>
  <ul class="collapse">
  <li><a href="#objective-selection-guidelines" id="toc-objective-selection-guidelines" class="nav-link" data-scroll-target="#objective-selection-guidelines"><span class="header-section-number">8.13.1</span> Objective Selection Guidelines</a></li>
  <li><a href="#when-to-pretrain-from-scratch-vs-fine-tune" id="toc-when-to-pretrain-from-scratch-vs-fine-tune" class="nav-link" data-scroll-target="#when-to-pretrain-from-scratch-vs-fine-tune"><span class="header-section-number">8.13.2</span> When to Pretrain from Scratch vs Fine-Tune</a></li>
  </ul></li>
  <li><a href="#case-studies-how-leading-models-were-pretrained" id="toc-case-studies-how-leading-models-were-pretrained" class="nav-link" data-scroll-target="#case-studies-how-leading-models-were-pretrained"><span class="header-section-number">8.14</span> Case Studies: How Leading Models Were Pretrained</a>
  <ul class="collapse">
  <li><a href="#dnabert" id="toc-dnabert" class="nav-link" data-scroll-target="#dnabert"><span class="header-section-number">8.14.1</span> DNABERT</a></li>
  <li><a href="#hyenadna" id="toc-hyenadna" class="nav-link" data-scroll-target="#hyenadna"><span class="header-section-number">8.14.2</span> HyenaDNA</a></li>
  <li><a href="#enformer" id="toc-enformer" class="nav-link" data-scroll-target="#enformer"><span class="header-section-number">8.14.3</span> Enformer</a></li>
  <li><a href="#esm-2" id="toc-esm-2" class="nav-link" data-scroll-target="#esm-2"><span class="header-section-number">8.14.4</span> ESM-2</a></li>
  </ul></li>
  <li><a href="#the-relationship-between-pretraining-and-transfer" id="toc-the-relationship-between-pretraining-and-transfer" class="nav-link" data-scroll-target="#the-relationship-between-pretraining-and-transfer"><span class="header-section-number">8.15</span> The Relationship Between Pretraining and Transfer</a></li>
  <li><a href="#open-questions-and-future-directions" id="toc-open-questions-and-future-directions" class="nav-link" data-scroll-target="#open-questions-and-future-directions"><span class="header-section-number">8.16</span> Open Questions and Future Directions</a></li>
  <li><a href="#summary-and-best-practices" id="toc-summary-and-best-practices" class="nav-link" data-scroll-target="#summary-and-best-practices"><span class="header-section-number">8.17</span> Summary and Best Practices</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part III: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch08-pretrain.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-pretrain" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Transfer (domain adaptation, few-shot) — deployment strategies</p>
</div>
</div>
<p>Foundation models do not emerge fully formed. Before they can predict variant effects, interpret regulatory sequences, or guide experimental design, they must first learn general representations from vast amounts of unlabeled genomic data. This learning process is called pretraining, and the choice of pretraining objective fundamentally shapes what a model learns, how efficiently it trains, and which downstream tasks it ultimately excels at.</p>
<p>This chapter explores the landscape of pretraining strategies for genomic models. We begin with the conceptual foundations: why pretraining works, how self-supervised learning creates useful supervision from sequence structure alone, and the connections to language modeling paradigms from NLP. We then survey the major families of pretraining objectives, including masked language modeling, next-token prediction, denoising, contrastive learning, and multi-task approaches. For each, we examine the core algorithmic principles, genomic adaptations, and trade-offs between objectives.</p>
<p>Beyond objectives themselves, effective pretraining requires careful decisions about data curation, augmentation strategies, curriculum design, and computational infrastructure. We address these practical considerations alongside the theoretical motivations. Finally, we examine how leading models were pretrained in practice, extracting lessons from DNABERT, HyenaDNA, Enformer, and ESM-2 that inform future work.</p>
<p>By the end of this chapter you should understand:</p>
<ul>
<li>Why self-supervised pretraining is particularly well-suited to genomics.</li>
<li>The algorithmic mechanics and biological implications of major pretraining objectives.</li>
<li>How to select and design pretraining strategies for specific model architectures and downstream applications.</li>
<li>Practical considerations for data preparation, optimization, and scaling.</li>
<li>The relationship between pretraining choices and downstream transfer performance.</li>
</ul>
<section id="learning-from-unlabeled-genomes" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="learning-from-unlabeled-genomes"><span class="header-section-number">8.1</span> Learning from Unlabeled Genomes</h2>
<p>Genomics presents a paradox: we have enormous quantities of sequence data but relatively sparse functional annotations. Reference genomes span billions of nucleotides across thousands of species. Population sequencing projects catalog millions of individuals. Yet experimental labels—ChIP-seq peaks, expression measurements, clinical outcomes—are available for only a tiny fraction of possible sequences and contexts.</p>
<p>This imbalance motivates pretraining. Rather than training models from scratch on small labeled datasets, we can first learn general-purpose sequence representations from unlabeled genomes, then adapt these representations to specific tasks through fine-tuning or few-shot learning. The intuition is that many patterns relevant to regulatory function, splice site recognition, or protein folding are embedded in sequence statistics themselves. A model that learns to predict missing nucleotides or adjacent sequence context must implicitly capture motifs, constraints, and compositional structure that generalize across tasks.</p>
<p>Self-supervised learning provides the algorithmic framework for extracting supervision from unlabeled data. Instead of requiring external labels, self-supervised objectives construct prediction tasks from the data’s inherent structure. In genomics, this might mean masking portions of a sequence and predicting the masked content, predicting the next token in a sequence, or distinguishing augmented views of the same sequence from unrelated sequences. These artificial tasks force the model to build representations that capture sequence properties useful for many downstream applications.</p>
<p>The connection to natural language processing is direct. Models like BERT and GPT revolutionized NLP by pretraining on massive text corpora before fine-tuning on specific tasks. Genomic sequences are discrete symbol sequences much like text, and many of the same algorithmic principles apply. However, genomics introduces unique considerations: DNA has no word boundaries, both strands encode information, motifs are compositional, and biological function depends on spatial organization at multiple scales.</p>
</section>
<section id="the-pretraining-paradigm" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="the-pretraining-paradigm"><span class="header-section-number">8.2</span> The Pretraining Paradigm</h2>
<p>The standard deep learning workflow for genomics now follows a two-stage process: pretraining followed by fine-tuning or adaptation. During pretraining, the model processes large volumes of unlabeled sequence data under a self-supervised objective designed to encourage useful representations. The result is a pretrained model with learned parameters that encode general sequence properties. During fine-tuning, these pretrained parameters are adapted to specific labeled tasks, often with far less data than would be required to train from scratch.</p>
<p>This paradigm succeeds because of the transfer hypothesis: features learned on one task can improve performance on related tasks. A model pretrained to predict masked DNA tokens learns motif patterns, sequence constraints, and compositional structure. When fine-tuned to predict transcription factor binding, these representations provide a strong initialization that accelerates convergence and improves generalization. The pretrained model has already learned “what DNA looks like,” so the fine-tuning stage need only specialize these representations to the binding prediction task.</p>
<p>Genomics is particularly well-suited to this approach. We have orders of magnitude more unlabeled sequence than labeled functional data. Reference genomes provide billions of training examples at zero annotation cost. Even when functional labels exist, they are often sparse (covering small genomic regions), noisy (from experimental variability), or context-specific (measured in particular cell types or conditions). Pretraining allows us to leverage the full scope of genomic sequence diversity before specializing to narrower labeled datasets.</p>
<p>This contrasts sharply with training models from scratch on supervised tasks alone. Without pretraining, models must learn both general sequence structure and task-specific patterns simultaneously from limited labeled examples. This is not only data-inefficient but also risks overfitting to spurious correlations in small datasets. Pretraining separates these learning stages: general representations come from abundant unlabeled data, while task-specific refinement uses precious labeled examples more efficiently.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion (pretraining paradigm overview):</strong> A two-stage diagram showing (1) pretraining on unlabeled genome sequences with self-supervised objectives, producing a pretrained model, then (2) fine-tuning on labeled data (e.g., ChIP-seq peaks, variant effects) to produce a task-specific model. Arrows indicate parameter initialization from pretraining to fine-tuning.</p>
</div>
</div>
</section>
<section id="masked-language-modeling" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="masked-language-modeling"><span class="header-section-number">8.3</span> Masked Language Modeling</h2>
<p>Masked language modeling treats sequences as partially observed and trains models to predict missing content from surrounding context. Inspired by BERT’s success in NLP, MLM has become the dominant pretraining objective for genomic models. The core idea is simple: randomly mask portions of an input sequence, feed the corrupted sequence to the model, and train the model to reconstruct the original tokens at masked positions.</p>
<p>In practice, a masking strategy replaces selected tokens with a special <code>[MASK]</code> token or leaves them unchanged with some probability. For DNA sequences, this might mean masking individual nucleotides, k-mers, or byte-pair encoding tokens depending on the tokenization scheme. The model processes the masked sequence through its layers and produces predictions for the masked positions. The loss function is typically cross-entropy over the vocabulary at each masked position, computed only for masked tokens to avoid wasting computation on unmasked positions.</p>
<p>MLM encourages bidirectional context integration. Unlike left-to-right language models that can only condition on past tokens, MLM models see both left and right context when predicting masked positions. For genomics, this is biologically appropriate: regulatory function depends on patterns both upstream and downstream of any given position. A transcription factor binding site might be recognized through flanking sequences on both sides, and splicing signals require coordination between donor and acceptor sites separated by hundreds of bases.</p>
<p>The choice of masking strategy significantly impacts what models learn. Random masking of individual tokens creates a simple objective where each prediction is relatively local. Span masking, which masks contiguous blocks of tokens, forces models to infer longer-range dependencies and compositional patterns. Whole-word masking in NLP masks all tokens corresponding to a word, preventing trivial solutions from subword statistics alone. In genomics, masking entire k-mers or motifs rather than individual bases may encourage learning of functional modules rather than just nucleotide co-occurrence.</p>
<p>Masking rates present a trade-off. Higher masking rates (e.g., 30-40%) provide more supervision per sequence but make the prediction task harder and may destabilize training. Lower masking rates (e.g., 10-15%) are more stable but require more data to achieve equivalent coverage. The standard 15% rate from BERT represents a reasonable compromise, though genomic models have explored a range of values depending on context length and tokenization granularity.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion (MLM mechanics):</strong> A schematic showing an input DNA sequence, the same sequence with 15% of tokens masked, the model architecture processing the masked sequence, and predictions at masked positions. Highlight how bidirectional attention allows each position to see both upstream and downstream context.</p>
</div>
</div>
<section id="genomic-adaptations-of-mlm" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="genomic-adaptations-of-mlm"><span class="header-section-number">8.3.1</span> Genomic Adaptations of MLM</h3>
<p>DNABERT pioneered MLM for genomic sequences by applying it to overlapping k-mer tokens. Rather than treating DNA as individual nucleotides, DNABERT tokenizes sequences into all possible 6-mers with overlapping windows. Masking then operates at the k-mer level: entire 6-mers are masked as units. This design encourages the model to learn k-mer level patterns that correspond to transcription factor binding motifs and other short functional elements.</p>
<p>DNABERT-2 adopted byte-pair encoding tokenization, which learns a vocabulary of variable-length subword units from the training corpus. BPE tokens might represent single nucleotides, common motifs, or repeated elements depending on their frequency. MLM with BPE tokens balances the flexibility of single-nucleotide models with the compositional structure of fixed k-mer approaches. However, BPE introduces its own complexities: the learned vocabulary may not align with biological functional units, and different tokenization schemes can lead to different learned representations.</p>
<p>The Nucleotide Transformer family applies MLM with variable-length masking and very long context windows, pretraining on sequences up to 1000 nucleotides or more. By scaling both model capacity and context length, these models capture longer-range dependencies relevant to enhancer-promoter interactions, chromatin domain structure, and coordinated regulation of gene clusters.</p>
<p>Biological considerations inform masking decisions beyond algorithmic choices. Masking functional elements like transcription factor binding sites or splice motifs creates harder but more biologically relevant prediction tasks. If the training corpus includes evolutionary conservation information, masking conserved regions may teach models about functional constraint. Conversely, masking repetitive or low-complexity regions may provide less informative supervision.</p>
</section>
<section id="what-mlm-learns" class="level3" data-number="8.3.2">
<h3 data-number="8.3.2" class="anchored" data-anchor-id="what-mlm-learns"><span class="header-section-number">8.3.2</span> What MLM Learns</h3>
<p>MLM objectives drive models to capture multiple aspects of sequence organization. At the lowest level, models learn nucleotide-level statistics and local constraints: CpG dinucleotide frequencies, GC content biases, and simple repeat patterns. These basic properties are necessary but not sufficient for biological function.</p>
<p>At a higher level, MLM encourages learning of motif patterns. To accurately predict masked positions in transcription factor binding sites, models must recognize surrounding motif context. Predicting splice donor or acceptor sequences requires models to encode the consensus patterns characteristic of these sites. Over many training examples, models implicitly build representations of motifs as distributed patterns across embedding dimensions.</p>
<p>Beyond individual motifs, MLM captures sequence grammar: how motifs combine, their spatial relationships, and context-dependent usage. If certain transcription factor motifs co-occur at specific distances, masking one motif and predicting it from the other reinforces this grammatical relationship in the learned representations. This compositional learning is difficult to achieve with supervised learning alone, which typically provides coarse binary labels rather than fine-grained structural information.</p>
<p>Finally, MLM captures evolutionary conservation patterns. Conserved sequences are often conserved because mutations would disrupt function. By learning to predict these conserved patterns from surrounding context, models implicitly learn which sequence features are constrained by selection. This knowledge transfers to downstream tasks like variant effect prediction, where the model can recognize when a mutation disrupts a learned conserved pattern.</p>
</section>
</section>
<section id="next-token-prediction-and-autoregressive-models" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="next-token-prediction-and-autoregressive-models"><span class="header-section-number">8.4</span> Next-Token Prediction and Autoregressive Models</h2>
<p>Next-token prediction represents an alternative pretraining paradigm where models learn to predict each token in a sequence given only the preceding tokens. This autoregressive approach, popularized by GPT-style language models, treats sequence generation as a core capability rather than a secondary feature. The objective is straightforward: for a sequence of length <span class="math inline">\(T\)</span>, predict token <span class="math inline">\(t\)</span> from tokens <span class="math inline">\(1\)</span> through <span class="math inline">\(t-1\)</span>, maximizing the likelihood of the observed sequence under the model.</p>
<p>Algorithmically, next-token prediction requires causal masking in the attention mechanism. Unlike MLM’s bidirectional attention, autoregressive models prevent each position from attending to future positions. This ensures predictions at position <span class="math inline">\(t\)</span> depend only on positions <span class="math inline">\(1, \ldots, t-1\)</span>, matching the conditional probability factorization inherent in the objective. The loss function is still cross-entropy over the vocabulary, but computed at every position rather than only at masked locations.</p>
<p>This objective has a distinct flavor from MLM. By predicting sequences token by token, autoregressive models naturally learn generative capabilities. They can sample new sequences by predicting the first token, conditioning on it to predict the second, and so forth. This makes autoregressive pretraining attractive for sequence design applications where generating novel sequences is the goal rather than a side benefit.</p>
<p>The trade-off is computational and statistical efficiency. MLM sees bidirectional context and predicts multiple positions per sequence in parallel. Autoregressive models see only left context and must process sequences sequentially during generation. For pretraining, however, teacher forcing allows efficient parallel computation: the model predicts all positions simultaneously during training by feeding in the ground truth sequence shifted by one position. Generation at inference time is slower, but pretraining speed is comparable to MLM.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion (autoregressive vs bidirectional):</strong> A side-by-side comparison showing (1) bidirectional attention in MLM where each position sees full context, and (2) causal attention in autoregressive models where each position sees only leftward context. Illustrate the difference in attention masks and how it affects representation learning.</p>
</div>
</div>
<section id="genomic-applications-of-autoregressive-pretraining" class="level3" data-number="8.4.1">
<h3 data-number="8.4.1" class="anchored" data-anchor-id="genomic-applications-of-autoregressive-pretraining"><span class="header-section-number">8.4.1</span> Genomic Applications of Autoregressive Pretraining</h3>
<p>DNA sequences have no inherent directionality: both strands encode information, and regulatory function is often strand-agnostic. This complicates autoregressive modeling, which assumes a natural left-to-right reading order. Early autoregressive genomic models addressed this by training separate models for forward and reverse strands or by augmenting training data with reverse-complement sequences.</p>
<p>Evo-2 represents a recent large-scale autoregressive genomic model trained on whole genomes with long-context transformers. By predicting next tokens across chromosome-length sequences, Evo-2 learns long-range dependencies and generates coherent synthetic genomes. This capability is useful for designing regulatory circuits, generating training data through synthetic augmentation, and exploring sequence space beyond observed genomes.</p>
<p>Protein sequence models also benefit from autoregressive pretraining. ESM models, which predict amino acid sequences autoregressively, learn protein grammar and evolutionary constraints that transfer to structure prediction and function annotation tasks. For proteins, where N-terminus to C-terminus directionality has biological significance, autoregressive models are more natural than for bidirectional DNA.</p>
<p>Sequence design is a primary use case for autoregressive genomic models. Generating functional promoters, enhancers, or protein coding sequences benefits from coherent left-to-right generation that respects grammatical constraints learned during pretraining. Conditional generation, where the model generates sequences conditioned on desired properties, is also straightforward with autoregressive models by incorporating conditioning information into the context at each step.</p>
</section>
<section id="trade-offs-between-mlm-and-autoregressive-objectives" class="level3" data-number="8.4.2">
<h3 data-number="8.4.2" class="anchored" data-anchor-id="trade-offs-between-mlm-and-autoregressive-objectives"><span class="header-section-number">8.4.2</span> Trade-offs Between MLM and Autoregressive Objectives</h3>
<p>The choice between MLM and next-token prediction involves several considerations. For tasks requiring understanding of full sequence context, MLM’s bidirectional attention provides richer representations. Predicting transcription factor binding at a specific location benefits from seeing both upstream and downstream sequence. MLM models learn these bidirectional relationships explicitly during pretraining.</p>
<p>For generative tasks, autoregressive models are more principled. Their sequential prediction structure matches the generation process exactly, whereas generating from MLM models requires iterative masking and filling or auxiliary generative heads. Autoregressive models can also naturally handle variable-length sequences and streaming data.</p>
<p>Pretraining efficiency differs between objectives. MLM predicts only 15% of tokens per sequence but uses bidirectional context for each prediction. Autoregressive models predict all tokens but with unidirectional context. Empirically, MLM often converges faster to good downstream performance, but autoregressive models scale well to very large datasets and long contexts.</p>
<p>Task-specific performance depends on alignment between pretraining and downstream objectives. If the downstream task involves predicting missing information from context (variant effect prediction, binding site identification), MLM pretraining provides better transfer. If the downstream task involves generation or sequential decision-making (sequence design, sampling from conditional distributions), autoregressive pretraining aligns more naturally.</p>
</section>
</section>
<section id="denoising-objectives" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="denoising-objectives"><span class="header-section-number">8.5</span> Denoising Objectives</h2>
<p>Denoising objectives generalize masked language modeling by introducing more complex forms of corruption beyond simple token masking. The core principle remains: corrupt the input in some way, then train the model to reconstruct the original uncorrupted sequence. By varying the corruption strategy, we can teach models different aspects of sequence structure and robustness properties.</p>
<p>Token substitution replaces input tokens with random tokens from the vocabulary. Unlike masking, which uses a special symbol, substitution creates realistic corrupted sequences that resemble sequencing errors or natural variation. The model must learn to distinguish correct from incorrect tokens based on surrounding context. This encourages representations that capture local consistency and motif structure.</p>
<p>Deletion and insertion corruptions remove or add tokens at random positions, shifting subsequent tokens and changing sequence length. This teaches models about position-invariant features and functional elements that remain identifiable despite surrounding changes. For genomics, insertions and deletions are biologically realistic: indels are common mutation types, and models that handle them gracefully during pretraining may better predict their effects downstream.</p>
<p>Sequence permutation randomly shuffles spans of tokens, disrupting syntactic structure while preserving token content. Reconstructing the original order from the scrambled sequence forces the model to learn ordering dependencies and grammatical constraints. In genomics, this might reveal motif ordering rules or constraints on enhancer element organization.</p>
<p>Multi-corruption strategies combine several corruption types simultaneously: some tokens masked, others substituted, still others deleted or permuted. This creates a richer supervision signal that captures multiple aspects of sequence organization. However, multi-corruption makes the reconstruction task harder and may slow convergence if not carefully balanced.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion (denoising strategies):</strong> A set of examples showing an original DNA sequence followed by the same sequence under different corruption types (masking, substitution, deletion/insertion, permutation). For each, show the corrupted input and highlight the model’s reconstruction task.</p>
</div>
</div>
<section id="genomic-denoising-strategies" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1" class="anchored" data-anchor-id="genomic-denoising-strategies"><span class="header-section-number">8.5.1</span> Genomic Denoising Strategies</h3>
<p>Simulating sequencing errors provides biologically motivated corruption for genomic models. Base miscalls, systematic biases from sequencing platforms, and quality score patterns can all be incorporated into corruption strategies. Models trained with such corruptions may generalize better to real sequencing data with platform-specific error profiles.</p>
<p>Variant augmentation introduces biologically realistic sequence changes based on population variation. Randomly substituting alleles at known polymorphic sites or inserting variants from databases like gnomAD creates corrupted sequences that reflect natural genetic diversity. The model learns to recognize variants as systematic deviations from reference patterns, which may improve variant effect prediction downstream.</p>
<p>Structural variation simulation models larger-scale genomic changes: tandem duplications, copy number variation, and segmental rearrangements. These corruptions are harder to implement but capture realistic sources of genomic diversity beyond single-nucleotide changes. Models trained with structural variation corruptions may better understand how gene dosage, enhancer duplication, or domain boundary disruptions affect function.</p>
<p>Robustness to distribution shift is a key benefit of denoising pretraining. If downstream applications involve sequences from different populations, environments, or sequencing platforms than the pretraining corpus, models pretrained with appropriate corruptions can maintain performance despite distribution mismatch. This is particularly valuable in clinical genomics, where validation cohorts often differ from discovery cohorts in ancestry, sequencing technology, or phenotyping protocols.</p>
</section>
</section>
<section id="contrastive-learning" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="contrastive-learning"><span class="header-section-number">8.6</span> Contrastive Learning</h2>
<p>Contrastive learning takes a fundamentally different approach to self-supervised pretraining. Instead of reconstructing corrupted inputs, contrastive objectives train models to produce similar representations for different views of the same sequence while distinguishing them from representations of unrelated sequences. The intuition is that augmented versions of a sequence (e.g., with minor corruptions or transformations) should map to nearby points in representation space, while unrelated sequences should map to distant points.</p>
<p>The algorithmic framework typically involves constructing positive pairs and negative samples. For a given anchor sequence, positive pairs are created through augmentation: reverse complementation, random cropping, variant injection, or other transformations that preserve functional identity. Negative samples are drawn from other sequences in the training batch. The model produces embeddings for all sequences, and the contrastive loss encourages anchor and positive embeddings to be similar (high cosine similarity or low distance) while pushing apart anchor and negative embeddings.</p>
<p>InfoNCE loss is the most common contrastive objective. For an anchor embedding <span class="math inline">\(z_i\)</span> and positive embedding <span class="math inline">\(z_i^+\)</span>, InfoNCE maximizes:</p>
<p><span class="math display">\[\log \frac{\exp(z_i \cdot z_i^+ / \tau)}{\sum_j \exp(z_i \cdot z_j / \tau)}\]</span></p>
<p>where the sum in the denominator runs over the positive and all negative samples, and <span class="math inline">\(\tau\)</span> is a temperature parameter controlling the concentration of the distribution. Lower temperatures make the model more discriminative.</p>
<p>NT-Xent (normalized temperature-scaled cross entropy) is a variant used in SimCLR and related methods. It differs primarily in normalization details but shares the same core principle: maximize agreement between augmented views while minimizing agreement with unrelated examples.</p>
<p>Triplet loss offers an alternative formulation using anchor-positive-negative triplets directly:</p>
<p><span class="math display">\[\max(0, d(z_a, z_p) - d(z_a, z_n) + m)\]</span></p>
<p>where <span class="math inline">\(d\)</span> is a distance metric (often Euclidean distance) and <span class="math inline">\(m\)</span> is a margin hyperparameter. This loss explicitly enforces that positives are closer than negatives by at least margin <span class="math inline">\(m\)</span>.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion (contrastive learning):</strong> A diagram showing (1) an anchor sequence, (2) positive examples created through augmentation (reverse complement, cropped versions, variants), (3) negative examples from other sequences, and (4) the embedding space where the model pulls positives together and pushes negatives apart.</p>
</div>
</div>
<section id="contrastive-strategies-for-genomic-sequences" class="level3" data-number="8.6.1">
<h3 data-number="8.6.1" class="anchored" data-anchor-id="contrastive-strategies-for-genomic-sequences"><span class="header-section-number">8.6.1</span> Contrastive Strategies for Genomic Sequences</h3>
<p>Augmentation design is critical for contrastive learning. Augmentations must preserve functional identity while introducing variability: if augmentations change function, the contrastive objective will learn meaningless invariances. For genomic sequences, several augmentation strategies are biologically grounded.</p>
<p>Reverse complementation is the simplest and most reliable augmentation. DNA is double-stranded, and many regulatory elements function identically on either strand. Training the model to treat forward and reverse complement sequences as equivalent captures strand symmetry inherent in molecular biology.</p>
<p>Random cropping extracts overlapping windows from longer sequences. If a transcription factor binding site appears in multiple cropped windows, the model learns that the site is the functionally relevant feature regardless of surrounding context. This teaches position-invariant representations useful for tasks where absolute genomic coordinates are less important than local sequence content.</p>
<p>Variant injection introduces common polymorphisms or simulated mutations into sequences. If the variants are neutral or do not disrupt function, treating the variant and reference sequences as positive pairs teaches the model robustness to genetic variation. This is particularly valuable for cross-population generalization, where models must recognize functional elements despite surrounding sequence polymorphism.</p>
<p>Negative sampling strategies also matter. Random sequences from the genome provide straightforward negatives, but they may be too easy to distinguish: any functional sequence is easily separable from random intergenic sequence. Harder negatives, such as sequences from orthologous regions in related species or sequences with similar motif content but different functional annotations, provide more informative supervision.</p>
<p>Cross-species contrastive learning leverages evolutionary relationships. Orthologous sequences from different species share functional identity despite nucleotide divergence. Treating orthologous pairs as positives and non-orthologous pairs as negatives teaches the model to extract species-invariant functional features. This can improve cross-species transfer: a model pretrained with human-mouse contrastive pairs may generalize better to rat or primate sequences.</p>
</section>
<section id="applications-of-contrastive-genomic-representations" class="level3" data-number="8.6.2">
<h3 data-number="8.6.2" class="anchored" data-anchor-id="applications-of-contrastive-genomic-representations"><span class="header-section-number">8.6.2</span> Applications of Contrastive Genomic Representations</h3>
<p>Sequence embedding quality improves with contrastive pretraining. Models trained contrastively produce embedding spaces where functionally similar sequences cluster together. This is useful for nearest-neighbor search, sequence retrieval, and unsupervised clustering of regulatory elements based on learned representations.</p>
<p>Variant effect prediction benefits from contrastive learning through robustness to genetic variation. If the model learns that sequences differing only by neutral variants are functionally equivalent, it will better distinguish disruptive variants that genuinely alter function from benign polymorphisms. This aligns with the clinical need to prioritize rare, deleterious variants over common, neutral ones.</p>
<p>Evolutionary relationships emerge naturally in contrastive representations. If orthologous sequences are treated as positives during pretraining, the learned embedding space reflects evolutionary distance: closely related species have nearby embeddings, while distantly related species are separated. This can inform phylogenetic analyses and cross-species prediction tasks.</p>
</section>
</section>
<section id="multi-task-pretraining" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="multi-task-pretraining"><span class="header-section-number">8.7</span> Multi-Task Pretraining</h2>
<p>Multi-task pretraining combines multiple related objectives during the same training run, jointly optimizing for several prediction tasks. The rationale is that different tasks provide complementary supervision signals: masking captures local patterns, chromatin prediction captures regulatory function, and conservation scoring captures evolutionary constraint. By learning representations that satisfy all tasks simultaneously, the model may develop richer and more general features than any single objective alone.</p>
<p>Task selection is the first design decision. Ideally, tasks should be diverse enough to provide distinct supervision signals but related enough to benefit from shared representations. For genomic models, common combinations include:</p>
<ul>
<li>Masked language modeling for general sequence structure.</li>
<li>Chromatin accessibility prediction for regulatory function.</li>
<li>Gene expression prediction for transcriptional output.</li>
<li>Evolutionary conservation scoring for functional constraint.</li>
<li>Variant frequency prediction from population databases.</li>
</ul>
<p>Each task operates on the same input sequence but predicts different outputs using task-specific head layers. The shared backbone encoder processes the sequence into intermediate representations, and separate prediction heads map these representations to task-specific outputs.</p>
<p>Task weighting determines how much each task contributes to the total loss. With <span class="math inline">\(L_1, \ldots, L_K\)</span> representing individual task losses, the multi-task loss is typically:</p>
<p><span class="math display">\[L_{\text{total}} = \sum_{k=1}^K w_k L_k\]</span></p>
<p>where <span class="math inline">\(w_k\)</span> are task weights. Equal weighting (<span class="math inline">\(w_k = 1\)</span> for all <span class="math inline">\(k\)</span>) is simple but may lead to imbalanced learning if tasks have different scales or difficulties. Task-specific weights can be tuned by grid search, but this is expensive with many tasks.</p>
<p>Dynamic task weighting adjusts weights during training based on learning dynamics. Uncertainty-based weighting uses the magnitude of task losses as a signal: tasks with higher loss receive higher weight, encouraging the model to focus on harder objectives. Gradient-based methods balance task contributions by equalizing gradient magnitudes across tasks. These approaches require careful implementation to avoid instabilities.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion (multi-task architecture):</strong> A diagram showing a shared encoder backbone processing an input sequence, then branching into multiple task-specific prediction heads (MLM head, chromatin prediction head, conservation scoring head, etc.). Illustrate how gradients from all tasks flow back through the shared encoder.</p>
</div>
</div>
<section id="genomic-multi-task-examples" class="level3" data-number="8.7.1">
<h3 data-number="8.7.1" class="anchored" data-anchor-id="genomic-multi-task-examples"><span class="header-section-number">8.7.1</span> Genomic Multi-Task Examples</h3>
<p>Enformer and Borzoi exemplify large-scale multi-task pretraining for genomics. Enformer predicts over 5,000 genomic assays simultaneously: ChIP-seq signals for hundreds of transcription factors and histone marks, DNase-seq and ATAC-seq accessibility, CAGE transcription initiation, and more. This massive multi-task objective forces the model to learn representations that capture diverse regulatory signals across cell types and experimental conditions.</p>
<p>The task diversity in Enformer provides supervision far richer than any single assay alone. A model trained only on DNase-seq might learn general accessibility patterns but miss transcription factor specificity. A model trained only on H3K27ac ChIP-seq might capture active enhancers but miss repressive marks. Training on all assays jointly allows the model to disentangle overlapping and complementary signals, learning representations that generalize across regulatory contexts.</p>
<p>Borzoi extends this paradigm to full RNA-seq coverage prediction, jointly modeling transcription initiation, splicing, and transcript abundance. By predicting both chromatin signals and transcriptomic outputs, Borzoi learns connections between regulatory state and gene expression that are difficult to capture with either modality alone.</p>
<p>Combined MLM and chromatin prediction represents another multi-task configuration. The model predicts masked tokens through a language modeling head while simultaneously predicting chromatin accessibility or other functional readouts through regression heads. This hybrid objective balances sequence-level pretraining with functional-level supervision, potentially combining the benefits of both approaches.</p>
<p>RNA, DNA, and protein joint training is an emerging direction. Models like those in the ESM family predict protein sequences and structures jointly, while genomic models are beginning to incorporate RNA-seq, ribosome profiling, and protein-DNA binding data into unified multi-task frameworks. These cross-modality models may learn representations that bridge sequence, structure, and function across biological scales.</p>
</section>
<section id="when-multi-task-helps-and-when-it-hurts" class="level3" data-number="8.7.2">
<h3 data-number="8.7.2" class="anchored" data-anchor-id="when-multi-task-helps-and-when-it-hurts"><span class="header-section-number">8.7.2</span> When Multi-Task Helps and When It Hurts</h3>
<p>Task interference is a primary concern with multi-task learning. If tasks require conflicting representations, jointly optimizing for both may compromise performance on each compared to single-task baselines. In genomics, this might occur if one task benefits from very local features while another requires long-range context, forcing the model to choose a suboptimal architecture for both.</p>
<p>Negative transfer occurs when adding a task during pretraining actually hurts downstream performance compared to training without it. This can happen if the additional task introduces noise, if task weights are poorly balanced, or if the auxiliary task shifts the learned representations away from features useful for the target downstream application.</p>
<p>Computational overhead increases with the number of tasks. Each task requires a prediction head, loss computation, and storage of task-specific labels. Data loading and preprocessing become more complex when different tasks operate on different genomic regions or require different data formats. These costs must be weighed against potential benefits.</p>
<p>The benefits of multi-task pretraining are largest when tasks are complementary and data for individual tasks is limited. If chromatin data is sparse for a particular cell type but gene expression data is abundant, jointly training on both may improve performance on both compared to single-task models. The shared representations allow information to flow between tasks, compensating for data scarcity in any single modality.</p>
</section>
</section>
<section id="data-strategies-for-pretraining" class="level2" data-number="8.8">
<h2 data-number="8.8" class="anchored" data-anchor-id="data-strategies-for-pretraining"><span class="header-section-number">8.8</span> Data Strategies for Pretraining</h2>
<p>Corpus construction establishes the foundation for pretraining. The choice of training data determines what patterns the model can learn and how well it will generalize to downstream tasks. For genomic models, this involves decisions about reference genomes, population variation, repeat handling, and chromosome representation.</p>
<p>Reference genomes are the standard starting point. Human genome assemblies like GRCh38 provide a high-quality, contiguous reference spanning all chromosomes. Training on the reference genome allows models to learn patterns characteristic of human DNA: base composition, repeat structure, gene organization, and regulatory architecture. However, the reference genome represents only a single haploid consensus sequence, missing the rich variation present in human populations.</p>
<p>Population-scale variation can be incorporated through variant databases like gnomAD. Rather than training only on the reference sequence, we can inject variants at their observed population frequencies to create synthetic diploid genomes that reflect real genetic diversity. This teaches models that common polymorphisms are normal variation rather than errors, potentially improving robustness and variant effect prediction.</p>
<p>Pan-genome approaches extend this further by explicitly representing multiple high-quality genome assemblies from diverse individuals. Instead of a single linear reference, pan-genomes capture structural variation, copy number differences, and population-specific haplotypes. Models trained on pan-genome representations may better understand how sequences function across genetic backgrounds.</p>
<p>Repeat masking decisions impact pretraining. Simple repeats, tandem repeats, and transposable elements occupy substantial genomic fractions but contribute less to regulatory function than unique sequences. Hard-masking repeats (replacing them with Ns) reduces training data size but may discard information relevant to some tasks. Soft-masking (lowercase) retains sequence information while marking repetitive regions, allowing models to learn differential representations for repeats and unique sequences.</p>
<p>Chromosome representation determines context window and segmentation. Some models train on entire chromosomes as single sequences, though this requires efficient long-range architectures. Others segment chromosomes into fixed-length windows (e.g., 1 kb, 10 kb, or 100 kb) and train on these fragments independently. Overlapping windows provide additional training examples but may create artificial dependencies at boundaries.</p>
<section id="data-augmentation-and-sampling" class="level3" data-number="8.8.1">
<h3 data-number="8.8.1" class="anchored" data-anchor-id="data-augmentation-and-sampling"><span class="header-section-number">8.8.1</span> Data Augmentation and Sampling</h3>
<p>Data augmentation artificially increases training diversity without requiring additional labeled data. For genomic sequences, several augmentation strategies are standard.</p>
<p>Reverse complementation exploits DNA strand symmetry. Augmenting each training sequence with its reverse complement doubles the effective training data and encourages models to learn strand-invariant representations. This is typically done on-the-fly during training rather than pre-computing augmented examples.</p>
<p>Random cropping extracts variable-length windows from longer sequences or takes overlapping segments with stochastic offsets. This teaches position-invariant features: the model must recognize functional elements regardless of their absolute position within the training window.</p>
<p>Variant injection randomly substitutes reference alleles with alternate alleles from population databases. This simulates genetic variation and teaches models to distinguish functional variants from neutral polymorphisms. Injection probabilities can match population allele frequencies for realistic augmentation.</p>
<p>Sampling strategies determine which genomic regions contribute to training. Uniform sampling draws sequences randomly across the genome, weighting all regions equally. This is unbiased but means coding regions (2% of the human genome) are underrepresented.</p>
<p>GC content balancing samples sequences to match a target GC distribution, preventing models from exploiting compositional biases. High GC regions (CpG islands, gene-dense loci) and low GC regions (AT-rich deserts, heterochromatin) are sampled proportionally rather than uniformly.</p>
<p>Functional element enrichment oversamples genomic regions containing regulatory elements, transcription factor binding sites, or other features of interest. This biases the training distribution toward functional regions but can improve performance on downstream regulatory tasks by providing more supervision where it matters most.</p>
<p>Negative example construction is important for contrastive and classification tasks. Random genomic sequences provide simple negatives, but shuffled sequences (preserving dinucleotide or higher-order statistics) or scrambled functional elements provide harder negatives that force models to learn genuine functional patterns rather than compositional shortcuts.</p>
</section>
</section>
<section id="curriculum-learning-and-training-schedules" class="level2" data-number="8.9">
<h2 data-number="8.9" class="anchored" data-anchor-id="curriculum-learning-and-training-schedules"><span class="header-section-number">8.9</span> Curriculum Learning and Training Schedules</h2>
<p>Curriculum learning orders training examples from easy to hard, gradually increasing task difficulty as the model improves. This mimics educational curricula where foundational concepts are taught before advanced topics. In genomics, curriculum strategies can be applied to context length, pattern complexity, or functional difficulty.</p>
<p>Context length curricula start with short sequences and progressively increase length as training proceeds. A transformer model might train first on 512-base windows, then 2 kb, then 10 kb, gradually expanding receptive field and long-range dependency requirements. This stabilizes early training by focusing on local patterns before introducing distant interactions.</p>
<p>Low-to-high complexity curricula begin with simple sequences (e.g., coding regions with clear structure) before introducing complex regulatory regions with overlapping, context-dependent motifs. This progression allows models to master basic patterns before tackling harder cases.</p>
<p>Coarse-to-fine curricula move from chromosome-level patterns to base-level resolution. Early training might focus on predicting gene density, chromatin state, or large-scale structure. Later training refines to single-base predictions, motif boundaries, and precise functional annotations.</p>
<p>Training schedules encompass learning rate decay, batch size scaling, and warmup phases. Learning rate warmup gradually increases the learning rate from near-zero over the first few thousand steps. This prevents early training instability when the model has random initializations and large gradient variance.</p>
<p>Cosine decay schedules reduce learning rate following a cosine curve from peak to near-zero over training. This provides aggressive learning early when gradients are informative and gentle refinement late when the model nears convergence. Step decay schedules drop learning rate by a fixed factor at predetermined intervals (e.g., every 10 epochs).</p>
<p>Batch size scaling increases batch size during training, enabled by distributed training infrastructure. Larger batches reduce gradient variance but may require learning rate adjustments to maintain convergence speed. Some studies suggest optimal batch sizes scale with model size: larger models benefit from larger batches.</p>
<p>Context length curriculum is particularly important for transformer models trained on genomic sequences. Training directly on long contexts (e.g., 100 kb) from initialization is unstable: attention matrices scale quadratically with sequence length, memory requirements are prohibitive, and gradients may vanish or explode. Starting with short contexts and progressively increasing length allows models to build stable representations before facing long-range dependencies.</p>
</section>
<section id="loss-functions-and-optimization" class="level2" data-number="8.10">
<h2 data-number="8.10" class="anchored" data-anchor-id="loss-functions-and-optimization"><span class="header-section-number">8.10</span> Loss Functions and Optimization</h2>
<p>Objective-specific losses translate pretraining objectives into differentiable optimization targets. The choice of loss function impacts what the model learns and how efficiently it trains.</p>
<p>Cross-entropy loss is standard for MLM and next-token prediction. For a vocabulary of size <span class="math inline">\(V\)</span>, the model predicts a distribution <span class="math inline">\(\hat{p}(x_i | \text{context})\)</span> over tokens at position <span class="math inline">\(i\)</span>, and cross-entropy measures disagreement with the true token:</p>
<p><span class="math display">\[L_{\text{CE}} = -\log \hat{p}(x_i = x_i^{\text{true}} | \text{context})\]</span></p>
<p>Averaged over all masked or predicted positions, this loss is minimized when the model’s predicted distribution matches the true conditional distribution.</p>
<p>Mean squared error or Poisson loss apply to continuous-valued predictions like chromatin signal intensity. If predicting ChIP-seq read counts, Poisson loss accounts for count-based variability:</p>
<p><span class="math display">\[L_{\text{Poisson}} = -\log \text{Poisson}(y; \hat{\lambda})\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the observed count and <span class="math inline">\(\hat{\lambda}\)</span> is the predicted rate. MSE is simpler but ignores the discrete, count-based nature of the data.</p>
<p>Multi-label classification losses handle cases where sequences have multiple overlapping labels (e.g., binding sites for multiple transcription factors). Binary cross-entropy per label or focal loss (which downweights easy examples) are common choices.</p>
<p>Regression losses for continuous targets might use MSE, mean absolute error, or Huber loss (which is robust to outliers). The choice depends on the target distribution and desired sensitivity to extreme values.</p>
<p>Optimization algorithms determine how gradient information updates model parameters. AdamW, a variant of Adam with decoupled weight decay, is the current standard for transformer pretraining. AdamW maintains running averages of gradients and squared gradients, enabling adaptive per-parameter learning rates that accelerate convergence compared to vanilla SGD.</p>
<p>Learning rate schedules modulate the effective step size during training. Warmup phases prevent early instability. Cosine decay provides smooth reduction toward convergence. Linear decay is simpler but may not perform as well. Plateau-based schedules reduce learning rate when validation loss stops improving, though these require careful validation set construction to avoid overfitting to the validation metric.</p>
<p>Gradient clipping prevents training instability from occasional large gradients. Clipping gradients by global norm (scaling all gradients proportionally when the total norm exceeds a threshold) is standard practice, especially for recurrent and transformer models where exploding gradients can occur.</p>
<p>Mixed precision training uses lower-precision arithmetic (float16 or bfloat16 instead of float32) to reduce memory consumption and accelerate computation on modern GPUs. Loss scaling prevents underflow in float16, and careful handling of gradient updates ensures numerical stability. Mixed precision is now standard for large-scale pretraining.</p>
<section id="loss-scaling-and-multi-task-balancing" class="level3" data-number="8.10.1">
<h3 data-number="8.10.1" class="anchored" data-anchor-id="loss-scaling-and-multi-task-balancing"><span class="header-section-number">8.10.1</span> Loss Scaling and Multi-Task Balancing</h3>
<p>Multi-task loss balancing ensures all tasks contribute meaningfully to learning. If one task has loss values 100 times larger than another, gradients will be dominated by the high-magnitude task unless weights compensate.</p>
<p>Manual weighting requires setting <span class="math inline">\(w_k\)</span> for each task through grid search or heuristic reasoning. This is feasible for small numbers of tasks but scales poorly to hundreds or thousands of tasks as in Enformer.</p>
<p>Uncertainty-based weighting parameterizes each task loss with a learned noise parameter <span class="math inline">\(\sigma_k^2\)</span>:</p>
<p><span class="math display">\[L_{\text{total}} = \sum_{k=1}^K \frac{1}{2 \sigma_k^2} L_k + \log \sigma_k\]</span></p>
<p>The model learns <span class="math inline">\(\sigma_k\)</span> during training, automatically balancing task contributions based on their uncertainty. Tasks with high uncertainty (high <span class="math inline">\(\sigma_k\)</span>) receive lower effective weight, while confident tasks (low <span class="math inline">\(\sigma_k\)</span>) receive higher weight.</p>
<p>Dynamic loss scaling adjusts task weights based on learning progress. If one task’s loss plateaus while another continues improving, increasing weight on the plateaued task may reinvigorate learning. Gradient magnitude balancing normalizes task gradients to similar scales before combining them, preventing any single task from dominating updates.</p>
</section>
</section>
<section id="pretraining-at-scale" class="level2" data-number="8.11">
<h2 data-number="8.11" class="anchored" data-anchor-id="pretraining-at-scale"><span class="header-section-number">8.11</span> Pretraining at Scale</h2>
<p>Large-scale pretraining requires substantial computational resources and careful infrastructure design. Modern genomic foundation models train on clusters with hundreds or thousands of GPUs, consuming months of compute time and terabytes of data.</p>
<p>FLOPs (floating-point operations) scale with model size, sequence length, and training dataset size. A transformer with <span class="math inline">\(L\)</span> layers, hidden dimension <span class="math inline">\(d\)</span>, and attention heads <span class="math inline">\(h\)</span> processing sequences of length <span class="math inline">\(n\)</span> requires approximately <span class="math inline">\(12 L d^2 n + 4 L d n^2\)</span> FLOPs per forward pass (ignoring feedforward layers). For models with billions of parameters on sequences of hundreds of kilobases, this amounts to petaFLOPs per training step.</p>
<p>Memory footprint includes model parameters, activations, gradients, and optimizer states. For a model with <span class="math inline">\(P\)</span> parameters, each forward pass stores activations proportional to <span class="math inline">\(P \cdot n\)</span> (sequence length times depth). Backward passes compute gradients of size <span class="math inline">\(P\)</span>, and AdamW stores two momentum buffers of size <span class="math inline">\(P\)</span> each. The total memory scales as <span class="math inline">\(\sim 4P\)</span> for parameters and optimizer states, plus activation memory that grows with batch size and sequence length.</p>
<p>Distributed training strategies parallelize computation across many devices. Data parallelism replicates the model on each device and splits training batches, aggregating gradients across devices. This scales well up to the point where communication overhead dominates, typically when batch size per device becomes too small.</p>
<p>Model parallelism splits the model itself across devices, with different layers or layer components on different GPUs. Pipeline parallelism stages layers sequentially, overlapping forward and backward passes across pipeline stages. These strategies enable training models too large to fit on a single device but introduce communication and synchronization overhead.</p>
<section id="data-throughput-and-infrastructure" class="level3" data-number="8.11.1">
<h3 data-number="8.11.1" class="anchored" data-anchor-id="data-throughput-and-infrastructure"><span class="header-section-number">8.11.1</span> Data Throughput and Infrastructure</h3>
<p>Data loading and preprocessing can become bottlenecks at scale. Efficiently reading genomic sequences, tokenizing them, and assembling batches requires optimized data pipelines. Preprocessing (tokenization, masking, augmentation) should happen in parallel with training using dedicated CPU workers.</p>
<p>Tokenization caching precomputes tokenized sequences and stores them on disk, avoiding repeated tokenization during training. This trades disk space for CPU time and significantly speeds up data loading when tokenization is expensive (e.g., byte-pair encoding).</p>
<p>GPU utilization monitoring ensures hardware is fully used. Profiling tools identify bottlenecks: if GPUs idle waiting for data, improve data loading; if utilization is low during computation, increase batch size or optimize kernel efficiency.</p>
<p>Multi-GPU and multi-node training distributes work across machines connected by high-bandwidth networks. Frameworks like PyTorch Distributed Data Parallel and DeepSpeed provide efficient synchronization and communication primitives. Gradient accumulation allows simulating large batch sizes by accumulating gradients over multiple mini-batches before updating parameters.</p>
<p>Checkpointing strategies save model state periodically to enable recovery from failures and to provide intermediate models for evaluation. Frequent checkpointing (every few hundred steps) provides fine-grained recovery but consumes significant disk space and I/O bandwidth. Balancing checkpoint frequency with storage constraints is essential for long training runs.</p>
<p>Fault tolerance mechanisms handle hardware failures during training. Preemptible cloud instances, power outages, or hardware errors can interrupt training. Automatic checkpoint loading and job restarting minimize lost work. Some frameworks support dynamic rescheduling and elastic training, adjusting parallelism as resources become available or fail.</p>
</section>
</section>
<section id="monitoring-and-debugging-pretraining" class="level2" data-number="8.12">
<h2 data-number="8.12" class="anchored" data-anchor-id="monitoring-and-debugging-pretraining"><span class="header-section-number">8.12</span> Monitoring and Debugging Pretraining</h2>
<p>Pretraining runs span weeks or months, and early detection of issues prevents wasted computation. Careful monitoring and debugging practices are essential.</p>
<p>Training loss curves should decrease smoothly in the early stages, eventually plateauing as the model approaches convergence. Sudden spikes suggest numerical instability, poor learning rate schedules, or corrupted batches. Persistent plateaus may indicate insufficient model capacity, inappropriate objectives, or optimization hyperparameters.</p>
<p>Perplexity measures how well a language model predicts sequences. For an MLM or next-token objective, perplexity is the exponential of the average cross-entropy loss:</p>
<p><span class="math display">\[\text{perplexity} = \exp(L_{\text{CE}})\]</span></p>
<p>Lower perplexity indicates better sequence modeling. Tracking perplexity on held-out validation data monitors generalization: if training perplexity decreases but validation perplexity increases, the model is overfitting.</p>
<p>Gradient norms indicate whether gradients are well-scaled. Very small gradients (vanishing gradient problem) prevent learning, while very large gradients (exploding gradients) destabilize training. Tracking the global gradient norm and per-layer gradient norms helps diagnose these issues early.</p>
<p>Red flags include loss explosions (often from numerical overflow or learning rate too high), gradient clipping activating frequently (gradients consistently too large), mode collapse in generative models (generating repetitive or degenerate sequences), and overfitting to the pretraining corpus (validation loss diverges from training loss).</p>
<section id="debugging-strategies" class="level3" data-number="8.12.1">
<h3 data-number="8.12.1" class="anchored" data-anchor-id="debugging-strategies"><span class="header-section-number">8.12.1</span> Debugging Strategies</h3>
<p>Probing tasks provide sanity checks during pretraining. Simple downstream evaluations (e.g., predicting DNase-seq peaks from sequence) can be run periodically on intermediate checkpoints. If probing performance plateaus or degrades, pretraining may not be learning useful representations.</p>
<p>Attention pattern visualization examines what the model attends to. Inspecting attention weights at different layers and heads can reveal whether the model learns meaningful patterns: do heads attend to motif boundaries, promoter-enhancer distances, or splice sites? Or do attention patterns appear random or dominated by positional biases?</p>
<p>Embedding space analysis projects learned representations into low-dimensional space using t-SNE or UMAP. Sequences with similar function should cluster together. If embeddings fail to separate functional classes (e.g., enhancers vs silencers) or show no structure, representations may not capture relevant biology.</p>
<p>Ablation studies isolate the impact of design choices. If uncertain whether multi-task training helps, train a baseline model without auxiliary tasks and compare downstream performance. If unsure about a data augmentation strategy, run training with and without it. Ablations are expensive but provide definitive answers when debugging complex pretraining pipelines.</p>
</section>
</section>
<section id="choosing-the-right-pretraining-strategy" class="level2" data-number="8.13">
<h2 data-number="8.13" class="anchored" data-anchor-id="choosing-the-right-pretraining-strategy"><span class="header-section-number">8.13</span> Choosing the Right Pretraining Strategy</h2>
<p>Selecting a pretraining approach involves balancing computational budget, target downstream tasks, data availability, and model architecture constraints. No single strategy is universally optimal, so understanding trade-offs is essential.</p>
<p>Compute budget determines feasible scale. Pretraining large models on long contexts requires significant resources. If compute is limited, smaller models, shorter contexts, or single-task objectives may be more practical. If abundant compute is available, multi-task pretraining on diverse objectives and long contexts provides maximum flexibility.</p>
<p>Target downstream tasks influence objective selection. For tasks requiring sequence understanding (variant effect prediction, binding site classification), MLM or denoising objectives align well. For generative design (creating synthetic promoters or enhancers), autoregressive or diffusion-based objectives are more natural. For cross-species applications, contrastive learning on orthologous sequences may improve transfer.</p>
<p>Data availability shapes corpus construction. If only reference genomes are available, standard MLM pretraining suffices. If functional assays (chromatin, expression) exist at scale, multi-task pretraining leverages this supervision. If population variant databases are accessible, denoising with variant augmentation improves robustness.</p>
<p>Model architecture constraints also matter. Convolutional models have limited receptive fields, so curriculum strategies on context length are less critical. Transformer models benefit from progressive context scaling. Recurrent models face vanishing gradients on long sequences, making denoising objectives with deletions/insertions challenging.</p>
<section id="objective-selection-guidelines" class="level3" data-number="8.13.1">
<h3 data-number="8.13.1" class="anchored" data-anchor-id="objective-selection-guidelines"><span class="header-section-number">8.13.1</span> Objective Selection Guidelines</h3>
<p>For most general-purpose DNA or protein models, MLM pretraining provides a strong default. It learns bidirectional context, scales efficiently, and transfers well to diverse downstream tasks. DNABERT and DNABERT-2 exemplify this approach for genomics, while ESM models demonstrate its effectiveness for proteins.</p>
<p>Next-token prediction is preferred when generation is the primary goal. If designing sequences from scratch, sampling from autoregressive models produces coherent outputs respecting learned grammar. Evo-2 and GPT-style genomic models exemplify this.</p>
<p>Multi-task pretraining makes sense when functional labels are available at scale and tasks are complementary. Enformer’s success with thousands of chromatin assays shows the power of multi-task learning when data supports it. However, multi-task requires infrastructure to handle heterogeneous data and careful loss balancing.</p>
<p>Contrastive learning is valuable for cross-species or variant-focused applications. If the goal is to transfer models trained on model organisms to related species, or to improve robustness to genetic variation, contrastive pretraining on orthologous pairs or variant-augmented sequences provides targeted benefits.</p>
</section>
<section id="when-to-pretrain-from-scratch-vs-fine-tune" class="level3" data-number="8.13.2">
<h3 data-number="8.13.2" class="anchored" data-anchor-id="when-to-pretrain-from-scratch-vs-fine-tune"><span class="header-section-number">8.13.2</span> When to Pretrain from Scratch vs Fine-Tune</h3>
<p>Starting from a pretrained model is almost always preferable if an appropriate model exists. Fine-tuning a DNABERT-2 checkpoint on a new binding prediction task is faster and more data-efficient than training from scratch. However, several scenarios favor pretraining from scratch.</p>
<p>New tokenization schemes require retraining. If switching from k-mer tokens to byte-pair encoding or single-nucleotide tokens, the existing vocabulary and embeddings are incompatible. Starting fresh is necessary.</p>
<p>New species without suitable existing models may benefit from pretraining on that species’ genome. While DNABERT trained on human DNA transfers reasonably to mouse, more distant organisms (plants, bacteria) may require species-specific pretraining to capture their unique sequence properties.</p>
<p>Different architectures cannot reuse pretrained weights. If moving from transformers to convolutional models or hybrid architectures, pretrained parameters do not apply directly. However, knowledge distillation or representation transfer may still help.</p>
<p>More data for the same domain suggests continued pretraining rather than starting fresh. If an existing model was pretrained on 3 billion tokens but a larger corpus is now available, continued pretraining from the existing checkpoint on the new data extends coverage without discarding prior learning.</p>
</section>
</section>
<section id="case-studies-how-leading-models-were-pretrained" class="level2" data-number="8.14">
<h2 data-number="8.14" class="anchored" data-anchor-id="case-studies-how-leading-models-were-pretrained"><span class="header-section-number">8.14</span> Case Studies: How Leading Models Were Pretrained</h2>
<p>Examining how successful models were pretrained provides concrete lessons and design patterns.</p>
<section id="dnabert" class="level3" data-number="8.14.1">
<h3 data-number="8.14.1" class="anchored" data-anchor-id="dnabert"><span class="header-section-number">8.14.1</span> DNABERT</h3>
<p>DNABERT introduced MLM pretraining to genomics by adapting BERT’s architecture to DNA sequences with overlapping k-mer tokenization. The model was pretrained on the human genome with 6-mer tokens, masking 15% of tokens at random. Training used standard BERT hyperparameters: AdamW optimizer with warmup, dropout regularization, and layer normalization.</p>
<p>Key lessons from DNABERT include the importance of tokenization choice (k-mers capture motif-level patterns better than single nucleotides), the value of strand symmetry (reverse complement augmentation improved performance), and the transferability of representations (pretrained DNABERT generalized well to diverse regulatory tasks despite training only on raw genome sequence).</p>
</section>
<section id="hyenadna" class="level3" data-number="8.14.2">
<h3 data-number="8.14.2" class="anchored" data-anchor-id="hyenadna"><span class="header-section-number">8.14.2</span> HyenaDNA</h3>
<p>HyenaDNA demonstrated that efficient long-range architectures enable pretraining on extremely long contexts. By using Hyena layers (subquadratic attention alternatives), HyenaDNA scaled to 1 Mb contexts, far beyond standard transformers. Pretraining used single-nucleotide next-token prediction on the human genome with a curriculum that progressively increased context length from short to long.</p>
<p>Lessons from HyenaDNA include the feasibility of million-base contexts with appropriate architectures, the benefits of curriculum learning for context scaling, and the emergence of long-range regulatory patterns when models have sufficient receptive field.</p>
</section>
<section id="enformer" class="level3" data-number="8.14.3">
<h3 data-number="8.14.3" class="anchored" data-anchor-id="enformer"><span class="header-section-number">8.14.3</span> Enformer</h3>
<p>Enformer pioneered multi-task chromatin prediction at scale. The model was pretrained jointly on over 5,000 assays from ENCODE, Roadmap Epigenomics, and other consortia, using a hybrid convolutional-transformer architecture. Task weighting was carefully balanced to prevent any single assay from dominating training. The model predicts chromatin signals across 128 kb windows with 128 bp resolution.</p>
<p>Key insights from Enformer include the power of large-scale multi-task learning (joint training on diverse assays improves all tasks), the importance of architectural design (combining convolutions for local patterns with transformers for long-range interactions balances efficiency and capacity), and the value of attention for interpretability (attention weights reveal learned enhancer-promoter contacts).</p>
</section>
<section id="esm-2" class="level3" data-number="8.14.4">
<h3 data-number="8.14.4" class="anchored" data-anchor-id="esm-2"><span class="header-section-number">8.14.4</span> ESM-2</h3>
<p>ESM-2 represents the state-of-the-art for protein language models, scaling to 15 billion parameters on evolutionary databases containing billions of protein sequences. Pretraining used standard MLM on amino acid sequences, but at unprecedented scale. ESM-2 demonstrated that pretraining on evolutionary diversity (hundreds of millions of protein families) transfers exceptionally well to structure prediction, function annotation, and protein design tasks.</p>
<p>Lessons from ESM-2 include the benefit of extreme scale (larger models and more data continue to improve even at billions of parameters), the value of evolutionary information (pretraining on diverse sequences captures constraints not visible in individual genomes), and the emergence of structural understanding from sequence alone (ESM-2 representations contain information about 3D structure despite no explicit structural supervision).</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion (case study comparison table):</strong> A table summarizing DNABERT, HyenaDNA, Enformer, and ESM-2 along dimensions such as objective (MLM vs next-token vs multi-task), architecture (transformer vs hybrid vs Hyena), pretraining corpus (human genome vs multi-species vs protein databases), context length, and key innovations.</p>
</div>
</div>
</section>
</section>
<section id="the-relationship-between-pretraining-and-transfer" class="level2" data-number="8.15">
<h2 data-number="8.15" class="anchored" data-anchor-id="the-relationship-between-pretraining-and-transfer"><span class="header-section-number">8.15</span> The Relationship Between Pretraining and Transfer</h2>
<p>Pretraining objectives predict transfer performance, though not always in obvious ways. MLM pretraining emphasizes bidirectional understanding, which benefits classification and interpretation tasks requiring full context. Next-token prediction emphasizes generation, favoring sequence design applications. Multi-task pretraining on functional assays directly optimizes for functional understanding, often providing best transfer to similar downstream tasks.</p>
<p>Misalignment between pretraining and downstream objectives can cause problems. If a model is pretrained autoregressively but then fine-tuned for bidirectional classification, the causal attention structure limits information flow. Conversely, an MLM-pretrained model cannot generate sequences without additional architectural modifications. Bridging these gaps may require intermediate objectives, hybrid architectures, or multi-stage pretraining.</p>
<p>Intermediate objectives provide gradual adaptation. A model might first pretrain with MLM on raw sequence, then continue pretraining with chromatin prediction on functional labels, and finally fine-tune on a specific variant effect task. Each stage specializes representations progressively, transferring knowledge from abundant unlabeled data through intermediate supervision to the final task.</p>
<p>For further discussion of deployment and transfer strategies, see <a href="p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>.</p>
</section>
<section id="open-questions-and-future-directions" class="level2" data-number="8.16">
<h2 data-number="8.16" class="anchored" data-anchor-id="open-questions-and-future-directions"><span class="header-section-number">8.16</span> Open Questions and Future Directions</h2>
<p>Despite rapid progress, many fundamental questions about genomic pretraining remain open.</p>
<p>Optimal objective combinations are unclear. Should we jointly train with MLM and chromatin prediction, or train sequentially? How many auxiliary tasks help before we hit diminishing returns? Do contrastive and generative objectives complement each other, or do they interfere?</p>
<p>Scaling laws relating pretraining compute, data size, and model capacity to downstream performance are not yet well-characterized for genomics. In NLP, power-law relationships predict optimal model sizing and training duration given a compute budget. Establishing similar laws for genomic models would guide resource allocation.</p>
<p>Task-aware pretraining vs truly general objectives presents a design tension. Enformer’s multi-task objective is highly task-aware, directly optimizing for chromatin predictions. DNABERT’s MLM is more general, agnostic to downstream tasks. Which approach generalizes better to unforeseen applications?</p>
<p>Incorporating biological priors (e.g., known motifs, pathway structure, evolutionary constraints) vs learning from scratch remains debated. Hand-engineered features risk encoding false assumptions, but pure data-driven learning may rediscover basic biology inefficiently. Hybrid approaches that combine priors with learned representations are underexplored.</p>
<p>Continual pretraining as new data arrives is increasingly relevant. As sequencing technologies improve and new assays emerge, how do we update pretrained models without catastrophic forgetting of prior knowledge? Online learning and elastic weight consolidation are potential solutions but remain largely untested in genomics.</p>
</section>
<section id="summary-and-best-practices" class="level2" data-number="8.17">
<h2 data-number="8.17" class="anchored" data-anchor-id="summary-and-best-practices"><span class="header-section-number">8.17</span> Summary and Best Practices</h2>
<p>Pretraining objectives form the foundation of modern genomic models. This chapter has surveyed the major families: masked language modeling for bidirectional sequence understanding, next-token prediction for generation, denoising for robustness, contrastive learning for invariant representations, and multi-task learning for functional supervision.</p>
<p>Core principles apply across objectives. Self-supervised learning leverages abundant unlabeled sequence data to learn general representations. Transfer from pretraining to fine-tuning improves data efficiency and generalization. Careful design of objectives, data strategies, and optimization details determines success.</p>
<p>When designing pretraining strategies, consider:</p>
<ul>
<li><strong>Objective alignment</strong>: Match pretraining objective to downstream task characteristics (bidirectional understanding vs generation).</li>
<li><strong>Data coverage</strong>: Pretrain on diverse, representative genomic sequences; augment to cover variation.</li>
<li><strong>Scale appropriately</strong>: Larger models and more data improve performance, but compute budget constrains ambition.</li>
<li><strong>Monitor carefully</strong>: Track loss curves, validation metrics, and probing tasks to catch issues early.</li>
<li><strong>Evaluate transfer</strong>: Pretraining is only valuable if it improves downstream performance; benchmark regularly.</li>
</ul>
<p>Best practices distilled from leading models include:</p>
<ul>
<li>Use MLM for most general-purpose DNA and protein models.</li>
<li>Apply multi-task pretraining when functional labels exist at scale.</li>
<li>Incorporate data augmentation (reverse complement, cropping, variant injection) for robustness.</li>
<li>Scale context length gradually via curriculum learning for transformers.</li>
<li>Balance multi-task losses carefully to avoid task domination.</li>
<li>Leverage existing pretrained models when possible; pretrain from scratch only when necessary.</li>
</ul>
<p>The next chapter (<a href="p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>) explores how to deploy and adapt pretrained models to specific downstream tasks, closing the loop from general-purpose pretraining to specialized applications.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p2-ch07-foundation.html" class="pagination-link" aria-label="Genomic Foundation Models: Concepts &amp; Taxonomy">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p2-ch09-transfer.html" class="pagination-link" aria-label="Transfer Learning &amp; Deployment">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning &amp; Deployment</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>