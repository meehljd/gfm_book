<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Genomic Foundation Models: Concepts &amp; Taxonomy – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p2-ch08-pretrain.html" rel="next">
<link href="./p2-ch06-transformers.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part III: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch07-foundation.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-pgs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS &amp; Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-cadd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Deleteriousness Scores</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-tokens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-foundation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretrain.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning &amp; Deployment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">CNN Sequence-to-Function Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RNA &amp; Transcript-Level Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-plm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-hybrid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Long-range Hybrid Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">p4–multi-modal_multi-scale.qmd</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-sc-epi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Single-Cell &amp; Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Graphs, Networks, and Biology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Multi-Omics &amp; Systems Biology</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch18-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch19-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-vep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-confound.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Interpretability &amp; Mechanisms</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch23-clinical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch24-variants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Pathogenic Variant Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-drugs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Drug Discovery &amp; Biotech</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Future Work &amp; Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-model-list.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#from-task-specific-models-to-foundation-models" id="toc-from-task-specific-models-to-foundation-models" class="nav-link active" data-scroll-target="#from-task-specific-models-to-foundation-models"><span class="header-section-number">7.1</span> From Task-Specific Models to Foundation Models</a></li>
  <li><a href="#defining-genomic-foundation-models" id="toc-defining-genomic-foundation-models" class="nav-link" data-scroll-target="#defining-genomic-foundation-models"><span class="header-section-number">7.2</span> Defining Genomic Foundation Models</a>
  <ul class="collapse">
  <li><a href="#essential-properties" id="toc-essential-properties" class="nav-link" data-scroll-target="#essential-properties"><span class="header-section-number">7.2.1</span> Essential Properties</a></li>
  <li><a href="#what-doesnt-count" id="toc-what-doesnt-count" class="nav-link" data-scroll-target="#what-doesnt-count"><span class="header-section-number">7.2.2</span> What Doesn’t Count</a></li>
  <li><a href="#why-definition-matters" id="toc-why-definition-matters" class="nav-link" data-scroll-target="#why-definition-matters"><span class="header-section-number">7.2.3</span> Why Definition Matters</a></li>
  </ul></li>
  <li><a href="#a-taxonomy-of-genomic-foundation-models" id="toc-a-taxonomy-of-genomic-foundation-models" class="nav-link" data-scroll-target="#a-taxonomy-of-genomic-foundation-models"><span class="header-section-number">7.3</span> A Taxonomy of Genomic Foundation Models</a>
  <ul class="collapse">
  <li><a href="#dna-language-models" id="toc-dna-language-models" class="nav-link" data-scroll-target="#dna-language-models"><span class="header-section-number">7.3.1</span> DNA Language Models</a></li>
  <li><a href="#sequence-to-function-foundation-models" id="toc-sequence-to-function-foundation-models" class="nav-link" data-scroll-target="#sequence-to-function-foundation-models"><span class="header-section-number">7.3.2</span> Sequence-to-Function Foundation Models</a></li>
  <li><a href="#variant-centric-foundation-models" id="toc-variant-centric-foundation-models" class="nav-link" data-scroll-target="#variant-centric-foundation-models"><span class="header-section-number">7.3.3</span> Variant-Centric Foundation Models</a></li>
  <li><a href="#multi-omic-foundation-models" id="toc-multi-omic-foundation-models" class="nav-link" data-scroll-target="#multi-omic-foundation-models"><span class="header-section-number">7.3.4</span> Multi-Omic Foundation Models</a></li>
  </ul></li>
  <li><a href="#design-dimensions-a-framework-for-understanding-models" id="toc-design-dimensions-a-framework-for-understanding-models" class="nav-link" data-scroll-target="#design-dimensions-a-framework-for-understanding-models"><span class="header-section-number">7.4</span> Design Dimensions: A Framework for Understanding Models</a>
  <ul class="collapse">
  <li><a href="#data-composition" id="toc-data-composition" class="nav-link" data-scroll-target="#data-composition"><span class="header-section-number">7.4.1</span> Data Composition</a></li>
  <li><a href="#architecture-families" id="toc-architecture-families" class="nav-link" data-scroll-target="#architecture-families"><span class="header-section-number">7.4.2</span> Architecture Families</a></li>
  <li><a href="#context-length-capabilities" id="toc-context-length-capabilities" class="nav-link" data-scroll-target="#context-length-capabilities"><span class="header-section-number">7.4.3</span> Context Length Capabilities</a></li>
  <li><a href="#tokenization-strategies" id="toc-tokenization-strategies" class="nav-link" data-scroll-target="#tokenization-strategies"><span class="header-section-number">7.4.4</span> Tokenization Strategies</a></li>
  </ul></li>
  <li><a href="#understanding-model-capabilities-through-the-taxonomy" id="toc-understanding-model-capabilities-through-the-taxonomy" class="nav-link" data-scroll-target="#understanding-model-capabilities-through-the-taxonomy"><span class="header-section-number">7.5</span> Understanding Model Capabilities Through the Taxonomy</a>
  <ul class="collapse">
  <li><a href="#matching-model-families-to-task-requirements" id="toc-matching-model-families-to-task-requirements" class="nav-link" data-scroll-target="#matching-model-families-to-task-requirements"><span class="header-section-number">7.5.1</span> Matching Model Families to Task Requirements</a></li>
  <li><a href="#trade-offs-between-model-families" id="toc-trade-offs-between-model-families" class="nav-link" data-scroll-target="#trade-offs-between-model-families"><span class="header-section-number">7.5.2</span> Trade-offs Between Model Families</a></li>
  <li><a href="#why-multiple-families-persist" id="toc-why-multiple-families-persist" class="nav-link" data-scroll-target="#why-multiple-families-persist"><span class="header-section-number">7.5.3</span> Why Multiple Families Persist</a></li>
  </ul></li>
  <li><a href="#evaluation-beyond-single-benchmarks" id="toc-evaluation-beyond-single-benchmarks" class="nav-link" data-scroll-target="#evaluation-beyond-single-benchmarks"><span class="header-section-number">7.6</span> Evaluation: Beyond Single Benchmarks</a>
  <ul class="collapse">
  <li><a href="#multi-task-assessment-requirements" id="toc-multi-task-assessment-requirements" class="nav-link" data-scroll-target="#multi-task-assessment-requirements"><span class="header-section-number">7.6.1</span> Multi-task Assessment Requirements</a></li>
  <li><a href="#transfer-capability-versus-pretraining-performance" id="toc-transfer-capability-versus-pretraining-performance" class="nav-link" data-scroll-target="#transfer-capability-versus-pretraining-performance"><span class="header-section-number">7.6.2</span> Transfer Capability Versus Pretraining Performance</a></li>
  <li><a href="#robustness-across-domains-and-distributions" id="toc-robustness-across-domains-and-distributions" class="nav-link" data-scroll-target="#robustness-across-domains-and-distributions"><span class="header-section-number">7.6.3</span> Robustness Across Domains and Distributions</a></li>
  <li><a href="#benchmark-suites-and-community-resources" id="toc-benchmark-suites-and-community-resources" class="nav-link" data-scroll-target="#benchmark-suites-and-community-resources"><span class="header-section-number">7.6.4</span> Benchmark Suites and Community Resources</a></li>
  <li><a href="#evaluation-regimes" id="toc-evaluation-regimes" class="nav-link" data-scroll-target="#evaluation-regimes"><span class="header-section-number">7.6.5</span> Evaluation Regimes</a></li>
  </ul></li>
  <li><a href="#the-foundation-model-ecosystem" id="toc-the-foundation-model-ecosystem" class="nav-link" data-scroll-target="#the-foundation-model-ecosystem"><span class="header-section-number">7.7</span> The Foundation Model Ecosystem</a>
  <ul class="collapse">
  <li><a href="#model-hubs-and-distribution" id="toc-model-hubs-and-distribution" class="nav-link" data-scroll-target="#model-hubs-and-distribution"><span class="header-section-number">7.7.1</span> Model Hubs and Distribution</a></li>
  <li><a href="#documentation-and-reproducibility-requirements" id="toc-documentation-and-reproducibility-requirements" class="nav-link" data-scroll-target="#documentation-and-reproducibility-requirements"><span class="header-section-number">7.7.2</span> Documentation and Reproducibility Requirements</a></li>
  <li><a href="#community-benchmarks-and-leaderboards" id="toc-community-benchmarks-and-leaderboards" class="nav-link" data-scroll-target="#community-benchmarks-and-leaderboards"><span class="header-section-number">7.7.3</span> Community Benchmarks and Leaderboards</a></li>
  <li><a href="#industry-versus-academic-models" id="toc-industry-versus-academic-models" class="nav-link" data-scroll-target="#industry-versus-academic-models"><span class="header-section-number">7.7.4</span> Industry Versus Academic Models</a></li>
  </ul></li>
  <li><a href="#open-questions-and-frontiers" id="toc-open-questions-and-frontiers" class="nav-link" data-scroll-target="#open-questions-and-frontiers"><span class="header-section-number">7.8</span> Open Questions and Frontiers</a>
  <ul class="collapse">
  <li><a href="#convergence-or-divergence" id="toc-convergence-or-divergence" class="nav-link" data-scroll-target="#convergence-or-divergence"><span class="header-section-number">7.8.1</span> Convergence or Divergence</a></li>
  <li><a href="#what-genomic-foundation-models-still-cannot-do" id="toc-what-genomic-foundation-models-still-cannot-do" class="nav-link" data-scroll-target="#what-genomic-foundation-models-still-cannot-do"><span class="header-section-number">7.8.2</span> What Genomic Foundation Models Still Cannot Do</a></li>
  <li><a href="#scaling-limits" id="toc-scaling-limits" class="nav-link" data-scroll-target="#scaling-limits"><span class="header-section-number">7.8.3</span> Scaling Limits</a></li>
  <li><a href="#clinical-deployment-readiness" id="toc-clinical-deployment-readiness" class="nav-link" data-scroll-target="#clinical-deployment-readiness"><span class="header-section-number">7.8.4</span> Clinical Deployment Readiness</a></li>
  </ul></li>
  <li><a href="#summary-and-forward-references" id="toc-summary-and-forward-references" class="nav-link" data-scroll-target="#summary-and-forward-references"><span class="header-section-number">7.9</span> Summary and Forward References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part III: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch07-foundation.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-foundation" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="content-visible callout callout-style-default callout-warning callout-empty-content callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>The preceding chapters traced deep learning architectures from convolutional networks for regulatory sequence prediction through recurrent and transformer-based models for variant effect prediction. These architectures established that neural networks could learn genomic patterns directly from sequence, often matching or exceeding expert-crafted features. However, most models operated within narrow task boundaries: DeepSEA predicted chromatin accessibility, SpliceAI predicted splicing outcomes, and Enformer mapped sequences to molecular readouts. Each model solved its specific problem well but offered limited transferability to other genomic questions.</p>
<p>The emergence of foundation models represents a fundamental shift in how we approach computational genomics. Rather than training separate models for each task, foundation models learn general representations that can be adapted to diverse downstream applications. This paradigm follows the transformative success of large language models in natural language processing and protein language models in structural biology, where models pretrained on vast unlabeled corpora have become infrastructure for entire research communities.</p>
<p>Genomics presents unique challenges for the foundation model paradigm. The scale of genomic context necessary to capture distal regulatory interactions far exceeds typical sequence lengths in protein or language modeling. Single-nucleotide resolution is often essential, ruling out aggressive tokenization schemes that work well in other domains. The diversity of genomic tasks spans orders of magnitude, from predicting local chromatin states to estimating polygenic disease risk across populations. Despite these challenges, genomic foundation models have rapidly emerged as practical tools for variant interpretation, regulatory genomics, and complex trait prediction.</p>
<p>This chapter addresses the conceptual landscape of genomic foundation models rather than their implementation details. We define what distinguishes foundation models from task-specific architectures, organize the emerging ecosystem into a practical taxonomy, and establish design principles that guide model selection and development. The framework developed here will inform subsequent chapters as we examine training procedures, deployment strategies, and specific application domains.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Figure suggestion:</strong> A timeline diagram showing the progression from hand-crafted scores (CADD, SIFT circa 2014) through task-specific deep models (DeepSEA 2015, SpliceAI 2019, Enformer 2021) to foundation models (DNABERT 2021, HyenaDNA 2023, NT v2 2023, GROVER 2024). Include parameter counts and context lengths as secondary axes to show the scaling trends.</p>
</div>
</div>
<section id="from-task-specific-models-to-foundation-models" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="from-task-specific-models-to-foundation-models"><span class="header-section-number">7.1</span> From Task-Specific Models to Foundation Models</h2>
<p>The history of computational genomics reveals a consistent pattern: models become more general while maintaining or improving task-specific performance. Hand-crafted scores such as CADD, DANN, and SIFT established that integration of diverse genomic annotations could improve variant pathogenicity prediction <span class="citation" data-cites="rentzsch_cadd_2019 schubach_cadd_2024">(<a href="references.html#ref-rentzsch_cadd_2019" role="doc-biblioref">Rentzsch et al. 2019</a>; <a href="references.html#ref-schubach_cadd_2024" role="doc-biblioref">Schubach et al. 2024</a>)</span>. These approaches relied on expert feature engineering, combining conservation scores, functional annotations, and population frequency data through ensemble methods or logistic regression.</p>
<p>Task-specific deep learning models demonstrated that neural networks could learn relevant features directly from sequence. DeepSEA predicted chromatin accessibility and transcription factor binding from 1 kb sequences using convolutional architectures <span class="citation" data-cites="zhou_deepsea_2015">(<a href="references.html#ref-zhou_deepsea_2015" role="doc-biblioref">J. Zhou and Troyanskaya 2015</a>)</span>. ExPecto extended this approach to gene expression prediction by modeling regulatory elements across multiple cell types <span class="citation" data-cites="zhou_expecto_2018">(<a href="references.html#ref-zhou_expecto_2018" role="doc-biblioref">J. Zhou et al. 2018</a>)</span>. Sei organized regulatory predictions into interpretable sequence classes through unsupervised clustering <span class="citation" data-cites="chen_deepsea_2022">(<a href="references.html#ref-chen_deepsea_2022" role="doc-biblioref">Chen et al. 2022</a>)</span>. SpliceAI achieved near-perfect splice site prediction through dilated convolutions over 10 kb contexts <span class="citation" data-cites="jaganathan_spliceai_2019">(<a href="references.html#ref-jaganathan_spliceai_2019" role="doc-biblioref">Jaganathan et al. 2019</a>)</span>. Enformer scaled sequence-to-function modeling to 200 kb windows and thousands of chromatin tracks through transformer architectures <span class="citation" data-cites="avsec_enformer_2021">(<a href="references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>.</p>
<p>These models succeeded within their specific domains but remained difficult to repurpose. Training a DeepSEA model required chromatin accessibility data. Using SpliceAI for regulatory prediction would require complete retraining on different labels. Each application domain needed its own model, trained from scratch on task-specific data.</p>
<p>Sequence language models introduced self-supervised learning to genomics. DNABERT applied masked language modeling to DNA sequences, demonstrating that general representations could be learned without task-specific labels <span class="citation" data-cites="ji_dnabert_2021">(<a href="references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. ESM and ESM-2 showed that protein language models pretrained on sequence alone could transfer effectively to structure prediction, variant effect prediction, and protein design <span class="citation" data-cites="rives_esm_2021 lin_esm-2_2022">(<a href="references.html#ref-rives_esm_2021" role="doc-biblioref">Rives et al. 2021</a>; <a href="references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. The Nucleotide Transformer family scaled DNA language modeling to cross-species training corpora <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. HyenaDNA used implicit convolutions to reach million-token contexts at single-nucleotide resolution <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>.</p>
<p>True genomic foundation models emerged when models satisfied several criteria simultaneously: pretraining on large-scale genomic data with minimal supervision, production of general-purpose representations useful across diverse tasks, demonstrated transfer capability across assays and tissues and species, and standardized interfaces for embedding extraction and downstream adaptation. These properties distinguish foundation models from earlier approaches. A large Enformer model trained on chromatin data remains task-specific despite its size. A DNA language model trained on reference genomes qualifies as a foundation model even if its parameter count is modest, provided it produces reusable representations.</p>
<p>The shift from task-specific to foundation models changes the relationship between model developers and users. Task-specific models deliver predictions as their primary product. Foundation models deliver representations that users adapt to their own tasks. This distinction affects everything from model architecture design to evaluation strategies to deployment infrastructure.</p>
</section>
<section id="defining-genomic-foundation-models" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="defining-genomic-foundation-models"><span class="header-section-number">7.2</span> Defining Genomic Foundation Models</h2>
<p>The term “foundation model” appears frequently in genomics literature, sometimes applied to any large neural network trained on biological sequences. For practical purposes, establishing working criteria helps separate true genomic foundation models from ordinary deep learning approaches that happen to operate on DNA or protein sequences.</p>
<section id="essential-properties" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="essential-properties"><span class="header-section-number">7.2.1</span> Essential Properties</h3>
<p>A genomic foundation model satisfies several key properties that distinguish it from task-specific architectures.</p>
<p><strong>Large-scale pretraining with minimal supervision.</strong> Foundation models train on entire genomes, pan-genomic sequence collections, or large assay compendia. The pretraining objectives include masked language modeling, next-token prediction, denoising, or multi-task sequence-to-function prediction. Critically, these objectives do not require dense task-specific labels for every training example. A model that requires annotated enhancers or curated pathogenic variants for every training instance does not qualify as a foundation model under this criterion.</p>
<p><strong>General-purpose representations.</strong> Foundation models produce embeddings that prove useful across many downstream tasks. These representations can be extracted through forward passes and reused with simple linear probes or lightweight adapter modules rather than requiring full model retraining. The representations should encode biological information at multiple scales, from local sequence motifs to long-range regulatory grammar.</p>
<p><strong>Broad transfer capability.</strong> Foundation models support diverse downstream applications without architectural modifications or complete retraining. Transfer occurs across multiple dimensions: different assays (from chromatin accessibility to gene expression), different tissues and cell types, different species, and different variant types (from SNVs to structural variants). Evidence of broad transfer requires evaluation across multiple benchmarks rather than demonstration of performance on a single task.</p>
<p><strong>Scale along at least one dimension.</strong> Foundation models operate at a scale that would be impractical for task-specific training. Some scale context length, as HyenaDNA scales to million-token windows at single-nucleotide resolution. Others scale parameter count, as the ESM and Nucleotide Transformer families reach billions of parameters. Still others scale data diversity through pan-genomic pretraining across hundreds of species or integration of many assays and cell types. The scaling dimension chosen reflects the model’s intended applications and architectural constraints.</p>
<p><strong>Standardized interfaces.</strong> Foundation models typically expose consistent APIs for common operations. These include embedding extraction for sequences or variants, sequence probability scoring, and mask-based in-silico mutagenesis for variant effect prediction. Models distributed through repositories such as Hugging Face often include documented recipes for downstream fine-tuning and example notebooks demonstrating common use cases.</p>
</section>
<section id="what-doesnt-count" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="what-doesnt-count"><span class="header-section-number">7.2.2</span> What Doesn’t Count</h3>
<p>Many excellent genomic models fail one or more of these criteria and should not be classified as foundation models. Early versions of DeepSEA trained specifically on chromatin accessibility data from a limited set of cell types lack the generality and standardized interface of foundation models, though later iterations that integrate many assays begin to approach foundation model territory <span class="citation" data-cites="zhou_deepsea_2015">(<a href="references.html#ref-zhou_deepsea_2015" role="doc-biblioref">J. Zhou and Troyanskaya 2015</a>)</span>. SpliceAI predicts splicing outcomes exceptionally well but was designed for that specific task and provides neither general-purpose embeddings nor easy transfer to other genomic prediction problems <span class="citation" data-cites="jaganathan_spliceai_2019">(<a href="references.html#ref-jaganathan_spliceai_2019" role="doc-biblioref">Jaganathan et al. 2019</a>)</span>. Even a very large Enformer-like model trained solely on human chromatin tracks remains bound to its specific prediction interface despite its scale and sophistication <span class="citation" data-cites="avsec_enformer_2021">(<a href="references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>.</p>
<p>The distinction between large models and foundation models matters for several reasons. It affects evaluation strategy, since foundation models must be assessed across families of tasks rather than single benchmarks. It affects integration into existing pipelines, since foundation models serve as feature extractors while task-specific models typically provide end-to-end predictions. It affects how we think about model development, since foundation model training requires different infrastructure and data curation than task-specific supervised learning.</p>
</section>
<section id="why-definition-matters" class="level3" data-number="7.2.3">
<h3 data-number="7.2.3" class="anchored" data-anchor-id="why-definition-matters"><span class="header-section-number">7.2.3</span> Why Definition Matters</h3>
<p>Clear definitions enable meaningful comparisons and guide appropriate use. A practitioner selecting a model for regulatory variant interpretation needs to understand whether a model provides general embeddings that can be adapted to their specific cell type or delivers fixed predictions for a predetermined set of assays. A researcher developing new methods needs to know whether their model should be evaluated on single-task performance or multi-task transfer capability. A clinical laboratory implementing variant interpretation pipelines needs to understand what guarantees about robustness and generalization a model can provide.</p>
<p>The framework established here will guide our taxonomy of genomic foundation models and inform discussions of evaluation strategies and practical deployment.</p>
</section>
</section>
<section id="a-taxonomy-of-genomic-foundation-models" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="a-taxonomy-of-genomic-foundation-models"><span class="header-section-number">7.3</span> A Taxonomy of Genomic Foundation Models</h2>
<p>The landscape of genomic foundation models can be organized into four broad families. Each family exhibits distinct characteristics, characteristic strengths and limitations, and typical application domains. Understanding this taxonomy helps practitioners select appropriate models for their tasks and helps researchers position new contributions within the broader ecosystem.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Figure suggestion:</strong> A 2x2 taxonomy grid organizing genomic foundation models. One axis could represent “sequence-native” versus “annotation-guided” approaches. The other axis could represent “molecular-scale” versus “systems-scale” predictions. The four quadrants would contain: DNA language models (HyenaDNA, DNABERT-2, NT v2, Caduceus, Evo-2, GROVER), sequence-to-function models (Enformer, Borzoi, Sei), variant-centric models (AlphaMissense, ESM-1v, EVE, CADD, Delphi, MIFM), and multi-omic models (Omni-DNA, cross-modal architectures, systems models). Each quadrant should list representative models with brief characterizations.</p>
</div>
</div>
<section id="dna-language-models" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="dna-language-models"><span class="header-section-number">7.3.1</span> DNA Language Models</h3>
<p>DNA language models learn sequence representations from raw nucleotide strings through self-supervised objectives. These models treat DNA as a language to be modeled without explicit functional labels, discovering patterns through statistical regularities in genomic sequence.</p>
<p><strong>Core characteristics.</strong> DNA language models typically use masked language modeling or autoregressive next-token prediction as their pretraining objective. They train on reference genomes or pan-genomic sequence collections spanning multiple species. The resulting models produce per-position or pooled sequence embeddings that can be extracted and used for downstream tasks. Critically, these embeddings are not tied to specific assays or cell types, making them applicable to any task that benefits from general sequence context.</p>
<p><strong>Representative models.</strong> DNABERT and DNABERT-2 apply BERT-style masked language modeling to DNA sequences, using overlapping k-mers as tokens <span class="citation" data-cites="ji_dnabert_2021 zhou_dnabert-2_2024">(<a href="references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>; <a href="references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Z. Zhou et al. 2024</a>)</span>. The Nucleotide Transformer family scales this approach to larger parameter counts and cross-species training, demonstrating improved transfer to diverse downstream tasks <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. HyenaDNA achieves subquadratic complexity through implicit convolutions, enabling context lengths up to one million nucleotides at single-base resolution <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. Caduceus incorporates bidirectional processing and reverse-complement equivariance as architectural inductive biases. Evo-2 combines long-range attention with biological tokenization strategies. GROVER integrates learned BPE-style tokenization with training on regulatory tracks in addition to raw sequence <span class="citation" data-cites="sanabria_grover_2024">(<a href="references.html#ref-sanabria_grover_2024" role="doc-biblioref">Sanabria et al. 2024</a>)</span>.</p>
<p><strong>Strengths.</strong> DNA language models provide truly general representations not bound to specific assays, cell types, or experimental conditions. They can process and generate novel sequences that do not appear in reference genomes, making them suitable for de novo design tasks and synthetic biology applications. Their self-supervised training requires only genome sequences, making them scalable to any species with assembled genomes.</p>
<p><strong>Limitations.</strong> Without explicit functional grounding during pretraining, DNA language models may not capture subtle regulatory patterns that manifest only under specific cellular conditions. Their representations encode sequence patterns but do not directly predict molecular phenotypes. Performance on tasks requiring fine-grained functional discrimination may lag models trained with functional supervision.</p>
<p><strong>Typical applications.</strong> These models excel at tasks where general sequence context matters: sequence classification (identifying promoters, enhancers, transposons), motif discovery and refinement, variant effect prediction through embedding perturbation, sequence generation for synthetic biology, and transfer learning to new species or genomic contexts with limited labeled data.</p>
</section>
<section id="sequence-to-function-foundation-models" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="sequence-to-function-foundation-models"><span class="header-section-number">7.3.2</span> Sequence-to-Function Foundation Models</h3>
<p>Sequence-to-function models predict molecular readouts directly from sequence through supervised or semi-supervised training on assay compendia. These models blur into foundation model territory when their output space is sufficiently broad and their internal representations prove useful for tasks beyond the original assay set.</p>
<p><strong>Core characteristics.</strong> These models map DNA sequences to high-dimensional vectors of molecular measurements, including chromatin accessibility, histone modifications, transcription factor binding, and gene expression levels. Training typically uses large collections of functional genomics assays spanning many cell types and conditions. The models learn regulatory grammar through supervised prediction of molecular phenotypes rather than through self-supervised sequence modeling.</p>
<p><strong>Representative models.</strong> Enformer predicts thousands of chromatin and expression tracks from 200 kb sequence windows through a transformer architecture with attention over long genomic contexts <span class="citation" data-cites="avsec_enformer_2021">(<a href="references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. Borzoi extends this approach with refined architectures and expanded assay coverage. Sei organizes sequence-to-function predictions into interpretable sequence classes through unsupervised clustering, providing a discrete vocabulary for regulatory elements <span class="citation" data-cites="chen_deepsea_2022">(<a href="references.html#ref-chen_deepsea_2022" role="doc-biblioref">Chen et al. 2022</a>)</span>. Earlier models including DeepSEA and Basset established the sequence-to-function paradigm at smaller scales <span class="citation" data-cites="zhou_deepsea_2015">(<a href="references.html#ref-zhou_deepsea_2015" role="doc-biblioref">J. Zhou and Troyanskaya 2015</a>)</span>.</p>
<p><strong>Strengths.</strong> Explicit functional supervision provides strong mechanistic grounding. Predictions can be interpreted through comparison to experimental measurements. The models naturally support variant effect prediction by computing differences between reference and alternative allele predictions. When trained on sufficiently diverse assay collections, internal representations generalize beyond the specific prediction targets.</p>
<p><strong>Limitations.</strong> Models remain tied to the specific assays and cell types present during training. Extending predictions to new cell types typically requires retraining or collection of new data. Very rare cell types or transient cellular states may not be adequately represented in training data. The supervised training paradigm limits scalability compared to self-supervised approaches.</p>
<p><strong>Typical applications.</strong> These models serve well for regulatory variant interpretation in well-studied cell types, expression quantitative trait locus (eQTL) fine-mapping, enhancer identification and characterization, transcription factor binding site prediction, and regulatory mechanism discovery through perturbation analysis.</p>
</section>
<section id="variant-centric-foundation-models" class="level3" data-number="7.3.3">
<h3 data-number="7.3.3" class="anchored" data-anchor-id="variant-centric-foundation-models"><span class="header-section-number">7.3.3</span> Variant-Centric Foundation Models</h3>
<p>A third class of foundation models focuses on genetic variants as the fundamental unit of analysis rather than on raw sequence. These models embed variants using contextual information from local sequence, gene structure, population genetics, and external annotations, then predict variant pathogenicity, molecular consequences, or trait-level effect sizes.</p>
<p><strong>Core characteristics.</strong> Variant-centric models typically integrate information from multiple sources: local sequence context around the variant, conservation and population frequency patterns, protein structural context for coding variants, and functional annotations from databases. They may use genomic foundation models as feature extractors, combining sequence embeddings with variant-specific features. The outputs include pathogenicity scores, effect size estimates, or functional consequence predictions.</p>
<p><strong>Representative models.</strong> AlphaMissense applies protein language models to predict pathogenicity of missense variants across the human proteome <span class="citation" data-cites="cheng_alphamissense_2023">(<a href="references.html#ref-cheng_alphamissense_2023" role="doc-biblioref">Cheng et al. 2023</a>)</span>. ESM-1v uses evolutionary context to predict variant effects on protein function. EVE combines evolutionary and structural information for variant interpretation. CADD and its successors integrate diverse genomic annotations to score deleteriousness of any genetic variant <span class="citation" data-cites="rentzsch_cadd_2019 schubach_cadd_2024">(<a href="references.html#ref-rentzsch_cadd_2019" role="doc-biblioref">Rentzsch et al. 2019</a>; <a href="references.html#ref-schubach_cadd_2024" role="doc-biblioref">Schubach et al. 2024</a>)</span>. Delphi, MIFM, and related models couple genomic foundation model embeddings with polygenic score estimation for complex trait prediction <span class="citation" data-cites="georgantas_delphi_2024 rakowski_mifm_2025 wu_genome-wide_2024">(<a href="references.html#ref-georgantas_delphi_2024" role="doc-biblioref">Georgantas, Kutalik, and Richiardi 2024</a>; <a href="references.html#ref-rakowski_mifm_2025" role="doc-biblioref">Rakowski and Lippert 2025</a>; <a href="references.html#ref-wu_genome-wide_2024" role="doc-biblioref">Wu et al. 2024</a>)</span>.</p>
<p><strong>Strengths.</strong> Direct focus on variants aligns naturally with clinical applications and genetic association studies. Integration of multiple information sources provides robust predictions. Models can provide calibrated uncertainty estimates through ensemble approaches or variational methods. The variant-level interface simplifies integration into existing genetic analysis pipelines.</p>
<p><strong>Limitations.</strong> Most current models focus on single-nucleotide variants, with limited coverage of insertions, deletions, and structural variants. Predictions may be biased toward well-studied regions of the genome or commonly occurring variant types. The integration of multiple data sources creates dependencies on external databases that may not be uniformly available or accurate.</p>
<p><strong>Typical applications.</strong> Clinical variant interpretation for rare disease diagnosis, polygenic risk score construction for complex traits, prioritization of variants in genome-wide association studies, therapeutic target identification through variant effect prediction, and stratification of patient cohorts by genetic risk.</p>
</section>
<section id="multi-omic-foundation-models" class="level3" data-number="7.3.4">
<h3 data-number="7.3.4" class="anchored" data-anchor-id="multi-omic-foundation-models"><span class="header-section-number">7.3.4</span> Multi-Omic Foundation Models</h3>
<p>The fourth category comprises models that natively integrate multiple molecular modalities. These models jointly process DNA sequence, chromatin state, gene expression, protein abundance, 3D genome structure, or even phenotypic descriptions, learning representations that span traditional boundaries between genomics, transcriptomics, and other omics layers.</p>
<p><strong>Core characteristics.</strong> Multi-omic models employ architectures that can handle heterogeneous input types, including transformer variants with cross-attention mechanisms, graph neural networks over molecular interaction networks, or modality-specific encoders combined through fusion layers. Training objectives encourage alignment across modalities through contrastive learning, joint prediction tasks, or generative modeling of multiple data types simultaneously.</p>
<p><strong>Representative models.</strong> Omni-DNA explores transformer-based autoregressive models that jointly handle DNA sequence and task-specific tokens representing molecular measurements <span class="citation" data-cites="li_omnidna_2025">(<a href="references.html#ref-li_omnidna_2025" role="doc-biblioref">Li et al. 2025</a>)</span>. Models integrating Hi-C or Micro-C data with sequence information capture 3D genome organization. Cross-modal architectures align DNA embeddings with chromatin state predictions or gene expression measurements through contrastive objectives. Systems-level models incorporate pathway annotations or gene ontology terms to constrain learned representations toward biologically meaningful subspaces.</p>
<p><strong>Strengths.</strong> Unified representations enable cross-modal queries such as “predict expression changes from a sequence variant” or “identify variants that affect chromatin organization in specific cell types.” Joint training can improve performance on individual modalities through multi-task learning effects. Mechanistic relationships between molecular layers can emerge naturally from data rather than requiring explicit modeling.</p>
<p><strong>Limitations.</strong> Data engineering becomes substantially more complex. Different molecular modalities require different measurement technologies, temporal sampling strategies, and quality control procedures. Training objectives must balance multiple tasks with potentially conflicting gradients. Computational requirements scale with the number of modalities and the complexity of cross-modal interactions. The field is still early, with few widely adopted models reaching production maturity.</p>
<p><strong>Typical applications.</strong> Systems biology investigations of disease mechanisms, drug discovery through multi-target effect prediction, cellular state modeling and trajectory inference, integration of genetic and environmental effects on phenotypes, and mechanistic modeling of regulatory networks.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Table suggestion:</strong> A comparison table with rows for the four model families and columns for: Context Length (typical range), Parameter Count (order of magnitude), Pretraining Objective (primary approach), Key Strengths (2-3 bullet points), Primary Limitations (2-3 bullet points), and Representative Applications (3-4 specific use cases). This would provide practitioners a quick reference for family characteristics and trade-offs.</p>
</div>
</div>
</section>
</section>
<section id="design-dimensions-a-framework-for-understanding-models" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="design-dimensions-a-framework-for-understanding-models"><span class="header-section-number">7.4</span> Design Dimensions: A Framework for Understanding Models</h2>
<p>Within and across the four families of genomic foundation models, individual models differ along several orthogonal design dimensions. Understanding these dimensions helps practitioners evaluate model suitability for specific tasks and helps researchers position new architectures within the design space.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Figure suggestion:</strong> A multi-axis diagram showing four orthogonal design dimensions: Data (species coverage, assay diversity, population diversity), Architecture (transformer/CNN/Hyena, attention patterns, parameter scaling), Objectives (MLM, autoregressive, multi-task, contrastive), and Tokenization (character-level, k-mer, learned BPE, resolution). Each axis should show concrete examples of choices (e.g., “1 species” vs “50+ species” on the data axis, “100M params” vs “10B params” on the architecture axis). The diagram should convey that models can make independent choices along each dimension.</p>
</div>
</div>
<section id="data-composition" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="data-composition"><span class="header-section-number">7.4.1</span> Data Composition</h3>
<p>The choice of training data fundamentally shapes what patterns a model can learn and how well it generalizes beyond its training distribution.</p>
<p><strong>Species coverage.</strong> Models trained exclusively on human genomes focus on patterns relevant to human genetics and clinical applications. Cross-species training, as employed by Nucleotide Transformer and many protein language models, encourages learning of conserved regulatory elements and evolutionary constraints <span class="citation" data-cites="dalla-torre_nucleotide_2023 rives_esm_2021">(<a href="references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>; <a href="references.html#ref-rives_esm_2021" role="doc-biblioref">Rives et al. 2021</a>)</span>. Pan-genomic approaches that incorporate multiple species may improve out-of-domain generalization but risk diluting human-specific signals relevant for clinical applications.</p>
<p><strong>Sequence diversity.</strong> Training on reference genomes alone provides clean sequences but limited exposure to population-level variation. Incorporating sequences from diverse populations and variant databases improves robustness to common genetic variation. However, variant-augmented training requires careful design to avoid learning spurious associations between neutral variants and functional effects. Some models sample alleles from population databases to augment reference sequences during training, while others train on multiple reference assemblies when available.</p>
<p><strong>Annotation integration.</strong> Models may train on raw sequence alone or may incorporate functional annotations as additional input channels or auxiliary prediction targets. DeepSEA-style models predict chromatin accessibility and transcription factor binding from sequence, effectively using these annotations as labels <span class="citation" data-cites="zhou_deepsea_2015">(<a href="references.html#ref-zhou_deepsea_2015" role="doc-biblioref">J. Zhou and Troyanskaya 2015</a>)</span>. Language models like GROVER integrate regulatory track information during pretraining rather than treating it as a downstream task <span class="citation" data-cites="sanabria_grover_2024">(<a href="references.html#ref-sanabria_grover_2024" role="doc-biblioref">Sanabria et al. 2024</a>)</span>. The degree of annotation integration trades off between generality (raw sequence only) and functional grounding (annotation-aware training).</p>
<p><strong>Scale.</strong> The quantity of training data varies from single reference genomes (approximately 3 billion bases for human) to pan-genomic collections spanning hundreds of species and trillions of bases. For sequence-to-function models, scale also includes the number of assays, cell types, and experimental conditions represented. Larger and more diverse training data generally improves generalization, but with diminishing returns beyond a certain scale and potential concerns about data quality degradation when scraping very large corpora.</p>
</section>
<section id="architecture-families" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="architecture-families"><span class="header-section-number">7.4.2</span> Architecture Families</h3>
<p>Architectural choices determine computational properties including maximum context length, memory requirements, training efficiency, and ease of adaptation to downstream tasks.</p>
<p><strong>Transformer architectures.</strong> Transformers dominate current genomic foundation models and come in several variants. Encoder-only models following the BERT design, such as DNABERT and Nucleotide Transformer, excel at classification and embedding tasks. They process sequences bidirectionally through masked language modeling objectives. Decoder-only models following the GPT design, including GROVER and some Omni-DNA variants, use autoregressive prediction and naturally support generative tasks. Encoder-decoder architectures combine bidirectional context encoding with flexible output generation, useful for tasks like sequence-to-text explanation or structure-guided design.</p>
<p>The attention mechanism itself varies across implementations. Full dense attention provides exact computation of all pairwise interactions but scales quadratically with sequence length. Sparse attention patterns, including local windows and strided patterns, reduce complexity while maintaining long-range modeling capacity. Linear attention approximations trade exact computation for reduced asymptotic complexity. Flash attention and other algorithmic optimizations improve memory efficiency and speed without changing model behavior.</p>
<p><strong>Sub-quadratic long-range models.</strong> Attention-free architectures address the quadratic complexity bottleneck directly. Hyena-based models like HyenaDNA use implicit convolutions parameterized by small neural networks to achieve subquadratic scaling <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. State space models including Mamba and related architectures process sequences recurrently with linear complexity. These approaches enable much longer contexts than standard transformers with comparable parameter counts.</p>
<p><strong>Hybrid architectures.</strong> Many successful models combine multiple architectural components. CNN-transformer hybrids use local convolutions followed by global attention, as seen in Enformer and related models <span class="citation" data-cites="avsec_enformer_2021">(<a href="references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. Multi-scale approaches process sequences at multiple resolutions, using dilated convolutions or hierarchical attention patterns. Cross-attention mechanisms integrate information from multiple input modalities, such as sequence and chromatin state or DNA and protein sequence.</p>
<p><strong>Parameter scaling.</strong> Genomic foundation models range from approximately 100 million parameters at the small end to over 10 billion parameters for the largest models. Larger models generally achieve better performance on downstream tasks, but with significantly higher computational costs for training and inference. The optimal scale depends on available training data, computational budget, and deployment constraints. Recent work suggests that smaller, more carefully trained models can approach or match the performance of larger models on many tasks.</p>
</section>
<section id="context-length-capabilities" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="context-length-capabilities"><span class="header-section-number">7.4.3</span> Context Length Capabilities</h3>
<p>The genomic context a model can process constrains which biological phenomena it can capture and which applications it can serve effectively.</p>
<p><strong>Short context (under 1 kb).</strong> Models with kilobase-scale contexts capture local sequence patterns including transcription factor binding motifs, splice sites, and promoter elements. These models work well for tasks focused on local regulatory logic but cannot capture distal enhancer-promoter interactions or chromatin domain structure. Most early deep learning models for genomics operated at this scale due to computational constraints.</p>
<p><strong>Medium context (1-10 kb).</strong> At this scale, models capture complete genes with their proximal regulatory regions, local regulatory grammar involving multiple nearby elements, and some distal interactions within topologically associating domains. Many current models including DNABERT-2 and standard transformer implementations reach contexts in this range. This scale balances biological relevance with computational tractability for many applications.</p>
<p><strong>Long context (10-200 kb).</strong> Models reaching this scale can represent distal enhancer-promoter interactions, topologically associating domains (TADs) and their internal structure, and multi-gene regulatory clusters. Enformer operates at 200 kb context, enabling prediction of long-range regulatory effects <span class="citation" data-cites="avsec_enformer_2021">(<a href="references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. This scale is particularly relevant for understanding complex regulatory regions such as gene-dense loci and super-enhancers.</p>
<p><strong>Ultra-long context (over 200 kb).</strong> A few models extend beyond 200 kb, with HyenaDNA reaching up to one million nucleotides through its sub-quadratic architecture <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. At this scale, models can capture chromosomal domains, multi-megabase structural variants, and complex haplotype structure. Applications include structural variant interpretation, haplotype-aware variant effect prediction, and modeling of very long-range regulatory interactions.</p>
<p>The effective use of long context requires careful consideration of tokenization strategy and positional encoding schemes, as discussed below.</p>
</section>
<section id="tokenization-strategies" class="level3" data-number="7.4.4">
<h3 data-number="7.4.4" class="anchored" data-anchor-id="tokenization-strategies"><span class="header-section-number">7.4.4</span> Tokenization Strategies</h3>
<p>How sequences are discretized into tokens affects model capacity, context length, positional resolution, and computational efficiency. This dimension interacts strongly with architecture choice and context requirements.</p>
<p><strong>Character-level tokenization.</strong> Treating each nucleotide as a separate token maintains single-base resolution and makes no assumptions about relevant sequence units. HyenaDNA and many sequence-to-function models use this approach <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. Character-level tokenization provides maximum flexibility for variant effect prediction and precise regulatory element mapping. However, it imposes the longest sequence lengths, requiring efficient architectures or aggressive downsampling for long-range modeling.</p>
<p><strong>K-mer tokenization.</strong> Grouping nucleotides into overlapping or non-overlapping k-mers reduces sequence length by a factor approaching k. For 6-mers, the vocabulary size reaches 4,096 tokens. DNABERT uses overlapping 6-mers, allowing the model to reach longer effective contexts within transformer attention limits <span class="citation" data-cites="ji_dnabert_2021">(<a href="references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. K-mer approaches introduce positional ambiguity at word boundaries and may not align naturally with biological units, but they often improve performance through implicit modeling of short motifs.</p>
<p><strong>Learned tokenization.</strong> Byte-pair encoding (BPE) and related methods discover tokenization schemes from data rather than using fixed vocabularies. Models like GROVER explore learned tokenization for DNA, potentially allocating vocabulary capacity more efficiently than k-mer schemes <span class="citation" data-cites="sanabria_grover_2024">(<a href="references.html#ref-sanabria_grover_2024" role="doc-biblioref">Sanabria et al. 2024</a>)</span>. Recent work with BioToken demonstrates that learned tokenizers can improve downstream task performance compared to fixed k-mer schemes <span class="citation" data-cites="medvedev_biotoken_2025">(<a href="references.html#ref-medvedev_biotoken_2025" role="doc-biblioref">Medvedev et al. 2025</a>)</span>. However, learned tokenization complicates interpretation and may not transfer well across domains or species.</p>
<p><strong>Hierarchical and multi-resolution approaches.</strong> Some models use different tokenization schemes at different architectural layers. For example, early layers might operate on character-level tokens to capture fine-grained patterns, while later layers use learned pooling to reduce sequence length and expand receptive fields. This approach attempts to combine the resolution benefits of character-level encoding with the efficiency of coarser tokenization.</p>
<p>The choice of tokenization strategy should align with both the architecture’s computational constraints and the biological scales relevant to downstream applications. Single-nucleotide variant effect prediction strongly favors character-level or fine-grained tokenization. Expression prediction or chromatin state modeling may benefit from coarser tokenization that implicitly captures regulatory motifs.</p>
</section>
</section>
<section id="understanding-model-capabilities-through-the-taxonomy" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="understanding-model-capabilities-through-the-taxonomy"><span class="header-section-number">7.5</span> Understanding Model Capabilities Through the Taxonomy</h2>
<p>The taxonomy and design dimensions established above provide a framework for matching models to specific genomic analysis tasks. Different families excel at different applications, and no single model dominates across all use cases.</p>
<section id="matching-model-families-to-task-requirements" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="matching-model-families-to-task-requirements"><span class="header-section-number">7.5.1</span> Matching Model Families to Task Requirements</h3>
<p><strong>Novel sequence analysis.</strong> When working with sequences not present in reference genomes, such as synthetic constructs, pathogen genomes, or rare structural variants, DNA language models provide the most flexibility. Their self-supervised training on general sequence distributions transfers well to novel contexts without requiring task-specific labels. Models like HyenaDNA and Nucleotide Transformer can process and embed arbitrary sequences, making them suitable for de novo design and synthetic biology applications.</p>
<p><strong>Regulatory prediction in well-studied contexts.</strong> For predicting chromatin accessibility, transcription factor binding, or gene expression in cell types well-represented in training data, sequence-to-function models offer strong baselines. Enformer-style models directly output predictions for relevant assays and can be queried with specific genomic loci <span class="citation" data-cites="avsec_enformer_2021">(<a href="references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. The mechanistic grounding from functional supervision often produces more interpretable predictions than language model embeddings.</p>
<p><strong>Clinical variant interpretation.</strong> Variant-centric models like AlphaMissense and ESM-1v provide pathogenicity scores calibrated for clinical use <span class="citation" data-cites="cheng_alphamissense_2023">(<a href="references.html#ref-cheng_alphamissense_2023" role="doc-biblioref">Cheng et al. 2023</a>)</span>. These models integrate evolutionary context, population frequency, and protein structural information in ways optimized for distinguishing pathogenic from benign variants. For missense variant interpretation specifically, protein language models outperform DNA-based approaches on most benchmarks.</p>
<p><strong>Systems-level questions.</strong> Tasks requiring integration of multiple molecular layers, such as predicting phenotypic effects of regulatory variants on downstream pathways, benefit from multi-omic models. These models can jointly reason about DNA sequence, gene expression, and pathway activation. However, the relative immaturity of this model family means that practitioners often need to combine predictions from separate DNA and expression models rather than using a unified multi-omic architecture.</p>
</section>
<section id="trade-offs-between-model-families" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="trade-offs-between-model-families"><span class="header-section-number">7.5.2</span> Trade-offs Between Model Families</h3>
<p>The taxonomy reveals consistent trade-offs that guide model selection and development priorities.</p>
<p><strong>Generality versus specificity.</strong> DNA language models provide the most general representations but may underperform specialized models on specific tasks. Variant-centric models achieve excellent performance on pathogenicity prediction but offer limited utility for other genomic questions. Sequence-to-function models occupy a middle ground, specialized for regulatory prediction but still useful for related tasks like enhancer design.</p>
<p><strong>Mechanistic grounding versus flexibility.</strong> Models trained with functional supervision (sequence-to-function and some variant-centric models) produce predictions that align with known biology and can be validated against experimental measurements. Self-supervised language models learn representations from sequence statistics without mechanistic constraints, providing greater flexibility but potentially missing important biological patterns that require specific cellular contexts to manifest.</p>
<p><strong>Single-task performance versus transfer breadth.</strong> Task-specific fine-tuning of foundation models typically achieves the best performance on individual benchmarks. However, fully fine-tuned models lose some transfer capability, requiring separate models for each new task. Lightweight adaptation through linear probes or low-rank fine-tuning preserves more general knowledge while accepting some performance loss.</p>
</section>
<section id="why-multiple-families-persist" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="why-multiple-families-persist"><span class="header-section-number">7.5.3</span> Why Multiple Families Persist</h3>
<p>The existence of four distinct families of genomic foundation models reflects fundamental differences in their intended use cases and biological scope. DNA language models emphasize generality and novel sequence handling. Sequence-to-function models prioritize mechanistic grounding and interpretability. Variant-centric models optimize for clinical decision-making. Multi-omic models aim for systems-level integration.</p>
<p>This specialization suggests that a single universal genomic foundation model may not emerge soon. Different applications have genuinely different requirements regarding context length, resolution, functional grounding, and interpretability. The field may converge on a small set of architectural patterns rather than a single dominant approach, similar to how natural language processing maintains distinct model families for different tasks despite the success of large language models.</p>
</section>
</section>
<section id="evaluation-beyond-single-benchmarks" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="evaluation-beyond-single-benchmarks"><span class="header-section-number">7.6</span> Evaluation: Beyond Single Benchmarks</h2>
<p>Foundation models by their nature resist evaluation on single tasks. Their value lies in transfer and reuse across many applications, making comprehensive evaluation substantially more complex than benchmarking task-specific models.</p>
<section id="multi-task-assessment-requirements" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="multi-task-assessment-requirements"><span class="header-section-number">7.6.1</span> Multi-task Assessment Requirements</h3>
<p>A genomic foundation model should be evaluated across families of related tasks rather than on isolated benchmarks. For DNA language models, this includes sequence classification tasks (promoter identification, enhancer detection, repeat classification), variant effect prediction across multiple variant types, transcription factor motif discovery, and transfer to non-human species when trained on cross-species corpora.</p>
<p>For sequence-to-function models, evaluation should span prediction of held-out assays, transfer to novel cell types or tissues, accuracy on regulatory variant effect prediction, and consistency between predicted and experimentally measured effects. Variant-centric models require assessment across coding and non-coding variants, calibration analysis for clinical thresholds, performance stratification by population ancestry, and comparison against existing clinical interpretation guidelines.</p>
<p>The diversity of evaluation tasks complicates comparison across models. A model that excels at promoter classification may underperform on eQTL fine-mapping. Direct comparisons require controlling for differences in training data, model scale, and evaluation protocols.</p>
</section>
<section id="transfer-capability-versus-pretraining-performance" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="transfer-capability-versus-pretraining-performance"><span class="header-section-number">7.6.2</span> Transfer Capability Versus Pretraining Performance</h3>
<p>Foundation models are intended for transfer, making performance on pretraining objectives only moderately predictive of downstream utility. A model with slightly worse masked language modeling loss may produce better embeddings for downstream classification if the loss function better aligns with useful representations. Conversely, a model that achieves very low pretraining loss through memorization may transfer poorly to novel sequences or tasks.</p>
<p>Evaluation should explicitly test transfer capability through several mechanisms:</p>
<ul>
<li><strong>Zero-shot performance</strong> using frozen embeddings with no task-specific training</li>
<li><strong>Few-shot learning</strong> with minimal labeled examples (10-100 per class)</li>
<li><strong>Cross-domain transfer</strong> from training to evaluation on different sequence types, species, or assay modalities</li>
<li><strong>Robustness to distribution shift</strong> including population variation, sequencing artifacts, and batch effects</li>
</ul>
<p>These evaluations reveal whether a model has learned general principles of genomic organization or has simply memorized patterns in its training data.</p>
</section>
<section id="robustness-across-domains-and-distributions" class="level3" data-number="7.6.3">
<h3 data-number="7.6.3" class="anchored" data-anchor-id="robustness-across-domains-and-distributions"><span class="header-section-number">7.6.3</span> Robustness Across Domains and Distributions</h3>
<p>Genomic foundation models deployed in clinical or research settings encounter sequences, variants, and contexts not represented in training data. Evaluation should probe robustness to various distribution shifts:</p>
<ul>
<li><strong>Population diversity:</strong> Performance stratified by genetic ancestry</li>
<li><strong>Sequencing technology:</strong> Consistency across Illumina, PacBio, and Nanopore data</li>
<li><strong>Assembly quality:</strong> Degradation when applied to draft genomes versus finished assemblies</li>
<li><strong>Rare variants:</strong> Calibration for very low-frequency or singleton variants</li>
<li><strong>Structural variants:</strong> Handling of insertions, deletions, inversions beyond single-nucleotide changes</li>
</ul>
<p>Models showing strong performance on reference genome benchmarks may fail on real-world data if they have learned spurious correlations with assembly artifacts or population-specific patterns.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Figure suggestion:</strong> An evaluation pyramid with four tiers. The base tier represents molecular readouts (chromatin accessibility, TF binding, RNA expression). The second tier shows functional predictions (regulatory element annotation, variant effect scores). The third tier represents cellular phenotypes (cell state classification, differentiation trajectories). The apex shows organismal and clinical outcomes (disease risk, drug response, organismal fitness). Each tier should indicate representative evaluation datasets and the strength of validation evidence typically available (strong at the base, weaker toward the apex). Arrows connecting tiers illustrate that robust validation requires accumulation of evidence across multiple levels.</p>
</div>
</div>
</section>
<section id="benchmark-suites-and-community-resources" class="level3" data-number="7.6.4">
<h3 data-number="7.6.4" class="anchored" data-anchor-id="benchmark-suites-and-community-resources"><span class="header-section-number">7.6.4</span> Benchmark Suites and Community Resources</h3>
<p>Several standardized benchmark suites enable systematic comparison of genomic foundation models.</p>
<p><strong>ProteinGym</strong> evaluates variant effect prediction across thousands of proteins through deep mutational scanning data <span class="citation" data-cites="notin_proteingym_2023">(<a href="references.html#ref-notin_proteingym_2023" role="doc-biblioref">Notin et al. 2023</a>)</span>. The benchmark includes multiple protein families and variant types, enabling assessment of transfer across proteins and mechanistic understanding of mutation effects.</p>
<p><strong>TraitGym</strong> assesses genomic foundation models on complex trait prediction tasks <span class="citation" data-cites="benegas_traitgym_2025">(<a href="references.html#ref-benegas_traitgym_2025" role="doc-biblioref">Benegas, Eraslan, and Song 2025</a>)</span>. The benchmark spans quantitative traits, binary outcomes, and disease phenotypes, testing models’ ability to integrate regulatory information for polygenic prediction.</p>
<p><strong>Nucleotide Transformer benchmarks</strong> provide diverse DNA-level tasks including regulatory element classification, enhancer-promoter linking, and transcript abundance prediction across cell types <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. These benchmarks explicitly compare transformer-based DNA foundation models.</p>
<p><strong>Comparative evaluations</strong> such as the recent comparison of five DNA foundation models (DNABERT-2, Nucleotide Transformer V2, HyenaDNA, Caduceus-Ph, GROVER) across classification, expression prediction, variant effect, and TAD recognition tasks reveal that no single model dominates all benchmarks <span class="citation" data-cites="manzo_comparative_2025">(<a href="references.html#ref-manzo_comparative_2025" role="doc-biblioref">Manzo, Borkowski, and Ovcharenko 2025</a>)</span>. Such studies establish performance bands and identify model-specific strengths.</p>
<p><strong>GV-Rep and variant representation benchmarks</strong> explicitly test how well genomic foundation models represent genetic variants and clinical contexts. These resources fill a gap in evaluation infrastructure by focusing on the variant representation layer rather than end-to-end task performance.</p>
<p>Task-specific datasets remain relevant for focused applications. ClinVar provides ground-truth pathogenicity labels for clinical variant interpretation <span class="citation" data-cites="landrum_clinvar_2018">(<a href="references.html#ref-landrum_clinvar_2018" role="doc-biblioref">Landrum et al. 2018</a>)</span>. ENCODE and Cistrome supply functional genomics data for regulatory prediction <span class="citation" data-cites="kagda_encode_2025 zheng_cistrome_2019">(<a href="references.html#ref-kagda_encode_2025" role="doc-biblioref">Kagda et al. 2025</a>; <a href="references.html#ref-zheng_cistrome_2019" role="doc-biblioref">Zheng et al. 2019</a>)</span>. GTEx enables eQTL-based validation of expression prediction models <span class="citation" data-cites="gtex_2020">(<a href="references.html#ref-gtex_2020" role="doc-biblioref">The GTEx Consortium 2020</a>)</span>. Combining standardized benchmarks with domain-specific validation provides the most complete assessment of model capabilities.</p>
</section>
<section id="evaluation-regimes" class="level3" data-number="7.6.5">
<h3 data-number="7.6.5" class="anchored" data-anchor-id="evaluation-regimes"><span class="header-section-number">7.6.5</span> Evaluation Regimes</h3>
<p>Genomic foundation models can be evaluated in several distinct regimes that test different aspects of utility and deployment readiness.</p>
<p><strong>Zero-shot evaluation</strong> uses frozen model embeddings with no task-specific training. This tests whether useful information is directly accessible through simple operations like cosine similarity or k-nearest neighbors. Zero-shot evaluation provides a lower bound on model utility and can reveal whether representations encode biologically meaningful structure.</p>
<p><strong>Linear probe evaluation</strong> trains shallow linear or logistic regression classifiers on frozen embeddings. This regime tests whether relevant information is linearly separable in the model’s representation space, providing a diagnostic for representation quality independent of downstream model complexity. Linear probes are computationally inexpensive and robust to overfitting on small labeled datasets.</p>
<p><strong>Lightweight adaptation</strong> includes approaches like low-rank adaptation (LoRA), prompt tuning, or small MLP heads trained on frozen or partially frozen backbone models. These methods balance performance with computational cost and stability. They enable task-specific tuning without the full computational expense of end-to-end fine-tuning and with reduced risk of catastrophic forgetting.</p>
<p><strong>Full fine-tuning</strong> updates all model parameters on downstream tasks. This typically yields the best single-task performance but requires more labeled data and computation. Fine-tuning risks overfitting to narrow task distributions and losing general knowledge acquired during pretraining. Full fine-tuning evaluation establishes performance ceilings but may not reflect practical deployment constraints.</p>
<p><strong>Adapter ablation studies</strong> that compare these evaluation regimes on the same tasks reveal how much task-specific information must be learned versus how much can be extracted from pretrained representations. Large gaps between linear probe and fine-tuned performance suggest that relevant information is present but not easily accessible. Small gaps indicate that pretrained representations already encode task-relevant structure.</p>
</section>
</section>
<section id="the-foundation-model-ecosystem" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="the-foundation-model-ecosystem"><span class="header-section-number">7.7</span> The Foundation Model Ecosystem</h2>
<p>Genomic foundation models exist within a broader ecosystem of infrastructure, community resources, and shared practices that enable their development, distribution, and application.</p>
<section id="model-hubs-and-distribution" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="model-hubs-and-distribution"><span class="header-section-number">7.7.1</span> Model Hubs and Distribution</h3>
<p>Most genomic foundation models are distributed through centralized repositories that provide standardized interfaces and documentation. Hugging Face hosts many DNA and protein language models with documented APIs for embedding extraction, tokenization, and fine-tuning. GitHub repositories often accompany publications, providing model weights, training code, and example notebooks. Some models are distributed through domain-specific platforms like ProteinGym or integrated into larger software ecosystems.</p>
<p>Standardized distribution formats reduce friction in model adoption. Models packaged with consistent APIs can be swapped easily in downstream pipelines, enabling rapid benchmarking and experimentation. However, inconsistent documentation, incomplete training details, and missing preprocessing code remain common challenges in reproducing published results.</p>
</section>
<section id="documentation-and-reproducibility-requirements" class="level3" data-number="7.7.2">
<h3 data-number="7.7.2" class="anchored" data-anchor-id="documentation-and-reproducibility-requirements"><span class="header-section-number">7.7.2</span> Documentation and Reproducibility Requirements</h3>
<p>Responsible distribution of genomic foundation models requires comprehensive documentation covering training data provenance, preprocessing procedures, model architecture details, training hyperparameters, evaluation protocols, and known limitations or failure modes.</p>
<p>Data provenance is particularly important given that training data may include cohort-level genomic datasets with specific use restrictions or population-specific biases. Models trained on data from predominantly European ancestries should document this limitation. Models incorporating clinical annotations should clarify whether those annotations have been validated or are computationally predicted.</p>
<p>Reproducibility remains an active challenge. Many published foundation models cannot be retrained from scratch due to computational costs, proprietary data, or incomplete method descriptions. Standardized reporting guidelines analogous to those in machine learning conferences may improve reproducibility in genomic foundation model publications.</p>
</section>
<section id="community-benchmarks-and-leaderboards" class="level3" data-number="7.7.3">
<h3 data-number="7.7.3" class="anchored" data-anchor-id="community-benchmarks-and-leaderboards"><span class="header-section-number">7.7.3</span> Community Benchmarks and Leaderboards</h3>
<p>Public leaderboards for genomic tasks encourage model development but also risk overfitting to specific benchmark distributions. ProteinGym and TraitGym provide test sets with held-out targets to mitigate this concern <span class="citation" data-cites="notin_proteingym_2023 benegas_traitgym_2025">(<a href="references.html#ref-notin_proteingym_2023" role="doc-biblioref">Notin et al. 2023</a>; <a href="references.html#ref-benegas_traitgym_2025" role="doc-biblioref">Benegas, Eraslan, and Song 2025</a>)</span>. Community challenges such as CAGI (Critical Assessment of Genome Interpretation) enable head-to-head comparison of variant interpretation methods including foundation model approaches.</p>
<p>Leaderboards work best when they capture diverse evaluation criteria rather than single metrics. Rankings should consider computational efficiency, calibration quality, robustness across populations, and interpretability in addition to raw accuracy. Multi-objective evaluation prevents optimization for narrow benchmark performance at the expense of practical utility.</p>
</section>
<section id="industry-versus-academic-models" class="level3" data-number="7.7.4">
<h3 data-number="7.7.4" class="anchored" data-anchor-id="industry-versus-academic-models"><span class="header-section-number">7.7.4</span> Industry Versus Academic Models</h3>
<p>Genomic foundation models are developed by both academic research groups and industry. Academic models typically emphasize reproducibility and open access, with model weights and training code released publicly. Industry models may offer superior performance through access to proprietary data or computational resources but with limited transparency about training procedures and restricted access.</p>
<p>Notable industry contributions include NVIDIA’s BioNeMo platform, which provides optimized implementations of genomic foundation models for efficient inference, and Microsoft’s integration of genomic models into Azure cloud infrastructure. Some models occupy a middle ground, with academic groups partnering with industry for computational resources while maintaining open publication and model release.</p>
<p>Licensing terms vary. Most academic models use permissive open-source licenses allowing commercial use. Some models restrict commercial applications or require citation. Users should review license terms before deploying models in commercial or clinical settings.</p>
</section>
</section>
<section id="open-questions-and-frontiers" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="open-questions-and-frontiers"><span class="header-section-number">7.8</span> Open Questions and Frontiers</h2>
<p>Despite rapid progress, genomic foundation models face several fundamental challenges that represent important directions for future research.</p>
<section id="convergence-or-divergence" class="level3" data-number="7.8.1">
<h3 data-number="7.8.1" class="anchored" data-anchor-id="convergence-or-divergence"><span class="header-section-number">7.8.1</span> Convergence or Divergence</h3>
<p>An open question is whether the field will converge toward a small number of unified architectures or whether specialized model families will persist. In natural language processing, decoder-only transformers have largely unified the field despite earlier diversity of architectures. In genomics, the diversity of sequence lengths, resolution requirements, and functional contexts may preclude such convergence.</p>
<p>A unified model would need to handle sequences from single nucleotides to megabases, operate on raw sequence and functional annotations, transfer across species and cell types, and serve both mechanistic modeling and predictive tasks. Whether such generality is achievable or desirable remains unclear. Specialized models may continue to outperform unified approaches on their target applications, suggesting that moderate architectural diversity could persist.</p>
</section>
<section id="what-genomic-foundation-models-still-cannot-do" class="level3" data-number="7.8.2">
<h3 data-number="7.8.2" class="anchored" data-anchor-id="what-genomic-foundation-models-still-cannot-do"><span class="header-section-number">7.8.2</span> What Genomic Foundation Models Still Cannot Do</h3>
<p>Current genomic foundation models have significant limitations that constrain their applicability.</p>
<p><strong>Causal reasoning.</strong> Existing models learn correlations in training data but do not distinguish causal from spurious relationships. A model might learn that certain motifs correlate with gene expression in training data due to cell-type-specific confounding rather than direct regulatory function. Integrating causal structure into model training or inference could improve robustness to distribution shifts and enable counterfactual reasoning about perturbations.</p>
<p><strong>Mechanistic structure.</strong> Most models treat regulatory effects as black-box functions of sequence without imposing mechanistic constraints from biochemistry or physics. Hybrid approaches that combine neural network flexibility with mechanistic models of transcription factor binding, chromatin remodeling, or RNA folding may improve interpretability and generalization.</p>
<p><strong>Rare variant interpretation.</strong> Foundation models trained on reference genomes and common variants may not calibrate well for ultra-rare or de novo variants. Improved integration of protein structure constraints, evolutionary information, and functional assay data could strengthen rare variant interpretation.</p>
<p><strong>Long-range and structural variants.</strong> While models like HyenaDNA demonstrate feasibility of very long contexts, most models do not adequately handle complex structural variants involving inversions, duplications, or translocations <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. Better integration of sequence topology and copy number could improve structural variant interpretation.</p>
<p><strong>Temporal and dynamic regulation.</strong> Current models produce static predictions that do not account for temporal dynamics of gene regulation, developmental trajectories, or response to environmental perturbations. Incorporating temporal information through sequential modeling or dynamical systems could enable prediction of regulatory dynamics.</p>
</section>
<section id="scaling-limits" class="level3" data-number="7.8.3">
<h3 data-number="7.8.3" class="anchored" data-anchor-id="scaling-limits"><span class="header-section-number">7.8.3</span> Scaling Limits</h3>
<p>The recent success of large language models has encouraged scaling of genomic foundation models along multiple dimensions. However, biological constraints may impose practical limits. Single-nucleotide resolution over megabase contexts may not provide additional benefits if most regulatory interactions occur over shorter ranges. Parameter counts beyond billions may yield diminishing returns given limited training data and finite biological complexity.</p>
<p>Alternative scaling strategies deserve exploration. Rather than naively increasing model size, future work might scale the diversity of training data across species, populations, and functional contexts, incorporate more structured biological knowledge through hybrid architectures, or improve sample efficiency through better pretraining objectives. The most productive scaling dimension likely depends on the target application.</p>
</section>
<section id="clinical-deployment-readiness" class="level3" data-number="7.8.4">
<h3 data-number="7.8.4" class="anchored" data-anchor-id="clinical-deployment-readiness"><span class="header-section-number">7.8.4</span> Clinical Deployment Readiness</h3>
<p>Translation of genomic foundation models to clinical use faces substantial challenges beyond predictive performance. Clinical deployment requires robust performance across diverse patient populations including underrepresented ancestries, calibrated uncertainty quantification for risk-benefit decision-making, interpretability that enables clinicians to understand model predictions, prospective validation demonstrating clinical utility beyond retrospective benchmarks, and regulatory approval processes that current models do not yet satisfy.</p>
<p>Addressing these requirements will likely require domain adaptation techniques that adjust models to clinical populations, ensemble methods that provide well-calibrated uncertainty estimates, mechanistic interpretation frameworks that connect predictions to biological mechanisms, integration with electronic health record systems and clinical workflows, and accumulation of prospective validation evidence demonstrating patient benefit.</p>
</section>
</section>
<section id="summary-and-forward-references" class="level2" data-number="7.9">
<h2 data-number="7.9" class="anchored" data-anchor-id="summary-and-forward-references"><span class="header-section-number">7.9</span> Summary and Forward References</h2>
<p>This chapter established a conceptual framework for understanding genomic foundation models as a distinct class of computational tools. We defined genomic foundation models through five essential properties: large-scale pretraining with minimal supervision, general-purpose representations, broad transfer capability, scale along at least one dimension, and standardized interfaces for downstream use. This definition separates true foundation models from task-specific architectures that may be large but lack generality.</p>
<p>We organized the emerging ecosystem into four families. DNA language models learn sequence distributions through self-supervision and provide general embeddings. Sequence-to-function models predict molecular readouts from sequence through functionally supervised training. Variant-centric models focus on genetic variants as atomic units, integrating sequence context with population and functional annotations. Multi-omic models jointly represent DNA and other molecular layers through cross-modal architectures.</p>
<p>Four orthogonal design dimensions shape model behavior: data composition including species coverage and assay diversity, architecture families spanning transformers to sub-quadratic alternatives, pretraining objectives from masked language modeling to multi-task prediction, and tokenization strategies that trade resolution against context length. Understanding these dimensions helps match models to applications and positions new contributions within the design space.</p>
<p>Evaluation of genomic foundation models requires multi-task assessment, measurement of transfer capability, and robustness testing across domains and populations. Emerging benchmark suites including ProteinGym, TraitGym, and DNA foundation model comparisons enable systematic evaluation, though gaps remain in coverage of clinical and rare variant interpretation tasks <span class="citation" data-cites="notin_proteingym_2023 benegas_traitgym_2025">(<a href="references.html#ref-notin_proteingym_2023" role="doc-biblioref">Notin et al. 2023</a>; <a href="references.html#ref-benegas_traitgym_2025" role="doc-biblioref">Benegas, Eraslan, and Song 2025</a>)</span>.</p>
<p>The foundation model ecosystem includes model hubs, documentation standards, community benchmarks, and industry contributions that collectively enable model development and deployment. Open challenges include potential convergence toward unified architectures versus persistence of specialized families, integration of causal and mechanistic structure, improved rare and structural variant interpretation, and satisfaction of clinical deployment requirements.</p>
<p>The conceptual framework established here will guide subsequent chapters. <a href="p2-ch08-pretrain.html" class="quarto-xref"><span>Chapter 8</span></a> examines how genomic foundation models are actually trained, including data curation, objective design, and optimization strategies. <a href="p5-ch20-vep.html" class="quarto-xref"><span>Chapter 20</span></a> recasts variant effect prediction in the foundation model era, while <a href="p4-ch17-systems.html" class="quarto-xref"><span>Chapter 17</span></a> explores multi-omic integration and systems-level modeling. Throughout these applications, the taxonomy and design principles developed in this chapter provide structure for understanding a rapidly evolving field.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-avsec_enformer_2021" class="csl-entry" role="listitem">
Avsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. <span>“[<span>Enformer</span>] <span>Effective</span> Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.”</span> <em>Nature Methods</em> 18 (October): 1196–1203. <a href="https://doi.org/10.1038/s41592-021-01252-x">https://doi.org/10.1038/s41592-021-01252-x</a>.
</div>
<div id="ref-benegas_traitgym_2025" class="csl-entry" role="listitem">
Benegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025. <span>“[<span>TraitGym</span>] <span>Benchmarking</span> <span>DNA</span> <span>Sequence</span> <span>Models</span> for <span>Causal</span> <span>Regulatory</span> <span>Variant</span> <span>Prediction</span> in <span>Human</span> <span>Genetics</span>.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.02.11.637758">https://doi.org/10.1101/2025.02.11.637758</a>.
</div>
<div id="ref-chen_deepsea_2022" class="csl-entry" role="listitem">
Chen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. <span>“[<span>DeepSEA</span> <span>Sei</span>] <span>A</span> Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.”</span> <em>Nature Genetics</em> 54 (7): 940–49. <a href="https://doi.org/10.1038/s41588-022-01102-2">https://doi.org/10.1038/s41588-022-01102-2</a>.
</div>
<div id="ref-cheng_alphamissense_2023" class="csl-entry" role="listitem">
Cheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. <span>“[<span>AlphaMissense</span>] <span>Accurate</span> Proteome-Wide Missense Variant Effect Prediction with <span>AlphaMissense</span>.”</span> <em>Science</em> 381 (6664): eadg7492. <a href="https://doi.org/10.1126/science.adg7492">https://doi.org/10.1126/science.adg7492</a>.
</div>
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-georgantas_delphi_2024" class="csl-entry" role="listitem">
Georgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. <span>“Delphi: <span>A</span> <span>Deep</span>-Learning <span>Method</span> for <span>Polygenic</span> <span>Risk</span> <span>Prediction</span>.”</span> medRxiv. <a href="https://doi.org/10.1101/2024.04.19.24306079">https://doi.org/10.1101/2024.04.19.24306079</a>.
</div>
<div id="ref-jaganathan_spliceai_2019" class="csl-entry" role="listitem">
Jaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. <span>“[<span>SpliceAI</span>] <span>Predicting</span> <span>Splicing</span> from <span>Primary</span> <span>Sequence</span> with <span>Deep</span> <span>Learning</span>.”</span> <em>Cell</em> 176 (3): 535–548.e24. <a href="https://doi.org/10.1016/j.cell.2018.12.015">https://doi.org/10.1016/j.cell.2018.12.015</a>.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-kagda_encode_2025" class="csl-entry" role="listitem">
Kagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. <span>“Data Navigation on the <span>ENCODE</span> Portal.”</span> <em>Nature Communications</em> 16 (1): 9592. <a href="https://doi.org/10.1038/s41467-025-64343-9">https://doi.org/10.1038/s41467-025-64343-9</a>.
</div>
<div id="ref-landrum_clinvar_2018" class="csl-entry" role="listitem">
Landrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. <span>“<span>ClinVar</span>: Improving Access to Variant Interpretations and Supporting Evidence.”</span> <em>Nucleic Acids Research</em> 46 (D1): D1062–67. <a href="https://doi.org/10.1093/nar/gkx1153">https://doi.org/10.1093/nar/gkx1153</a>.
</div>
<div id="ref-li_omnidna_2025" class="csl-entry" role="listitem">
Li, Zehui, Vallijah Subasri, Yifei Shen, Dongsheng Li, Yiren Zhao, Guy-Bart Stan, and Caihua Shan. 2025. <span>“Omni-<span>DNA</span>: <span>A</span> <span>Unified</span> <span>Genomic</span> <span>Foundation</span> <span>Model</span> for <span>Cross</span>-<span>Modal</span> and <span>Multi</span>-<span>Task</span> <span>Learning</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2502.03499">https://doi.org/10.48550/arXiv.2502.03499</a>.
</div>
<div id="ref-lin_esm-2_2022" class="csl-entry" role="listitem">
Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. <span>“[<span>ESM</span>-2] <span>Language</span> Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.07.20.500902">https://doi.org/10.1101/2022.07.20.500902</a>.
</div>
<div id="ref-manzo_comparative_2025" class="csl-entry" role="listitem">
Manzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025. <span>“Comparative <span>Analysis</span> of <span>Deep</span> <span>Learning</span> <span>Models</span> for <span>Predicting</span> <span>Causative</span> <span>Regulatory</span> <span>Variants</span>.”</span> <em>bioRxiv: The Preprint Server for Biology</em>, June, 2025.05.19.654920. <a href="https://doi.org/10.1101/2025.05.19.654920">https://doi.org/10.1101/2025.05.19.654920</a>.
</div>
<div id="ref-medvedev_biotoken_2025" class="csl-entry" role="listitem">
Medvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. <span>“<span>BioToken</span> and <span>BioFM</span> – <span>Biologically</span>-<span>Informed</span> <span>Tokenization</span> <span>Enables</span> <span>Accurate</span> and <span>Efficient</span> <span>Genomic</span> <span>Foundation</span> <span>Models</span>.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.03.27.645711">https://doi.org/10.1101/2025.03.27.645711</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-notin_proteingym_2023" class="csl-entry" role="listitem">
Notin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. <span>“<span>ProteinGym</span>: <span>Large</span>-<span>Scale</span> <span>Benchmarks</span> for <span>Protein</span> <span>Fitness</span> <span>Prediction</span> and <span>Design</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 36 (December): 64331–79. <a href="https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html">https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html</a>.
</div>
<div id="ref-rakowski_mifm_2025" class="csl-entry" role="listitem">
Rakowski, Alexander, and Christoph Lippert. 2025. <span>“[<span>MIFM</span>] <span>Multiple</span> Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.”</span> medRxiv. <a href="https://doi.org/10.1101/2025.06.13.25329551">https://doi.org/10.1101/2025.06.13.25329551</a>.
</div>
<div id="ref-rentzsch_cadd_2019" class="csl-entry" role="listitem">
Rentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. <span>“<span>CADD</span>: Predicting the Deleteriousness of Variants Throughout the Human Genome.”</span> <em>Nucleic Acids Research</em> 47 (D1): D886–94. <a href="https://doi.org/10.1093/nar/gky1016">https://doi.org/10.1093/nar/gky1016</a>.
</div>
<div id="ref-rives_esm_2021" class="csl-entry" role="listitem">
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. <span>“[<span>ESM</span>-1b] <span>Biological</span> Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.”</span> <em>Proceedings of the National Academy of Sciences of the United States of America</em> 118 (15): e2016239118. <a href="https://doi.org/10.1073/pnas.2016239118">https://doi.org/10.1073/pnas.2016239118</a>.
</div>
<div id="ref-sanabria_grover_2024" class="csl-entry" role="listitem">
Sanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. <span>“[<span>GROVER</span>] <span>DNA</span> Language Model <span>GROVER</span> Learns Sequence Context in the Human Genome.”</span> <em>Nature Machine Intelligence</em> 6 (8): 911–23. <a href="https://doi.org/10.1038/s42256-024-00872-0">https://doi.org/10.1038/s42256-024-00872-0</a>.
</div>
<div id="ref-schubach_cadd_2024" class="csl-entry" role="listitem">
Schubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. <span>“<span>CADD</span> V1.7: Using Protein Language Models, Regulatory <span>CNNs</span> and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.”</span> <em>Nucleic Acids Research</em> 52 (D1): D1143–54. <a href="https://doi.org/10.1093/nar/gkad989">https://doi.org/10.1093/nar/gkad989</a>.
</div>
<div id="ref-gtex_2020" class="csl-entry" role="listitem">
The GTEx Consortium. 2020. <span>“The <span>GTEx</span> <span>Consortium</span> Atlas of Genetic Regulatory Effects Across Human Tissues.”</span> <em>Science</em> 369 (6509): 1318–30. <a href="https://doi.org/10.1126/science.aaz1776">https://doi.org/10.1126/science.aaz1776</a>.
</div>
<div id="ref-wu_genome-wide_2024" class="csl-entry" role="listitem">
Wu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray, Peter M. Visscher, and Jian Zeng. 2024. <span>“Genome-Wide Fine-Mapping Improves Identification of Causal Variants.”</span> <em>Research Square</em>, August, rs.3.rs–4759390. <a href="https://doi.org/10.21203/rs.3.rs-4759390/v1">https://doi.org/10.21203/rs.3.rs-4759390/v1</a>.
</div>
<div id="ref-zheng_cistrome_2019" class="csl-entry" role="listitem">
Zheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei Sun, Chen-Hao Chen, et al. 2019. <span>“Cistrome <span>Data</span> <span>Browser</span>: Expanded Datasets and New Tools for Gene Regulatory Analysis.”</span> <em>Nucleic Acids Research</em> 47 (D1): D729–35. <a href="https://doi.org/10.1093/nar/gky1094">https://doi.org/10.1093/nar/gky1094</a>.
</div>
<div id="ref-zhou_expecto_2018" class="csl-entry" role="listitem">
Zhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. <span>“[<span>Expecto</span>] <span>Deep</span> Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.”</span> <em>Nature Genetics</em> 50 (8): 1171–79. <a href="https://doi.org/10.1038/s41588-018-0160-6">https://doi.org/10.1038/s41588-018-0160-6</a>.
</div>
<div id="ref-zhou_deepsea_2015" class="csl-entry" role="listitem">
Zhou, Jian, and Olga G. Troyanskaya. 2015. <span>“[<span>DeepSEA</span>] <span>Predicting</span> Effects of Noncoding Variants with Deep Learning–Based Sequence Model.”</span> <em>Nature Methods</em> 12 (10): 931–34. <a href="https://doi.org/10.1038/nmeth.3547">https://doi.org/10.1038/nmeth.3547</a>.
</div>
<div id="ref-zhou_dnabert-2_2024" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p2-ch06-transformers.html" class="pagination-link" aria-label="Transformer Architecture for Genomics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p2-ch08-pretrain.html" class="pagination-link" aria-label="Pretraining Objectives &amp; Strategies">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>