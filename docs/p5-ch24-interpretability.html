<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>24&nbsp; Interpretability and Mechanism – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p6--translation.html" rel="next">
<link href="./p5-ch23-uncertainty.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p5--eval-interp.html">Part V: Evaluation and Reliability</a></li><li class="breadcrumb-item"><a href="./p5-ch24-interpretability.html"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability and Mechanism</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation and Tokenization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Models for Genomic Sequence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Attention and Transformers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives and Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning and Adaptation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">The Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Long-Context Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Variant Effect Prediction with Foundation Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p4--multi-scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Multi-Scale Modeling</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RNA Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Single-Cell and Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">3D Genome and Spatial Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch18-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch19-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Evaluation Methodology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability and Mechanism</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Rare Disease and Variant Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Drug Discovery and Target Identification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch28-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Sequence Design and Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch29-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Regulatory, Ethical, and Future Considerations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Training Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#attribution-methods-and-input-importance" id="toc-attribution-methods-and-input-importance" class="nav-link active" data-scroll-target="#attribution-methods-and-input-importance"><span class="header-section-number">24.1</span> Attribution Methods and Input Importance</a>
  <ul class="collapse">
  <li><a href="#in-silico-mutagenesis" id="toc-in-silico-mutagenesis" class="nav-link" data-scroll-target="#in-silico-mutagenesis"><span class="header-section-number">24.1.1</span> In Silico Mutagenesis</a></li>
  <li><a href="#gradient-based-attribution" id="toc-gradient-based-attribution" class="nav-link" data-scroll-target="#gradient-based-attribution"><span class="header-section-number">24.1.2</span> Gradient-Based Attribution</a></li>
  <li><a href="#reconciling-attribution-methods" id="toc-reconciling-attribution-methods" class="nav-link" data-scroll-target="#reconciling-attribution-methods"><span class="header-section-number">24.1.3</span> Reconciling Attribution Methods</a></li>
  </ul></li>
  <li><a href="#interpreting-convolutional-filters" id="toc-interpreting-convolutional-filters" class="nav-link" data-scroll-target="#interpreting-convolutional-filters"><span class="header-section-number">24.2</span> Interpreting Convolutional Filters</a>
  <ul class="collapse">
  <li><a href="#from-filters-to-position-weight-matrices" id="toc-from-filters-to-position-weight-matrices" class="nav-link" data-scroll-target="#from-filters-to-position-weight-matrices"><span class="header-section-number">24.2.1</span> From Filters to Position Weight Matrices</a></li>
  <li><a href="#deeper-layers-and-combinatorial-patterns" id="toc-deeper-layers-and-combinatorial-patterns" class="nav-link" data-scroll-target="#deeper-layers-and-combinatorial-patterns"><span class="header-section-number">24.2.2</span> Deeper Layers and Combinatorial Patterns</a></li>
  </ul></li>
  <li><a href="#motif-discovery-from-attributions" id="toc-motif-discovery-from-attributions" class="nav-link" data-scroll-target="#motif-discovery-from-attributions"><span class="header-section-number">24.3</span> Motif Discovery from Attributions</a></li>
  <li><a href="#probing-learned-representations" id="toc-probing-learned-representations" class="nav-link" data-scroll-target="#probing-learned-representations"><span class="header-section-number">24.4</span> Probing Learned Representations</a>
  <ul class="collapse">
  <li><a href="#probing-methodology" id="toc-probing-methodology" class="nav-link" data-scroll-target="#probing-methodology"><span class="header-section-number">24.4.1</span> Probing Methodology</a></li>
  <li><a href="#limitations-of-probing" id="toc-limitations-of-probing" class="nav-link" data-scroll-target="#limitations-of-probing"><span class="header-section-number">24.4.2</span> Limitations of Probing</a></li>
  </ul></li>
  <li><a href="#attention-patterns-in-transformer-models" id="toc-attention-patterns-in-transformer-models" class="nav-link" data-scroll-target="#attention-patterns-in-transformer-models"><span class="header-section-number">24.5</span> Attention Patterns in Transformer Models</a>
  <ul class="collapse">
  <li><a href="#what-attention-patterns-reveal" id="toc-what-attention-patterns-reveal" class="nav-link" data-scroll-target="#what-attention-patterns-reveal"><span class="header-section-number">24.5.1</span> What Attention Patterns Reveal</a></li>
  <li><a href="#why-attention-weights-mislead" id="toc-why-attention-weights-mislead" class="nav-link" data-scroll-target="#why-attention-weights-mislead"><span class="header-section-number">24.5.2</span> Why Attention Weights Mislead</a></li>
  </ul></li>
  <li><a href="#regulatory-vocabularies-and-global-interpretability" id="toc-regulatory-vocabularies-and-global-interpretability" class="nav-link" data-scroll-target="#regulatory-vocabularies-and-global-interpretability"><span class="header-section-number">24.6</span> Regulatory Vocabularies and Global Interpretability</a>
  <ul class="collapse">
  <li><a href="#sequence-classes-from-sei" id="toc-sequence-classes-from-sei" class="nav-link" data-scroll-target="#sequence-classes-from-sei"><span class="header-section-number">24.6.1</span> Sequence Classes from Sei</a></li>
  <li><a href="#embedding-geometry-and-regulatory-programs" id="toc-embedding-geometry-and-regulatory-programs" class="nav-link" data-scroll-target="#embedding-geometry-and-regulatory-programs"><span class="header-section-number">24.6.2</span> Embedding Geometry and Regulatory Programs</a></li>
  </ul></li>
  <li><a href="#mechanistic-interpretability" id="toc-mechanistic-interpretability" class="nav-link" data-scroll-target="#mechanistic-interpretability"><span class="header-section-number">24.7</span> Mechanistic Interpretability</a>
  <ul class="collapse">
  <li><a href="#circuits-and-features" id="toc-circuits-and-features" class="nav-link" data-scroll-target="#circuits-and-features"><span class="header-section-number">24.7.1</span> Circuits and Features</a></li>
  <li><a href="#applications-to-genomic-models" id="toc-applications-to-genomic-models" class="nav-link" data-scroll-target="#applications-to-genomic-models"><span class="header-section-number">24.7.2</span> Applications to Genomic Models</a></li>
  </ul></li>
  <li><a href="#validation-from-explanations-to-experiments" id="toc-validation-from-explanations-to-experiments" class="nav-link" data-scroll-target="#validation-from-explanations-to-experiments"><span class="header-section-number">24.8</span> Validation: From Explanations to Experiments</a>
  <ul class="collapse">
  <li><a href="#faithfulness-testing" id="toc-faithfulness-testing" class="nav-link" data-scroll-target="#faithfulness-testing"><span class="header-section-number">24.8.1</span> Faithfulness Testing</a></li>
  <li><a href="#experimental-validation" id="toc-experimental-validation" class="nav-link" data-scroll-target="#experimental-validation"><span class="header-section-number">24.8.2</span> Experimental Validation</a></li>
  </ul></li>
  <li><a href="#interpretability-in-clinical-variant-assessment" id="toc-interpretability-in-clinical-variant-assessment" class="nav-link" data-scroll-target="#interpretability-in-clinical-variant-assessment"><span class="header-section-number">24.9</span> Interpretability in Clinical Variant Assessment</a></li>
  <li><a href="#practical-approaches-for-foundation-model-analysis" id="toc-practical-approaches-for-foundation-model-analysis" class="nav-link" data-scroll-target="#practical-approaches-for-foundation-model-analysis"><span class="header-section-number">24.10</span> Practical Approaches for Foundation Model Analysis</a></li>
  <li><a href="#from-explanations-to-understanding" id="toc-from-explanations-to-understanding" class="nav-link" data-scroll-target="#from-explanations-to-understanding"><span class="header-section-number">24.11</span> From Explanations to Understanding</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p5--eval-interp.html">Part V: Evaluation and Reliability</a></li><li class="breadcrumb-item"><a href="./p5-ch24-interpretability.html"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability and Mechanism</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-interpretability" class="quarto-section-identifier"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability and Mechanism</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Genomic foundation models achieve remarkable predictive accuracy while offering essentially no explanation for their predictions. A model can predict that a particular variant will disrupt gene regulation with 95% confidence, yet a clinician examining that prediction cannot determine whether the model detected a disrupted transcription factor binding site, learned a spurious correlation with GC content, or exploited some pattern in the training data that has nothing to do with biology. This opacity creates a fundamental tension: the models most capable of capturing complex regulatory logic are precisely those whose internal representations resist human understanding.</p>
<p>The stakes of this tension extend beyond scientific curiosity. Variant interpretation guidelines from the American College of Medical Genetics require that computational evidence be weighed alongside functional assays, segregation data, and population frequency. A pathogenicity score alone satisfies only weak evidence criteria; knowing that a variant disrupts a specific CTCF binding site in a cardiac enhancer provides interpretable mechanistic evidence that can be combined with clinical presentation and family history. When models cannot explain their predictions, clinicians cannot integrate computational evidence with biological reasoning. The same limitation affects research applications: a model that predicts enhancer activity cannot generate testable hypotheses about regulatory grammar unless its internal computations can be translated into statements about motifs, spacing constraints, and combinatorial logic.</p>
<p>This chapter examines the toolkit for extracting meaning from genomic foundation models, from classical attribution methods that identify important input positions through motif discovery algorithms that translate attributions into regulatory vocabularies, to emerging techniques from mechanistic interpretability that probe the internal circuits of transformer architectures. Throughout, a critical distinction guides the analysis: plausible explanations that match biological intuition are not the same as faithful explanations that accurately reflect model computation. Understanding when these diverge, and how to validate interpretability claims experimentally, determines whether model explanations accelerate discovery or provide false comfort.</p>
<section id="attribution-methods-and-input-importance" class="level2" data-number="24.1">
<h2 data-number="24.1" class="anchored" data-anchor-id="attribution-methods-and-input-importance"><span class="header-section-number">24.1</span> Attribution Methods and Input Importance</h2>
<p>When a model predicts that a 200-kilobase genomic region will show high chromatin accessibility in hepatocytes, a natural question arises: which bases within that region drive the prediction? Attribution methods answer this question by assigning importance scores to input positions, identifying where the model focuses its computational attention. These scores can reveal candidate regulatory elements, highlight the sequence features underlying variant effects, and provide the raw material for downstream motif discovery.</p>
<section id="in-silico-mutagenesis" class="level3" data-number="24.1.1">
<h3 data-number="24.1.1" class="anchored" data-anchor-id="in-silico-mutagenesis"><span class="header-section-number">24.1.1</span> In Silico Mutagenesis</h3>
<p>The most direct approach to measuring input importance is simply to change each base and observe what happens to the prediction. In silico mutagenesis (ISM) systematically introduces mutations at every position, computing the difference between mutant and reference predictions. For a sequence of length <span class="math inline">\(L\)</span>, ISM creates three mutant sequences at each position (substituting each non-reference nucleotide), yielding <span class="math inline">\(3L\)</span> forward passes through the model. The resulting mutation effect matrix captures how sensitive the prediction is to changes at each position and to each alternative base.</p>
<p>ISM provides true counterfactual information rather than approximations. When ISM shows that mutating position 47 from A to G reduces the predicted accessibility by 0.3 log-fold, that is a direct observation about model behavior, not an estimate derived from gradients or attention weights. This directness makes ISM the gold standard for faithfulness: if ISM identifies a position as important, perturbing that position genuinely changes the output.</p>
<p>The limitation is computational cost. Scoring all single-nucleotide substitutions in a 200-kilobase input requires 600,000 forward passes, which becomes prohibitive for large models or genome-wide analysis. Practical applications often restrict ISM to targeted windows around variants of interest, using faster methods to identify candidate regions for detailed analysis. For variant effect prediction specifically, ISM reduces to comparing reference and alternative allele predictions, requiring only two forward passes per variant.</p>
</section>
<section id="gradient-based-attribution" class="level3" data-number="24.1.2">
<h3 data-number="24.1.2" class="anchored" data-anchor-id="gradient-based-attribution"><span class="header-section-number">24.1.2</span> Gradient-Based Attribution</h3>
<p>Gradient-based methods approximate the counterfactual information from ISM using backpropagation. The gradient of the output with respect to each input position measures how much an infinitesimal change at that position would affect the prediction. With one-hot encoded sequence, the gradient at each base indicates the sensitivity to substituting that nucleotide.</p>
<p>The simplest approach, often called saliency mapping, computes raw gradients and visualizes their magnitudes across the sequence. A common variant multiplies gradients by inputs (gradient × input), focusing on positions where the current nucleotide is both important and present. These methods require only a single backward pass, making them orders of magnitude faster than ISM.</p>
<p>Gradient-based methods suffer from saturation in regions where the model is already confident. If a strong motif drives the prediction into a saturated region of the output nonlinearity, small perturbations produce near-zero gradients even though the motif is functionally critical. DeepLIFT addresses this limitation by comparing activations between an input sequence and a reference, propagating differences through the network using custom rules that avoid gradient saturation. The resulting attributions satisfy a completeness property: contributions sum to the difference between input and reference predictions.</p>
<p>Integrated gradients provide theoretical grounding through the path integral of gradients along a linear interpolation from reference to input:</p>
<p><span class="math display">\[\text{IG}_i(x) = (x_i - x'_i) \int_{\alpha=0}^{1} \frac{\partial f(x' + \alpha(x - x'))}{\partial x_i} \, d\alpha\]</span></p>
<p>This integral, approximated by summing gradients at discrete interpolation steps, satisfies sensitivity (any input that affects the output receives nonzero attribution) and implementation invariance (functionally equivalent networks produce identical attributions). Integrated gradients have become a standard choice for genomic models, balancing computational efficiency with theoretical guarantees.</p>
<p>All gradient-based methods require choosing a reference sequence, which substantially affects the resulting attributions. Common choices include dinucleotide-shuffled versions of the input (preserving local composition while disrupting motifs), average non-functional sequence, or simply zeros. The reference defines what counts as informative: attributions highlight features that differ from the reference and contribute to the prediction difference. A shuffled reference emphasizes motif content; a zero reference treats any sequence information as potentially important.</p>
<p><strong>Figure recommendation</strong>: A multi-panel comparison showing the same genomic region analyzed by ISM, gradient × input, DeepLIFT, and integrated gradients. Highlight a known transcription factor binding site and show how different methods assign importance to the motif core versus flanking regions. Include a small table comparing computational cost (forward passes required) and typical failure modes for each method.</p>
</section>
<section id="reconciling-attribution-methods" class="level3" data-number="24.1.3">
<h3 data-number="24.1.3" class="anchored" data-anchor-id="reconciling-attribution-methods"><span class="header-section-number">24.1.3</span> Reconciling Attribution Methods</h3>
<p>Different attribution methods can produce strikingly different importance maps for the same sequence and prediction. A position might show high importance under ISM but near-zero gradients due to saturation, or high gradient magnitude but minimal effect when actually mutated due to redundancy with nearby positions. This disagreement reflects genuine differences in what each method measures: gradients capture local sensitivity, ISM captures counterfactual effects, and DeepLIFT captures contribution relative to a reference.</p>
<p>Practical workflows often combine multiple methods. Gradient-based approaches efficiently scan long sequences to identify candidate regions, ISM validates importance in targeted windows, and agreement across methods increases confidence that identified features genuinely drive predictions. Disagreement flags positions for closer investigation, potentially revealing saturation effects, redundancy, or artifacts in individual methods.</p>
</section>
</section>
<section id="interpreting-convolutional-filters" class="level2" data-number="24.2">
<h2 data-number="24.2" class="anchored" data-anchor-id="interpreting-convolutional-filters"><span class="header-section-number">24.2</span> Interpreting Convolutional Filters</h2>
<p>Convolutional neural networks remain central to genomic sequence modeling, as discussed in <a href="p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>, and their first-layer filters offer a particularly tractable interpretability target. Each filter slides along the sequence computing dot products with local windows, and high activation indicates that the local sequence matches the filter’s learned pattern. This architecture creates a natural correspondence between filters and sequence motifs.</p>
<section id="from-filters-to-position-weight-matrices" class="level3" data-number="24.2.1">
<h3 data-number="24.2.1" class="anchored" data-anchor-id="from-filters-to-position-weight-matrices"><span class="header-section-number">24.2.1</span> From Filters to Position Weight Matrices</h3>
<p>Converting learned filters to interpretable motifs follows a standard workflow. The trained model processes a large sequence set, typically training data or genome-wide tiles, recording positions where each filter’s activation exceeds a threshold. The fixed-length windows around high-activation positions are extracted and aligned, and nucleotide frequencies at each position are computed to build a position weight matrix (PWM). This PWM can be visualized as a sequence logo and compared to databases like JASPAR or HOCOMOCO.</p>
<p>When this procedure is applied to models trained on chromatin accessibility or transcription factor binding, first-layer filters frequently match known transcription factor motifs. DeepSEA filters include recognizable matches to CTCF, AP-1, and cell-type-specific factors. This correspondence validates that models discover biologically meaningful patterns rather than arbitrary correlations, and it provides a direct link between model weights and decades of experimental characterization of transcription factor binding preferences.</p>
<p>Several complications affect filter interpretation. DNA is double-stranded, and models may learn forward and reverse-complement versions of the same motif as separate filters. Some filters capture general sequence composition (GC-rich regions, homopolymer runs) rather than specific binding sites. These patterns can be biologically meaningful in contexts like nucleosome positioning or purely artifactual depending on the training task. Distinguishing informative filters from compositional shortcuts requires cross-referencing with known biology and testing whether filter-derived motifs predict binding in held-out data.</p>
</section>
<section id="deeper-layers-and-combinatorial-patterns" class="level3" data-number="24.2.2">
<h3 data-number="24.2.2" class="anchored" data-anchor-id="deeper-layers-and-combinatorial-patterns"><span class="header-section-number">24.2.2</span> Deeper Layers and Combinatorial Patterns</h3>
<p>Beyond the first layer, convolutional filters combine lower-level patterns into complex representations. Deeper layers can encode motif pairs that co-occur at characteristic spacing, orientation preferences between binding sites, and contextual dependencies where a motif’s importance varies with surrounding sequence. These combinatorial patterns capture aspects of regulatory grammar that individual motifs cannot represent.</p>
<p>Direct interpretation of deeper filters becomes increasingly difficult as receptive fields expand and nonlinearities accumulate. The activation of a layer-5 filter depends on intricate combinations of earlier patterns, resisting simple biological annotation. Indirect approaches prove more tractable: analyzing which input regions drive high activation at deeper layers, clustering high-activation sequences to find common themes, or probing whether deeper representations encode specific biological properties.</p>
</section>
</section>
<section id="motif-discovery-from-attributions" class="level2" data-number="24.3">
<h2 data-number="24.3" class="anchored" data-anchor-id="motif-discovery-from-attributions"><span class="header-section-number">24.3</span> Motif Discovery from Attributions</h2>
<p>Attribution maps highlight important positions but do not directly reveal motifs. A DeepLIFT track might show scattered high-importance bases throughout a sequence without indicating that those bases collectively form instances of the same transcription factor binding site. TF-MoDISco (Transcription Factor Motif Discovery from Importance Scores) bridges this gap by discovering motifs from attribution scores rather than raw sequences.</p>
<p>The insight underlying TF-MoDISco is that importance-weighted sequences focus motif discovery on positions the model actually uses. Traditional motif finders must contend with the fact that most positions in regulatory sequences do not participate in functional motifs. By extracting seqlets (short windows where total importance exceeds a threshold) and clustering them based on both sequence content and importance profiles, TF-MoDISco identifies patterns that drive model predictions.</p>
<p>The workflow proceeds through several stages. Base-level importance scores are computed for many sequences using DeepLIFT, ISM, or integrated gradients. Windows where total importance exceeds a threshold are extracted as seqlets, each representing a candidate motif instance. These seqlets are compared using metrics that consider both sequence content and importance profiles, then clustered into groups corresponding to putative motifs. Within each cluster, seqlets are aligned and consolidated into PWMs and importance-weighted logos. The resulting motifs can be matched to known transcription factors or flagged as novel patterns.</p>
<p>Beyond individual motifs, TF-MoDISco enables grammar inference by analyzing motif co-occurrence. Mapping discovered motif instances back to genomic coordinates reveals characteristic spacing between motif pairs, orientation preferences, and cell-type-specific usage patterns. These grammatical rules can be validated through in silico experiments: inserting or removing motifs in synthetic sequences and checking whether predictions change as expected.</p>
<p>Applications to models like BPNet trained on ChIP-seq data have recovered known transcription factor motifs, discovered novel sequence variants, and revealed spacing constraints validated through synthetic reporter assays. The same workflow applies to foundation model analysis: use the model to produce base-level attributions for a downstream task, run TF-MoDISco to extract a task-specific motif vocabulary, and analyze how motif usage varies across conditions.</p>
<p><strong>Figure recommendation</strong>: A pipeline diagram showing the complete TF-MoDISco workflow, from input sequences through attribution computation, seqlet extraction, clustering, and motif logo generation. Include an example showing discovered motifs aligned to JASPAR matches and a small grammar diagram illustrating inferred spacing constraints between co-occurring motifs.</p>
</section>
<section id="probing-learned-representations" class="level2" data-number="24.4">
<h2 data-number="24.4" class="anchored" data-anchor-id="probing-learned-representations"><span class="header-section-number">24.4</span> Probing Learned Representations</h2>
<p>Attribution methods ask which input positions matter; probing asks what information the model’s internal representations encode. A probing classifier is a simple supervised model (typically linear) trained to predict some property of interest from the hidden representations of a pretrained model. If a linear probe can accurately predict a property, that property is encoded in an accessible form within the representation.</p>
<section id="probing-methodology" class="level3" data-number="24.4.1">
<h3 data-number="24.4.1" class="anchored" data-anchor-id="probing-methodology"><span class="header-section-number">24.4.1</span> Probing Methodology</h3>
<p>The standard probing workflow extracts hidden states from a pretrained model for a set of inputs where the property of interest is known. These hidden states, without further transformation, serve as features for training a simple classifier to predict the property. The classifier’s accuracy indicates how well the representation encodes the probed property, while its simplicity (linearity, minimal parameters) ensures that the probe identifies information present in the representation rather than information the probe itself computes.</p>
<p>For protein language models like ESM-2, probing has revealed that representations encode secondary structure, solvent accessibility, contact maps, and even 3D coordinates to a surprising degree, as discussed in <a href="p3-ch12-protein-lm.html" class="quarto-xref"><span>Chapter 12</span></a>. These properties emerge despite training on sequence alone, demonstrating that masked language modeling on evolutionary sequences induces representations that capture structural information. For DNA language models, probing can assess whether representations encode chromatin state, gene boundaries, promoter versus enhancer identity, or species-specific regulatory signatures.</p>
<p>Probing provides diagnostic information distinct from downstream task performance. A model might achieve high accuracy on a regulatory prediction task by learning shortcuts (correlations with GC content, distance to annotated genes) rather than encoding genuine regulatory grammar. Probing can detect such shortcuts: if representations strongly encode GC content but weakly encode transcription factor binding site presence, the model may be exploiting composition rather than sequence logic. This diagnostic function complements the confounder analysis discussed in <a href="p5-ch22-confounding.html" class="quarto-xref"><span>Chapter 22</span></a>.</p>
</section>
<section id="limitations-of-probing" class="level3" data-number="24.4.2">
<h3 data-number="24.4.2" class="anchored" data-anchor-id="limitations-of-probing"><span class="header-section-number">24.4.2</span> Limitations of Probing</h3>
<p>Probing results require careful interpretation. A probe’s failure to predict some property might indicate that the representation does not encode it, or might reflect limitations of the probe architecture, insufficient training data, or mismatch between the probe’s capacity and the complexity of the encoding. Linear probes may miss nonlinearly encoded information; more complex probes risk learning the property themselves rather than reading it from the representation.</p>
<p>The selectivity-accessibility tradeoff complicates interpretation. A representation might encode a property accessibly (recoverable by a linear probe) or selectively (encoded but requiring nonlinear decoding). Properties encoded selectively might be present but not easily extracted, while properties encoded accessibly might be incidentally correlated with the training objective rather than causally important. Combining probing with causal interventions (ablating representation components and measuring effects on downstream predictions) provides stronger evidence about which encoded properties actually matter.</p>
<p><strong>Figure recommendation</strong>: A schematic showing the probing setup, with a pretrained model producing hidden representations, a linear probe trained on those representations, and accuracy metrics for different probed properties. Include a bar chart comparing probe accuracy for different properties (secondary structure, binding site presence, GC content) across model layers, illustrating how different information localizes to different depths.</p>
</section>
</section>
<section id="attention-patterns-in-transformer-models" class="level2" data-number="24.5">
<h2 data-number="24.5" class="anchored" data-anchor-id="attention-patterns-in-transformer-models"><span class="header-section-number">24.5</span> Attention Patterns in Transformer Models</h2>
<p>Transformer-based genomic models use self-attention to aggregate information across long sequence contexts, potentially capturing distal regulatory interactions invisible to models with narrow receptive fields. Attention weights indicate which positions each position attends to, creating natural candidates for interpretability: perhaps high attention weights identify functionally related sequence elements.</p>
<section id="what-attention-patterns-reveal" class="level3" data-number="24.5.1">
<h3 data-number="24.5.1" class="anchored" data-anchor-id="what-attention-patterns-reveal"><span class="header-section-number">24.5.1</span> What Attention Patterns Reveal</h3>
<p>When attention weights are analyzed in genomic language models, certain heads exhibit strikingly structured patterns. Some heads preferentially connect positions within the same predicted gene or operon, suggesting the model has learned gene boundaries from sequence alone. Other heads show long-range connections that align with known enhancer-promoter relationships or chromatin loop anchors. Still others cluster positions by functional annotation, connecting genes with similar Gene Ontology terms despite lacking explicit functional labels during training.</p>
<p>In models like Enformer that predict regulatory outputs from long genomic windows, attention can reveal which distal regions influence predictions at a target gene. Contribution scores aggregated across attention heads often peak at known enhancers, insulators, and chromatin domain boundaries. These patterns suggest that the model has learned aspects of regulatory architecture from the correlation between sequence and chromatin output labels.</p>
</section>
<section id="why-attention-weights-mislead" class="level3" data-number="24.5.2">
<h3 data-number="24.5.2" class="anchored" data-anchor-id="why-attention-weights-mislead"><span class="header-section-number">24.5.2</span> Why Attention Weights Mislead</h3>
<p>Raw attention weights require skeptical interpretation. High attention between two positions indicates information flow in the model’s computation but does not necessarily indicate causal influence on predictions. Attention serves multiple computational roles beyond identifying important features: routing information for intermediate computations, implementing positional reasoning, and satisfying architectural constraints. A position receiving high attention might be used for bookkeeping rather than contributing to the final output.</p>
<p>Several specific issues undermine naive attention interpretation. Attention weights describe information movement before value vectors are applied; positions with high attention but small value vector magnitudes contribute little to the output. Multi-head attention averages across heads with different functions; examining average attention obscures specialized head behavior. Cross-layer effects mean that the importance of early-layer attention depends on what later layers do with the routed information.</p>
<p>More robust approaches combine attention analysis with perturbation experiments. If deleting a position that receives high attention changes the prediction substantially, the attention is functionally meaningful. If deletion has minimal effect, the attention may serve computational purposes unrelated to the target output. Attention rollout and attention flow methods propagate attention through layers to better capture information movement across the full network, though these too provide correlational rather than causal evidence.</p>
<p><strong>Figure recommendation</strong>: An attention heatmap for a genomic region containing a gene and distal enhancer, showing different attention head patterns: one head connecting enhancer to promoter, another attending uniformly, a third showing local patterns. Annotate with known regulatory elements and indicate which attention patterns survive deletion testing.</p>
</section>
</section>
<section id="regulatory-vocabularies-and-global-interpretability" class="level2" data-number="24.6">
<h2 data-number="24.6" class="anchored" data-anchor-id="regulatory-vocabularies-and-global-interpretability"><span class="header-section-number">24.6</span> Regulatory Vocabularies and Global Interpretability</h2>
<p>Local interpretability methods explain individual predictions; global interpretability characterizes what a model has learned across its entire training distribution. For genomic models trained to predict thousands of chromatin features, global interpretability asks whether the model has learned a coherent vocabulary of regulatory sequence classes and how those classes map to biological programs.</p>
<section id="sequence-classes-from-sei" class="level3" data-number="24.6.1">
<h3 data-number="24.6.1" class="anchored" data-anchor-id="sequence-classes-from-sei"><span class="header-section-number">24.6.1</span> Sequence Classes from Sei</h3>
<p>Sei exemplifies the global interpretability approach by learning a vocabulary of regulatory sequence classes that summarize chromatin profile diversity across the genome. The model predicts tens of thousands of chromatin outputs (transcription factor binding, histone modifications, accessibility across cell types), then compresses this high-dimensional prediction space into approximately 40 sequence classes through dimensionality reduction and clustering.</p>
<p>Each sequence class corresponds to a characteristic regulatory activity pattern. Some classes show promoter-like signatures (H3K4me3, TSS proximity, broad expression). Others exhibit enhancer patterns (H3K27ac, H3K4me1, cell-type-restricted activity). Repressive classes display H3K27me3 or H3K9me3 enrichment. Cell-type-specific classes capture lineage-restricted regulatory programs (neuronal, immune, hepatic). This vocabulary transforms thousands of raw chromatin predictions into a compact, interpretable representation.</p>
<p>Variants can be characterized by their effects on sequence class scores, yielding functional descriptions more informative than raw pathogenicity predictions. A variant that shifts a region from enhancer-like to promoter-like class, or from active to repressive, provides mechanistic hypotheses about its functional consequences. GWAS enrichment analysis can identify which sequence classes are overrepresented among disease-associated variants, revealing the regulatory programs most relevant to specific phenotypes.</p>
</section>
<section id="embedding-geometry-and-regulatory-programs" class="level3" data-number="24.6.2">
<h3 data-number="24.6.2" class="anchored" data-anchor-id="embedding-geometry-and-regulatory-programs"><span class="header-section-number">24.6.2</span> Embedding Geometry and Regulatory Programs</h3>
<p>Beyond discrete sequence classes, the continuous geometry of learned representations encodes regulatory relationships. Sequences with similar regulatory functions cluster in embedding space; directions in this space correspond to biological axes of variation. Dimensionality reduction techniques (UMAP, t-SNE, PCA) visualize these relationships, revealing how the model organizes regulatory diversity.</p>
<p>For foundation models trained on diverse genomic tasks, embedding geometry can capture cross-task relationships. Sequences that function as enhancers in one cell type might cluster near sequences with enhancer function in related cell types, even if trained independently. Variants that disrupt shared regulatory logic should produce similar embedding perturbations. These geometric properties enable transfer of interpretability insights across tasks and provide compact summaries of model knowledge.</p>
</section>
</section>
<section id="mechanistic-interpretability" class="level2" data-number="24.7">
<h2 data-number="24.7" class="anchored" data-anchor-id="mechanistic-interpretability"><span class="header-section-number">24.7</span> Mechanistic Interpretability</h2>
<p>Classical interpretability methods treat models as input-output functions, probing what they compute without examining how they compute it. Mechanistic interpretability takes a different approach, attempting to reverse-engineer the algorithms implemented by neural network weights. This emerging field, most developed for language models, offers tools increasingly applicable to genomic foundation models.</p>
<section id="circuits-and-features" class="level3" data-number="24.7.1">
<h3 data-number="24.7.1" class="anchored" data-anchor-id="circuits-and-features"><span class="header-section-number">24.7.1</span> Circuits and Features</h3>
<p>The central hypothesis of mechanistic interpretability is that neural networks implement interpretable computations through identifiable circuits: connected subnetworks that perform specific functions. A circuit might detect whether a motif is present, compute the distance between two motifs, or integrate evidence across regulatory elements. Identifying circuits requires tracing information flow through the network and characterizing what each component contributes.</p>
<p>Features are the atomic units of this analysis: directions in activation space that correspond to interpretable concepts. In language models, features have been found that activate for specific topics, syntactic structures, or semantic properties. Analogous features in genomic models might activate for transcription factor binding sites, coding versus non-coding sequence, or regulatory element types. Sparse autoencoders trained on model activations can extract interpretable features by encouraging representations where most features are inactive for any given input.</p>
<p>Superposition complicates feature identification. Neural networks can represent more features than they have dimensions by using overlapping, nearly orthogonal directions. Features active for different inputs can share parameters, enabling high-capacity representations but complicating interpretation. Techniques from compressed sensing and dictionary learning help decompose superposed representations into constituent features.</p>
</section>
<section id="applications-to-genomic-models" class="level3" data-number="24.7.2">
<h3 data-number="24.7.2" class="anchored" data-anchor-id="applications-to-genomic-models"><span class="header-section-number">24.7.2</span> Applications to Genomic Models</h3>
<p>Mechanistic interpretability remains nascent for genomic foundation models, but initial applications show promise. Attention head analysis in DNA language models has identified heads specialized for different genomic functions: some attend within genes, others across regulatory regions, still others implement positional computations. Probing activations at different layers reveals hierarchical feature construction, from local sequence patterns in early layers to long-range regulatory relationships in later layers.</p>
<p>Circuit analysis can explain specific model behaviors. If a model predicts that a variant disrupts regulation, mechanistic analysis can trace which features activate differently for reference versus variant sequence, which attention heads route information about the variant to the prediction, and which intermediate computations change. This mechanistic trace provides far richer explanation than attribution scores alone, potentially identifying the regulatory logic the model has learned.</p>
<p>The challenge is scalability. Current mechanistic interpretability techniques require substantial manual analysis and work best for small models or specific behaviors. Foundation models with billions of parameters resist exhaustive circuit enumeration. Developing automated tools for circuit discovery and scaling mechanistic analysis to large genomic models represents an active research frontier.</p>
<p><strong>Figure recommendation</strong>: A simplified circuit diagram showing how a hypothetical genomic model might process a sequence containing a transcription factor binding site. Show information flow from input embeddings through attention heads that detect the motif, through intermediate layers that contextualize it, to the final prediction. Annotate with interpretable feature descriptions at each stage.</p>
</section>
</section>
<section id="validation-from-explanations-to-experiments" class="level2" data-number="24.8">
<h2 data-number="24.8" class="anchored" data-anchor-id="validation-from-explanations-to-experiments"><span class="header-section-number">24.8</span> Validation: From Explanations to Experiments</h2>
<p>Interpretability methods produce explanations, but explanations are only valuable if they accurately reflect model behavior and connect to biological reality. Validation closes the loop by testing whether interpretability-derived hypotheses hold when subjected to experimental scrutiny.</p>
<section id="faithfulness-testing" class="level3" data-number="24.8.1">
<h3 data-number="24.8.1" class="anchored" data-anchor-id="faithfulness-testing"><span class="header-section-number">24.8.1</span> Faithfulness Testing</h3>
<p>An interpretation is faithful if it accurately describes what the model does. Testing faithfulness requires interventions: changing the features identified as important and verifying that predictions change accordingly. If an attribution method highlights certain positions as driving a prediction, deleting or scrambling those positions should reduce the prediction. If discovered motifs are claimed to be necessary for regulatory activity, removing them from sequences should impair predicted and measured function.</p>
<p>Sanity checks provide baseline validation. When model weights are randomized, attributions should degrade to uninformative noise. When training labels are scrambled, discovered motifs should disappear or lose predictive power. These checks identify methods that produce plausible-looking outputs regardless of model content, revealing explanations that reflect method biases rather than genuine model features.</p>
<p>Counterfactual experiments go further by testing whether identified features are sufficient as well as necessary. Inserting discovered motifs into neutral sequences should increase predicted regulatory activity if the motifs genuinely encode functional elements. Constructing synthetic sequences that combine motifs according to discovered grammatical rules should produce predictions consistent with those rules. Discrepancies between expected and observed effects indicate gaps in the interpretation.</p>
</section>
<section id="experimental-validation" class="level3" data-number="24.8.2">
<h3 data-number="24.8.2" class="anchored" data-anchor-id="experimental-validation"><span class="header-section-number">24.8.2</span> Experimental Validation</h3>
<p>The ultimate test of interpretability connects model-derived hypotheses to biological experiments. Motifs discovered through TF-MoDISco can be tested through electrophoretic mobility shift assays, ChIP-qPCR, or reporter constructs. Predicted spacing constraints can be validated by varying distances between motifs in synthetic constructs and measuring activity. Hypothesized enhancer-promoter connections can be tested through CRISPR deletion of predicted enhancers and measurement of target gene expression.</p>
<p>This experimental validation distinguishes genuine mechanistic discovery from pattern matching that happens to produce plausible-looking results. A model might learn that certain k-mers correlate with regulatory activity for confounded reasons (batch effects, mappability artifacts) yet produce motif logos resembling real transcription factors. Only experimental testing can determine whether model-derived hypotheses reflect causal regulatory logic.</p>
<p>High-throughput functional assays enable systematic validation at scale. Massively parallel reporter assays (MPRAs) can test thousands of model-predicted regulatory elements simultaneously. Perturb-seq combines CRISPR perturbations with single-cell RNA-seq to measure effects of knocking out predicted regulatory factors. These technologies create opportunities for iterative model improvement: interpretability generates hypotheses, experiments test them, and results refine both model architecture and training.</p>
<p><strong>Figure recommendation</strong>: A validation pipeline showing the loop from interpretability analysis through hypothesis generation, experimental testing, and model refinement. Include specific examples: TF-MoDISco motif → EMSA validation, attention-derived enhancer prediction → CRISPR deletion, probing-derived feature → reporter assay.</p>
</section>
</section>
<section id="interpretability-in-clinical-variant-assessment" class="level2" data-number="24.9">
<h2 data-number="24.9" class="anchored" data-anchor-id="interpretability-in-clinical-variant-assessment"><span class="header-section-number">24.9</span> Interpretability in Clinical Variant Assessment</h2>
<p>Variant interpretation guidelines require that computational predictions be weighed alongside experimental and clinical evidence, as discussed further in <a href="p6-ch26-rare-disease.html" class="quarto-xref"><span>Chapter 26</span></a>. Interpretability determines whether model predictions can contribute meaningful evidence beyond raw pathogenicity scores.</p>
<p>Current ACMG-AMP criteria allow computational evidence as supporting (PP3) or opposing (BP4) pathogenicity, but the evidence strength depends on understanding what the prediction reflects. A splice site disruption score from SpliceAI provides interpretable mechanistic evidence: the variant is predicted to alter splicing because it changes the consensus splice site sequence. This prediction can be evaluated against splice site models, tested with minigene assays, and combined with observations of aberrant transcripts in patient samples. The interpretation enables evidence integration.</p>
<p>Foundation model predictions are less immediately interpretable but potentially more informative. A pathogenicity score from ESM-1v reflects evolutionary constraint inferred from protein language modeling, but the specific sequence features driving the prediction require attribution analysis to identify. An expression effect predicted by Enformer might result from disrupted transcription factor binding, altered chromatin accessibility, or changed 3D regulatory contacts; interpretability analysis distinguishes these mechanisms and guides experimental validation.</p>
<p>For clinical utility, interpretability must be communicated effectively. Genome browsers displaying attribution tracks alongside variant calls help clinicians identify mechanistic hypotheses. Reports that accompany pathogenicity scores with regulatory vocabulary classifications (this variant shifts an enhancer toward a repressive state) provide actionable context. These communication challenges extend interpretability beyond algorithm development to user interface design and clinical workflow integration.</p>
</section>
<section id="practical-approaches-for-foundation-model-analysis" class="level2" data-number="24.10">
<h2 data-number="24.10" class="anchored" data-anchor-id="practical-approaches-for-foundation-model-analysis"><span class="header-section-number">24.10</span> Practical Approaches for Foundation Model Analysis</h2>
<p>Working with genomic foundation models requires matching interpretability methods to specific questions. Several complementary strategies address different aspects of model behavior.</p>
<p>For understanding variant effects, the primary goal is explaining why a specific variant receives a particular prediction. Attribution methods (ISM for validation, integrated gradients for efficiency) identify which input positions drive the difference between reference and alternative predictions. If the variant falls within a discovered motif, the interpretation is straightforward. If attributions spread across the sequence, the effect may operate through long-range regulatory changes requiring attention analysis or contribution scores from models like Enformer.</p>
<p>For characterizing model representations, probing classifiers diagnose what information is encoded and at which layers. Probing for known regulatory features (promoter versus enhancer, tissue specificity, evolutionary conservation) establishes which biological properties the model captures. Probing for potential confounders (GC content, distance to annotated genes, technical artifacts) identifies shortcuts that might inflate benchmark performance without reflecting genuine regulatory understanding.</p>
<p>For discovering regulatory logic, TF-MoDISco applied to high-confidence predictions extracts motif vocabularies specific to prediction tasks or cell types. Grammar analysis of motif co-occurrence reveals combinatorial rules. Sei-style sequence class analysis situates local motifs within global regulatory programs. Comparing discovered vocabularies across models or training conditions reveals shared versus idiosyncratic features.</p>
<p>For debugging and auditing, interpretability methods identify what features drive predictions in held-out distributions. If a model fails on a new cell type, attribution analysis can reveal whether it relies on cell-type-specific versus generalizable features. If performance degrades on specific genomic regions, local interpretability can identify confounding patterns or training data gaps.</p>
<p>For generating experimental hypotheses, interpretability produces testable predictions. Discovered motifs can be synthesized and tested. Predicted regulatory elements can be perturbed. Hypothesized TF binding can be validated by ChIP. Model-derived predictions that survive experimental testing represent genuine mechanistic insights; predictions that fail point toward model limitations or confounding.</p>
</section>
<section id="from-explanations-to-understanding" class="level2" data-number="24.11">
<h2 data-number="24.11" class="anchored" data-anchor-id="from-explanations-to-understanding"><span class="header-section-number">24.11</span> From Explanations to Understanding</h2>
<p>Interpretability for genomic foundation models is evolving from post hoc explanation toward model-assisted mechanistic discovery. Attribution methods identify important positions, motif discovery extracts regulatory vocabularies, probing characterizes learned representations, and mechanistic interpretability traces computational circuits. Combined with experimental validation, these tools transform black-box predictors into sources of testable biological hypotheses.</p>
<p>The distinction between plausibility and faithfulness remains central. Models can produce compelling motifs, structured attention patterns, and interpretable probing results while operating through mechanisms that do not correspond to biological reality. Only interventional experiments, both computational (deletion tests, counterfactual sequence generation) and biological (reporter assays, CRISPR perturbations), can distinguish genuine regulatory insight from sophisticated pattern matching.</p>
<p>As foundation models grow in scale and capability, interpretability becomes simultaneously more important and more challenging. Larger models implement more complex computations, potentially capturing subtler regulatory logic but resisting simple interpretation. Mechanistic interpretability offers a path forward by characterizing model internals directly, though scaling these techniques to billion-parameter genomic models remains an open problem.</p>
<p>The integration of interpretability with model development points toward a future where understanding and prediction advance together. Motifs and grammars discovered through interpretation can inform architecture design and training objectives. Experimentally validated hypotheses can become supervision signals for model refinement. Interpretability failures that reveal confounding can drive improvements in training data and evaluation protocols. In this vision, interpretability is not merely a tool for explaining existing models but a methodology for building models that embody genuine biological understanding, creating systems whose predictions we trust because we understand the mechanisms they have learned.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p5-ch23-uncertainty.html" class="pagination-link" aria-label="Uncertainty Quantification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p6--translation.html" class="pagination-link" aria-label="Part VI — Translation and Application">
        <span class="nav-page-text">Part VI — Translation and Application</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>