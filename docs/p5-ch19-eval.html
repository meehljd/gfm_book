<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>19&nbsp; Evaluation of Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p5-ch20-vep.html" rel="next">
<link href="./p5-ch18-benchmarks.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p5--eval-interp.html">Part V: Evaluation and Reliability</a></li><li class="breadcrumb-item"><a href="./p5-ch19-eval.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-pgs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS &amp; Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-cadd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Deleteriousness Scores</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-tokens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-foundation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretrain.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning &amp; Deployment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">CNN Sequence-to-Function Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RNA &amp; Transcript-Level Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-plm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-hybrid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Long-range Hybrid Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">p4–multi-modal_multi-scale.qmd</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-sc-epi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Single-Cell &amp; Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Graphs, Networks, and Biology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Multi-Omics &amp; Systems Biology</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch18-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch19-eval.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-vep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-confound.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Interpretability &amp; Mechanisms</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch23-clinical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch24-variants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Pathogenic Variant Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-drugs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Drug Discovery &amp; Biotech</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Future Work &amp; Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-model-list.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#evaluation-as-a-multi-scale-problem" id="toc-evaluation-as-a-multi-scale-problem" class="nav-link active" data-scroll-target="#evaluation-as-a-multi-scale-problem"><span class="header-section-number">19.1</span> Evaluation as a Multi-Scale Problem</a></li>
  <li><a href="#metric-families-across-genomic-tasks" id="toc-metric-families-across-genomic-tasks" class="nav-link" data-scroll-target="#metric-families-across-genomic-tasks"><span class="header-section-number">19.2</span> Metric Families Across Genomic Tasks</a>
  <ul class="collapse">
  <li><a href="#classification-metrics" id="toc-classification-metrics" class="nav-link" data-scroll-target="#classification-metrics"><span class="header-section-number">19.2.1</span> Classification Metrics</a></li>
  <li><a href="#regression-and-correlation-metrics" id="toc-regression-and-correlation-metrics" class="nav-link" data-scroll-target="#regression-and-correlation-metrics"><span class="header-section-number">19.2.2</span> Regression and Correlation Metrics</a></li>
  <li><a href="#ranking-and-prioritization-metrics" id="toc-ranking-and-prioritization-metrics" class="nav-link" data-scroll-target="#ranking-and-prioritization-metrics"><span class="header-section-number">19.2.3</span> Ranking and Prioritization Metrics</a></li>
  <li><a href="#generative-and-language-model-metrics" id="toc-generative-and-language-model-metrics" class="nav-link" data-scroll-target="#generative-and-language-model-metrics"><span class="header-section-number">19.2.4</span> Generative and Language Model Metrics</a></li>
  </ul></li>
  <li><a href="#levels-of-evaluation-from-base-pairs-to-bedside" id="toc-levels-of-evaluation-from-base-pairs-to-bedside" class="nav-link" data-scroll-target="#levels-of-evaluation-from-base-pairs-to-bedside"><span class="header-section-number">19.3</span> Levels of Evaluation: From Base Pairs to Bedside</a>
  <ul class="collapse">
  <li><a href="#molecular-and-regulatory-level-evaluation" id="toc-molecular-and-regulatory-level-evaluation" class="nav-link" data-scroll-target="#molecular-and-regulatory-level-evaluation"><span class="header-section-number">19.3.1</span> Molecular and Regulatory-Level Evaluation</a></li>
  <li><a href="#variant-level-evaluation" id="toc-variant-level-evaluation" class="nav-link" data-scroll-target="#variant-level-evaluation"><span class="header-section-number">19.3.2</span> Variant-Level Evaluation</a></li>
  <li><a href="#trait--and-individual-level-evaluation" id="toc-trait--and-individual-level-evaluation" class="nav-link" data-scroll-target="#trait--and-individual-level-evaluation"><span class="header-section-number">19.3.3</span> Trait- and Individual-Level Evaluation</a></li>
  <li><a href="#clinical-and-decision-level-evaluation" id="toc-clinical-and-decision-level-evaluation" class="nav-link" data-scroll-target="#clinical-and-decision-level-evaluation"><span class="header-section-number">19.3.4</span> Clinical and Decision-Level Evaluation</a></li>
  </ul></li>
  <li><a href="#data-splits-leakage-and-robustness" id="toc-data-splits-leakage-and-robustness" class="nav-link" data-scroll-target="#data-splits-leakage-and-robustness"><span class="header-section-number">19.4</span> Data Splits, Leakage, and Robustness</a>
  <ul class="collapse">
  <li><a href="#axes-of-splitting" id="toc-axes-of-splitting" class="nav-link" data-scroll-target="#axes-of-splitting"><span class="header-section-number">19.4.1</span> Axes of Splitting</a></li>
  <li><a href="#types-of-leakage" id="toc-types-of-leakage" class="nav-link" data-scroll-target="#types-of-leakage"><span class="header-section-number">19.4.2</span> Types of Leakage</a></li>
  <li><a href="#robustness-and-distribution-shift" id="toc-robustness-and-distribution-shift" class="nav-link" data-scroll-target="#robustness-and-distribution-shift"><span class="header-section-number">19.4.3</span> Robustness and Distribution Shift</a></li>
  </ul></li>
  <li><a href="#evaluating-foundation-models-zero-shot-probing-and-fine-tuning" id="toc-evaluating-foundation-models-zero-shot-probing-and-fine-tuning" class="nav-link" data-scroll-target="#evaluating-foundation-models-zero-shot-probing-and-fine-tuning"><span class="header-section-number">19.5</span> Evaluating Foundation Models: Zero-Shot, Probing, and Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#zero-shot-and-few-shot-evaluation" id="toc-zero-shot-and-few-shot-evaluation" class="nav-link" data-scroll-target="#zero-shot-and-few-shot-evaluation"><span class="header-section-number">19.5.1</span> Zero-Shot and Few-Shot Evaluation</a></li>
  <li><a href="#probing-and-linear-evaluation" id="toc-probing-and-linear-evaluation" class="nav-link" data-scroll-target="#probing-and-linear-evaluation"><span class="header-section-number">19.5.2</span> Probing and Linear Evaluation</a></li>
  <li><a href="#full-fine-tuning-and-task-specific-heads" id="toc-full-fine-tuning-and-task-specific-heads" class="nav-link" data-scroll-target="#full-fine-tuning-and-task-specific-heads"><span class="header-section-number">19.5.3</span> Full Fine-Tuning and Task-Specific Heads</a></li>
  </ul></li>
  <li><a href="#uncertainty-calibration-and-reliability" id="toc-uncertainty-calibration-and-reliability" class="nav-link" data-scroll-target="#uncertainty-calibration-and-reliability"><span class="header-section-number">19.6</span> Uncertainty, Calibration, and Reliability</a></li>
  <li><a href="#benchmarks-leaderboards-and-their-limits" id="toc-benchmarks-leaderboards-and-their-limits" class="nav-link" data-scroll-target="#benchmarks-leaderboards-and-their-limits"><span class="header-section-number">19.7</span> Benchmarks, Leaderboards, and Their Limits</a></li>
  <li><a href="#putting-it-all-together-an-evaluation-checklist" id="toc-putting-it-all-together-an-evaluation-checklist" class="nav-link" data-scroll-target="#putting-it-all-together-an-evaluation-checklist"><span class="header-section-number">19.8</span> Putting It All Together: An Evaluation Checklist</a></li>
  <li><a href="#looking-forward" id="toc-looking-forward" class="nav-link" data-scroll-target="#looking-forward"><span class="header-section-number">19.9</span> Looking Forward</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p5--eval-interp.html">Part V: Evaluation and Reliability</a></li><li class="breadcrumb-item"><a href="./p5-ch19-eval.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-eval" class="quarto-section-identifier"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>By this point in the book, we have seen genomic models deployed at almost every scale. Variant calling from NGS reads (<a href="p1-ch01-ngs.html" class="quarto-xref"><span>Chapter 1</span></a>), polygenic scores and GWAS (<a href="p1-ch03-pgs.html" class="quarto-xref"><span>Chapter 3</span></a>), deleteriousness scores and variant effect predictors (<a href="p1-ch04-cadd.html" class="quarto-xref"><span>Chapter 4</span></a>, <a href="p5-ch20-vep.html" class="quarto-xref"><span>Chapter 20</span></a>), CNN-based sequence-to-function models (<a href="p3-ch10-cnn.html#sec-reg" class="quarto-xref"><span>Section 10.1</span></a> through <a href="p3-ch10-cnn.html#sec-splice" class="quarto-xref"><span>Section 10.3</span></a>), and genomic language models and foundation models (<a href="p3-ch11-dna.html" class="quarto-xref"><span>Chapter 11</span></a>, <a href="p2-ch07-foundation.html" class="quarto-xref"><span>Chapter 7</span></a>) have each introduced their own metrics and benchmarks. Clinical risk prediction and pathogenic variant discovery (<a href="p6-ch23-clinical.html" class="quarto-xref"><span>Chapter 23</span></a>, <a href="p6-ch24-variants.html" class="quarto-xref"><span>Chapter 24</span></a>) add still more evaluation considerations. What has been missing is a single place to answer a deceptively simple question: what does it mean for a genomic model to work, and how should we systematically evaluate it?</p>
<p>This chapter provides that unifying view. We describe the major families of evaluation metrics and show how they map to typical genomic tasks. We organize evaluation across four levels, from molecular readouts through variant-level predictions to trait-level risk scores and finally to clinical decisions. We discuss data splitting, leakage, and robustness, the mechanics that make or break benchmarks regardless of how sophisticated the underlying architecture may be. We explain how to evaluate foundation models across different usage regimes, from zero-shot scoring through linear probing to full fine-tuning. Finally, we connect evaluation to the broader theme of reliability, linking forward to the detailed treatments of confounders in <a href="p5-ch21-confound.html" class="quarto-xref"><span>Chapter 21</span></a> and interpretability in <a href="p5-ch22-interp.html" class="quarto-xref"><span>Chapter 22</span></a>.</p>
<p>Throughout, the theme is that architecture and scale matter, but evaluation choices often matter more. A state-of-the-art model evaluated on a leaky benchmark tells us less than a modest model evaluated on a clean one. A foundation model that achieves impressive perplexity but fails to improve downstream variant interpretation has not demonstrated clinical utility. Getting evaluation right is prerequisite to knowing whether any of the sophisticated methods covered in this book actually work.</p>
<section id="evaluation-as-a-multi-scale-problem" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="evaluation-as-a-multi-scale-problem"><span class="header-section-number">19.1</span> Evaluation as a Multi-Scale Problem</h2>
<p>Genomic models are deployed at very different scales, and understanding this hierarchy is essential for designing appropriate evaluations. It helps to keep a simple mental pyramid in mind, with molecular readouts at the base and clinical decisions at the apex.</p>
<p>At the <strong>molecular and regulatory level</strong>, models take local sequence and epigenomic context as input and predict outputs such as chromatin accessibility, histone marks, transcription factor binding, splicing outcomes, or expression levels. Representative models at this level include DeepSEA-style chromatin predictors, SpliceAI for splice site prediction, and Enformer for long-range regulatory modeling. Evaluation here typically involves comparing predicted tracks or binary annotations against experimental measurements.</p>
<p>At the <strong>variant level</strong>, models take a specific variant (whether SNV, indel, or structural variant) and its surrounding context as input, producing outputs such as pathogenicity scores, predicted molecular impact, or fine-mapping posterior probabilities. Examples include CADD-style deleteriousness scores, AlphaMissense-like variant effect predictors, and Bayesian fine-mapping methods. Evaluation focuses on concordance with clinical annotations, allele frequency patterns, or experimental measurements of variant effects.</p>
<p>At the <strong>trait and individual level</strong>, models take a person’s genotype or sequence along with other features as input and produce risk scores for complex traits, predicted phenotypes, or endophenotypes. Classical polygenic scores and GFM-augmented risk models (<a href="p1-ch03-pgs.html" class="quarto-xref"><span>Chapter 3</span></a>, <a href="p6-ch23-clinical.html" class="quarto-xref"><span>Chapter 23</span></a>) operate at this level. Evaluation compares predicted risk against observed outcomes in held-out cohorts, often with attention to calibration and discrimination across ancestry groups.</p>
<p>At the <strong>clinical and decision level</strong>, the inputs are model predictions combined with contextual factors such as guidelines, utility assumptions, and patient preferences. The outputs are actual decisions: whether to treat or not treat, screen or not screen, include a patient in a trial or exclude them. Examples include screening strategies, clinical decision support tools, and trial enrichment protocols. Evaluation at this level requires moving beyond accuracy metrics to consider decision curves, net benefit, and prospective validation.</p>
<p>Good evaluation starts from the intended level of action. If the goal is variant prioritization in a rare disease pipeline, improvement in AUROC on a chromatin benchmark is only indirectly relevant. If the goal is clinical risk stratification, better perplexity on a DNA language model test set is useful only insofar as it leads to more discriminative, better calibrated risk scores. The rest of the chapter climbs this pyramid while keeping a few core metric families in view.</p>
</section>
<section id="metric-families-across-genomic-tasks" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="metric-families-across-genomic-tasks"><span class="header-section-number">19.2</span> Metric Families Across Genomic Tasks</h2>
<p>Most evaluation in this book falls into four broad metric families, each suited to different types of predictions and scientific questions.</p>
<section id="classification-metrics" class="level3" data-number="19.2.1">
<h3 data-number="19.2.1" class="anchored" data-anchor-id="classification-metrics"><span class="header-section-number">19.2.1</span> Classification Metrics</h3>
<p>For binary or multi-class outputs such as pathogenic versus benign, open versus closed chromatin, or presence versus absence of a histone mark, the standard metrics derive from the confusion matrix. The area under the receiver operating characteristic curve (AUROC or simply AUC) measures the probability that a randomly chosen positive example is ranked above a randomly chosen negative example, providing a threshold-independent summary of discrimination. The area under the precision-recall curve (AUPRC) is more informative when positives are rare, as is typically the case when identifying pathogenic variants among many benign ones or causal variants among many correlated candidates. Simple metrics like accuracy, sensitivity, and specificity are intuitive but sensitive to class imbalance and require choosing specific decision thresholds.</p>
<p>In practice, variant effect predictors and clinical risk models typically report AUROC and AUPRC for prioritization tasks. Regulatory prediction models often report per-task AUROC averaged over hundreds of chromatin assays, sometimes with weighting schemes that emphasize difficult or clinically relevant targets.</p>
</section>
<section id="regression-and-correlation-metrics" class="level3" data-number="19.2.2">
<h3 data-number="19.2.2" class="anchored" data-anchor-id="regression-and-correlation-metrics"><span class="header-section-number">19.2.2</span> Regression and Correlation Metrics</h3>
<p>For continuous outputs such as expression levels, log-odds of accessibility, or quantitative traits, the standard metrics measure association between predicted and observed values. Pearson correlation measures linear association, while Spearman correlation measures rank-based association and is robust to monotone transformations of the data. The coefficient of determination (<span class="math inline">\(R^2\)</span>) measures the fraction of variance explained, often computed against a simple baseline such as a mean-only model. Mean-squared error and root mean-squared error provide absolute measures of prediction error in the original units.</p>
<p>Sequence-to-expression models and multi-omics integrations frequently use correlation between predicted and observed tracks, as in Enformer-style evaluations that compare predicted and measured gene expression across cell types. Polygenic score performance is often reported as incremental <span class="math inline">\(R^2\)</span>, the additional variance explained by genomic features over and above clinical covariates.</p>
</section>
<section id="ranking-and-prioritization-metrics" class="level3" data-number="19.2.3">
<h3 data-number="19.2.3" class="anchored" data-anchor-id="ranking-and-prioritization-metrics"><span class="header-section-number">19.2.3</span> Ranking and Prioritization Metrics</h3>
<p>Many genomics workflows are fundamentally about ranking rather than absolute prediction. The goal may be to prioritize variants in a locus for follow-up, rank genes or targets for experimental validation, or select individuals at highest risk for screening. While AUROC and AUPRC capture some aspects of ranking quality, additional metrics can be more directly relevant.</p>
<p>Top-k recall or enrichment measures the fraction of true positives captured in the top k predictions, directly addressing questions like “how many real causal variants would land in our top 20 candidates?” Enrichment over baseline measures how much more likely a high-scoring bucket is to contain true positives compared to random expectation. Normalized discounted cumulative gain (NDCG) emphasizes getting highly relevant items near the top of the ranked list, with diminishing returns for items placed lower. These metrics often align better with practical questions about how predictions will actually be used in experimental workflows where resources limit validation efforts to a small number of top candidates.</p>
</section>
<section id="generative-and-language-model-metrics" class="level3" data-number="19.2.4">
<h3 data-number="19.2.4" class="anchored" data-anchor-id="generative-and-language-model-metrics"><span class="header-section-number">19.2.4</span> Generative and Language Model Metrics</h3>
<p>Self-supervised genomic language models (<a href="p3-ch11-dna.html" class="quarto-xref"><span>Chapter 11</span></a>) introduce their own metrics related to the pretraining objective. Perplexity and cross-entropy on masked-token reconstruction tasks measure how well the model predicts held-out sequence content. Bits-per-base for next-token prediction or compression-style objectives provides a related measure of the model’s ability to capture sequence statistics.</p>
<p>These metrics are important for assessing representation quality and for comparing pretraining runs, but they come with important caveats. They are distribution-specific, tied to the particular pretraining corpus and task, which limits comparability across models trained on different data. More importantly, improvements in perplexity do not automatically translate into better variant or trait predictions. A model might achieve excellent perplexity by capturing abundant patterns in the genome, such as repetitive elements and sequence composition, that are largely irrelevant for functional prediction. As a result, generative metrics should always be paired with downstream task metrics to assess real utility.</p>
</section>
</section>
<section id="levels-of-evaluation-from-base-pairs-to-bedside" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="levels-of-evaluation-from-base-pairs-to-bedside"><span class="header-section-number">19.3</span> Levels of Evaluation: From Base Pairs to Bedside</h2>
<p>We now walk through the pyramid from molecular readouts to clinical decisions, focusing on what good evaluation looks like at each level and the common pitfalls that can undermine it.</p>
<section id="molecular-and-regulatory-level-evaluation" class="level3" data-number="19.3.1">
<h3 data-number="19.3.1" class="anchored" data-anchor-id="molecular-and-regulatory-level-evaluation"><span class="header-section-number">19.3.1</span> Molecular and Regulatory-Level Evaluation</h3>
<p>At the molecular level, the core tasks include predicting chromatin accessibility, histone marks, and transcription factor binding profiles; predicting splicing outcomes such as percent spliced in (PSI) values or transcription start and termination sites; and predicting readouts from functional assays like massively parallel reporter assays (MPRAs) or CRISPR perturbation screens.</p>
<p>Common evaluation setups involve multi-task classification, where AUROC or AUPRC is computed for each assay and then averaged with or without weighting across assays. Track-wise regression computes Pearson or Spearman correlation between predicted and observed signal profiles across genomic positions. Out-of-cell-type prediction trains on some cell types and tests on others to assess generalization beyond the training distribution.</p>
<p>Several design choices shape the meaning of reported metrics. The granularity of labels matters: base-resolution predictions present a different challenge than predictions averaged over 128-base-pair bins. The size of context windows determines whether the evaluation tests local sequence features or long-range regulatory architecture. The definition of held-out biology, whether new transcription factors, new cell types, or entirely new genomic loci, determines what kind of generalization is actually being tested.</p>
<p>Common pitfalls include overfitting to specific assays or idiosyncratic lab protocols and inadvertent leakage when nearby genomic regions or replicate experiments are split across train and test sets. A model might appear to generalize to new regions while actually leveraging sequence similarity or chromatin context shared with training examples.</p>
<p>For noisy assays, reporting performance relative to replicate concordance provides important context. If technical replicates of the same experiment correlate at <span class="math inline">\(r = 0.85\)</span>, a model achieving <span class="math inline">\(r = 0.80\)</span> may be approaching the practical ceiling imposed by measurement noise. Reporting fraction of explainable variance, computed as the ratio of model performance to replicate concordance, can be more informative than raw correlation values.</p>
</section>
<section id="variant-level-evaluation" class="level3" data-number="19.3.2">
<h3 data-number="19.3.2" class="anchored" data-anchor-id="variant-level-evaluation"><span class="header-section-number">19.3.2</span> Variant-Level Evaluation</h3>
<p>At the variant level, tasks include classifying variants as pathogenic versus benign or damaging versus tolerated, predicting functional impact such as effects on splicing, expression, or protein stability, and fine-mapping to assign posterior probabilities of causality to variants in associated loci.</p>
<p>Common benchmarks derive from clinical labels in resources like ClinVar and HGMD, from curated variant sets assembled by diagnostic laboratories, from population-based labels using allele frequency strata in gnomAD-like resources, and from functional assays including saturation mutagenesis, MPRAs, and deep mutational scanning experiments. The choice of benchmark profoundly shapes what the evaluation measures, as discussed extensively in <a href="p5-ch18-benchmarks.html" class="quarto-xref"><span>Chapter 18</span></a>.</p>
<p>Metrics typically include AUROC and AUPRC on binary labels, correlation or rank metrics against experimental effect sizes, and calibration-style metrics for probabilistic outputs. Reliability diagrams for pathogenicity probabilities or fine-mapping posteriors assess whether variants scored at 80% pathogenic are truly pathogenic about 80% of the time.</p>
<p>Several design questions deserve careful attention. The definition of the negative class matters enormously: common and presumably benign variants, frequency-matched controls, synonymous variants, or synthetic negatives as in CADD (<a href="p1-ch04-cadd.html" class="quarto-xref"><span>Chapter 4</span></a>) each create different evaluation contexts with different biases. The choice of what is held out determines the kind of generalization being tested. Holding out entire genes tests whether the model has learned general principles about variant effects versus gene-specific patterns. Holding out specific loci tests whether the model can extrapolate to new genomic contexts. Holding out particular variant types (for example, only evaluating on frameshifts or splice-disrupting variants) tests whether the model has learned type-specific consequences.</p>
<p>For fine-mapping and similar tasks where multiple variants per locus compete for causal status, evaluating top-k recall of causal variants per risk locus is often more informative than global AUC across all variants. In practice, researchers follow up only a handful of variants per locus, so knowing that the causal variant consistently ranks in the top three is more valuable than knowing the model achieves high genome-wide discrimination.</p>
<p>This level is also where issues of circularity become especially acute. Scores trained on ClinVar and then evaluated on overlapping or highly correlated variants create feedback loops that inflate apparent performance without demonstrating real predictive power. Similarly, models that incorporate features derived from the same underlying data as the evaluation labels can appear to work well while providing no incremental utility beyond what those features already captured. We return to these problems in detail in <a href="p5-ch21-confound.html" class="quarto-xref"><span>Chapter 21</span></a>.</p>
</section>
<section id="trait--and-individual-level-evaluation" class="level3" data-number="19.3.3">
<h3 data-number="19.3.3" class="anchored" data-anchor-id="trait--and-individual-level-evaluation"><span class="header-section-number">19.3.3</span> Trait- and Individual-Level Evaluation</h3>
<p>At the trait and individual level, tasks include predicting quantitative traits such as LDL cholesterol, height, or estimated glomerular filtration rate from genotypes and other features, case-control risk prediction for complex diseases like coronary artery disease or type 2 diabetes, and multi-trait and multi-task risk modeling that jointly predicts related phenotypes.</p>
<p>For quantitative traits, incremental <span class="math inline">\(R^2\)</span> measures the variance explained by genomic features over and above clinical covariates, directly quantifying what genetics adds to prediction. For binary or time-to-event outcomes, AUROC, AUPRC, and the concordance index (C-index) measure discrimination. Net reclassification improvement (NRI) asks how often individuals are moved across clinically meaningful risk thresholds in the correct direction, a metric more directly tied to clinical utility than discrimination alone.</p>
<p>Important evaluation settings include within-ancestry versus cross-ancestry performance, building on the portability issues discussed in <a href="p1-ch03-pgs.html" class="quarto-xref"><span>Chapter 3</span></a>. Models trained predominantly on European ancestry cohorts often show substantial performance degradation when applied to African, East Asian, or admixed populations. Reporting ancestry-stratified metrics is now considered essential for polygenic score evaluations.</p>
<p>Within-cohort versus external validation compares models trained and tested in the same biobank against models validated in entirely separate cohorts with different recruitment, sequencing, and clinical practices. External validation provides stronger evidence of generalizability but is often not feasible until after model publication. When external validation is not possible, careful documentation of the training cohort’s characteristics and explicit caveats about likely performance in other settings become especially important.</p>
<p>Joint versus marginal contribution of genetics examines how much predictive information comes from genomic features when combined with electronic health records and other multi-omic data (<a href="p4-ch17-systems.html" class="quarto-xref"><span>Chapter 17</span></a>). A model that achieves high absolute performance might contribute little beyond what clinical variables already provide. Reporting both absolute metrics and incremental gains over strong non-genomic baselines is essential for understanding real impact.</p>
<p>Even for purely research models, reporting absolute performance alongside incremental gain over strong baselines is essential for understanding real impact. A polygenic score that achieves 0.65 AUROC for a disease sounds moderately impressive until one learns that clinical variables alone achieve 0.63 AUROC. The incremental value may still be scientifically interesting, but the practical utility for clinical decision-making is limited.</p>
</section>
<section id="clinical-and-decision-level-evaluation" class="level3" data-number="19.3.4">
<h3 data-number="19.3.4" class="anchored" data-anchor-id="clinical-and-decision-level-evaluation"><span class="header-section-number">19.3.4</span> Clinical and Decision-Level Evaluation</h3>
<p>Clinical risk models, treatment response predictors, and trial enrichment models (<a href="p6-ch23-clinical.html" class="quarto-xref"><span>Chapter 23</span></a>) ultimately need to be evaluated in terms of decisions, not just scores. Beyond discrimination and calibration, several additional concepts become important.</p>
<p>Decision curves and net benefit compare different decision thresholds or policies by weighting true positives versus false positives according to clinical utilities. A model that achieves high AUROC but offers no net benefit at clinically relevant thresholds has not demonstrated clinical value. The net benefit framework explicitly incorporates the relative costs of false positives versus false negatives, allowing evaluation to reflect real-world trade-offs in screening, diagnosis, or treatment decisions.</p>
<p>Cost-sensitive and utility-aware evaluation explicitly models different misclassification costs, recognizing that missing a high-risk patient has different consequences than unnecessary screening. For cancer screening, a false negative might delay diagnosis by years with substantial mortality consequences, while a false positive leads to additional imaging or biopsy. These asymmetric costs should be reflected in evaluation metrics and decision thresholds.</p>
<p>Prospective and interventional evaluation through randomized trials, pragmatic trials, and observational implementations with careful monitoring provides the strongest evidence for clinical utility but is expensive and time-consuming. Retrospective validation on historical data can identify promising models but cannot fully account for how clinician behavior, patient adherence, or health system workflows will change in response to model predictions. The gap between retrospective performance and prospective impact is often substantial.</p>
<p>This chapter provides only a high-level overview of clinical evaluation. <a href="p6-ch23-clinical.html" class="quarto-xref"><span>Chapter 23</span></a> goes deeper into clinical metrics and deployment considerations, while <a href="p6-ch24-variants.html" class="quarto-xref"><span>Chapter 24</span></a> discusses evaluation of variant-centric discovery workflows in rare disease and oncology settings.</p>
</section>
</section>
<section id="data-splits-leakage-and-robustness" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="data-splits-leakage-and-robustness"><span class="header-section-number">19.4</span> Data Splits, Leakage, and Robustness</h2>
<p>Metrics mean little without well-designed data splits. In genomics, the usual approach of randomly assigning 80% of examples to training, 10% to validation, and 10% to testing often fails to test the kind of generalization we actually care about. The structure of genomic data, with its hierarchical organization from bases to variants to individuals to populations, creates many opportunities for subtle information leakage.</p>
<section id="axes-of-splitting" class="level3" data-number="19.4.1">
<h3 data-number="19.4.1" class="anchored" data-anchor-id="axes-of-splitting"><span class="header-section-number">19.4.1</span> Axes of Splitting</h3>
<p>Several axes exist along which we can and often should split data. Splitting by individual ensures that genomes from the same person or family do not appear in both training and test sets, preventing models from memorizing individual-specific patterns. This is essential for trait prediction and clinical risk modeling but may be less relevant for purely sequence-based regulatory models that do not use individual-level labels.</p>
<p>Splitting by locus or region holds out contiguous genomic segments such as specific chromosomes or megabase windows, testing whether models can generalize to entirely new genomic contexts. Chromosome-based splits are common in regulatory genomics, where models trained on chromosomes 1 through 16 are tested on chromosomes 17 through 22. This approach reduces sequence similarity between train and test sets and forces models to rely on general principles rather than memorizing local patterns.</p>
<p>Splitting by gene or target holds out entire genes or protein families for variant effect and protein models, testing whether the model has learned general principles versus gene-specific idiosyncrasies. For example, a protein model evaluated on entirely held-out protein families provides stronger evidence of biological understanding than a model evaluated on random variants across all proteins, some of which may have close homologs in the training set.</p>
<p>Splitting by assay, cell type, or tissue trains on some experimental contexts and tests on unseen ones, assessing whether learned regulatory logic transfers across biological conditions. This is particularly relevant for multi-task models like Enformer that predict regulatory readouts across many cell types. Holding out entire cell types or tissue contexts tests whether the model has learned general regulatory principles or has simply memorized cell-type-specific patterns.</p>
<p>Splitting by ancestry or cohort trains in one population or recruitment setting and evaluates in others, testing whether models generalize across human diversity. This is essential for assessing model fairness and for understanding performance degradation in underrepresented populations. As discussed in <a href="p1-ch03-pgs.html" class="quarto-xref"><span>Chapter 3</span></a> and <a href="p5-ch21-confound.html" class="quarto-xref"><span>Chapter 21</span></a>, ancestry-aware evaluation has become a standard requirement for genomic risk prediction.</p>
<p>Different scientific questions imply different splitting strategies. The question “Can this model generalize to new loci in the same cell type?” calls for locus or chromosome-based splits. The question “Can it generalize to new cell types?” requires cell-type splits. The question “Can it generalize to different populations or clinical settings?” demands ancestry and cohort splits. Matching the split to the intended use case is essential for meaningful evaluation.</p>
</section>
<section id="types-of-leakage" class="level3" data-number="19.4.2">
<h3 data-number="19.4.2" class="anchored" data-anchor-id="types-of-leakage"><span class="header-section-number">19.4.2</span> Types of Leakage</h3>
<p>Leakage arises when information about the test set sneaks into training, inflating apparent performance without improving real-world generalization. Several forms of leakage are common in genomics.</p>
<p>Duplicate or near-duplicate sequences across splits can occur when overlapping windows around the same variant appear in both training and test sets. In regulatory genomics, if training windows overlap with test windows by even 100 base pairs, sequence similarity may allow models to effectively memorize test examples through their training neighbors. Careful attention to window boundaries and minimum separation distances between train and test regions is required.</p>
<p>Shared individuals or families across train and test can happen when different cohorts containing related individuals are combined without careful deduplication. Even distant relatives share genomic segments identical by descent, and models can exploit this structure if related individuals appear in both training and test partitions. Pedigree-aware splitting or explicit removal of relatives below a kinship threshold is necessary to prevent this leakage.</p>
<p>Benchmark construction leakage occurs when evaluation labels are derived from resources that also guided model design or pretraining. For example, if a foundation model is pretrained on all publicly available chromatin data including ENCODE, and then evaluated on ENCODE-derived benchmarks, the pretraining has seen information about the test distribution even if the exact test examples were held out. This subtle form of leakage is difficult to avoid entirely but should at least be acknowledged and quantified when possible.</p>
<p>Hyperparameter tuning leakage results from repeatedly evaluating on the test set while choosing checkpoints or model configurations, gradually overfitting to the test distribution. Best practice maintains a completely untouched final test set, using only training and validation data for all model development decisions. When iterative model development requires feedback, the validation set should be used for intermediate decisions, with the test set reserved for final reporting only.</p>
<p>The practical takeaway is straightforward in principle but demanding in practice: always define the split to match the generalization you care about, then audit carefully for potential linkage and dataset overlap. This often requires detailed provenance tracking of where every training and test example originated and whether any pathways exist for information to flow from test back to training.</p>
</section>
<section id="robustness-and-distribution-shift" class="level3" data-number="19.4.3">
<h3 data-number="19.4.3" class="anchored" data-anchor-id="robustness-and-distribution-shift"><span class="header-section-number">19.4.3</span> Robustness and Distribution Shift</h3>
<p>Robustness is evaluated by deliberately shifting the data distribution beyond what the model encountered during training. Technical shifts involve new sequencing platforms, different coverage levels, or altered assay protocols. A model trained on Illumina short-read data may perform poorly on PacBio long-read data if it has learned platform-specific artifacts. Similarly, models trained on high-coverage whole genome sequencing may degrade substantially when applied to lower-coverage exome sequencing.</p>
<p>Biological shifts involve new species, tissues, disease subtypes, or ancestry groups not represented in training. Cross-species evaluation tests whether regulatory logic learned from human data transfers to mouse or other model organisms. Cross-tissue evaluation tests whether a model trained on blood and brain can generalize to liver or kidney. Cross-ancestry evaluation tests whether patterns learned predominantly from European populations apply to African, East Asian, or admixed individuals.</p>
<p>Clinical shifts involve new hospitals, different care patterns, or later time periods with evolving patient populations and medical practices. A risk model trained on academic medical center data may perform differently in community hospitals with different patient demographics and care protocols. Temporal validation, where models trained on earlier time periods are evaluated on later ones, can reveal degradation due to changes in diagnostic coding, treatment guidelines, or population health trends.</p>
<p>Robustness evaluations typically involve training on one platform or cohort and testing on another, comparing performance across subgroups such as ancestry-stratified AUROC, and stress-testing models under label noise or missing data. These experiments often reveal that performance on curated, independently and identically distributed benchmarks overestimates usefulness in messy real-world settings, especially for high-stakes clinical decisions.</p>
<p>A model that performs well on curated benchmarks may still struggle in real-world deployment for several reasons. Population diversity issues arise when training corpora underrepresent certain ancestries, leading to biased variant scoring (<a href="p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a>, <a href="p5-ch21-confound.html" class="quarto-xref"><span>Chapter 21</span></a>). Assay heterogeneity means that experimental conditions, laboratories, and technologies in deployment differ from the curated datasets used in training. Different labs use different antibodies for ChIP-seq, different enzymes for ATAC-seq, and different sequencing depths, all of which can affect the mapping between sequence and measured regulatory activity. Phenotypic complexity reflects the reality that many clinically relevant phenotypes involve long causal chains from variant to molecular consequence to tissue-level effect to disease, and models may capture only part of this cascade.</p>
<p>For these reasons, genomic model evaluation increasingly includes cross-population robustness testing, out-of-distribution evaluation on new tissues, cell types, or species, and end-to-end assessments on clinically relevant endpoints often combined with traditional statistical genetics tools. Reporting performance stratified by potential sources of distribution shift has become expected practice, particularly for models intended for clinical deployment.</p>
</section>
</section>
<section id="evaluating-foundation-models-zero-shot-probing-and-fine-tuning" class="level2" data-number="19.5">
<h2 data-number="19.5" class="anchored" data-anchor-id="evaluating-foundation-models-zero-shot-probing-and-fine-tuning"><span class="header-section-number">19.5</span> Evaluating Foundation Models: Zero-Shot, Probing, and Fine-Tuning</h2>
<p>Genomic foundation models (<a href="p2-ch07-foundation.html" class="quarto-xref"><span>Chapter 7</span></a>) complicate evaluation because there are multiple ways to use them, each testing different aspects of the learned representations. The appropriate evaluation regime depends on the downstream application and available resources.</p>
<section id="zero-shot-and-few-shot-evaluation" class="level3" data-number="19.5.1">
<h3 data-number="19.5.1" class="anchored" data-anchor-id="zero-shot-and-few-shot-evaluation"><span class="header-section-number">19.5.1</span> Zero-Shot and Few-Shot Evaluation</h3>
<p>In zero-shot settings, we apply the pretrained model without any task-specific training. Examples include using masked-token probabilities to rank variants by predicted deleteriousness and using embedding similarities to cluster sequences or annotate motifs. Evaluation in this regime focuses on how well these raw scores correlate with functional or clinical labels and whether few-shot adaptation with small linear heads trained on limited labeled data already yields strong performance.</p>
<p>Zero-shot performance serves as a stress test of representation quality and inductive biases. Strong zero-shot performance suggests that the pretraining objective has captured biologically relevant structure that transfers without explicit supervision. For example, if masked language model probabilities on sequence variations correlate strongly with experimentally measured variant effects without any fine-tuning, this indicates that the model has internalized functional constraints during pretraining.</p>
<p>Weak zero-shot performance combined with strong fine-tuned performance suggests that pretraining provides useful initialization but the learned representations are not directly interpretable for the task. This pattern is common when the pretraining objective (for example, next-token prediction) differs substantially from the downstream task (for example, clinical pathogenicity classification). The representations may still be useful as features for subsequent learning, but they do not encode the target concept directly.</p>
<p>Few-shot evaluation examines how quickly models can adapt to new tasks with minimal labeled data. This is particularly relevant for rare diseases, underrepresented populations, or novel assays where large labeled datasets are impractical. If a foundation model can achieve competitive performance with 100 labeled examples while a model trained from scratch requires 10,000 examples, this demonstrates substantial practical value even if the final saturated performance is similar.</p>
</section>
<section id="probing-and-linear-evaluation" class="level3" data-number="19.5.2">
<h3 data-number="19.5.2" class="anchored" data-anchor-id="probing-and-linear-evaluation"><span class="header-section-number">19.5.2</span> Probing and Linear Evaluation</h3>
<p>A common evaluation pattern freezes the foundation model, extracts embeddings for sequences, variants, or loci, and trains simple probes such as linear models or shallow MLPs on downstream labels. This approach isolates the usefulness of learned representations from the model’s capacity to adapt during fine-tuning.</p>
<p>Key evaluation questions in the probing regime include how much label efficiency is gained compared to training from scratch, how stable probe results are across random seeds and small dataset variations, and whether probes perform well across diverse tasks or only on those similar to the pretraining objectives. Linear probing provides a clean measure of how much useful information is linearly decodable from model representations.</p>
<p>Layer-wise probing analysis can reveal how information is organized within the model. Early layers of a genomic transformer might encode local motifs and k-mer statistics, while deeper layers encode more abstract patterns like regulatory grammar or evolutionary constraints. Observing which layers are most informative for which tasks provides insight into the model’s internal representations and can guide feature extraction for downstream applications.</p>
<p>Probing also enables diagnosing failure modes. If a task requires information that should be present in the training data but probes fail to decode it, this suggests either that the pretraining objective did not incentivize learning that information or that it is encoded in a non-linear or distributed way that simple probes cannot access. This diagnostic capability makes probing valuable both for understanding models and for improving them.</p>
</section>
<section id="full-fine-tuning-and-task-specific-heads" class="level3" data-number="19.5.3">
<h3 data-number="19.5.3" class="anchored" data-anchor-id="full-fine-tuning-and-task-specific-heads"><span class="header-section-number">19.5.3</span> Full Fine-Tuning and Task-Specific Heads</h3>
<p>For high-value tasks, practitioners often fine-tune the foundation model end-to-end, adding task-specific heads for classification, regression, or ranking and adapting to new modalities or clinical contexts. Evaluation then looks similar to classic deep model evaluation but with additional questions specific to the foundation model paradigm.</p>
<p>Transfer versus from-scratch baselines ask whether fine-tuning a foundation model meaningfully outperforms training a comparable architecture from scratch on the same downstream data. If the fine-tuned foundation model and from-scratch baseline converge to similar performance given sufficient data, the primary benefit of pretraining is data efficiency rather than improved final performance. This distinction matters for resource allocation: if labeled data are abundant, pretraining may offer limited advantage, but if labeled data are scarce, pretraining can be essential.</p>
<p>Catastrophic forgetting asks whether fine-tuning degrades performance on other tasks, and whether that degradation matters for the intended use. A model fine-tuned aggressively on pathogenicity prediction might lose the ability to predict splicing effects or regulatory activity. If the deployment scenario requires multi-task performance, techniques to mitigate forgetting such as multi-task fine-tuning or parameter-efficient adaptation become important.</p>
<p>Robustness and fairness ask whether foundation model features inherit or amplify biases present in the pretraining data or introduced during fine-tuning. If pretraining data are dominated by European ancestry samples, fine-tuning on a more diverse dataset may not fully overcome the ancestral imbalance in the learned representations. Explicit evaluation of performance across demographic groups is necessary to detect and mitigate these issues.</p>
<p>Across all evaluation regimes, it is helpful to report absolute performance, the delta compared to strong baselines, and data efficiency curves showing how performance varies with the amount of labeled data. This comprehensive reporting reveals whether pretraining provides genuine benefit or merely matches well-tuned task-specific models. Data efficiency curves are particularly informative: plotting performance as a function of training set size often shows that foundation models achieve with 1% of the data what from-scratch models require 10% or more to achieve, even when both eventually converge to similar asymptotic performance.</p>
</section>
</section>
<section id="uncertainty-calibration-and-reliability" class="level2" data-number="19.6">
<h2 data-number="19.6" class="anchored" data-anchor-id="uncertainty-calibration-and-reliability"><span class="header-section-number">19.6</span> Uncertainty, Calibration, and Reliability</h2>
<p>Metrics like AUROC summarize ranking quality but say little about how trustworthy individual predictions are. For many applications, especially those involving clinical decisions, we care not only about whether the model is correct on average but also about whether its confidence estimates are meaningful.</p>
<p>Calibration refers to the property that predicted probabilities match observed frequencies. A variant scored at 0.8 probability of being pathogenic should truly be pathogenic about 80% of the time. Well-calibrated models support rational decision-making because the probability scores can be interpreted at face value. Poorly calibrated models, even if they rank examples correctly, provide misleading confidence estimates that can lead to inappropriate decisions.</p>
<p>The distinction between epistemic and aleatoric uncertainty is also important. Epistemic uncertainty arises from limited data and could in principle be reduced by gathering more training examples. A model might express high epistemic uncertainty about variants in an underrepresented ancestral population simply because the training data contained few similar examples. Aleatoric uncertainty reflects inherent noise in the problem and cannot be reduced by additional data. Measurement noise in assays, stochastic biological processes, and incomplete penetrance of genetic variants all contribute to aleatoric uncertainty.</p>
<p>Models that can distinguish these uncertainty types provide more actionable predictions, flagging cases where more data might help versus cases where uncertainty is irreducible. Ensemble methods, Bayesian neural networks, and other uncertainty quantification techniques can decompose total predictive uncertainty into epistemic and aleatoric components, though these decompositions are approximate and depend on modeling assumptions.</p>
<p>Selective prediction or abstention allows models to say “I don’t know” when confidence is low, focusing predictions on cases where the model is reliable. This capability is particularly valuable in clinical settings where the cost of errors is high. A variant interpretation tool that abstains on 20% of variants but achieves 99% accuracy on the remaining 80% may be more useful than a tool that attempts to classify all variants at 90% accuracy, because the high-confidence predictions can be trusted while the abstained cases are flagged for manual review.</p>
<p>Evaluation tools for uncertainty and calibration include reliability diagrams that plot predicted probabilities against observed frequencies, Brier scores that combine calibration and discrimination in a single metric, and calibration curves stratified by subgroup to identify differential calibration across ancestry, sex, or clinical site. Coverage versus accuracy curves for selective prediction show how accuracy changes as the model restricts predictions to increasingly confident cases: if the model predicts only on the 50% most confident samples, how accurate is it?</p>
<p>Reliability diagrams are constructed by binning predictions into intervals (for example, 0 to 0.1, 0.1 to 0.2, and so on), computing the mean predicted probability and empirical frequency within each bin, and plotting one against the other. A perfectly calibrated model produces points along the diagonal. Systematic deviations reveal patterns of over-confidence (predictions above the diagonal) or under-confidence (predictions below the diagonal).</p>
<p>Expected calibration error (ECE) provides a scalar summary by computing the weighted average absolute difference between predicted probabilities and empirical frequencies across bins. Lower ECE indicates better calibration. However, ECE is sensitive to bin size and binning strategy, so it should be reported alongside reliability diagrams for interpretability.</p>
<p>For clinical risk models, <a href="p6-ch23-clinical.html" class="quarto-xref"><span>Chapter 23</span></a> covers calibration and uncertainty in more depth. For variant-centric tasks, similar tools apply to pathogenicity probabilities or fine-mapping posteriors, which must be interpreted cautiously in light of confounders discussed in <a href="p5-ch21-confound.html" class="quarto-xref"><span>Chapter 21</span></a>. Even well-calibrated models can give misleading risk estimates if the training data systematically differ from the deployment population in ways that affect the relationship between features and outcomes.</p>
</section>
<section id="benchmarks-leaderboards-and-their-limits" class="level2" data-number="19.7">
<h2 data-number="19.7" class="anchored" data-anchor-id="benchmarks-leaderboards-and-their-limits"><span class="header-section-number">19.7</span> Benchmarks, Leaderboards, and Their Limits</h2>
<p>Benchmark suites such as those introduced for Nucleotide Transformer and related genomic language models (see <a href="p5-ch18-benchmarks.html" class="quarto-xref"><span>Chapter 18</span></a>) serve important roles in the field. They provide standardized datasets, metrics, and splits that enable apples-to-apples comparisons between architectures. They encourage reproducibility by defining shared baselines against which progress can be measured. Well-designed benchmarks can accelerate progress by focusing community effort on common challenges and facilitating rapid iteration on model architectures.</p>
<p>However, benchmark-centric culture has well-documented pitfalls. Overfitting to the benchmark can occur when models are tuned aggressively on a small panel of tasks, achieving impressive headline numbers while degrading on tasks outside the benchmark. This is particularly problematic when the same benchmark is used for both model development and final evaluation, creating incentives to optimize for benchmark-specific quirks rather than general capability.</p>
<p>Narrow task coverage is common. Many existing suites focus on chromatin and transcription factor binding while underrepresenting splicing, structural variation, or clinical endpoints. A model that achieves state-of-the-art performance on chromatin benchmarks may perform no better than baselines on splice site prediction or pathogenicity classification. Relying solely on benchmark rankings without examining task-specific performance can give a misleading picture of model capabilities.</p>
<p>Misaligned incentives can emerge when the community prizes fractional improvements in AUROC over more important but harder-to-measure gains in robustness, calibration, or fairness. A model that improves AUROC from 0.89 to 0.90 on a saturated benchmark may receive more attention than a model that maintains 0.88 AUROC while dramatically improving calibration, cross-ancestry performance, and uncertainty quantification. Yet the latter may be far more valuable for real-world deployment.</p>
<p>Good practice treats benchmark scores as necessary but not sufficient evidence of model quality. They should be complemented with task-specific evaluations that mirror the intended downstream usage. Benchmarks should be periodically refreshed to include new assays, ancestries, and edge cases that stress-test models in new ways. The goal is to use benchmarks as a starting point for evaluation rather than as the final word on model quality.</p>
<p>Benchmark deprecation should also be considered when performance saturates or when evaluation becomes dominated by optimization tricks rather than scientific advances. The computer vision community has grappled with this issue as models have saturated classic benchmarks like ImageNet, leading to the introduction of more challenging variants and out-of-distribution evaluation protocols. Genomics may face similar needs as foundation models mature.</p>
<p>When reporting benchmark results, providing confidence intervals, reporting performance on multiple random seeds, and conducting ablation studies that isolate the contribution of different model components all strengthen the evidence for genuine progress. Transparency about what was and was not tuned on the benchmark helps readers interpret results critically.</p>
</section>
<section id="putting-it-all-together-an-evaluation-checklist" class="level2" data-number="19.8">
<h2 data-number="19.8" class="anchored" data-anchor-id="putting-it-all-together-an-evaluation-checklist"><span class="header-section-number">19.8</span> Putting It All Together: An Evaluation Checklist</h2>
<p>When designing or reviewing an evaluation for a genomic model, walking through a systematic checklist can help identify gaps and potential problems.</p>
<p>The first question concerns the <strong>level of decision</strong>. Is the model intended for molecular assay design, variant prioritization, patient risk stratification, or clinical action? The answer should determine which metrics are reported and how they are interpreted. Enrichment metrics make sense for variant ranking. Net benefit matters for clinical decisions. Choosing metrics aligned with the actual decision context ensures that evaluation measures what matters.</p>
<p>The second question concerns <strong>baselines</strong>. What are the comparison points? Strong non-deep baselines like logistic regression and classical polygenic scores establish floors that any sophisticated model should exceed. Prior deep models such as DeepSEA, SpliceAI, Enformer, and earlier foundation models establish the relevant state of the art. Reporting both absolute performance and gains over these baselines provides necessary context. A model achieving 0.85 AUROC might represent substantial progress if baselines are at 0.70, or minimal progress if baselines are at 0.83.</p>
<p>The third question concerns <strong>split design</strong>. Are individuals, loci, genes, assays, and ancestries appropriately separated between training and test sets? Is there any plausible path for leakage or circularity? These questions require careful auditing of data provenance and split construction. Documenting exactly how splits were constructed and what overlap checks were performed builds confidence in evaluation validity.</p>
<p>The fourth question concerns <strong>robustness</strong>. How does performance vary across cohorts, ancestries, platforms, and time? How does the model behave under label noise or missing data? Robustness evaluations reveal whether benchmark performance translates to real-world utility. Reporting stratified metrics by subgroup and evaluating on external datasets from different sources tests whether apparent performance generalizes beyond the specific training distribution.</p>
<p>The fifth question concerns <strong>uncertainty and calibration</strong>. For probabilistic outputs, are calibration and decision-level trade-offs reported? Are subgroup-specific metrics examined to identify differential performance across populations? Models deployed in high-stakes settings require not just good average performance but reliable confidence estimates that support appropriate action.</p>
<p>The sixth question concerns <strong>usage regimes for foundation models</strong>. How does the model perform in zero-shot, probing, and fine-tuning settings? Does pretraining help when labeled data are scarce, as measured by data efficiency curves? Understanding where the value of pretraining comes from (better final performance, faster convergence, improved data efficiency) helps determine whether the investment in large-scale pretraining is justified.</p>
<p>The seventh question concerns the <strong>story beyond the benchmark</strong>. Does improved performance actually change downstream decisions or experimental design? For models intended for clinical deployment, are there plans for prospective or interventional evaluation? The ultimate test of model utility is whether it enables better science or better care, not just higher numbers on a leaderboard.</p>
<p>This checklist is not exhaustive but covers the most common evaluation pitfalls in genomics. Working through it systematically at the design stage can prevent problems that are difficult or impossible to fix after the fact. Reviewers and readers can use the same checklist to critically evaluate published work.</p>
</section>
<section id="looking-forward" class="level2" data-number="19.9">
<h2 data-number="19.9" class="anchored" data-anchor-id="looking-forward"><span class="header-section-number">19.9</span> Looking Forward</h2>
<p>This chapter has provided a framework for thinking about evaluation across the full range of genomic models. The subsequent chapters flesh out specific aspects of reliability that evaluation alone cannot address.</p>
<p><a href="p5-ch21-confound.html" class="quarto-xref"><span>Chapter 21</span></a> examines confounders, bias, and fairness in detail, showing how evaluation can mislead when data are structured in problematic ways. Population stratification, batch effects, label circularity, and benchmark leakage can all create illusions of performance that evaporate in deployment. Understanding these failure modes is essential for interpreting evaluation results critically.</p>
<p><a href="p5-ch22-interp.html" class="quarto-xref"><span>Chapter 22</span></a> focuses on interpretability and mechanisms, turning models from black boxes into sources of testable biological hypotheses. When evaluation shows that a model works, interpretability helps us understand why it works and whether the reasons are biologically meaningful or artifacts of confounded data.</p>
<p>Together, these chapters aim to equip readers with the critical perspective needed to engage with the emerging literature on genomic foundation models. The question is never simply “what is the AUROC?” but rather “what has really been demonstrated, and how much should we trust it?” With careful attention to evaluation design, data splitting, robustness testing, and calibration assessment, we can distinguish models that represent genuine advances from those that merely perform well on convenient benchmarks.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p5-ch18-benchmarks.html" class="pagination-link" aria-label="Benchmarks for Genomic Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p5-ch20-vep.html" class="pagination-link" aria-label="Variant Effect Prediction">
        <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>