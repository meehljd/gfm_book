<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Convolutional Networks – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_2/p2-ch07-attention.html" rel="next">
<link href="../part_2/p2-ch05-representations.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch06-cnn.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch12-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch14-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch15-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch16-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch17-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--cellular-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Cellular Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch19-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch20-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch21-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch22-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch25-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Causal Inference with Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch26-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch28-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch29-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch30-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch31-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch06-convolutions" id="toc-sec-ch06-convolutions" class="nav-link active" data-scroll-target="#sec-ch06-convolutions"><span class="header-section-number">6.1</span> Convolutions as Sequence Pattern Detectors</a></li>
  <li><a href="#sec-ch06-deepsea" id="toc-sec-ch06-deepsea" class="nav-link" data-scroll-target="#sec-ch06-deepsea"><span class="header-section-number">6.2</span> <em>DeepSEA</em>: Regulatory Prediction from Sequence</a>
  <ul class="collapse">
  <li><a href="#sec-ch06-deepsea-architecture" id="toc-sec-ch06-deepsea-architecture" class="nav-link" data-scroll-target="#sec-ch06-deepsea-architecture"><span class="header-section-number">6.2.1</span> Architecture and Training</a></li>
  <li><a href="#sec-ch06-deepsea-validation" id="toc-sec-ch06-deepsea-validation" class="nav-link" data-scroll-target="#sec-ch06-deepsea-validation"><span class="header-section-number">6.2.2</span> Learned Representations and Biological Validation</a></li>
  <li><a href="#sec-ch06-deepsea-vep" id="toc-sec-ch06-deepsea-vep" class="nav-link" data-scroll-target="#sec-ch06-deepsea-vep"><span class="header-section-number">6.2.3</span> Variant Effect Prediction</a></li>
  </ul></li>
  <li><a href="#sec-ch06-basset" id="toc-sec-ch06-basset" class="nav-link" data-scroll-target="#sec-ch06-basset"><span class="header-section-number">6.3</span> Cell-Type Specificity and Regulatory Grammar</a></li>
  <li><a href="#sec-ch06-expecto" id="toc-sec-ch06-expecto" class="nav-link" data-scroll-target="#sec-ch06-expecto"><span class="header-section-number">6.4</span> <em>ExPecto</em>: From Chromatin to Expression</a>
  <ul class="collapse">
  <li><a href="#sec-ch06-expecto-architecture" id="toc-sec-ch06-expecto-architecture" class="nav-link" data-scroll-target="#sec-ch06-expecto-architecture"><span class="header-section-number">6.4.1</span> Modular Architecture</a></li>
  <li><a href="#sec-ch06-expecto-validation" id="toc-sec-ch06-expecto-validation" class="nav-link" data-scroll-target="#sec-ch06-expecto-validation"><span class="header-section-number">6.4.2</span> Expression Prediction and Variant Effects</a></li>
  </ul></li>
  <li><a href="#sec-ch06-spliceai" id="toc-sec-ch06-spliceai" class="nav-link" data-scroll-target="#sec-ch06-spliceai"><span class="header-section-number">6.5</span> <em>SpliceAI</em>: Clinical-Grade Splicing Prediction</a>
  <ul class="collapse">
  <li><a href="#sec-ch06-spliceai-architecture" id="toc-sec-ch06-spliceai-architecture" class="nav-link" data-scroll-target="#sec-ch06-spliceai-architecture"><span class="header-section-number">6.5.1</span> Architecture: Depth and Dilation</a></li>
  <li><a href="#sec-ch06-spliceai-performance" id="toc-sec-ch06-spliceai-performance" class="nav-link" data-scroll-target="#sec-ch06-spliceai-performance"><span class="header-section-number">6.5.2</span> Performance and Validation</a></li>
  <li><a href="#sec-ch06-spliceai-clinical" id="toc-sec-ch06-spliceai-clinical" class="nav-link" data-scroll-target="#sec-ch06-spliceai-clinical"><span class="header-section-number">6.5.3</span> Clinical Impact</a></li>
  </ul></li>
  <li><a href="#sec-ch06-receptive-field" id="toc-sec-ch06-receptive-field" class="nav-link" data-scroll-target="#sec-ch06-receptive-field"><span class="header-section-number">6.6</span> Receptive Field Ceiling</a></li>
  <li><a href="#sec-ch06-sequential" id="toc-sec-ch06-sequential" class="nav-link" data-scroll-target="#sec-ch06-sequential"><span class="header-section-number">6.7</span> Sequential Approaches and Their Costs</a>
  <ul class="collapse">
  <li><a href="#sec-ch06-vanishing-gradient" id="toc-sec-ch06-vanishing-gradient" class="nav-link" data-scroll-target="#sec-ch06-vanishing-gradient"><span class="header-section-number">6.7.1</span> Vanishing Gradient Problem</a></li>
  <li><a href="#sec-ch06-danq" id="toc-sec-ch06-danq" class="nav-link" data-scroll-target="#sec-ch06-danq"><span class="header-section-number">6.7.2</span> <em>DanQ</em>: Combining Convolutions and Recurrence</a></li>
  <li><a href="#sec-ch06-sequential-bottleneck" id="toc-sec-ch06-sequential-bottleneck" class="nav-link" data-scroll-target="#sec-ch06-sequential-bottleneck"><span class="header-section-number">6.7.3</span> Sequential Bottleneck</a></li>
  </ul></li>
  <li><a href="#sec-ch06-specialization" id="toc-sec-ch06-specialization" class="nav-link" data-scroll-target="#sec-ch06-specialization"><span class="header-section-number">6.8</span> Specialization and Its Limits</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch06-cnn.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch06-cnn" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>The network discovered what biologists had spent decades cataloging—without ever being told what to look for.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled" title="Chapter Overview">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Estimated reading time:</strong> 35-45 minutes</p>
<p><strong>Prerequisites:</strong> Understanding of one-hot encoding and tokenization (see <a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>), basic familiarity with neural network concepts (layers, activation functions, gradient descent). No prior deep learning architecture knowledge required.</p>
<p><strong>You will learn:</strong></p>
<ul>
<li>How convolutional filters detect sequence patterns analogous to transcription factor motifs</li>
<li>The end-to-end learning paradigm: from raw sequence to regulatory predictions</li>
<li>How <em>DeepSEA</em>, <em>Basset</em>, <em>ExPecto</em>, and <em>SpliceAI</em> transformed variant interpretation</li>
<li>Why receptive field limitations constrain what CNNs can learn about long-range regulation</li>
<li>The tradeoffs between task-specific and general-purpose genomic models</li>
</ul>
<p><strong>Key insight:</strong> Convolutional networks proved that gradient descent on raw DNA sequence could discover regulatory patterns without encoding human assumptions about what matters. This paradigm, not any specific architecture, was the lasting contribution that modern foundation models inherit.</p>
</div>
</div>
<p>In 2015, a <strong>convolutional neural network</strong> trained on ENCODE chromatin data learned to recognize transcription factor binding motifs that matched entries in the JASPAR database, despite never seeing those motifs during training <span class="citation" data-cites="zhou_deepsea_2015">(<a href="../bib/references.html#ref-zhou_deepsea_2015" role="doc-biblioref">Zhou and Troyanskaya 2015</a>)</span>. Through gradient descent on raw sequence, the model found the same patterns that experimental biologists had painstakingly assembled into curated databases. This was not merely a demonstration that deep learning could match human-curated databases. The deeper insight was that learned representations could transcend existing annotations: predicting regulatory effects for any sequence, in any genomic context, including regions never assayed in any experiment. For the first time, computational methods could move beyond annotating known regulatory elements to predicting the functional consequences of sequence variation genome-wide.</p>
<p>The early convolutional models established paradigms that persist in modern genomic AI. <em>DeepSEA</em> demonstrated that CNNs could predict chromatin marks and transcription factor binding directly from DNA sequence, enabling variant effect prediction without requiring experimental measurements for every variant of interest. <em>Basset</em> extended this approach to chromatin accessibility across cell types, learning representations that transferred to new cellular contexts. <em>SpliceAI</em> achieved clinical-grade accuracy for splice site prediction, demonstrating that deep learning could match or exceed hand-crafted algorithms developed over decades. Each model followed a common pattern: train on functional genomics data, learn sequence features through convolutional filters, and apply to variant interpretation. The success was substantial; the paradigm seemed complete.</p>
<p>Yet these models revealed a fundamental architectural limitation. Convolutional networks integrate information only within their <strong>receptive fields</strong>, the local region of input that contributes to each output position. Genomic regulation routinely operates across distances that exceed practical receptive field sizes: enhancers control genes across tens of kilobases, topologically associating domains span megabases, GWAS variants often lie far from the genes they affect. A model analyzing a variant 50 kilobases from a gene promoter cannot connect the variant to its target using local convolutions alone. Understanding both what CNNs achieved and where they reached this architectural ceiling establishes the foundation for the attention mechanisms examined in <a href="p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>.</p>
<section id="sec-ch06-convolutions" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="sec-ch06-convolutions"><span class="header-section-number">6.1</span> Convolutions as Sequence Pattern Detectors</h2>
<div class="callout callout-style-default callout-tip callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading about how convolutions work, consider: if you wanted to detect a transcription factor binding site (typically 6-12 nucleotides) at any position along a DNA sequence, what properties would your detection algorithm need? How would it differ from simply searching for an exact sequence match?</p>
</div>
</div>
<p>A variant in an enhancer 50 kilobases from its target gene cannot be connected to that gene by a model that sees only 1,000 base pairs of context. Consider a patient with familial hypercholesterolemia whose whole-genome sequencing reveals a novel variant upstream of <em>LDLR</em>. The variant sits within a known enhancer region, but the enhancer and the <em>LDLR</em> promoter lie beyond the window any convolutional layer can span. The model might correctly identify regulatory features at the variant position, but it cannot learn that those features regulate <em>LDLR</em> rather than some other gene. This receptive field constraint, inherent to convolutional architectures, determines what relationships these networks can and cannot discover. The constraint is not a limitation of training data or compute; it is architectural.</p>
<p>A convolutional filter slides across an input sequence, computing similarity scores at each position. For genomic applications, the input is typically <strong>one-hot encoded</strong> DNA: a binary matrix with four rows (A, C, G, T) and columns for each position (see <a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a> for detailed treatment of sequence encoding strategies). Filters learn weight patterns that respond to specific nucleotide arrangements. A filter of width 8 nucleotides, for instance, computes a weighted sum of the underlying nucleotides at each position, producing high activation when the sequence matches its learned pattern and low activation otherwise. This operation is mathematically equivalent to scanning a position weight matrix across the sequence, but with a crucial difference: the filter weights are learned during training rather than derived from aligned binding site sequences.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Insight">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>Convolutional filters are learned position weight matrices. The difference from classical PWMs is not in what they compute, but in how they are derived: through gradient descent on prediction tasks rather than sequence alignment. This means filters discover patterns that predict the training labels, not just patterns that recur in sequences.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Worked Example: Convolution Computation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Worked Example: Convolution Computation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a filter of width 4 learning to detect the TATA box motif. Why width 4? The TATA box consensus is approximately 4-6 bp, so a 4-bp filter can capture its core pattern. Filters narrower than the motif miss context; filters much wider waste capacity on flanking sequence that varies between binding sites. The first CNN layer typically uses multiple filter widths (4, 8, 12, 16 bp) to capture motifs of different lengths simultaneously—transcription factor binding sites range from 6-12 bp, so no single width suffices.</p>
<p>After training, the filter weights might be:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Position</th>
<th>A</th>
<th>C</th>
<th>G</th>
<th>T</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>-0.2</td>
<td>-0.5</td>
<td>-0.5</td>
<td><strong>1.8</strong></td>
</tr>
<tr class="even">
<td>2</td>
<td><strong>1.5</strong></td>
<td>-0.3</td>
<td>-0.4</td>
<td>-0.2</td>
</tr>
<tr class="odd">
<td>3</td>
<td>-0.2</td>
<td>-0.5</td>
<td>-0.5</td>
<td><strong>1.9</strong></td>
</tr>
<tr class="even">
<td>4</td>
<td><strong>1.6</strong></td>
<td>-0.4</td>
<td>-0.3</td>
<td>-0.3</td>
</tr>
</tbody>
</table>
<p><strong>For input sequence “TATA”:</strong></p>
<ul>
<li>Position 1 (T): weight = 1.8</li>
<li>Position 2 (A): weight = 1.5</li>
<li>Position 3 (T): weight = 1.9</li>
<li>Position 4 (A): weight = 1.6</li>
<li><strong>Total activation: 6.8</strong> (high match)</li>
</ul>
<p><strong>For input sequence “GCGC”:</strong></p>
<ul>
<li>Position 1 (G): weight = -0.5</li>
<li>Position 2 (C): weight = -0.3</li>
<li>Position 3 (G): weight = -0.5</li>
<li>Position 4 (C): weight = -0.4</li>
<li><strong>Total activation: -1.7</strong> (low match)</li>
</ul>
<p>The filter produces high activation for sequences matching its learned pattern (TATA) and low/negative activation for mismatches. As this filter slides across a long sequence, it produces a profile of TATA box likelihood at each position.</p>
</div>
</div>
<p>The first layer of a genomic CNN typically contains hundreds of such filters, each learning to detect different local patterns. Analysis of trained filters consistently reveals correspondence to known transcription factor binding motifs. The CTCF insulator motif, the ETS family consensus sequence, the AP-1 binding site: these patterns emerge from training on chromatin data without any explicit motif supervision. The network identifies them because they predict the training labels. Supervision on chromatin state induces discovery of the sequence patterns that create chromatin state, providing unsupervised motif learning as a byproduct of supervised prediction.</p>
<p>Deeper layers operate on the output of earlier layers rather than raw sequence. A second-layer filter might learn to detect specific arrangements of first-layer motifs: two ETS sites within 20 base pairs, or a CTCF motif flanked by particular spacing patterns. This hierarchical feature learning enables CNNs to capture regulatory grammar beyond individual motifs, including spacing constraints, orientation preferences, and combinatorial requirements that govern transcription factor cooperativity.</p>
<div id="fig-conv-pattern-detector" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-conv-pattern-detector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/01-A-fig-conv-pattern-detector.svg" class="img-fluid figure-img"></p>
<figcaption>Sliding convolution</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/01-B-fig-conv-pattern-detector.svg" class="img-fluid figure-img"></p>
<figcaption>Learned filter as sequence logo</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/01-C-fig-conv-pattern-detector.svg" class="img-fluid figure-img"></p>
<figcaption>Multiple filter diversity</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-conv-pattern-detector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Convolutional filters as learned position weight matrices. (A) A filter of width 8 slides across one-hot encoded DNA, producing activation scores at each position. High activation indicates sequence matching the learned pattern. (B) Visualizing filter weights as sequence logos reveals correspondence to known transcription factor motifs. This filter has learned the CTCF binding site consensus matching the JASPAR database entry. (C) Multiple first-layer filters detect diverse motifs including CTCF, ETS, AP-1, and TATA box elements. This specialization emerges from training on chromatin prediction without explicit motif supervision.
</figcaption>
</figure>
</div>
<p>Between convolutional layers, spatial resolution must decrease while the receptive field expands. <strong>Pooling</strong> operations achieve this tradeoff: max pooling selects the strongest activation within a window, achieving position-invariant detection where the network responds to a motif’s presence somewhere in a region rather than its exact position. Why does this position invariance matter? Transcription factors typically locate their binding sites through scanning and diffusion rather than measuring exact distances from fixed landmarks. A CTCF site functions whether it sits at position 150 or 180 within an enhancer; the regulatory logic depends on which motifs co-occur, not their precise coordinates. Pooling encodes this biological flexibility directly into the architecture, preventing the network from memorizing position-specific patterns that would fail to generalize across genomic contexts.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Deep Dive: Receptive Field">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deep Dive: Receptive Field
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>For biology readers:</strong> The receptive field is how much input sequence can influence each output prediction:</p>
<p><strong>The concept:</strong> Imagine looking at a photo through a small window—you can only see a portion at a time. A CNN’s receptive field is like this window: it determines how much genomic context the model can “see” when making predictions at any position.</p>
<p><strong>How it grows:</strong></p>
<ul>
<li>A single convolutional layer with filter width 8 sees 8 nucleotides</li>
<li>Stacking layers expands the view: each layer adds context from the previous layer’s outputs</li>
<li>Pooling (downsampling) dramatically expands receptive field by compressing spatial information</li>
</ul>
<p><strong>Typical sizes:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Receptive Field</th>
<th>What It Can Capture</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>DeepSEA</em></td>
<td>~1,000 bp</td>
<td>Local motifs, TF binding</td>
</tr>
<tr class="even">
<td><em>SpliceAI</em></td>
<td>~10,000 bp</td>
<td>Splice sites + nearby regulatory elements</td>
</tr>
<tr class="odd">
<td><em>Enformer</em></td>
<td>~200,000 bp</td>
<td>Enhancer-promoter interactions</td>
</tr>
</tbody>
</table>
<p><strong>The limitation:</strong> Biology doesn’t respect receptive field boundaries. If an enhancer 50 kb from a gene affects its expression, a model with 10 kb receptive field cannot learn this relationship—it simply cannot see both elements simultaneously. This architectural ceiling motivated the move to attention mechanisms.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled" title="Mathematical Detail">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Detail
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following paragraph introduces the receptive field formula. If you prefer to focus on intuition, the key point is: more layers and wider filters increase how much sequence a CNN can see, but these increases are limited by computational constraints.</p>
</div>
</div>
<p>The receptive field of a convolutional network defines how much input sequence can influence a single output prediction. For a network with kernel width <span class="math inline">\(k\)</span>, pooling factor <span class="math inline">\(p\)</span>, and <span class="math inline">\(L\)</span> layers, the receptive field grows with depth but remains fundamentally limited by architecture. A three-layer network with typical parameters might integrate information from 200 to 1,000 base pairs. Reaching further requires either more layers (increasing computational cost and training difficulty) or <strong>dilated convolutions</strong> that space filter weights to sample larger regions. When biological dependencies span tens of kilobases, this receptive field ceiling becomes the fundamental constraint that no amount of training data can overcome.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Knowledge Check">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before continuing, make sure you can answer: (1) What is a convolutional filter learning to detect? (2) Why does pooling create position invariance? (3) What determines how far a CNN can “see” in the input sequence?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li>A convolutional filter learns to detect a specific sequence pattern or motif, responding strongly when that pattern appears in its receptive field. (2) Pooling creates position invariance by taking the maximum (or average) activation within a region, so the filter responds similarly whether the motif appears at position 10 or position 15 within that region. (3) The receptive field size determines how far a CNN can see, which is determined by the combination of filter width, number of layers, pooling operations, and dilation rates across the network.</li>
</ol>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-ch06-deepsea" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="sec-ch06-deepsea"><span class="header-section-number">6.2</span> <em>DeepSEA</em>: Regulatory Prediction from Sequence</h2>
<p>A patient presents with a rare disease phenotype, and whole-genome sequencing reveals a novel variant in an intron 15 kilobases from the nearest exon. The variant does not disrupt any annotated regulatory element. No prior patient in any database carries this exact change. The clinician must decide: is this variant pathogenic, or is it an irrelevant passenger? Annotation-based methods offer no guidance. The variant overlaps nothing cataloged, so overlap-based interpretation returns nothing useful. Yet introns harbor splice regulatory elements, and 15 kilobases places the variant well within range of enhancers that might control the adjacent gene.</p>
<p>Existing approaches to noncoding variant interpretation relied on this overlap paradigm. If a variant fell within a ChIP-seq peak or DNase hypersensitive site, it might be flagged as potentially regulatory. The strategy grounded predictions in experimental observations, but it could not predict whether a variant would strengthen or weaken regulatory activity, could not score variants in regions lacking experimental coverage, and provided no mechanism for quantifying effect magnitude. A variant might fall within an enhancer, but would it matter? The data indicated where regulatory elements existed; they did not indicate how sequence changes would affect them.</p>
<p><em>DeepSEA</em>, introduced by Zhou and Troyanskaya, reframed the problem: rather than asking whether a variant overlaps known annotations, ask what regulatory activities a sequence encodes and how mutations would alter them <span class="citation" data-cites="zhou_deepsea_2015">(<a href="../bib/references.html#ref-zhou_deepsea_2015" role="doc-biblioref">Zhou and Troyanskaya 2015</a>)</span>. The shift from annotation lookup to sequence-based prediction enabled scoring any variant in any genomic context, including regions never assayed in any experiment. This reframing would prove more consequential than any specific architectural choice.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Insight">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>DeepSEA’s</em> lasting contribution was conceptual, not architectural. The shift from “does this variant overlap an annotation?” to “what does this sequence encode?” changed variant interpretation from database lookup to computational prediction. This reframing enabled scoring variants in any genomic context, including regions never experimentally assayed.</p>
</div>
</div>
<div id="fig-deepsea-architecture" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deepsea-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/02-A-fig-deepsea-architecture.svg" class="img-fluid figure-img"></p>
<figcaption>Architecture schematic</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/02-B-fig-deepsea-architecture.svg" class="img-fluid figure-img"></p>
<figcaption>First-layer filter motif match</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/02-C-fig-deepsea-architecture.svg" class="img-fluid figure-img"></p>
<figcaption>Allelic imbalance validation</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deepsea-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: DeepSEA: regulatory prediction from sequence. (A) Architecture schematic showing progression from 1,000 bp one-hot input through three convolutional layers to 919 chromatin feature predictions. (B) First-layer filters learn to recognize known transcription factor motifs, with this example matching JASPAR’s CTCF consensus. (C) Variant effect predictions validated against allelic imbalance measurements, confirming that sequence-based predictions capture genuine regulatory variation. DeepSEA demonstrated that deep learning on functional genomics data could discover regulatory patterns without encoding human assumptions about what matters.
</figcaption>
</figure>
</div>
<section id="sec-ch06-deepsea-architecture" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="sec-ch06-deepsea-architecture"><span class="header-section-number">6.2.1</span> Architecture and Training</h3>
<p>The clinical scenario described above demands a model that can predict function from sequence alone. <em>DeepSEA’s</em> architecture was deliberately simple by contemporary standards, placing the emphasis on the learning framework rather than architectural complexity.</p>
<p>Input sequences of 1,000 base pairs, one-hot encoded, passed through three convolutional layers with 320, 480, and 960 filters respectively. Max pooling after each convolution compressed spatial dimensions. A fully connected layer with 925 units integrated information across the compressed representation, and a final output layer with 919 sigmoid units produced independent probability predictions for each chromatin profile.</p>
<p>Training data came from ENCODE and Roadmap Epigenomics (see <a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a> for comprehensive treatment of these resources): 690 transcription factor binding profiles, 104 histone modification profiles, and 125 DNase I hypersensitivity profiles spanning diverse cell types <span class="citation" data-cites="kagda_encode_2025 kundaje_roadmap_2015">(<a href="../bib/references.html#ref-kagda_encode_2025" role="doc-biblioref">Kagda et al. 2025</a>; <a href="../bib/references.html#ref-kundaje_roadmap_2015" role="doc-biblioref">Kundaje et al. 2015</a>)</span>. For each 1,000 bp input, the model predicted whether the central 200 bp region exhibited each chromatin feature. Chromosome 8 was held out for evaluation.</p>
<p>The <strong>multi-task learning</strong> formulation proved essential for generalization. Predicting 919 features simultaneously forced the network to learn shared representations useful across many prediction problems. Why does joint training help? Consider the alternative: 919 separate models, each learning from scratch that certain nucleotide patterns indicate regulatory activity. Each model would independently discover that GC-rich regions often mark promoters, that the CTCF motif signals insulator function, that splice site dinucleotides follow consensus sequences. Multi-task learning amortizes this redundant effort. The first convolutional layer learns general sequence patterns (GC content, common dinucleotides, ubiquitous motifs); these representations then feed task-specific combinations in later layers. Joint training also provides implicit regularization: a filter that helps predict many chromatin features captures genuine sequence grammar, while a filter useful for only one task may reflect dataset-specific artifacts. This pressure toward shared representations prevents overfitting to any single task while ensuring that learned features generalize across regulatory contexts.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Deep Dive: Multi-Task Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deep Dive: Multi-Task Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>For biology readers:</strong> Multi-task learning means training one model to predict many outputs simultaneously:</p>
<p><strong>The concept:</strong> Instead of training 919 separate models (one per chromatin feature), <em>DeepSEA</em> trains one model with 919 output nodes. All predictions share the same learned sequence features.</p>
<p><strong>Why it helps:</strong></p>
<ol type="1">
<li><p><strong>Shared learning:</strong> Basic motifs (CTCF, ETS family) are useful for predicting many chromatin features. Learning them once and sharing across tasks is more efficient than re-learning for each task.</p></li>
<li><p><strong>Regularization:</strong> Predicting many related outputs prevents the model from memorizing quirks of any single dataset. It must learn general patterns useful across contexts.</p></li>
<li><p><strong>Transfer:</strong> Features learned from abundant data (e.g., well-studied cell lines) help predictions for scarce data (e.g., rare cell types with few experiments).</p></li>
</ol>
<p><strong>The biology analog:</strong> Think of it like this—understanding general transcription factor binding helps you interpret specific experiments across many cell types, rather than starting from scratch for each.</p>
<p><strong>Trade-off:</strong> Multi-task learning assumes tasks share structure. If tasks are unrelated, forcing them to share features can hurt performance. For chromatin features, the shared regulatory grammar makes multi-task learning highly effective.</p>
</div>
</div>
<div id="tbl-deepsea-design" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-deepsea-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.1: Summary of <em>DeepSEA’s</em> key architectural and training decisions.
</figcaption>
<div aria-describedby="tbl-deepsea-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 50%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Design Choice</th>
<th><em>DeepSEA</em> Implementation</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input length</td>
<td>1,000 bp</td>
<td>Balance between context and computation</td>
</tr>
<tr class="even">
<td>Encoding</td>
<td>One-hot (4 × L matrix)</td>
<td>Preserves single-nucleotide resolution</td>
</tr>
<tr class="odd">
<td>Architecture</td>
<td>3 conv layers + 1 FC</td>
<td>Sufficient depth for motif combinations</td>
</tr>
<tr class="even">
<td>Output targets</td>
<td>919 chromatin profiles</td>
<td>Multi-task learning improves generalization</td>
</tr>
<tr class="odd">
<td>Loss function</td>
<td>Binary cross-entropy</td>
<td>Independent predictions per feature</td>
</tr>
<tr class="even">
<td>Validation</td>
<td>Chromosome 8 held out</td>
<td>Ensures genomic generalization</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-ch06-deepsea-validation" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="sec-ch06-deepsea-validation"><span class="header-section-number">6.2.2</span> Learned Representations and Biological Validation</h3>
<p>Any sequence model faces a fundamental question: do learned features correspond to biological reality, or do they exploit statistical shortcuts that happen to correlate with labels? <em>DeepSEA</em> provided the first large-scale evidence that deep learning could recover genuine regulatory logic.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why is it significant that <em>DeepSEA’s</em> learned filters matched known transcription factor motifs? What would it mean if the filters learned to predict chromatin state without capturing recognizable biological patterns?</p>
</div>
</div>
<p>Analysis of first-layer filters revealed learned patterns matching known transcription factor motifs. The network had independently recovered sequence preferences cataloged in JASPAR and TRANSFAC, confirming that the training objective (predicting chromatin state) induced biologically meaningful feature extraction. This interpretability distinguished deep learning from prior black-box approaches and suggested that the models captured genuine regulatory logic rather than spurious correlations. Systematic methods for extracting and visualizing these learned representations, from filter analysis to attribution mapping, are examined in <a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 24</span></a>.</p>
<p>Deeper layers combined first-layer patterns into more complex representations, capturing motif spacing requirements, orientation preferences, and cooperative binding arrangements. The network encoded relationships between sequence features that position weight matrices, operating independently at each motif, could not represent.</p>
<p><em>DeepSEA</em> outperformed gkm-SVM (gapped <span class="math inline">\(k\)</span>-mer support vector machines) on nearly all transcription factor binding prediction tasks <span class="citation" data-cites="zhou_deepsea_2015">(<a href="../bib/references.html#ref-zhou_deepsea_2015" role="doc-biblioref">Zhou and Troyanskaya 2015</a>)</span>. The pattern of improvement revealed something fundamental: gkm-SVM showed no benefit from longer input sequences, while <em>DeepSEA</em> performance improved substantially with additional context. <em>K</em>-mer methods tally motif occurrences but cannot learn relationships between patterns at different positions. Hierarchical feature learning enables exactly what <span class="math inline">\(k\)</span>-mer methods cannot provide: representations of combinatorial regulatory logic.</p>
</section>
<section id="sec-ch06-deepsea-vep" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="sec-ch06-deepsea-vep"><span class="header-section-number">6.2.3</span> Variant Effect Prediction</h3>
<p>With a trained sequence-to-chromatin model, variant scoring becomes straightforward: predict chromatin profiles for reference and alternative sequences, compute the difference. This <strong>in silico mutagenesis</strong> produces a 919-dimensional vector describing predicted changes across all features. The model never encounters variant data during training; effect prediction emerges from learned sequence-function relationships applied to mutations the model has never seen.</p>
<p>Validation used allelic imbalance data from digital genomic footprinting. For variants showing allele-specific DNase I sensitivity, <em>DeepSEA</em> predictions correlated with experimentally observed biases: variants predicted to increase accessibility tended to show higher accessibility on the corresponding allele. This correlation would not exist if the model merely learned coarse sequence features insensitive to point mutations.</p>
<p>Systematic characterization of regulatory elements requires more than single-variant scoring. <strong>In silico saturation mutagenesis</strong> predicts effects of all possible substitutions across a regulatory element, identifying positions where mutations most strongly perturb function. These critical positions typically correspond to transcription factor binding motifs, providing motif discovery that emerges from learned representations rather than explicit sequence alignment. The approach enables characterization of any regulatory element, including those in cell types or conditions never experimentally profiled. Foundation models extend these principles to longer contexts and richer representations (<a href="../part_3/p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a>), while clinical integration requires calibration approaches that map model outputs to actionable categories (<a href="../part_6/p6-ch28-rare-disease.html" class="quarto-xref"><span>Chapter 28</span></a>).</p>
<div class="callout callout-style-default callout-important callout-titled" title="Checkpoint: What You Should Understand So Far">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Checkpoint: What You Should Understand So Far
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before continuing, make sure you can explain:</p>
<ol type="1">
<li><strong>How convolutional filters work</strong>: Learned weight patterns that slide across sequences, producing high activation when input matches the learned motif</li>
<li><strong>Why multi-task learning helps</strong>: Shared representations across 919 chromatin features prevent overfitting and discover general regulatory patterns</li>
<li><strong>In silico mutagenesis</strong>: Predict effects by comparing reference vs.&nbsp;alternative sequence predictions—no variant training data required</li>
<li><strong>The receptive field limitation</strong>: CNNs can only integrate information within their architectural context window (typically 1-10 kb)</li>
</ol>
<p>If any of these are unclear, re-read the relevant sections before proceeding. The remaining sections build on these foundations.</p>
</div>
</div>
</section>
</section>
<section id="sec-ch06-basset" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec-ch06-basset"><span class="header-section-number">6.3</span> Cell-Type Specificity and Regulatory Grammar</h2>
<p>A variant that disrupts cardiac-specific gene regulation may be lethal in the heart but entirely silent in neurons. A regulatory element active during embryonic development may be permanently silenced in adult tissues. Clinical variant interpretation therefore requires models that capture not just what sequence patterns predict regulatory activity, but how those predictions vary across the dozens of cell types and developmental stages where a variant might act. <em>DeepSEA’s</em> 919 chromatin features spanned multiple cell types, but the question remained: could architectural modifications better capture cell-type-specific programs or learn richer representations of the combinatorial grammar governing transcription factor cooperativity?</p>
<p><em>Basset</em>, introduced by Kelley et al.&nbsp;in 2016, focused specifically on predicting chromatin accessibility from sequence <span class="citation" data-cites="kelley_basset_2016">(<a href="../bib/references.html#ref-kelley_basset_2016" role="doc-biblioref">Kelley, Snoek, and Rinn 2016</a>)</span>. Rather than <em>DeepSEA’s</em> diverse chromatin features, <em>Basset</em> predicted DNase-seq peaks across 164 cell types, enabling detailed analysis of cell-type-specific regulatory activity. The architectural refinements <em>Basset</em> introduced would influence subsequent models: <strong>batch normalization</strong> after convolutional layers stabilized training and enabled deeper networks, while larger filters in early layers (19 nucleotides in the first layer) captured longer motifs directly rather than requiring the network to compose them from smaller patterns.</p>
<p>The key contribution was demonstrating that <em>in silico</em> saturation mutagenesis profiles from trained models could identify causal variants underlying disease-associated haplotypes. GWAS identifies associated regions but cannot distinguish the causal variant from nearby variants in linkage disequilibrium (see <a href="../part_1/p1-ch03-gwas.html" class="quarto-xref"><span>Chapter 3</span></a> for the statistical foundations of association studies). <em>Basset’s</em> saturation mutagenesis provided a principled approach: the variant with the strongest predicted regulatory effect within an associated haplotype is the most likely causal candidate. Foundation models like <em>Enformer</em> (<a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>) extend this principle to longer contexts that capture more distal regulatory influences on GWAS signals. This moved beyond simple peak overlap toward mechanistic variant prioritization, and the original Basset study confirmed that model-prioritized variants (high-PICS SNPs) showed higher rates of predicted accessibility change than nearby SNPs selected by GWAS association strength alone <span class="citation" data-cites="kelley_basset_2016">(<a href="../bib/references.html#ref-kelley_basset_2016" role="doc-biblioref">Kelley, Snoek, and Rinn 2016</a>)</span>.</p>
<p><em>DanQ</em> explored whether regulatory grammar involves sequential dependencies that convolutions alone might miss, combining convolutional layers with bidirectional LSTMs to integrate motif detections across the input window <span class="citation" data-cites="quang_danq_2016">(<a href="../bib/references.html#ref-quang_danq_2016" role="doc-biblioref">Quang and Xie 2016</a>)</span>. The hybrid architecture achieved modest improvements on chromatin prediction benchmarks, though the recurrent components introduced costs examined in <a href="#sec-ch06-sequential" class="quarto-xref"><span>Section 6.7</span></a>.</p>
<div id="tbl-cnn-models" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-cnn-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.2: Progression of CNN-based regulatory genomics models. Note the trend toward longer input contexts and more specialized prediction targets.
</figcaption>
<div aria-describedby="tbl-cnn-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 9%">
<col style="width: 30%">
<col style="width: 22%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Year</th>
<th>Prediction Target</th>
<th>Input Length</th>
<th>Key Innovation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>DeepSEA</em></td>
<td>2015</td>
<td>919 chromatin features</td>
<td>1,000 bp</td>
<td>Multi-task learning, in silico mutagenesis</td>
</tr>
<tr class="even">
<td><em>Basset</em></td>
<td>2016</td>
<td>164 cell-type DNase-seq</td>
<td>600 bp</td>
<td>Batch normalization, larger first-layer filters</td>
</tr>
<tr class="odd">
<td><em>DanQ</em></td>
<td>2016</td>
<td>919 chromatin features</td>
<td>1,000 bp</td>
<td>CNN + bidirectional LSTM hybrid</td>
</tr>
<tr class="even">
<td><em>ExPecto</em></td>
<td>2018</td>
<td>218 tissue expression</td>
<td>40 kb (aggregated)</td>
<td>Tissue-specific expression from chromatin</td>
</tr>
<tr class="odd">
<td><em>SpliceAI</em></td>
<td>2019</td>
<td>Splice sites</td>
<td>10,000 bp</td>
<td>Dilated convolutions, 32-layer depth</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>These variations illustrated a broader principle: multiple architectures could learn useful regulatory representations from sequence. The specific choices (filter sizes, layer depths, recurrent components) mattered less than the fundamental framework of learning from one-hot encoded sequence to predict chromatin labels. This robustness suggested that the underlying signal, sequence determinants of regulatory activity, was strong enough to be captured by diverse architectural approaches. For clinical applications, prediction quality depends more on training data quality and task definition than on architectural details within the CNN family.</p>
</section>
<section id="sec-ch06-expecto" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="sec-ch06-expecto"><span class="header-section-number">6.4</span> <em>ExPecto</em>: From Chromatin to Expression</h2>
<p>A patient’s tumor harbors a somatic variant in a putative enhancer region. Chromatin profiling in matching tissue shows the region is accessible. The variant is predicted to disrupt a transcription factor binding site. Yet the clinician’s question remains unanswered: does this variant actually change expression of a target gene? Which gene? By how much? In which tissues?</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Chromatin accessibility tells us where the genome is “open” for regulation, but accessibility alone does not tell us which genes are affected or by how much. What additional information would a model need to predict gene expression from sequence? How would you connect a variant in an enhancer to its target gene?</p>
</div>
</div>
<p>Chromatin accessibility and transcription factor binding are intermediate phenotypes, means rather than ends. The ultimate functional readout for most regulatory variants is their effect on gene expression. A variant might disrupt a binding site, but sites can be redundant, effects can be buffered, and the relationship between binding and expression is not one-to-one. Predicting expression change from sequence requires integrating regulatory signals across distances that determine which enhancers control which promoters. A variant that disrupts binding but does not alter expression is unlikely to be pathogenic, while a variant with modest chromatin effects but strong expression consequences may drive disease.</p>
<p><em>ExPecto</em>, introduced by Zhou et al.&nbsp;in 2018, addressed these questions by extending sequence-to-chromatin prediction toward tissue-specific gene expression <span class="citation" data-cites="zhou_expecto_2018">(<a href="../bib/references.html#ref-zhou_expecto_2018" role="doc-biblioref">Zhou et al. 2018</a>)</span>. The framework predicts expression levels across 218 tissues and cell types by integrating predicted chromatin signals across a 40 kb promoter-proximal window. This context expansion, from <em>DeepSEA’s</em> 1 kb to <em>ExPecto’s</em> 40 kb, represented a significant architectural commitment: expression prediction requires integrating regulatory signals from distances far exceeding typical motif sizes.</p>
<div id="fig-expecto-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-expecto-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/03-fig-expecto-pipeline.svg" class="img-fluid figure-img"></p>
<figcaption>ExPecto pipeline: from chromatin to expression</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-expecto-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: ExPecto pipeline: from chromatin to expression. The modular architecture comprises three components. (1) Beluga CNN scans a 40kb window around each transcription start site with 2kb sliding windows, predicting 2,002 chromatin features at 200 spatial positions and generating over 400,000 features per gene. (2) Spatial transformation applies exponential decay functions separately for upstream and downstream regions, encoding the prior that nearby elements contribute more to expression than distant ones, reducing dimensionality to approximately 20,000 features. (3) Tissue-specific linear regression models (218 total, one per tissue) predict log expression from transformed features. This modular design separates shared sequence-to-chromatin processing from tissue-specific expression modeling, enabling interpretable analysis of which chromatin features drive expression in each context.
</figcaption>
</figure>
</div>
<section id="sec-ch06-expecto-architecture" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="sec-ch06-expecto-architecture"><span class="header-section-number">6.4.1</span> Modular Architecture</h3>
<p><em>ExPecto</em> comprises three sequential components, each addressing a distinct computational challenge. The separation proved essential: jointly optimizing all components end-to-end would be computationally prohibitive, and the modular design enables interpretability at each stage.</p>
<p>The first component, an enhanced CNN called <em>Beluga</em>, predicts 2,002 chromatin profiles from 2,000 bp input sequences. <em>Beluga</em> incorporated architectural improvements over <em>DeepSEA</em>: six convolutional layers with <strong>residual connections</strong>, expanded chromatin targets, and broader cell-type coverage. This CNN scans the 40 kb region surrounding each transcription start site with a moving window, generating chromatin predictions at 200 spatial positions and producing over 400,000 features per gene.</p>
<p>The second component transforms these high-dimensional features through spatial aggregation. Ten exponential decay functions, applied separately to upstream and downstream regions, encode the prior belief that nearby elements contribute more than distant ones. This transformation reduces dimensionality while preserving spatial relationships, producing approximately 20,000 features per gene that capture both which chromatin features are predicted and where they occur relative to the TSS.</p>
<p>The final component comprises 218 <em>L</em>₂-regularized linear regression models, one per tissue, predicting log expression from spatially-transformed features. Linear models were chosen deliberately: they provide interpretability, prevent overfitting given the high-dimensional feature space, and enable coefficient analysis to identify which chromatin features drive expression in each tissue. The combination of a shared sequence-to-chromatin CNN with separate tissue-specific linear heads cleanly separates sequence-level regulatory grammar from tissue-specific regulatory programs.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Insight">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>ExPecto’s</em> modular design reflects a key principle: different aspects of gene regulation require different modeling approaches. Convolutional networks excel at learning local sequence patterns (motifs), while linear models with spatial priors capture how these patterns integrate across distance. This separation enables interpretation at each stage and prevents end-to-end black boxes.</p>
</div>
</div>
</section>
<section id="sec-ch06-expecto-validation" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="sec-ch06-expecto-validation"><span class="header-section-number">6.4.2</span> Expression Prediction and Variant Effects</h3>
<p><em>ExPecto</em> achieved 0.819 median Spearman correlation between predicted and observed expression across tissues. Analysis of model coefficients revealed automatic learning of cell-type-relevant features: the liver expression model weighted HepG2-derived transcription factor features most heavily; breast tissue models emphasized estrogen receptor features from breast cancer cell lines. These tissue-specific patterns emerged purely from learning to predict expression, without tissue identity information provided to the chromatin model.</p>
<p>Variant effect prediction follows the same logic as <em>DeepSEA</em>: compare expression predictions for reference and alternative sequences. Because the model never trains on variant data, predictions are unconfounded by linkage disequilibrium, a critical distinction from association-based methods (see <a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 12</span></a> for detailed treatment of confounding in genomic models). <em>ExPecto</em> correctly predicted expression change direction for 92% of the strongest GTEx eQTL variants, and experimental validation confirmed that model-prioritized variants (not the GWAS lead SNPs) showed allele-specific regulatory activity in luciferase reporter assays <span class="citation" data-cites="zhou_expecto_2018">(<a href="../bib/references.html#ref-zhou_expecto_2018" role="doc-biblioref">Zhou et al. 2018</a>)</span>.</p>
<p>The 40 kb window represents an empirically optimized trade-off. Smaller windows decreased performance; larger windows showed negligible improvement. Most promoter-proximal regulatory information lies within 40 kb of the TSS, at least within <em>ExPecto’s</em> linear modeling framework. Distal enhancers beyond this window, while biologically important, require architectural approaches that can model longer-range dependencies. This limitation points toward the transformer architectures and hybrid models examined in <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>.</p>
</section>
</section>
<section id="sec-ch06-spliceai" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="sec-ch06-spliceai"><span class="header-section-number">6.5</span> <em>SpliceAI</em>: Clinical-Grade Splicing Prediction</h2>
<p>A child presents with developmental delay and dysmorphic features consistent with a known genetic syndrome. Clinical exome sequencing reveals no pathogenic coding variants in the implicated gene. The case is signed out as “unsolved,” the family left without answers. Three years later, research RNA sequencing identifies aberrant splicing in the syndromic gene: an intronic variant 150 base pairs from the nearest exon creates a cryptic splice site, inserting a premature stop codon. The diagnosis was hiding in plain sight, invisible to methods that only examine canonical splice dinucleotides.</p>
<p>This scenario, replicated across thousands of unsolved rare disease cases, illustrates a systematic blind spot in clinical genomics. Splice-disrupting mutations represent a major mechanism of Mendelian disease (see <a href="../part_6/p6-ch28-rare-disease.html" class="quarto-xref"><span>Chapter 28</span></a> for broader treatment of rare disease diagnosis), yet variants affecting splicing outside canonical GT/AG dinucleotides are systematically underascertained. Prior splice prediction methods captured essential splice site motifs but could not model the long-range determinants contributing to splicing specificity. MaxEntScan operates on approximately 9 bp of context around donor and acceptor sites <span class="citation" data-cites="yeo_maxentscan_2004">(<a href="../bib/references.html#ref-yeo_maxentscan_2004" role="doc-biblioref">Yeo and Burge 2004</a>)</span>. The method established the paradigm of quantitative splice site scoring using maximum entropy distributions and remains a standard baseline for variant interpretation, but its narrow window fundamentally limits what biology it can capture. These methods produced many false positives and missed variants acting through distal mechanisms: branch points, exonic splicing enhancers, and intron length constraints that previous models could not see.</p>
<p><em>SpliceAI</em>, introduced by Jaganathan et al.&nbsp;in 2019, demonstrated that deep neural networks could learn sequence-intrinsic splicing rules sufficient to predict the majority of splice sites used by the spliceosome <span class="citation" data-cites="jaganathan_spliceai_2019">(<a href="../bib/references.html#ref-jaganathan_spliceai_2019" role="doc-biblioref">Jaganathan et al. 2019</a>)</span>. The model predicts splice site locations directly from pre-mRNA sequence using 10,000 nucleotides of context, an order of magnitude beyond prior methods. This context expansion enabled recognition of distant splicing determinants invisible to annotation-based approaches.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>SpliceAI</em> uses 10,000 nucleotides of context, compared to MaxEntScan’s 9 nucleotides. What biological features might be captured by this expanded context? Think about the components involved in splicing beyond just the GT/AG dinucleotides.</p>
</div>
</div>
<section id="sec-ch06-spliceai-architecture" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="sec-ch06-spliceai-architecture"><span class="header-section-number">6.5.1</span> Architecture: Depth and Dilation</h3>
<p>Learning splicing rules from 10 kb of sequence context requires an architecture that can integrate information across this entire span while maintaining nucleotide-level resolution. <em>SpliceAI</em> achieves this through two innovations: extreme depth enabled by residual connections, and dilated convolutions that expand receptive fields without proportional parameter growth.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Mathematical Detail">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Detail
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following section describes residual connections and dilated convolutions. The key intuition: residual connections let gradients flow through very deep networks by providing “shortcuts,” and dilated convolutions let filters “skip” positions to see further without adding parameters.</p>
</div>
</div>
<p><em>SpliceAI</em> employs an ultra-deep residual network with 32 convolutional layers. Residual connections address the <strong>vanishing gradient problem</strong> that otherwise prevents training at this depth:</p>
<p><span class="math display">\[
\text{output} = \text{input} + F(\text{input})
\]</span></p>
<p>Why does this simple reformulation enable deeper networks? During backpropagation, gradients must flow through every layer to update early parameters. In standard networks, each layer multiplies the gradient by its weight matrix; when these multiplications compound across 32 layers, gradients either explode (if weights are large) or vanish toward zero (if weights are small). Residual connections provide an alternative path: the gradient can flow directly through the identity shortcut, bypassing the learned transformation <span class="math inline">\(F(\text{input})\)</span> entirely. This means early layers receive gradient signal regardless of what intermediate layers have learned. The network can then focus on learning what to <em>add</em> to the identity mapping rather than learning the entire transformation from scratch, a substantially easier optimization problem. Skip connections from every fourth residual block feed directly to the penultimate layer, further stabilizing training dynamics by providing multiple gradient highways that bypass potential bottlenecks.</p>
<p>Dilated convolutions expand the receptive field efficiently. A dilated convolution with rate <em>d</em> samples input positions at intervals of <em>d</em> rather than consecutively. Why use dilation rather than simply larger filters or more layers? Standard convolutions face a tradeoff: expanding the receptive field requires either wide filters (adding parameters proportionally) or deep stacking (adding parameters and gradient path length). Dilation circumvents this tradeoff by reusing the same filter weights while sampling input at wider intervals. A 3-wide filter with dilation rate 8 effectively spans 17 positions (3 samples with 8-position gaps) while using only 3 weight values. Stacking convolutions with increasing dilation rates (1, 2, 4, 8, 16, and so on) allows the network to integrate information across the full 10 kb window while maintaining sensitivity to local patterns at early layers where dilation is small. Standard convolutions with small kernels would require impractical depth to achieve equivalent receptive fields.</p>
<p>For each position in the pre-mRNA sequence, <em>SpliceAI</em> outputs three probabilities: splice acceptor, splice donor, or neither. This per-position classification enables fine-grained predictions across entire transcripts. Training used GENCODE annotations, with odd and even chromosomes split for training and testing.</p>
<div id="fig-spliceai-architecture" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-spliceai-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/04-A-fig-spliceai-architecture.svg" class="img-fluid figure-img"></p>
<figcaption>Dilated convolutions</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/04-B-fig-spliceai-architecture.svg" class="img-fluid figure-img"></p>
<figcaption>Residual block structure</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-spliceai-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: SpliceAI architecture innovations for long-range splicing prediction. (A) Dilated convolutions expand the receptive field efficiently by sampling input positions at intervals. Stacking layers with dilation rates 1, 2, 4, 8, 16… enables integration of 10,000 nucleotides of context without proportional parameter growth. Lower layers with small dilation capture local splice site grammar while upper layers with large dilation integrate distal determinants like branch points and exonic splicing enhancers. (B) Residual block structure with skip connections from every 4th block to the output. This enables training of 32 layers by providing multiple gradient pathways, preventing vanishing gradients that would otherwise block learning in such a deep network.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch06-spliceai-performance" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="sec-ch06-spliceai-performance"><span class="header-section-number">6.5.2</span> Performance and Validation</h3>
<p><em>SpliceAI</em> achieved 95% top-<span class="math inline">\(k\)</span> accuracy for splice site identification (compared to 57% for <em>MaxEntScan</em>) and 0.98 precision-recall area under the curve (auPRC). Complex genes exceeding 100 kb are often reconstructed to nucleotide precision. Performance improved dramatically with context length:</p>
<div id="tbl-spliceai-context" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-spliceai-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.3: <em>SpliceAI</em> performance improves substantially with context length, confirming that distal sequence features contribute meaningfully to splicing decisions. Diminishing returns above 2 kb suggest most determinants lie within this range.
</figcaption>
<div aria-describedby="tbl-spliceai-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model Variant</th>
<th>Context (each side)</th>
<th>auPRC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>SpliceAI</em>-80nt</td>
<td>40 bp</td>
<td>0.87</td>
</tr>
<tr class="even">
<td><em>SpliceAI</em>-400nt</td>
<td>200 bp</td>
<td>0.93</td>
</tr>
<tr class="odd">
<td><em>SpliceAI</em>-2k</td>
<td>1,000 bp</td>
<td>0.96</td>
</tr>
<tr class="even">
<td><em>SpliceAI</em>-10k</td>
<td>5,000 bp</td>
<td>0.98</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>This progression confirms that distal sequence features contribute meaningfully to splicing decisions. The diminishing returns above 2 kb suggest that most splicing determinants lie within this range, though the additional context still provides measurable benefit.</p>
<p>The delta score quantifies variant effects by comparing predictions for reference and alternative sequences:</p>
<p><span class="math display">\[
\Delta\text{score} = \max_{|p - v| \leq 50} \left| P_{\text{alt}}(p) - P_{\text{ref}}(p) \right|
\]</span></p>
<p>Validation against GTEx RNA-seq showed that mutations with higher delta scores showed higher validation rates at novel splice junctions: approximately <span class="math inline">\(50\%\)</span> at <span class="math inline">\(\Delta \geq 0.2\)</span>, <span class="math inline">\(75\%\)</span> at <span class="math inline">\(\Delta \geq 0.5\)</span>, and <span class="math inline">\(85\%\)</span> at <span class="math inline">\(\Delta \geq 0.8\)</span>. Population genetics provided orthogonal support: predicted cryptic splice variants showed <span class="math inline">\(78\%\)</span> depletion at common allele frequencies, nearly matching the depletion of frameshift and stop-gain variants. Natural selection treats these variants as deleterious, confirming their functional impact.</p>
</section>
<section id="sec-ch06-spliceai-clinical" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="sec-ch06-spliceai-clinical"><span class="header-section-number">6.5.3</span> Clinical Impact</h3>
<p><em>SpliceAI’s</em> most significant contribution may be quantifying cryptic splice mutations as a major, previously underappreciated cause of rare genetic disorders. Analysis of <em>de novo</em> mutations in over <span class="math inline">\(4{,}000\)</span> individuals with intellectual disability found significant enrichment of predicted splice-disrupting variants compared to unaffected controls (<span class="math inline">\(1.51\)</span>-fold, <span class="math inline">\(p = 4.2 \times 10^{-4}\)</span>). Approximately <span class="math inline">\(9\%\)</span> of pathogenic <em>de novo</em> mutations in intellectual disability act through cryptic splicing <span class="citation" data-cites="jaganathan_spliceai_2019">(<a href="../bib/references.html#ref-jaganathan_spliceai_2019" role="doc-biblioref">Jaganathan et al. 2019</a>)</span>. Including these variants in gene discovery analyses identified additional candidate genes that would have fallen below discovery thresholds when considering only protein-coding mutations.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Practical Guidance: Using *SpliceAI* Scores">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Using <em>SpliceAI</em> Scores
</div>
</div>
<div class="callout-body-container callout-body">
<p>When interpreting <em>SpliceAI</em> delta scores in clinical contexts:</p>
<ul>
<li><strong>Delta &lt; 0.2</strong>: Low confidence; variant unlikely to affect splicing</li>
<li><strong>Delta 0.2-0.5</strong>: Moderate confidence; consider RNA-seq validation if clinically relevant</li>
<li><strong>Delta 0.5-0.8</strong>: High confidence; strong candidate for splice effect</li>
<li><strong>Delta &gt; 0.8</strong>: Very high confidence; treat similarly to canonical splice site variants</li>
</ul>
<p>Remember that <em>SpliceAI</em> predicts whether splicing <em>changes</em>, not whether the change is pathogenic. A variant might create a new in-frame splice site that has minimal functional impact.</p>
</div>
</div>
<p>This clinical utility explains <em>SpliceAI’s</em> rapid adoption. Illumina integrated <em>SpliceAI</em> into their annotation pipelines. Clinical genetics laboratories worldwide use delta scores to flag potential splice-affecting variants for RNA-seq follow-up. The model exemplifies how task-specific deep learning can achieve clinical-grade accuracy on well-defined problems. <em>SpliceAI’s</em> integration into modern variant interpretation workflows is examined in <a href="../part_3/p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a>, and its role in rare disease diagnosis pipelines appears in <a href="../part_6/p6-ch28-rare-disease.html" class="quarto-xref"><span>Chapter 28</span></a>.</p>
</section>
</section>
<section id="sec-ch06-receptive-field" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="sec-ch06-receptive-field"><span class="header-section-number">6.6</span> Receptive Field Ceiling</h2>
<p>Consider a 45-year-old woman with early-onset breast cancer and a family history suggesting hereditary risk. Whole-genome sequencing identifies a novel variant 80 kilobases upstream of <em>BRCA1</em>, within an established enhancer region. The enhancer is known to regulate <em>BRCA1</em> expression in mammary epithelium. Does this variant reduce <em>BRCA1</em> expression enough to increase cancer risk? <em>DeepSEA</em> can predict whether the variant disrupts transcription factor binding at that position. <em>SpliceAI</em> confirms no splice effects. <em>ExPecto’s</em> 40 kb window cannot reach from the variant to the <em>BRCA1</em> promoter. No convolutional model can connect the enhancer variant to its target gene because the distance exceeds their receptive fields. The clinical question remains unanswered.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Insight">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>The receptive field limitation is architectural, not statistical. No amount of training data can teach a CNN to model dependencies that exceed its receptive field. This fundamental constraint, not model performance on short-range tasks, motivated the architectural shift to attention mechanisms.</p>
</div>
</div>
<p>This case illustrates a fundamental limitation rooted in architecture: convolutional networks can only integrate information within their receptive fields. <em>DeepSEA’s</em> three-layer architecture effectively considers roughly 1 kb of context. <em>ExPecto’s</em> <em>Beluga</em> component operates on 2 kb windows, aggregated across a 40 kb region by the spatial transformation layer. <em>SpliceAI</em> pushes to 10 kb through dilated convolutions and 32 layers. Each expansion required significant architectural engineering, and each reached a practical ceiling beyond which further expansion yielded diminishing returns or became computationally prohibitive.</p>
<p>The limitation matters because genomic regulation routinely operates across distances these models cannot reach. Enhancers regulate promoters 50 to 500 kilobases away. The <em>beta-globin</em> locus control region sits 40 to 60 kb from the genes it activates. Polycomb-mediated repression involves chromatin contacts spanning megabases. Topologically associating domains organize regulatory interactions across hundreds of kilobases (see <a href="../part_4/p4-ch20-3d-genome.html" class="quarto-xref"><span>Chapter 20</span></a> for detailed treatment of chromatin architecture). When regulatory elements and their targets lie beyond a model’s receptive field, the model cannot learn their relationship regardless of how much training data is available. The constraint is architectural, not statistical. Attention mechanisms (<a href="p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>) provide the architectural solution, while hybrid models like <em>Enformer</em> (<a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>) combine convolutional motif detection with transformer-based long-range integration.</p>
<p>This creates a systematic mismatch between biological importance and computational accessibility. A variant within a distal enhancer may have profound effects on gene expression, but a model with a 10 kb receptive field cannot connect the enhancer sequence to its target promoter. The model might correctly predict that the enhancer sequence contains regulatory features, but it cannot predict which gene those features regulate or how strongly. We can predict local regulatory potential, yet we cannot predict long-range regulatory effects.</p>
<p>The architectural response to this challenge evolved through two stages. Recurrent networks initially seemed promising, carrying context through hidden states rather than expanding receptive fields. When recurrence proved insufficient, attention mechanisms provided the architectural solution that modern genomic models required.</p>
<div id="fig-receptive-field-ceiling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-receptive-field-ceiling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch06/05-fig-receptive-field-ceiling.svg" class="img-fluid figure-img"></p>
<figcaption>The receptive field ceiling of convolutional architectures</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-receptive-field-ceiling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: The receptive field ceiling of convolutional architectures. Context windows of representative CNN-based models compared to biologically relevant regulatory distances. DeepSEA integrates approximately 1kb of context; SpliceAI extends to 10kb through dilated convolutions; ExPecto aggregates predictions across 40kb. These contexts suffice for local features like transcription factor binding sites (~10bp) and promoter elements (~1kb) but cannot capture enhancer-promoter interactions typically spanning 10-100kb. Even SpliceAI’s 10kb context falls short of the distances at which most GWAS signals reside from their target genes. This architectural limitation, not data scarcity, motivated the shift to attention mechanisms that compute direct interactions across arbitrary distances.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Knowledge Check">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before proceeding, ensure you can explain: (1) Why can’t a CNN learn dependencies that exceed its receptive field, even with more training data? (2) What biological phenomena require context longer than typical CNN receptive fields? (3) What architectural approaches extend receptive fields in CNNs?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li>A CNN cannot learn dependencies beyond its receptive field because positions outside this window have zero gradient with respect to the output—information simply cannot flow between them, regardless of training data quantity. (2) Distal enhancer-promoter interactions (tens of kilobases), long-range chromatin loops, TAD boundary effects, and coordinated regulation across large genomic domains all require longer context than typical CNN receptive fields of 200-1000 bp. (3) Receptive fields can be extended through dilated convolutions, deeper networks with more layers, or hybrid architectures that combine CNNs with attention mechanisms.</li>
</ol>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-ch06-sequential" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="sec-ch06-sequential"><span class="header-section-number">6.7</span> Sequential Approaches and Their Costs</h2>
<p>If convolutional networks cannot reach far enough, why not simply carry information forward through the sequence? Recurrent neural networks offered an intuitive solution to the receptive field problem: maintain a hidden state that accumulates context as the network processes each position in turn. Where a convolutional filter sees only its local window, a <strong>recurrent neural network (RNN)’s</strong> hidden state can, in principle, carry information from the beginning of a sequence to its end. For biological sequences, this seemed natural. DNA is read by polymerases in one direction; transcripts are processed sequentially by ribosomes; regulatory elements exert effects that propagate through chromatin. A computational architecture that mirrors this sequential logic appeared well-suited to genomic modeling.</p>
<p>The hidden state mechanism works as follows. At each position <em>t</em>, the network combines the current input <span class="math inline">\(x_t\)</span> with the previous hidden state <span class="math inline">\(h_{t-1}\)</span> to produce a new hidden state <span class="math inline">\(h_t\)</span>. This recurrence allows information from early positions to influence computations at later positions through the chain of hidden states. A regulatory element at position 1,000 can, in theory, affect predictions at position 50,000 because its influence persists in the hidden state across all intervening positions. No receptive field limits this reach; the constraint becomes whether information survives the journey.</p>
<section id="sec-ch06-vanishing-gradient" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="sec-ch06-vanishing-gradient"><span class="header-section-number">6.7.1</span> Vanishing Gradient Problem</h3>
<div class="callout callout-style-default callout-warning callout-titled" title="Mathematical Detail">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Detail
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section discusses gradient propagation through recurrent networks. The key intuition: when you multiply small numbers many times, the result quickly approaches zero. This is why RNNs struggle to learn long-range dependencies.</p>
</div>
</div>
<p>Information rarely survives. Training RNNs requires backpropagating gradients through time, computing how errors at late positions depend on parameters applied at early positions. These gradients pass through the same recurrent weight matrix at each step. When gradients are multiplied through hundreds or thousands of steps, they either explode (growing exponentially) or vanish (shrinking toward zero). The vanishing gradient problem makes it nearly impossible for RNNs to learn dependencies spanning more than a few dozen positions. A regulatory element 10,000 base pairs upstream might as well not exist: by the time gradients propagate backward through 10,000 recurrent steps, they have decayed to numerical insignificance.</p>
<p>Gating mechanisms that control information flow resolved the vanishing gradient problem. <strong>Long Short-Term Memory (LSTM)</strong> networks achieve this through a separate cell state with learned gates that determine what information to store, what to forget, and what to output <span class="citation" data-cites="hochreiter_long_1997">(<a href="../bib/references.html#ref-hochreiter_long_1997" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>)</span>. The forget gate can preserve information indefinitely by setting its value near one, allowing gradients to flow through the cell state without repeated multiplication by small values. <strong>Gated Recurrent Units (GRUs)</strong> simplified this design by combining gates while retaining the core insight: learned gating prevents gradient decay <span class="citation" data-cites="cho_learning_2014">(<a href="../bib/references.html#ref-cho_learning_2014" role="doc-biblioref">Cho et al. 2014</a>)</span>.</p>
<p>These gated architectures extended effective memory from tens to hundreds of positions, sometimes thousands. For natural language, where most dependencies span fewer than 50 words, LSTMs proved transformative. For genomic sequences, where relevant context can span tens of kilobases (tens of thousands of nucleotides), even gated recurrence falls short. The mathematics of gradient propagation through recurrent connections imposes limits that no gating mechanism fully overcomes.</p>
<div id="tbl-rnn-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-rnn-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.4: Comparison of sequence modeling architectures for learning long-range dependencies. Theoretical range assumes perfect gradient flow; practical range reflects empirical observations.
</figcaption>
<div aria-describedby="tbl-rnn-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 28%">
<col style="width: 25%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Architecture</th>
<th>Theoretical Range</th>
<th>Practical Range</th>
<th>Key Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Vanilla RNN</td>
<td>Unlimited</td>
<td>~10-50 positions</td>
<td>Vanishing gradients</td>
</tr>
<tr class="even">
<td>LSTM/GRU</td>
<td>Unlimited</td>
<td>~100-1000 positions</td>
<td>Gradient decay through gates</td>
</tr>
<tr class="odd">
<td>Bidirectional LSTM</td>
<td>Unlimited</td>
<td>~100-1000 positions</td>
<td>Sequential computation bottleneck</td>
</tr>
<tr class="even">
<td>Attention</td>
<td>Unlimited</td>
<td>Limited by memory</td>
<td>Quadratic complexity</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-ch06-danq" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="sec-ch06-danq"><span class="header-section-number">6.7.2</span> <em>DanQ</em>: Combining Convolutions and Recurrence</h3>
<p>The <em>DanQ</em> model represented the most influential attempt to apply recurrent architectures to regulatory genomics <span class="citation" data-cites="quang_danq_2016">(<a href="../bib/references.html#ref-quang_danq_2016" role="doc-biblioref">Quang and Xie 2016</a>)</span>. Rather than replacing convolutions entirely, <em>DanQ</em> combined them: convolutional layers first extracted local sequence motifs, then a bidirectional LSTM integrated these motif detections across the 1,000 base pair input window. The architecture recognized that convolutions excel at detecting local patterns while recurrence might capture their long-range relationships.</p>
<p><em>DanQ</em> processed sequences in both directions simultaneously (bidirectional recurrence), allowing each position to incorporate context from both upstream and downstream. Training on the same <em>DeepSEA</em> chromatin prediction task, <em>DanQ</em> achieved modest improvements over the purely convolutional baseline, with the LSTM component learning to weight motif combinations based on their relative positions and co-occurrence patterns.</p>
<p>The improvement was real but limited. Within a 1,000 base pair window, convolutional receptive fields already capture most relevant dependencies, leaving less room for recurrence to contribute. The fundamental problem remained: neither convolutions nor recurrence could reach the 50 to 100 kilobase distances where enhancers regulate their target genes. <em>DanQ</em> demonstrated that hybrid architectures could outperform pure convolutions, but the gains did not justify the added complexity for most applications. The model saw limited adoption compared to simpler convolutional alternatives.</p>
</section>
<section id="sec-ch06-sequential-bottleneck" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="sec-ch06-sequential-bottleneck"><span class="header-section-number">6.7.3</span> Sequential Bottleneck</h3>
<p>Even if recurrence could maintain gradients across genomic distances, a more fundamental constraint would remain. RNNs process sequences one position at a time. Each hidden state <span class="math inline">\(h_t\)</span> depends on the previous hidden state <span class="math inline">\(h_{t-1}\)</span>, creating an inherently sequential computation that cannot be parallelized. Training on a 100,000 base pair sequence requires 100,000 sequential steps, each waiting for the previous step to complete. Modern GPUs achieve their speed through massive parallelism; sequential dependencies eliminate this advantage.</p>
<p>This computational bottleneck made RNNs impractical for the long contexts that genomic applications require. A transformer processes all positions simultaneously, computing attention scores in parallel across the entire sequence. For a 100 kilobase context, a transformer performs one parallel operation where an RNN would require 100,000 sequential steps. The difference in training time is not incremental; it is the difference between feasible and infeasible. When <em>Enformer</em> extended genomic modeling to 200 kilobase contexts (see <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>), recurrent architectures were not considered. The sequential bottleneck had already disqualified them.</p>
<p>The attention mechanism resolved both limitations simultaneously. <strong>Self-attention</strong> computes direct interactions between all positions without sequential dependencies, enabling parallel processing across arbitrary context lengths. Attention weights are computed through matrix operations that GPUs execute efficiently, and gradients flow directly between any two positions without passing through intermediate states. The path from position 1 to position 100,000 involves a single attention computation rather than 100,000 recurrent steps. This architectural shift, examined in <a href="p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>, enabled the long-range modeling that genomic applications demand.</p>
</section>
</section>
<section id="sec-ch06-specialization" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="sec-ch06-specialization"><span class="header-section-number">6.8</span> Specialization and Its Limits</h2>
<p>The convolutional models examined here established paradigms that persist in modern genomic AI. End-to-end learning from one-hot encoded sequence demonstrated that gradient descent on functional labels could discover regulatory patterns without encoding human assumptions about what matters. Multi-task training across hundreds of chromatin features showed that shared representations improve both accuracy and generalization. <em>In silico</em> mutagenesis, comparing predictions for reference and alternative sequences, established the dominant approach for deep learning-based variant effect prediction: scoring variants without training on variant labels, thereby avoiding the ascertainment biases that confound association-based methods (see <a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 12</span></a>).</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>SpliceAI</em> achieves clinical-grade accuracy on splice prediction that general-purpose foundation models have not matched. Under what circumstances might a specialized model outperform a general-purpose foundation model? When might the reverse be true?</p>
</div>
</div>
<p>These principles carry forward into foundation model architectures. What CNNs could not resolve was the receptive field limitation. Genomic regulation operates across scales that exceed practical convolutional depth: enhancers modulating genes across hundreds of kilobases, topologically associating domains spanning megabases. Dilated convolutions and deeper networks extend reach but cannot fundamentally escape the constraint that convolutions aggregate local information through hierarchical composition.</p>
<p>Yet specialization retains value even as general-purpose models advance. <em>SpliceAI</em> achieves clinical-grade splice site prediction that broader foundation models have not matched. When the prediction target is well-defined, training data abundant, and the relevant context fits within architectural constraints, task-specific models remain competitive with or superior to general-purpose approaches. This tension between specialized accuracy and general capability defines architectural choices across genomic AI. For clinical deployment requiring high reliability on specific tasks, specialized architectures may remain preferred. For discovery applications requiring broad coverage across diverse molecular mechanisms, the foundation model paradigm (see <a href="../part_3/p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a>) offers different trade-offs. Attention mechanisms provide the architectural substrate for long-range modeling while inheriting the end-to-end learning principles that convolutional networks established.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Test Yourself">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Test Yourself
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reviewing the summary, test your recall:</p>
<ol type="1">
<li>How do convolutional filters differ from classical position weight matrices, and why does this difference matter for discovering regulatory patterns?</li>
<li>Explain why multi-task training across 919 chromatin features helps regularization and improves generalization compared to single-task training.</li>
<li>A CNN has three convolutional layers with kernel width 8 and pooling factor 2. Approximately what receptive field does this architecture achieve, and why does this limit regulatory modeling?</li>
<li>Why did SpliceAI use dilated convolutions with increasing dilation rates across 32 layers, and what biological patterns required this architectural choice?</li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Chapter Summary">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Key concepts covered:</strong> Convolutional filters as learned position weight matrices, receptive fields and their architectural limits, multi-task learning for regularization, <em>in silico</em> mutagenesis for variant effect prediction, dilated convolutions for expanded context, residual connections for deep networks, the sequential bottleneck of recurrent architectures.</p>
<p><strong>Models examined:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 33%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Contribution</th>
<th>Limitation Revealed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>DeepSEA</em></td>
<td>End-to-end learning discovers motifs; in silico mutagenesis for VEP</td>
<td>1 kb context insufficient for distal regulation</td>
</tr>
<tr class="even">
<td><em>Basset</em></td>
<td>Cell-type specific accessibility; causal variant prioritization</td>
<td>Cannot model enhancer-promoter relationships</td>
</tr>
<tr class="odd">
<td><em>ExPecto</em></td>
<td>Expression from chromatin; tissue-specific prediction</td>
<td>40 kb aggregation, not end-to-end</td>
</tr>
<tr class="even">
<td><em>SpliceAI</em></td>
<td>Clinical-grade splicing; dilated convolutions</td>
<td>10 kb still misses some long-range effects</td>
</tr>
<tr class="odd">
<td><em>DanQ</em></td>
<td>Hybrid CNN-RNN</td>
<td>Recurrence adds cost without solving receptive field problem</td>
</tr>
</tbody>
</table>
<p><strong>Main takeaways:</strong></p>
<ol type="1">
<li>CNNs proved that gradient descent on DNA sequence discovers regulatory patterns matching experimental biology</li>
<li>Multi-task learning across hundreds of chromatin features enables generalization and interpretability</li>
<li><em>In silico</em> mutagenesis enables variant effect prediction without training on variant data</li>
<li>Receptive field limitations are architectural constraints that no amount of training data overcomes</li>
<li>Specialized models (<em>SpliceAI</em>) can achieve clinical-grade accuracy that general models do not match</li>
<li>Attention mechanisms, not recurrence, provide the path beyond receptive field constraints</li>
</ol>
<p><strong>Looking ahead:</strong> <a href="p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a> examines how self-attention computes direct interactions between all positions, enabling the long-range modeling that CNNs cannot achieve. Understanding CNNs’ strengths (motif detection, multi-task learning) and limitations (receptive fields) clarifies why modern genomic foundation models typically combine convolutional pattern detection with attention-based long-range integration.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-cho_learning_2014" class="csl-entry" role="listitem">
Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. <span>“On the <span>Properties</span> of <span>Neural</span> <span>Machine</span> <span>Translation</span>: <span>Encoder</span>-<span>Decoder</span> <span>Approaches</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1409.1259">https://doi.org/10.48550/arXiv.1409.1259</a>.
</div>
<div id="ref-hochreiter_long_1997" class="csl-entry" role="listitem">
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. <span>“Long <span>Short</span>-<span>Term</span> <span>Memory</span>.”</span> <em>Neural Computation</em> 9 (8): 1735–80. <a href="https://doi.org/10.1162/neco.1997.9.8.1735">https://doi.org/10.1162/neco.1997.9.8.1735</a>.
</div>
<div id="ref-jaganathan_spliceai_2019" class="csl-entry" role="listitem">
Jaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. <span>“[<span>SpliceAI</span>] <span>Predicting</span> <span>Splicing</span> from <span>Primary</span> <span>Sequence</span> with <span>Deep</span> <span>Learning</span>.”</span> <em>Cell</em> 176 (3): 535–548.e24. <a href="https://doi.org/10.1016/j.cell.2018.12.015">https://doi.org/10.1016/j.cell.2018.12.015</a>.
</div>
<div id="ref-kagda_encode_2025" class="csl-entry" role="listitem">
Kagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. <span>“Data Navigation on the <span>ENCODE</span> Portal.”</span> <em>Nature Communications</em> 16 (1): 9592. <a href="https://doi.org/10.1038/s41467-025-64343-9">https://doi.org/10.1038/s41467-025-64343-9</a>.
</div>
<div id="ref-kelley_basset_2016" class="csl-entry" role="listitem">
Kelley, David R., Jasper Snoek, and John L. Rinn. 2016. <span>“Basset: Learning the Regulatory Code of the Accessible Genome with Deep Convolutional Neural Networks.”</span> <em>Genome Research</em> 26 (7): 990–99. <a href="https://doi.org/10.1101/gr.200535.115">https://doi.org/10.1101/gr.200535.115</a>.
</div>
<div id="ref-kundaje_roadmap_2015" class="csl-entry" role="listitem">
Kundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. <span>“Integrative Analysis of 111 Reference Human Epigenomes.”</span> <em>Nature</em> 518 (7539): 317–30. <a href="https://doi.org/10.1038/nature14248">https://doi.org/10.1038/nature14248</a>.
</div>
<div id="ref-quang_danq_2016" class="csl-entry" role="listitem">
Quang, Daniel, and Xiaohui Xie. 2016. <span>“<span>DanQ</span>: A Hybrid Convolutional and Recurrent Deep Neural Network for Quantifying the Function of <span>DNA</span> Sequences.”</span> <em>Nucleic Acids Research</em> 44 (11): e107. <a href="https://doi.org/10.1093/nar/gkw226">https://doi.org/10.1093/nar/gkw226</a>.
</div>
<div id="ref-yeo_maxentscan_2004" class="csl-entry" role="listitem">
Yeo, Gene, and Christopher B. Burge. 2004. <span>“Maximum <span>Entropy</span> <span>Modeling</span> of <span>Short</span> <span>Sequence</span> <span>Motifs</span> with <span>Applications</span> to <span>RNA</span> <span>Splicing</span> <span>Signals</span>.”</span> <em>Journal of Computational Biology</em> 11 (2-3): 377–94. <a href="https://doi.org/10.1089/1066527041410418">https://doi.org/10.1089/1066527041410418</a>.
</div>
<div id="ref-zhou_expecto_2018" class="csl-entry" role="listitem">
Zhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. <span>“[<span>Expecto</span>] <span>Deep</span> Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.”</span> <em>Nature Genetics</em> 50 (8): 1171–79. <a href="https://doi.org/10.1038/s41588-018-0160-6">https://doi.org/10.1038/s41588-018-0160-6</a>.
</div>
<div id="ref-zhou_deepsea_2015" class="csl-entry" role="listitem">
Zhou, Jian, and Olga G. Troyanskaya. 2015. <span>“[<span>DeepSEA</span>] <span>Predicting</span> Effects of Noncoding Variants with Deep Learning–Based Sequence Model.”</span> <em>Nature Methods</em> 12 (10): 931–34. <a href="https://doi.org/10.1038/nmeth.3547">https://doi.org/10.1038/nmeth.3547</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_2/p2-ch05-representations.html" class="pagination-link" aria-label="Tokens and Embeddings">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_2/p2-ch07-attention.html" class="pagination-link" aria-label="Transformers and Attention">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>