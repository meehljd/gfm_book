<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>8&nbsp; Pretraining Strategies – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_2/p2-ch09-transfer.html" rel="next">
<link href="../part_2/p2-ch07-attention.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch08-pretraining.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch12-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch14-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch15-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch16-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch17-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--cellular-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Cellular Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch19-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch20-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch21-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch22-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch25-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Causal Inference with Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch26-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch28-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch29-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch30-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch31-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch08-mlm" id="toc-sec-ch08-mlm" class="nav-link active" data-scroll-target="#sec-ch08-mlm"><span class="header-section-number">8.1</span> Masked Language Modeling</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-mlm-strategies" id="toc-sec-ch08-mlm-strategies" class="nav-link" data-scroll-target="#sec-ch08-mlm-strategies"><span class="header-section-number">8.1.1</span> Masking Strategies and Their Implications</a></li>
  <li><a href="#sec-ch08-mlm-learning" id="toc-sec-ch08-mlm-learning" class="nav-link" data-scroll-target="#sec-ch08-mlm-learning"><span class="header-section-number">8.1.2</span> What Masked Language Models Learn</a></li>
  </ul></li>
  <li><a href="#sec-ch08-autoregressive" id="toc-sec-ch08-autoregressive" class="nav-link" data-scroll-target="#sec-ch08-autoregressive"><span class="header-section-number">8.2</span> Next-Token Prediction</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-autoregressive-genomics" id="toc-sec-ch08-autoregressive-genomics" class="nav-link" data-scroll-target="#sec-ch08-autoregressive-genomics"><span class="header-section-number">8.2.1</span> Genomic Applications</a></li>
  </ul></li>
  <li><a href="#sec-ch08-comparison" id="toc-sec-ch08-comparison" class="nav-link" data-scroll-target="#sec-ch08-comparison"><span class="header-section-number">8.3</span> MLM and Autoregressive Comparison</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-hybrid" id="toc-sec-ch08-hybrid" class="nav-link" data-scroll-target="#sec-ch08-hybrid"><span class="header-section-number">8.3.1</span> Hybrid Architectures</a></li>
  </ul></li>
  <li><a href="#sec-ch08-denoising" id="toc-sec-ch08-denoising" class="nav-link" data-scroll-target="#sec-ch08-denoising"><span class="header-section-number">8.4</span> Span Corruption and Denoising</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-corruption" id="toc-sec-ch08-corruption" class="nav-link" data-scroll-target="#sec-ch08-corruption"><span class="header-section-number">8.4.1</span> Corruption Beyond Masking</a></li>
  <li><a href="#sec-ch08-biological-corruption" id="toc-sec-ch08-biological-corruption" class="nav-link" data-scroll-target="#sec-ch08-biological-corruption"><span class="header-section-number">8.4.2</span> Biologically Motivated Corruption</a></li>
  </ul></li>
  <li><a href="#sec-ch08-contrastive" id="toc-sec-ch08-contrastive" class="nav-link" data-scroll-target="#sec-ch08-contrastive"><span class="header-section-number">8.5</span> Contrastive Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-augmentation" id="toc-sec-ch08-augmentation" class="nav-link" data-scroll-target="#sec-ch08-augmentation"><span class="header-section-number">8.5.1</span> Augmentation Design for Genomic Sequences</a></li>
  <li><a href="#sec-ch08-cross-species" id="toc-sec-ch08-cross-species" class="nav-link" data-scroll-target="#sec-ch08-cross-species"><span class="header-section-number">8.5.2</span> Cross-Species Contrastive Learning</a></li>
  </ul></li>
  <li><a href="#sec-ch08-multitask" id="toc-sec-ch08-multitask" class="nav-link" data-scroll-target="#sec-ch08-multitask"><span class="header-section-number">8.6</span> Multi-Task Pretraining</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-task-selection" id="toc-sec-ch08-task-selection" class="nav-link" data-scroll-target="#sec-ch08-task-selection"><span class="header-section-number">8.6.1</span> Task Selection and Architecture</a></li>
  <li><a href="#sec-ch08-loss-weighting" id="toc-sec-ch08-loss-weighting" class="nav-link" data-scroll-target="#sec-ch08-loss-weighting"><span class="header-section-number">8.6.2</span> Loss Weighting and Balancing</a></li>
  <li><a href="#sec-ch08-multitask-examples" id="toc-sec-ch08-multitask-examples" class="nav-link" data-scroll-target="#sec-ch08-multitask-examples"><span class="header-section-number">8.6.3</span> Large-Scale Multi-Task Examples</a></li>
  <li><a href="#sec-ch08-multitask-failure" id="toc-sec-ch08-multitask-failure" class="nav-link" data-scroll-target="#sec-ch08-multitask-failure"><span class="header-section-number">8.6.4</span> When Multi-Task Learning Fails</a></li>
  </ul></li>
  <li><a href="#sec-ch08-staged" id="toc-sec-ch08-staged" class="nav-link" data-scroll-target="#sec-ch08-staged"><span class="header-section-number">8.7</span> Staged Pretraining Strategies</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-context-curriculum" id="toc-sec-ch08-context-curriculum" class="nav-link" data-scroll-target="#sec-ch08-context-curriculum"><span class="header-section-number">8.7.1</span> Context Length Curricula</a></li>
  <li><a href="#sec-ch08-domain-adaptive" id="toc-sec-ch08-domain-adaptive" class="nav-link" data-scroll-target="#sec-ch08-domain-adaptive"><span class="header-section-number">8.7.2</span> Domain-Adaptive Pretraining</a></li>
  <li><a href="#sec-ch08-continued-pretraining" id="toc-sec-ch08-continued-pretraining" class="nav-link" data-scroll-target="#sec-ch08-continued-pretraining"><span class="header-section-number">8.7.3</span> Continued Pretraining on Expanded Data</a></li>
  <li><a href="#sec-ch08-multiobjective-schedule" id="toc-sec-ch08-multiobjective-schedule" class="nav-link" data-scroll-target="#sec-ch08-multiobjective-schedule"><span class="header-section-number">8.7.4</span> Multi-Objective Schedules</a></li>
  <li><a href="#sec-ch08-data-complexity" id="toc-sec-ch08-data-complexity" class="nav-link" data-scroll-target="#sec-ch08-data-complexity"><span class="header-section-number">8.7.5</span> Data Complexity Curricula</a></li>
  <li><a href="#sec-ch08-staged-practical" id="toc-sec-ch08-staged-practical" class="nav-link" data-scroll-target="#sec-ch08-staged-practical"><span class="header-section-number">8.7.6</span> Practical Considerations</a></li>
  </ul></li>
  <li><a href="#sec-ch08-data" id="toc-sec-ch08-data" class="nav-link" data-scroll-target="#sec-ch08-data"><span class="header-section-number">8.8</span> Data Strategies for Pretraining</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-reference-genomes" id="toc-sec-ch08-reference-genomes" class="nav-link" data-scroll-target="#sec-ch08-reference-genomes"><span class="header-section-number">8.8.1</span> Reference Genomes and Population Diversity</a></li>
  <li><a href="#sec-ch08-repeats" id="toc-sec-ch08-repeats" class="nav-link" data-scroll-target="#sec-ch08-repeats"><span class="header-section-number">8.8.2</span> Repeat Handling</a></li>
  <li><a href="#sec-ch08-multispecies" id="toc-sec-ch08-multispecies" class="nav-link" data-scroll-target="#sec-ch08-multispecies"><span class="header-section-number">8.8.3</span> Multi-Species and Augmentation Strategies</a></li>
  </ul></li>
  <li><a href="#sec-ch08-optimization" id="toc-sec-ch08-optimization" class="nav-link" data-scroll-target="#sec-ch08-optimization"><span class="header-section-number">8.9</span> Optimization and Scaling</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-hyperparameters" id="toc-sec-ch08-hyperparameters" class="nav-link" data-scroll-target="#sec-ch08-hyperparameters"><span class="header-section-number">8.9.1</span> Optimization Hyperparameters</a></li>
  <li><a href="#sec-ch08-scaling" id="toc-sec-ch08-scaling" class="nav-link" data-scroll-target="#sec-ch08-scaling"><span class="header-section-number">8.9.2</span> Scaling Laws and Emergence</a></li>
  </ul></li>
  <li><a href="#sec-ch08-diagnostics" id="toc-sec-ch08-diagnostics" class="nav-link" data-scroll-target="#sec-ch08-diagnostics"><span class="header-section-number">8.10</span> Training Diagnostics</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-monitoring" id="toc-sec-ch08-monitoring" class="nav-link" data-scroll-target="#sec-ch08-monitoring"><span class="header-section-number">8.10.1</span> Monitoring Loss and Gradients</a></li>
  <li><a href="#sec-ch08-probing" id="toc-sec-ch08-probing" class="nav-link" data-scroll-target="#sec-ch08-probing"><span class="header-section-number">8.10.2</span> Functional Probing</a></li>
  </ul></li>
  <li><a href="#sec-ch08-selection" id="toc-sec-ch08-selection" class="nav-link" data-scroll-target="#sec-ch08-selection"><span class="header-section-number">8.11</span> Strategy Selection</a></li>
  <li><a href="#sec-ch08-case-studies" id="toc-sec-ch08-case-studies" class="nav-link" data-scroll-target="#sec-ch08-case-studies"><span class="header-section-number">8.12</span> Pretraining in Practice: Case Studies</a>
  <ul class="collapse">
  <li><a href="#sec-ch08-dnabert" id="toc-sec-ch08-dnabert" class="nav-link" data-scroll-target="#sec-ch08-dnabert"><span class="header-section-number">8.12.1</span> DNABERT</a></li>
  <li><a href="#sec-ch08-hyenadna" id="toc-sec-ch08-hyenadna" class="nav-link" data-scroll-target="#sec-ch08-hyenadna"><span class="header-section-number">8.12.2</span> HyenaDNA</a></li>
  <li><a href="#sec-ch08-enformer" id="toc-sec-ch08-enformer" class="nav-link" data-scroll-target="#sec-ch08-enformer"><span class="header-section-number">8.12.3</span> Enformer</a></li>
  <li><a href="#sec-ch08-esm2" id="toc-sec-ch08-esm2" class="nav-link" data-scroll-target="#sec-ch08-esm2"><span class="header-section-number">8.12.4</span> ESM-2</a></li>
  </ul></li>
  <li><a href="#sec-ch08-open-questions" id="toc-sec-ch08-open-questions" class="nav-link" data-scroll-target="#sec-ch08-open-questions"><span class="header-section-number">8.13</span> Open Questions</a></li>
  <li><a href="#sec-ch08-sequence-to-knowledge" id="toc-sec-ch08-sequence-to-knowledge" class="nav-link" data-scroll-target="#sec-ch08-sequence-to-knowledge"><span class="header-section-number">8.14</span> From Sequence Statistics to Biological Knowledge</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch08-pretraining.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch08-pretraining" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>What you ask a model to predict during pretraining determines what it learns to see.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Estimated reading time:</strong> 35-45 minutes</p>
<p><strong>Prerequisites:</strong> Before reading this chapter, you should be familiar with:</p>
<ul>
<li>Tokenization strategies for genomic sequences (<a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>)</li>
<li>Attention mechanisms and transformer architecture (<a href="p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>)</li>
<li>Basic supervised learning concepts (loss functions, gradient descent)</li>
</ul>
<p><strong>Learning Objectives:</strong> After completing this chapter, you will be able to:</p>
<ol type="1">
<li>Explain how masked language modeling, next-token prediction, and contrastive learning differ in what they teach models</li>
<li>Select an appropriate pretraining objective based on downstream application requirements</li>
<li>Describe how augmentation and corruption strategies improve model robustness</li>
<li>Analyze tradeoffs between bidirectional understanding and generative capability</li>
<li>Apply staged pretraining strategies to manage computational constraints</li>
<li>Evaluate when multi-task pretraining provides benefits over single-task approaches</li>
</ol>
<p><strong>Clinical Relevance:</strong> The pretraining objective you choose determines whether your model excels at variant interpretation (requiring bidirectional context) or sequence design (requiring generation). Understanding these tradeoffs is essential for deploying foundation models in clinical genomics.</p>
</div>
</div>
<p>The choice of pretraining objective is not merely technical; it encodes assumptions about what matters in biological sequence. <strong>Masked language modeling</strong> encourages bidirectional context integration: the model learns to predict missing <strong>tokens</strong> using information from both upstream and downstream sequence. <strong>Next-token prediction</strong> builds <strong>autoregressive</strong> capabilities: the model learns to generate sequence one position at a time, enabling design of novel proteins or regulatory elements. <strong>Contrastive learning</strong> teaches invariance: the model learns that functionally equivalent sequences should map to similar representations regardless of species or polymorphism. Each objective produces a different model, and those differences propagate to downstream performance. A model pretrained with masked language modeling may excel at variant effect prediction (where context on both sides matters) but struggle at sequence generation. A model pretrained for generation may produce plausible sequences but provide representations less suited for classification tasks. Understanding what each objective teaches, and what assumptions each encodes, is prerequisite to selecting the right foundation model for a given application.</p>
<p>Self-supervised pretraining addresses a fundamental asymmetry in genomic data. Reference genomes span billions of nucleotides across thousands of species. Population sequencing projects catalog genetic variation in millions of individuals. Functional genomics consortia measure chromatin accessibility and gene expression across hundreds of cell types (see <a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a> for comprehensive treatment of these resources). Yet experimental labels remain sparse: for any given sequence, we typically lack direct measurements of its regulatory function, its effect on splicing, or its contribution to disease risk. Self-supervised objectives extract training signal from the sequences themselves, without requiring experimental labels. The resulting models learn representations that capture evolutionary constraints, sequence grammar, and functional relationships, all from the patterns present in unlabeled data. When these representations are applied to downstream tasks with limited labels, the pretrained knowledge makes scarce labeled data go further.</p>
<div id="fig-pretraining-objectives" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pretraining-objectives-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/01-A-fig-pretraining-objectives.svg" class="img-fluid figure-img"></p>
<figcaption>Masked language modeling</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/01-B-fig-pretraining-objectives.svg" class="img-fluid figure-img"></p>
<figcaption>Next-token prediction</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/01-C-fig-pretraining-objectives.svg" class="img-fluid figure-img"></p>
<figcaption>Contrastive learning</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pretraining-objectives-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: Comparison of major pretraining objectives for genomic sequences. (A) Masked language modeling corrupts sequences with [MASK] tokens and trains models to reconstruct original content using bidirectional context. This produces representations suited for classification and variant interpretation where full context matters. (B) Next-token prediction trains models to generate sequences autoregressively, predicting each position from preceding context. This enables sequence generation for therapeutic design applications. (C) Contrastive learning teaches invariance by bringing augmented versions of the same sequence together in embedding space while pushing unrelated sequences apart. Each objective encodes different assumptions about what patterns matter and produces models with different downstream strengths.
</figcaption>
</figure>
</div>
<section id="sec-ch08-mlm" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="sec-ch08-mlm"><span class="header-section-number">8.1</span> Masked Language Modeling</h2>
<p>Consider predicting whether a splice site variant in <em>DMD</em> will cause exon skipping in Duchenne muscular dystrophy. The model must recognize the canonical GT-AG splice signals, encode how flanking sequences modulate splicing efficiency, and integrate information from both the upstream exon and downstream intron. A model trained only on labeled splice variants would see perhaps a few hundred <em>DMD</em> examples across the entire clinical literature. A model pretrained on billions of nucleotides learns splice grammar across the entire genome, then applies that knowledge to the specific clinical question. Masked language modeling provides this pretraining by teaching models to predict missing sequence content from surrounding context, and the bidirectional attention it requires captures exactly the upstream-downstream integration that splice prediction demands.</p>
<p>MLM treats sequences as partially observed and trains models to reconstruct missing content. The procedure is straightforward: randomly mask portions of an input sequence, feed the corrupted sequence to the model, and train the model to predict the original tokens at masked positions. A <strong>masking strategy</strong> replaces selected tokens with a special <code>[MASK]</code> token, leaving the surrounding context intact. The model processes the masked sequence through its layers and produces predictions for the masked positions, typically optimizing cross-entropy loss over the vocabulary at each masked location.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Worked Example: MLM on a Promoter Sequence">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Worked Example: MLM on a Promoter Sequence
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider this 12-nucleotide promoter fragment containing a TATA box:</p>
<p><strong>Original sequence:</strong> <code>GCTATAAAGCTT</code></p>
<p><strong>Step 1 - Masking (15%):</strong> Randomly mask ~2 positions <code>GCT[MASK]TAAGC[MASK]T</code></p>
<p><strong>Step 2 - Model prediction:</strong> The model sees the masked sequence and, for each [MASK] position, outputs a probability distribution over {A, C, G, T}.</p>
<p><strong>Step 3 - Loss computation:</strong> For position 4 (true answer: A), suppose the model predicts:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Nucleotide</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>0.70</td>
</tr>
<tr class="even">
<td>T</td>
<td>0.20</td>
</tr>
<tr class="odd">
<td>G</td>
<td>0.05</td>
</tr>
<tr class="even">
<td>C</td>
<td>0.05</td>
</tr>
</tbody>
</table>
<p>Cross-entropy loss = <span class="math inline">\(-\log(0.70) = 0.36\)</span></p>
<p><strong>What the model learns:</strong> Positions within TATA boxes strongly predict A/T nucleotides from surrounding context. After seeing millions of promoters, the model internalizes that sequences matching <code>...TAT_AA...</code> almost always have A or T in the masked position—this is exactly the regulatory grammar that transfers to splice prediction and variant interpretation.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: What MLM Prediction Difficulty Reveals
</div>
</div>
<div class="callout-body-container callout-body">
<p>The key insight is that accurate prediction requires learning genuine sequence structure. To predict a masked position in a transcription factor binding site, the model must recognize the surrounding motif context. To predict masked splice donor sequences, the model must encode the consensus GT dinucleotide and the flanking patterns that modulate splicing strength. Over millions of training examples, models build distributed representations of motifs, compositional rules, and sequence constraints that transfer to tasks never seen during pretraining. The <em>DMD</em> splice variant can be evaluated using patterns learned from every splice site in the genome.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before continuing, consider: If a model confidently predicts the masked nucleotides in a TATA box (high probability for A or T at each masked position), but struggles to predict nucleotides in a random intergenic region (nearly uniform probabilities), what does this tell you about functional constraint at these two locations?</p>
</div>
</div>
<p>MLM encourages bidirectional context integration, and this bidirectionality has direct clinical relevance. Unlike autoregressive models that condition only on preceding tokens, MLM models see both left and right context when predicting masked positions. For genomics, this matches biological reality: regulatory function depends on patterns both upstream and downstream of any given position. A transcription factor binding site is recognized through flanking sequences on both sides. Splicing signals require coordination between donor and acceptor sites separated by hundreds of bases. Missense variants disrupt protein function through effects that depend on the entire domain context, not just the preceding amino acids. The bidirectional attention mechanisms examined in <a href="p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a> naturally capture these dependencies.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>How would span masking (masking consecutive tokens) differ from random masking in terms of what the model learns? What biological sequences might benefit from each approach?</p>
</div>
</div>
<section id="sec-ch08-mlm-strategies" class="level3" data-number="8.1.1">
<h3 data-number="8.1.1" class="anchored" data-anchor-id="sec-ch08-mlm-strategies"><span class="header-section-number">8.1.1</span> Masking Strategies and Their Implications</h3>
<div id="fig-masking-strategies" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-masking-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/02-A-fig-masking-strategies.svg" class="img-fluid figure-img"></p>
<figcaption>Random token masking</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/02-B-fig-masking-strategies.svg" class="img-fluid figure-img"></p>
<figcaption>Span masking</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-masking-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: Masking strategies encode different learning pressures. (A) Random token masking distributes [MASK] tokens throughout the sequence. Individual masked positions can often be predicted from immediately adjacent context, encouraging local pattern learning. (B) Span masking replaces contiguous blocks with single sentinel tokens, removing local context entirely. Predicting masked spans requires reasoning from more distant sequence features, forcing the model to learn compositional patterns and longer-range dependencies. For regulatory sequence modeling, span masking may better capture how transcription factor binding sites and other functional elements operate as integrated units.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading about span masking, consider: If you wanted a model to learn that transcription factor binding motifs function as <em>units</em> rather than as collections of independent nucleotides, how would you modify the masking strategy to force this understanding?</p>
</div>
</div>
<p>Predicting whether a regulatory variant disrupts an entire transcription factor binding site or merely alters its affinity requires models that learn compositional patterns, not just local nucleotide statistics. The tension between local and compositional learning plays out in masking strategy design.</p>
<p>Random masking of individual tokens creates predictions that are relatively local: each masked position can often be inferred from immediately adjacent nucleotides. This approach is efficient but may not force models to learn higher-order structure. <strong>Span masking</strong>, which masks contiguous blocks of tokens, forces models to infer longer-range dependencies and compositional patterns. If an entire transcription factor binding motif is masked, the model cannot rely on partial motif information and must instead recognize the motif’s role from surrounding regulatory context.</p>
<p>Why does span masking force compositional learning while random masking does not? When individual nucleotides are masked, adjacent positions provide strong local cues—if you see <code>TATA_A</code>, the missing position is almost certainly <code>A</code> or <code>T</code> based on TATA box grammar. But when an entire 6-bp motif is masked, those local cues vanish. The model must now ask: “Given that there’s an enhancer upstream and a core promoter downstream, what kind of regulatory element belongs here?” This requires learning which regulatory elements co-occur and why—the compositional grammar of regulation rather than mere nucleotide statistics. For clinical variant interpretation, span masking may better capture whether a regulatory variant disrupts an entire binding site or merely modulates its affinity.</p>
<p>Masking rates present a fundamental tradeoff between supervision density and prediction difficulty. Higher masking rates (30-40% of tokens) provide more supervision per sequence but make prediction harder and may destabilize training. Since each masked token becomes a prediction target, higher rates extract more learning signal from a single forward pass through the model. Lower masking rates (10-15%) produce more stable training but require more data to achieve equivalent coverage. The standard 15% rate from <em>BERT</em> represents a reasonable compromise, though genomic models have explored values ranging from 10% to 40% depending on context length and <strong>tokenization</strong> granularity <span class="citation" data-cites="devlin_bert_2019">(<a href="../bib/references.html#ref-devlin_bert_2019" role="doc-biblioref">Devlin et al. 2019</a>)</span>. <em>DNABERT</em> used 15% masking on 6-mer tokens, while later models have experimented with adaptive masking rates that increase as training progresses, starting conservatively and becoming more aggressive as the model’s predictions improve <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>.</p>
<div id="tbl-masking-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-masking-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.1: Comparison of masking strategies for genomic MLM pretraining.
</figcaption>
<div aria-describedby="tbl-masking-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 20%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>Masking Strategy</th>
<th>Mechanism</th>
<th>Strengths</th>
<th>Limitations</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Random token</strong> (15%)</td>
<td>Mask individual tokens uniformly</td>
<td>Simple, stable training, efficient</td>
<td>May learn only local patterns</td>
<td>General pretraining, limited compute</td>
</tr>
<tr class="even">
<td><strong>Higher rate</strong> (30-40%)</td>
<td>More tokens masked per sequence</td>
<td>More supervision signal per example</td>
<td>Harder optimization, may destabilize</td>
<td>Large datasets, robust architectures</td>
</tr>
<tr class="odd">
<td><strong>Span masking</strong></td>
<td>Mask contiguous blocks</td>
<td>Forces compositional learning</td>
<td>More complex implementation</td>
<td>Regulatory elements, motif grammar</td>
</tr>
<tr class="even">
<td><strong>Adaptive rate</strong></td>
<td>Increase masking as training progresses</td>
<td>Curriculum effect, stable-to-aggressive</td>
<td>Requires tuning schedule</td>
<td>Long training runs</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Tokenization interacts with masking in ways that affect what biological patterns models learn (see <a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a> for comprehensive treatment of tokenization strategies). <em>DNABERT</em> pioneered MLM for genomic sequences by applying it to overlapping <span class="math inline">\(k\)</span>-mer tokens: rather than treating DNA as individual nucleotides, <em>DNABERT</em> tokenizes sequences into all possible 6-mers with overlapping windows <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. Masking then operates at the <span class="math inline">\(k\)</span>-mer level, with entire 6-mers masked as units. This design encourages learning of <span class="math inline">\(k\)</span>-mer level patterns corresponding to transcription factor binding motifs (typically 6-12 base pairs) and other short functional elements. <em>DNABERT-2</em> adopted <strong>byte-pair encoding (BPE)</strong> tokenization, which learns a vocabulary of variable-length subword units from the training corpus <span class="citation" data-cites="zhou_dnabert-2_2024">(<a href="../bib/references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Zhou et al. 2024</a>)</span>. BPE tokens represent single nucleotides, common motifs, or repeated elements depending on their frequency. MLM with BPE balances flexibility with compositional structure, though the learned vocabulary may not align with biological functional units in interpretable ways.</p>
<p>The design decisions explored by <em>DNABERT</em> and <em>DNABERT-2</em> established patterns that subsequent DNA language models have built upon and refined. <a href="../part_3/p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a> examines how these architectural and tokenization choices have evolved as the field has scaled to longer contexts and larger training corpora.</p>
</section>
<section id="sec-ch08-mlm-learning" class="level3" data-number="8.1.2">
<h3 data-number="8.1.2" class="anchored" data-anchor-id="sec-ch08-mlm-learning"><span class="header-section-number">8.1.2</span> What Masked Language Models Learn</h3>
<p>MLM objectives drive models to capture multiple levels of sequence organization, from local nucleotide statistics to long-range regulatory grammar. At the lowest level, models learn base composition and local constraints: CpG dinucleotide frequencies, GC content biases, and simple repeat patterns. These basic properties are necessary but not sufficient for biological function prediction.</p>
<p>At higher levels, MLM captures motif patterns and sequence grammar. Predicting masked positions in regulatory regions requires recognizing transcription factor binding sites, understanding how motifs combine in enhancers and promoters, and learning context-dependent usage patterns. If certain transcription factor motifs co-occur at specific distances (as they do in developmental enhancers where factors like <em>HOX</em> proteins bind cooperatively), masking one motif and predicting it from the other reinforces this grammatical relationship. This compositional learning is difficult to achieve with supervised learning alone, which typically provides coarse binary labels (“enhancer” versus “non-enhancer”) rather than fine-grained structural information about sequence organization.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Pause and recall: What are the three main levels of sequence organization that MLM objectives help models learn? How does each level contribute to downstream variant interpretation tasks?</p>
</div>
</div>
<p>MLM also captures evolutionary conservation patterns implicitly, and this has direct relevance for clinical variant interpretation. Conserved sequences are constrained because mutations would disrupt function. By learning to predict conserved patterns from surrounding context, models encode which sequence features are under selection. This knowledge transfers to variant effect prediction, where the model recognizes when a mutation disrupts a learned conserved pattern. A variant that replaces a highly predictable position (one the model confidently fills in during MLM) is more likely to be damaging than one at a position where the model is uncertain. The connection between pretraining on raw sequence and downstream variant interpretation illustrates how self-supervised objectives capture biologically meaningful structure without explicit functional labels. The variant effect prediction approaches in <a href="../part_3/p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a> leverage these learned patterns directly, while probing methods (<a href="p2-ch09-transfer.html#sec-ch09-feature-extraction" class="quarto-xref"><span>Section 9.3</span></a>) reveal what specific patterns models have captured.</p>
</section>
</section>
<section id="sec-ch08-autoregressive" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="sec-ch08-autoregressive"><span class="header-section-number">8.2</span> Next-Token Prediction</h2>
<p>Designing a novel promoter sequence for gene therapy requires generating DNA that respects learned regulatory grammar while achieving specific expression characteristics. Masked language modeling can evaluate whether a candidate sequence looks “natural,” but it cannot generate sequences from scratch. A gene therapy team optimizing a CAR-T construct needs promoter variants to test; they cannot simply evaluate candidates one by one when the search space spans <span class="math inline">\(4^{500}\)</span> possible 500-base-pair sequences. Next-token prediction provides the generative capability missing from MLM, learning to predict each token given only preceding tokens and thereby acquiring the ability to sample coherent novel sequences that respect learned biological constraints.</p>
<p>Next-token prediction represents an alternative paradigm where models learn to predict each token in a sequence given only the preceding tokens. The intuition is familiar from everyday experience: predicting the next word in “The cat sat on the…” is easier than predicting a word from the middle of a sentence, because you have a clear thread of context leading up to the prediction point. Weather forecasting works similarly—tomorrow’s weather is predicted from today’s conditions, and next week’s weather is predicted by chaining together day-by-day forecasts. This autoregressive approach, popularized by GPT-style language models, treats sequence generation as a core capability rather than a secondary feature.</p>
<p>For a sequence of length <span class="math inline">\(T\)</span>, the model predicts token <span class="math inline">\(t\)</span> from tokens <span class="math inline">\(1\)</span> through <span class="math inline">\(t-1\)</span>, maximizing the likelihood of the observed sequence under the model’s learned distribution. The probability of a sequence factors as the product of conditional probabilities for each token given its predecessors—just as the probability of a week’s weather is the product of each day’s probability given the preceding days:</p>
<p><span class="math display">\[P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_1, \ldots, x_{t-1})\]</span></p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Detail
</div>
</div>
<div class="callout-body-container callout-body">
<p>The autoregressive factorization above is exact by the chain rule of probability. However, it requires choosing an ordering over tokens. For natural language, left-to-right reading order provides a natural choice. For DNA, there is no inherent directionality, which presents a challenge discussed below.</p>
</div>
</div>
<p>Algorithmically, next-token prediction requires <strong>causal masking</strong> in the attention mechanism. Each position attends only to earlier positions, ensuring predictions at position <span class="math inline">\(t\)</span> depend exclusively on positions <span class="math inline">\(1\)</span> through <span class="math inline">\(t-1\)</span>. Why enforce this restriction during training when we have access to the full sequence? The constraint ensures that the model learns exactly the conditional distributions it will use during generation. If training allowed each position to peek at future tokens, the model would learn different representations than what it needs when generating novel sequences where future tokens do not yet exist. This alignment between training and inference is what makes autoregressive generation principled rather than <em>ad hoc</em>. The loss function is cross-entropy over the vocabulary, computed at every position rather than only at masked locations. During training, <strong>teacher forcing</strong> allows efficient parallel computation: the model predicts all positions simultaneously by feeding in the ground truth sequence shifted by one position. Generation at inference time is inherently sequential, predicting one token at a time and conditioning each prediction on all previous outputs.</p>
<p>Autoregressive models develop systematic <strong>positional bias</strong> during training. Early positions in a sequence are predicted from minimal context: the first token has no conditioning information at all, the second token conditions only on the first, and so on. Later positions benefit from increasingly rich context as the full preceding sequence informs each prediction. This creates asymmetric representation quality across the sequence, with early positions learned less reliably than later ones. For natural language, this asymmetry is partially justified by syntactic structure (sentence openings are more formulaic than continuations), but genomic sequences have no such directional bias. Position 1 of a regulatory element carries as much functional information as position 500. Training dynamics that systematically disadvantage early positions introduce artifacts unrelated to biology.</p>
<p>Several strategies mitigate positional bias. Training on both forward and reverse-complement sequences ensures that each position appears early in some training examples and late in others, averaging out directional effects. Prefix conditioning provides bidirectional context for an initial segment before autoregressive generation begins, giving all generated positions access to rich conditioning information. Some architectures incorporate bidirectional “warm-up” layers before causal attention, building position-independent representations that subsequent autoregressive layers can condition on. The severity of positional bias depends on sequence length and model capacity; for short sequences (under 1000 tokens), the effect is modest, but for genome-scale contexts exceeding 100 kilobases, early positions may be substantially underrepresented in learned distributions.</p>
<p>The fundamental difference from MLM lies in what the model can see during prediction. Autoregressive models build representations from unidirectional context, learning to generate sequences that respect learned constraints. This makes autoregressive pretraining attractive for sequence design applications (see <a href="../part_6/p6-ch30-design.html" class="quarto-xref"><span>Chapter 30</span></a>). Sampling new sequences proceeds naturally: predict the first token, condition on it to predict the second, and continue token by token. The generation process directly uses the learned conditional distributions without requiring additional architectural modifications or iterative refinement procedures.</p>
<section id="sec-ch08-autoregressive-genomics" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="sec-ch08-autoregressive-genomics"><span class="header-section-number">8.2.1</span> Genomic Applications</h3>
<p>DNA sequences present a complication that natural language does not: they have no inherent directionality. Both strands encode information, and regulatory function is often strand-agnostic. A transcription factor binding site functions identically whether read 5’-to-3’ or on the reverse complement strand. This contrasts with natural language, where left-to-right reading order carries meaning. Early autoregressive genomic models addressed this by training separate models for forward and reverse strands or by augmenting training data with reverse-complement sequences. More recent approaches treat strand symmetry as an architectural constraint, ensuring that forward and reverse complement sequences produce equivalent representations through weight sharing or explicit symmetrization.</p>
<p><em>Evo</em> represents a large-scale autoregressive genomic model trained on whole genomes with long-context architectures <span class="citation" data-cites="nguyen_sequence_2024">(<a href="../bib/references.html#ref-nguyen_sequence_2024" role="doc-biblioref">Nguyen et al. 2024</a>)</span>. Using StripedHyena layers to achieve contexts exceeding 100 kilobases, <em>Evo</em> learns long-range dependencies including gene structure, repeat organization, and regulatory architecture spanning tens of kilobases. This enables generating coherent synthetic genomes that respect higher-order structure, not just local motif patterns. For therapeutic applications, <em>Evo’s</em> generative capability could design synthetic regulatory circuits, generate diverse candidate sequences for directed evolution, or produce training data through synthetic augmentation when real labeled data is scarce. The <em>Evo</em> architecture is examined in detail in <a href="../part_3/p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>, while sequence design applications are treated in <a href="../part_6/p6-ch30-design.html" class="quarto-xref"><span>Chapter 30</span></a>.</p>
<p>Protein sequence models trained autoregressively typically generate N-terminus to C-terminus, matching ribosomal synthesis and co-translational folding. Whether this biological asymmetry meaningfully improves learned representations remains unclear: autoregressive models learn conditional sequence distributions, not physical processes, and bidirectional masked language models like ESM-2 perform excellently despite having no inherent directionality. For design applications, generation direction is likely a second-order effect. <em>ESM</em> models and protein design systems like <em>ProtGPT2</em> predict amino acid sequences autoregressively, learning protein grammar and evolutionary constraints that transfer to structure prediction and function annotation <span class="citation" data-cites="ferruz_protgpt2_2022">(<a href="../bib/references.html#ref-ferruz_protgpt2_2022" role="doc-biblioref">Ferruz, Schmidt, and Höcker 2022</a>)</span>. For designing therapeutic proteins (antibodies, enzymes, peptide drugs), autoregressive generation produces candidates that respect learned constraints on foldability and function. <a href="../part_3/p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a> examines these protein language models in detail.</p>
</section>
</section>
<section id="sec-ch08-comparison" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="sec-ch08-comparison"><span class="header-section-number">8.3</span> MLM and Autoregressive Comparison</h2>
<div id="fig-bidirectional-vs-autoregressive" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bidirectional-vs-autoregressive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/03-A-fig-bidirectional-vs-autoregressive.svg" class="img-fluid figure-img"></p>
<figcaption>Bidirectional context in MLM</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/03-B-fig-bidirectional-vs-autoregressive.svg" class="img-fluid figure-img"></p>
<figcaption>Causal context in autoregressive models</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bidirectional-vs-autoregressive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: Information flow determines downstream capabilities. (A) Masked language models use bidirectional attention, allowing each position to integrate information from the entire sequence. This produces richer representations suited for understanding tasks like variant interpretation, where both flanking regions inform the prediction. (B) Autoregressive models use causal attention, where each position sees only preceding context. This restriction is essential for generation (future tokens cannot exist during sampling) but produces less informed representations. The choice between objectives should match the intended downstream application: understanding tasks favor bidirectional pretraining; generation tasks require autoregressive structure.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Knowledge Check">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>A researcher wants to predict whether a missense variant in the middle of a protein domain is pathogenic. They need to assess how the variant disrupts interactions with residues both before and after it in the sequence. Which pretraining objective would provide better representations for this task, and why?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Masked language modeling (MLM) would provide better representations for this task. MLM uses bidirectional attention, allowing the model to integrate information from both upstream and downstream residues when evaluating the variant’s impact. Since pathogenicity depends on how the variant disrupts interactions with the entire domain (both before and after the mutation), having access to full sequence context produces richer representations than autoregressive models, which only see preceding residues.</p>
</div>
</div>
</div>
</div>
</div>
<p>The tension between bidirectional understanding and generative capability represents the fundamental tradeoff between these objectives. For tasks requiring understanding of full sequence context, MLM’s bidirectional attention provides richer representations. Predicting transcription factor binding at a specific location benefits from seeing both upstream and downstream sequence, information that autoregressive models cannot access during inference. Variant effect prediction similarly benefits from full context: a missense variant’s impact depends on the entire domain, not just the preceding residues.</p>
<p>Autoregressive models offer more principled generation. Their sequential prediction structure matches the generation process exactly, whereas generating from MLM models requires iterative masking and filling procedures that were not part of pretraining. A promoter design task using MLM would require starting with random sequence, masking positions, predicting fills, remasking, and iterating until convergence. This procedure is <em>ad hoc</em> and may not produce sequences that lie on the learned distribution. Autoregressive generation is direct: sample token by token from learned conditionals.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Alignment Principle
</div>
</div>
<div class="callout-body-container callout-body">
<p>Task-specific performance depends on alignment between pretraining and downstream objectives. If the downstream task involves predicting missing information from context (variant effect prediction, binding site identification, conservation scoring), MLM pretraining provides better transfer. If the downstream task involves generation or sequential decision-making (sequence design, sampling from conditional distributions, therapeutic protein generation), autoregressive pretraining aligns more naturally.</p>
</div>
</div>
<p>Training efficiency differs between objectives in ways that affect practical decisions. MLM predicts only 15% of tokens per sequence but uses bidirectional context for each prediction. Autoregressive models predict all tokens but with unidirectional context. The effective supervision per sequence is higher for autoregressive training, but each prediction is less informed. For fixed compute budgets, the tradeoffs roughly balance, with optimal choice depending on downstream applications rather than training efficiency alone.</p>
<div id="tbl-mlm-vs-ar" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-mlm-vs-ar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.2: Comparison of MLM and autoregressive pretraining objectives for genomic applications.
</figcaption>
<div aria-describedby="tbl-mlm-vs-ar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 32%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Criterion</th>
<th>MLM (BERT-style)</th>
<th>Autoregressive (GPT-style)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Context available</strong></td>
<td>Bidirectional (full sequence)</td>
<td>Unidirectional (preceding only)</td>
</tr>
<tr class="even">
<td><strong>Primary strength</strong></td>
<td>Understanding, classification</td>
<td>Generation, sampling</td>
</tr>
<tr class="odd">
<td><strong>Supervision per sequence</strong></td>
<td>~15% of tokens</td>
<td>100% of tokens</td>
</tr>
<tr class="even">
<td><strong>Strand symmetry</strong></td>
<td>Natural (no ordering)</td>
<td>Requires augmentation</td>
</tr>
<tr class="odd">
<td><strong>Variant effect prediction</strong></td>
<td>Strong (sees flanking context)</td>
<td>Weaker (misses downstream)</td>
</tr>
<tr class="even">
<td><strong>Sequence design</strong></td>
<td>Iterative, ad hoc</td>
<td>Direct sampling</td>
</tr>
<tr class="odd">
<td><strong>Layer selection for embeddings</strong></td>
<td>Final layer typically best</td>
<td>Intermediate layers often better</td>
</tr>
<tr class="even">
<td><strong>Example models</strong></td>
<td>DNABERT, ESM-2</td>
<td>Evo, ProtGPT2</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>This alignment extends beyond task type to affect how practitioners extract and use model representations. Encoder models trained with MLM produce final-layer embeddings that work reliably across diverse downstream tasks because the pretraining objective shaped representations for general utility. Decoder models trained with next-token prediction specialize their final layers for vocabulary prediction, often making intermediate layers superior for classification and regression tasks. This <strong>layer hunting problem</strong> adds hyperparameter search burden when using decoder models for non-generative applications, sometimes requiring evaluation across all layers to identify where task-relevant information concentrates. The practical implications for model deployment are examined in <span class="quarto-unresolved-ref">?sec-ch09-layer-selection</span>.</p>
<section id="sec-ch08-hybrid" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="sec-ch08-hybrid"><span class="header-section-number">8.3.1</span> Hybrid Architectures</h3>
<p>The dichotomy between MLM and autoregressive objectives is not absolute. Hybrid architectures attempt to capture bidirectional understanding while retaining generative capability, though they add complexity and training cost.</p>
<p><strong>Permutation language modeling</strong>, introduced in <em>XLNet</em>, trains on all possible token orderings rather than a fixed left-to-right sequence <span class="citation" data-cites="yang_xlnet_2019">(<a href="../bib/references.html#ref-yang_xlnet_2019" role="doc-biblioref"><strong>yang_xlnet_2019?</strong></a>)</span>. For each training example, the model samples a random permutation of positions and predicts tokens in that order, with each position attending only to positions earlier in the sampled permutation. Across many permutations, every token eventually conditions on every other token, achieving bidirectional context in expectation while maintaining autoregressive structure for any single forward pass. The approach is elegant but computationally expensive: the permutation sampling and bookkeeping add overhead, and generation still requires committing to a specific ordering. For genomic applications, permutation LM could address strand symmetry naturally (forward and reverse orderings are equally likely), but implementations remain rare in the biological literature. One example is <em>ProtXLNet</em> examined in <span class="quarto-unresolved-ref">?sec-ch12-alternative-architectures</span>.</p>
<p><strong>Prefix language modeling</strong> offers a more practical hybrid. The model processes an initial prefix bidirectionally, building rich contextualized representations, then switches to autoregressive generation for the remainder. This architecture underlies encoder-decoder models like T5 and has been adapted for protein design, where a conditioning context (desired function, scaffold structure, or homologous sequences) is encoded bidirectionally before generating novel sequence autoregressively. <em>ProGen2</em> applies this pattern to conditional protein generation, encoding functional annotations or partial sequences as prefix context before sampling completions <span class="citation" data-cites="nijkamp_progen2_2023">(<a href="../bib/references.html#ref-nijkamp_progen2_2023" role="doc-biblioref"><strong>nijkamp_progen2_2023?</strong></a>)</span>. The prefix provides the “understanding” that guides generation, combining MLM-style bidirectional encoding where context is known with autoregressive sampling where novelty is needed. For therapeutic design, this enables specifying desired properties (binding target, expression level, stability) as encoded context while generating diverse candidate sequences that respect those constraints.</p>
<p>The cost of hybrid approaches is architectural complexity and training overhead. Pure MLM and pure autoregressive models have simpler implementations and clearer training dynamics. Whether the benefits of hybridization justify the costs depends on application requirements: tasks demanding both rich understanding and flexible generation may warrant the complexity, while tasks emphasizing one capability over the other are better served by the appropriate pure objective.</p>
</section>
</section>
<section id="sec-ch08-denoising" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="sec-ch08-denoising"><span class="header-section-number">8.4</span> Span Corruption and Denoising</h2>
<p>Clinical variant interpretation must be robust to sequencing errors, population polymorphisms, and batch effects between discovery and validation cohorts. A pathogenic variant identified in a research study must remain classifiable as pathogenic when sequenced on a different platform in a clinical laboratory, surrounded by different technical artifacts and population-specific polymorphisms. A model trained only on pristine reference sequence may fail when encountering the noise and variation present in real patient data. <strong>Denoising objectives</strong> address this by training models on corrupted inputs, building tolerance to the kinds of perturbations that occur in clinical genomics pipelines.</p>
<p><strong>Span corruption</strong> generalizes masked language modeling by introducing more complex forms of input degradation. The <em>T5</em> model popularized this approach for natural language <span class="citation" data-cites="raffel_t5_2019">(<a href="../bib/references.html#ref-raffel_t5_2019" role="doc-biblioref">Raffel et al. 2023</a>)</span>, and the principles transfer to genomic sequences with biological adaptations. Rather than masking individual tokens, span corruption masks contiguous spans of variable length and replaces each span with a single sentinel token. The model then generates the original content of all masked spans in sequence, learning to reconstruct substantial missing regions rather than isolated positions.</p>
<p>This objective teaches different aspects of sequence structure than standard MLM. Reconstructing entire spans requires understanding longer-range dependencies and compositional patterns. If a span encompasses an entire transcription factor binding motif (typically 6-12 base pairs), the model cannot infer the motif from partial information and must instead reason about the motif’s role from surrounding regulatory context. Span lengths are typically sampled from a distribution (geometric or uniform) with a mean around 3-5 tokens, creating a mix of short and long reconstruction challenges within each training example.</p>
<section id="sec-ch08-corruption" class="level3" data-number="8.4.1">
<h3 data-number="8.4.1" class="anchored" data-anchor-id="sec-ch08-corruption"><span class="header-section-number">8.4.1</span> Corruption Beyond Masking</h3>
<p>Real clinical sequencing data contains substitution errors, missing bases, and spurious insertions that simple masking does not prepare a model to handle. If a model has only ever seen clean reference sequence with masked positions, will it recognize a pathogenic variant when the surrounding bases contain sequencing errors? Training with diverse corruption strategies builds the robustness that clinical deployment demands.</p>
<p>Denoising objectives extend beyond masking to include other forms of corruption that mirror real-world data degradation. Token substitution replaces input tokens with random tokens from the vocabulary, creating corrupted sequences that resemble sequencing errors or natural variation. The model learns to distinguish correct from incorrect tokens based on surrounding context, encouraging representations that capture local consistency and motif structure. Deletion and insertion corruptions remove or add tokens at random positions, teaching models about position-invariant features that remain identifiable despite surrounding changes. For genomics, insertions and deletions are biologically realistic mutation types (indels account for approximately 15% of pathogenic variants in ClinVar <span class="citation" data-cites="landrum_clinvar_2018">(<a href="../bib/references.html#ref-landrum_clinvar_2018" role="doc-biblioref">Landrum et al. 2018</a>)</span>), and models that handle them during pretraining may better predict their effects downstream.</p>
</section>
<section id="sec-ch08-biological-corruption" class="level3" data-number="8.4.2">
<h3 data-number="8.4.2" class="anchored" data-anchor-id="sec-ch08-biological-corruption"><span class="header-section-number">8.4.2</span> Biologically Motivated Corruption</h3>
<p>The most effective corruption strategies mirror actual sources of noise in clinical genomics data. Simulating sequencing errors provides corruption strategies that match experimental reality. Base miscalls follow platform-specific patterns: Illumina sequencing shows characteristic substitution biases (favoring certain nucleotide transitions over transversions, with error rates of 0.1-1% depending on read position and quality score), while nanopore sequencing exhibits distinct error profiles concentrated in homopolymer regions where the signal for consecutive identical bases becomes ambiguous <em>[Citation Needed]</em>. Training with corruptions that mimic these error patterns may improve generalization to real sequencing data with platform-specific artifacts. The sequencing technologies producing these error patterns are examined in <a href="../part_1/p1-ch01-ngs.html" class="quarto-xref"><span>Chapter 1</span></a>, while the confounding effects of platform-specific artifacts on model evaluation appear in <a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 12</span></a>.</p>
<p><strong>Variant augmentation</strong> introduces biologically realistic sequence changes based on population variation. Randomly substituting alleles at known polymorphic sites or injecting variants from databases like gnomAD creates corrupted sequences reflecting natural genetic diversity <span class="citation" data-cites="karczewski_mutational_2020">(<a href="../bib/references.html#ref-karczewski_mutational_2020" role="doc-biblioref">Karczewski et al. 2020</a>)</span>. This teaches models that common polymorphisms are normal variation rather than errors to be corrected, potentially improving robustness for variant effect prediction where distinguishing pathogenic variants from benign polymorphisms is the central challenge. A model trained only on reference sequence might flag any deviation as potentially damaging; a model trained with variant augmentation learns which deviations are within normal population variation.</p>
<p>Structural variation simulation models larger-scale genomic changes: tandem duplications, copy number variation, and segmental rearrangements. These corruptions are harder to implement but capture realistic sources of genomic diversity beyond single-nucleotide changes. Models trained with structural variation corruptions may better understand how gene dosage changes, enhancer duplications, or domain boundary disruptions affect function. For clinical applications involving copy number variants (which underlie conditions ranging from developmental disorders like DiGeorge syndrome to cancer predisposition in hereditary breast cancer), this training signal could improve predictive accuracy.</p>
<p>The benefit of denoising pretraining extends to robustness under distribution shift. If downstream applications involve sequences from different populations, experimental platforms, or tissue contexts than the pretraining corpus, models pretrained with appropriate corruptions can maintain performance despite distribution mismatch. This matters in clinical genomics, where validation cohorts often differ from discovery cohorts in ancestry composition, sequencing technology, or phenotyping protocols. A model trained with corruptions spanning these sources of variation generalizes more reliably than one trained only on pristine reference sequence. The confounding and evaluation challenges arising from such distribution shifts are examined in <a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 12</span></a> and <a href="p2-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 12</span></a>.</p>
</section>
</section>
<section id="sec-ch08-contrastive" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="sec-ch08-contrastive"><span class="header-section-number">8.5</span> Contrastive Learning</h2>
<p>Cross-population generalization presents a persistent challenge in clinical genomics. A variant classifier trained on European ancestry cohorts may perform poorly on African ancestry patients due to different patterns of linkage disequilibrium and background polymorphism. The classifier learned to recognize pathogenic variants against a European genetic background; African genomes present the same functional variants but surrounded by different neutral polymorphisms. Contrastive learning addresses this by teaching models to recognize functional equivalence despite sequence-level differences, producing representations where a regulatory element is recognizable regardless of the population-specific variants surrounding it.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading further, consider: If you wanted to train a model that recognizes a CTCF binding site as functionally equivalent whether it appears in a European or African genome, what kind of training pairs would you construct? What should be “similar” and what should be “different”?</p>
</div>
</div>
<p>Contrastive learning takes a fundamentally different approach to self-supervised pretraining than reconstruction-based objectives. Rather than recovering corrupted inputs, contrastive objectives train models to produce similar representations for different views of the same sequence while distinguishing them from representations of unrelated sequences. The intuition is that augmented versions of a sequence (with minor corruptions, reverse complementation, or variants) should map to nearby points in representation space, while unrelated sequences should map to distant points. This teaches invariance to transformations that do not change function.</p>
<p>The algorithmic framework constructs <strong>positive pairs</strong> and <strong>negative samples</strong>. For a given anchor sequence, positive pairs are created through augmentation: reverse complementation, random cropping, variant injection, or other transformations that preserve functional identity. Negative samples are drawn from other sequences in the training batch. The model produces <strong>embeddings</strong> for all sequences, and the contrastive loss encourages anchor and positive embeddings to be similar (high cosine similarity) while pushing apart anchor and negative embeddings.</p>
<p><strong>InfoNCE loss</strong> is the most common contrastive objective <span class="citation" data-cites="oord_representation_2018">(<a href="../bib/references.html#ref-oord_representation_2018" role="doc-biblioref">Oord, Li, and Vinyals 2019</a>)</span>. The intuition is like a matching game: given a photo of a person (the anchor), you must identify which of several voice recordings belongs to the same person (the positive) among many strangers’ voices (the negatives). The loss function rewards correctly matching anchor to positive while distinguishing them from negatives. For an anchor embedding <span class="math inline">\(z_i\)</span> and positive embedding <span class="math inline">\(z_i^{+}\)</span>, InfoNCE maximizes:</p>
<p><span class="math display">\[
\mathcal{L} = -\log \frac{\exp\!\left(z_i \cdot z_i^{+} / \tau\right)}{\sum_{j} \exp\!\left(z_i \cdot z_{j} / \tau\right)}
\]</span></p>
<p>where the sum runs over the positive and all negative samples, and <span class="math inline">\(\tau\)</span> is a temperature parameter controlling the sharpness of the distribution. Why take this particular mathematical form? The objective frames contrastive learning as a classification problem: given the anchor, identify which of the many candidates is the true positive pair. The softmax structure ensures the model can only increase the score for the positive pair by simultaneously decreasing scores for negatives, forcing it to learn discriminative features rather than simply inflating all similarity scores. The temperature <span class="math inline">\(\tau\)</span> controls how harshly the model is penalized for near-misses. Lower temperatures make the model more discriminative, requiring cleaner separation between positives and negatives; a low temperature means that even small differences in similarity are amplified into large differences in the loss. The objective is equivalent to classifying the positive pair among all possible pairs, and the model learns representations that make this classification easy.</p>
<section id="sec-ch08-augmentation" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1" class="anchored" data-anchor-id="sec-ch08-augmentation"><span class="header-section-number">8.5.1</span> Augmentation Design for Genomic Sequences</h3>
<p>If you pair a sequence with its reverse complement and call them “similar,” the model learns strand symmetry. If you pair a sequence with itself plus a common SNP and call them “similar,” the model learns robustness to population variation. The augmentations you choose define what “similarity” means, and therefore what invariances your model acquires. Choosing the wrong augmentations teaches the model to ignore differences that actually matter.</p>
<p>A CTCF binding site must be recognizable whether it appears on a European or African genetic background, whether read on the forward or reverse strand, and whether the surrounding sequence contains common polymorphisms or reference alleles. Augmentation design is critical for contrastive learning because augmentations must preserve functional identity while introducing variability. If augmentations change function, the contrastive objective will learn meaningless invariances. Several augmentation strategies are biologically grounded and preserve the functional relationships that matter for downstream clinical applications.</p>
<p>The double-stranded nature of DNA provides the simplest and most reliable augmentation. Many regulatory elements function identically on either strand, and a model that fails to recognize this symmetry has learned an incomplete representation of genomic function. Reverse complementation trains the model to treat forward and reverse complement sequences as equivalent, capturing strand symmetry inherent in molecular biology. This augmentation is universally applicable and introduces no risk of changing functional identity; a TATA box is a TATA box regardless of which strand is reported.</p>
<p>Position invariance presents a subtler challenge. A transcription factor binding site should be recognizable regardless of where it falls within an input window, yet models naturally learn position-specific features. Random cropping addresses this by extracting overlapping windows from longer sequences. If a binding site appears in multiple cropped windows at different positions, the model learns that the site itself is the functionally relevant feature, not its coordinates. This proves particularly useful for tasks where genomic location matters less than local sequence content. The augmentation also provides practical benefits: a single long sequence becomes many training examples, increasing effective data diversity without collecting new data.</p>
<p>Population diversity creates perhaps the most clinically consequential augmentation challenge. A classifier trained only on reference sequence may treat any deviation as potentially significant, when in fact most human genetic variation is neutral. Variant injection addresses this by introducing common polymorphisms or simulated mutations as augmentation. If the variants are neutral (common variants from gnomAD with high allele frequency, which are unlikely to be damaging; see <a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a> for gnomAD resource details), treating variant and reference sequences as positive pairs teaches robustness to genetic background. This is particularly valuable for cross-population generalization, where models must recognize functional elements despite surrounding sequence polymorphism that differs between ancestry groups. A model trained with variant augmentation learns that a CTCF binding site is functionally equivalent whether it appears on European or African genetic background.</p>
<div id="tbl-augmentation-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-augmentation-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.3: Augmentation strategies for genomic contrastive learning.
</figcaption>
<div aria-describedby="tbl-augmentation-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 19%">
<col style="width: 26%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Augmentation Type</th>
<th>How It Works</th>
<th>Invariance Taught</th>
<th>Clinical Relevance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Reverse complement</strong></td>
<td>Swap strand, reverse sequence</td>
<td>Strand symmetry</td>
<td>TF binding orientation-agnostic</td>
</tr>
<tr class="even">
<td><strong>Random cropping</strong></td>
<td>Extract overlapping windows</td>
<td>Position invariance</td>
<td>Motif recognition anywhere in window</td>
</tr>
<tr class="odd">
<td><strong>Variant injection</strong></td>
<td>Insert common polymorphisms</td>
<td>Population robustness</td>
<td>Cross-ancestry generalization</td>
</tr>
<tr class="even">
<td><strong>Nucleotide substitution</strong></td>
<td>Random base changes</td>
<td>Noise tolerance</td>
<td>Sequencing error robustness</td>
</tr>
<tr class="odd">
<td><strong>Ortholog pairing</strong></td>
<td>Pair sequences across species</td>
<td>Species invariance</td>
<td>Model organism to human transfer</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The choice of negative samples shapes what distinctions the model learns to make. Random genomic sequences provide straightforward negatives but may be too easy to distinguish: any functional regulatory sequence is readily separable from random intergenic sequence. Harder negatives force more informative learning. Sequences from paralogous genes share evolutionary history but have diverged in function; distinguishing them requires learning subtle functional signatures. Pseudogenes resemble their functional counterparts but lack activity; recognizing this difference teaches the model what makes a gene functional. Orthologous regions in distant species test whether the model has learned species-invariant features. The difficulty of negatives should match the granularity of distinctions required for downstream tasks.</p>
</section>
<section id="sec-ch08-cross-species" class="level3" data-number="8.5.2">
<h3 data-number="8.5.2" class="anchored" data-anchor-id="sec-ch08-cross-species"><span class="header-section-number">8.5.2</span> Cross-Species Contrastive Learning</h3>
<div id="fig-cross-species-contrastive" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cross-species-contrastive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/04-fig-cross-species-contrastive.svg" class="img-fluid figure-img"></p>
<figcaption>Cross-species contrastive learning for species-invariant representations</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cross-species-contrastive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.4: Cross-species contrastive learning for species-invariant representations. Orthologous sequences from human and mouse share functional identity despite 75 million years of divergence and substantial nucleotide differences (shown in alignment). Treating orthologs as positive pairs teaches the model to extract conserved functional features while ignoring species-specific sequence differences. Non-orthologous sequences serve as negatives. The resulting embedding space clusters orthologs together regardless of species, enabling transfer from model organism experiments to human predictions.
</figcaption>
</figure>
</div>
<p>Leveraging evolutionary relationships for self-supervision enables a distinctive form of contrastive learning. Orthologous sequences from different species share functional identity despite nucleotide divergence accumulated over millions of years of evolution. Treating orthologous pairs as positives and non-orthologous pairs as negatives teaches the model to extract species-invariant functional features. A human enhancer and its mouse ortholog should map to similar embeddings despite 75 million years of sequence divergence, while unrelated sequences should map to distant embeddings.</p>
<p>This approach has direct implications for drug development and therapeutic translation. Many drug targets are validated in mouse models before human clinical trials; roughly 95% of cancer drugs that succeed in mouse models fail in human trials, often because the models do not adequately capture human biology <em>[Citation Needed]</em>. A model pretrained with human-mouse contrastive pairs may generalize better to predicting drug response in humans based on mouse efficacy data, or to transferring regulatory circuit designs from model organisms to human cell types. The evolutionary record provides implicit labels about functional equivalence that would be expensive to obtain through direct experimental annotation.</p>
<p>Sequence embedding quality improves with contrastive pretraining in ways that benefit clinical applications. Models trained contrastively produce embedding spaces where functionally similar sequences cluster together, enabling nearest-neighbor search for annotating novel variants (finding similar characterized variants), sequence retrieval for identifying regulatory homologs, and unsupervised clustering of regulatory elements. For variant effect prediction, contrastive pretraining improves robustness: if the model learns that sequences differing only by neutral variants are functionally equivalent, it will better distinguish truly disruptive variants from benign polymorphisms.</p>
</section>
</section>
<section id="sec-ch08-multitask" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="sec-ch08-multitask"><span class="header-section-number">8.6</span> Multi-Task Pretraining</h2>
<p>Predicting variant pathogenicity requires integrating multiple lines of evidence: evolutionary conservation, protein structure effects, splicing changes, and regulatory disruption. A variant in <em>TTN</em> (the gene encoding titin, mutated in 25% of dilated cardiomyopathy cases <em>[Citation Needed]</em>) might be pathogenic because it disrupts protein folding, because it alters splicing, or because it affects regulatory binding sites. No single assay captures all these dimensions. <strong>Multi-task pretraining</strong> addresses this by jointly optimizing for diverse prediction tasks, learning representations that capture the multiple facets of genomic function relevant to clinical interpretation.</p>
<p>Multi-task pretraining combines multiple related objectives during the same training run, jointly optimizing for several prediction tasks. Different tasks provide complementary supervision signals: masking captures local sequence patterns, chromatin prediction captures regulatory function, conservation scoring captures evolutionary constraint, and expression prediction captures transcriptional consequences. Representations that satisfy all tasks simultaneously develop richer and more general features than any single objective alone.</p>
<section id="sec-ch08-task-selection" class="level3" data-number="8.6.1">
<h3 data-number="8.6.1" class="anchored" data-anchor-id="sec-ch08-task-selection"><span class="header-section-number">8.6.1</span> Task Selection and Architecture</h3>
<p>A model trained to predict chromatin accessibility learns different features than one trained to predict gene expression, even from identical sequences. Add evolutionary conservation prediction, and the representations shift again. The tasks you choose for multi-task pretraining determine what biological signals your model captures, so how do you select the right combination?</p>
<p>The first design decision is which tasks to include. Ideally, tasks should be diverse enough to provide distinct supervision signals but related enough to benefit from shared representations. For genomic models, effective combinations include masked language modeling for general sequence structure, chromatin accessibility prediction for regulatory function, gene expression prediction for transcriptional output, evolutionary conservation scoring for functional constraint, and variant frequency prediction from population databases. Each task operates on the same input sequence but predicts different outputs using task-specific head layers. The shared backbone encoder processes the sequence into intermediate representations, and separate prediction heads map these representations to task-specific outputs.</p>
</section>
<section id="sec-ch08-loss-weighting" class="level3" data-number="8.6.2">
<h3 data-number="8.6.2" class="anchored" data-anchor-id="sec-ch08-loss-weighting"><span class="header-section-number">8.6.2</span> Loss Weighting and Balancing</h3>
<p>Training a model on five tasks sounds straightforward until you discover that chromatin accessibility loss is ten times larger than expression prediction loss, causing the model to optimize almost exclusively for chromatin while ignoring expression. Worse, the expression task may actually be more important for your downstream clinical application. How you weight the contribution of each task to the total loss can make or break multi-task pretraining.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Technical Challenge
</div>
</div>
<div class="callout-body-container callout-body">
<p>Once tasks are selected, their relative contributions to the total loss must be determined. This is a significant source of training instability and requires careful tuning.</p>
</div>
</div>
<p>With <span class="math inline">\(\mathcal{L}_1, \ldots, \mathcal{L}_K\)</span> representing individual task losses, the multi-task loss combines them:</p>
<p><span class="math display">\[\mathcal{L}_{\text{total}} = \sum_{k=1}^K w_k \mathcal{L}_k\]</span></p>
<p>where <em>w_k</em> are task weights. Why not simply sum losses with equal weights? Different tasks operate at fundamentally different scales. A regression task predicting gene expression might have losses in the range of 0.1-1.0, while a classification task predicting binary binding states might have losses ranging from 0.01-0.1 depending on class balance. Without weighting, the high-loss task dominates gradient updates, effectively ignoring the signal from other tasks. Even with similar loss scales, tasks differ in how quickly they are learned. If one task converges rapidly while another requires extended training, equal weighting means the converged task continues providing gradient signal that may interfere with learning the harder task. Dynamic weighting approaches address this by adjusting weights during training based on learning progress, using uncertainty estimation, gradient norms, or task-specific validation performance as signals for rebalancing. Uncertainty-based weighting learns task weights as parameters, treating high-loss tasks as inherently more uncertain and down-weighting their contribution. Gradient-based methods normalize gradients across tasks to prevent any single task from dominating updates.</p>
</section>
<section id="sec-ch08-multitask-examples" class="level3" data-number="8.6.3">
<h3 data-number="8.6.3" class="anchored" data-anchor-id="sec-ch08-multitask-examples"><span class="header-section-number">8.6.3</span> Large-Scale Multi-Task Examples</h3>
<p><em>Enformer</em> exemplifies large-scale multi-task pretraining for genomics <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. The model predicts over 5,000 genomic assays simultaneously: ChIP-seq signals for hundreds of transcription factors and histone marks, DNase-seq and ATAC-seq accessibility across cell types, CAGE transcription initiation profiles, and more. This massive multi-task objective (covering 674 DNase-seq, 4,675 ChIP-seq, and 638 CAGE experiments from ENCODE and Roadmap Epigenomics <span class="citation" data-cites="kagda_encode_2025">(<a href="../bib/references.html#ref-kagda_encode_2025" role="doc-biblioref">Kagda et al. 2025</a>)</span>) forces the model to learn representations capturing diverse regulatory signals.</p>
<p>The task diversity in <em>Enformer</em> provides supervision far richer than any single assay. A model trained only on DNase-seq learns general accessibility patterns but misses transcription factor specificity: it cannot distinguish which factors bind to accessible regions. A model trained only on H3K27ac ChIP-seq captures active enhancers but misses repressive marks that indicate silenced regulatory elements. Training on all assays jointly allows the model to disentangle overlapping and complementary signals, learning representations that generalize across regulatory contexts. For clinical variant interpretation, this means <em>Enformer</em> can predict how a regulatory variant affects enhancer activity, chromatin state, transcription factor binding, and gene expression simultaneously. <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a> examines <em>Enformer</em> and related regulatory models in detail.</p>
<div id="fig-multitask-pretraining" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multitask-pretraining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/05-fig-multitask-pretraining.svg" class="img-fluid figure-img"></p>
<figcaption>Multi-task pretraining architecture for comprehensive regulatory prediction</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multitask-pretraining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.5: Multi-task pretraining architecture for comprehensive regulatory prediction. A shared encoder backbone (convolutional layers for local patterns, transformer layers for long-range integration) processes input sequences. Multiple prediction heads branch from shared representations to predict diverse genomic readouts: chromatin accessibility (DNase-seq, ATAC-seq), histone modifications (H3K27ac, H3K4me3, H3K27me3), transcription factor binding for hundreds of factors, and gene expression via CAGE. Enformer jointly predicts over 5,000 tracks (674 DNase + 4,675 ChIP-seq + 638 CAGE), forcing shared representations to capture diverse regulatory signals. This multi-task pressure produces representations that generalize beyond any single assay.
</figcaption>
</figure>
</div>
<p><em>Borzoi</em> extends this paradigm to full RNA-seq coverage prediction, jointly modeling transcription initiation, splicing, and transcript abundance <span class="citation" data-cites="linder_borzoi_2025">(<a href="../bib/references.html#ref-linder_borzoi_2025" role="doc-biblioref">Linder et al. 2025</a>)</span>. By predicting continuous coverage across gene bodies rather than just expression levels, <em>Borzoi</em> captures splicing patterns that are invisible to models predicting only total expression. This has direct clinical relevance: many pathogenic variants act through splicing disruption rather than protein-coding changes, and models that capture splicing patterns can identify variants that traditional expression-based approaches miss.</p>
<p>Combining MLM with functional prediction represents another multi-task configuration. The model predicts masked tokens through a language modeling head while simultaneously predicting chromatin accessibility or other functional readouts through regression heads. This hybrid objective balances sequence-level pretraining with functional supervision. The MLM component ensures the model learns general sequence patterns even in regions without functional annotations (the majority of the genome lacks chromatin or expression measurements in any given cell type), while the functional prediction component focuses learning on biologically relevant features.</p>
</section>
<section id="sec-ch08-multitask-failure" class="level3" data-number="8.6.4">
<h3 data-number="8.6.4" class="anchored" data-anchor-id="sec-ch08-multitask-failure"><span class="header-section-number">8.6.4</span> When Multi-Task Learning Fails</h3>
<p>More tasks should mean more learning signal, so why does adding a third task sometimes make performance on the original two tasks worse? Multi-task learning can fail in surprising ways, and understanding these failure modes is essential before committing computational resources to joint training.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider two tasks: (1) predicting splice site usage from a 20bp window, and (2) predicting enhancer-promoter interactions from a 200kb window. Why might jointly training these tasks hurt performance on both compared to training them separately? What property of representations might create conflict?</p>
</div>
</div>
<p>Task interference presents the primary concern with multi-task learning. If tasks require conflicting representations, jointly optimizing for both may compromise performance on each compared to single-task baselines. In genomics, this might occur if one task benefits from very local features (splice site prediction, which depends on short consensus sequences spanning roughly 10 base pairs) while another requires long-range context (enhancer activity prediction, which depends on distant promoter interactions spanning 100 kilobases). The shared backbone must compromise, potentially learning suboptimal representations for both.</p>
<p><strong>Negative transfer</strong> occurs when adding a task actually hurts downstream performance compared to training without it. This can happen if the additional task introduces noise (poorly measured assays with high experimental variance), if task weights are poorly balanced (causing one task to dominate gradients), or if the auxiliary task shifts learned representations away from features useful for target applications. The risk of negative transfer increases with task diversity: distantly related tasks are more likely to require conflicting representations.</p>
<p>The benefits of multi-task pretraining are largest when tasks are complementary and data for individual tasks is limited. If chromatin data is sparse for a particular cell type but gene expression data is abundant, jointly training on both may improve performance on both compared to single-task models. The shared representations allow information to flow between tasks, compensating for data scarcity in any single modality. When functional labels exist at scale and tasks are genuinely related, multi-task pretraining consistently outperforms single-task alternatives.</p>
</section>
</section>
<section id="sec-ch08-staged" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="sec-ch08-staged"><span class="header-section-number">8.7</span> Staged Pretraining Strategies</h2>
<p>Training a foundation model in a single phase rarely produces optimal results. The computational constraints that make pretraining expensive also make experimentation prohibitive: once committed to training at scale, practitioners cannot easily adjust hyperparameters, data mixtures, or objectives mid-run. <strong>Staged pretraining</strong> addresses this by decomposing training into sequential phases, each optimized for different learning goals. A model might learn basic sequence statistics from shorter contexts before extending to long-range dependencies, or acquire general sequence grammar before specializing to regulatory regions. These staged approaches improve both training stability and final model quality compared to monolithic training on the full data and context from the start.</p>
<p>The biological rationale mirrors curriculum learning in human education: master fundamentals before tackling advanced material. A medical student learns anatomy before pathology; a genomic model might learn local motif structure before enhancer-promoter interactions spanning 100 kilobases. When <em>HyenaDNA</em> attempted direct training on million-base contexts, optimization diverged. Progressive context extension, starting at shorter windows and gradually increasing, proved essential for stable learning <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. This curriculum effect appears across architectures: the structure of what is learned first shapes what can be learned later.</p>
<section id="sec-ch08-context-curriculum" class="level3" data-number="8.7.1">
<h3 data-number="8.7.1" class="anchored" data-anchor-id="sec-ch08-context-curriculum"><span class="header-section-number">8.7.1</span> Context Length Curricula</h3>
<p>An enhancer 50 kilobases from its target promoter can only be modeled if the context window spans that entire distance. But training directly on 100-kilobase sequences often fails: the optimization diverges, the model never converges, and weeks of compute are wasted. How do you build a model that understands long-range regulatory interactions when training on long sequences is so unstable?</p>
<p>Long-range genomic dependencies present a fundamental training challenge. Attention mechanisms scale quadratically with context length, making training on long sequences orders of magnitude more expensive than short sequences. Beyond computational cost, optimization dynamics change with context length: models processing thousands of tokens face different gradient distributions than those processing hundreds. Context length curricula address both challenges by training first on tractable short contexts, then progressively extending to longer sequences.</p>
<p><em>HyenaDNA</em> exemplifies this approach. Initial pretraining used contexts of a few thousand bases, allowing rapid iteration through the genome and stable optimization. As training progressed, context windows expanded through intermediate stages (8 kilobases, then 32 kilobases) until reaching the target of one million bases. Each stage inherited weights from the previous stage, with learning rate warmup to accommodate the new context regime. The curriculum proved necessary for convergence: ablations attempting direct long-context training without warmup phases showed instability and degraded final performance.</p>
<p><em>Gene42</em> extended this pattern with explicit continuous pretraining stages <span class="citation" data-cites="gene42_2024">(<a href="../bib/references.html#ref-gene42_2024" role="doc-biblioref"><strong>gene42_2024?</strong></a>)</span>. The model trained initially at 4,096 tokens, then continued pretraining at 8,192, 16,384, 32,768, 65,536, and finally 192,000 tokens. Each context extension required adjustments to positional encodings (specifically, modifications to rotary position embedding parameters to prevent distant-token interactions from collapsing). The staged approach enabled dense attention at scales where training from scratch would be computationally prohibitive. Notably, the longest-context checkpoints required only incremental compute beyond the shorter-context stages, amortizing the total training cost across the curriculum.</p>
<div id="fig-context-curriculum" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-context-curriculum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch08/06-fig-context-curriculum.svg" class="img-fluid figure-img"></p>
<figcaption>Context length curriculum for stable long-range pretraining</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-context-curriculum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.6: Context length curriculum for stable long-range pretraining. Training begins at short contexts (1-4 kb) where optimization is stable and local patterns are learned efficiently. At each stage transition, weights are inherited from the previous checkpoint with learning rate warmup to accommodate the new context regime. Progressive extension through intermediate stages (16 kb, 64 kb) enables the model to learn medium-range dependencies before tackling full long-range contexts. This curriculum proved essential for HyenaDNA: direct training at million-base contexts without warmup led to divergence. The inset shows how attention patterns become sparser at longer contexts, requiring more training steps to develop the structured patterns that capture distant regulatory relationships.
</figcaption>
</figure>
</div>
<p>The mechanism underlying context curricula relates to how attention patterns develop. Early in training, attention distributions are nearly uniform (each position attends similarly to all others). As learning progresses, sparse, structured attention patterns emerge: promoter positions attend to enhancer regions; splice site positions attend to branch points. These structured patterns require many training steps to develop. Starting at long contexts forces the model to learn both basic sequence statistics and long-range structure simultaneously, competing objectives that can interfere. The curriculum separates these learning phases: master local patterns first (at short context), then learn to integrate them across distance (at extended context).</p>
</section>
<section id="sec-ch08-domain-adaptive" class="level3" data-number="8.7.2">
<h3 data-number="8.7.2" class="anchored" data-anchor-id="sec-ch08-domain-adaptive"><span class="header-section-number">8.7.2</span> Domain-Adaptive Pretraining</h3>
<p>You have a pretrained model on human DNA and want to apply it to bacterial genomes. Do you continue training from the human checkpoint, hoping to preserve useful sequence patterns while adapting to different GC content and codon usage? Or do you start fresh, reasoning that bacterial genomes are different enough that the human pretraining provides no benefit and may even hurt? This decision can save months of compute or waste it entirely.</p>
<p>When should a genomic model build on existing pretrained weights versus train from scratch? The question parallels a broader tension in NLP, where <strong>domain-adaptive pretraining</strong> (continuing training from a general-domain checkpoint on domain-specific data) competes with <strong>from-scratch domain pretraining</strong> (training exclusively on domain-specific data). The answer depends on data abundance, domain distance, and whether vocabulary transfer is feasible.</p>
<p><em>BioBERT</em> pioneered domain-adaptive pretraining for biomedical text, initializing from general-domain <em>BERT</em> weights and continuing pretraining on PubMed abstracts <span class="citation" data-cites="lee_biobert_2020">(<a href="../bib/references.html#ref-lee_biobert_2020" role="doc-biblioref"><strong>lee_biobert_2020?</strong></a>)</span>. This approach leverages general language understanding (syntax, semantics, common knowledge) acquired during the initial pretraining phase, requiring only adaptation to domain-specific vocabulary and concepts. The strategy proved effective when biomedical data was limited and general-domain pretraining captured useful structure.</p>
<p><em>PubMedBERT</em> challenged this assumption by demonstrating that from-scratch pretraining on biomedical text alone could outperform domain-adaptive approaches when sufficient domain data exists <span class="citation" data-cites="gu_pubmedbert_2021">(<a href="../bib/references.html#ref-gu_pubmedbert_2021" role="doc-biblioref"><strong>gu_pubmedbert_2021?</strong></a>)</span>. The key insight was vocabulary mismatch: general-domain tokenizers fragment biomedical terms into meaningless subwords (“lymphoma” becomes “l”, “##ym”, “##ph”, “##oma”), forcing the model to reconstruct meaning from pieces rather than representing concepts directly. Training from scratch with a domain-specific vocabulary eliminated this overhead. When domain-specific data is abundant (as with PubMed’s millions of abstracts), the benefits of general-domain initialization may not justify the vocabulary mismatch cost.</p>
<p>For genomic foundation models, these lessons translate directly. DNA sequence tokenizers (k-mers, BPE, single nucleotides) differ fundamentally from text tokenizers, making vocabulary transfer impossible. A general-purpose language model cannot serve as initialization for a DNA model; sequence statistics must be learned from genomic data. The relevant decision becomes whether to continue pretraining a genomic model on new data (adding species, adding functional annotations) or train a new model from scratch.</p>
</section>
<section id="sec-ch08-continued-pretraining" class="level3" data-number="8.7.3">
<h3 data-number="8.7.3" class="anchored" data-anchor-id="sec-ch08-continued-pretraining"><span class="header-section-number">8.7.3</span> Continued Pretraining on Expanded Data</h3>
<p>As new genomic data becomes available (additional reference genomes, expanded population sequencing, new functional assays), practitioners face a choice: retrain from scratch incorporating all data, or continue pretraining the existing model on new data. Continued pretraining offers computational efficiency but risks <strong>catastrophic forgetting</strong>, where learning new patterns overwrites previously acquired knowledge.</p>
<p>The risk of catastrophic forgetting is real but manageable. When <em>DNABERT</em> checkpoints are continued on new species, performance on original species may degrade if the new training distribution differs substantially. Mitigation strategies include replay (mixing old and new data during continued pretraining), elastic weight consolidation (penalizing changes to weights important for prior tasks), and modular architectures that isolate new learning from established representations <span class="citation" data-cites="mccloskey_catastrophic_1989">(<a href="../bib/references.html#ref-mccloskey_catastrophic_1989" role="doc-biblioref">McCloskey and Cohen 1989</a>)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>When to continue pretraining vs.&nbsp;train from scratch:</strong></p>
<ul>
<li><strong>Continue pretraining</strong> when: New data is similar to original training data; you want to add species within the same kingdom; computational budget is limited; you need to preserve performance on original tasks</li>
<li><strong>Train from scratch</strong> when: New tokenization scheme is needed; target domain differs fundamentally (e.g., viral genomes after training on mammals); original model shows systematic biases you want to eliminate</li>
<li><strong>Hybrid approach</strong>: Use replay buffers mixing old and new data to balance adaptation with retention</li>
</ul>
</div>
</div>
<p>Continued pretraining makes sense when new data complements rather than contradicts prior training. Adding closely related species to a model pretrained on mammals will likely transfer well; adding bacterial genomes with fundamentally different GC content, codon usage, and regulatory logic may require more careful integration or separate models. The decision should be guided by biological similarity between old and new data distributions.</p>
</section>
<section id="sec-ch08-multiobjective-schedule" class="level3" data-number="8.7.4">
<h3 data-number="8.7.4" class="anchored" data-anchor-id="sec-ch08-multiobjective-schedule"><span class="header-section-number">8.7.4</span> Multi-Objective Schedules</h3>
<p>Beyond data and context curricula, the pretraining objective itself can be staged. A model might train with masked language modeling to learn sequence statistics, then switch to or add contrastive objectives to learn functional similarity, then incorporate task-specific prediction heads. Each objective teaches different aspects of sequence function.</p>
<p><em>DNABERT-S</em> demonstrates staged objective curricula <span class="citation" data-cites="zhou_dnabert-s_2024">(<a href="../bib/references.html#ref-zhou_dnabert-s_2024" role="doc-biblioref"><strong>zhou_dnabert-s_2024?</strong></a>)</span>. The model’s Curriculum Contrastive Learning (C²LR) strategy divides training into two phases. Phase I applies standard contrastive learning (distinguishing similar from dissimilar sequences using straightforward positive and negative pairs). Phase II introduces harder anchors through Manifold Instance Mixup, creating challenging training examples by mixing hidden representations at random layers. The curriculum ensures the model first masters basic discrimination before tackling the more difficult mixed-representation task.</p>
<p>Multi-task schedules represent another form of objective staging. Rather than training all tasks jointly from the start, some practitioners introduce tasks sequentially: begin with the primary self-supervised objective, then add auxiliary tasks once representations have stabilized. This staging prevents auxiliary tasks from dominating early learning when the model has not yet acquired basic sequence understanding. The optimal schedule depends on task interactions: complementary tasks (MLM plus chromatin prediction) may benefit from joint training, while potentially conflicting tasks (short-range splice prediction plus long-range enhancer prediction) may benefit from staging.</p>
</section>
<section id="sec-ch08-data-complexity" class="level3" data-number="8.7.5">
<h3 data-number="8.7.5" class="anchored" data-anchor-id="sec-ch08-data-complexity"><span class="header-section-number">8.7.5</span> Data Complexity Curricula</h3>
<p>Should the model see simple repetitive sequences first and complex regulatory regions later, or dive straight into the hardest examples? The order in which training data is presented affects what the model learns and how efficiently it learns it. Presenting complex enhancer grammar before the model has mastered basic motif recognition may waste training steps on examples the model cannot yet learn from.</p>
<p>Not all genomic sequences present equal learning difficulty. Repetitive regions offer little to learn beyond detecting repeats; complex regulatory regions require learning combinatorial motif grammar; intergenic regions provide evolutionary constraint signal distinct from coding regions. <strong>Data complexity curricula</strong> order training examples from simple to complex, allowing models to build representations progressively.</p>
<p>Complexity ordering can be implicit or explicit. Implicit ordering emerges from data sampling: if training oversamples certain regions early (promoters, conserved sequences), the model learns those patterns first. Explicit ordering requires defining complexity metrics (sequence entropy, motif density, evolutionary conservation, expression variability) and scheduling examples accordingly. While less explored in genomics than context curricula, data complexity scheduling offers potential for improving sample efficiency, particularly when some sequence classes are over-represented in training corpora.</p>
</section>
<section id="sec-ch08-staged-practical" class="level3" data-number="8.7.6">
<h3 data-number="8.7.6" class="anchored" data-anchor-id="sec-ch08-staged-practical"><span class="header-section-number">8.7.6</span> Practical Considerations</h3>
<p>Staged pretraining introduces complexity that must be weighed against benefits. Each stage requires decisions about duration (training steps or epochs), transition criteria (loss plateaus, validation metrics), learning rate schedules (warmup for each stage, decay patterns), and checkpoint selection (which intermediate checkpoint to continue from). Poor choices at stage transitions can negate the benefits of staging.</p>
<p>Diagnostic monitoring becomes more important with staged training. Track not only aggregate loss but per-stage metrics: does performance on short-context tasks degrade when extending to long contexts? Do earlier-stage representations remain useful? Does adding new data cause forgetting of prior patterns? These diagnostics require evaluation infrastructure beyond simple loss tracking but provide essential feedback for curriculum design.</p>
<p>The computational tradeoffs favor staging in most scenarios. Training a single long-context model from scratch requires expensive long-sequence batches for the entire training run. Staged training front-loads cheap short-context training, investing in expensive long-context training only after the model has learned basic patterns. The total compute may be similar or even higher with staging, but the amortized cost per useful representation is often lower because more learning happens during the efficient early stages.</p>
<p>When staged pretraining fails, the causes typically involve poor stage transitions or misaligned curricula. If later stages require unlearning early-stage representations (because the curriculum taught the wrong patterns first), staging may harm rather than help. Careful alignment between curriculum structure and intended final capabilities remains essential. The goal is not staging for its own sake but decomposing a difficult learning problem into tractable sequential subproblems.</p>
</section>
</section>
<section id="sec-ch08-data" class="level2" data-number="8.8">
<h2 data-number="8.8" class="anchored" data-anchor-id="sec-ch08-data"><span class="header-section-number">8.8</span> Data Strategies for Pretraining</h2>
<p>Corpus construction establishes the foundation for pretraining and determines what patterns the model can learn. A clinical variant classifier is only as good as the evolutionary and population diversity captured in its pretraining corpus. If the training data underrepresents African genetic variation (African populations harbor more genetic diversity than all other continental populations combined, yet constitute a small fraction of most reference panels <em>[Citation Needed]</em>), the resulting model will underperform on African ancestry patients. These data decisions have direct consequences for health equity and clinical utility.</p>
<section id="sec-ch08-reference-genomes" class="level3" data-number="8.8.1">
<h3 data-number="8.8.1" class="anchored" data-anchor-id="sec-ch08-reference-genomes"><span class="header-section-number">8.8.1</span> Reference Genomes and Population Diversity</h3>
<p>Human genome assemblies like GRCh38 provide the standard starting point, offering high-quality, contiguous sequence spanning all chromosomes (roughly 3.1 billion base pairs of assembled sequence, representing about 92% of the full genome before telomere-to-telomere completion <em>[Citation Needed]</em>). Training on the reference genome allows models to learn patterns characteristic of human DNA: base composition, repeat structure, gene organization, and regulatory architecture. The reference genome represents a single haploid consensus, missing variation present in human populations, but provides the foundation for most pretraining approaches.</p>
<p>Population-scale variation can be incorporated through variant databases. Rather than training only on reference sequence, injecting variants at observed population frequencies creates synthetic diploid genomes reflecting real genetic diversity. This teaches models that common polymorphisms are normal variation, potentially improving robustness and variant effect prediction. gnomAD provides allele frequencies across over 800,000 individuals spanning diverse ancestries, enabling population-aware training. <strong>Pan-genome</strong> approaches extend this by representing multiple high-quality assemblies from diverse individuals, capturing structural variation and population-specific haplotypes that a single reference cannot represent <span class="citation" data-cites="karczewski_mutational_2020">(<a href="../bib/references.html#ref-karczewski_mutational_2020" role="doc-biblioref">Karczewski et al. 2020</a>)</span>. <a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a> examines these data resources and their construction in detail.</p>
</section>
<section id="sec-ch08-repeats" class="level3" data-number="8.8.2">
<h3 data-number="8.8.2" class="anchored" data-anchor-id="sec-ch08-repeats"><span class="header-section-number">8.8.2</span> Repeat Handling</h3>
<p>Half the human genome consists of repetitive sequences, but should your model spend half its training capacity learning to recognize LINE elements and Alu repeats? The answer depends entirely on your downstream task. For variant interpretation in coding regions, repeats are noise; for studying repeat expansion disorders like Huntington disease, they are the signal. How you handle repeats during pretraining shapes what your model can and cannot do.</p>
<p>Repeat handling impacts pretraining in ways that depend on downstream applications. Simple repeats, tandem repeats, and transposable elements occupy roughly half of the human genome but contribute less directly to protein-coding function than unique sequences <em>[Citation Needed]</em>. Hard-masking repeats (replacing repetitive bases with N characters, rendering <code>ATCGATCGATCG</code> as <code>NNNNNNNNNNNN</code>) reduces training data but may discard information relevant to some tasks; many regulatory elements derive from transposable elements, and some disease-associated repeats (like the CGG expansion in FMR1 causing Fragile X syndrome, or the CAG expansion in HTT causing Huntington disease) are clinically important. Soft-masking retains sequence information while using lowercase to flag repetitive regions (<code>atcgatcgatcg</code>), allowing models to learn differential representations for repeats and unique sequences. Tools like RepeatMasker produce these annotations, and training pipelines can be configured to treat masked regions differently: exclude them entirely, downweight their contribution to loss, or process them normally while preserving the distinction in tokenization.</p>
</section>
<section id="sec-ch08-multispecies" class="level3" data-number="8.8.3">
<h3 data-number="8.8.3" class="anchored" data-anchor-id="sec-ch08-multispecies"><span class="header-section-number">8.8.3</span> Multi-Species and Augmentation Strategies</h3>
<p>Incorporating genomes from model organisms and related species enables models to learn evolutionary conservation patterns and may improve transfer between species. Including mouse, zebrafish, and other commonly used experimental organisms provides training signal about which sequence features are functionally constrained across evolution. For therapeutic development that relies on animal model data, multi-species pretraining provides the foundation for cross-species generalization.</p>
<p>Data augmentation strategies (see <a href="#sec-ch08-augmentation" class="quarto-xref"><span>Section 8.5.1</span></a>) complement multi-species training by artificially increasing diversity within species. These augmentations are typically applied on-the-fly during training rather than pre-computed, maintaining flexibility in the training pipeline and ensuring the model sees different augmented versions across epochs.</p>
</section>
</section>
<section id="sec-ch08-optimization" class="level2" data-number="8.9">
<h2 data-number="8.9" class="anchored" data-anchor-id="sec-ch08-optimization"><span class="header-section-number">8.9</span> Optimization and Scaling</h2>
<p>Training a model to predict variant effects in genes like <em>BRCA1</em> requires not just the right objective but also stable optimization that converges to useful representations. A model that diverges during training or gets stuck in poor local minima will fail clinically regardless of how well-designed its architecture may be. The optimization details that seem merely technical have direct consequences for whether the final model can reliably distinguish pathogenic from benign variants.</p>
<section id="sec-ch08-hyperparameters" class="level3" data-number="8.9.1">
<h3 data-number="8.9.1" class="anchored" data-anchor-id="sec-ch08-hyperparameters"><span class="header-section-number">8.9.1</span> Optimization Hyperparameters</h3>
<p>A learning rate that is too high causes loss to spike and never recover; a learning rate that is too low means the model crawls toward convergence over months instead of weeks. Gradient explosions can corrupt a week of training in a single batch. These optimization details seem purely technical until they determine whether your model learns useful representations or produces garbage.</p>
<p>Stable training requires careful attention to learning rate scheduling, gradient management, and numerical precision. Learning rate warmup gradually increases the learning rate from near-zero over the first several thousand steps, preventing early training instability when the model has random initializations and large gradient variance. After warmup, cosine decay schedules reduce the learning rate following a cosine curve from peak to near-zero over training, providing aggressive learning early when gradients are most informative and gentle refinement late as the model approaches convergence.</p>
<p><strong>Gradient clipping</strong> (see <a href="p2-ch07-attention.html#sec-ch07-gradients" class="quarto-xref"><span>Section 7.6.3</span></a>) uses a norm threshold of 1.0 in most genomic pretraining configurations. Without clipping, a single anomalous batch can destabilize training irreversibly.</p>
<p>Modern pretraining relies on mixed precision arithmetic (<code>float16</code> or <code>bfloat16</code> instead of <code>float32</code>) to reduce memory consumption and accelerate computation on modern GPUs. Loss scaling prevents numerical underflow in <code>float16</code>, and careful handling of gradient updates ensures stability. Mixed precision is now standard for large-scale pretraining, roughly doubling throughput with minimal impact on model quality.</p>
</section>
<section id="sec-ch08-scaling" class="level3" data-number="8.9.2">
<h3 data-number="8.9.2" class="anchored" data-anchor-id="sec-ch08-scaling"><span class="header-section-number">8.9.2</span> Scaling Laws and Emergence</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced Topic
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section discusses scaling laws and emergent capabilities that remain active research areas. The quantitative relationships described here are empirically observed but may not generalize to all architectures or domains.</p>
</div>
</div>
<p>Pretraining scales with model size, sequence length, and dataset size in predictable ways that have profound implications for what models can learn. Larger models with more parameters capture more complex patterns but require more data and compute to train. <em>ESM-2’s</em> largest variant has 15 billion parameters <span class="citation" data-cites="lin_esm-2_2022">(<a href="../bib/references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span> (roughly one parameter for every two amino acids in its training corpus), enabling it to capture subtle evolutionary constraints invisible to smaller models. Longer sequence contexts enable learning of long-range dependencies but increase memory requirements quadratically for standard attention. More diverse training data improves generalization but requires proportionally more training time.</p>
<p>The relationships between scale and capability follow power laws that predict optimal resource allocation <span class="citation" data-cites="hoffmann_training_2022">(<a href="../bib/references.html#ref-hoffmann_training_2022" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>. For a fixed computational budget, there exists an optimal balance between model size and training data: models that are too large undertrain on available data, while models that are too small cannot capture the complexity present in abundant data. These <strong>scaling laws</strong>, first characterized systematically for language models <span class="citation" data-cites="kaplan_scaling_2020">(<a href="../bib/references.html#ref-kaplan_scaling_2020" role="doc-biblioref">Kaplan et al. 2020</a>)</span>, appear to hold for genomic foundation models as well, though the precise exponents and constants differ. Understanding these relationships guides decisions about when to scale up versus when to improve data quality or model architecture. <a href="../part_3/p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a> examines these scaling relationships in detail, formalizing the observations introduced here into quantitative laws that define the foundation model paradigm.</p>
<p>These empirical scaling laws contradict classical intuitions from statistical learning theory. A helpful analogy: traditional theory says that a student who memorizes 100 flashcards cannot pass a 1000-question exam—there simply are not enough examples to learn general rules. The Vapnik-Chervonenkis framework formalizes this intuition, predicting generalization error scaling as <span class="math inline">\(O(1/\sqrt{N})\)</span>: halving the error requires quadrupling the training data, with model complexity strictly bounded by available examples <span class="citation" data-cites="vapnik_statistical_1998">(<a href="../bib/references.html#ref-vapnik_statistical_1998" role="doc-biblioref"><strong>vapnik_statistical_1998?</strong></a>)</span>. Models with parameters vastly exceeding training examples should memorize rather than generalize—like a student with more flashcard slots than study examples. Yet foundation models operate precisely in this “overparameterized” regime and still improve predictably with scale, as if the student discovered underlying patterns that make even unseen questions answerable. This <strong>benign overfitting</strong> reflects properties of gradient descent and high-dimensional loss landscapes that classical worst-case bounds did not anticipate <span class="citation" data-cites="zhang_understanding_2017 belkin_reconciling_2019">(<a href="../bib/references.html#ref-zhang_understanding_2017" role="doc-biblioref"><strong>zhang_understanding_2017?</strong></a>; <a href="../bib/references.html#ref-belkin_reconciling_2019" role="doc-biblioref"><strong>belkin_reconciling_2019?</strong></a>)</span>.</p>
<p>Beyond smooth improvements in loss, scale produces qualitative changes in model capabilities that were absent at smaller scales. Language models exhibit <strong>emergent behaviors</strong> (in-context learning, chain-of-thought reasoning, few-shot generalization) that appear only above certain parameter thresholds <em>[Citation Needed]</em>. Whether genomic models exhibit analogous emergent capabilities remains an active research question with early evidence suggesting they do. <em>ESM-2</em>, trained on evolutionary sequence databases containing hundreds of millions of protein sequences from UniRef <span class="citation" data-cites="suzek_uniref_2007">(<a href="../bib/references.html#ref-suzek_uniref_2007" role="doc-biblioref">Suzek et al. 2007</a>)</span>, develops structural understanding of proteins despite receiving no explicit structural supervision: the three-dimensional contacts emerge from predicting amino acid sequences alone. <em>Evo</em>, trained autoregressively on genomes, learns to generate sequences with realistic gene structure and regulatory organization. These emergent properties cannot be predicted by extrapolating from smaller models, making them both scientifically interesting and practically difficult to anticipate.</p>
</section>
</section>
<section id="sec-ch08-diagnostics" class="level2" data-number="8.10">
<h2 data-number="8.10" class="anchored" data-anchor-id="sec-ch08-diagnostics"><span class="header-section-number">8.10</span> Training Diagnostics</h2>
<p>A two-week pretraining run that begins diverging on day three but is not detected until day thirteen wastes ten days of compute and forces rollback to earlier checkpoints. The failure is not losing everything; it’s continuing to train a model that stopped learning useful representations long before anyone noticed. Early detection of training issues is essential for avoiding wasted computation and ensuring models achieve the representations necessary for clinical utility.</p>
<section id="sec-ch08-monitoring" class="level3" data-number="8.10.1">
<h3 data-number="8.10.1" class="anchored" data-anchor-id="sec-ch08-monitoring"><span class="header-section-number">8.10.1</span> Monitoring Loss and Gradients</h3>
<p>When loss suddenly spikes on day five of a two-week training run, is it a temporary anomaly that will self-correct, or the beginning of catastrophic divergence that will waste the remaining nine days? Knowing which metrics to watch, and what patterns signal trouble, lets you catch problems early and decide whether to continue, roll back, or restart with different hyperparameters.</p>
<p>Training loss curves should decrease smoothly in early stages, eventually plateauing as the model approaches convergence. Sudden spikes suggest numerical instability (often from learning rate issues or gradient explosion), inappropriate optimization hyperparameters, or corrupted data batches. Persistent plateaus may indicate insufficient model capacity, inappropriate objectives, or learning rates that prevent further improvement. Tracking loss on held-out validation data monitors generalization: if training loss decreases while validation loss increases, the model is <strong>overfitting</strong> to the training corpus.</p>
<p>Gradient norms indicate whether optimization is proceeding normally. Very small gradients suggest the <strong>vanishing gradient problem</strong>, preventing effective learning in early layers. Very large gradients suggest instability that gradient clipping should catch. Tracking per-layer gradient norms helps diagnose where problems originate in deep networks; if early layers show vanishing gradients while later layers have healthy magnitudes, the architecture may need residual connections or different initialization.</p>
</section>
<section id="sec-ch08-probing" class="level3" data-number="8.10.2">
<h3 data-number="8.10.2" class="anchored" data-anchor-id="sec-ch08-probing"><span class="header-section-number">8.10.2</span> Functional Probing</h3>
<p>Loss can decrease steadily for weeks while the model learns patterns useless for your downstream task. A model might become excellent at predicting repeat sequences (which dominate the genome) while learning nothing about regulatory elements (which matter for variant interpretation). Probing intermediate checkpoints on biologically meaningful tasks reveals whether learning is on track, regardless of what the loss curve shows.</p>
<p>Probing tasks provide functional sanity checks during pretraining that loss curves alone cannot capture. Simple downstream evaluations (predicting known splice sites, identifying transcription factor binding motifs, distinguishing exons from introns) can be run periodically on intermediate checkpoints to verify that learned representations capture biologically meaningful patterns. If probing performance plateaus or degrades while pretraining loss continues improving, the model may be learning patterns that do not transfer to downstream tasks. This dissociation between pretraining loss and probe performance signals a problem with the pretraining objective or data that would otherwise go undetected until final evaluation.</p>
</section>
</section>
<section id="sec-ch08-selection" class="level2" data-number="8.11">
<h2 data-number="8.11" class="anchored" data-anchor-id="sec-ch08-selection"><span class="header-section-number">8.11</span> Strategy Selection</h2>
<p>A clinician asking “will this <em>BRCA1</em> variant cause disease?” needs a model pretrained with objectives that capture protein function and evolutionary constraint. A synthetic biologist asking “can you design me a promoter with 10-fold higher expression?” needs generative capabilities that MLM does not provide. Selecting a pretraining approach involves matching computational investment to the clinical or research questions the model must ultimately answer.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Knowledge Check">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>For each scenario below, identify the most appropriate pretraining objective and justify your choice:</p>
<ol type="1">
<li>Building a model to predict which regulatory variants disrupt transcription factor binding</li>
<li>Designing novel enzyme sequences for industrial biocatalysis</li>
<li>Creating a variant classifier that works across diverse human populations</li>
<li>Predicting splice-altering variants in rare disease patients</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li>MLM - predicting binding disruption requires bidirectional context to assess how the variant affects the motif and flanking regions. (2) Autoregressive/next-token prediction - designing new sequences requires generation capabilities that sample coherent proteins respecting learned sequence grammar. (3) Contrastive learning - robustness across populations benefits from pretraining that learns invariance to genetic variation through contrastive objectives on variant pairs. (4) MLM - splice site prediction needs to assess how variants affect both donor and acceptor sites with full bidirectional context around the junction.</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<p>For most general-purpose DNA or protein models, MLM pretraining provides a strong default. It learns bidirectional context, scales efficiently, and transfers well to diverse downstream tasks. <em>DNABERT</em> and <em>DNABERT-2</em> exemplify this approach for genomics, while <em>ESM</em> models demonstrate its effectiveness for proteins. Start with MLM unless there is a specific reason to prefer alternatives.</p>
<p>Next-token prediction is preferred when generation is the primary goal. If designing sequences from scratch (therapeutic proteins, synthetic promoters, regulatory circuits), sampling from autoregressive models produces coherent outputs respecting learned grammar. <em>Evo</em> and similar models demonstrate this for genomic sequence generation. The autoregressive structure makes conditional generation straightforward, enabling design applications that MLM does not naturally support.</p>
<p>Multi-task pretraining makes sense when functional labels are available at scale and tasks are complementary. <em>Enformer’s</em> success with thousands of chromatin assays demonstrates the power of multi-task learning when data supports it. The infrastructure requirements are higher (handling heterogeneous data, balancing losses across tasks, maintaining separate prediction heads), but the resulting representations capture functional information that pure sequence-based objectives miss.</p>
<p>Contrastive learning is valuable for cross-species applications or when robustness to variation is critical. If transferring models trained on model organisms to related species, or improving robustness to genetic polymorphism across human populations, contrastive pretraining on orthologous pairs or variant-augmented sequences provides targeted benefits.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Decision Tree for Objective Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Start here:</strong> What is your primary downstream task?</p>
<ol type="1">
<li><strong>Variant effect prediction / classification</strong> → MLM (needs bidirectional context)</li>
<li><strong>Sequence generation / design</strong> → Autoregressive (needs sampling capability)</li>
<li><strong>Cross-population / cross-species transfer</strong> → Contrastive (needs invariance)</li>
<li><strong>Multiple regulatory predictions</strong> → Multi-task (needs diverse functional features)</li>
</ol>
<p><strong>Secondary considerations:</strong> - Limited compute? → Start with existing pretrained model, fine-tune - Need both understanding AND generation? → Consider hybrid (prefix LM) or ensemble - Clinical deployment with equity requirements? → Ensure training data includes diverse populations</p>
</div>
</div>
<p>When deciding whether to pretrain from scratch or start from existing models, starting from pretrained checkpoints is almost always preferable if an appropriate model exists. <strong>Fine-tuning</strong> a <em>DNABERT-2</em> checkpoint on a new task is faster and more data-efficient than training from scratch. Pretraining from scratch is necessary when using new tokenization schemes (incompatible vocabularies prevent weight transfer), targeting species without suitable existing models, or experimenting with fundamentally different architectures where pretrained weights cannot transfer. <a href="p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a> examines these adaptation strategies in detail.</p>
</section>
<section id="sec-ch08-case-studies" class="level2" data-number="8.12">
<h2 data-number="8.12" class="anchored" data-anchor-id="sec-ch08-case-studies"><span class="header-section-number">8.12</span> Pretraining in Practice: Case Studies</h2>
<p>Examining how successful models were pretrained provides concrete lessons and design patterns that inform new projects. Each case study illustrates how architectural choices, data decisions, and optimization strategies combine to produce models with distinct capabilities.</p>
<section id="sec-ch08-dnabert" class="level3" data-number="8.12.1">
<h3 data-number="8.12.1" class="anchored" data-anchor-id="sec-ch08-dnabert"><span class="header-section-number">8.12.1</span> DNABERT</h3>
<p><em>DNABERT</em> introduced MLM pretraining to genomics by adapting <em>BERT’s</em> architecture to DNA sequences with overlapping <span class="math inline">\(k\)</span>-mer tokenization <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. The model was pretrained on the human genome with 6-mer tokens, masking 15% of tokens at random. Standard <em>BERT</em> hyperparameters proved effective: AdamW optimizer with warmup, dropout regularization, and <strong>layer normalization</strong>. The key lessons include the importance of tokenization choice (k-mers capture motif-level patterns better than single nucleotides for regulatory prediction), the value of reverse complement augmentation for strand symmetry, and the transferability of representations across tasks never seen during pretraining. The full <em>DNABERT</em> architecture and its subsequent developments (<em>DNABERT-2</em>, <em>DNABERT-S</em>) are examined in <a href="../part_3/p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>.</p>
</section>
<section id="sec-ch08-hyenadna" class="level3" data-number="8.12.2">
<h3 data-number="8.12.2" class="anchored" data-anchor-id="sec-ch08-hyenadna"><span class="header-section-number">8.12.2</span> HyenaDNA</h3>
<p><em>HyenaDNA</em> demonstrated that efficient long-range architectures enable pretraining on extremely long contexts <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. By using Hyena layers with subquadratic complexity, <em>HyenaDNA</em> scaled to contexts spanning one million bases (compared to typical transformer limits of a few thousand bases), far beyond standard transformers. Pretraining used single-nucleotide next-token prediction with a <strong>curriculum</strong> that progressively increased context length from shorter windows to full million-base sequences. This curriculum learning proved essential: training directly on long contexts without warmup led to instability. The lessons include the feasibility of million-base contexts with appropriate architectures, the benefits of curriculum learning for context scaling, and the emergence of long-range regulatory patterns when models have sufficient <strong>receptive field</strong>.</p>
</section>
<section id="sec-ch08-enformer" class="level3" data-number="8.12.3">
<h3 data-number="8.12.3" class="anchored" data-anchor-id="sec-ch08-enformer"><span class="header-section-number">8.12.3</span> Enformer</h3>
<p><em>Enformer</em> pioneered multi-task chromatin prediction at scale <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. The model was pretrained jointly on over 5,000 assays from ENCODE, Roadmap Epigenomics, and related consortia, using a hybrid convolutional-transformer architecture with 200 kilobase context (spanning typical enhancer-promoter distances in mammalian genomes). Task weighting was balanced to prevent any single assay from dominating. Key insights include the power of large-scale multi-task learning for capturing diverse regulatory signals, the effectiveness of combining convolutions for local patterns with transformers for long-range interactions, and the interpretability benefits of attention patterns that reveal learned enhancer-promoter relationships. <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a> examines <em>Enformer’s</em> architecture and regulatory predictions in detail.</p>
</section>
<section id="sec-ch08-esm2" class="level3" data-number="8.12.4">
<h3 data-number="8.12.4" class="anchored" data-anchor-id="sec-ch08-esm2"><span class="header-section-number">8.12.4</span> ESM-2</h3>
<p><em>ESM-2</em> represents the state of the art for protein language models, scaling to 15 billion parameters trained on UniRef databases containing sequences from hundreds of millions of protein families <span class="citation" data-cites="lin_esm-2_2022">(<a href="../bib/references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. Pretraining used standard MLM on amino acid sequences at unprecedented scale. The lessons include the continued benefit of scaling (larger models and more data improve even at billions of parameters, with no plateau in sight), the value of evolutionary diversity (pretraining on distinct protein families captures constraints invisible in any single genome), and the emergence of structural understanding from sequence alone (<em>ESM-2</em> representations encode three-dimensional contacts despite no explicit structural supervision during pretraining). <a href="../part_3/p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a> examines <em>ESM-2</em> and related protein language models comprehensively.</p>
<div id="tbl-model-case-studies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-case-studies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.4: Summary of pretraining strategies across major genomic foundation models.
</figcaption>
<div aria-describedby="tbl-model-case-studies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 17%">
<col style="width: 14%">
<col style="width: 25%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Objective</th>
<th>Context</th>
<th>Key Innovation</th>
<th>Primary Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>DNABERT</strong></td>
<td>MLM (15%)</td>
<td>512 tokens</td>
<td>k-mer tokenization</td>
<td>Regulatory prediction</td>
</tr>
<tr class="even">
<td><strong>DNABERT-2</strong></td>
<td>MLM</td>
<td>512 tokens</td>
<td>BPE tokenization</td>
<td>Multi-species transfer</td>
</tr>
<tr class="odd">
<td><strong>HyenaDNA</strong></td>
<td>Autoregressive</td>
<td>1M bases</td>
<td>Subquadratic attention</td>
<td>Long-range dependencies</td>
</tr>
<tr class="even">
<td><strong>Evo</strong></td>
<td>Autoregressive</td>
<td>100kb+</td>
<td>StripedHyena layers</td>
<td>Genome generation</td>
</tr>
<tr class="odd">
<td><strong>Enformer</strong></td>
<td>Multi-task regression</td>
<td>200kb</td>
<td>5,000+ chromatin tracks</td>
<td>Regulatory variant effects</td>
</tr>
<tr class="even">
<td><strong>ESM-2</strong></td>
<td>MLM</td>
<td>Full protein</td>
<td>15B parameters</td>
<td>Protein structure/function</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="sec-ch08-open-questions" class="level2" data-number="8.13">
<h2 data-number="8.13" class="anchored" data-anchor-id="sec-ch08-open-questions"><span class="header-section-number">8.13</span> Open Questions</h2>
<p>Despite rapid progress, fundamental questions about genomic pretraining remain open, and resolving them will determine whether the next generation of models can achieve clinical-grade reliability.</p>
<p>Optimal objective combinations remain unclear: should we jointly train with MLM and chromatin prediction, or train sequentially? How many auxiliary tasks help before diminishing returns? Do contrastive and generative objectives complement each other or interfere? These questions have different answers for different downstream applications, and systematic characterization is incomplete.</p>
<p>Incorporating biological priors versus learning from scratch presents a design tension. Known motifs, pathway structure, and evolutionary constraints could be encoded in model architecture or initialization. Hand-engineered features risk encoding false assumptions, but pure data-driven learning may rediscover basic biology inefficiently. Hybrid approaches combining priors with learned representations remain underexplored.</p>
<p><strong>Continual pretraining</strong> as new data arrives is increasingly relevant. As sequencing technologies improve and new assays emerge, updating pretrained models without catastrophic forgetting of prior knowledge presents challenges. Online learning and elastic weight consolidation are potential solutions that remain largely untested in genomics at scale.</p>
<p>The relationship between pretraining scale and downstream performance follows predictable patterns that are still being characterized for genomic models. Understanding these relationships more precisely would guide resource allocation and set realistic expectations for what different scales of pretraining can achieve. These scaling considerations connect to the broader foundation model paradigm examined in <a href="../part_3/p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a>.</p>
</section>
<section id="sec-ch08-sequence-to-knowledge" class="level2" data-number="8.14">
<h2 data-number="8.14" class="anchored" data-anchor-id="sec-ch08-sequence-to-knowledge"><span class="header-section-number">8.14</span> From Sequence Statistics to Biological Knowledge</h2>
<p>The fundamental insight underlying self-supervised pretraining is that patterns relevant to biological function are embedded in sequence statistics themselves. A model that learns to predict masked nucleotides must implicitly capture the evolutionary constraints, regulatory grammar, and structural requirements that determine what sequences are viable. A model that learns to generate plausible protein sequences must internalize the constraints that distinguish functional proteins from random polymers. These objectives extract biological knowledge from sequence without requiring explicit functional labels, transforming abundant unlabeled data into learned representations that improve data efficiency for downstream applications.</p>
<p>The choice of pretraining objective shapes what models learn in ways that propagate to clinical utility. Masked language modeling teaches bidirectional sequence understanding, making it the natural choice for variant interpretation and regulatory prediction where full flanking context informs the prediction. Next-token prediction teaches generative capabilities essential for therapeutic protein design and synthetic sequence generation. Contrastive learning teaches invariance to perturbations, building robustness that transfers across species and populations. Aligning pretraining objectives with intended applications improves transfer; misalignment creates representational gaps that fine-tuning may struggle to bridge.</p>
<p>Self-supervised pretraining has become the default approach for building genomic foundation models. The DNA language models in <a href="../part_3/p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>, protein language models in <a href="../part_3/p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a>, and regulatory sequence models in <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a> each employ variants of these objectives tailored to their sequence modalities and downstream applications. The transfer learning methods examined in <a href="p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a> determine how effectively pretrained representations can be adapted to specific clinical and research tasks, completing the pipeline from raw sequence through learned representation to deployed application.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Test Yourself">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Test Yourself
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reviewing the summary, test your recall:</p>
<ol type="1">
<li>How does masked language modeling encourage bidirectional context integration, and why does this make MLM-pretrained models better suited for variant effect prediction than autoregressive models?</li>
<li>Explain why span masking forces compositional learning while random token masking encourages local pattern learning. Which is better for regulatory element prediction?</li>
<li>What is the positional bias problem in autoregressive models, and why does it create asymmetric representation quality across sequence positions?</li>
<li>When does zero-shot transfer succeed without any task-specific fine-tuning, and what alignment between pretraining and downstream tasks makes this possible?</li>
<li>Why does context length curriculum (training first on short sequences, then progressively extending) improve optimization stability compared to training directly on long contexts?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>MLM uses bidirectional attention, allowing each position to integrate information from the entire sequence. During prediction of a masked token, the model sees both upstream and downstream context. For variant effect prediction, pathogenicity often depends on how a variant disrupts interactions with residues or motifs on both sides, making bidirectional context essential. Autoregressive models only see preceding tokens, missing critical downstream information.</p></li>
<li><p>Random token masking allows models to predict each masked position from immediately adjacent context (e.g., predicting the missing base in TATA_A is easy from local cues). Span masking removes entire contiguous blocks, eliminating local context entirely. To predict a masked 6bp transcription factor binding motif, the model must reason from more distant regulatory context about what kind of element belongs there, forcing it to learn compositional patterns. Span masking is better for regulatory element prediction because it forces the model to treat motifs as integrated functional units.</p></li>
<li><p>Autoregressive models predict each token given only preceding tokens. Early positions in a sequence have minimal conditioning information (the first token has none), while later positions benefit from rich preceding context. This creates asymmetric representation quality: early positions are learned less reliably than later ones. For genomic sequences with no inherent directionality, this introduces artifacts unrelated to biology.</p></li>
<li><p>Zero-shot transfer succeeds when the pretraining objective perfectly aligns with the downstream task structure. For example, if a model is pretrained to predict masked tokens in transcription factor binding sites, and the downstream task is to identify disrupted binding sites, the representations learned during pretraining directly solve the downstream task without additional adaptation. This requires that the pretraining objective captures exactly the patterns the downstream task needs.</p></li>
<li><p>Starting with long contexts forces models to learn both basic sequence statistics and long-range structure simultaneously, creating competing optimization pressures that can interfere. Context curricula separate these phases: models first learn local patterns efficiently at short contexts where optimization is stable, then learn to integrate these patterns across distance at extended contexts. This staged approach allows attention patterns to develop gradually rather than requiring structured long-range attention from random initialization.</p></li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Core Concepts:</strong></p>
<ul>
<li><p><strong>Pretraining objectives encode assumptions</strong> about what matters in biological sequences. The objective you choose determines what your model learns and what it can do downstream.</p></li>
<li><p><strong>MLM (masked language modeling)</strong> teaches bidirectional context integration. Best for: variant effect prediction, binding site identification, any task where both upstream and downstream context matters.</p></li>
<li><p><strong>Next-token prediction (autoregressive)</strong> teaches generation. Best for: sequence design, therapeutic protein generation, sampling novel sequences that respect learned constraints.</p></li>
<li><p><strong>Contrastive learning</strong> teaches invariance to perturbations. Best for: cross-population generalization, cross-species transfer, robustness to genetic background variation.</p></li>
<li><p><strong>Multi-task pretraining</strong> learns representations capturing multiple facets of function. Best for: when diverse functional labels are available and tasks are complementary.</p></li>
<li><p><strong>Staged pretraining</strong> decomposes difficult learning into tractable phases. Context curricula, objective curricula, and data curricula all improve training stability.</p></li>
<li><p><strong>Data strategy determines what can be learned.</strong> Population diversity, species coverage, and repeat handling all affect downstream clinical utility.</p></li>
</ul>
<p><strong>Key Tradeoffs:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Choice</th>
<th>Favors…</th>
<th>At the cost of…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MLM</td>
<td>Understanding</td>
<td>Generation</td>
</tr>
<tr class="even">
<td>Autoregressive</td>
<td>Generation</td>
<td>Bidirectional context</td>
</tr>
<tr class="odd">
<td>Contrastive</td>
<td>Robustness</td>
<td>Reconstruction ability</td>
</tr>
<tr class="even">
<td>Multi-task</td>
<td>Rich features</td>
<td>Training complexity</td>
</tr>
<tr class="odd">
<td>Long context</td>
<td>Long-range patterns</td>
<td>Compute cost</td>
</tr>
</tbody>
</table>
<p><strong>Looking Ahead:</strong> <a href="p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a> examines how to adapt pretrained models to specific downstream tasks through fine-tuning, feature extraction, and other transfer learning strategies.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-avsec_enformer_2021" class="csl-entry" role="listitem">
Avsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. <span>“[<span>Enformer</span>] <span>Effective</span> Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.”</span> <em>Nature Methods</em> 18 (October): 1196–1203. <a href="https://doi.org/10.1038/s41592-021-01252-x">https://doi.org/10.1038/s41592-021-01252-x</a>.
</div>
<div id="ref-devlin_bert_2019" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: <span>Pre</span>-Training of <span>Deep</span> <span>Bidirectional</span> <span>Transformers</span> for <span>Language</span> <span>Understanding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.
</div>
<div id="ref-ferruz_protgpt2_2022" class="csl-entry" role="listitem">
Ferruz, Noelia, Steffen Schmidt, and Birte Höcker. 2022. <span>“<span>ProtGPT2</span> Is a Deep Unsupervised Language Model for Protein Design.”</span> <em>Nature Communications</em> 13 (1): 4348. <a href="https://doi.org/10.1038/s41467-022-32007-7">https://doi.org/10.1038/s41467-022-32007-7</a>.
</div>
<div id="ref-hoffmann_training_2022" class="csl-entry" role="listitem">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>“Training <span>Compute</span>-<span>Optimal</span> <span>Large</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2203.15556">https://doi.org/10.48550/arXiv.2203.15556</a>.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-kagda_encode_2025" class="csl-entry" role="listitem">
Kagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. <span>“Data Navigation on the <span>ENCODE</span> Portal.”</span> <em>Nature Communications</em> 16 (1): 9592. <a href="https://doi.org/10.1038/s41467-025-64343-9">https://doi.org/10.1038/s41467-025-64343-9</a>.
</div>
<div id="ref-kaplan_scaling_2020" class="csl-entry" role="listitem">
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. <span>“Scaling <span>Laws</span> for <span>Neural</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2001.08361">https://doi.org/10.48550/arXiv.2001.08361</a>.
</div>
<div id="ref-karczewski_mutational_2020" class="csl-entry" role="listitem">
Karczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. <span>“The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.”</span> <em>Nature</em> 581 (7809): 434–43. <a href="https://doi.org/10.1038/s41586-020-2308-7">https://doi.org/10.1038/s41586-020-2308-7</a>.
</div>
<div id="ref-landrum_clinvar_2018" class="csl-entry" role="listitem">
Landrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. <span>“<span>ClinVar</span>: Improving Access to Variant Interpretations and Supporting Evidence.”</span> <em>Nucleic Acids Research</em> 46 (D1): D1062–67. <a href="https://doi.org/10.1093/nar/gkx1153">https://doi.org/10.1093/nar/gkx1153</a>.
</div>
<div id="ref-lin_esm-2_2022" class="csl-entry" role="listitem">
Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. <span>“[<span>ESM</span>-2] <span>Language</span> Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.07.20.500902">https://doi.org/10.1101/2022.07.20.500902</a>.
</div>
<div id="ref-linder_borzoi_2025" class="csl-entry" role="listitem">
Linder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. <span>“[<span>Borzoi</span>] <span>Predicting</span> <span>RNA</span>-Seq Coverage from <span>DNA</span> Sequence as a Unifying Model of Gene Regulation.”</span> <em>Nature Genetics</em> 57 (4): 949–61. <a href="https://doi.org/10.1038/s41588-024-02053-6">https://doi.org/10.1038/s41588-024-02053-6</a>.
</div>
<div id="ref-mccloskey_catastrophic_1989" class="csl-entry" role="listitem">
McCloskey, Michael, and Neal Cohen. 1989. <span>“Catastrophic <span>Interference</span> in <span>Connectionist</span> <span>Networks</span>: <span>The</span> <span>Sequential</span> <span>Learning</span> <span>Problem</span>.”</span> In <em>Psychology of <span>Learning</span> and <span>Motivation</span></em>, 24:109–65. Academic Press. <a href="https://doi.org/10.1016/S0079-7421(08)60536-8">https://doi.org/10.1016/S0079-7421(08)60536-8</a>.
</div>
<div id="ref-nguyen_sequence_2024" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. <span>“Sequence Modeling and Design from Molecular to Genome Scale with <span>Evo</span>.”</span> <em>Science</em> 386 (6723): eado9336. <a href="https://doi.org/10.1126/science.ado9336">https://doi.org/10.1126/science.ado9336</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-oord_representation_2018" class="csl-entry" role="listitem">
Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. 2019. <span>“Representation <span>Learning</span> with <span>Contrastive</span> <span>Predictive</span> <span>Coding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1807.03748">https://doi.org/10.48550/arXiv.1807.03748</a>.
</div>
<div id="ref-raffel_t5_2019" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. <span>“Exploring the <span>Limits</span> of <span>Transfer</span> <span>Learning</span> with a <span>Unified</span> <span>Text</span>-to-<span>Text</span> <span>Transformer</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1910.10683">https://doi.org/10.48550/arXiv.1910.10683</a>.
</div>
<div id="ref-suzek_uniref_2007" class="csl-entry" role="listitem">
Suzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. <span>“<span>UniRef</span>: Comprehensive and Non-Redundant <span>UniProt</span> Reference Clusters.”</span> <em>Bioinformatics</em> 23 (10): 1282–88. <a href="https://doi.org/10.1093/bioinformatics/btm098">https://doi.org/10.1093/bioinformatics/btm098</a>.
</div>
<div id="ref-zhou_dnabert-2_2024" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_2/p2-ch07-attention.html" class="pagination-link" aria-label="Transformers and Attention">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_2/p2-ch09-transfer.html" class="pagination-link" aria-label="Transfer Learning Foundations">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>