<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Transfer and Adaptation – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_3/p3--architectures.html" rel="next">
<link href="../part_2/p2-ch08-pretraining.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch09-transfer.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer and Adaptation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch09-transfer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer and Adaptation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch10-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch11-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch12-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch14-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--multi-scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Systems and Scale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch15-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch16-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch17-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch19-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch20-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Benchmarks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch21-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Evaluation Principles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch22-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Confounders and Leakage</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch25-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch26-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch28-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch29-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Ethical and Frontiers</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#source-and-target-domains" id="toc-source-and-target-domains" class="nav-link active" data-scroll-target="#source-and-target-domains"><span class="header-section-number">9.1</span> Source and Target Domains</a>
  <ul class="collapse">
  <li><a href="#the-gap-between-pretraining-and-deployment" id="toc-the-gap-between-pretraining-and-deployment" class="nav-link" data-scroll-target="#the-gap-between-pretraining-and-deployment"><span class="header-section-number">9.1.1</span> The Gap Between Pretraining and Deployment</a></li>
  <li><a href="#recognizing-transfer-outcomes" id="toc-recognizing-transfer-outcomes" class="nav-link" data-scroll-target="#recognizing-transfer-outcomes"><span class="header-section-number">9.1.2</span> Recognizing Transfer Outcomes</a></li>
  </ul></li>
  <li><a href="#feature-extraction-with-frozen-backbones" id="toc-feature-extraction-with-frozen-backbones" class="nav-link" data-scroll-target="#feature-extraction-with-frozen-backbones"><span class="header-section-number">9.2</span> Feature Extraction with Frozen Backbones</a>
  <ul class="collapse">
  <li><a href="#linear-probing" id="toc-linear-probing" class="nav-link" data-scroll-target="#linear-probing"><span class="header-section-number">9.2.1</span> Linear Probing</a></li>
  <li><a href="#when-linear-probing-fails" id="toc-when-linear-probing-fails" class="nav-link" data-scroll-target="#when-linear-probing-fails"><span class="header-section-number">9.2.2</span> When Linear Probing Fails</a></li>
  </ul></li>
  <li><a href="#parameter-efficient-fine-tuning" id="toc-parameter-efficient-fine-tuning" class="nav-link" data-scroll-target="#parameter-efficient-fine-tuning"><span class="header-section-number">9.3</span> Parameter-Efficient Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#low-rank-adaptation" id="toc-low-rank-adaptation" class="nav-link" data-scroll-target="#low-rank-adaptation"><span class="header-section-number">9.3.1</span> Low-Rank Adaptation</a></li>
  <li><a href="#alternative-peft-approaches" id="toc-alternative-peft-approaches" class="nav-link" data-scroll-target="#alternative-peft-approaches"><span class="header-section-number">9.3.2</span> Alternative PEFT Approaches</a></li>
  </ul></li>
  <li><a href="#full-fine-tuning" id="toc-full-fine-tuning" class="nav-link" data-scroll-target="#full-fine-tuning"><span class="header-section-number">9.4</span> Full Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#making-full-fine-tuning-work" id="toc-making-full-fine-tuning-work" class="nav-link" data-scroll-target="#making-full-fine-tuning-work"><span class="header-section-number">9.4.1</span> Making Full Fine-Tuning Work</a></li>
  <li><a href="#the-risks-of-unconstrained-adaptation" id="toc-the-risks-of-unconstrained-adaptation" class="nav-link" data-scroll-target="#the-risks-of-unconstrained-adaptation"><span class="header-section-number">9.4.2</span> The Risks of Unconstrained Adaptation</a></li>
  </ul></li>
  <li><a href="#probing-representations" id="toc-probing-representations" class="nav-link" data-scroll-target="#probing-representations"><span class="header-section-number">9.5</span> Probing Representations</a>
  <ul class="collapse">
  <li><a href="#what-probing-reveals-about-pretrained-models" id="toc-what-probing-reveals-about-pretrained-models" class="nav-link" data-scroll-target="#what-probing-reveals-about-pretrained-models"><span class="header-section-number">9.5.1</span> What Probing Reveals About Pretrained Models</a></li>
  <li><a href="#probing-guides-adaptation-strategy" id="toc-probing-guides-adaptation-strategy" class="nav-link" data-scroll-target="#probing-guides-adaptation-strategy"><span class="header-section-number">9.5.2</span> Probing Guides Adaptation Strategy</a></li>
  </ul></li>
  <li><a href="#choosing-an-adaptation-strategy" id="toc-choosing-an-adaptation-strategy" class="nav-link" data-scroll-target="#choosing-an-adaptation-strategy"><span class="header-section-number">9.6</span> Choosing an Adaptation Strategy</a>
  <ul class="collapse">
  <li><a href="#data-quantity-determines-what-is-possible" id="toc-data-quantity-determines-what-is-possible" class="nav-link" data-scroll-target="#data-quantity-determines-what-is-possible"><span class="header-section-number">9.6.1</span> Data Quantity Determines What Is Possible</a></li>
  <li><a href="#task-similarity-determines-what-is-necessary" id="toc-task-similarity-determines-what-is-necessary" class="nav-link" data-scroll-target="#task-similarity-determines-what-is-necessary"><span class="header-section-number">9.6.2</span> Task Similarity Determines What Is Necessary</a></li>
  </ul></li>
  <li><a href="#domain-shift-and-cross-context-transfer" id="toc-domain-shift-and-cross-context-transfer" class="nav-link" data-scroll-target="#domain-shift-and-cross-context-transfer"><span class="header-section-number">9.7</span> Domain Shift and Cross-Context Transfer</a>
  <ul class="collapse">
  <li><a href="#types-of-domain-shift-in-genomics" id="toc-types-of-domain-shift-in-genomics" class="nav-link" data-scroll-target="#types-of-domain-shift-in-genomics"><span class="header-section-number">9.7.1</span> Types of Domain Shift in Genomics</a></li>
  <li><a href="#detecting-and-mitigating-shift" id="toc-detecting-and-mitigating-shift" class="nav-link" data-scroll-target="#detecting-and-mitigating-shift"><span class="header-section-number">9.7.2</span> Detecting and Mitigating Shift</a></li>
  </ul></li>
  <li><a href="#few-shot-and-zero-shot-learning" id="toc-few-shot-and-zero-shot-learning" class="nav-link" data-scroll-target="#few-shot-and-zero-shot-learning"><span class="header-section-number">9.8</span> Few-Shot and Zero-Shot Learning</a>
  <ul class="collapse">
  <li><a href="#few-shot-learning-with-minimal-examples" id="toc-few-shot-learning-with-minimal-examples" class="nav-link" data-scroll-target="#few-shot-learning-with-minimal-examples"><span class="header-section-number">9.8.1</span> Few-Shot Learning with Minimal Examples</a></li>
  <li><a href="#zero-shot-transfer-without-task-specific-data" id="toc-zero-shot-transfer-without-task-specific-data" class="nav-link" data-scroll-target="#zero-shot-transfer-without-task-specific-data"><span class="header-section-number">9.8.2</span> Zero-Shot Transfer Without Task-Specific Data</a></li>
  </ul></li>
  <li><a href="#when-transfer-fails" id="toc-when-transfer-fails" class="nav-link" data-scroll-target="#when-transfer-fails"><span class="header-section-number">9.9</span> When Transfer Fails</a>
  <ul class="collapse">
  <li><a href="#diagnosing-negative-transfer" id="toc-diagnosing-negative-transfer" class="nav-link" data-scroll-target="#diagnosing-negative-transfer"><span class="header-section-number">9.9.1</span> Diagnosing Negative Transfer</a></li>
  <li><a href="#remediation-when-transfer-fails" id="toc-remediation-when-transfer-fails" class="nav-link" data-scroll-target="#remediation-when-transfer-fails"><span class="header-section-number">9.9.2</span> Remediation When Transfer Fails</a></li>
  </ul></li>
  <li><a href="#case-studies-in-transfer-learning" id="toc-case-studies-in-transfer-learning" class="nav-link" data-scroll-target="#case-studies-in-transfer-learning"><span class="header-section-number">9.10</span> Case Studies in Transfer Learning</a>
  <ul class="collapse">
  <li><a href="#dnabert-for-chromatin-accessibility" id="toc-dnabert-for-chromatin-accessibility" class="nav-link" data-scroll-target="#dnabert-for-chromatin-accessibility"><span class="header-section-number">9.10.1</span> DNABERT for Chromatin Accessibility</a></li>
  <li><a href="#esm-for-variant-pathogenicity" id="toc-esm-for-variant-pathogenicity" class="nav-link" data-scroll-target="#esm-for-variant-pathogenicity"><span class="header-section-number">9.10.2</span> ESM for Variant Pathogenicity</a></li>
  <li><a href="#enformer-for-cross-tissue-expression" id="toc-enformer-for-cross-tissue-expression" class="nav-link" data-scroll-target="#enformer-for-cross-tissue-expression"><span class="header-section-number">9.10.3</span> Enformer for Cross-Tissue Expression</a></li>
  <li><a href="#cross-species-regulatory-prediction" id="toc-cross-species-regulatory-prediction" class="nav-link" data-scroll-target="#cross-species-regulatory-prediction"><span class="header-section-number">9.10.4</span> Cross-Species Regulatory Prediction</a></li>
  </ul></li>
  <li><a href="#validation-and-common-pitfalls" id="toc-validation-and-common-pitfalls" class="nav-link" data-scroll-target="#validation-and-common-pitfalls"><span class="header-section-number">9.11</span> Validation and Common Pitfalls</a>
  <ul class="collapse">
  <li><a href="#sources-of-spurious-success" id="toc-sources-of-spurious-success" class="nav-link" data-scroll-target="#sources-of-spurious-success"><span class="header-section-number">9.11.1</span> Sources of Spurious Success</a></li>
  <li><a href="#evaluation-practices-that-reveal-true-performance" id="toc-evaluation-practices-that-reveal-true-performance" class="nav-link" data-scroll-target="#evaluation-practices-that-reveal-true-performance"><span class="header-section-number">9.11.2</span> Evaluation Practices That Reveal True Performance</a></li>
  </ul></li>
  <li><a href="#emerging-directions" id="toc-emerging-directions" class="nav-link" data-scroll-target="#emerging-directions"><span class="header-section-number">9.12</span> Emerging Directions</a>
  <ul class="collapse">
  <li><a href="#in-context-learning" id="toc-in-context-learning" class="nav-link" data-scroll-target="#in-context-learning"><span class="header-section-number">9.12.1</span> In-Context Learning</a></li>
  <li><a href="#test-time-adaptation" id="toc-test-time-adaptation" class="nav-link" data-scroll-target="#test-time-adaptation"><span class="header-section-number">9.12.2</span> Test-Time Adaptation</a></li>
  <li><a href="#federated-learning-across-institutions" id="toc-federated-learning-across-institutions" class="nav-link" data-scroll-target="#federated-learning-across-institutions"><span class="header-section-number">9.12.3</span> Federated Learning Across Institutions</a></li>
  <li><a href="#toward-theoretical-foundations" id="toc-toward-theoretical-foundations" class="nav-link" data-scroll-target="#toward-theoretical-foundations"><span class="header-section-number">9.12.4</span> Toward Theoretical Foundations</a></li>
  </ul></li>
  <li><a href="#amplification-and-its-risks" id="toc-amplification-and-its-risks" class="nav-link" data-scroll-target="#amplification-and-its-risks"><span class="header-section-number">9.13</span> Amplification and Its Risks</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch09-transfer.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer and Adaptation</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-transfer" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer and Adaptation</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Transfer learning fails as often as it succeeds, and the failures are silent. A protein language model trained on human sequences may confidently score variants in mouse orthologs, producing predictions that look reasonable but reflect human-specific evolutionary pressures irrelevant to mouse biology. A foundation model pretrained on coding sequences may extract features actively misleading for noncoding regulatory elements. A classifier achieving 90% accuracy on common variants may collapse to chance performance on the rare variants that matter most clinically. Nothing in the model’s outputs signals these failures. The predictions look the same whether transfer has succeeded or catastrophically failed. This asymmetry between confident outputs and actual reliability creates the central methodological challenge of applying pretrained models: detecting when transfer works and when it does not, before the predictions reach clinical applications where failures have consequences.</p>
<p>The promise of transfer learning is substantial. Foundation models trained on billions of evolutionary sequences learn representations that capture protein structure, functional constraints, and sequence grammar without task-specific supervision. When these representations are applied to downstream tasks with limited labeled data, they can achieve performance that would be impossible for models trained from scratch. A variant effect predictor fine-tuned from ESM-2 can classify novel missense mutations using patterns learned from the entire protein universe, not just the handful of variants with clinical annotations. This capacity to generalize from abundant unlabeled data to rare clinical scenarios has driven much of the enthusiasm for genomic foundation models.</p>
<p>The reality requires careful navigation. Every adaptation decision involves tradeoffs: preserving pretrained knowledge versus enabling task-specific learning, computational efficiency versus model flexibility, rapid deployment versus careful validation. Full fine-tuning updates all parameters, risking catastrophic forgetting of pretrained knowledge. Feature extraction freezes all pretrained parameters, limiting adaptation to task-specific patterns. Parameter-efficient methods (adapters, LoRA, prompt tuning) navigate between these extremes, but each makes different assumptions about where adaptation should occur. This chapter examines when each strategy is appropriate, provides diagnostic tools for detecting transfer failures, and develops the methodological framework that separates practitioners who apply foundation models effectively from those who treat them as black boxes.</p>
<section id="source-and-target-domains" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="source-and-target-domains"><span class="header-section-number">9.1</span> Source and Target Domains</h2>
<p>When a cardiologist requests variant interpretation for a patient with hypertrophic cardiomyopathy, the clinical need (classifying a specific <em>MYH7</em> variant) differs fundamentally from the data available during model development (millions of protein sequences sampled across all of evolution). Bridging this gap requires understanding what properties of pretraining determine whether transfer will succeed. When this bridge fails, patients receive confident predictions based on patterns irrelevant to their clinical context.</p>
<div id="fig-domain-alignment" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-domain-alignment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER</strong></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-domain-alignment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: [Essential] Schematic illustrating domain shift in genomic transfer learning. Left panel (Source Domain): Diverse genomic sequences during pretraining, with learned representations capturing statistical regularities (local motifs, composition, conservation). Right panel (Target Domain): Sparse labeled examples for clinical task (e.g., pathogenic variants, tissue-specific enhancers), highlighting distributional differences. Center: Representation space showing well-transferred features (local motifs, conservation patterns) connected by solid arrows vs.&nbsp;poorly-transferred features (long-range regulatory logic, tissue-specific patterns) with dashed arrows indicating transfer failure.
</figcaption>
</figure>
</div>
<section id="the-gap-between-pretraining-and-deployment" class="level3" data-number="9.1.1">
<h3 data-number="9.1.1" class="anchored" data-anchor-id="the-gap-between-pretraining-and-deployment"><span class="header-section-number">9.1.1</span> The Gap Between Pretraining and Deployment</h3>
<p>The <strong>source domain</strong> encompasses the data and objectives used during pretraining. For DNA foundation models, source domains typically include reference genomes, pan-genomic collections spanning population diversity, or metagenomic assemblies sampling environmental sequence space <span class="citation" data-cites="ji_dnabert_2021 dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>; <a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. For protein models, databases like UniRef provide billions of sequences representing the diversity of evolutionary history <span class="citation" data-cites="suzek_uniref_2015">(<a href="../bib/references.html#ref-suzek_uniref_2015" role="doc-biblioref">Suzek et al. 2007</a>)</span>. Pretraining objectives (masked language modeling, next-token prediction, contrastive learning) encourage models to capture statistical regularities that help predict held-out tokens: local motifs, compositional patterns, and the signatures distinguishing functional from random sequence. These learned regularities become the representations that might transfer to downstream tasks.</p>
<p>The <strong>target domain</strong> presents a fundamentally different challenge. Rather than abundant unlabeled sequence, the target domain offers sparse labeled examples of a specific clinical or biological question: a few thousand enhancer sequences with luciferase measurements, several hundred variants with expert pathogenicity classifications, chromatin profiles across a handful of disease-relevant cell types. The target distribution often looks nothing like pretraining data. Pathogenic variants are rare outliers, not typical protein sequences. Tissue-specific enhancers exhibit patterns that genome-wide pretraining may never emphasize. Disease-associated regulatory elements may have been systematically underrepresented in reference data <span class="citation" data-cites="kircher_general_2014">(<a href="../bib/references.html#ref-kircher_general_2014" role="doc-biblioref">Kircher et al. 2014</a>)</span>.</p>
<p>Four factors determine whether this distributional gap can be bridged. Task relatedness measures whether target predictions depend on patterns the model learned during pretraining; predicting transcription factor binding after sequence pretraining succeeds because both involve local motif recognition, while predicting three-dimensional chromatin contacts may require spatial relationships the pretraining objective never captured. Target data quantity constrains which adaptation strategies avoid overfitting; with thousands of labeled examples, aggressive fine-tuning can reshape representations, but with dozens, only the lightest approaches remain viable. Model expressiveness influences adaptation flexibility, as larger models encode richer internal representations that can potentially serve more diverse downstream tasks but also risk memorizing small target datasets. Distribution overlap between source and target determines how much learned knowledge applies; human regulatory elements share patterns with mouse elements (enabling cross-species transfer) but diverge in species-specific enhancers (limiting it).</p>
</section>
<section id="recognizing-transfer-outcomes" class="level3" data-number="9.1.2">
<h3 data-number="9.1.2" class="anchored" data-anchor-id="recognizing-transfer-outcomes"><span class="header-section-number">9.1.2</span> Recognizing Transfer Outcomes</h3>
<p>Not all transfer helps, and distinguishing outcomes requires explicit validation. <strong>Positive transfer</strong> accelerates learning or improves final performance beyond training from scratch. <strong>Negative transfer</strong> occurs when pretraining actively hurts, either because learned features conflict with task requirements or because pretrained initialization creates optimization difficulties <span class="citation" data-cites="wang_characterizing_2019">(<a href="../bib/references.html#ref-wang_characterizing_2019" role="doc-biblioref"><span>“Placeholder: Wang Characterizing 2019”</span> 2019</a>)</span>. Neutral transfer describes situations where pretraining neither helps nor hurts, wasting computational resources on pretrained models without benefit. When a pharmacogenomics team adapts a DNA language model for <em>CYP2D6</em> metabolizer status prediction, they must empirically verify which outcome applies to their specific task rather than assuming transfer will help because it helped elsewhere.</p>
</section>
</section>
<section id="feature-extraction-with-frozen-backbones" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="feature-extraction-with-frozen-backbones"><span class="header-section-number">9.2</span> Feature Extraction with Frozen Backbones</h2>
<p>Clinical laboratories processing hundreds of variants daily cannot afford to fine-tune models for each new gene or variant class. When a novel gene enters diagnostic panels, classifiers must be deployed rapidly using whatever labeled examples exist. A molecular diagnostics team with 200 annotated <em>RYR1</em> variants for malignant hyperthermia risk prediction cannot fine-tune a 500-million parameter model; they need an approach that works with minimal data while avoiding adaptation risk entirely.</p>
<p><strong>Frozen feature extraction</strong> addresses this constraint by treating pretrained models as fixed representation engines. All backbone parameters remain frozen; only a lightweight classifier trained on the extracted representations learns from labeled data. The backbone never changes, eliminating catastrophic forgetting entirely and enabling deployment within hours rather than weeks. The fundamental tradeoff is clear: frozen features sacrifice adaptation flexibility for speed, safety, and efficiency.</p>
<section id="linear-probing" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="linear-probing"><span class="header-section-number">9.2.1</span> Linear Probing</h3>
<p>Why does the simplest possible classifier often suffice? If pretrained representations already encode task-relevant features in linearly separable form, adding complexity provides no benefit and risks overfitting. <strong>Linear probing</strong> tests this hypothesis by introducing only <span class="math inline">\(d \times c\)</span> parameters (where <span class="math inline">\(d\)</span> is the embedding dimension and <span class="math inline">\(c\)</span> is the number of output classes). Pass input sequences through the frozen model to obtain embeddings, typically from the final layer or from a designated [CLS] token aggregating sequence information, then train a linear classifier mapping embeddings to task labels.</p>
<p>Ji et al.&nbsp;demonstrated that <em>DNABERT</em> embeddings paired with linear probes achieve competitive chromatin accessibility prediction from a few hundred positive and negative examples, matching CNN baselines requiring far more labeled data <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. Dalla-Torre et al.&nbsp;showed similar results with <em>Nucleotide Transformer</em>, where linear probes on frozen embeddings approached fine-tuned performance for promoter detection and splice site recognition <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. These successes reflect alignment between pretraining objectives (predicting masked tokens from local context) and target tasks (distinguishing sequences based on motif patterns the model already learned to recognize).</p>
</section>
<section id="when-linear-probing-fails" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="when-linear-probing-fails"><span class="header-section-number">9.2.2</span> When Linear Probing Fails</h3>
<p>Linear probes fail when relevant information exists in embeddings but requires nonlinear transformation to extract. Shallow multilayer perceptrons (one or two hidden layers) extend linear probing by enabling more complex decision boundaries while maintaining computational efficiency. With several thousand labeled examples, shallow MLPs on <em>HyenaDNA</em> embeddings improve splice site prediction over linear probes by capturing interactions between features that linear models cannot represent <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. The additional expressiveness helps when task-relevant patterns are distributed across embedding dimensions in ways that linear combination cannot capture.</p>
<p>The more fundamental limitation cannot be addressed by classifier complexity: performance caps at how well pretrained representations already encode task-relevant features. If the pretraining objective emphasized patterns irrelevant to the downstream task, or if required features were actively suppressed during pretraining, frozen features will underperform models trained from scratch regardless of classifier sophistication. A model pretrained exclusively on coding sequence may encode features misleading for noncoding regulatory prediction; no linear probe can overcome representations that point in the wrong direction.</p>
</section>
</section>
<section id="parameter-efficient-fine-tuning" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="parameter-efficient-fine-tuning"><span class="header-section-number">9.3</span> Parameter-Efficient Fine-Tuning</h2>
<p>A research hospital developing tissue-specific expression predictors faces an impossible choice. Frozen features from <em>Enformer</em> provide reasonable baselines, but full fine-tuning for each of fifty tissue types would require months of GPU time and risk overfitting the thousands of tissue-specific training examples. The team needs an intermediate approach: enough flexibility to improve over frozen features, enough constraint to prevent overfitting, enough efficiency to iterate across dozens of tissues.</p>
<p><strong>Parameter-efficient fine-tuning (PEFT)</strong> methods resolve this tension by updating a small subset of parameters while keeping the majority frozen, enabling task-specific adaptation without the computational expense or overfitting risk of modifying all weights <span class="citation" data-cites="houlsby_parameter-efficient_2019">(<a href="../bib/references.html#ref-houlsby_parameter-efficient_2019" role="doc-biblioref"><span>“Placeholder: Houlsby Parameter-Efficient 2019”</span> 2019</a>)</span>. The key insight is that useful adaptation often requires changing only a small subspace of model behavior, not rewriting everything the model learned during pretraining.</p>
<section id="low-rank-adaptation" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="low-rank-adaptation"><span class="header-section-number">9.3.1</span> Low-Rank Adaptation</h3>
<p><strong>Low-Rank Adaptation (LoRA)</strong> has emerged as the dominant PEFT technique in genomic applications because it directly operationalizes this insight. Rather than updating a large weight matrix <span class="math inline">\(W\)</span> directly, LoRA introduces two smaller matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> whose product approximates the desired weight change: <span class="math inline">\(W' = W + BA\)</span> <span class="citation" data-cites="hu_lora_2021">(<a href="../bib/references.html#ref-hu_lora_2021" role="doc-biblioref"><span>“Hu_lora_2021,”</span> n.d.</a>)</span>. During fine-tuning, <span class="math inline">\(W\)</span> remains frozen while only <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> receive gradient updates. The rank of these matrices (typically 8 to 64 for genomic models) controls adaptation expressiveness: lower ranks introduce fewer parameters and stronger implicit regularization; higher ranks enable more flexible task-specific modification at greater overfitting risk.</p>
<p>The efficiency gains prove substantial. A transformer with 500 million parameters might require updating only 2 to 5 million LoRA parameters (representing the low-rank decompositions applied to attention weight matrices), reducing memory requirements by an order of magnitude compared with full fine-tuning. This efficiency enables training on consumer GPUs for models that would otherwise require specialized infrastructure, and enables systematic hyperparameter search that would be prohibitive with full parameter updates. Zhou et al.&nbsp;demonstrated that LoRA adapters on <em>Nucleotide Transformer</em> enable tissue-specific chromatin accessibility prediction, where separate low-rank matrices capture tissue-specific regulatory patterns while the pretrained backbone encodes general sequence understanding <span class="citation" data-cites="zhou_dnabert-2_2023">(<a href="../bib/references.html#ref-zhou_dnabert-2_2023" role="doc-biblioref">Zhou et al. 2024</a>)</span>.</p>
<div id="fig-lora-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lora-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER</strong></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lora-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: [High] Diagram showing LoRA modification to a transformer weight matrix. Show: Original frozen weight matrix W (large, e.g., 768×768). LoRA decomposition: small matrices A (768×r) and B (r×768) where r &lt;&lt; 768 (e.g., r=16). Combined transformation: W + BA. Annotate parameter counts: “500M frozen + 2M trainable.” Show how this applies to attention weight matrices (W^Q, W^K, W^V, W^O).
</figcaption>
</figure>
</div>
</section>
<section id="alternative-peft-approaches" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="alternative-peft-approaches"><span class="header-section-number">9.3.2</span> Alternative PEFT Approaches</h3>
<p><strong>Adapter layers</strong> take a different architectural approach by inserting small bottleneck modules between transformer layers rather than modifying existing weights. Each adapter consists of a down-projection reducing dimensionality (typically by a factor of 4 to 16), a nonlinear activation, and an up-projection restoring the original dimension. Original transformer parameters remain frozen; only adapter parameters update. Different adapters can be trained for different tasks, enabling multi-task deployment from a single shared backbone without interference between tasks <span class="citation" data-cites="houlsby_parameter-efficient_2019">(<a href="../bib/references.html#ref-houlsby_parameter-efficient_2019" role="doc-biblioref"><span>“Placeholder: Houlsby Parameter-Efficient 2019”</span> 2019</a>)</span>.</p>
<p><strong>Prefix tuning</strong> prepends learnable embeddings to the input sequence, providing task-specific context that conditions model behavior without modifying internal parameters <span class="citation" data-cites="li_prefix-tuning_2021">(<a href="../bib/references.html#ref-li_prefix-tuning_2021" role="doc-biblioref"><span>“Placeholder: Li Prefix-Tuning 2021”</span> 2021</a>)</span>. While less common in genomics (where natural prompt structure is absent), prefix tuning has found limited application in models accepting both sequence and task description inputs. Other variants include <strong>BitFit</strong> (tuning only bias terms, introducing minimal parameters) and soft prompt tuning <span class="citation" data-cites="zaken_bitfit_2022">(<a href="../bib/references.html#ref-zaken_bitfit_2022" role="doc-biblioref"><span>“Placeholder: Zaken Bitfit 2022”</span> 2022</a>)</span>.</p>
<p>PEFT methods suit intermediate data regimes: 1,000 to 10,000 labeled examples, where frozen features underperform but full fine-tuning risks overfitting. The implicit regularization from constrained parameter spaces often yields better final models than unconstrained optimization on limited data.</p>
</section>
</section>
<section id="full-fine-tuning" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="full-fine-tuning"><span class="header-section-number">9.4</span> Full Fine-Tuning</h2>
<p>When Avsec et al.&nbsp;sought to predict gene expression from sequence across hundreds of cell types, they required a model capturing tissue-specific regulatory logic unavailable from any generic pretrained representation <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. With millions of labeled examples spanning thousands of genomic tracks, they could afford to update all model parameters, reshaping internal representations entirely for their specific predictive task. Constrained adaptation would have left tissue-specific regulatory patterns unlearned.</p>
<p><strong>Full fine-tuning</strong> offers maximum flexibility: every parameter becomes tunable, enabling the model to learn whatever features the target task requires regardless of pretraining emphasis. This flexibility comes with risks proportional to its power.</p>
<section id="making-full-fine-tuning-work" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="making-full-fine-tuning-work"><span class="header-section-number">9.4.1</span> Making Full Fine-Tuning Work</h3>
<p>Full fine-tuning updates all model parameters during adaptation but requires careful attention to optimization dynamics. Learning rates must be substantially lower than during pretraining (often 10 to 100 times smaller) to avoid catastrophically disrupting learned representations in early training steps <span class="citation" data-cites="howard_universal_2018">(<a href="../bib/references.html#ref-howard_universal_2018" role="doc-biblioref"><span>“Placeholder: Howard Universal 2018”</span> 2018</a>)</span>. <strong>Gradual unfreezing</strong>, where top layers update first and deeper layers progressively join training, helps preserve low-level features (local motifs, basic sequence statistics) while allowing high-level task-specific adjustment. Regularization through weight decay, dropout, and early stopping on validation data prevents overfitting to target datasets.</p>
<p>The approach suits scenarios when labeled datasets are large (tens of thousands of examples or more), when the target task diverges substantially from pretraining such that constrained adaptation proves insufficient, or when performance requirements justify computational investment. <em>Enformer</em> fine-tuning for new chromatin assays requires updating most parameters to capture assay-specific signal characteristics distinct from original training conditions. Expression prediction across novel cell types benefits from full adaptation when sufficient tissue-specific data exists.</p>
</section>
<section id="the-risks-of-unconstrained-adaptation" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="the-risks-of-unconstrained-adaptation"><span class="header-section-number">9.4.2</span> The Risks of Unconstrained Adaptation</h3>
<p><strong>Catastrophic forgetting</strong> occurs when fine-tuning overwrites general knowledge useful for related tasks or out-of-distribution inputs; a model fine-tuned aggressively on lymphocyte data may lose performance on epithelial cells it previously handled well <span class="citation" data-cites="mccloskey_catastrophic_1989">(<a href="../bib/references.html#ref-mccloskey_catastrophic_1989" role="doc-biblioref"><span>“Placeholder: Mccloskey Catastrophic 1989”</span> 1989</a>)</span>. Overfitting afflicts small target datasets, where the model memorizes training examples rather than learning generalizable patterns. Computational expense can be prohibitive for models with billions of parameters. When negative transfer occurs (pretraining initialization actually hurts optimization), full fine-tuning may underperform training from scratch despite the additional expense.</p>
<p>The conservative strategy is to start with simpler methods and escalate only when they demonstrably fail. Establish frozen feature baselines first. If frozen features outperform random initialization, try PEFT methods before committing to full fine-tuning. Compare fine-tuned models against properly-tuned from-scratch baselines on the same target data. Monitor for overfitting through validation curves and early stopping. The goal is achieving required performance with minimal adaptation complexity.</p>
</section>
</section>
<section id="probing-representations" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="probing-representations"><span class="header-section-number">9.5</span> Probing Representations</h2>
<p>A variant effect predictor built on <em>ESM</em> embeddings achieves 85% accuracy in initial testing, but the team deploying it needs to understand why. Does the model genuinely capture evolutionary constraint relevant to pathogenicity, or has it learned spurious correlations that will fail on out-of-distribution variants? Before committing computational resources to adaptation, practitioners benefit from understanding what the pretrained model actually learned.</p>
<p><strong>Probing classifiers</strong> answer these diagnostic questions by systematically interrogating representations before deployment. The methodology converts the abstract question “will transfer help?” into concrete evidence about representation content: train lightweight classifiers to predict properties of interest from frozen embeddings, then examine how accurately different properties can be decoded. If chromatin accessibility can be predicted with 85% accuracy from a linear probe, the representations already encode accessibility-relevant features and frozen feature extraction will likely succeed. If transcription factor binding requires a deep nonlinear classifier to reach the same accuracy, relevant information exists but is not linearly separable, suggesting PEFT might help by reorganizing representations for easier extraction. If a property cannot be predicted above chance even with flexible classifiers, the representations may lack necessary information entirely, and transfer to this task may fail regardless of adaptation strategy.</p>
<section id="what-probing-reveals-about-pretrained-models" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="what-probing-reveals-about-pretrained-models"><span class="header-section-number">9.5.1</span> What Probing Reveals About Pretrained Models</h3>
<p>Systematic probing reveals what models learn during pretraining. Rives et al.&nbsp;demonstrated that <em>ESM</em> protein embeddings encode secondary structure so thoroughly that linear probes achieve near state-of-the-art helix/sheet/coil prediction accuracy <span class="citation" data-cites="rives_biological_2021">(<a href="../bib/references.html#ref-rives_biological_2021" role="doc-biblioref"><span>“Placeholder: Rives Biological 2021”</span> 2021</a>)</span>. Contact prediction (which residues are spatially close in folded structure) requires nonlinear probes but still achieves strong performance, indicating that tertiary structure information is present but requires transformation to extract. DNA language models show similar patterns: local motif information is recoverable by linear probes while long-range dependencies require multi-layer networks <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>.</p>
<p><strong>Layer-wise probing</strong> reveals how information transforms through the model. Early layers typically encode local compositional features (k-mer frequencies, simple motifs, sequence statistics) while later layers capture more abstract patterns (regulatory signatures, evolutionary constraints, functional classifications) <span class="citation" data-cites="jawahar_what_2019">(<a href="../bib/references.html#ref-jawahar_what_2019" role="doc-biblioref"><span>“Placeholder: Jawahar What 2019”</span> 2019</a>)</span>. For tasks depending on local features, representations from early or middle layers may outperform final-layer embeddings that have abstracted away relevant details. Layer selection becomes another hyperparameter to optimize during adaptation.</p>
</section>
<section id="probing-guides-adaptation-strategy" class="level3" data-number="9.5.2">
<h3 data-number="9.5.2" class="anchored" data-anchor-id="probing-guides-adaptation-strategy"><span class="header-section-number">9.5.2</span> Probing Guides Adaptation Strategy</h3>
<p>The diagnostic value extends beyond predicting which adaptation strategy to use. When probing reveals that required features are absent from pretrained representations, practitioners face a choice: commit to full fine-tuning with sufficient target data (hoping the model can learn missing features), switch to a different foundation model whose pretraining objective better aligns with task requirements, or proceed with from-scratch training that does not inherit inappropriate inductive biases. The investment in probing before adaptation often saves months of wasted effort on transfer that was doomed from the start.</p>
</section>
</section>
<section id="choosing-an-adaptation-strategy" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="choosing-an-adaptation-strategy"><span class="header-section-number">9.6</span> Choosing an Adaptation Strategy</h2>
<p>A clinical genomics laboratory evaluating foundation models for deployment must translate abstract transfer learning principles into concrete decisions. The molecular diagnostics team has 2,000 validated enhancer sequences: should they use frozen features, LoRA, or full fine-tuning? Which approach maximizes performance given available data? When should transfer be abandoned entirely in favor of task-specific training? Making the wrong choice means either leaving performance on the table or wasting months on approaches that will fail.</p>
<div id="fig-adaptation-decision-tree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adaptation-decision-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER</strong></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adaptation-decision-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: [Essential] Flowchart guiding adaptation strategy selection. Decision nodes: (1) “Labeled data quantity?” with branches &lt;500, 500-5000, &gt;10000. (2) “Task similarity to pretraining?” with branches high/moderate/low. (3) “Computational constraints?” with branches limited/moderate/substantial. Terminal nodes recommend: Linear probing, LoRA/Adapters, Full fine-tuning, or From-scratch training. Include expected tradeoffs at each terminal (accuracy, compute, overfitting risk).
</figcaption>
</figure>
</div>
<section id="data-quantity-determines-what-is-possible" class="level3" data-number="9.6.1">
<h3 data-number="9.6.1" class="anchored" data-anchor-id="data-quantity-determines-what-is-possible"><span class="header-section-number">9.6.1</span> Data Quantity Determines What Is Possible</h3>
<p>Data quantity provides the primary decision axis, though the thresholds depend on model size and task complexity.</p>
<p>With fewer than 500 labeled examples, linear probing typically represents the only viable approach. More complex adaptation overfits catastrophically, and the limited signal cannot support meaningful parameter updates. The molecular diagnostics team with 200 annotated <em>RYR1</em> variants for malignant hyperthermia risk prediction cannot fine-tune a 500-million parameter model; they must rely on whether pretrained representations already capture relevant evolutionary constraint.</p>
<p>With 500 to 5,000 labeled examples, PEFT methods offer favorable tradeoffs. LoRA adapters introduce enough flexibility to improve over frozen features while maintaining implicit regularization that prevents overfitting. Systematic hyperparameter search (rank selection, learning rate, regularization strength) becomes feasible at this scale. A research group with 2,000 validated enhancer sequences can meaningfully compare linear probes, LoRA with various ranks, and perhaps cautious full fine-tuning with aggressive regularization.</p>
<p>With more than 10,000 labeled examples, full fine-tuning becomes viable and may be necessary when target tasks diverge substantially from pretraining. The computational investment is justified when the adapted model will be deployed at scale or when the task is central to a research program. A consortium with 50,000 annotated regulatory elements across cell types has sufficient data to reshape pretrained representations entirely toward their specific predictive objectives.</p>
</section>
<section id="task-similarity-determines-what-is-necessary" class="level3" data-number="9.6.2">
<h3 data-number="9.6.2" class="anchored" data-anchor-id="task-similarity-determines-what-is-necessary"><span class="header-section-number">9.6.2</span> Task Similarity Determines What Is Necessary</h3>
<p>Task similarity provides the second decision axis. When target tasks closely resemble pretraining patterns (predicting transcription factor binding after sequence pretraining), frozen features often suffice because relevant patterns were already learned. When tasks diverge moderately (tissue-specific expression after genome-wide pretraining), PEFT enables selective adaptation. When tasks fundamentally differ from pretraining (three-dimensional chromatin contact prediction from sequence-only pretraining), full fine-tuning may be necessary to learn features the pretraining objective never emphasized.</p>
<p>Computational constraints impose practical limits. Linear probing requires minutes on CPUs. LoRA fine-tuning requires hours on single GPUs. Full fine-tuning of large models requires days on multiple GPUs or specialized hardware. When infrastructure is limited, simpler approaches may be the only feasible option regardless of what would theoretically be optimal.</p>
<p>Empirical validation supersedes heuristics. No rule reliably predicts which strategy will succeed for any specific combination of model, task, and data. The guidelines above indicate which strategies merit trying first, but final selection requires comparing approaches on held-out validation data matching the intended deployment distribution.</p>
</section>
</section>
<section id="domain-shift-and-cross-context-transfer" class="level2" data-number="9.7">
<h2 data-number="9.7" class="anchored" data-anchor-id="domain-shift-and-cross-context-transfer"><span class="header-section-number">9.7</span> Domain Shift and Cross-Context Transfer</h2>
<p>The <em>CYP2D6</em> gene encodes a cytochrome P450 enzyme metabolizing approximately 25% of clinically used drugs, including codeine (where poor metabolizers experience no analgesic effect) and tamoxifen (where poor metabolizers show reduced breast cancer treatment efficacy) <span class="citation" data-cites="gaedigk_pharmacogene_2018">(<a href="../bib/references.html#ref-gaedigk_pharmacogene_2018" role="doc-biblioref"><span>“Placeholder: Gaedigk Pharmacogene 2018”</span> 2018</a>)</span>. A foundation model trained on human genomic data and adapted for <em>CYP2D6</em> variant classification might achieve 90% accuracy on common variants well-represented in training data. But the variants most important clinically are rare: novel star alleles in underrepresented populations, structural variants creating gene duplications or deletions, population-specific haplotypes absent from reference databases. <strong>Domain shift between training and deployment distributions creates systematic blind spots precisely where clinical stakes are highest.</strong></p>
<section id="types-of-domain-shift-in-genomics" class="level3" data-number="9.7.1">
<h3 data-number="9.7.1" class="anchored" data-anchor-id="types-of-domain-shift-in-genomics"><span class="header-section-number">9.7.1</span> Types of Domain Shift in Genomics</h3>
<p>Three types of <strong>domain shift</strong> commonly afflict genomic transfer learning, each creating different patterns of failure.</p>
<p><strong>Cross-species transfer</strong> applies models trained on one organism to another, facing evolutionary divergence that introduces sequence differences affecting regulatory syntax, motif grammar, and functional constraints. Strategies for cross-species success include pretraining on multi-species data to learn conservation patterns, fine-tuning with species-specific adapters, and focusing on highly conserved features (core promoter elements, splice site consensus sequences) that transfer more readily than species-specific innovations <span class="citation" data-cites="kelley_cross-species_2020">(<a href="../bib/references.html#ref-kelley_cross-species_2020" role="doc-biblioref">Kelley 2020</a>)</span>. Human-to-mouse regulatory prediction works reasonably for conserved housekeeping genes but fails for rodent-specific enhancers that never existed in the human training distribution.</p>
<p><strong>Cross-tissue transfer</strong> confronts tissue-specific regulatory programs. Chromatin accessibility varies dramatically across tissues, with thousands of tissue-specific enhancers and repressors controlling cell-type identity. Models trained predominantly on one tissue may miss regulatory logic specific to others. Effective approaches include shared backbones with tissue-specific prediction heads (each head learns tissue-specific transformations of shared representations), tissue-conditional models accepting tissue identity as additional input, and meta-learning frameworks training across many tissues to extract general principles applicable to novel tissue types <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>.</p>
<p><strong>Cross-assay transfer</strong> handles different molecular readouts of related biology. ChIP-seq and ATAC-seq both measure chromatin state but with different biochemistry, resolution, and signal characteristics. Models trained on one assay may learn assay-specific artifacts rather than underlying biology, producing predictions that fail when applied to related assays measuring the same phenomenon differently. Multi-task pretraining across assays helps models distinguish biological signal from assay-specific noise.</p>
</section>
<section id="detecting-and-mitigating-shift" class="level3" data-number="9.7.2">
<h3 data-number="9.7.2" class="anchored" data-anchor-id="detecting-and-mitigating-shift"><span class="header-section-number">9.7.2</span> Detecting and Mitigating Shift</h3>
<p>Detecting domain shift before deployment prevents silent clinical failures. Statistical divergence measures comparing source and target distributions quantify distribution differences. Embedding visualizations (t-SNE or UMAP projections) reveal whether target examples fall within the source distribution or occupy unfamiliar regions of representation space. Monitoring performance on canary examples (known easy cases that should always be predicted correctly) provides early warning of severe shift during deployment.</p>
<p>When domain shift is detected, mitigation strategies include domain-adaptive fine-tuning, importance weighting of training examples, and explicit modeling of shift through domain-adversarial training <span class="citation" data-cites="ganin_domain-adversarial_2016">(<a href="../bib/references.html#ref-ganin_domain-adversarial_2016" role="doc-biblioref"><span>“Placeholder: Ganin Domain-Adversarial 2016”</span> 2016</a>)</span>. When shift is severe and cannot be mitigated, acknowledging that transfer is inappropriate for this context prevents overconfident deployment of models that will fail.</p>
<div id="fig-domain-shift-detection" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-domain-shift-detection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20A.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER A</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20B.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER B</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20C.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER C</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-domain-shift-detection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: [High] Three-panel visualization of domain shift detection. Panel A: UMAP/t-SNE projection of embeddings showing training distribution (dense cluster) and test examples at varying distances, with out-of-distribution examples clearly separated. Panel B: Calibration curves comparing confidence vs.&nbsp;accuracy for in-distribution (well-calibrated diagonal) vs.&nbsp;out-of-distribution examples (overconfident, curve below diagonal). Panel C: Performance degradation curve showing accuracy declining as distributional distance from training data increases.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="few-shot-and-zero-shot-learning" class="level2" data-number="9.8">
<h2 data-number="9.8" class="anchored" data-anchor-id="few-shot-and-zero-shot-learning"><span class="header-section-number">9.8</span> Few-Shot and Zero-Shot Learning</h2>
<p>A geneticist studying a newly characterized neurodevelopmental disorder has identified 15 patients with variants in a previously unstudied gene. Functional studies confirm pathogenicity for 8 variants; the remaining 7 are benign. Training a classifier from 15 examples using standard supervised learning would be absurd, yet the clinical need for variant interpretation is immediate. Parents are waiting for diagnoses. <strong>Few-shot and zero-shot learning</strong> address these extreme data scarcity scenarios that characterize many genomic applications, where clinical urgency outpaces data availability.</p>
<section id="few-shot-learning-with-minimal-examples" class="level3" data-number="9.8.1">
<h3 data-number="9.8.1" class="anchored" data-anchor-id="few-shot-learning-with-minimal-examples"><span class="header-section-number">9.8.1</span> Few-Shot Learning with Minimal Examples</h3>
<p><strong>Few-shot learning</strong> operates with 10 to 100 examples per class, a regime where standard adaptation overfits catastrophically. <strong>Meta-learning</strong> approaches train models explicitly for rapid adaptation by optimizing across many few-shot tasks during meta-training <span class="citation" data-cites="finn_model-agnostic_2017">(<a href="../bib/references.html#ref-finn_model-agnostic_2017" role="doc-biblioref"><span>“Placeholder: Finn Model-Agnostic 2017”</span> 2017</a>)</span>. <strong>Model-Agnostic Meta-Learning (MAML)</strong> finds parameter initializations that can be fine-tuned effectively from minimal data; the initialization represents a point in parameter space from which a few gradient steps reach good task-specific solutions. At deployment, the meta-trained model adapts to new tasks from a handful of labeled examples, having learned during meta-training what features are generally useful and how to adapt quickly.</p>
<p><strong>Prototypical networks</strong> offer a simpler approach well-suited to genomic classification <span class="citation" data-cites="snell_prototypical_2017">(<a href="../bib/references.html#ref-snell_prototypical_2017" role="doc-biblioref"><span>“Placeholder: Snell Prototypical 2017”</span> 2017</a>)</span>. The model learns to embed sequences such that examples from the same class cluster together. At inference, class prototypes are computed as the mean embedding of the few available examples per class, and novel sequences are classified based on distance to prototypes. With 10 pathogenic and 10 benign variants as prototypes, novel variants are classified by which prototype cluster they fall nearest in embedding space. The approach requires no gradient updates at deployment, only forward passes to compute embeddings and distances.</p>
</section>
<section id="zero-shot-transfer-without-task-specific-data" class="level3" data-number="9.8.2">
<h3 data-number="9.8.2" class="anchored" data-anchor-id="zero-shot-transfer-without-task-specific-data"><span class="header-section-number">9.8.2</span> Zero-Shot Transfer Without Task-Specific Data</h3>
<p><strong>Zero-shot transfer</strong> eliminates task-specific adaptation entirely, making predictions using only the pretrained model’s outputs. For protein variant effect prediction, <em>ESM</em> log-likelihood ratios score variants by how much they reduce the model’s probability of the observed sequence <span class="citation" data-cites="meier_language_2021">(<a href="../bib/references.html#ref-meier_language_2021" role="doc-biblioref"><span>“Placeholder: Meier Language 2021”</span> 2021</a>)</span>. Variants that violate the model’s learned expectations for natural proteins (disrupting conserved residues, introducing destabilizing substitutions) receive low likelihood ratios, flagging them as potentially deleterious. This approach proves competitive with supervised methods for <code>ClinVar</code> pathogenicity prediction because evolutionary constraint (what masked language modeling learns to predict) correlates with functional importance (what pathogenicity classification measures).</p>
<p>Zero-shot methods require strong alignment between pretraining objectives and target tasks. When this alignment exists (evolutionary constraint predicts pathogenicity), zero-shot approaches provide immediate predictions without any labeled data. When alignment is weaker (tissue-specific regulatory activity depends on factors beyond sequence conservation), few-shot learning with even a handful of examples typically outperforms zero-shot baselines. For most practical genomic applications, some labeled data improves predictions; few-shot rather than true zero-shot represents the realistic minimal-data regime.</p>
</section>
</section>
<section id="when-transfer-fails" class="level2" data-number="9.9">
<h2 data-number="9.9" class="anchored" data-anchor-id="when-transfer-fails"><span class="header-section-number">9.9</span> When Transfer Fails</h2>
<p>The research team had done everything right. They selected a state-of-the-art DNA foundation model pretrained on diverse genomic sequences. They applied LoRA adaptation using 5,000 carefully curated training examples. Validation accuracy reached 88%. But when deployed on prospectively collected samples, performance collapsed to 62%, barely better than chance for a binary classification task. Transfer had failed. For the patients whose variants were misclassified during those weeks before the failure was detected, the consequences were real: delayed diagnoses, inappropriate treatments, unnecessary anxiety. Understanding why transfer fails prevents repeating the mistake.</p>
<section id="diagnosing-negative-transfer" class="level3" data-number="9.9.1">
<h3 data-number="9.9.1" class="anchored" data-anchor-id="diagnosing-negative-transfer"><span class="header-section-number">9.9.1</span> Diagnosing Negative Transfer</h3>
<p><strong>Negative transfer</strong> occurs when pretraining actively hurts performance, producing adapted models worse than those trained from scratch on target data alone. Pretraining on human coding sequences may encode codon usage patterns and amino acid preferences that create false expectations when applied to bacterial sequences with different GC content and codon bias. Pretraining on healthy tissue samples may learn features of normal cellular function that prove misleading for cancer samples where regulatory programs are fundamentally altered. The pretrained initialization, rather than providing a useful starting point, creates an optimization landscape that leads to poor task-specific solutions <span class="citation" data-cites="wang_characterizing_2019">(<a href="../bib/references.html#ref-wang_characterizing_2019" role="doc-biblioref"><span>“Placeholder: Wang Characterizing 2019”</span> 2019</a>)</span>.</p>
<p>Diagnostic steps identify whether transfer helps or hurts. First, compare adapted model performance against a from-scratch baseline trained on identical target data with equivalent hyperparameter tuning; if the pretrained model does not meaningfully outperform from-scratch training, transfer provides no benefit and the computational overhead of working with large pretrained models is wasted. Second, establish that simpler adaptation strategies were tried before complex ones; if linear probing fails, full fine-tuning rarely succeeds unless target data is very large. Third, visualize embeddings from the pretrained model using dimensionality reduction; if target task classes are not separated in embedding space, pretrained representations may lack task-relevant features. Fourth, ablate pretraining entirely by comparing against randomly initialized models of identical architecture; this isolates whether pretrained weights provide value or whether architectural choices alone drive performance.</p>
</section>
<section id="remediation-when-transfer-fails" class="level3" data-number="9.9.2">
<h3 data-number="9.9.2" class="anchored" data-anchor-id="remediation-when-transfer-fails"><span class="header-section-number">9.9.2</span> Remediation When Transfer Fails</h3>
<p>When diagnostics reveal fundamental mismatches, several remediation strategies apply. Task-specific pretraining on data more closely aligned with target requirements can bridge the gap; pretraining specifically on regulatory regions for regulatory prediction tasks rather than genome-wide pretraining may produce more suitable representations. Hybrid architectures combining pretrained and randomly-initialized components allow selective use of transfer where it helps while avoiding its limitations elsewhere. Trying alternative foundation models whose pretraining objectives better match task requirements may reveal that the problem was model selection rather than transfer learning generally. And accepting that transfer provides no benefit for this specific task, proceeding with from-scratch training, remains a valid conclusion when evidence supports it.</p>
</section>
</section>
<section id="case-studies-in-transfer-learning" class="level2" data-number="9.10">
<h2 data-number="9.10" class="anchored" data-anchor-id="case-studies-in-transfer-learning"><span class="header-section-number">9.10</span> Case Studies in Transfer Learning</h2>
<p>The contrast between successful and failed transfer illuminates the principles developed throughout this chapter. Four cases span different outcomes and reveal the conditions that distinguish success from failure.</p>
<section id="dnabert-for-chromatin-accessibility" class="level3" data-number="9.10.1">
<h3 data-number="9.10.1" class="anchored" data-anchor-id="dnabert-for-chromatin-accessibility"><span class="header-section-number">9.10.1</span> DNABERT for Chromatin Accessibility</h3>
<p>Ji et al.&nbsp;pretrained <em>DNABERT</em> using masked language modeling on k-mer tokenized human genomic sequence <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. For ATAC-seq peak classification, they applied linear probes to [CLS] token embeddings without updating backbone parameters. The approach achieved competitive performance with CNNs trained from scratch while requiring approximately 10 times less labeled data.</p>
<p>Success reflected strong alignment between pretraining and target task: both involve recognizing local sequence motifs (transcription factor binding sites, nucleosome positioning signals) that determine chromatin state. The pretrained representations already encoded the relevant patterns; the linear probe simply learned to separate accessible from inaccessible regions in this well-structured embedding space.</p>
</section>
<section id="esm-for-variant-pathogenicity" class="level3" data-number="9.10.2">
<h3 data-number="9.10.2" class="anchored" data-anchor-id="esm-for-variant-pathogenicity"><span class="header-section-number">9.10.2</span> ESM for Variant Pathogenicity</h3>
<p>Rives et al.&nbsp;pretrained <em>ESM</em> on UniRef protein sequences using masked language modeling <span class="citation" data-cites="rives_biological_2021">(<a href="../bib/references.html#ref-rives_biological_2021" role="doc-biblioref"><span>“Placeholder: Rives Biological 2021”</span> 2021</a>)</span>. For <code>ClinVar</code> pathogenicity classification, Meier et al.&nbsp;showed that zero-shot scoring based on variant effects on sequence likelihood proved competitive with supervised methods <span class="citation" data-cites="meier_language_2021">(<a href="../bib/references.html#ref-meier_language_2021" role="doc-biblioref"><span>“Placeholder: Meier Language 2021”</span> 2021</a>)</span>. Adding a linear probe on <em>ESM</em> embeddings improved performance further, but the zero-shot baseline was already strong.</p>
<p>Success reflected implicit alignment: evolutionary constraint (what masked language modeling captures) correlates with functional importance (what pathogenicity measures). The pretraining objective, though never explicitly targeting variant classification, learned representations directly relevant to it.</p>
</section>
<section id="enformer-for-cross-tissue-expression" class="level3" data-number="9.10.3">
<h3 data-number="9.10.3" class="anchored" data-anchor-id="enformer-for-cross-tissue-expression"><span class="header-section-number">9.10.3</span> Enformer for Cross-Tissue Expression</h3>
<p>Avsec et al.&nbsp;pretrained <em>Enformer</em> on thousands of chromatin and expression tracks spanning dozens of cell types <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. Fine-tuning with tissue-specific prediction heads for individual cell types captured regulatory logic unavailable from frozen features, outperforming both frozen <em>Enformer</em> and from-scratch models trained on individual tissues.</p>
<p>Success required both the large scale of pretraining (establishing general sequence-to-function mappings) and extensive fine-tuning data (enabling tissue-specific adaptation). With smaller fine-tuning datasets, the approach would have overfit; without diverse pretraining, the model would have lacked transferable regulatory knowledge.</p>
</section>
<section id="cross-species-regulatory-prediction" class="level3" data-number="9.10.4">
<h3 data-number="9.10.4" class="anchored" data-anchor-id="cross-species-regulatory-prediction"><span class="header-section-number">9.10.4</span> Cross-Species Regulatory Prediction</h3>
<p>Models pretrained on human regulatory sequences and applied to zebrafish enhancer prediction often underperform zebrafish-specific models despite the apparent relevance of regulatory sequence patterns <span class="citation" data-cites="kelley_cross-species_2020">(<a href="../bib/references.html#ref-kelley_cross-species_2020" role="doc-biblioref">Kelley 2020</a>)</span>. The failure reflects both sequence divergence (zebrafish regulatory motifs differ from human) and lineage-specific regulatory innovations (teleost-specific enhancers have no human homologs from which to transfer).</p>
<p><strong>The boundary between success and failure corresponds to evolutionary conservation: patterns shared across species transfer; species-specific patterns do not.</strong> Transfer succeeds for deeply conserved elements (core promoters, splice sites) but fails for lineage-specific regulatory logic.</p>
</section>
</section>
<section id="validation-and-common-pitfalls" class="level2" data-number="9.11">
<h2 data-number="9.11" class="anchored" data-anchor-id="validation-and-common-pitfalls"><span class="header-section-number">9.11</span> Validation and Common Pitfalls</h2>
<p>A research group reports that their foundation model adaptation achieves 95% accuracy for splice variant classification, far exceeding previous methods. Six months later, clinical deployment reveals performance closer to 70%, with systematic failures on the rare variants that matter most. The initial evaluation was not wrong, but it was misleading. <strong>Proper validation separates genuine transfer success from evaluation artifacts that dissolve on contact with clinical reality.</strong></p>
<div id="fig-validation-pitfalls" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-validation-pitfalls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20A.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER A</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20B.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER B</strong></figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20C.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER C</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20D.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER D</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-validation-pitfalls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.5: [High] Multi-panel figure illustrating common validation failures. Panel A: Venn diagram showing pretraining data / test data overlap creating leakage (inflated performance). Panel B: Timeline showing temporal leakage when training on variants annotated after test set creation. Panel C: Bar chart comparing foundation model against weak baseline (large gap, misleading) vs.&nbsp;properly-tuned baseline (small gap, realistic). Panel D: Stratified performance showing aggregate accuracy (90%) vs.&nbsp;rare-variant accuracy (50%), revealing hidden failures.
</figcaption>
</figure>
</div>
<section id="sources-of-spurious-success" class="level3" data-number="9.11.1">
<h3 data-number="9.11.1" class="anchored" data-anchor-id="sources-of-spurious-success"><span class="header-section-number">9.11.1</span> Sources of Spurious Success</h3>
<p>Test set overlap with pretraining data creates artificial performance inflation. Foundation models trained on massive genomic corpora may inadvertently include sequences later used for evaluation. When benchmarking on variants that appeared in pretraining data (even if unlabeled at pretraining time), the model has seen the sequences and may have memorized relevant patterns. Verifying that test sequences were excluded from pretraining requires careful provenance tracking, which published benchmarks often lack <span class="citation" data-cites="sainz_nlp_2023">(<a href="../bib/references.html#ref-sainz_nlp_2023" role="doc-biblioref"><span>“Placeholder: Lee Pre-Training 2020”</span> 2020</a>)</span>. Chromosome-based splits help but do not fully address the problem when pretraining spans multiple species or includes population-level diversity.</p>
<p>Temporal leakage uses future information unavailable at prediction time. Evaluating a variant pathogenicity model on variants annotated after training data was collected creates an unrealistically favorable setting; the model may have seen related variants or learned from the same evidence that later informed annotations. Temporal splits (training on variants discovered before a cutoff, evaluating on variants discovered afterward) provide more realistic assessment of prospective performance <span class="citation" data-cites="landrum_clinvar_2018">(<a href="../bib/references.html#ref-landrum_clinvar_2018" role="doc-biblioref">Landrum et al. 2018</a>)</span>.</p>
<p>Inappropriate baselines inflate apparent transfer benefits. Comparing adapted foundation models against weak or poorly-tuned from-scratch baselines makes transfer look more valuable than warranted. Strong baselines require equivalent hyperparameter tuning, appropriate architectures for the task, and sufficient training on the same target data. When properly-tuned CNNs match or exceed foundation model performance for a task, the additional complexity of pretrained models may not be justified.</p>
</section>
<section id="evaluation-practices-that-reveal-true-performance" class="level3" data-number="9.11.2">
<h3 data-number="9.11.2" class="anchored" data-anchor-id="evaluation-practices-that-reveal-true-performance"><span class="header-section-number">9.11.2</span> Evaluation Practices That Reveal True Performance</h3>
<p>Single-metric reporting obscures important performance characteristics. A model achieving 90% overall accuracy may show 95% accuracy on common variants and 50% accuracy on rare variants, with the clinically important rare cases hidden by aggregate metrics. Stratified evaluation by allele frequency, variant type, gene family, and other clinically relevant categories reveals whether transfer benefits generalize or concentrate in particular subgroups.</p>
<p>Confidence interval reporting and multiple training runs reveal performance variability. A single training run may produce misleadingly good or bad results through random initialization effects or data sampling. Testing on multiple independent datasets rather than a single benchmark reveals whether gains generalize beyond the specific evaluation setting.</p>
</section>
</section>
<section id="emerging-directions" class="level2" data-number="9.12">
<h2 data-number="9.12" class="anchored" data-anchor-id="emerging-directions"><span class="header-section-number">9.12</span> Emerging Directions</h2>
<p>A rare disease geneticist encounters a variant in a gene with no previous pathogenicity annotations. Current approaches require either zero-shot scoring (which may lack task-specific calibration) or collecting enough labeled examples to train an adapter (which takes months). Emerging transfer learning methods may collapse this timeline, enabling immediate adaptation from a handful of examples provided at inference time rather than during training.</p>
<section id="in-context-learning" class="level3" data-number="9.12.1">
<h3 data-number="9.12.1" class="anchored" data-anchor-id="in-context-learning"><span class="header-section-number">9.12.1</span> In-Context Learning</h3>
<p><strong>In-context learning</strong> enables predictions by conditioning on examples provided in the input context without any parameter updates <span class="citation" data-cites="brown_language_2020">(<a href="../bib/references.html#ref-brown_language_2020" role="doc-biblioref"><span>“Placeholder: Brown Language 2020”</span> 2020</a>)</span>. Very large language models exhibit this capability, performing tasks by observing demonstrations rather than through explicit fine-tuning. Early evidence suggests genomic foundation models at sufficient scale may exhibit similar behavior, classifying variants based on a few pathogenic and benign examples included in the input prompt. This could transform deployment: rather than training adapters or fine-tuning parameters, practitioners would provide examples of desired behavior at inference time.</p>
</section>
<section id="test-time-adaptation" class="level3" data-number="9.12.2">
<h3 data-number="9.12.2" class="anchored" data-anchor-id="test-time-adaptation"><span class="header-section-number">9.12.2</span> Test-Time Adaptation</h3>
<p><strong>Test-time adaptation</strong> updates models during inference based on characteristics of test examples rather than freezing parameters after training <span class="citation" data-cites="wang_tent_2021">(<a href="../bib/references.html#ref-wang_tent_2021" role="doc-biblioref"><span>“Placeholder: Wang Tent 2021”</span> 2021</a>)</span>. For genomic applications facing distribution shift between development and deployment populations, test-time adaptation could adjust model behavior to match deployment-specific characteristics without requiring labeled examples from the deployment distribution. A model developed on European-ancestry data could adapt its uncertainty calibration when encountering African-ancestry variants that differ from training distributions.</p>
</section>
<section id="federated-learning-across-institutions" class="level3" data-number="9.12.3">
<h3 data-number="9.12.3" class="anchored" data-anchor-id="federated-learning-across-institutions"><span class="header-section-number">9.12.3</span> Federated Learning Across Institutions</h3>
<p><strong>Federated transfer learning</strong> enables collaborative model development across institutions without sharing raw genomic data <span class="citation" data-cites="rieke_future_2020">(<a href="../bib/references.html#ref-rieke_future_2020" role="doc-biblioref"><span>“Placeholder: Rieke Future 2020”</span> 2020</a>)</span>. Institutions train local models on private patient data and share only aggregated parameter updates, enabling foundation models to learn from far more diverse data than any single institution can access while preserving patient privacy. This approach could help address the population bias in current genomic datasets by enabling contributions from institutions serving underrepresented populations.</p>
</section>
<section id="toward-theoretical-foundations" class="level3" data-number="9.12.4">
<h3 data-number="9.12.4" class="anchored" data-anchor-id="toward-theoretical-foundations"><span class="header-section-number">9.12.4</span> Toward Theoretical Foundations</h3>
<p>Theoretical foundations for predicting transfer success based on measurable properties of source and target domains would reduce trial-and-error <span class="citation" data-cites="ben-david_theory_2010">(<a href="../bib/references.html#ref-ben-david_theory_2010" role="doc-biblioref"><span>“Placeholder: Ben-David Theory 2010”</span> 2010</a>)</span>. Currently practitioners must empirically test whether transfer helps; theoretical guidance specifying when transfer will succeed based on domain divergence measures, task similarity metrics, or representation analysis could focus effort on promising applications and avoid wasted investment in doomed transfer attempts.</p>
</section>
</section>
<section id="amplification-and-its-risks" class="level2" data-number="9.13">
<h2 data-number="9.13" class="anchored" data-anchor-id="amplification-and-its-risks"><span class="header-section-number">9.13</span> Amplification and Its Risks</h2>
<p>Transfer learning amplifies the value of pretrained models by connecting learned representations to specific applications. A foundation model pretrained on billions of sequences encodes patterns that would require orders of magnitude more labeled data to learn from scratch. Effective transfer realizes this investment; ineffective transfer inherits hidden limitations without the promised benefits.</p>
<p>The risks are concrete. Domain shift between pretraining and deployment contexts causes silent failures: models trained on research cohorts may miscalibrate on clinical populations, models trained on one species may fail unpredictably on another, models trained on one assay technology may not generalize to its successor. These failures produce confident predictions that are systematically wrong, often in ways that correlate with clinically relevant subgroups. Detection through distribution divergence measures and embedding visualization can identify shift before deployment, but mitigation requires either domain-adaptive fine-tuning or acceptance that some shifts cannot be bridged.</p>
<p>Validating transfer claims requires adversarial rigor. Test for contamination between pretraining and evaluation data through sequence-level deduplication. Implement temporal splits that respect real-world prediction scenarios. Compare against properly-tuned baselines trained from scratch with equivalent effort. Stratify performance by ancestry, variant type, and other clinically meaningful categories. The goal is establishing whether transfer provides genuine benefit under realistic deployment conditions, not demonstrating impressive numbers on favorable benchmarks. The foundation models examined in subsequent chapters assume that transfer succeeds; the methods here determine whether that assumption holds for specific applications.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-avsec_enformer_2021" class="csl-entry" role="listitem">
Avsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. <span>“[<span>Enformer</span>] <span>Effective</span> Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.”</span> <em>Nature Methods</em> 18 (October): 1196–1203. <a href="https://doi.org/10.1038/s41592-021-01252-x">https://doi.org/10.1038/s41592-021-01252-x</a>.
</div>
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-hu_lora_2021" class="csl-entry" role="listitem">
<span>“Hu_lora_2021.”</span> n.d.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-kelley_cross-species_2020" class="csl-entry" role="listitem">
Kelley, David R. 2020. <span>“Cross-Species Regulatory Sequence Activity Prediction.”</span> <em>PLOS Computational Biology</em> 16 (7): e1008050. <a href="https://doi.org/10.1371/journal.pcbi.1008050">https://doi.org/10.1371/journal.pcbi.1008050</a>.
</div>
<div id="ref-kircher_general_2014" class="csl-entry" role="listitem">
Kircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory M. Cooper, and Jay Shendure. 2014. <span>“A General Framework for Estimating the Relative Pathogenicity of Human Genetic Variants.”</span> <em>Nature Genetics</em> 46 (3): 310–15. <a href="https://doi.org/10.1038/ng.2892">https://doi.org/10.1038/ng.2892</a>.
</div>
<div id="ref-landrum_clinvar_2018" class="csl-entry" role="listitem">
Landrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. <span>“<span>ClinVar</span>: Improving Access to Variant Interpretations and Supporting Evidence.”</span> <em>Nucleic Acids Research</em> 46 (D1): D1062–67. <a href="https://doi.org/10.1093/nar/gkx1153">https://doi.org/10.1093/nar/gkx1153</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-ben-david_theory_2010" class="csl-entry" role="listitem">
<span>“Placeholder: Ben-David Theory 2010.”</span> 2010.
</div>
<div id="ref-brown_language_2020" class="csl-entry" role="listitem">
<span>“Placeholder: Brown Language 2020.”</span> 2020.
</div>
<div id="ref-finn_model-agnostic_2017" class="csl-entry" role="listitem">
<span>“Placeholder: Finn Model-Agnostic 2017.”</span> 2017.
</div>
<div id="ref-gaedigk_pharmacogene_2018" class="csl-entry" role="listitem">
<span>“Placeholder: Gaedigk Pharmacogene 2018.”</span> 2018.
</div>
<div id="ref-ganin_domain-adversarial_2016" class="csl-entry" role="listitem">
<span>“Placeholder: Ganin Domain-Adversarial 2016.”</span> 2016.
</div>
<div id="ref-houlsby_parameter-efficient_2019" class="csl-entry" role="listitem">
<span>“Placeholder: Houlsby Parameter-Efficient 2019.”</span> 2019.
</div>
<div id="ref-howard_universal_2018" class="csl-entry" role="listitem">
<span>“Placeholder: Howard Universal 2018.”</span> 2018.
</div>
<div id="ref-jawahar_what_2019" class="csl-entry" role="listitem">
<span>“Placeholder: Jawahar What 2019.”</span> 2019.
</div>
<div id="ref-sainz_nlp_2023" class="csl-entry" role="listitem">
<span>“Placeholder: Lee Pre-Training 2020.”</span> 2020.
</div>
<div id="ref-li_prefix-tuning_2021" class="csl-entry" role="listitem">
<span>“Placeholder: Li Prefix-Tuning 2021.”</span> 2021.
</div>
<div id="ref-mccloskey_catastrophic_1989" class="csl-entry" role="listitem">
<span>“Placeholder: Mccloskey Catastrophic 1989.”</span> 1989.
</div>
<div id="ref-meier_language_2021" class="csl-entry" role="listitem">
<span>“Placeholder: Meier Language 2021.”</span> 2021.
</div>
<div id="ref-rieke_future_2020" class="csl-entry" role="listitem">
<span>“Placeholder: Rieke Future 2020.”</span> 2020.
</div>
<div id="ref-rives_biological_2021" class="csl-entry" role="listitem">
<span>“Placeholder: Rives Biological 2021.”</span> 2021.
</div>
<div id="ref-snell_prototypical_2017" class="csl-entry" role="listitem">
<span>“Placeholder: Snell Prototypical 2017.”</span> 2017.
</div>
<div id="ref-wang_characterizing_2019" class="csl-entry" role="listitem">
<span>“Placeholder: Wang Characterizing 2019.”</span> 2019.
</div>
<div id="ref-wang_tent_2021" class="csl-entry" role="listitem">
<span>“Placeholder: Wang Tent 2021.”</span> 2021.
</div>
<div id="ref-zaken_bitfit_2022" class="csl-entry" role="listitem">
<span>“Placeholder: Zaken Bitfit 2022.”</span> 2022.
</div>
<div id="ref-suzek_uniref_2015" class="csl-entry" role="listitem">
Suzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. <span>“<span>UniRef</span>: Comprehensive and Non-Redundant <span>UniProt</span> Reference Clusters.”</span> <em>Bioinformatics</em> 23 (10): 1282–88. <a href="https://doi.org/10.1093/bioinformatics/btm098">https://doi.org/10.1093/bioinformatics/btm098</a>.
</div>
<div id="ref-zhou_dnabert-2_2023" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_2/p2-ch08-pretraining.html" class="pagination-link" aria-label="Pretraining Strategies">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_3/p3--architectures.html" class="pagination-link" aria-label="Part III: Foundation Model Families">
        <span class="nav-page-text">Part III: Foundation Model Families</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>