<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Transformers and Attention – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_2/p2-ch08-pretraining.html" rel="next">
<link href="../part_2/p2-ch06-cnn.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch07-attention.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch12-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch14-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch15-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch16-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch17-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--multi-modal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Systems and Scale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch19-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch20-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch21-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch22-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch25-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Causal Inference with Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch26-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch28-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch29-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch30-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch31-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch07-self-attention" id="toc-sec-ch07-self-attention" class="nav-link active" data-scroll-target="#sec-ch07-self-attention"><span class="header-section-number">7.1</span> Self-Attention Mechanism</a>
  <ul class="collapse">
  <li><a href="#sec-ch07-qkv" id="toc-sec-ch07-qkv" class="nav-link" data-scroll-target="#sec-ch07-qkv"><span class="header-section-number">7.1.1</span> Query, Key, and Value Vectors</a></li>
  <li><a href="#sec-ch07-multihead" id="toc-sec-ch07-multihead" class="nav-link" data-scroll-target="#sec-ch07-multihead"><span class="header-section-number">7.1.2</span> Multi-Head Attention</a></li>
  </ul></li>
  <li><a href="#sec-ch07-positional-encoding" id="toc-sec-ch07-positional-encoding" class="nav-link" data-scroll-target="#sec-ch07-positional-encoding"><span class="header-section-number">7.2</span> Positional Encoding</a>
  <ul class="collapse">
  <li><a href="#sec-ch07-absolute-position" id="toc-sec-ch07-absolute-position" class="nav-link" data-scroll-target="#sec-ch07-absolute-position"><span class="header-section-number">7.2.1</span> Absolute Position Encodings</a></li>
  <li><a href="#sec-ch07-relative-position" id="toc-sec-ch07-relative-position" class="nav-link" data-scroll-target="#sec-ch07-relative-position"><span class="header-section-number">7.2.2</span> Relative Position Encodings</a></li>
  <li><a href="#sec-ch07-genomic-position" id="toc-sec-ch07-genomic-position" class="nav-link" data-scroll-target="#sec-ch07-genomic-position"><span class="header-section-number">7.2.3</span> Genomic Position Considerations</a></li>
  </ul></li>
  <li><a href="#sec-ch07-transformer-block" id="toc-sec-ch07-transformer-block" class="nav-link" data-scroll-target="#sec-ch07-transformer-block"><span class="header-section-number">7.3</span> Transformer Block</a>
  <ul class="collapse">
  <li><a href="#sec-ch07-block-components" id="toc-sec-ch07-block-components" class="nav-link" data-scroll-target="#sec-ch07-block-components"><span class="header-section-number">7.3.1</span> Block Components</a></li>
  <li><a href="#sec-ch07-depth" id="toc-sec-ch07-depth" class="nav-link" data-scroll-target="#sec-ch07-depth"><span class="header-section-number">7.3.2</span> Information Flow and Depth</a></li>
  </ul></li>
  <li><a href="#sec-ch07-scaling" id="toc-sec-ch07-scaling" class="nav-link" data-scroll-target="#sec-ch07-scaling"><span class="header-section-number">7.4</span> Scaling to Genomic Sequences</a>
  <ul class="collapse">
  <li><a href="#sec-ch07-quadratic-barrier" id="toc-sec-ch07-quadratic-barrier" class="nav-link" data-scroll-target="#sec-ch07-quadratic-barrier"><span class="header-section-number">7.4.1</span> Quadratic Barrier</a></li>
  <li><a href="#sec-ch07-parameters" id="toc-sec-ch07-parameters" class="nav-link" data-scroll-target="#sec-ch07-parameters"><span class="header-section-number">7.4.2</span> Parameter Considerations</a></li>
  <li><a href="#sec-ch07-context-strategies" id="toc-sec-ch07-context-strategies" class="nav-link" data-scroll-target="#sec-ch07-context-strategies"><span class="header-section-number">7.4.3</span> Context Length Strategies</a></li>
  <li><a href="#sec-ch07-memory" id="toc-sec-ch07-memory" class="nav-link" data-scroll-target="#sec-ch07-memory"><span class="header-section-number">7.4.4</span> Memory and Precision</a></li>
  </ul></li>
  <li><a href="#sec-ch07-variants" id="toc-sec-ch07-variants" class="nav-link" data-scroll-target="#sec-ch07-variants"><span class="header-section-number">7.5</span> Architectural Variants for Genomics</a>
  <ul class="collapse">
  <li><a href="#sec-ch07-encoder-only" id="toc-sec-ch07-encoder-only" class="nav-link" data-scroll-target="#sec-ch07-encoder-only"><span class="header-section-number">7.5.1</span> Encoder-Only Transformers</a></li>
  <li><a href="#sec-ch07-decoder-only" id="toc-sec-ch07-decoder-only" class="nav-link" data-scroll-target="#sec-ch07-decoder-only"><span class="header-section-number">7.5.2</span> Decoder-Only Transformers</a></li>
  <li><a href="#sec-ch07-encoder-decoder" id="toc-sec-ch07-encoder-decoder" class="nav-link" data-scroll-target="#sec-ch07-encoder-decoder"><span class="header-section-number">7.5.3</span> Encoder-Decoder Transformers</a></li>
  <li><a href="#sec-ch07-hybrid" id="toc-sec-ch07-hybrid" class="nav-link" data-scroll-target="#sec-ch07-hybrid"><span class="header-section-number">7.5.4</span> Hybrid CNN-Transformer Models</a></li>
  </ul></li>
  <li><a href="#sec-ch07-training" id="toc-sec-ch07-training" class="nav-link" data-scroll-target="#sec-ch07-training"><span class="header-section-number">7.6</span> Training Dynamics</a>
  <ul class="collapse">
  <li><a href="#sec-ch07-optimization" id="toc-sec-ch07-optimization" class="nav-link" data-scroll-target="#sec-ch07-optimization"><span class="header-section-number">7.6.1</span> Optimization</a></li>
  <li><a href="#sec-ch07-regularization" id="toc-sec-ch07-regularization" class="nav-link" data-scroll-target="#sec-ch07-regularization"><span class="header-section-number">7.6.2</span> Regularization</a></li>
  <li><a href="#sec-ch07-gradients" id="toc-sec-ch07-gradients" class="nav-link" data-scroll-target="#sec-ch07-gradients"><span class="header-section-number">7.6.3</span> Gradient Stability</a></li>
  <li><a href="#sec-ch07-distributed" id="toc-sec-ch07-distributed" class="nav-link" data-scroll-target="#sec-ch07-distributed"><span class="header-section-number">7.6.4</span> Distributed Training</a></li>
  </ul></li>
  <li><a href="#sec-ch07-limitations" id="toc-sec-ch07-limitations" class="nav-link" data-scroll-target="#sec-ch07-limitations"><span class="header-section-number">7.7</span> Limitations and Emerging Alternatives</a>
  <ul class="collapse">
  <li><a href="#sec-ch07-quadratic-ceiling" id="toc-sec-ch07-quadratic-ceiling" class="nav-link" data-scroll-target="#sec-ch07-quadratic-ceiling"><span class="header-section-number">7.7.1</span> Quadratic Ceiling</a></li>
  <li><a href="#sec-ch07-ssm" id="toc-sec-ch07-ssm" class="nav-link" data-scroll-target="#sec-ch07-ssm"><span class="header-section-number">7.7.2</span> State Space Models</a></li>
  <li><a href="#sec-ch07-choosing" id="toc-sec-ch07-choosing" class="nav-link" data-scroll-target="#sec-ch07-choosing"><span class="header-section-number">7.7.3</span> Choosing Architectures</a></li>
  </ul></li>
  <li><a href="#sec-ch07-conclusion" id="toc-sec-ch07-conclusion" class="nav-link" data-scroll-target="#sec-ch07-conclusion"><span class="header-section-number">7.8</span> Capacity Without Direction</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch07-attention.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch07-attention" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled" title="Chapter Overview">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Prerequisites:</strong> This chapter builds on the convolutional architectures from <a href="p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> (particularly the receptive field concept) and the tokenization/embedding strategies from <a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>. Familiarity with matrix multiplication and basic neural network concepts (from <a href="../appendix/app-a-dl.html" class="quarto-xref"><span>Appendix A</span></a>) will help with the mathematical sections.</p>
<p><strong>Learning Objectives:</strong> After completing this chapter, you should be able to:</p>
<ol type="1">
<li>Explain how self-attention computes relationships between all sequence positions simultaneously</li>
<li>Describe the query-key-value mechanism and why it separates “what to look for” from “what to send”</li>
<li>Compare absolute versus relative positional encodings and their genomic implications</li>
<li>Articulate why quadratic complexity limits standard transformers for genomic applications</li>
<li>Select appropriate transformer variants (encoder-only, decoder-only, hybrid) for different genomic tasks</li>
</ol>
<p><strong>Key Insight:</strong> Attention asks “what distant information matters here?” rather than “what local pattern exists here?” This shift from local to global context is what enables modeling of enhancer-promoter interactions, long-range splicing effects, and other regulatory relationships that span tens of kilobases.</p>
</div>
</div>
<p>Where convolutional networks ask “what local pattern exists here,” attention asks a different question: “what distant information matters here?” This reformulation changed what genomic models could learn. The convolutional architectures examined in <a href="p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> detect motifs, chromatin marks, and regulatory grammar with high fidelity, but they remain blind to dependencies beyond their <strong>receptive field</strong>. Attention computes direct interactions between all positions simultaneously, allowing a nucleotide near a gene promoter to attend to an enhancer 100 kilobases away without information passing through intermediate layers. The shift is not merely architectural; it reflects a different assumption about how sequence encodes function. Local patterns matter, but so do long-range relationships that convolutions cannot capture.</p>
<p>The <strong>attention mechanism</strong>, introduced for machine translation by Vaswani et al., resolved a tension that had constrained sequence modeling for years <span class="citation" data-cites="vaswani_attention_2017">(<a href="../bib/references.html#ref-vaswani_attention_2017" role="doc-biblioref">Vaswani et al. 2023</a>)</span>. Recurrent networks could maintain context across arbitrary distances but processed sequences one position at a time, creating training bottlenecks that limited practical sequence length. Convolutional networks processed sequences in parallel but could only integrate information within fixed receptive fields. Attention achieves both: parallel computation across all positions with direct modeling of arbitrary-range dependencies. For genomic sequences, where enhancer-promoter interactions span tens of kilobases and topologically associating domains organize contacts across megabases, this capacity for long-range modeling proved transformative.</p>
<p>Attention enables capabilities that convolutions cannot achieve. Regulatory interactions spanning distances beyond any practical <strong>convolutional neural network (CNN)</strong> receptive field become directly modelable when every position can attend to every other. Different attention heads specialize for different relationship types: some learn to focus on nearby positions for local motif context, others attend across tens of kilobases to capture enhancer-promoter relationships, and still others exhibit periodic patterns suggestive of nucleosome spacing or chromatin organization. These specializations emerge from training without explicit supervision. The transformer architecture that houses these attention mechanisms has become the dominant substrate for genomic foundation models, from <em>DNABERT’s</em> regulatory sequence representations to <em>Enformer’s</em> expression predictions to <em>ESM-2’s</em> protein embeddings <span class="citation" data-cites="lin_evolutionary_2023">(<a href="../bib/references.html#ref-lin_evolutionary_2023" role="doc-biblioref"><strong>lin_evolutionary_2023?</strong></a>)</span>. Yet attention mechanisms still struggle with the quadratic computational cost that limits context length, creating a tension between the long-range modeling that genomic applications require and the practical constraints of current hardware.</p>
<section id="sec-ch07-self-attention" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="sec-ch07-self-attention"><span class="header-section-number">7.1</span> Self-Attention Mechanism</h2>
<p>A 28-year-old woman presents with dilated cardiomyopathy and a variant of uncertain significance in the <em>LMNA</em> gene’s promoter region. Her clinician needs to determine whether this variant disrupts regulatory elements that control <em>LMNA</em> expression in cardiac tissue. The relevant information spans thousands of base pairs: transcription factor binding sites flanking the variant, enhancers that drive cardiac-specific expression, and insulators that constrain regulatory interactions. A model that can only aggregate local context (through convolutional or recurrent operations) must pass information through many intermediate layers, each adding noise and limiting what survives the journey. When this information pathway fails, the variant appears as noise rather than the pathogenic regulatory disruption it may represent. The fundamental question is how to let any position in a sequence directly access information from any other position, regardless of distance.</p>
<div id="fig-self-attention" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/01-A-fig-self-attention.svg" class="img-fluid figure-img"></p>
<figcaption>Input embeddings</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/01-B-fig-self-attention.svg" class="img-fluid figure-img"></p>
<figcaption>Query, Key, Value projections</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/01-C-fig-self-attention.svg" class="img-fluid figure-img"></p>
<figcaption>Attention score matrix</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/01-D-fig-self-attention.svg" class="img-fluid figure-img"></p>
<figcaption>Attention weight matrix</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/01-E-fig-self-attention.svg" class="img-fluid figure-img"></p>
<figcaption>Weighted value aggregation</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.1: Step-by-step visualization of self-attention on a regulatory sequence. (A) Input embeddings represent each nucleotide position as a learned vector. (B) Linear projections produce query (what to seek), key (what to advertise), and value (what to send) vectors through learned weight matrices. (C) Attention scores are computed as scaled dot products between all query-key pairs, capturing content-based relevance. (D) Softmax normalization converts scores to attention weights forming probability distributions over positions. (E) Each output is a weighted sum of value vectors, with weights determining how much each position contributes. This mechanism enables direct communication between any two positions regardless of sequence distance.
</figcaption>
</figure>
</div>
<p><strong>Self-attention</strong> answers this question by computing all pairwise interactions simultaneously, allowing the model to directly relate any position to any other regardless of distance. Where convolutions apply fixed filters uniformly across the sequence, attention performs dynamic routing: each position queries the entire sequence and aggregates information based on content-dependent relevance scores. The routing changes for every input because attention weights depend on what the sequence contains, not just where positions sit relative to each other. For the <em>LMNA</em> variant, this means the model can directly assess whether the variant position interacts with known cardiac enhancers without that signal degrading through layer after layer of local aggregation.</p>
<section id="sec-ch07-qkv" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="sec-ch07-qkv"><span class="header-section-number">7.1.1</span> Query, Key, and Value Vectors</h3>
<div class="callout callout-style-default callout-warning callout-titled" title="Mathematical Content Ahead">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Content Ahead
</div>
</div>
<div class="callout-body-container callout-body">
<p>The next section presents the mathematical formulation of attention. The key equations are the scaled dot-product (<span class="math inline">\(\text{score}(q_i, k_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}\)</span>) and the weighted aggregation. If you find the notation challenging, focus on the intuition: queries ask questions, keys advertise content, and values carry the actual information that flows between positions.</p>
</div>
</div>
<p>At each position in the input sequence, self-attention computes three vectors: a <strong>query</strong>, a <strong>key</strong>, and a <strong>value</strong>. These vectors emerge from multiplying the input embedding at that position by three learned weight matrices <span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>, and <span class="math inline">\(W^V\)</span>. The query represents what information this position seeks from other positions. The key represents what information this position offers to queries from elsewhere. The value represents the actual information this position contributes when attended to. This query-key-value structure separates the question of “which positions should interact” (determined by query-key similarity) from “what information flows between them” (determined by values).</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Key Insight: The Information Retrieval Analogy">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: The Information Retrieval Analogy
</div>
</div>
<div class="callout-body-container callout-body">
<p>Think of attention like a search engine for sequence positions. Each position issues a <strong>query</strong> describing what it’s looking for. Every position also publishes a <strong>key</strong> describing what it has to offer. The dot product between query and key measures relevance, just as a search engine ranks documents by query match. High-scoring positions then send their <strong>value</strong> (the actual content) to the querying position. The separation of key (what I advertise) from value (what I actually send) allows the same position to advertise relevance for one type of information while sending different information when attended to.</p>
</div>
</div>
<p>The attention mechanism computes similarity scores between each query and all keys. For position <span class="math inline">\(i\)</span>, we compute the dot product between its query <span class="math inline">\(q_i\)</span> and every key <span class="math inline">\(k_j\)</span> across all positions <span class="math inline">\(j = 1, \ldots, L\)</span>, where <span class="math inline">\(L\)</span> is sequence length. These scores are scaled by <span class="math inline">\(\sqrt{d_k}\)</span> (the square root of the key dimension) to prevent the dot products from growing large in high dimensions, which would push softmax outputs toward extreme values and create vanishing gradients:</p>
<p><span class="math display">\[
\text{score}(q_i, k_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
\]</span></p>
<p>A softmax function converts these scores into attention weights <span class="math inline">\(\alpha_{ij}\)</span> that form a probability distribution over positions:</p>
<p><span class="math display">\[
\alpha_{ij} = \frac{\exp(\text{score}(q_i, k_j))}{\sum_{j'=1}^L \exp(\text{score}(q_i, k_{j'}))}
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Deep Dive: The Softmax Function">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deep Dive: The Softmax Function
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>For biology readers:</strong> Softmax converts raw scores into probabilities that sum to 1:</p>
<p><strong>What it does:</strong> Given a set of scores (which could be any real numbers), softmax produces positive values that sum to 1, interpretable as probabilities or weights.</p>
<p><strong>The formula intuition:</strong></p>
<ol type="1">
<li>Exponentiate each score: <span class="math inline">\(\exp(\text{score})\)</span> makes all values positive</li>
<li>Divide by the sum: ensures outputs sum to 1</li>
</ol>
<p><strong>Example:</strong> If three positions have scores [2.0, 1.0, 0.5], softmax produces:</p>
<ul>
<li>Position 1: <span class="math inline">\(\exp(2.0)/(\exp(2.0)+\exp(1.0)+\exp(0.5)) \approx 0.59\)</span></li>
<li>Position 2: <span class="math inline">\(\exp(1.0)/... \approx 0.22\)</span></li>
<li>Position 3: <span class="math inline">\(\exp(0.5)/... \approx 0.13\)</span></li>
</ul>
<p><strong>Key properties:</strong></p>
<ul>
<li>Higher scores → higher weights (exponential amplifies differences)</li>
<li>All weights are positive and sum to 1</li>
<li>Very high scores dominate; very low scores become negligible</li>
</ul>
<p><strong>In attention:</strong> Softmax turns similarity scores into “how much to pay attention to each position.” High-scoring positions receive most of the attention weight; low-scoring positions are effectively ignored.</p>
</div>
</div>
<p>These weights determine how strongly position <span class="math inline">\(i\)</span> attends to each other position. High weight means position <span class="math inline">\(i\)</span> aggregates substantial information from position <span class="math inline">\(j\)</span>; low weight means position <span class="math inline">\(j\)</span> contributes little to the output at position <span class="math inline">\(i\)</span>. The final output at position <span class="math inline">\(i\)</span> is a weighted sum of all value vectors:</p>
<p><span class="math display">\[
\text{output}_i = \sum_{j=1}^L \alpha_{ij} v_j
\]</span></p>
<p>This weighted aggregation forms the core of self-attention. Each output position receives a mixture of information from across the entire sequence, with mixture proportions learned through backpropagation. For genomic sequences, this means a position near a splice site can attend to both the upstream exon and downstream intron, integrating context that determines whether splicing occurs. A position in a promoter can attend to distant enhancers, learning which distal elements influence expression at this gene. When predicting the pathogenicity of a variant in the <em>SCN5A</em> promoter (mutations in which cause Brugada syndrome and long QT syndrome, affecting approximately 1 in 2,000 individuals), the model can simultaneously consider the core promoter elements, upstream enhancers that drive cardiac-specific expression, and downstream regulatory regions that modulate expression levels.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Stop and Think: Query-Key-Value Separation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think: Query-Key-Value Separation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a transcription factor binding site within a promoter region.</p>
<ol type="1">
<li>What kind of “query” might this position issue? (What information would help determine its regulatory function?)</li>
<li>What “key” might it advertise? (What aspect of its identity is relevant to other positions?)</li>
<li>What “value” might it send when attended to? (What information should flow to positions that find it relevant?)</li>
</ol>
<p>The separation of these three roles is what gives attention its flexibility. A TATA box might query for nearby transcription start sites while advertising its identity as a core promoter element.</p>
</div>
</div>
</section>
<section id="sec-ch07-multihead" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="sec-ch07-multihead"><span class="header-section-number">7.1.2</span> Multi-Head Attention</h3>
<div id="fig-multihead-attention" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multihead-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/02-fig-multihead-attention.svg" class="img-fluid figure-img"></p>
<figcaption>Multi-head attention captures diverse biological relationships</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multihead-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.2: Multi-head attention captures diverse biological relationships. Different heads in a trained genomic transformer learn specialized attention patterns. Head 1 attends locally to nearby positions, capturing motif context. Head 2 shows periodic attention at approximately 200 base pair intervals, potentially reflecting nucleosome spacing. Head 3 attends specifically to positions matching sequence motifs like CTCF binding sites. Head 4 exhibits long-range attention connecting distant positions, consistent with enhancer-promoter interactions. This specialization emerges from training without explicit supervision, demonstrating that the chromatin prediction objective induces biologically meaningful attention patterns.
</figcaption>
</figure>
</div>
<p>A patient presenting with a complex arrhythmia may carry variants affecting both a cardiac ion channel’s coding sequence and its distal enhancer. Understanding this case requires the model to simultaneously track local splice site context around the coding variant and enhancer-promoter relationships spanning 50 kilobases. A single attention operation cannot capture both patterns effectively: when forced to learn one pattern of position interactions, the model faces an impossible choice between attending strongly to nearby positions for local regulatory context or attending to distant positions for enhancer-gene relationships. Genomic sequences exhibit multiple types of dependencies simultaneously, and forcing all these interaction types through a single attention pattern creates destructive competition.</p>
<p><strong>Multi-head attention</strong> extends the basic mechanism by running multiple attention operations in parallel, each with independent learned projections <span class="citation" data-cites="vaswani_attention_2017">(<a href="../bib/references.html#ref-vaswani_attention_2017" role="doc-biblioref">Vaswani et al. 2023</a>)</span>. If we use <em>H</em> heads, we split the model dimension <em>d</em> into <em>H</em> subspaces of dimension <em>d</em>/<em>H</em>, compute separate queries, keys, and values for each head, run attention independently, concatenate outputs, and project back to dimension <em>d</em>. Different heads can specialize in different interaction types without competing for attention capacity.</p>
<p>In genomic models, one head might attend to nearby positions (capturing local motif context) while another attends to positions at characteristic distances (capturing nucleosome spacing or enhancer-promoter loops). Empirical analysis of trained genomic transformers reveals diverse attention patterns: some heads attend locally regardless of content, others attend to specific sequence motifs like TATA boxes or CTCF binding sites, and still others show distance-dependent patterns suggestive of chromatin organization <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. This specialization emerges from training without explicit supervision, reflecting the model’s discovery that different types of interactions require different aggregation patterns. Methods for visualizing and interpreting these learned attention patterns are examined in <a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 25</span></a>.</p>
<div id="fig-attention-patterns" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/03-A-fig-attention-patterns.svg" class="img-fluid figure-img"></p>
<figcaption>Enhancer-promoter attention</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/03-B-fig-attention-patterns.svg" class="img-fluid figure-img"></p>
<figcaption>Genomic context overlay</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/03-C-fig-attention-patterns.svg" class="img-fluid figure-img"></p>
<figcaption>Local attention head</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.3: Attention patterns in genomic transformers reveal learned regulatory relationships. (A) An attention head showing strong weights between a promoter and distal enhancer approximately 50kb apart, demonstrating learned long-range regulatory attention. (B) The same pattern overlaid on genomic context, showing how attention arcs connect the promoter to its regulatory enhancer. (C) A different head from the same model shows local attention patterns, capturing nearby sequence context. This head specialization emerges from training: some heads learn to integrate long-range regulatory signals while others focus on local motif relationships.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Key Insight: Head Specialization">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Head Specialization
</div>
</div>
<div class="callout-body-container callout-body">
<p>Multi-head attention is not just about capacity; it is about division of labor. Different heads learn to track different biological relationships:</p>
<ul>
<li><strong>Local heads</strong> capture motif context and nearby regulatory grammar</li>
<li><strong>Periodic heads</strong> may learn nucleosome spacing (~147 bp) or helical periodicity (~10.5 bp)</li>
<li><strong>Long-range heads</strong> capture enhancer-promoter interactions across tens of kilobases</li>
<li><strong>Motif-specific heads</strong> attend selectively to known binding sites (CTCF, TATA)</li>
</ul>
<p>This specialization emerges without supervision. The model discovers that genomic function depends on multiple relationship types operating simultaneously.</p>
</div>
</div>
<p>The multi-head structure also provides redundancy that aids training. If one head fails to learn useful patterns, others can compensate. Gradient flow through multiple parallel paths stabilizes optimization. For genomic applications where training data may be limited compared to natural language corpora, this redundancy helps prevent individual heads from overfitting to spurious correlations. The number of heads represents a design choice: too few heads limit the diversity of learnable patterns, while too many heads reduce the dimensionality available to each head, potentially limiting their individual expressiveness. Most genomic transformers use 8 to 16 heads, balancing diversity against per-head capacity.</p>
</section>
</section>
<section id="sec-ch07-positional-encoding" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="sec-ch07-positional-encoding"><span class="header-section-number">7.2</span> Positional Encoding</h2>
<p>A patient with hypertrophic cardiomyopathy carries a variant in the <em>MYH7</em> gene’s promoter region. Determining pathogenicity requires knowing precisely where the variant sits relative to the transcription start site: a variant at position -30 (where the TATA box resides) carries entirely different implications than the same sequence at position +500 (within the 5’ UTR). Position is not merely bookkeeping for genomic sequences; it encodes biological function. The canonical TATA box must appear 25 to 30 base pairs upstream of transcription initiation to function; the same sequence elsewhere carries no regulatory significance. Splice site recognition depends on the invariant GT and AG dinucleotides appearing at precise distances from exon boundaries. Enhancer-promoter interactions require specific distance relationships that vary by locus and cell type. A model that cannot distinguish position 100 from position 10,000 cannot learn the positional grammar that governs gene regulation.</p>
<p>Self-attention, by design, computes interactions based purely on content: the attention weight between positions depends only on their query and key vectors, not on where they sit in the sequence. The basic concepts of position encoding were introduced in <a href="p2-ch05-representations.html#sec-ch05-position" class="quarto-xref"><span>Section 5.6.1</span></a>; here we examine the specific implementations that transformer architectures employ. Shuffling input token order changes nothing about how attention weights are computed. The model has no inherent notion of sequence order, a property called <strong>permutation invariance</strong>. For genomic data where position matters fundamentally, this blindness to order would be catastrophic. DNA has 5’ to 3’ directionality that determines transcription direction. Distance from transcription start sites determines promoter versus enhancer classification. Strand orientation distinguishes sense from antisense transcription. <strong>Positional encodings</strong> inject information about token positions into the model, breaking permutation invariance by making the model aware of where each token sits in the sequence.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Stop and Think: Why Position Matters">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think: Why Position Matters
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading about specific positional encoding methods, consider these questions:</p>
<ol type="1">
<li>A TATA box sequence (TATAAA) appears at two locations: 28 bp upstream of a transcription start site, and 5,000 bp upstream. Should the model treat these identically?</li>
<li>An enhancer-promoter interaction works when the enhancer is 50 kb upstream but not when it is 500 kb upstream. What does the model need to learn about distance?</li>
<li>If you were designing a positional encoding for genomic sequences, would you encode absolute positions (position 1, 2, 3…) or relative distances between positions? What are the tradeoffs?</li>
</ol>
</div>
</div>
<div id="fig-position-encodings" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-position-encodings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/04-A-fig-position-encodings.svg" class="img-fluid figure-img"></p>
<figcaption>Learned absolute positional embeddings</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/04-B-fig-position-encodings.svg" class="img-fluid figure-img"></p>
<figcaption>Sinusoidal positional encoding</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/04-C-fig-position-encodings.svg" class="img-fluid figure-img"></p>
<figcaption>ALiBi attention bias</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/04-D-fig-position-encodings.svg" class="img-fluid figure-img"></p>
<figcaption>Rotary Position Embeddings</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-position-encodings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.4: Positional encoding approaches for transformers. (A) Learned absolute embeddings assign a trained vector to each position; effective but limited to training sequence length. (B) Sinusoidal encodings use fixed sine/cosine functions at different frequencies, creating unique position fingerprints that generalize beyond training length. (C) ALiBi applies linear attention penalties based on distance, encouraging local attention while naturally extrapolating to long sequences. (D) RoPE encodes position through geometric rotation, making dot products depend on relative rather than absolute positions. Each approach encodes different assumptions about how position information should influence attention.
</figcaption>
</figure>
</div>
<section id="sec-ch07-absolute-position" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="sec-ch07-absolute-position"><span class="header-section-number">7.2.1</span> Absolute Position Encodings</h3>
<div class="callout callout-style-default callout-warning callout-titled" title="Mathematical Content Ahead">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Content Ahead
</div>
</div>
<div class="callout-body-container callout-body">
<p>The sinusoidal positional encoding formulas below may appear complex. The key intuition is that different dimensions of the encoding oscillate at different frequencies, creating a unique “fingerprint” for each position. Low-frequency components indicate coarse position (beginning vs.&nbsp;end of sequence), while high-frequency components indicate fine position (exact nucleotide location).</p>
</div>
</div>
<p>The original transformer used sinusoidal functions with different frequencies for each embedding dimension <span class="citation" data-cites="vaswani_attention_2017">(<a href="../bib/references.html#ref-vaswani_attention_2017" role="doc-biblioref">Vaswani et al. 2023</a>)</span>. For position <span class="math inline">\(pos\)</span> and dimension <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_\text{model}}}\right)
\]</span></p>
<p><span class="math display">\[
\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_\text{model}}}\right)
\]</span></p>
<p>These fixed patterns have useful properties. They are deterministic (the same for all sequences), allow the model to learn to attend by relative positions (since <span class="math inline">\(\mathrm{PE}(pos+k)\)</span> can be expressed as a linear function of <span class="math inline">\(\mathrm{PE}(pos)\)</span>), and generalize to sequence lengths not seen during training. The different frequencies across dimensions create a unique “fingerprint” for each position: lower-frequency components capture coarse position while higher-frequency components capture fine position.</p>
<p>Many genomic models use <strong>learned positional embeddings</strong> instead: lookup tables where each position has a learned vector added to the input embedding. <em>DNABERT</em> and <em>Nucleotide Transformer</em> both employ learned positional embeddings, allowing the model to discover position-dependent patterns specific to genomic data <span class="citation" data-cites="ji_dnabert_2021 dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>; <a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. The trade-off is that learned embeddings do not automatically extrapolate to longer sequences. A model trained with maximum sequence length of 512 tokens has no learned embedding for position 513, creating a hard boundary on sequence length at inference time. This fixed maximum context proves particularly restrictive for genomics, where biological phenomena span scales from individual binding sites to megabase regulatory domains.</p>
</section>
<section id="sec-ch07-relative-position" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="sec-ch07-relative-position"><span class="header-section-number">7.2.2</span> Relative Position Encodings</h3>
<p>Absolute encodings treat position 1,000 and position 1,001 as having different representations even though their relative relationship (adjacent positions) may matter more than their absolute locations. For genomic applications, relative distance often carries more biological meaning than absolute coordinates: nucleosomes are spaced approximately 200 base pairs apart regardless of genomic location, and enhancer-promoter interactions depend on distance rather than absolute position. A transcription factor binding site 50 bases upstream of a transcription start site has similar effects whether the TSS sits at genomic position 1,000 or 1,000,000. <strong>Relative positional encodings</strong> address this by encoding distances between positions rather than absolute coordinates.</p>
<p><em>T5</em>-style relative position bias adds a learnable scalar to attention scores based on the distance between query and key positions <span class="citation" data-cites="raffel_t5_2019">(<a href="../bib/references.html#ref-raffel_t5_2019" role="doc-biblioref">Raffel et al. 2023</a>)</span>. This bias helps the model learn that nearby positions often interact more strongly than distant ones while remaining agnostic about absolute position. The learned biases can capture genomic-specific distance preferences, such as the characteristic spacing of regulatory elements or the periodicity of nucleosome positioning.</p>
<p>Extrapolation to longer sequences than seen during training poses a persistent challenge for learned position embeddings. A model trained on 1-kilobase sequences may behave unpredictably when asked to process 10-kilobase sequences at inference, since the position embeddings for distant positions were never optimized. Attention with Linear Biases (<em>ALiBi</em>) addresses this limitation by adding a fixed linear penalty to attention scores based on distance, without learned parameters <span class="citation" data-cites="press_train_2022">(<a href="../bib/references.html#ref-press_train_2022" role="doc-biblioref">Press, Smith, and Lewis 2022</a>)</span>. For a head with slope <span class="math inline">\(m\)</span>, attention between positions separated by distance <span class="math inline">\(|i - j|\)</span> is penalized by <span class="math inline">\(m|i - j|\)</span>. Different heads use different slopes, encouraging some to focus locally and others globally. Because the linear penalty extrapolates naturally, <em>ALiBi</em> generalizes well beyond training lengths, making it attractive for genomic applications where sequence length varies dramatically. The linear distance penalty may not perfectly capture biological relationships (where some regulatory interactions span consistent long distances while others operate locally), but the simplicity and extrapolation properties have proven valuable.</p>
<p>An alternative approach encodes position through geometric transformation rather than additive bias. Rotary Position Embeddings (<em>RoPE</em>) multiply query and key vectors by rotation matrices whose angles depend on position <span class="citation" data-cites="su_roformer_2024">(<a href="../bib/references.html#ref-su_roformer_2024" role="doc-biblioref">Su et al. 2024</a>)</span>. The dot product between rotated query and key then depends on their relative distance, combining benefits of relative encoding with efficient implementation. <em>RoPE</em> has become standard in recent large language models and appears increasingly in genomic transformers, including variants of <em>Nucleotide Transformer</em>, offering a balance between the flexibility of learned embeddings and the extrapolation capability of fixed schemes. The choice between <em>ALiBi</em> and <em>RoPE</em> often depends on whether the application prioritizes aggressive length extrapolation (favoring <em>ALiBi</em>) or compatibility with pretrained language model architectures (favoring <em>RoPE</em>).</p>
<p>The following table summarizes the key tradeoffs among positional encoding approaches:</p>
<div id="tbl-position-encodings" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-position-encodings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.1: Comparison of positional encoding approaches for genomic transformers. Length extrapolation indicates whether the encoding generalizes to sequences longer than seen during training.
</figcaption>
<div aria-describedby="tbl-position-encodings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 29%">
<col style="width: 24%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Encoding Type</th>
<th style="text-align: left;">How Position is Encoded</th>
<th style="text-align: left;">Length Extrapolation</th>
<th style="text-align: left;">Genomic Models Using It</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Sinusoidal (fixed)</td>
<td style="text-align: left;">Sine/cosine at different frequencies</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">Original Transformer</td>
</tr>
<tr class="even">
<td style="text-align: left;">Learned absolute</td>
<td style="text-align: left;">Lookup table per position</td>
<td style="text-align: left;">Poor (hard limit)</td>
<td style="text-align: left;"><em>DNABERT</em>, <em>Nucleotide Transformer</em></td>
</tr>
<tr class="odd">
<td style="text-align: left;">T5-style relative</td>
<td style="text-align: left;">Learned bias by distance</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">Some genomic variants</td>
</tr>
<tr class="even">
<td style="text-align: left;">ALiBi</td>
<td style="text-align: left;">Linear attention penalty by distance</td>
<td style="text-align: left;">Excellent</td>
<td style="text-align: left;">Long-context models</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RoPE</td>
<td style="text-align: left;">Query/key rotation by position</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;"><em>NT-v2</em>, recent LLMs</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-ch07-genomic-position" class="level3" data-number="7.2.3">
<h3 data-number="7.2.3" class="anchored" data-anchor-id="sec-ch07-genomic-position"><span class="header-section-number">7.2.3</span> Genomic Position Considerations</h3>
<p>Genomic sequences impose requirements on positional encoding beyond what natural language demands. DNA has strand directionality: ACGT on the forward strand has different regulatory meaning than the same sequence on the reverse strand. Positional encodings should enable the model to learn strand-specific patterns. Some genomic transformers encode both strands separately and combine predictions; others rely on the model learning strand orientation from sequence content alone.</p>
<p>Genomic coordinates pose another design choice. Should position 1 correspond to a fixed genomic landmark (transcription start site, gene start) or simply indicate sequence order without biological reference? Models predicting regulatory activity often center sequences on promoters, using positions relative to the TSS. Foundation models trained on random genomic segments typically use positional encodings reflecting sequence order without genomic coordinate reference. The choice affects what the model can learn: TSS-relative positions enable learning of distance-dependent regulatory patterns, while sequence-order positions require the model to learn these patterns implicitly from content.</p>
<p>Absolute genomic coordinates carry accumulated knowledge with no linguistic analog. The position <em>chr17</em>:41,276,045 indexes decades of clinical observations, population frequencies, and functional studies that inform variant interpretation before a model processes a single nucleotide. Some recent models have explored incorporating these absolute coordinates, allowing models to learn position-specific patterns like centromeric sequences or telomeric regions. Circular genomes like mitochondrial DNA and bacterial chromosomes create additional complexity: they have no beginning or end, creating wraparound relationships that linear position encodings cannot naturally represent. These adaptations illustrate that position encoding is not merely a technical detail but a design choice shaping what biological patterns a model can capture.</p>
<p>The choice of positional encoding interacts with tokenization strategies (see <a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>). K-mer tokenization reduces sequence length (and thus attention cost) but changes what “position” means: position 1 might represent nucleotides 1 through 6 rather than a single base. A model using 6-mer tokens with learned positional embeddings learns different position-dependent patterns than one using single-nucleotide tokens, even if both cover the same genomic region.</p>
</section>
</section>
<section id="sec-ch07-transformer-block" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="sec-ch07-transformer-block"><span class="header-section-number">7.3</span> Transformer Block</h2>
<p>A clinician interpreting a <em>BRCA1</em> variant needs a model that does more than identify isolated motifs or single long-range interactions. The variant’s pathogenicity depends on how multiple regulatory signals integrate: local splice site grammar, enhancer contacts from 20 kilobases upstream, and transcription factor binding sites whose effects depend on chromatin context. Single attention layers identify pairwise relationships, but understanding complex regulatory logic requires building hierarchical representations where simple patterns combine into compound signals. This hierarchical integration emerges from stacking <strong>transformer blocks</strong>, the modular units that combine attention and nonlinear processing to build increasingly abstract representations through repeated application.</p>
<div id="fig-transformer-block" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/05-fig-transformer-block.svg" class="img-fluid figure-img"></p>
<figcaption>Transformer block architecture with pre-norm configuration</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.5: Transformer block architecture with pre-norm configuration. Input embeddings (dimension d) pass through layer normalization before multi-head self-attention, which computes interactions across all positions. A residual connection adds the input directly to the attention output, creating gradient highways for stable training. A second layer normalization precedes the position-wise feed-forward network, which expands representations to 4d dimensions, applies GELU nonlinearity, and projects back to d dimensions. Another residual connection completes the block. Stacking multiple blocks (inset) enables hierarchical representation learning, with early layers capturing local patterns and later layers integrating them into complex regulatory predictions.
</figcaption>
</figure>
</div>
<section id="sec-ch07-block-components" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="sec-ch07-block-components"><span class="header-section-number">7.3.1</span> Block Components</h3>
<p>Each transformer block accomplishes two distinct functions: enabling positions to share information across the sequence, and transforming that aggregated information through nonlinear processing. The multi-head self-attention layer handles global communication, allowing each position to gather information from the entire sequence. The <strong>position-wise feed-forward network</strong> processes each position independently, applying nonlinear transformations to the aggregated information. Separating these functions into distinct components allows each to be optimized independently and provides clear computational semantics: attention determines which positions are relevant to each other (the “what to consider” question), while the feed-forward network determines how to combine that information (the “what to conclude” question).</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Key Insight: Attention vs. Feed-Forward">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Attention vs.&nbsp;Feed-Forward
</div>
</div>
<div class="callout-body-container callout-body">
<p>The transformer block’s two-stage design reflects a fundamental separation of concerns:</p>
<ul>
<li><strong>Attention</strong>: “Which other positions should I consider?” (inter-position communication)</li>
<li><strong>Feed-forward</strong>: “Given what I’ve gathered, what should I conclude?” (per-position computation)</li>
</ul>
<p>This separation explains why transformers can be so effective: attention handles the variable-length, content-dependent routing problem, while feed-forward networks handle the fixed-size, position-local computation problem. Each component can be optimized for its specific role.</p>
</div>
</div>
<p>The feed-forward network consists of two linear transformations with a nonlinearity between them. Typically, this expands the dimension by a factor of four (from model dimension <em>d</em> to 4<em>d</em>), applies GELU or similar activation, then projects back to dimension <em>d</em>. This expansion allows processing through a high-dimensional nonlinear transformation before producing output for the next layer. The position-wise nature means each position is transformed identically but independently; cross-position information flows only through attention.</p>
<p>Deep transformer networks would be untrainable without mechanisms to stabilize gradient flow. <strong>Layer normalization</strong> addresses activation scale by normalizing across the feature dimension at each position, preventing the explosive growth or collapse of activations that would otherwise occur across dozens of layers. Two conventions exist for placement: post-norm applies normalization after each sublayer, while pre-norm applies it before. Pre-norm has become standard because it improves training stability for deep networks, though post-norm can achieve slightly better final performance with careful tuning <span class="citation" data-cites="xiong_layer_2020">(<a href="../bib/references.html#ref-xiong_layer_2020" role="doc-biblioref">Xiong et al. 2020</a>)</span>.</p>
<p>Normalization alone cannot solve the <strong>vanishing gradient</strong> problem that plagues deep networks. <strong>Residual connections</strong> provide the second essential stabilization mechanism by adding each sublayer’s input directly to its output, creating gradient highways that bypass the transformations within attention and feed-forward operations. These connections serve two critical functions. First, they allow gradients to flow directly through many layers without repeated transformation, enabling training of very deep networks. Second, they create an inductive bias toward incremental refinement: each layer makes small adjustments to the representation rather than constructing entirely new representations from scratch. For genomic models, this incremental refinement maps naturally onto biological interpretation, where early layers might identify motifs, middle layers might recognize motif combinations, and later layers might integrate these patterns into regulatory predictions.</p>
</section>
<section id="sec-ch07-depth" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="sec-ch07-depth"><span class="header-section-number">7.3.2</span> Information Flow and Depth</h3>
<p>The flow through a pre-norm transformer block proceeds as follows. Input <em>X</em> is normalized, processed by multi-head attention to produce <em>X’</em>, and added back via residual connection, yielding <em>X</em> + <em>X’</em>. This sum is normalized, passed through the feed-forward network to produce <em>X’’</em>, and added via another residual connection, yielding final output <em>X</em> + <em>X’</em> + <em>X’’</em>. Each layer thus adds refinements to the representation while preserving information from earlier processing.</p>
<p>Stacking depth determines how many times this refinement occurs. Shallow transformers (6 layers or fewer) are parameter-efficient but may lack capacity for complex hierarchical patterns. Deep transformers (12 to 24 layers) can learn sophisticated representations that capture how promoter elements, enhancer contacts, and chromatin state combine to determine expression. Most genomic transformers use 6 to 24 layers, varying by application. Models for short sequences (small RNAs, individual binding sites) might use fewer layers, while foundation models for long genomic contexts often use deeper stacks to build representations that integrate information across multiple biological scales.</p>
<p>The choice of depth balances capacity against trainability. Deeper networks learn more complex functions but are harder to optimize, prone to overfitting without sufficient data, and more expensive at training and inference. For genomic models, depth often correlates with the complexity of patterns being modeled. Simple motif recognition tasks might benefit more from wider layers (larger <em>d</em>) than deeper stacks, while tasks requiring hierarchical integration (understanding how promoter-enhancer-insulator relationships determine expression) may benefit from additional depth that builds increasingly abstract representations layer by layer.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Knowledge Check: Transformer Block Components">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check: Transformer Block Components
</div>
</div>
<div class="callout-body-container callout-body">
<p>Test your understanding of transformer block architecture:</p>
<ol type="1">
<li>What are the two main components of a transformer block, and what does each do?</li>
<li>Why do residual connections enable training of deep networks?</li>
<li>If a model has 12 layers with 8 attention heads each, how many different attention patterns can it learn?</li>
<li>What is the difference between pre-norm and post-norm layer placement?</li>
</ol>
<p>Answers: (1) Multi-head attention for inter-position communication; feed-forward network for per-position computation. (2) Residual connections create gradient highways that allow signals to flow directly through many layers without degradation. (3) 96 different attention patterns (12 layers x 8 heads). (4) Pre-norm normalizes before each sublayer (more stable training); post-norm normalizes after (potentially better final performance but harder to train).</p>
</div>
</div>
</section>
</section>
<section id="sec-ch07-scaling" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="sec-ch07-scaling"><span class="header-section-number">7.4</span> Scaling to Genomic Sequences</h2>
<p>A 52-year-old patient presents with unexplained cardiomyopathy, and whole-genome sequencing reveals a structural variant spanning 500 kilobases on chromosome 14, disrupting the <em>MYH7</em> locus and several upstream regulatory elements. The clinical team needs to assess whether this variant explains the patient’s phenotype. Standard transformers cannot help: the quadratic complexity of self-attention makes 500-kilobase contexts computationally intractable. This gap between clinical need and computational capability defines a central challenge for genomic AI. The attention mechanism enables long-range modeling in principle, but practical constraints on memory and computation limit what contexts can actually be processed. Effective application of transformers to genomics requires strategies for managing these constraints.</p>
<section id="sec-ch07-quadratic-barrier" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="sec-ch07-quadratic-barrier"><span class="header-section-number">7.4.1</span> Quadratic Barrier</h3>
<p>Computing all pairwise attention scores requires <span class="math inline">\(O(L^2)\)</span> operations, where <em>L</em> is sequence length. For a 10-kilobase sequence tokenized at single-nucleotide resolution, this means 100 million attention computations per layer. A 200-kilobase sequence requires 40 billion computations per layer. Memory requirements scale similarly because the attention matrix must be stored for backpropagation.</p>
<p>This scaling constraint directly limits what clinical questions transformers can address. The <em>HLA</em> region (critical for transplant matching and autoimmune disease risk in the approximately <span class="math inline">\(40{,}000\)</span> organ transplants performed annually in the United States) spans approximately <span class="math inline">\(4\)</span> megabases and contains the most polymorphic genes in the human genome. Modeling this region with standard self-attention would require <span class="math inline">\(16 \times 10^{12}\)</span> attention computations per layer, far exceeding practical limits. Structural variant detection often requires analyzing megabase-scale contexts to identify breakpoints and assess functional impact, yet these contexts remain computationally intractable for standard transformers. A patient with a suspected chromosomal translocation cannot benefit from transformer-based analysis when the relevant context exceeds computational capacity.</p>
</section>
<section id="sec-ch07-parameters" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="sec-ch07-parameters"><span class="header-section-number">7.4.2</span> Parameter Considerations</h3>
<p>The number of parameters a transformer can effectively utilize depends on both training data quantity and the complexity of patterns to be learned. Transformer parameters come primarily from two sources. Width (model dimension <em>d</em>) determines embedding and hidden state sizes; increasing width allows more complex pattern representation at each position but increases parameters quadratically because weight matrices scale as <em>d</em> × <em>d</em>. Depth (number of layers) determines how many refinement steps occur; increasing depth allows hierarchical abstractions through repeated processing but increases parameters linearly.</p>
<p>Scaling laws from natural language processing suggest performance improves smoothly with increased parameters, data, and compute <span class="citation" data-cites="kaplan_scaling_2020">(<a href="../bib/references.html#ref-kaplan_scaling_2020" role="doc-biblioref">Kaplan et al. 2020</a>)</span>. Similar principles apply to genomics, though optimal ratios may differ. Genomic sequences are less compressible than natural language: each nucleotide carries less predictable information than words in structured sentences. The entropy of DNA sequence is higher than English text, meaning more parameters may be needed to model the same sequence length. This asymmetry suggests genomic models might benefit relatively more from depth (more processing of high-entropy information) than from width (more dimensions per position when each position carries limited structure).</p>
<p>The architectural landscape of genomic foundation models reveals distinct design philosophies. Table 7.1 compares representative models across key architectural dimensions, illustrating how different approaches balance model capacity, context length, and computational efficiency.</p>
<div id="tbl-model-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.2: Architectural comparison of genomic foundation models. Width refers to model/hidden dimension; depth to number of layers. Context length is reported in base pairs (bp) where applicable. Models marked with dagger use non-transformer architectures.
</figcaption>
<div aria-describedby="tbl-model-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 20%">
<col style="width: 8%">
<col style="width: 15%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Architecture</th>
<th style="text-align: left;">Width (<em>d</em>)</th>
<th style="text-align: left;">Depth (Layers)</th>
<th style="text-align: left;">Heads</th>
<th style="text-align: left;">Parameters</th>
<th style="text-align: left;">Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><em>DNABERT</em></td>
<td style="text-align: left;">Encoder-only</td>
<td style="text-align: left;">768</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">110M</td>
<td style="text-align: left;">~0.5 kb</td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>DNABERT-2</em> <span class="citation" data-cites="zhou_dnabert2_2024">(<a href="../bib/references.html#ref-zhou_dnabert2_2024" role="doc-biblioref"><strong>zhou_dnabert2_2024?</strong></a>)</span></td>
<td style="text-align: left;">Encoder-only</td>
<td style="text-align: left;">768</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">117M</td>
<td style="text-align: left;">Variable</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Nucleotide Transformer</em> (500M)</td>
<td style="text-align: left;">Encoder-only</td>
<td style="text-align: left;">1,024</td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">500M</td>
<td style="text-align: left;">6 kb</td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Nucleotide Transformer</em> (2.5B)</td>
<td style="text-align: left;">Encoder-only</td>
<td style="text-align: left;">2,560</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">2.5B</td>
<td style="text-align: left;">6 kb</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>NT-v2</em> (250M)</td>
<td style="text-align: left;">Encoder-only</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">250M</td>
<td style="text-align: left;">12 kb</td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>HyenaDNA</em></td>
<td style="text-align: left;">Hyena stack</td>
<td style="text-align: left;">128–256</td>
<td style="text-align: left;">2–8</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">0.4M–6.6M</td>
<td style="text-align: left;">up to 1M bp</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Caduceus</em></td>
<td style="text-align: left;">BiMamba</td>
<td style="text-align: left;">256</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">~7M</td>
<td style="text-align: left;">131 kb</td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Evo 2</em> (7B)</td>
<td style="text-align: left;">StripedHyena 2</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">1M bp</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Evo 2</em> (40B)</td>
<td style="text-align: left;">StripedHyena 2</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">40B</td>
<td style="text-align: left;">1M bp</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Several patterns emerge from this comparison. First, traditional transformer-based models like <em>DNABERT</em> and <em>Nucleotide Transformer</em> inherit architectures closely resembling their NLP counterparts, with <em>DNABERT</em> using the same 12-layer, 768-dimension configuration as BERT-base <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. The <em>Nucleotide Transformer</em> family scales this approach, with the 500M variant using 24 layers and 20 attention heads, and the 2.5B variant expanding to 32 layers while maintaining the same head count <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. This scaling primarily increases width (hidden dimension grows from 1,024 to 2,560) rather than dramatically increasing depth, following conventional transformer scaling practices.</p>
<p>Second, models designed for long-range genomic modeling adopt fundamentally different architectures to circumvent attention’s quadratic complexity. <em>HyenaDNA</em> replaces attention entirely with implicit long convolutions, achieving million-base-pair contexts with models containing only 2 to 8 layers and a few million parameters <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. <em>Caduceus</em> extends the Mamba state-space architecture with bidirectional processing and reverse-complement equivariance, using 16 layers to achieve 131 kb context <span class="citation" data-cites="schiff_caduceus_2024">(<a href="../bib/references.html#ref-schiff_caduceus_2024" role="doc-biblioref">Schiff et al. 2024</a>)</span>. <em>Evo 2</em> represents the current frontier, with 7B and 40B parameter variants achieving million-token context through the StripedHyena 2 architecture <span class="citation" data-cites="brixi_evo_2025">(<a href="../bib/references.html#ref-brixi_evo_2025" role="doc-biblioref">Brixi et al. 2025</a>)</span>. These non-transformer approaches demonstrate that architectural innovation can achieve genomic-scale context lengths that remain computationally intractable for standard attention mechanisms.</p>
<p>Third, the relationship between parameter count and context length is not straightforward. <em>HyenaDNA</em> achieves the longest context among early models (1 million bp) with fewer than 7 million parameters, while <em>Nucleotide Transformer</em> 2.5B processes only 6 kb despite 400 times more parameters. This inversion reflects a fundamental tradeoff: dense attention captures rich pairwise interactions but scales poorly with sequence length, while subquadratic alternatives like Hyena operators and state-space models sacrifice some interaction modeling capacity for computational tractability. The optimal choice depends on whether the task requires capturing dense local interactions or sparse long-range dependencies.</p>
<p>The relationship between parameter count and downstream task performance is not always monotonic: a well-trained smaller model can outperform a poorly trained larger one, and task-specific fine-tuning often matters more than pretraining scale for focused clinical applications. <em>Nucleotide Transformer</em> v2 demonstrated this principle dramatically, achieving comparable or superior performance to the 2.5B-parameter v1 models with only 250M parameters by incorporating architectural improvements and training for more tokens <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. Similarly, <em>HyenaDNA</em> achieves state-of-the-art results on 12 of 18 benchmarks from the <em>Nucleotide Transformer</em> suite while using 1,500 times fewer parameters. These results suggest that architectural efficiency and training strategy may matter as much as raw parameter count for genomic applications.</p>
</section>
<section id="sec-ch07-context-strategies" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="sec-ch07-context-strategies"><span class="header-section-number">7.4.3</span> Context Length Strategies</h3>
<p>Standard self-attention’s <span class="math inline">\(O(L^2)\)</span> complexity becomes prohibitive for long genomic contexts, forcing architectural choices that trade expressiveness for tractability. The strategies employed reflect different assumptions about which interactions matter most for genomic modeling.</p>
<p>The quadratic complexity of full attention becomes prohibitive for genomic sequences, but different applications tolerate different trade-offs between efficiency and expressiveness. When most relevant interactions are local (as often holds for regulatory sequences where nearby elements interact more strongly than distant ones), restricting attention to fixed windows reduces complexity to <span class="math inline">\(O(Lw)\)</span> where <em>w</em> is window size. For clinical variant interpretation in coding sequences, where splice sites and reading frame context typically lie within a few hundred bases, local attention may capture the relevant biology. The trade-off is missing long-range interactions that fall outside windows, potentially critical for understanding distal enhancer effects or structural variant consequences.</p>
<p>Hierarchical approaches recover some long-range capability while maintaining efficiency. Lower layers can use local windows to capture fine-grained patterns, while upper layers attend to every <span class="math inline">\(k\)</span>-th position to capture global structure. Hybrid models like <em>Enformer</em> apply CNNs to downsample sequences before transformer layers, reducing the effective sequence length that attention must handle <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. A 200-kilobase genomic region might be compressed to roughly 1,500 positions after CNN processing, making full attention tractable at the cost of single-nucleotide resolution in transformer layers.</p>
<p>Mathematical approximations offer yet another path, preserving dense connectivity while reducing computational cost. <em>Linformer</em> approximates the attention matrix through low-rank decomposition, reducing complexity to linear in sequence length <span class="citation" data-cites="wang_linformer_2020">(<a href="../bib/references.html#ref-wang_linformer_2020" role="doc-biblioref">Wang et al. 2020</a>)</span>. <em>Performer</em> uses random feature methods to approximate attention scores without explicitly computing the full <span class="math inline">\(L\timesL\)</span> matrix <span class="citation" data-cites="choromanski_rethinking_2022">(<a href="../bib/references.html#ref-choromanski_rethinking_2022" role="doc-biblioref">Choromanski et al. 2022</a>)</span>. These approximations trade some expressiveness for efficiency and may miss certain long-range dependencies that low-rank structure cannot capture. The choice among sparse patterns, hierarchical designs, and mathematical approximations depends on whether the target biology demands single-nucleotide resolution, long-range connectivity, or both.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Practical Guidance: Choosing Context Length Strategies">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Choosing Context Length Strategies
</div>
</div>
<div class="callout-body-container callout-body">
<p>When selecting a context length strategy for a genomic application, consider:</p>
<p><strong>Use local/windowed attention when:</strong> - The biological signal is primarily local (splice sites, TF binding sites) - Single-nucleotide resolution is critical - Training data is limited</p>
<p><strong>Use hierarchical/hybrid approaches when:</strong> - You need both local detail and long-range context - The regulatory architecture involves enhancer-promoter interactions - You can tolerate some loss of fine-grained resolution</p>
<p><strong>Use subquadratic architectures (Hyena, Mamba) when:</strong> - Context length is the primary constraint (&gt;100 kb) - You need single-nucleotide resolution over long ranges - You’re willing to adopt newer, less mature architectures</p>
</div>
</div>
</section>
<section id="sec-ch07-memory" class="level3" data-number="7.4.4">
<h3 data-number="7.4.4" class="anchored" data-anchor-id="sec-ch07-memory"><span class="header-section-number">7.4.4</span> Memory and Precision</h3>
<p>Memory requirements compound computational challenges for genomic transformers. Training requires storing activations for backpropagation, and attention matrices are particularly memory-intensive. A <span class="math inline">\(100\)</span>-kilobase sequence with <span class="math inline">\(16\)</span> attention heads and <span class="math inline">\(12\)</span> layers requires storing <span class="math inline">\(16 \times 12 \times 100{,}000 \times 100{,}000\)</span> attention weights, approximately <span class="math inline">\(2\)</span> terabytes at <span class="math inline">\(32\)</span>-bit precision before considering other activations.</p>
<p>Memory constraints often limit model size and sequence length more than computational budget. Recomputing activations during the backward pass rather than storing them (a technique called <strong>gradient checkpointing</strong>) trades additional computation for reduced memory footprint, enabling training of larger models or longer sequences on fixed hardware at the cost of 20 to 30 percent additional training time. Further memory savings come from reducing numerical precision: using 16-bit floating point for most computations while maintaining 32-bit precision for critical operations like loss computation and optimizer updates. Modern GPUs accelerate 16-bit arithmetic substantially, providing near 2x speedup with minimal precision loss. <em>Flash Attention</em> implements memory-efficient attention computation that avoids materializing the full attention matrix, enabling longer contexts within fixed memory budgets <span class="citation" data-cites="dao_flashattention_2022">(<a href="../bib/references.html#ref-dao_flashattention_2022" role="doc-biblioref">Dao et al. 2022</a>)</span>. Together, these optimizations determine the practical limits of what can be trained on available hardware.</p>
</section>
</section>
<section id="sec-ch07-variants" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="sec-ch07-variants"><span class="header-section-number">7.5</span> Architectural Variants for Genomics</h2>
<p>Foundation model architectures encode assumptions about what matters. A model optimized for scoring existing variants differs fundamentally from one designed to generate novel sequences, which differs again from one predicting molecular interactions. The choice of architecture shapes what questions can be asked.</p>
<div id="fig-encoder-decoder" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encoder-decoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/06-A-fig-encoder-decoder.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER A</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/06-B-fig-encoder-decoder.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER B</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/06-C-fig-encoder-decoder.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER C</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encoder-decoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.6: [Enhancing] Three-panel comparison. Panel A (Encoder-only, e.g., <em>BERT</em>/<em>DNABERT</em>): Bidirectional attention pattern (full matrix), typical use for classification/embedding. Panel B (Decoder-only, e.g., GPT/<em>Evo</em>): Causal attention pattern (lower triangular), typical use for generation. Panel C (Hybrid CNN-Transformer, e.g., <em>Enformer</em>): CNN downsampling followed by transformer, showing how hybrid reduces sequence length before attention.
</figcaption>
</figure>
</div>
<section id="sec-ch07-encoder-only" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="sec-ch07-encoder-only"><span class="header-section-number">7.5.1</span> Encoder-Only Transformers</h3>
<p>When a clinical laboratory queries a pathogenicity database for a novel missense variant, they need a model that integrates information from the entire protein sequence: upstream domains that establish structural context, downstream regions that complete functional units, and evolutionary patterns that distinguish tolerated from deleterious changes. <strong>Encoder-only transformers</strong> process sequences bidirectionally, allowing each position to attend to all other positions including those that follow in the sequence. This bidirectional context produces richer representations than unidirectional processing because each position’s representation incorporates information from the entire sequence.</p>
<p><em>DNABERT</em> exemplifies this architecture, trained with <strong>masked language modeling</strong> objectives where random tokens are masked and predicted from bidirectional context <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. The model learns to predict held-out k-mers based on surrounding sequence, implicitly learning sequence patterns and constraints that transfer to downstream tasks. <em>Nucleotide Transformer</em> follows similar principles at larger scale <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. These models excel at representation learning: producing embeddings that capture biological properties useful for variant effect prediction, function classification, or other tasks that require fixed-length representations of variable-length sequences.</p>
<p>Bidirectional attention suits tasks where both upstream and downstream context matters for understanding a position. Transcription factor binding depends on flanking sequence in both directions. Splice site recognition requires seeing both exonic and intronic context. Variant pathogenicity may depend on protein domain context from both N-terminal and C-terminal directions. The limitation is that encoder-only architectures cannot generate sequences autoregressively because they require seeing the full sequence to produce any output; they answer “what does this sequence mean” rather than “what sequence should come next.”</p>
</section>
<section id="sec-ch07-decoder-only" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="sec-ch07-decoder-only"><span class="header-section-number">7.5.2</span> Decoder-Only Transformers</h3>
<p>Generating synthetic genomic sequences for therapeutic design, creating diverse antibody libraries for drug discovery, or sampling from learned regulatory grammars all require models that produce sequences one token at a time. <strong>Decoder-only transformers</strong> use <strong>causal attention</strong> where each position attends only to itself and preceding positions. This structure enables <strong>autoregressive generation</strong>: the model produces sequences one token at a time, conditioning each new token on all previous tokens.</p>
<p><em>GenSLM</em> applies this architecture to genomic data, training on next-token prediction to learn sequence distributions <span class="citation" data-cites="zvyagin_genslms_2022">(<a href="../bib/references.html#ref-zvyagin_genslms_2022" role="doc-biblioref">Zvyagin et al. 2022</a>)</span>. The model learns to predict the next nucleotide or <span class="math inline">\(k\)</span>-mer given all preceding context, implicitly learning the statistical regularities of genomic sequence. This objective aligns naturally with generation tasks: sampling proceeds by repeatedly predicting the next token and appending it to the sequence. Causal attention is essential for generation because the model must produce each position before it can condition subsequent positions on that output.</p>
<p>The trade-off is that causal attention produces less rich representations for fixed sequences because each position sees only partial context. Position 100 in a 1000-position sequence has access to only the first 100 positions, not the remaining 900 that might provide relevant information. For variant effect prediction where downstream context matters, this limitation can be substantial. The choice between encoder and decoder architectures reflects a fundamental tension: representation learning benefits from bidirectional context, while generation requires causal structure.</p>
</section>
<section id="sec-ch07-encoder-decoder" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="sec-ch07-encoder-decoder"><span class="header-section-number">7.5.3</span> Encoder-Decoder Transformers</h3>
<p>Some genomic tasks require transforming one sequence into another of different length or structure. Predicting protein sequence from coding DNA, generating variant descriptions from sequence context, or translating between sequence representations all involve input-output relationships that neither pure encoder nor pure decoder architectures handle naturally. <strong>Encoder-decoder architectures</strong> combine bidirectional encoding with autoregressive decoding <span class="citation" data-cites="vaswani_attention_2017">(<a href="../bib/references.html#ref-vaswani_attention_2017" role="doc-biblioref">Vaswani et al. 2023</a>)</span>.</p>
<p>The encoder processes an input sequence with full bidirectional attention, producing contextualized representations. The decoder then generates output tokens autoregressively, attending both to its own previous outputs (through causal self-attention) and to encoder representations (through <strong>cross-attention</strong>). This cross-attention allows each decoder position to query the full encoded input when generating output, combining the benefits of bidirectional understanding with autoregressive generation.</p>
<p>Encoder-decoder models are less common in genomic applications than encoder-only or decoder-only variants because most genomic tasks either need representations (favoring encoders) or generation (favoring decoders), not both simultaneously. Machine translation exemplifies the encoder-decoder use case: encode a sentence in one language, decode into another. Genomic analogs might include predicting protein sequences from codon-optimized DNA or generating clinical variant reports from sequence features, but these applications remain less developed than pure representation or generation tasks.</p>
<p>The following table summarizes when to use each architectural variant:</p>
<div id="tbl-architecture-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-architecture-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.3: Architectural variants for genomic transformers and their optimal use cases. The attention pattern determines what context each position can access during computation.
</figcaption>
<div aria-describedby="tbl-architecture-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Architecture</th>
<th style="text-align: left;">Attention Pattern</th>
<th style="text-align: left;">Optimal Use Cases</th>
<th style="text-align: left;">Example Models</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Encoder-only</td>
<td style="text-align: left;">Bidirectional (full)</td>
<td style="text-align: left;">Variant effect prediction, sequence classification, embedding extraction</td>
<td style="text-align: left;"><em>DNABERT</em>, <em>Nucleotide Transformer</em>, <em>ESM-2</em></td>
</tr>
<tr class="even">
<td style="text-align: left;">Decoder-only</td>
<td style="text-align: left;">Causal (triangular)</td>
<td style="text-align: left;">Sequence generation, design, sampling from learned distributions</td>
<td style="text-align: left;"><em>GenSLM</em>, <em>Evo</em>, GPT-style models</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Encoder-decoder</td>
<td style="text-align: left;">Bidirectional + causal</td>
<td style="text-align: left;">Sequence-to-sequence tasks, translation, structured output</td>
<td style="text-align: left;">Machine translation models</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hybrid (CNN + transformer)</td>
<td style="text-align: left;">Hierarchical</td>
<td style="text-align: left;">Long-context regulatory prediction with downsampling</td>
<td style="text-align: left;"><em>Enformer</em>, <em>Borzoi</em></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-ch07-hybrid" class="level3" data-number="7.5.4">
<h3 data-number="7.5.4" class="anchored" data-anchor-id="sec-ch07-hybrid"><span class="header-section-number">7.5.4</span> Hybrid CNN-Transformer Models</h3>
<p>The most successful genomic transformers combine convolutional and attention mechanisms rather than using transformers alone. This hybrid approach exploits CNNs’ efficiency for local pattern extraction while using transformers for long-range integration, matching the multi-scale structure of genomic regulation where both local motifs and distal interactions determine function.</p>
<p><em>Enformer</em> and <em>Borzoi</em> apply convolutional stems to long sequences, downsampling through pooling, then pass compressed representations through transformer layers <span class="citation" data-cites="avsec_enformer_2021 linder_borzoi_2023">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>; <a href="../bib/references.html#ref-linder_borzoi_2023" role="doc-biblioref"><strong>linder_borzoi_2023?</strong></a>)</span>. The CNN layers handle motif recognition, nucleosome positioning signals, and local chromatin features with parameter efficiency that pure transformers cannot match. Transformer layers then integrate across the broader regulatory landscape, learning enhancer-promoter relationships and TAD boundary effects. This division of labor achieves state-of-the-art performance on regulatory prediction tasks while remaining computationally tractable for 200-kilobase contexts. The regulatory sequence models in <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a> examine <em>Enformer</em> and <em>Borzoi</em> comprehensively, including their applications to variant effect prediction and expression modeling.</p>
<p>The hybrid approach also addresses the quadratic attention bottleneck indirectly. By downsampling sequences before transformer layers (often by factors of 128 or more), hybrids reduce effective sequence length and thus attention cost. A 200-kilobase genomic region compressed to 1,500 positions requires only 2.25 million attention computations per layer rather than 40 billion for the uncompressed sequence. The cost is loss of single-nucleotide resolution in transformer layers, though the CNN stem preserves local detail that attention layers integrate but do not need to resolve. <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a> examines <em>Enformer</em> and related hybrid architectures in detail.</p>
</section>
</section>
<section id="sec-ch07-training" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="sec-ch07-training"><span class="header-section-number">7.6</span> Training Dynamics</h2>
<p>When a model trained to predict pathogenic variants misclassifies a disease-causing <em>BRCA1</em> mutation as benign, the consequences extend beyond benchmark metrics. Clinical laboratories may return incorrect results; patients may forego preventive surgeries that could save their lives. Training failures matter clinically because they determine what models learn and what they miss. The evaluation methodology in <a href="p2-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 12</span></a> examines how to detect such failures before clinical deployment, while <a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 13</span></a> addresses systematic biases that cause models to fail on clinically important subgroups. A model that overfits to common polymorphisms in training data will fail on the rare variants that matter most for diagnosis. A model whose gradients vanish during training will never learn the subtle regulatory patterns that distinguish pathogenic from benign promoter variants. Understanding training dynamics helps predict and prevent these failures.</p>
<section id="sec-ch07-optimization" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="sec-ch07-optimization"><span class="header-section-number">7.6.1</span> Optimization</h3>
<p>Genomic transformers inherit their training foundations from natural language processing but require adjustments for biological data. The Adam optimizer and its variant <em>AdamW</em> remain standard, using adaptive learning rates that maintain per-parameter estimates adjusted based on gradient statistics <span class="citation" data-cites="loshchilov_decoupled_2019">(<a href="../bib/references.html#ref-loshchilov_decoupled_2019" role="doc-biblioref">Loshchilov and Hutter 2019</a>)</span>. <em>AdamW</em> applies weight decay directly to parameter updates rather than to the loss function, improving generalization and training stability.</p>
<p><strong>Learning rate schedules</strong> typically use warmup (linearly increasing learning rate from near-zero to peak over the first several thousand steps) followed by decay (linear or cosine decrease over the remaining training). Warmup addresses a specific instability: transformers with random initialization can produce extreme gradients early in training, and adaptive optimizers need time to build accurate gradient statistics. Warmup allows the optimizer to stabilize before applying full learning rates. Skipping warmup often causes training collapse in the first few hundred steps, manifesting as loss spikes or NaN values.</p>
<p>For genomic data, learning rate tuning may require adjustment from NLP defaults. Regulatory sequences with highly conserved motifs (TATA boxes, splice site dinucleotides) create strong signals that models can overfit quickly; lower learning rates may prevent latching onto these patterns before learning subtler regulatory grammar. Protein sequences exhibit weaker positional conservation than regulatory DNA, potentially benefiting from higher rates that encourage broader exploration of the loss landscape. Empirically, genomic transformers often use peak learning rates of 1e-4 to 5e-4 <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>, somewhat lower than the 1e-3 to 3e-3 common in language modeling.</p>
</section>
<section id="sec-ch07-regularization" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="sec-ch07-regularization"><span class="header-section-number">7.6.2</span> Regularization</h3>
<p>Regularization prevents overfitting, particularly important when training data is limited relative to model size. Genomic datasets, while growing rapidly, remain smaller than the trillion-token corpora used for large language models. A model with 100 million parameters trained on 10 billion nucleotides faces different overfitting risks than one trained on 1 trillion tokens.</p>
<p>Transformers’ high parameter counts create substantial overfitting risk, particularly for genomic applications where labeled training data may be limited. Two complementary regularization strategies have proven essential. Randomly zeroing activations during training (<strong>dropout</strong>) forces the network to learn robust features that remain informative even when some information pathways are blocked. Applying dropout to attention weights specifically prevents over-reliance on particular position pairs, encouraging distributed representations. Genomic transformers, like DNABERT, use standard dropout rates of 0.1 to 0.2.</p>
<p>Constraining parameter magnitudes provides orthogonal regularization. <strong>Weight decay</strong> penalizes large parameter values, encouraging smaller weights that generalize better to unseen data. For transformers, this penalty is typically applied to all parameters except biases and layer normalization parameters. The coefficient requires careful tuning: too little provides insufficient regularization; too much constrains capacity and reduces model expressiveness. Values of 0.01 to 0.1 are common, with higher values for smaller datasets where overfitting risk is greater. The interaction between dropout and weight decay means that optimal settings for each depend on the other, requiring joint tuning rather than independent optimization.</p>
</section>
<section id="sec-ch07-gradients" class="level3" data-number="7.6.3">
<h3 data-number="7.6.3" class="anchored" data-anchor-id="sec-ch07-gradients"><span class="header-section-number">7.6.3</span> Gradient Stability</h3>
<p>Gradient issues plague deep network training and require specific attention for genomic transformers. Vanishing gradients occur when gradients become extremely small through many layers, preventing learning in early layers. <strong>Exploding gradients</strong> are the opposite: gradients grow exponentially and destabilize training. Transformers mitigate vanishing gradients through residual connections that provide direct gradient paths, allowing gradients to flow from output to early layers without passing through potentially attenuating transformations. Exploding gradients are addressed through <strong>gradient clipping</strong>, which rescales gradients when their norm exceeds a threshold.</p>
<p>For genomic transformers, gradient issues manifest differently than in language models. Natural language has nested grammatical organization (words form phrases, phrases form clauses, clauses form sentences), and this hierarchy creates structural landmarks that concentrate attention on syntactically meaningful positions. Genomic sequences lack equivalent organization: functional elements like promoters, splice sites, and enhancers are scattered without consistent positional relationships, so attention patterns must be learned entirely from data without structural priors to guide gradient flow. This absence of hierarchy interacts with imbalanced token frequencies to compound the problem. Common k-mers receive large gradients from frequent occurrence while rare but biologically important tokens (such as k-mers containing the stop codon TAG) receive small gradients from infrequent appearance. In language, frequent function words are grammatically constrained in their attention patterns; in genomic sequences, common k-mers have no such constraints, allowing their gradients to dominate through frequency alone rather than biological importance. Addressing these imbalances may require loss reweighting or adaptive sampling that ensures rare tokens appear frequently enough for effective learning.</p>
</section>
<section id="sec-ch07-distributed" class="level3" data-number="7.6.4">
<h3 data-number="7.6.4" class="anchored" data-anchor-id="sec-ch07-distributed"><span class="header-section-number">7.6.4</span> Distributed Training</h3>
<p>The computational scale of genomic foundation models typically exceeds single-graphics processing unit (GPU) capacity, requiring distributed training strategies. <strong>Data parallelism</strong> replicates the model across GPUs, splitting batches across devices and aggregating gradients. This approach scales well up to batch sizes limited by convergence requirements but does not help when the model itself exceeds GPU memory. <strong>Model parallelism</strong> splits the model across devices, necessary when parameters exceed single-GPU memory. <strong>Pipeline parallelism</strong> divides layers across devices and pipelines forward and backward passes, interleaving computation to improve device utilization.</p>
<p>Batch size selection involves competing considerations. Larger batches provide more stable gradient estimates and better GPU utilization but require more memory and may reduce generalization. Genomic transformers often use <strong>gradient accumulation</strong> to simulate large batches: small batches process sequentially, gradients accumulate, then a single parameter update occurs. This strategy provides large-batch gradient stability without the memory cost, though it increases training time proportionally. Effective batch sizes of 256 to 4096 sequences are common for genomic transformers, achieved through accumulation over many smaller physical batches (e.g., Nucleotide Transformer’s 2.5B model accumulated gradients from physical batches of just 2 sequences to reach 1 million tokens per update <span class="citation" data-cites="dallatorre_nucleotide_2024">(<a href="../bib/references.html#ref-dallatorre_nucleotide_2024" role="doc-biblioref"><strong>dallatorre_nucleotide_2024?</strong></a>)</span>).</p>
</section>
</section>
<section id="sec-ch07-limitations" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="sec-ch07-limitations"><span class="header-section-number">7.7</span> Limitations and Emerging Alternatives</h2>
<p>A 48-year-old patient presents with a suspected Lynch syndrome diagnosis, and genetic testing reveals a structural variant spanning 3 megabases on chromosome 2 that may disrupt the <em>MSH2</em> gene and its upstream regulatory region. The clinical team needs to determine whether this variant explains the patient’s early-onset colorectal cancer and guides surveillance recommendations for family members. Standard transformers cannot address this question: the quadratic complexity of self-attention makes 3-megabase contexts computationally intractable. Current models can span 200 kilobases with hybrid architectures, yet the structural variants and chromosomal rearrangements that cause many inherited cancer syndromes remain beyond reach. This gap between clinical need and computational capability defines the frontier of genomic AI.</p>
<div id="fig-quadratic-ceiling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quadratic-ceiling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch07/07-fig-quadratic-ceiling.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER</strong></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quadratic-ceiling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.7: [High] Log-log plot showing computational cost (FLOPs or memory) vs.&nbsp;sequence length for different architectures. Show: Standard attention (quadratic curve, <span class="math inline">\(O(L^2)\)</span>), sparse/local attention (sub-quadratic), state space models like <em>Hyena</em>/<em>Mamba</em> (linear). Annotate biologically relevant context lengths: promoter (<span class="math inline">\(\sim 1\ \mathrm{kb}\)</span>), gene (<span class="math inline">\(\sim 10\ \mathrm{kb}\)</span>), enhancer-promoter (<span class="math inline">\(\sim 100\ \mathrm{kb}\)</span>), TAD (<span class="math inline">\(\sim 1\ \mathrm{Mb}\)</span>), chromosome arm (<span class="math inline">\(\sim 100\ \mathrm{Mb}\)</span>). Draw vertical lines at each scale showing which architectures are tractable.
</figcaption>
</figure>
</div>
<section id="sec-ch07-quadratic-ceiling" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="sec-ch07-quadratic-ceiling"><span class="header-section-number">7.7.1</span> Quadratic Ceiling</h3>
<p>The quadratic complexity of self-attention remains transformers’ most severe limitation for genomics. Computing all pairwise attention scores requires <span class="math inline">\(O(L^2)\)</span> operations and memory. For genomic contexts exceeding <span class="math inline">\(100\)</span> kilobases (roughly <span class="math inline">\(100{,}000\)</span> single-nucleotide tokens), this becomes prohibitive. Even with sparse approximations and efficient implementations, transformers struggle at megabase scales where many regulatory interactions occur and structural variants manifest their effects.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Big-O Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Big-O Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Big-O notation describes how computational cost scales with input size. For genomic models, the input size L is typically sequence length. When we say attention has <span class="math inline">\(O(L^2)\)</span> complexity, we mean that doubling the sequence length quadruples the computation: a 10,000 bp sequence requires 100 million pairwise comparisons. Sub-quadratic approaches like Hyena’s <span class="math inline">\(O(L \log L)\)</span> scale far more gently: doubling the sequence length only slightly more than doubles the cost. For chromosome-scale sequences of millions of base pairs, this difference determines whether analysis is feasible at all.</p>
</div>
</div>
<p>Recent models have pushed context lengths substantially. <em>Enformer</em> handles 200 kilobases; emerging models approach 1 megabase. But these achievements rely on hybrid architectures with significant downsampling or hierarchical windowing that may miss certain long-range patterns or single-nucleotide resolution details. Pure transformers without such modifications remain limited to shorter contexts. The fundamental constraint shapes what questions transformers can address and motivates alternatives that escape quadratic scaling.</p>
</section>
<section id="sec-ch07-ssm" class="level3" data-number="7.7.2">
<h3 data-number="7.7.2" class="anchored" data-anchor-id="sec-ch07-ssm"><span class="header-section-number">7.7.2</span> State Space Models</h3>
<p><strong>State space models (SSMs)</strong> address the quadratic barrier directly by achieving linear complexity while maintaining long-range modeling capability. Rather than computing all pairwise interactions, SSMs represent sequences as continuous-time dynamical systems, maintaining memory through recurrent state updates that propagate information across positions without explicit pairwise computation.</p>
<p>Architectures like <em>S4</em>, <em>Hyena</em>, and <em>Mamba</em> have demonstrated competitive or superior performance to transformers on various sequence modeling tasks while scaling to much longer contexts <span class="citation" data-cites="gu_efficiently_2022 poli_hyena_2023 gu_mamba_2024">(<a href="../bib/references.html#ref-gu_efficiently_2022" role="doc-biblioref">Gu et al. 2022</a>; <a href="../bib/references.html#ref-poli_hyena_2023" role="doc-biblioref">Poli et al. 2023</a>; <a href="../bib/references.html#ref-gu_mamba_2024" role="doc-biblioref">Gu and Dao 2024</a>)</span>. For genomics, this capability enables whole-chromosome or potentially whole-genome modeling that remains intractable for standard transformers. <em>HyenaDNA</em> processes sequences up to 1 million nucleotides at single-nucleotide resolution, enabling analysis of structural variants and long-range regulatory interactions that transformers cannot approach <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. The <em>Evo</em> model extends this further, achieving context lengths sufficient for bacterial genome-scale modeling <span class="citation" data-cites="nguyen_sequence_2024">(<a href="../bib/references.html#ref-nguyen_sequence_2024" role="doc-biblioref">Nguyen et al. 2024</a>)</span>. The DNA language model architectures in <a href="../part_3/p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a> examine <em>HyenaDNA</em>, <em>Caduceus</em>, and <em>Evo</em> in detail, exploring how linear complexity enables new categories of genomic analysis including sequence design applications (<a href="../part_6/p6-ch30-design.html" class="quarto-xref"><span>Chapter 30</span></a>).</p>
</section>
<section id="sec-ch07-choosing" class="level3" data-number="7.7.3">
<h3 data-number="7.7.3" class="anchored" data-anchor-id="sec-ch07-choosing"><span class="header-section-number">7.7.3</span> Choosing Architectures</h3>
<p>The choice between transformers and alternatives depends on the biological question and computational constraints. Transformers excel when global context matters but sequences are not extremely long (under 10 to 50 kilobases depending on computational resources). Attention maps provide interpretability, showing which positions the model considers relevant for predictions. Transformers benefit from extensive tooling and pretrained models from NLP that transfer readily to genomics.</p>
<p>CNNs remain preferable when computational efficiency is paramount and local patterns dominate. For splice site prediction or promoter classification where relevant context spans at most a few hundred base pairs, a well-designed CNN may outperform transformers while using far fewer parameters. The inductive bias toward local patterns also regularizes against overfitting when training data is limited. The convolutional architectures examined in <a href="p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> established these design principles.</p>
<p>Hybrid approaches often achieve the best practical results for intermediate-scale problems. Models combining CNNs for local feature extraction with transformers for long-range integration outperform pure architectures on regulatory prediction tasks, as <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a> demonstrates with <em>Enformer</em> and related models. The optimal combination depends on the specific biological question and the scale of relevant interactions.</p>
<p>The transition toward sub-quadratic architectures continues. Early results suggest SSMs match or exceed transformers on some genomic benchmarks while scaling to longer contexts. The question is no longer whether alternatives to quadratic attention exist, but which tasks benefit most from linear-complexity architectures and which retain advantages from explicit pairwise attention computation.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Stop and Think: Architecture Selection">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think: Architecture Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p>You are developing a model for a specific genomic task. For each scenario below, which architecture would you choose and why?</p>
<ol type="1">
<li><p><strong>Predicting pathogenicity of missense variants in a 300-amino-acid protein domain.</strong> Context is ~900 bp, and you need rich bidirectional representations.</p></li>
<li><p><strong>Generating novel regulatory sequences that drive tissue-specific expression.</strong> You need to sample from a learned distribution of functional sequences.</p></li>
<li><p><strong>Predicting the effects of a 2 Mb structural variant on nearby gene expression.</strong> The variant spans multiple genes and regulatory elements.</p></li>
<li><p><strong>Real-time variant annotation in a clinical setting</strong> where inference speed matters and context is limited to the immediate gene region.</p></li>
</ol>
<p>Consider the tradeoffs between context length, computational cost, generation vs.&nbsp;representation, and interpretability for each case.</p>
</div>
</div>
</section>
</section>
<section id="sec-ch07-conclusion" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="sec-ch07-conclusion"><span class="header-section-number">7.8</span> Capacity Without Direction</h2>
<p>The transformer architecture provides the computational substrate for modern genomic foundation models, but architecture alone does not determine what models learn. Attention mechanisms enable pairwise interaction modeling across arbitrary sequence distances. Position encodings break permutation invariance to preserve the sequential structure essential to regulatory grammar. Stacked blocks build hierarchical representations through iterative refinement. These components create capacity; training objectives and data determine how that capacity is used.</p>
<p>Self-supervised pretraining transforms architectural capacity into biological knowledge. Masked language modeling teaches models to predict held-out tokens from context, implicitly learning the sequence patterns and evolutionary constraints that determine biological function. Next-token prediction in autoregressive models captures sequential dependencies required for sequence generation. Applied to massive genomic datasets, these objectives enable transformers to learn representations that transfer across diverse downstream tasks without task-specific supervision. Foundation models like <em>DNABERT</em> for regulatory sequence, <em>ESM-2</em> for proteins, and <em>Enformer</em> for expression prediction each demonstrate that transformers trained on biological sequence capture patterns that generalize beyond their training objectives. The pretraining objectives that shape these learned representations are examined in <a href="p2-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>.</p>
<p>Attention introduced a paradigm shift in how genomic models access context. Where convolutional networks aggregate local information through hierarchical composition, attention enables direct communication between any two positions regardless of distance. The computational challenge shifts from extending receptive fields to managing the quadratic complexity of pairwise attention. State space models and linear attention variants address this bottleneck while maintaining long-range capability, and whether these alternatives ultimately complement or displace standard transformers remains an open question. What is clear is that attention-based architectures have become the default substrate for genomic foundation models, with the pretraining objectives examined in <a href="p2-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a> determining what biological knowledge they acquire.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Chapter Summary">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Core Concepts:</strong></p>
<ul>
<li><strong>Self-attention</strong> computes pairwise interactions between all sequence positions, enabling direct modeling of long-range dependencies that convolutions cannot capture</li>
<li>The <strong>query-key-value</strong> mechanism separates “what to look for” (query-key matching) from “what to send” (values), providing flexible information routing</li>
<li><strong>Multi-head attention</strong> allows parallel specialization: different heads learn to track local motifs, periodic spacing, long-range interactions, and motif-specific patterns</li>
<li><strong>Positional encodings</strong> break permutation invariance; relative encodings (ALiBi, RoPE) often outperform absolute encodings for length generalization</li>
</ul>
<p><strong>Architectural Variants:</strong></p>
<ul>
<li><strong>Encoder-only</strong> (bidirectional): Best for variant effect prediction, classification, embeddings</li>
<li><strong>Decoder-only</strong> (causal): Required for sequence generation and design</li>
<li><strong>Hybrid CNN-Transformer</strong>: Combines local pattern detection with long-range integration</li>
</ul>
<p><strong>Key Limitations:</strong></p>
<ul>
<li><strong>Quadratic complexity</strong> (<span class="math inline">\(O(L^2)\)</span>) limits practical context to ~10-50 kb for standard transformers</li>
<li><strong>State space models</strong> (HyenaDNA, Mamba, Evo) achieve linear complexity for megabase-scale contexts</li>
<li>The choice of architecture encodes assumptions about what biological relationships matter</li>
</ul>
<p><strong>Looking Ahead:</strong> <a href="p2-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a> examines how self-supervised objectives (masked language modeling, next-token prediction, contrastive learning) transform architectural capacity into biological knowledge. The specific pretraining strategy determines what patterns the model learns and what downstream tasks it can address.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-avsec_enformer_2021" class="csl-entry" role="listitem">
Avsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. <span>“[<span>Enformer</span>] <span>Effective</span> Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.”</span> <em>Nature Methods</em> 18 (October): 1196–1203. <a href="https://doi.org/10.1038/s41592-021-01252-x">https://doi.org/10.1038/s41592-021-01252-x</a>.
</div>
<div id="ref-brixi_evo_2025" class="csl-entry" role="listitem">
Brixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. <span>“[<span>Evo</span> 2] <span>Genome</span> Modeling and Design Across All Domains of Life with <span>Evo</span> 2.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.02.18.638918">https://doi.org/10.1101/2025.02.18.638918</a>.
</div>
<div id="ref-choromanski_rethinking_2022" class="csl-entry" role="listitem">
Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2022. <span>“Rethinking <span>Attention</span> with <span>Performers</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2009.14794">https://doi.org/10.48550/arXiv.2009.14794</a>.
</div>
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-dao_flashattention_2022" class="csl-entry" role="listitem">
Dao, Tri, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. <span>“<span>FlashAttention</span>: <span>Fast</span> and <span>Memory</span>-<span>Efficient</span> <span>Exact</span> <span>Attention</span> with <span>IO</span>-<span>Awareness</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 35 (December): 16344–59. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html">https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html</a>.
</div>
<div id="ref-gu_mamba_2024" class="csl-entry" role="listitem">
Gu, Albert, and Tri Dao. 2024. <span>“Mamba: <span>Linear</span>-<span>Time</span> <span>Sequence</span> <span>Modeling</span> with <span>Selective</span> <span>State</span> <span>Spaces</span>.”</span> In. <a href="https://openreview.net/forum?id=tEYskw1VY2">https://openreview.net/forum?id=tEYskw1VY2</a>.
</div>
<div id="ref-gu_efficiently_2022" class="csl-entry" role="listitem">
Gu, Albert, Karan Goel, Ankit Gupta, and Christopher Ré. 2022. <span>“On the <span>Parameterization</span> and <span>Initialization</span> of <span>Diagonal</span> <span>State</span> <span>Space</span> <span>Models</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 35 (December): 35971–83. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/e9a32fade47b906de908431991440f7c-Abstract-Conference.html">https://proceedings.neurips.cc/paper_files/paper/2022/hash/e9a32fade47b906de908431991440f7c-Abstract-Conference.html</a>.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-kaplan_scaling_2020" class="csl-entry" role="listitem">
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. <span>“Scaling <span>Laws</span> for <span>Neural</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2001.08361">https://doi.org/10.48550/arXiv.2001.08361</a>.
</div>
<div id="ref-loshchilov_decoupled_2019" class="csl-entry" role="listitem">
Loshchilov, Ilya, and Frank Hutter. 2019. <span>“Decoupled <span>Weight</span> <span>Decay</span> <span>Regularization</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1711.05101">https://doi.org/10.48550/arXiv.1711.05101</a>.
</div>
<div id="ref-nguyen_sequence_2024" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. <span>“Sequence Modeling and Design from Molecular to Genome Scale with <span>Evo</span>.”</span> <em>Science</em> 386 (6723): eado9336. <a href="https://doi.org/10.1126/science.ado9336">https://doi.org/10.1126/science.ado9336</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-poli_hyena_2023" class="csl-entry" role="listitem">
Poli, Michael, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. 2023. <span>“Hyena <span>Hierarchy</span>: <span>Towards</span> <span>Larger</span> <span>Convolutional</span> <span>Language</span> <span>Models</span>.”</span> In <em>Proceedings of the 40th <span>International</span> <span>Conference</span> on <span>Machine</span> <span>Learning</span></em>, 28043–78. PMLR. <a href="https://proceedings.mlr.press/v202/poli23a.html">https://proceedings.mlr.press/v202/poli23a.html</a>.
</div>
<div id="ref-press_train_2022" class="csl-entry" role="listitem">
Press, Ofir, Noah A. Smith, and Mike Lewis. 2022. <span>“Train <span>Short</span>, <span>Test</span> <span>Long</span>: <span>Attention</span> with <span>Linear</span> <span>Biases</span> <span>Enables</span> <span>Input</span> <span>Length</span> <span>Extrapolation</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2108.12409">https://doi.org/10.48550/arXiv.2108.12409</a>.
</div>
<div id="ref-raffel_t5_2019" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. <span>“Exploring the <span>Limits</span> of <span>Transfer</span> <span>Learning</span> with a <span>Unified</span> <span>Text</span>-to-<span>Text</span> <span>Transformer</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1910.10683">https://doi.org/10.48550/arXiv.1910.10683</a>.
</div>
<div id="ref-schiff_caduceus_2024" class="csl-entry" role="listitem">
Schiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. <span>“Caduceus: <span>Bi</span>-<span>Directional</span> <span>Equivariant</span> <span>Long</span>-<span>Range</span> <span>DNA</span> <span>Sequence</span> <span>Modeling</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2403.03234">https://doi.org/10.48550/arXiv.2403.03234</a>.
</div>
<div id="ref-su_roformer_2024" class="csl-entry" role="listitem">
Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. <span>“<span>RoFormer</span>: <span>Enhanced</span> Transformer with <span>Rotary</span> <span>Position</span> <span>Embedding</span>.”</span> <em>Neurocomputing</em> 568 (February): 127063. <a href="https://doi.org/10.1016/j.neucom.2023.127063">https://doi.org/10.1016/j.neucom.2023.127063</a>.
</div>
<div id="ref-vaswani_attention_2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention <span>Is</span> <span>All</span> <span>You</span> <span>Need</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-wang_linformer_2020" class="csl-entry" role="listitem">
Wang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. <span>“Linformer: <span>Self</span>-<span>Attention</span> with <span>Linear</span> <span>Complexity</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2006.04768">https://doi.org/10.48550/arXiv.2006.04768</a>.
</div>
<div id="ref-xiong_layer_2020" class="csl-entry" role="listitem">
Xiong, Ruibin, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. 2020. <span>“On <span>Layer</span> <span>Normalization</span> in the <span>Transformer</span> <span>Architecture</span>.”</span> In <em>Proceedings of the 37th <span>International</span> <span>Conference</span> on <span>Machine</span> <span>Learning</span></em>, 10524–33. PMLR. <a href="https://proceedings.mlr.press/v119/xiong20b.html">https://proceedings.mlr.press/v119/xiong20b.html</a>.
</div>
<div id="ref-zvyagin_genslms_2022" class="csl-entry" role="listitem">
Zvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. <span>“<span>GenSLMs</span>: <span>Genome</span>-Scale Language Models Reveal <span>SARS</span>-<span>CoV</span>-2 Evolutionary Dynamics.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.10.10.511571">https://doi.org/10.1101/2022.10.10.511571</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_2/p2-ch06-cnn.html" class="pagination-link" aria-label="Convolutional Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_2/p2-ch08-pretraining.html" class="pagination-link" aria-label="Pretraining Strategies">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>