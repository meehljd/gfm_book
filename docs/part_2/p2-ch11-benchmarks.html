<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Benchmarks and Evaluation – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_2/p2-ch12-confounding.html" rel="next">
<link href="../part_2/p2-ch10-adaptation.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch11-benchmarks.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch11-benchmarks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch12-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch14-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch15-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch16-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch17-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--multi-modal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Systems and Scale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch19-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch20-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch21-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch22-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch25-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Causal Inference with Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch26-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch28-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch29-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch30-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch31-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch11-protein-benchmarks" id="toc-sec-ch11-protein-benchmarks" class="nav-link active" data-scroll-target="#sec-ch11-protein-benchmarks"><span class="header-section-number">11.1</span> Protein Language Model Benchmarks</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-tape" id="toc-sec-ch11-tape" class="nav-link" data-scroll-target="#sec-ch11-tape"><span class="header-section-number">11.1.1</span> TAPE: Tasks Assessing Protein Embeddings</a></li>
  <li><a href="#sec-ch11-flip" id="toc-sec-ch11-flip" class="nav-link" data-scroll-target="#sec-ch11-flip"><span class="header-section-number">11.1.2</span> FLIP: Function-Linked Protein Benchmark</a></li>
  <li><a href="#sec-ch11-proteingym" id="toc-sec-ch11-proteingym" class="nav-link" data-scroll-target="#sec-ch11-proteingym"><span class="header-section-number">11.1.3</span> ProteinGym: Comprehensive Variant Effect Evaluation</a></li>
  <li><a href="#sec-ch11-structure-benchmarks" id="toc-sec-ch11-structure-benchmarks" class="nav-link" data-scroll-target="#sec-ch11-structure-benchmarks"><span class="header-section-number">11.1.4</span> Structure Prediction Benchmarks</a></li>
  </ul></li>
  <li><a href="#sec-ch11-dna-benchmarks" id="toc-sec-ch11-dna-benchmarks" class="nav-link" data-scroll-target="#sec-ch11-dna-benchmarks"><span class="header-section-number">11.2</span> DNA and Regulatory Benchmarks</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-classical-regulatory" id="toc-sec-ch11-classical-regulatory" class="nav-link" data-scroll-target="#sec-ch11-classical-regulatory"><span class="header-section-number">11.2.1</span> Classical Regulatory Prediction Tasks</a></li>
  <li><a href="#sec-ch11-quantitative-regulatory" id="toc-sec-ch11-quantitative-regulatory" class="nav-link" data-scroll-target="#sec-ch11-quantitative-regulatory"><span class="header-section-number">11.2.2</span> Quantitative Regulatory Prediction</a></li>
  <li><a href="#sec-ch11-genomic-benchmarks" id="toc-sec-ch11-genomic-benchmarks" class="nav-link" data-scroll-target="#sec-ch11-genomic-benchmarks"><span class="header-section-number">11.2.3</span> Genomic Benchmarks</a></li>
  <li><a href="#sec-ch11-bend" id="toc-sec-ch11-bend" class="nav-link" data-scroll-target="#sec-ch11-bend"><span class="header-section-number">11.2.4</span> BEND: Benchmark for DNA Language Models</a></li>
  <li><a href="#sec-ch11-long-range" id="toc-sec-ch11-long-range" class="nav-link" data-scroll-target="#sec-ch11-long-range"><span class="header-section-number">11.2.5</span> Long-Range Benchmarks</a></li>
  <li><a href="#sec-ch11-cross-species" id="toc-sec-ch11-cross-species" class="nav-link" data-scroll-target="#sec-ch11-cross-species"><span class="header-section-number">11.2.6</span> Cross-Species Evaluation</a></li>
  </ul></li>
  <li><a href="#sec-ch11-vep-benchmarks" id="toc-sec-ch11-vep-benchmarks" class="nav-link" data-scroll-target="#sec-ch11-vep-benchmarks"><span class="header-section-number">11.3</span> Variant Effect Prediction Benchmarks</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-clinical-databases" id="toc-sec-ch11-clinical-databases" class="nav-link" data-scroll-target="#sec-ch11-clinical-databases"><span class="header-section-number">11.3.1</span> Clinical Variant Databases</a></li>
  <li><a href="#sec-ch11-cagi" id="toc-sec-ch11-cagi" class="nav-link" data-scroll-target="#sec-ch11-cagi"><span class="header-section-number">11.3.2</span> CAGI: Critical Assessment of Genome Interpretation</a></li>
  <li><a href="#sec-ch11-dms-benchmarks" id="toc-sec-ch11-dms-benchmarks" class="nav-link" data-scroll-target="#sec-ch11-dms-benchmarks"><span class="header-section-number">11.3.3</span> Deep Mutational Scanning Benchmarks</a></li>
  <li><a href="#sec-ch11-noncoding-benchmarks" id="toc-sec-ch11-noncoding-benchmarks" class="nav-link" data-scroll-target="#sec-ch11-noncoding-benchmarks"><span class="header-section-number">11.3.4</span> Regulatory and Non-Coding Variant Benchmarks</a></li>
  </ul></li>
  <li><a href="#sec-ch11-trait-benchmarks" id="toc-sec-ch11-trait-benchmarks" class="nav-link" data-scroll-target="#sec-ch11-trait-benchmarks"><span class="header-section-number">11.4</span> Trait and Population-Level Benchmarks</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-pgs-evaluation" id="toc-sec-ch11-pgs-evaluation" class="nav-link" data-scroll-target="#sec-ch11-pgs-evaluation"><span class="header-section-number">11.4.1</span> Polygenic Score Evaluation</a></li>
  <li><a href="#sec-ch11-traitgym" id="toc-sec-ch11-traitgym" class="nav-link" data-scroll-target="#sec-ch11-traitgym"><span class="header-section-number">11.4.2</span> TraitGym</a></li>
  <li><a href="#sec-ch11-embedgem" id="toc-sec-ch11-embedgem" class="nav-link" data-scroll-target="#sec-ch11-embedgem"><span class="header-section-number">11.4.3</span> EmbedGEM Framework</a></li>
  </ul></li>
  <li><a href="#sec-ch11-benchmark-construction" id="toc-sec-ch11-benchmark-construction" class="nav-link" data-scroll-target="#sec-ch11-benchmark-construction"><span class="header-section-number">11.5</span> Benchmark Construction and Hidden Assumptions</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-label-provenance" id="toc-sec-ch11-label-provenance" class="nav-link" data-scroll-target="#sec-ch11-label-provenance"><span class="header-section-number">11.5.1</span> Data Sources and Label Provenance</a></li>
  <li><a href="#sec-ch11-splitting" id="toc-sec-ch11-splitting" class="nav-link" data-scroll-target="#sec-ch11-splitting"><span class="header-section-number">11.5.2</span> Splitting Strategies and Leakage</a></li>
  <li><a href="#sec-ch11-metrics" id="toc-sec-ch11-metrics" class="nav-link" data-scroll-target="#sec-ch11-metrics"><span class="header-section-number">11.5.3</span> Metric Selection and Aggregation</a></li>
  <li><a href="#sec-ch11-goodhart" id="toc-sec-ch11-goodhart" class="nav-link" data-scroll-target="#sec-ch11-goodhart"><span class="header-section-number">11.5.4</span> Goodhart’s Law and Benchmark Gaming</a></li>
  </ul></li>
  <li><a href="#sec-ch11-saturation-staleness" id="toc-sec-ch11-saturation-staleness" class="nav-link" data-scroll-target="#sec-ch11-saturation-staleness"><span class="header-section-number">11.6</span> Benchmark Saturation and Staleness</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-saturation" id="toc-sec-ch11-saturation" class="nav-link" data-scroll-target="#sec-ch11-saturation"><span class="header-section-number">11.6.1</span> Saturation: When Benchmarks Stop Discriminating</a></li>
  <li><a href="#sec-ch11-staleness" id="toc-sec-ch11-staleness" class="nav-link" data-scroll-target="#sec-ch11-staleness"><span class="header-section-number">11.6.2</span> Staleness: When Benchmarks Diverge from Practice</a></li>
  <li><a href="#sec-ch11-leakage-scale" id="toc-sec-ch11-leakage-scale" class="nav-link" data-scroll-target="#sec-ch11-leakage-scale"><span class="header-section-number">11.6.3</span> Leakage from Scale</a></li>
  </ul></li>
  <li><a href="#sec-ch11-deployment-gap" id="toc-sec-ch11-deployment-gap" class="nav-link" data-scroll-target="#sec-ch11-deployment-gap"><span class="header-section-number">11.7</span> Benchmark-Deployment Gap</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-distribution-shift" id="toc-sec-ch11-distribution-shift" class="nav-link" data-scroll-target="#sec-ch11-distribution-shift"><span class="header-section-number">11.7.1</span> Distribution Shift</a></li>
  <li><a href="#sec-ch11-calibration-requirements" id="toc-sec-ch11-calibration-requirements" class="nav-link" data-scroll-target="#sec-ch11-calibration-requirements"><span class="header-section-number">11.7.2</span> Calibration Requirements</a></li>
  <li><a href="#sec-ch11-metric-mismatch" id="toc-sec-ch11-metric-mismatch" class="nav-link" data-scroll-target="#sec-ch11-metric-mismatch"><span class="header-section-number">11.7.3</span> Metric Mismatch</a></li>
  <li><a href="#sec-ch11-practical-constraints" id="toc-sec-ch11-practical-constraints" class="nav-link" data-scroll-target="#sec-ch11-practical-constraints"><span class="header-section-number">11.7.4</span> Practical Constraints</a></li>
  </ul></li>
  <li><a href="#sec-ch11-systematic-gaps" id="toc-sec-ch11-systematic-gaps" class="nav-link" data-scroll-target="#sec-ch11-systematic-gaps"><span class="header-section-number">11.8</span> Systematic Gaps in Current Benchmarks</a></li>
  <li><a href="#sec-ch11-proxy-problem" id="toc-sec-ch11-proxy-problem" class="nav-link" data-scroll-target="#sec-ch11-proxy-problem"><span class="header-section-number">11.9</span> The Proxy Problem</a></li>
  <li><a href="#sec-ch11-evaluation-methodology" id="toc-sec-ch11-evaluation-methodology" class="nav-link" data-scroll-target="#sec-ch11-evaluation-methodology"><span class="header-section-number">11.10</span> Evaluation Methodology</a></li>
  <li><a href="#sec-ch11-eval" id="toc-sec-ch11-eval" class="nav-link" data-scroll-target="#sec-ch11-eval"><span class="header-section-number">12</span> Evaluation Principles</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-random-splits-fail" id="toc-sec-ch11-random-splits-fail" class="nav-link" data-scroll-target="#sec-ch11-random-splits-fail"><span class="header-section-number">12.1</span> Why Random Splits Fail</a></li>
  <li><a href="#sec-ch11-homology-aware-splitting" id="toc-sec-ch11-homology-aware-splitting" class="nav-link" data-scroll-target="#sec-ch11-homology-aware-splitting"><span class="header-section-number">12.2</span> Homology-Aware Splitting</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-clustering-tools" id="toc-sec-ch11-clustering-tools" class="nav-link" data-scroll-target="#sec-ch11-clustering-tools"><span class="header-section-number">12.2.1</span> Clustering Tools and Workflows</a></li>
  <li><a href="#sec-ch11-homology-practical" id="toc-sec-ch11-homology-practical" class="nav-link" data-scroll-target="#sec-ch11-homology-practical"><span class="header-section-number">12.2.2</span> Practical Considerations</a></li>
  </ul></li>
  <li><a href="#sec-ch11-splitting-biological-axis" id="toc-sec-ch11-splitting-biological-axis" class="nav-link" data-scroll-target="#sec-ch11-splitting-biological-axis"><span class="header-section-number">12.3</span> Splitting by Biological Axis</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-splitting-individual" id="toc-sec-ch11-splitting-individual" class="nav-link" data-scroll-target="#sec-ch11-splitting-individual"><span class="header-section-number">12.3.1</span> Splitting by Individual</a></li>
  <li><a href="#sec-ch11-splitting-genomic-region" id="toc-sec-ch11-splitting-genomic-region" class="nav-link" data-scroll-target="#sec-ch11-splitting-genomic-region"><span class="header-section-number">12.3.2</span> Splitting by Genomic Region</a></li>
  <li><a href="#sec-ch11-splitting-gene-family" id="toc-sec-ch11-splitting-gene-family" class="nav-link" data-scroll-target="#sec-ch11-splitting-gene-family"><span class="header-section-number">12.3.3</span> Splitting by Gene or Protein Family</a></li>
  <li><a href="#sec-ch11-splitting-experimental-context" id="toc-sec-ch11-splitting-experimental-context" class="nav-link" data-scroll-target="#sec-ch11-splitting-experimental-context"><span class="header-section-number">12.3.4</span> Splitting by Experimental Context</a></li>
  <li><a href="#sec-ch11-splitting-ancestry" id="toc-sec-ch11-splitting-ancestry" class="nav-link" data-scroll-target="#sec-ch11-splitting-ancestry"><span class="header-section-number">12.3.5</span> Splitting by Ancestry</a></li>
  <li><a href="#sec-ch11-splitting-time" id="toc-sec-ch11-splitting-time" class="nav-link" data-scroll-target="#sec-ch11-splitting-time"><span class="header-section-number">12.3.6</span> Splitting by Time</a></li>
  </ul></li>
  <li><a href="#sec-ch11-leakage-detection" id="toc-sec-ch11-leakage-detection" class="nav-link" data-scroll-target="#sec-ch11-leakage-detection"><span class="header-section-number">12.4</span> Leakage Taxonomy and Detection</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-label-leakage" id="toc-sec-ch11-label-leakage" class="nav-link" data-scroll-target="#sec-ch11-label-leakage"><span class="header-section-number">12.4.1</span> Label Leakage</a></li>
  <li><a href="#sec-ch11-feature-leakage" id="toc-sec-ch11-feature-leakage" class="nav-link" data-scroll-target="#sec-ch11-feature-leakage"><span class="header-section-number">12.4.2</span> Feature Leakage</a></li>
  <li><a href="#sec-ch11-temporal-leakage" id="toc-sec-ch11-temporal-leakage" class="nav-link" data-scroll-target="#sec-ch11-temporal-leakage"><span class="header-section-number">12.4.3</span> Temporal Leakage</a></li>
  <li><a href="#sec-ch11-benchmark-leakage" id="toc-sec-ch11-benchmark-leakage" class="nav-link" data-scroll-target="#sec-ch11-benchmark-leakage"><span class="header-section-number">12.4.4</span> Benchmark Leakage</a></li>
  <li><a href="#sec-ch11-detecting-leakage" id="toc-sec-ch11-detecting-leakage" class="nav-link" data-scroll-target="#sec-ch11-detecting-leakage"><span class="header-section-number">12.4.5</span> Detecting Leakage</a></li>
  </ul></li>
  <li><a href="#sec-ch11-metrics-genomic-tasks" id="toc-sec-ch11-metrics-genomic-tasks" class="nav-link" data-scroll-target="#sec-ch11-metrics-genomic-tasks"><span class="header-section-number">12.5</span> Metrics for Genomic Tasks</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-discrimination-metrics" id="toc-sec-ch11-discrimination-metrics" class="nav-link" data-scroll-target="#sec-ch11-discrimination-metrics"><span class="header-section-number">12.5.1</span> Discrimination Metrics</a></li>
  <li><a href="#sec-ch11-regression-correlation-metrics" id="toc-sec-ch11-regression-correlation-metrics" class="nav-link" data-scroll-target="#sec-ch11-regression-correlation-metrics"><span class="header-section-number">12.5.2</span> Regression and Correlation Metrics</a></li>
  <li><a href="#sec-ch11-ranking-prioritization-metrics" id="toc-sec-ch11-ranking-prioritization-metrics" class="nav-link" data-scroll-target="#sec-ch11-ranking-prioritization-metrics"><span class="header-section-number">12.5.3</span> Ranking and Prioritization Metrics</a></li>
  <li><a href="#sec-ch11-clinical-utility-metrics" id="toc-sec-ch11-clinical-utility-metrics" class="nav-link" data-scroll-target="#sec-ch11-clinical-utility-metrics"><span class="header-section-number">12.5.4</span> Clinical Utility Metrics</a></li>
  </ul></li>
  <li><a href="#sec-ch11-baseline-selection" id="toc-sec-ch11-baseline-selection" class="nav-link" data-scroll-target="#sec-ch11-baseline-selection"><span class="header-section-number">12.6</span> Baseline Selection</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-strong-baselines" id="toc-sec-ch11-strong-baselines" class="nav-link" data-scroll-target="#sec-ch11-strong-baselines"><span class="header-section-number">12.6.1</span> Strong Baselines, Not Straw Men</a></li>
  <li><a href="#sec-ch11-historical-baselines" id="toc-sec-ch11-historical-baselines" class="nav-link" data-scroll-target="#sec-ch11-historical-baselines"><span class="header-section-number">12.6.2</span> Historical Baselines and Progress Tracking</a></li>
  <li><a href="#sec-ch11-non-dl-baselines" id="toc-sec-ch11-non-dl-baselines" class="nav-link" data-scroll-target="#sec-ch11-non-dl-baselines"><span class="header-section-number">12.6.3</span> Non-Deep-Learning Baselines</a></li>
  </ul></li>
  <li><a href="#sec-ch11-ablation-studies" id="toc-sec-ch11-ablation-studies" class="nav-link" data-scroll-target="#sec-ch11-ablation-studies"><span class="header-section-number">12.7</span> Ablation Studies</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-component-isolation" id="toc-sec-ch11-component-isolation" class="nav-link" data-scroll-target="#sec-ch11-component-isolation"><span class="header-section-number">12.7.1</span> Component Isolation</a></li>
  <li><a href="#sec-ch11-hyperparameter-sensitivity" id="toc-sec-ch11-hyperparameter-sensitivity" class="nav-link" data-scroll-target="#sec-ch11-hyperparameter-sensitivity"><span class="header-section-number">12.7.2</span> Hyperparameter Sensitivity</a></li>
  <li><a href="#sec-ch11-architecture-search-confounds" id="toc-sec-ch11-architecture-search-confounds" class="nav-link" data-scroll-target="#sec-ch11-architecture-search-confounds"><span class="header-section-number">12.7.3</span> Architecture Search Confounds</a></li>
  <li><a href="#sec-ch11-reporting-standards" id="toc-sec-ch11-reporting-standards" class="nav-link" data-scroll-target="#sec-ch11-reporting-standards"><span class="header-section-number">12.7.4</span> Reporting Standards</a></li>
  </ul></li>
  <li><a href="#sec-ch11-statistical-rigor" id="toc-sec-ch11-statistical-rigor" class="nav-link" data-scroll-target="#sec-ch11-statistical-rigor"><span class="header-section-number">12.8</span> Statistical Rigor</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-significance-testing" id="toc-sec-ch11-significance-testing" class="nav-link" data-scroll-target="#sec-ch11-significance-testing"><span class="header-section-number">12.8.1</span> Significance Testing</a></li>
  <li><a href="#sec-ch11-effect-sizes" id="toc-sec-ch11-effect-sizes" class="nav-link" data-scroll-target="#sec-ch11-effect-sizes"><span class="header-section-number">12.8.2</span> Effect Sizes</a></li>
  <li><a href="#sec-ch11-confidence-intervals" id="toc-sec-ch11-confidence-intervals" class="nav-link" data-scroll-target="#sec-ch11-confidence-intervals"><span class="header-section-number">12.8.3</span> Confidence Intervals on Metrics</a></li>
  <li><a href="#sec-ch11-variance-random-seeds" id="toc-sec-ch11-variance-random-seeds" class="nav-link" data-scroll-target="#sec-ch11-variance-random-seeds"><span class="header-section-number">12.8.4</span> Variance Across Random Seeds</a></li>
  </ul></li>
  <li><a href="#sec-ch11-evaluating-fm" id="toc-sec-ch11-evaluating-fm" class="nav-link" data-scroll-target="#sec-ch11-evaluating-fm"><span class="header-section-number">12.9</span> Evaluating Foundation Models</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-zero-shot-eval" id="toc-sec-ch11-zero-shot-eval" class="nav-link" data-scroll-target="#sec-ch11-zero-shot-eval"><span class="header-section-number">12.9.1</span> Zero-Shot Evaluation</a></li>
  <li><a href="#sec-ch11-linear-probing" id="toc-sec-ch11-linear-probing" class="nav-link" data-scroll-target="#sec-ch11-linear-probing"><span class="header-section-number">12.9.2</span> Linear Probing</a></li>
  <li><a href="#sec-ch11-fine-tuning-eval" id="toc-sec-ch11-fine-tuning-eval" class="nav-link" data-scroll-target="#sec-ch11-fine-tuning-eval"><span class="header-section-number">12.9.3</span> Fine-Tuning Evaluation</a></li>
  <li><a href="#sec-ch11-transfer-tasks" id="toc-sec-ch11-transfer-tasks" class="nav-link" data-scroll-target="#sec-ch11-transfer-tasks"><span class="header-section-number">12.9.4</span> Transfer Across Tasks</a></li>
  </ul></li>
  <li><a href="#sec-ch11-calibration" id="toc-sec-ch11-calibration" class="nav-link" data-scroll-target="#sec-ch11-calibration"><span class="header-section-number">12.10</span> Calibration Essentials</a>
  <ul class="collapse">
  <li><a href="#sec-ch11-assessing-calibration" id="toc-sec-ch11-assessing-calibration" class="nav-link" data-scroll-target="#sec-ch11-assessing-calibration"><span class="header-section-number">12.10.1</span> Assessing Calibration</a></li>
  <li><a href="#sec-ch11-recalibration-methods" id="toc-sec-ch11-recalibration-methods" class="nav-link" data-scroll-target="#sec-ch11-recalibration-methods"><span class="header-section-number">12.10.2</span> Recalibration Methods</a></li>
  <li><a href="#sec-ch11-calibration-comparison" id="toc-sec-ch11-calibration-comparison" class="nav-link" data-scroll-target="#sec-ch11-calibration-comparison"><span class="header-section-number">12.10.3</span> Calibration in Model Comparison</a></li>
  </ul></li>
  <li><a href="#sec-ch11-putting-together" id="toc-sec-ch11-putting-together" class="nav-link" data-scroll-target="#sec-ch11-putting-together"><span class="header-section-number">12.11</span> Putting It All Together</a></li>
  <li><a href="#sec-ch11-question-behind-metric" id="toc-sec-ch11-question-behind-metric" class="nav-link" data-scroll-target="#sec-ch11-question-behind-metric"><span class="header-section-number">12.12</span> The Question Behind the Metric</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_2/p2--principles.html">Part II: Sequence Architectures</a></li><li class="breadcrumb-item"><a href="../part_2/p2-ch11-benchmarks.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch11-benchmarks" class="quarto-section-identifier"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Prerequisites:</strong> This chapter assumes familiarity with basic machine learning concepts including train/test splits, common metrics (accuracy, auROC), and model evaluation (<a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>). Understanding of genomic data types (<a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a>) and variant effect prediction concepts (<a href="../part_1/p1-ch04-vep-classical.html" class="quarto-xref"><span>Chapter 4</span></a>) will help contextualize benchmark tasks.</p>
<p><strong>Learning Objectives:</strong> After completing this chapter, you should be able to:</p>
<ol type="1">
<li>Navigate the major benchmark suites for protein (<em>TAPE</em>, <em>FLIP</em>, <em>ProteinGym</em>), DNA (<em>BEND</em>, <em>Genomic Benchmarks</em>), and variant effect prediction (ClinVar, CAGI)</li>
<li>Identify and prevent common leakage pathways in genomic evaluations (homology leakage, label circularity, temporal leakage)</li>
<li>Select appropriate splitting strategies (homology-aware, chromosome-based, ancestry-stratified) based on the generalization question being asked</li>
<li>Choose metrics aligned with deployment objectives, distinguishing discrimination, calibration, and clinical utility</li>
<li>Design ablation studies and baseline comparisons that isolate genuine model contributions</li>
<li>Critically evaluate benchmark claims by identifying proxy-target gaps and systematic biases</li>
</ol>
<p><strong>Chapter Structure:</strong> We first survey <em>what</em> benchmarks exist across modalities (Sections 1-5), then examine <em>how</em> to evaluate properly (Sections 6-11), and finally address the gap between benchmark success and deployment value (Sections 12-13).</p>
</div>
</div>
<p>Every <strong>benchmark</strong> measures a proxy. ClinVar pathogenicity labels proxy clinical impact. Area under the receiver operating characteristic curve (auROC) on held-out variants proxies discrimination ability in deployment. Chromatin accessibility predictions proxy regulatory function. The gap between proxy and target varies across benchmarks, across variant types, and across populations. A model achieving state-of-the-art performance on ClinVar may systematically miscalibrate predictions for the rare variants that matter most clinically, because ClinVar’s composition does not reflect the distribution of variants clinicians actually encounter. A DNA language model excelling at enhancer classification may have learned GC content rather than regulatory grammar, because the benchmark’s negative examples differ from positives in ways that have nothing to do with enhancer function.</p>
<p>Understanding what benchmarks actually measure, and how that differs from what we need to know, is prerequisite to interpreting any leaderboard result. The genomic AI field has accumulated substantial evaluation infrastructure. Dozens of benchmark suites target different modalities: protein structure and function, DNA regulatory elements, variant pathogenicity, gene expression prediction, and more. Hundreds of individual tasks probe specific capabilities. Thousands of models have reported results, creating leaderboards that rank approaches and track progress over time. This infrastructure enables comparison and drives methodological improvement. Yet the relationship between benchmark success and deployment value remains poorly characterized. A <strong>foundation model</strong> that dominates protein benchmarks may fail on the specific protein family relevant to a drug discovery campaign. A variant effect predictor that leads regulatory benchmarks may provide no clinical utility for the variant classes that lack representation in evaluation data.</p>
<div id="fig-benchmark-landscape" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-benchmark-landscape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch11/01-fig-benchmark-landscape.svg" class="img-fluid figure-img"></p>
<figcaption>The genomic AI benchmark landscape</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-benchmark-landscape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.1: The genomic AI benchmark landscape organized by task category (rows) and evaluation paradigm (columns). Major benchmarks are positioned according to what they test: variant effect prediction relies on ClinVar, DMS assays, and ProteinGym; regulatory classification uses ENCODE and custom enhancer datasets; expression prediction evaluates against GTEx and tissue-specific measurements; protein structure benchmarks include CASP and AlphaFold evaluations. Evaluation paradigms range from zero-shot (no task-specific training) through linear probing (frozen embeddings) to full fine-tuning. Color intensity indicates benchmark maturity and community adoption. Warning symbols mark benchmarks with known limitations including potential leakage or approaching saturation.
</figcaption>
</figure>
</div>
<section id="sec-ch11-protein-benchmarks" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="sec-ch11-protein-benchmarks"><span class="header-section-number">11.1</span> Protein Language Model Benchmarks</h2>
<p><strong>Protein language models</strong> (<a href="../part_3/p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a>) benefit from the longest-established and most systematic evaluation ecosystem in genomic AI, reflecting both the longer history of computational protein science and the relative tractability of protein structure and function prediction compared to the regulatory genomics tasks discussed in <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>. The maturity of protein benchmarks reflects both the longer history of computational protein science and the relative tractability of protein structure and function prediction compared to regulatory genomics.</p>
<section id="sec-ch11-tape" class="level3" data-number="11.1.1">
<h3 data-number="11.1.1" class="anchored" data-anchor-id="sec-ch11-tape"><span class="header-section-number">11.1.1</span> TAPE: Tasks Assessing Protein Embeddings</h3>
<p>The Tasks Assessing Protein Embeddings (<em>TAPE</em>) benchmark, introduced in 2019, established the template for systematic protein representation evaluation <span class="citation" data-cites="rao_evaluating_2019">(<a href="../bib/references.html#ref-rao_evaluating_2019" role="doc-biblioref">Rao et al. 2019</a>)</span>. <em>TAPE</em> frames protein language model assessment as <strong>transfer learning</strong> evaluation (<a href="p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>): pretrained models generate <strong>embeddings</strong> (<a href="p2-ch05-representations.html#sec-ch05-embeddings" class="quarto-xref"><span>Section 5.6</span></a>), which are then used as features for supervised prediction on downstream tasks. This framework decouples representation quality from task-specific modeling, enabling comparison across architectures that may have very different inductive biases.</p>
<p><em>TAPE</em> comprises five tasks spanning different aspects of protein biology. Secondary structure prediction requires classifying each residue as helix, sheet, or coil, testing whether embeddings capture local structural preferences. Contact prediction asks whether residue pairs are spatially proximate in the folded structure, probing the representation’s ability to encode tertiary structure information from sequence alone. Remote homology detection requires classifying proteins into structural superfamilies, testing whether embeddings capture evolutionary relationships that transcend sequence similarity. Fluorescence prediction and stability prediction use data from deep mutational scanning experiments to assess whether embeddings encode fitness landscapes.</p>
<p>The benchmark’s design reflects deliberate methodological choices. Train, validation, and test splits enforce sequence identity thresholds to prevent homology-based leakage (<a href="#sec-ch11-leakage-detection" class="quarto-xref"><span>Section 12.4</span></a>). Evaluation uses simple linear or shallow neural network heads rather than complex task-specific architectures, isolating representation quality from modeling capacity. Standardized preprocessing and data loading eliminate confounds from inconsistent implementation.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: The Transfer Learning Evaluation Framework
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>TAPE</em> established a crucial principle: evaluate representations separately from task-specific modeling. By using simple linear classifiers on top of frozen embeddings, <em>TAPE</em> isolates what the pretrained model learned from what a complex head might learn during fine-tuning. This approach became the standard template for foundation model evaluation across genomics, enabling fair comparison between models with different architectures and pretraining objectives.</p>
</div>
</div>
<p><em>TAPE’s</em> influence extended beyond its specific tasks. The benchmark established norms for protein representation evaluation: systematic coverage of diverse prediction targets, controlled transfer learning protocols, and explicit attention to data splitting. Subsequent benchmarks adopted and extended this framework.</p>
</section>
<section id="sec-ch11-flip" class="level3" data-number="11.1.2">
<h3 data-number="11.1.2" class="anchored" data-anchor-id="sec-ch11-flip"><span class="header-section-number">11.1.2</span> FLIP: Function-Linked Protein Benchmark</h3>
<p>The <em>FLIP</em> (Function-Linked Integrated Protein) benchmark addresses gaps in <em>TAPE’s</em> coverage by focusing on experimentally measured functional properties <span class="citation" data-cites="dallago_flip_2022">(<a href="../bib/references.html#ref-dallago_flip_2022" role="doc-biblioref">Dallago et al. 2022</a>)</span>. Where <em>TAPE</em> includes structurally derived labels and computational annotations, <em>FLIP</em> emphasizes high-throughput experimental assays that directly measure protein fitness.</p>
<p><em>FLIP</em> aggregates deep mutational scanning datasets across diverse proteins and functional readouts. The benchmark includes assays measuring enzymatic activity, binding affinity, thermostability, and expression level. Each dataset provides quantitative measurements for thousands of single-point mutations, enabling evaluation of fine-grained variant effect prediction.</p>
<p>The benchmark’s value lies in its experimental grounding. Computational structure predictions and evolutionary conservation scores, while useful, are indirect proxies for function. Deep mutational scanning provides direct measurements of how sequence changes affect the property of interest. Models that perform well on <em>FLIP</em> demonstrate the ability to predict experimentally validated functional consequences rather than computationally inferred annotations.</p>
<p><em>FLIP</em> also introduced systematic evaluation of different splitting strategies. Random splits, where training and test variants are sampled uniformly from the same protein, represent the easiest setting. Contiguous splits, where training and test variants occupy different sequence regions, test spatial generalization. Modulo splits, which interleave training and test positions along the sequence, provide intermediate difficulty. Performance typically degrades from random to contiguous splits, revealing how much models rely on local sequence context versus genuine functional understanding.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think: Splitting Strategy Implications
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading on, consider: If a model achieves 0.85 correlation on <em>FLIP</em> with random splits but only 0.60 correlation with contiguous splits, what does this reveal about what the model has learned? What kind of information would be available in random splits but not contiguous splits?</p>
<p><em>Hint: Think about what information from nearby positions might “leak” across random splits.</em></p>
</div>
</div>
</section>
<section id="sec-ch11-proteingym" class="level3" data-number="11.1.3">
<h3 data-number="11.1.3" class="anchored" data-anchor-id="sec-ch11-proteingym"><span class="header-section-number">11.1.3</span> ProteinGym: Comprehensive Variant Effect Evaluation</h3>
<p><em>ProteinGym</em> has emerged as the most comprehensive benchmark for protein variant effect prediction, compiling 217 deep mutational scanning assays across diverse protein families <span class="citation" data-cites="notin_proteingym_2024">(<a href="../bib/references.html#ref-notin_proteingym_2024" role="doc-biblioref">Notin et al. 2023</a>)</span>. The benchmark’s scale enables statistically robust comparison across modeling approaches while its diversity reveals where different methods excel or struggle.</p>
<p>The primary evaluation metric is Spearman correlation between predicted and experimentally measured fitness effects. This rank-based metric is appropriate for deep mutational scanning data, where absolute fitness values depend on assay-specific <strong>calibration</strong> but relative rankings are more comparable across experiments. <em>ProteinGym</em> reports correlations for each assay individually and aggregated across the full benchmark, enabling both global comparison and identification of task-specific strengths.</p>
<p><em>ProteinGym</em> distinguishes between zero-shot and supervised evaluation regimes. In zero-shot evaluation, models predict variant effects without any task-specific training, relying entirely on representations learned during pretraining. Models like <em>ESM-1v</em> (<a href="../part_3/p3-ch15-protein-lm.html#sec-ch15-esm-family" class="quarto-xref"><span>Section 15.1</span></a>) compute effects as log-likelihood ratios under the pretrained language model, while structure-based methods like <em>AlphaMissense</em> (<a href="../part_3/p3-ch17-vep-fm.html#sec-ch17-alphamissense" class="quarto-xref"><span>Section 18.2.3</span></a>) incorporate predicted structural consequences. In supervised evaluation, models are <strong>fine-tuned</strong> on a subset of measured variants before predicting held-out effects. The gap between zero-shot and supervised performance indicates how much task-specific information improves over general-purpose representations.</p>
<p>The benchmark reveals systematic patterns in model performance. Protein language models generally outperform conservation-based methods, particularly for variants in regions with sparse evolutionary sampling. Structure-aware models show advantages for variants affecting protein stability or buried residues. Ensemble methods that combine multiple predictors often achieve the highest correlations, suggesting that different approaches capture complementary information.</p>
<p><em>ProteinGym’s</em> limitations mirror those of its constituent datasets. Deep mutational scanning experiments are biased toward well-studied proteins amenable to high-throughput screening. Assay-specific selection pressures affect which variants appear deleterious: a variant may strongly affect enzymatic activity while leaving thermostability unchanged, or vice versa. The benchmark measures correlation with specific experimental readouts rather than clinical pathogenicity, which integrates multiple functional consequences in complex ways.</p>
<div id="tbl-protein-benchmarks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-protein-benchmarks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;11.1: Comparison of major protein benchmarks. Each benchmark makes different trade-offs between scale, label quality, and evaluation rigor.
</figcaption>
<div aria-describedby="tbl-protein-benchmarks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Tasks</th>
<th>Labels</th>
<th>Splitting</th>
<th>Strengths</th>
<th>Limitations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>TAPE</em></td>
<td>5 (structure, homology, fitness)</td>
<td>Mixed (experimental + computational)</td>
<td>Homology-aware</td>
<td>Established template; diverse tasks</td>
<td>Some computational labels; limited DMS coverage</td>
</tr>
<tr class="even">
<td><em>FLIP</em></td>
<td>Multiple DMS datasets</td>
<td>Experimental only</td>
<td>Multiple strategies (random, contiguous, modulo)</td>
<td>Experimental grounding; systematic split analysis</td>
<td>Limited to proteins with DMS data</td>
</tr>
<tr class="odd">
<td><em>ProteinGym</em></td>
<td>217 DMS assays</td>
<td>Experimental only</td>
<td>Zero-shot + supervised</td>
<td>Comprehensive scale; diverse protein families</td>
<td>Biased toward well-studied proteins</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-ch11-structure-benchmarks" class="level3" data-number="11.1.4">
<h3 data-number="11.1.4" class="anchored" data-anchor-id="sec-ch11-structure-benchmarks"><span class="header-section-number">11.1.4</span> Structure Prediction Benchmarks</h3>
<p>Protein structure prediction benchmarks derive from the Critical Assessment of protein Structure Prediction (CASP) tradition, which has evaluated computational methods against experimentally determined structures since 1994 <span class="citation" data-cites="kryshtafovych_critical_2021">(<a href="../bib/references.html#ref-kryshtafovych_critical_2021" role="doc-biblioref"><strong>kryshtafovych_critical_2021?</strong></a>)</span>. The dramatic success of <em>AlphaFold2</em> at CASP14 in 2020 transformed the field, but structure prediction benchmarks remain relevant for evaluating single-sequence methods and assessing whether language model pretraining improves structural accuracy.</p>
<p>Structure prediction quality is typically assessed using the Global Distance Test (GDT-TS) and Template Modeling score (TM-score). GDT-TS measures the percentage of residues that can be superimposed within various distance thresholds, providing a single number between 0 and 100 that correlates well with visual assessment of structural similarity. TM-score normalizes by protein length, enabling comparison across proteins of different sizes.</p>
<p>For protein language models, the relevant evaluation setting is single-sequence structure prediction, where the model receives only the target sequence without multiple sequence alignments. This tests whether pretraining on evolutionary sequence databases enables structure prediction without explicit evolutionary analysis at inference time. <em>ESMFold</em> (<a href="../part_3/p3-ch15-protein-lm.html#sec-ch15-esmfold" class="quarto-xref"><span>Section 15.4</span></a>) demonstrated that single-sequence prediction can approach MSA-based methods for many proteins, though performance gaps remain for sequences with sparse evolutionary coverage.</p>
<p>Structure prediction benchmarks complement sequence-based evaluations by testing whether learned representations encode biophysical constraints. A model that achieves high accuracy on contact prediction or secondary structure classification may still fail to integrate these local predictions into globally consistent structures. The emergence of accurate single-sequence structure prediction from language model embeddings suggests that pretraining captures substantial structural information, even without explicit structural supervision.</p>
</section>
</section>
<section id="sec-ch11-dna-benchmarks" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="sec-ch11-dna-benchmarks"><span class="header-section-number">11.2</span> DNA and Regulatory Benchmarks</h2>
<p>DNA foundation models (<a href="../part_3/p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>) and regulatory models (<a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>) face a less mature but rapidly developing benchmark landscape compared to the protein ecosystem. Early deep learning work in genomics focused on individual tasks derived from ENCODE-style assays (<a href="../part_1/p1-ch02-data.html#sec-ch02-encode" class="quarto-xref"><span>Section 2.4.1</span></a>), establishing evaluation paradigms that later benchmark suites would systematize. Recent efforts have introduced benchmark suites that attempt to standardize evaluation across multiple tasks, tissues, and species.</p>
<section id="sec-ch11-classical-regulatory" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="sec-ch11-classical-regulatory"><span class="header-section-number">11.2.1</span> Classical Regulatory Prediction Tasks</h3>
<p>The earliest deep learning benchmarks for genomics framed regulatory prediction as classification over short sequence windows. Transcription factor binding prediction asks whether a specific TF ChIP-seq peak overlaps a given sequence window, typically around 1 kilobase centered on the binding site. Open chromatin prediction requires classifying regions as accessible or inaccessible based on DNase-seq or ATAC-seq signal. Histone mark prediction asks whether a chromatin modification peak (H3K27ac, H3K4me3, etc.) is present at each position.</p>
<p>These tasks derive from consortia like ENCODE and Roadmap Epigenomics (<a href="../part_1/p1-ch02-data.html#sec-ch02-encode" class="quarto-xref"><span>Section 2.4.1</span></a>), which systematically profiled chromatin states across cell types. Benchmark construction typically involves defining positive regions from called peaks and sampling negative regions from elsewhere in the genome, extracting fixed-length sequences centered on each region, and evaluating binary classification using auROC or average <strong>precision</strong>.</p>
<p>Models such as <em>DeepSEA</em>, <em>Basset</em>, and <em>DanQ</em> established baseline performance on these tasks (<a href="p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> for architectural details). Their success demonstrated that convolutional networks could learn sequence features predictive of regulatory state without hand-crafted motifs. Modern foundation models still report performance on similar tasks as sanity checks, though these classical benchmarks have significant limitations.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Limitation Alert: Binary Classification Over Short Windows
</div>
</div>
<div class="callout-body-container callout-body">
<p>The primary limitation of classical regulatory benchmarks is that binary classification over short windows fails to capture the quantitative, cell-type-specific, and long-range nature of transcriptional regulation. A region may be weakly accessible in some cell types and strongly accessible in others; binary labels collapse this continuous variation. Short windows cannot assess whether models capture distal regulatory interactions that span tens to hundreds of kilobases. Evaluation on curated peak regions may overestimate performance relative to genome-wide prediction, where the vast majority of positions are regulatory “background.”</p>
</div>
</div>
</section>
<section id="sec-ch11-quantitative-regulatory" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="sec-ch11-quantitative-regulatory"><span class="header-section-number">11.2.2</span> Quantitative Regulatory Prediction</h3>
<p>Beyond binary classification, benchmarks increasingly require prediction of quantitative regulatory readouts. Signal regression asks models to predict per-base or per-bin signal intensity from ChIP-seq, ATAC-seq, or related assays. Gene expression prediction requires predicting transcript abundance (TPM, counts) from promoter sequences or larger genomic contexts. Massively parallel reporter assays (MPRAs) provide systematic measurements of enhancer or promoter activity for thousands of sequences, enabling evaluation of quantitative activity prediction.</p>
<p>Hybrid architectures like <em>Enformer</em> (<a href="../part_3/p3-ch16-regulatory.html#sec-ch16-enformer" class="quarto-xref"><span>Section 16.2</span></a>) popularized benchmarks combining large receptive fields with dense quantitative targets across many assays and cell types. Evaluation metrics shift from auROC to Pearson or Spearman correlation between predicted and observed profiles. Some benchmarks report correlation relative to replicate concordance, establishing an upper bound set by experimental reproducibility.</p>
<p>Quantitative benchmarks better reflect the continuous nature of regulatory activity but introduce new challenges. Heterogeneous noise across assays and laboratories complicates aggregation: should a model be penalized equally for poor performance on a low-quality assay versus a high-quality one? Cell-type diversity raises questions about how to weight performance across tissues: is accurate prediction in a rare cell type more or less important than in a common one? The relationship between predicted and observed signal depends on assay-specific calibration that may not transfer across experimental batches.</p>
</section>
<section id="sec-ch11-genomic-benchmarks" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="sec-ch11-genomic-benchmarks"><span class="header-section-number">11.2.3</span> Genomic Benchmarks</h3>
<p>The <em>Genomic Benchmarks</em> resource provides standardized classification datasets for DNA sequence models <span class="citation" data-cites="gresova_genomic_2023">(<a href="../bib/references.html#ref-gresova_genomic_2023" role="doc-biblioref">Grešová et al. 2023</a>)</span>. The benchmark compiles tasks including enhancer identification, promoter recognition, splice site detection, and coding sequence classification across multiple species. Standardized train, validation, and test splits enable direct comparison of different architectures without confounds from inconsistent data processing.</p>
<p><em>Genomic Benchmarks</em> emphasizes accessibility and reproducibility. Datasets are available in a unified format with documented preprocessing. Baseline results for multiple architectures provide reference points for new models. The benchmark includes tasks of varying difficulty, from relatively easy (distinguishing coding from non-coding sequence) to challenging (identifying tissue-specific enhancers).</p>
<p>The benchmark’s limitations reflect its design priorities. Focus on classification rather than regression excludes quantitative prediction tasks. Task difficulty varies substantially, with some tasks approaching saturation where gains become difficult to measure. Species coverage, while broader than many benchmarks, remains biased toward well-studied model organisms.</p>
</section>
<section id="sec-ch11-bend" class="level3" data-number="11.2.4">
<h3 data-number="11.2.4" class="anchored" data-anchor-id="sec-ch11-bend"><span class="header-section-number">11.2.4</span> BEND: Benchmark for DNA Language Models</h3>
<p><em>BEND</em> (Benchmark for Evaluating DNA Models) provides a unified framework for evaluating genomic foundation models across diverse tasks <span class="citation" data-cites="marin_bend_2024">(<a href="../bib/references.html#ref-marin_bend_2024" role="doc-biblioref">Marin et al. 2024</a>)</span>. The benchmark includes regulatory element classification, chromatin accessibility prediction, variant effect scoring, and gene expression prediction. Standardized splits and evaluation protocols enable fair comparison across model families.</p>
<p><em>BEND’s</em> design reflects lessons learned from earlier benchmarks. Tasks span multiple biological scales, from nucleotide-level variant effects to kilobase-scale regulatory elements. Evaluation includes both zero-shot settings (using pretrained representations directly) and fine-tuned settings (adapting models to specific tasks). Performance is reported separately for each task rather than aggregated into a single score, acknowledging that different models may excel at different aspects of genomic prediction.</p>
<p>Comparative evaluations using <em>BEND</em> reveal that no single model dominates across all tasks. Architecture choices (CNN versus transformer versus state space model), tokenization schemes (single nucleotide versus k-mer versus BPE), and pretraining corpora all influence task-specific performance (<a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>). These patterns inform model selection for specific applications while highlighting the limitations of aggregate benchmarks that obscure such variation.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check: DNA Benchmark Landscape
</div>
</div>
<div class="callout-body-container callout-body">
<p>Test your understanding of the DNA benchmark landscape:</p>
<ol type="1">
<li>What is the key difference between classical regulatory benchmarks (binary classification) and modern quantitative benchmarks?</li>
<li>Why might a model perform well on <em>Genomic Benchmarks</em> but poorly on <em>BEND</em>?</li>
<li>What does it mean when “no single model dominates across all tasks” in <em>BEND</em>?</li>
</ol>
<p><em>Consider how benchmark design choices affect what capabilities are actually being measured.</em></p>
</div>
</div>
</section>
<section id="sec-ch11-long-range" class="level3" data-number="11.2.5">
<h3 data-number="11.2.5" class="anchored" data-anchor-id="sec-ch11-long-range"><span class="header-section-number">11.2.5</span> Long-Range Benchmarks</h3>
<p>Long-range regulatory interactions, where enhancers tens to hundreds of kilobases from their target genes influence expression, require benchmarks that specifically test extended context modeling. The Long Range Benchmark (<em>LRB</em>) evaluates models’ ability to integrate information across large genomic distances, with tasks including predicting distal enhancer-promoter interactions, modeling TAD boundary effects, and identifying long-range regulatory dependencies <em>[Citation Needed]</em>.</p>
<p><em>DNALongBench</em> extends evaluation to ultra-long contexts spanning up to millions of base pairs <em>[Citation Needed]</em>. Tasks at this scale test whether models can leverage chromosome-level context for regulatory prediction, potentially capturing effects from 3D chromatin organization and large-scale chromatin domains.</p>
<p>These benchmarks are particularly relevant for evaluating efficient attention mechanisms, state space models, and other architectures designed to extend effective context length (<a href="p2-ch07-attention.html#sec-ch07-scaling" class="quarto-xref"><span>Section 7.4</span></a>). Performance on long-range benchmarks does not necessarily correlate with short-range task performance, indicating that different architectural choices optimize for different aspects of sequence modeling.</p>
</section>
<section id="sec-ch11-cross-species" class="level3" data-number="11.2.6">
<h3 data-number="11.2.6" class="anchored" data-anchor-id="sec-ch11-cross-species"><span class="header-section-number">11.2.6</span> Cross-Species Evaluation</h3>
<p><em>GenBench</em> and related resources test whether models trained on one organism generalize to related species <em>[Citation Needed]</em>. Cross-species evaluation is important for several reasons. Many applications require predictions in non-human organisms (agricultural genomics, model organism research, comparative genomics). Multi-species training may improve within-species performance by providing additional evolutionary signal (<a href="p2-ch08-pretraining.html#sec-ch08-multispecies" class="quarto-xref"><span>Section 8.8.3</span></a>). The ability to transfer across species indicates that models have learned general principles of genome organization rather than species-specific artifacts.</p>
<p>Cross-species benchmarks typically evaluate models on held-out species not seen during training. Performance degradation from training to held-out species indicates the degree to which learned representations depend on species-specific features. Some architectures show better cross-species transfer than others, suggesting differences in how well they capture conserved regulatory principles.</p>
</section>
</section>
<section id="sec-ch11-vep-benchmarks" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="sec-ch11-vep-benchmarks"><span class="header-section-number">11.3</span> Variant Effect Prediction Benchmarks</h2>
<p><strong>Variant effect prediction</strong> (VEP) benchmarks connect sequence changes to molecular or phenotypic consequences, addressing the clinically central question of which variants matter. These benchmarks span multiple biological levels, from molecular function to clinical pathogenicity.</p>
<section id="sec-ch11-clinical-databases" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="sec-ch11-clinical-databases"><span class="header-section-number">11.3.1</span> Clinical Variant Databases</h3>
<p>ClinVar provides the most widely used labels for clinical variant effect prediction, aggregating pathogenicity assertions from clinical laboratories and researchers worldwide (<a href="../part_1/p1-ch02-data.html#sec-ch02-clinvar" class="quarto-xref"><span>Section 2.8.1</span></a>). Benchmarks derived from ClinVar frame variant interpretation as classification: given a variant, predict whether it is pathogenic, likely pathogenic, benign, or likely benign.</p>
<p>ClinVar’s value as a benchmark stems from its clinical relevance. Variants classified in ClinVar represent the actual population of variants encountered in clinical testing. Performance on ClinVar directly addresses whether a model can assist variant interpretation workflows. The database’s scale (over 2 million variant submissions as of 2024) enables statistically robust evaluation <span class="citation" data-cites="landrum_clinvar_2018">(<a href="../bib/references.html#ref-landrum_clinvar_2018" role="doc-biblioref">Landrum et al. 2018</a>)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Critical Limitation: ClinVar Circularity
</div>
</div>
<div class="callout-body-container callout-body">
<p>ClinVar’s limitations as a benchmark are equally important. Submission heterogeneity means that label quality varies dramatically: expert-curated panels provide high-confidence classifications while single-laboratory submissions may reflect limited evidence. Version sensitivity means that benchmark composition changes over time as new submissions arrive and old classifications are updated. Most consequentially, <strong>circularity with computational predictors creates feedback loops</strong>: variants may have been classified using the very tools being evaluated, inflating apparent performance. This circularity problem, examined in detail for classical predictors in <a href="../part_1/p1-ch04-vep-classical.html#sec-ch04-circularity" class="quarto-xref"><span>Section 4.5</span></a> and for its broader confounding implications in <span class="quarto-unresolved-ref">?sec-ch22-label-bias</span>, represents one of the most insidious forms of benchmark contamination.</p>
</div>
</div>
<p>Ancestry and gene coverage biases profoundly shape what ClinVar benchmarks measure. Variants from European ancestry individuals and well-studied disease genes are heavily overrepresented. High performance on ClinVar demonstrates accuracy for this specific population rather than robust generalization across human genetic diversity (<a href="../part_1/p1-ch03-gwas.html#sec-ch03-portability" class="quarto-xref"><span>Section 3.7</span></a>). Benchmarks stratified by ancestry reveal substantial performance gaps, with models typically performing worse on variants from underrepresented populations.</p>
<p>Best practices for using ClinVar as a benchmark include specifying the exact database version and download date, excluding variants with conflicting assertions, stratifying performance by evidence level and ancestry, and comparing to baselines using only allele frequency to detect circularity. These practices are detailed in <a href="#sec-ch11-leakage-detection" class="quarto-xref"><span>Section 12.4</span></a>, with specific guidance on detecting label leakage in <a href="#sec-ch11-label-leakage" class="quarto-xref"><span>Section 12.4.1</span></a>.</p>
</section>
<section id="sec-ch11-cagi" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="sec-ch11-cagi"><span class="header-section-number">11.3.2</span> CAGI: Critical Assessment of Genome Interpretation</h3>
<p>The Critical Assessment of Genome Interpretation (CAGI) challenges provide prospective evaluation of variant effect predictors on unpublished datasets <span class="citation" data-cites="hoskins_cagi6_2023">(<a href="../bib/references.html#ref-hoskins_cagi6_2023" role="doc-biblioref"><strong>hoskins_cagi6_2023?</strong></a>)</span>. Unlike retrospective benchmarks that evaluate models on historical data, CAGI distributes prediction targets before ground truth is available, preventing any possibility of overfitting to known labels.</p>
<p>CAGI challenges cover diverse prediction targets. Some challenges focus on molecular phenotypes: predicting the effect of variants on protein stability, binding affinity, or enzymatic activity. Others target clinical phenotypes: predicting disease risk, drug response, or clinical severity from individual genomes. The diversity of challenges tests whether models generalize across different types of variant effects.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: The Value of Prospective Evaluation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The prospective design of CAGI provides several crucial advantages over retrospective benchmarks:</p>
<ol type="1">
<li><strong>Predictions must be made before labels are known</strong>, eliminating leakage from any source</li>
<li><strong>The timeline forces models to commit</strong> to predictions rather than post-hoc optimization</li>
<li><strong>Community participation enables fair comparison</strong> across many approaches under identical conditions</li>
</ol>
<p>This prospective design represents the gold standard for benchmark validity. When evaluating any retrospective benchmark result, ask: “Would this performance hold up in a CAGI-style prospective evaluation?”</p>
</div>
</div>
<p>CAGI’s limitation is scale: challenges include hundreds to thousands of variants rather than the millions available in databases like ClinVar. Statistical power to detect small performance differences is correspondingly limited. The challenges also depend on experimental collaborators willing to withhold data until after the prediction deadline, limiting the range of phenotypes that can be assessed.</p>
</section>
<section id="sec-ch11-dms-benchmarks" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="sec-ch11-dms-benchmarks"><span class="header-section-number">11.3.3</span> Deep Mutational Scanning Benchmarks</h3>
<p>Deep mutational scanning (DMS) provides systematic experimental measurement of variant effects across entire proteins or regulatory elements (<a href="../part_1/p1-ch02-data.html#sec-ch02-dms" class="quarto-xref"><span>Section 2.4.4</span></a>). DMS benchmarks test whether models can predict these experimentally determined effects, providing direct validation against measured functional consequences rather than inferred clinical classifications.</p>
<p>MaveDB aggregates DMS datasets in a standardized format, enabling systematic benchmarking across diverse proteins and assays <span class="citation" data-cites="esposito_mavedb_2019">(<a href="../bib/references.html#ref-esposito_mavedb_2019" role="doc-biblioref"><strong>esposito_mavedb_2019?</strong></a>)</span>. <em>ProteinGym’s</em> DMS component (discussed above) represents the most comprehensive benchmark in this space. For non-coding variants, MPRA datasets provide analogous systematic measurements of regulatory activity.</p>
<p>DMS benchmarks have distinct strengths and limitations compared to clinical databases. The experimental grounding means that labels reflect actual measured effects rather than clinical inference that may involve multiple assumptions. The relationship between DMS fitness and clinical pathogenicity is complex: a variant may substantially affect enzymatic activity without causing disease if the residual activity suffices for normal physiology. DMS benchmarks measure one component of the variant interpretation puzzle rather than the full clinical picture.</p>
</section>
<section id="sec-ch11-noncoding-benchmarks" class="level3" data-number="11.3.4">
<h3 data-number="11.3.4" class="anchored" data-anchor-id="sec-ch11-noncoding-benchmarks"><span class="header-section-number">11.3.4</span> Regulatory and Non-Coding Variant Benchmarks</h3>
<p>Non-coding variants require specialized benchmarks because their effects operate through different mechanisms than coding variants. The foundation model approaches to non-coding variant effect prediction are examined in <span class="quarto-unresolved-ref">?sec-ch14-dna-vep</span>, with the underlying regulatory models detailed in <a href="../part_3/p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>. MPRA-based benchmarks test whether models can predict the quantitative effect of variants on enhancer or promoter activity measured in reporter assays. <strong>Expression quantitative trait locus</strong> (eQTL)-based benchmarks use naturally occurring variants associated with expression changes, treating the statistical evidence for eQTL status as a proxy for regulatory impact.</p>
<p>The challenge for non-coding benchmarks is connecting molecular effects to phenotypic consequences. A variant may alter chromatin accessibility without affecting any gene’s expression. A variant may affect expression without influencing disease risk. This gap between molecular and clinical effects complicates interpretation: high performance on MPRA prediction does not necessarily translate to accurate regulatory disease variant interpretation.</p>
<p>Fine-mapped <strong>genome-wide association study</strong> (GWAS) variants provide another benchmark source for non-coding VEP. Statistical <strong>fine-mapping</strong> identifies putatively causal variants within associated loci (<a href="../part_1/p1-ch03-gwas.html#sec-ch03-fine-mapping" class="quarto-xref"><span>Section 3.4</span></a>), and models can be evaluated on their ability to prioritize these variants over nearby non-causal variants. Performance on fine-mapping tasks more directly assesses clinical relevance than molecular phenotype prediction, though fine-mapping itself has substantial uncertainty.</p>
<div id="tbl-vep-benchmarks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-vep-benchmarks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;11.2: Comparison of variant effect prediction benchmark types. Each provides different evidence about model capabilities, with distinct validity trade-offs.
</figcaption>
<div aria-describedby="tbl-vep-benchmarks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 21%">
<col style="width: 17%">
<col style="width: 20%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>Benchmark Type</th>
<th>Label Source</th>
<th>Strengths</th>
<th>Limitations</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ClinVar</td>
<td>Clinical assertions</td>
<td>Clinical relevance; scale</td>
<td>Circularity; ancestry bias; heterogeneous quality</td>
<td>Clinical deployment validation (with caveats)</td>
</tr>
<tr class="even">
<td>CAGI</td>
<td>Prospective experiments</td>
<td>No leakage possible; forces commitment</td>
<td>Limited scale; infrequent</td>
<td>Gold-standard validation</td>
</tr>
<tr class="odd">
<td>DMS/MaveDB</td>
<td>High-throughput assays</td>
<td>Direct experimental measurement</td>
<td>Assay-specific; fitness != pathogenicity</td>
<td>Molecular mechanism understanding</td>
</tr>
<tr class="even">
<td>MPRA</td>
<td>Reporter assays</td>
<td>Quantitative; regulatory focus</td>
<td>Reporter != endogenous; context-dependent</td>
<td>Regulatory variant effects</td>
</tr>
<tr class="odd">
<td>eQTL/GWAS</td>
<td>Statistical associations</td>
<td>Population-level evidence</td>
<td>Correlation != causation; LD confounding</td>
<td>Common variant prioritization</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="sec-ch11-trait-benchmarks" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="sec-ch11-trait-benchmarks"><span class="header-section-number">11.4</span> Trait and Population-Level Benchmarks</h2>
<p>At the individual and population level, benchmarks assess whether models improve prediction of complex traits and disease risk.</p>
<section id="sec-ch11-pgs-evaluation" class="level3" data-number="11.4.1">
<h3 data-number="11.4.1" class="anchored" data-anchor-id="sec-ch11-pgs-evaluation"><span class="header-section-number">11.4.1</span> Polygenic Score Evaluation</h3>
<p><strong>Polygenic score</strong> (PGS) benchmarks evaluate how well genotype-derived scores predict disease risk or quantitative traits (<a href="../part_1/p1-ch03-gwas.html#sec-ch03-pgs-construction" class="quarto-xref"><span>Section 3.5</span></a>). Common evaluation settings include within-biobank evaluation, where a single large cohort is partitioned into training and test sets, and cross-biobank evaluation, where models trained in one population are tested in another. The integration of foundation model features with PGS approaches represents an emerging research direction (<a href="../part_6/p6-ch27-clinical-risk.html#sec-ch27-pgs-to-fm" class="quarto-xref"><span>Section 27.1</span></a>).</p>
<p>Metrics depend on the phenotype. For quantitative traits, benchmarks report the coefficient of determination (<span class="math inline">\(R^2\)</span>) or incremental <span class="math inline">\(R^2\)</span> over non-genetic covariates. For binary disease outcomes, auROC and area under the precision-<strong>recall</strong> curve (auPRC) quantify discrimination. Calibration metrics assess whether predicted risks match observed event rates (<a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-calibration" class="quarto-xref"><span>Section 23.2</span></a>). The clinical utility of PGS, discussed in <a href="../part_6/p6-ch27-clinical-risk.html" class="quarto-xref"><span>Chapter 27</span></a>, depends on all these properties: a score may discriminate well (high auROC) while being poorly calibrated (predicted risks do not match actual event rates).</p>
<p>Cross-population evaluation is particularly important because PGS portability is a major limitation of current methods (<a href="../part_1/p1-ch03-gwas.html#sec-ch03-portability" class="quarto-xref"><span>Section 3.7</span></a>). Benchmarks stratified by ancestry typically reveal substantial performance degradation from European ancestry (where most GWAS have been conducted) to other populations. This degradation stems from multiple sources: different <strong>linkage disequilibrium</strong> patterns mean that tag SNPs identify different causal variants, population-specific variants are absent from training data, and effect sizes may differ across populations due to gene-environment interactions.</p>
</section>
<section id="sec-ch11-traitgym" class="level3" data-number="11.4.2">
<h3 data-number="11.4.2" class="anchored" data-anchor-id="sec-ch11-traitgym"><span class="header-section-number">11.4.2</span> TraitGym</h3>
<p><em>TraitGym</em> provides a framework specifically designed to assess complex trait prediction using genomic foundation models <span class="citation" data-cites="yan_traitgym_2024">(<a href="../bib/references.html#ref-yan_traitgym_2024" role="doc-biblioref"><strong>yan_traitgym_2024?</strong></a>)</span>. The benchmark evaluates whether foundation model embeddings or variant scores improve prediction beyond traditional polygenic score methods.</p>
<p><em>TraitGym’s</em> design addresses several limitations of standard PGS benchmarks. Ancestry stratification is built into the evaluation protocol, requiring models to report performance separately for different population groups. Multiple phenotypes spanning different genetic architectures (highly polygenic versus more oligogenic) test generalization across trait types. Comparison to appropriate baselines (standard PGS methods, clinical covariates alone) isolates the contribution of foundation model features.</p>
<p>The benchmark is particularly relevant for assessing claims that genomic foundation models add predictive value beyond classical statistical genetics. Foundation models incur substantial computational costs compared to linear PGS models; <em>TraitGym</em> helps determine whether these costs are justified by improved prediction.</p>
</section>
<section id="sec-ch11-embedgem" class="level3" data-number="11.4.3">
<h3 data-number="11.4.3" class="anchored" data-anchor-id="sec-ch11-embedgem"><span class="header-section-number">11.4.3</span> EmbedGEM Framework</h3>
<p>The <em>EmbedGEM</em> framework evaluates whether foundation model embeddings capture biologically meaningful genetic signal, as opposed to technical artifacts or confounders <span class="citation" data-cites="mukherjee_embedgem_2024">(<a href="../bib/references.html#ref-mukherjee_embedgem_2024" role="doc-biblioref">Mukherjee et al. 2024</a>)</span>. The framework assesses embeddings along two axes: <strong>heritability</strong> and disease relevance.</p>
<p>The heritability axis measures how much genetic signal an embedding captures. <em>EmbedGEM</em> counts the number of genome-wide significant loci associated with embedding components and quantifies the strength of association through mean chi-squared statistics. Higher values indicate that the embedding reflects heritable biology rather than noise.</p>
<p>The disease relevance axis measures whether embedding-associated variants predict clinically meaningful outcomes. Polygenic scores constructed from embedding GWAS hits are evaluated for their ability to predict disease in independent cohorts. Incremental predictive value over standard clinical models indicates that the embedding captures disease-relevant genetic information.</p>
<p>This two-axis evaluation addresses a critical question for foundation model deployment: do learned representations discover novel biology or merely recapitulate known associations with additional computational overhead? Embeddings that show high heritability but low disease relevance may capture biological signal that is not clinically actionable. Embeddings that show disease relevance without novel genetic discoveries may not add value beyond existing PGS methods.</p>
</section>
</section>
<section id="sec-ch11-benchmark-construction" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="sec-ch11-benchmark-construction"><span class="header-section-number">11.5</span> Benchmark Construction and Hidden Assumptions</h2>
<p>Beyond cataloging benchmark suites, understanding how benchmarks are constructed reveals assumptions that shape what they measure and what they miss.</p>
<section id="sec-ch11-label-provenance" class="level3" data-number="11.5.1">
<h3 data-number="11.5.1" class="anchored" data-anchor-id="sec-ch11-label-provenance"><span class="header-section-number">11.5.1</span> Data Sources and Label Provenance</h3>
<p>Benchmark labels derive from diverse sources with different properties. Experimental assays (ChIP-seq, DMS, MPRA) provide direct measurements but are limited by assay-specific artifacts and selection pressures. Computational annotations (gene calls, functional predictions, conservation scores) provide broader coverage but introduce circular dependencies if models are trained and evaluated on overlapping sources. Clinical classifications aggregate expert judgment but reflect the evidence available at classification time, which may include the very predictors being benchmarked.</p>
<p>The provenance of benchmark labels determines what success on that benchmark actually means. High performance on experimentally derived labels suggests the model captures the specific molecular process assayed. High performance on clinical labels may indicate genuine clinical utility or may reflect circularity with existing prediction tools. Understanding label provenance is prerequisite to interpreting benchmark results.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think: Label Provenance
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a variant effect predictor that achieves 0.95 auROC on a ClinVar benchmark. Before interpreting this result, you should ask:</p>
<ol type="1">
<li>What evidence types contributed to the ClinVar classifications? (Functional assays? Segregation? Computational predictions?)</li>
<li>Did any of those computational predictions use similar features to your model?</li>
<li>How would you detect whether circularity inflated your performance?</li>
</ol>
<p><em>These questions apply to any benchmark with aggregated or curated labels.</em></p>
</div>
</div>
</section>
<section id="sec-ch11-splitting" class="level3" data-number="11.5.2">
<h3 data-number="11.5.2" class="anchored" data-anchor-id="sec-ch11-splitting"><span class="header-section-number">11.5.2</span> Splitting Strategies and Leakage</h3>
<p>How benchmarks partition data into training and test sets determines whether evaluation measures generalization or memorization (<a href="#sec-ch11-eval" class="quarto-xref"><span>Chapter 12</span></a>). Random splitting, where examples are assigned to splits uniformly at random, represents the weakest form of evaluation. In genomics, random splits often permit homology-based leakage: training and test sequences may share sufficient similarity that memorization suffices for good performance.</p>
<p>Homology-aware splitting clusters sequences by similarity before assigning clusters to splits, ensuring that test sequences are evolutionarily distant from training sequences. This approach is standard for protein benchmarks (using tools like <code>CD-HIT</code> or <code>MMseqs2</code>) but less consistently applied for DNA benchmarks.</p>
<p>Chromosome-based splitting holds out entire chromosomes for testing, preventing any position-based leakage within chromosomes. This approach is common for regulatory benchmarks but does not account for homologous sequences on different chromosomes. Temporal splitting reserves recent data for testing, appropriate when benchmarks derive from databases with submission timestamps. Each splitting strategy tests different aspects of generalization; the choice should match the intended deployment scenario.</p>
<div id="fig-leakage-pathways" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-leakage-pathways-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch11/02-fig-leakage-pathways.svg" class="img-fluid figure-img"></p>
<figcaption>Data leakage pathways in genomic foundation model evaluation</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-leakage-pathways-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.2: Data leakage pathways in genomic foundation model evaluation. The legitimate pipeline (blue) flows from pretraining through fine-tuning to benchmark evaluation. Leakage pathways (red dashed) create spurious performance: direct overlap when pretraining includes benchmark sequences; homology leakage when training and test sets share high-similarity sequences; label circularity when computational predictions influence ground truth labels that later serve as training targets; resource sharing when databases like ENCODE appear in both pretraining and evaluation; and community iteration when the field collectively overfits to standard benchmarks through publication cycles. Each pathway inflates apparent performance without improving genuine biological understanding.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch11-metrics" class="level3" data-number="11.5.3">
<h3 data-number="11.5.3" class="anchored" data-anchor-id="sec-ch11-metrics"><span class="header-section-number">11.5.3</span> Metric Selection and Aggregation</h3>
<p>Benchmark metrics determine what aspects of model performance are measured. Discrimination metrics (auROC, auPRC, correlation) assess whether models rank predictions correctly. Calibration metrics (expected calibration error, reliability diagrams) assess whether predicted probabilities match observed frequencies (<a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-calibration" class="quarto-xref"><span>Section 23.2</span></a>). Clinical utility metrics (net benefit, decision curves) assess whether predictions improve decisions compared to treating all patients the same (<a href="../part_6/p6-ch27-clinical-risk.html" class="quarto-xref"><span>Chapter 27</span></a>).</p>
<p>Different metrics can yield different rankings of models. A model with superior discrimination may have poor calibration, predicting the right relative order but wrong absolute probabilities. Choosing which metric to optimize, and how to aggregate across multiple tasks or datasets, involves implicit decisions about what matters for downstream use.</p>
<p>Aggregation across tasks raises additional issues. Mean performance across many tasks weights each task equally, regardless of clinical importance or dataset quality. Median performance is robust to outliers but obscures variation. Reporting full distributions of task-level performance provides more information but complicates comparison. The choice of aggregation method can substantially affect which model appears best.</p>
</section>
<section id="sec-ch11-goodhart" class="level3" data-number="11.5.4">
<h3 data-number="11.5.4" class="anchored" data-anchor-id="sec-ch11-goodhart"><span class="header-section-number">11.5.4</span> Goodhart’s Law and Benchmark Gaming</h3>
<p>Benchmarks create incentive structures, and incentive structures invite optimization. Goodhart’s Law, that a measure ceases to be a good measure once it becomes a target, applies with particular force to machine learning evaluation. When model development prioritizes leaderboard position, the benchmark becomes the optimization target rather than a proxy for the underlying capability it was designed to measure.</p>
<p>Gaming takes multiple forms in genomic AI. Architectural choices may be tuned specifically to benchmark characteristics: receptive fields sized to match benchmark sequence lengths, output heads designed for benchmark label distributions, hyperparameters selected through extensive benchmark-specific search. Such tuning improves benchmark performance without necessarily improving generalization to deployment scenarios that differ from benchmark conditions.</p>
<p>More subtle gaming arises from selective reporting. Models may be evaluated on many benchmarks with only favorable results published. Benchmark versions may be chosen to maximize apparent performance. Evaluation protocols may deviate from published standards in ways that inflate metrics. The cumulative effect is a literature where reported performance systematically overestimates deployment capability.</p>
<p>The circularity between predictors and databases creates particularly insidious gaming dynamics. When ClinVar classifications incorporate computational predictions, and those predictions are then benchmarked against ClinVar, the benchmark rewards models that resemble their predecessors rather than models that provide independent information (<a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>). This circularity is rarely acknowledged in benchmark reporting, yet it fundamentally compromises the validity of performance claims.</p>
<p>Mitigating gaming requires structural changes to evaluation practice: prospective benchmarks like CAGI where predictions precede labels, held-out evaluation consortia that resist optimization pressure, and reporting standards that require disclosure of all benchmarks attempted rather than only those where performance was favorable. The field’s maturation depends on developing evaluation cultures that reward honest assessment over leaderboard position.</p>
</section>
</section>
<section id="sec-ch11-saturation-staleness" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="sec-ch11-saturation-staleness"><span class="header-section-number">11.6</span> Benchmark Saturation and Staleness</h2>
<p>Benchmarks have finite useful lifetimes. As models improve, benchmarks saturate; as data and methods evolve, benchmarks become stale.</p>
<section id="sec-ch11-saturation" class="level3" data-number="11.6.1">
<h3 data-number="11.6.1" class="anchored" data-anchor-id="sec-ch11-saturation"><span class="header-section-number">11.6.1</span> Saturation: When Benchmarks Stop Discriminating</h3>
<p>A benchmark saturates when the best models achieve performance that cannot be meaningfully improved. Saturation may reflect fundamental limits (the benchmark approaches the Bayes error rate), measurement noise (the benchmark’s labels are too noisy to support finer discrimination), or ceiling effects (the metric itself cannot distinguish between excellent and perfect performance).</p>
<p>Saturation is problematic because it removes the benchmark’s value for model selection. When all reasonable models achieve 0.97 auROC, differences between 0.970 and 0.975 are unlikely to reflect meaningful capability differences. Yet benchmark reporting conventions often emphasize such decimal places, creating an illusion of progress.</p>
<p>Detecting saturation requires estimating the irreducible error. For benchmarks with replicate measurements, comparing model performance to replicate concordance provides an upper bound: models cannot systematically outperform the reproducibility of the underlying assay. For benchmarks without replicates, saturation is harder to diagnose. One heuristic is tracking the rate of improvement: when new methods provide diminishing gains despite substantial architectural innovations, saturation is likely.</p>
<p>The response to saturation should be moving to harder benchmarks that still discriminate between methods, developing new benchmarks that capture aspects of performance that existing benchmarks miss, and retiring saturated benchmarks from active leaderboard competition while retaining them as sanity checks.</p>
<div id="fig-benchmark-saturation" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-benchmark-saturation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch11/03-A-fig-benchmark-saturation.svg" class="img-fluid figure-img"></p>
<figcaption>Benchmark saturation over time</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch11/03-B-fig-benchmark-saturation.svg" class="img-fluid figure-img"></p>
<figcaption>Benchmark staleness timeline</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-benchmark-saturation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.3: Benchmark saturation and staleness limit evaluation validity. (A) Performance saturation: multiple model generations converge toward benchmark ceilings where marginal improvements no longer indicate meaningful advances. The saturation zone (shaded) suggests benchmarks have lost discriminative power. (B) Benchmark staleness: growing temporal gaps between benchmark creation and current evaluation. Benchmarks created years ago may reflect outdated annotations, superseded biological understanding, or distributions that no longer match contemporary data. Together, saturation and staleness motivate development of new benchmarks that capture aspects of biological prediction that current standards miss.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch11-staleness" class="level3" data-number="11.6.2">
<h3 data-number="11.6.2" class="anchored" data-anchor-id="sec-ch11-staleness"><span class="header-section-number">11.6.2</span> Staleness: When Benchmarks Diverge from Practice</h3>
<p>Benchmarks become stale when they no longer reflect current data, methods, or clinical practice. Assays evolve: a benchmark constructed from early ENCODE data may not represent current experimental protocols. Annotations improve: gene models, variant classifications, and functional element maps are continuously updated. Clinical practice shifts: treatment guidelines and diagnostic criteria change the meaning of historical labels.</p>
<p>Staleness is insidious because it erodes benchmark validity gradually rather than abruptly. A benchmark that accurately represented regulatory prediction in 2015 may systematically misrepresent it in 2025, yet the benchmark’s continued use perpetuates optimization for an outdated target.</p>
<p>Addressing staleness requires periodic benchmark refresh with updated data and annotations, version control that documents exactly what each benchmark version contains, and awareness that performance on historical benchmarks may not predict performance on current data.</p>
</section>
<section id="sec-ch11-leakage-scale" class="level3" data-number="11.6.3">
<h3 data-number="11.6.3" class="anchored" data-anchor-id="sec-ch11-leakage-scale"><span class="header-section-number">11.6.3</span> Leakage from Scale</h3>
<p>Modern foundation models are pretrained on corpora that may include most publicly available genomic data. This creates novel leakage risks distinct from classical train-test overlap. A model pretrained on all ENCODE data may effectively have seen the exact experiments used in many regulatory benchmarks. A model pretrained on all UniRef may have seen sequences highly similar to protein benchmark test sets. This pretraining-benchmark overlap inflates performance in ways that are difficult to detect and even more difficult to correct.</p>
<p>Leakage from scale is particularly problematic because it is often undocumented. Model papers rarely enumerate exactly which datasets were included in pretraining corpora, and benchmark papers rarely specify which datasets should be excluded. The result is ambiguity about whether benchmark success reflects genuine generalization or memorization from pretraining.</p>
<p>Mitigating leakage from scale requires explicit documentation of pretraining corpora, tools or hashes that help identify overlap between pretraining data and benchmark test sets, and held-out evaluation consortia that reserve data specifically for assessment without any use in pretraining.</p>
</section>
</section>
<section id="sec-ch11-deployment-gap" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="sec-ch11-deployment-gap"><span class="header-section-number">11.7</span> Benchmark-Deployment Gap</h2>
<p>High benchmark performance does not guarantee deployment success. Understanding why requires examining the systematic differences between benchmark settings and real-world applications.</p>
<div id="fig-proxy-target-gap" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-proxy-target-gap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch11/04-fig-proxy-target-gap.svg" class="img-fluid figure-img"></p>
<figcaption>The proxy-target gap in genomic AI evaluation</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-proxy-target-gap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.4: The proxy-target gap in genomic AI evaluation. What benchmarks measure (left) differs systematically from what we ultimately want (right). ClinVar labels proxy for clinical impact but reflect curation biases and circularity with computational predictions. Held-out auROC proxies for deployment discrimination but assumes matched distributions. DMS correlations proxy for protein function but capture only selected perturbations in specific assay conditions. Expression prediction accuracy proxies for regulatory understanding but may reflect technical confounds. Arrow widths indicate proxy strength; gap annotations identify sources of misalignment. The central insight: high benchmark performance does not guarantee deployment success.
</figcaption>
</figure>
</div>
<section id="sec-ch11-distribution-shift" class="level3" data-number="11.7.1">
<h3 data-number="11.7.1" class="anchored" data-anchor-id="sec-ch11-distribution-shift"><span class="header-section-number">11.7.1</span> Distribution Shift</h3>
<p>Benchmark test sets sample from the same distribution as training sets. Deployment populations may differ systematically. For variant effect prediction, benchmark variants are typically common enough to appear in multiple databases, while deployment often targets rare variants seen in single individuals. For regulatory prediction, benchmarks derive from well-studied cell types and tissues, while deployment may require prediction in understudied contexts.</p>
<p><strong>Distribution shift</strong> manifests as degraded performance, but the pattern of degradation varies. The transfer learning framework in <span class="quarto-unresolved-ref">?sec-ch09-domain-shift</span> examines how models handle distribution shift from a methodological perspective, while <a href="#sec-ch11-distribution-shift" class="quarto-xref"><span>Section 11.7.1</span></a> addresses the confounding implications when shift correlates with protected attributes. Some models degrade gracefully, maintaining reasonable accuracy across the distribution shift. Others degrade catastrophically, with confident predictions that prove systematically wrong. Benchmarks that include held-out subpopulations or <strong>out-of-distribution</strong> test sets provide some information about robustness, but cannot anticipate every deployment scenario.</p>
</section>
<section id="sec-ch11-calibration-requirements" class="level3" data-number="11.7.2">
<h3 data-number="11.7.2" class="anchored" data-anchor-id="sec-ch11-calibration-requirements"><span class="header-section-number">11.7.2</span> Calibration Requirements</h3>
<p>Clinical deployment requires not just accurate rankings but accurate probability estimates (<a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-calibration" class="quarto-xref"><span>Section 23.2</span></a>). A variant classifier that achieves 0.95 auROC by assigning probability 0.9 to all pathogenic variants and 0.3 to all benign variants discriminates well but provides miscalibrated uncertainty. Clinical decisions that depend on thresholded predictions (reporting variants above a certain probability) will perform poorly if those probabilities do not reflect actual pathogenicity rates.</p>
<p>Most benchmark metrics emphasize discrimination over calibration. auROC is invariant to monotonic transformations of predicted probabilities. Correlation measures rank preservation. As a result, models may be optimized for benchmark success through strategies that damage calibration. The benchmark-deployment gap for calibration can be large even when discrimination metrics are excellent.</p>
</section>
<section id="sec-ch11-metric-mismatch" class="level3" data-number="11.7.3">
<h3 data-number="11.7.3" class="anchored" data-anchor-id="sec-ch11-metric-mismatch"><span class="header-section-number">11.7.3</span> Metric Mismatch</h3>
<p>Benchmarks optimize specific metrics that may not align with deployment objectives. auROC weights errors equally regardless of where they occur on the score distribution, but clinical utility may depend primarily on performance at specific operating points. Correlation rewards getting the overall pattern right but may not penalize systematic errors in clinically important regions.</p>
<p>The gap between optimized metrics and deployment objectives creates misaligned incentives. Model developers optimize for benchmark success, which rewards specific metric improvements. Deployment success may require different tradeoffs: prioritizing calibration over discrimination, minimizing false negatives over false positives, or performing well on specific subpopulations rather than overall.</p>
</section>
<section id="sec-ch11-practical-constraints" class="level3" data-number="11.7.4">
<h3 data-number="11.7.4" class="anchored" data-anchor-id="sec-ch11-practical-constraints"><span class="header-section-number">11.7.4</span> Practical Constraints</h3>
<p>Deployment environments impose constraints that benchmarks typically ignore. Inference speed matters when predictions must be returned in clinical timescales. Model size matters when deployment hardware has limited memory. <strong>Interpretability</strong> matters when predictions must be explained to clinicians or patients (<a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 25</span></a>). Benchmarks that evaluate only accuracy miss these dimensions of deployment fitness.</p>
<p>The benchmark-deployment gap is not merely a technical inconvenience. It represents a fundamental tension between evaluation tractability and deployment validity. Benchmarks are valuable precisely because they are standardized, reproducible, and comparable across methods. Deployment is valuable precisely because it addresses the specific needs of real-world applications. Bridging this gap requires benchmark designs that better approximate deployment conditions and deployment evaluations that provide feedback to benchmark development.</p>
</section>
</section>
<section id="sec-ch11-systematic-gaps" class="level2" data-number="11.8">
<h2 data-number="11.8" class="anchored" data-anchor-id="sec-ch11-systematic-gaps"><span class="header-section-number">11.8</span> Systematic Gaps in Current Benchmarks</h2>
<p>Despite the proliferation of benchmark suites, systematic gaps remain in the genomic evaluation landscape.</p>
<p>Variant types remain inadequately covered: structural variants, inversions, copy number variants, and complex rearrangements are rarely evaluated despite accounting for substantial genomic variation and disease burden (<a href="../part_1/p1-ch01-ngs.html#sec-ch01-complex" class="quarto-xref"><span>Section 1.5.4</span></a>). Repeat regions are often excluded or masked. Multi-variant effects and haplotype-specific phenomena receive minimal attention; the phasing challenges that underlie compound heterozygosity interpretation (<a href="../part_1/p1-ch01-ngs.html#sec-ch01-phasing-importance" class="quarto-xref"><span>Section 1.4.1</span></a>) rarely appear in benchmark settings.</p>
<p>Population representation shows profound disparities: non-European ancestry groups remain severely underrepresented (<a href="../part_1/p1-ch03-gwas.html#sec-ch03-portability" class="quarto-xref"><span>Section 3.7</span></a>). The confounding implications of this underrepresentation extend beyond benchmark validity to fairness concerns examined in <span class="quarto-unresolved-ref">?sec-ch22-fairness</span>. Performance stratified by ancestry reveals gaps that aggregate metrics conceal. Environmental diversity (lifestyle, exposures, treatments) that shapes phenotypic expression is rarely incorporated.</p>
<div id="fig-cross-population-performance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cross-population-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch11/05-fig-cross-population-performance.svg" class="img-fluid figure-img"></p>
<figcaption>Cross-population performance reveals systematic failures</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cross-population-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.5: Cross-population performance reveals systematic failures for underrepresented ancestries. Relative performance (European as reference) degrades substantially for non-European groups across multiple model types: polygenic risk scores (blue), variant classifiers (green), and regulatory predictors (orange). African-ancestry individuals show 40-75% performance reductions despite constituting only 2% of typical training data while representing 16% of the global population. These disparities reflect both training data composition (inset: ~78% European) and fundamental differences in linkage disequilibrium structure across populations. Aggregate benchmark metrics that do not report stratified performance by ancestry conceal these failures.
</figcaption>
</figure>
</div>
<p>Modality coverage remains uneven: long-read sequencing data is scarce in benchmarks despite its advantages for structural variants and phasing (<a href="../part_1/p1-ch01-ngs.html#sec-ch01-longread" class="quarto-xref"><span>Section 1.2.4</span></a>). Single-cell benchmarks are emerging but remain limited compared to bulk assay benchmarks; the evaluation challenges specific to single-cell models are examined in <span class="quarto-unresolved-ref">?sec-ch16-evaluation</span>. Spatial transcriptomics and other emerging modalities have minimal coverage, though multi-omic integration approaches (<a href="../part_4/p4-ch22-multi-omics.html" class="quarto-xref"><span>Chapter 22</span></a>) are beginning to address cross-modality assessment.</p>
<p>Clinical endpoints are underrepresented: most benchmarks use molecular surrogates rather than hard clinical endpoints. Disease incidence, progression, treatment response, and patient-reported outcomes are rarely the direct prediction target. The gap between molecular proxy accuracy and clinical utility remains poorly characterized.</p>
<p>These gaps mean that strong benchmark performance may not predict utility for underserved populations, understudied variant classes, or clinical applications that depend on endpoints the benchmarks do not measure.</p>
</section>
<section id="sec-ch11-proxy-problem" class="level2" data-number="11.9">
<h2 data-number="11.9" class="anchored" data-anchor-id="sec-ch11-proxy-problem"><span class="header-section-number">11.9</span> The Proxy Problem</h2>
<p>Benchmarks structure the incentives of genomic AI development. The specific tasks, metrics, and leaderboards that the community adopts determine what models are optimized for, what claims of progress are evaluated against, and what capabilities receive attention versus neglect. A benchmark that emphasizes European-ancestry variants produces models tuned for European-ancestry performance. A benchmark that rewards discrimination (auROC) over calibration produces models that rank variants well but estimate probabilities poorly. A benchmark that reuses training data from widely available resources creates indirect leakage that inflates apparent performance. The benchmark landscape is not neutral infrastructure but an active force shaping what the field builds.</p>
<p>The landscape surveyed here spans protein benchmarks (<em>TAPE</em>, <em>FLIP</em>, <em>ProteinGym</em>), DNA and regulatory benchmarks (<em>Genomic Benchmarks</em>, <em>BEND</em>), variant effect benchmarks (ClinVar, CAGI, DMS), and trait-level benchmarks (<em>TraitGym</em>, <em>EmbedGEM</em>). Across all categories, persistent challenges emerge: saturation that reduces discriminative power as models approach ceiling performance, staleness that erodes validity as benchmarks age, leakage risks that inflate apparent capabilities, and systematic gaps in population diversity, variant type coverage, and clinical endpoint representation.</p>
<p>The benchmark-deployment gap represents perhaps the most consequential limitation. Strong performance on established benchmarks does not guarantee that models will behave reliably when deployed in clinical or research settings with different data distributions, patient populations, or outcome definitions. Proper benchmark use requires attention to experiment design, metric selection, and common pitfalls (<a href="#sec-ch11-eval" class="quarto-xref"><span>Chapter 12</span></a>). The confounding issues that plague both benchmark construction and model training receive dedicated treatment in <a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>, while uncertainty quantification methods (<a href="../part_5/p5-ch23-uncertainty.html" class="quarto-xref"><span>Chapter 23</span></a>) provide tools for assessing when benchmark performance translates to deployment confidence. Interpretability approaches (<a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 25</span></a>) reveal whether benchmark success reflects genuine biological learning or exploitation of shortcuts. Together with this catalog of what benchmarks exist, these methodological principles provide the critical apparatus for evaluating genomic foundation model claims.</p>
</section>
<section id="sec-ch11-evaluation-methodology" class="level2" data-number="11.10">
<h2 data-number="11.10" class="anchored" data-anchor-id="sec-ch11-evaluation-methodology"><span class="header-section-number">11.10</span> Evaluation Methodology</h2>
<p>The preceding sections examined <em>what</em> benchmarks measure. This section examines <em>how</em> to evaluate models properly—the methodological foundations that determine whether benchmark results translate to deployment success.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Difficulty Warning: Methodological Rigor
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following sections on evaluation methodology require careful attention. The concepts of leakage, confounding, and proper experimental design are subtle but essential. A model developer who masters these principles will avoid the common pitfalls that produce misleading benchmark results. Take time with each section; the investment will pay dividends in every evaluation you conduct.</p>
</div>
</div>
<p>CD-HIT: Li W, Godzik A. Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences. Bioinformatics. 2006;22(13):1658-1659. doi:10.1093/bioinformatics/btl158 MMseqs2: Steinegger M, Söding J. MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature Biotechnology. 2017;35(11):1026-1028. doi:10.1038/nbt.3988</p>
<p>Statistical Methods</p>
<p>DeLong’s method: DeLong ER, DeLong DM, Clarke-Pearson DL. Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. Biometrics. 1988;44(3):837-845. Benjamini-Hochberg procedure: Benjamini Y, Hochberg Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society, Series B. 1995;57(1):289-300. doi:10.1111/j.2517-6161.1995.tb02031.x</p>
<p>Kinship Estimation</p>
<p>KING: Manichaikul A, Mychaleckyj JC, Rich SS, Daly K, Sale M, Chen WM. Robust relationship inference in genome-wide association studies. Bioinformatics. 2010;26(22):2867-2873. doi:10.1093/bioinformatics/btq559</p>
<p>Calibration Methods</p>
<p>Platt scaling: Platt J. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers. 1999;10(3):61-74. Temperature scaling: Guo C, Pleiss G, Sun Y, Weinberger KQ. On calibration of modern neural networks. Proceedings of the 34th International Conference on Machine Learning (ICML). 2017;70:1321-1330.</p>
<p>Optional (Twilight Zone Reference)</p>
<p>Twilight zone (30% sequence identity): Rost B. Twilight zone of protein sequence alignments. Protein Engineering. 1999;12(2):85-94. doi:10.1093/protein/12.2.85 :::</p>
</section>
<section id="sec-ch11-eval" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Evaluation Principles</h1>
<p>Genomic data makes it exceptionally easy to fool yourself. Sequences share evolutionary history, so a model that memorizes training sequences may appear to generalize when tested on homologs. Variants cluster in families and populations, so ancestry-stratified performance can masquerade as genuine prediction. Experimental measurements carry <strong>batch effects</strong> invisible to the untrained eye, so a model can learn to distinguish sequencing centers rather than biological states. Training labels often derive from the very databases used for evaluation, creating circular validations that inflate performance without testing genuine predictive power. Every shortcut that simplifies evaluation in other machine learning domains becomes an opportunity for false confidence in genomics.</p>
<p>Random data splits that work perfectly well for natural images become actively misleading when applied to biological sequences. A protein held out for testing may share 90% sequence identity with a training protein, allowing the model to succeed through memorization rather than generalization. A variant classified as pathogenic in the test set may come from the same gene family as training variants, letting the model exploit gene-level signals rather than learning variant-specific effects. A cell line in the test set may have been processed at the same sequencing center as training samples, enabling the model to recognize batch signatures rather than biological patterns. These leakages are not hypothetical; they have inflated reported performance across the genomic machine learning literature.</p>
<p>The difference between valid and misleading evaluation often lies not in benchmark choice but in methodological details: data splitting strategies, metric selection, baseline comparisons, ablation designs, and statistical testing. <a href="#sec-ch11-benchmarks" class="quarto-xref"><span>Chapter 11</span></a> catalogs what benchmark tasks exist, how they are constructed, and what capabilities they probe. The complementary question: given a benchmark, how do we apply it to produce trustworthy results? The difference between valid and misleading evaluation often lies not in benchmark choice but in methodological details: data splitting strategies, metric selection, baseline comparisons, ablation designs, and statistical testing. These principles apply across all benchmark categories, from chromatin state prediction to clinical variant classification. By mastering evaluation methodology, practitioners can distinguish genuine advances from artifacts that will not survive deployment.</p>
<section id="sec-ch11-random-splits-fail" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="sec-ch11-random-splits-fail"><span class="header-section-number">12.1</span> Why Random Splits Fail</h2>
<p>The standard machine learning recipe calls for randomly partitioning data into training, validation, and test sets. For image classification or sentiment analysis, this approach works well because individual examples are approximately independent. A photograph of a cat shares no special relationship with another photograph of a different cat beyond their common label. Random assignment ensures that training and test distributions match, and performance on the test set provides an unbiased estimate of performance on new examples from the same distribution.</p>
<p>Genomic data violates these assumptions at every level. Consider a protein dataset where the goal is to predict stability from sequence. Proteins in the same family share evolutionary history and often similar structures. If a training set includes beta-lactamase variants from <em>E. coli</em> and the test set includes beta-lactamase variants from <em>Klebsiella</em>, the model may appear to generalize to “new” proteins while actually recognizing sequence patterns it saw during training. The test performance reflects memorization of family-specific features rather than general principles of protein stability.</p>
<div id="fig-random-splits-fail" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-random-splits-fail-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch11/06-A-fig-random-splits-fail.svg" class="img-fluid figure-img"></p>
<figcaption>Images: Independent samples</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch11/06-B-fig-random-splits-fail.svg" class="img-fluid figure-img"></p>
<figcaption>Proteins: Related by evolution</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_2/ch11/06-C-fig-random-splits-fail.svg" class="img-fluid figure-img"></p>
<figcaption>Variants: Multiple dependencies</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-random-splits-fail-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.1: Why random data splits fail for genomic machine learning. (A) Image classification: samples are truly independent, so random assignment provides valid performance estimates. (B) Protein classification: evolutionary relationships create dependencies; random splits place homologs (&gt;80% identity) across train/test, enabling memorization of shared sequences rather than learning generalizable features. (C) Variant prediction: multiple dependencies compound—same genes across splits, related individuals sharing rare variants, and population structure confounding features and labels. These dependencies require structured splitting strategies that explicitly account for sequence homology, family relatedness, and population structure.
</figcaption>
</figure>
</div>
<p>The problem compounds when sequence identity is high. Two proteins sharing 80% sequence identity will typically have similar structures and functions. A model trained on one and tested on the other is not really being tested on a novel example; it is being asked to interpolate within a region of sequence space it has already explored. Even at 30% sequence identity, the so-called “twilight zone” of homology detection <span class="citation" data-cites="rost_twilight_1999">(<a href="../bib/references.html#ref-rost_twilight_1999" role="doc-biblioref"><strong>rost_twilight_1999?</strong></a>)</span>, proteins often share structural and functional similarities that can be exploited by sufficiently powerful models.</p>
<p>Variant-level data presents analogous challenges. Variants within the same gene share genomic context, and variants affecting the same protein domain share structural environment. Variants from the same individual share haplotype background. Variants from the same population share allele frequency distributions shaped by demographic history. Each of these relationships creates opportunities for models to learn shortcuts that generalize within the training distribution but fail on genuinely novel examples.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: The Independence Assumption
</div>
</div>
<div class="callout-body-container callout-body">
<p>The fundamental issue is that genomic data points are not independent. Random splits assume independence; when this assumption is violated, the test set no longer provides an unbiased estimate of generalization. The consequence is systematic overestimation of performance. A model that achieves 0.90 auROC with random splitting might achieve only 0.75 auROC when evaluated on truly held-out examples, with the gap reflecting how much the model learned about biology versus how much it learned about the structure of the training data.</p>
</div>
</div>
</section>
<section id="sec-ch11-homology-aware-splitting" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="sec-ch11-homology-aware-splitting"><span class="header-section-number">12.2</span> Homology-Aware Splitting</h2>
<p>The solution to homology-driven leakage is to explicitly account for sequence similarity when constructing data splits. Rather than random assignment, examples are clustered by sequence identity, and entire clusters are assigned to training, validation, or test sets. This ensures that no test example is “too similar” to any training example, forcing the model to demonstrate genuine generalization.</p>
<div id="fig-homology-splitting" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-homology-splitting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_5/ch21/02-fig-homology-splitting.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER</strong></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-homology-splitting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.2: [Essential] Step-by-step workflow. Steps: (1) All sequences; (2) Clustering (CD-HIT/MMseqs2 at threshold); (3) Cluster visualization; (4) Split assignment (entire clusters to train/val/test); (5) Validation (no test &gt;X% identity to training). Code snippets. Comparison table (strategy, strictness, efficiency, use case). Key insight: threshold determines hardness.
</figcaption>
</figure>
</div>
<section id="sec-ch11-clustering-tools" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="sec-ch11-clustering-tools"><span class="header-section-number">12.2.1</span> Clustering Tools and Workflows</h3>
<p>Two tools dominate homology-aware splitting in practice. CD-HIT clusters sequences by greedy incremental clustering, assigning each sequence to an existing cluster if it exceeds a similarity threshold to the cluster representative, or creating a new cluster otherwise <span class="citation" data-cites="li_cd-hit_2006">(<a href="../bib/references.html#ref-li_cd-hit_2006" role="doc-biblioref"><strong>li_cd-hit_2006?</strong></a>)</span>. The algorithm is fast and scales to millions of sequences. For proteins, a typical workflow clusters at 40% sequence identity for stringent splitting or 70% for moderate splitting. For nucleotide sequences, thresholds are typically higher (80-95%) due to different evolutionary rates.</p>
<p>MMseqs2 offers faster clustering with similar sensitivity, becoming essential for large-scale analyses <span class="citation" data-cites="steinegger_mmseqs2_2017">(<a href="../bib/references.html#ref-steinegger_mmseqs2_2017" role="doc-biblioref"><strong>steinegger_mmseqs2_2017?</strong></a>)</span>. The tool supports multiple clustering modes and can handle databases with hundreds of millions of sequences. For foundation model pretraining where deduplication affects billions of sequences, MMseqs2 is often the only practical option.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Choosing Identity Thresholds
</div>
</div>
<div class="callout-body-container callout-body">
<p>The choice of identity threshold involves trade-offs:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 22%">
<col style="width: 28%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Threshold</th>
<th>Proteins</th>
<th>Nucleotides</th>
<th>Trade-off</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Stringent</strong></td>
<td>30-40%</td>
<td>80-85%</td>
<td>Hardest test; may lack training data</td>
</tr>
<tr class="even">
<td><strong>Moderate</strong></td>
<td>50-70%</td>
<td>85-90%</td>
<td>Balanced; typical benchmark choice</td>
</tr>
<tr class="odd">
<td><strong>Permissive</strong></td>
<td>80-90%</td>
<td>95%+</td>
<td>Retains data but allows some leakage</td>
</tr>
</tbody>
</table>
<p><strong>Rule of thumb for proteins:</strong> Use 40% identity for variant effect prediction, 30% for structure prediction (where even distant homologs share structure).</p>
<p><strong>Rule of thumb for DNA:</strong> Use 80% for regulatory prediction, but consider gene-family splits instead of sequence identity alone.</p>
</div>
</div>
</section>
<section id="sec-ch11-homology-practical" class="level3" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored" data-anchor-id="sec-ch11-homology-practical"><span class="header-section-number">12.2.2</span> Practical Considerations</h3>
<p>Several subtleties affect the quality of homology-aware splits. When one cluster contains half the data and is assigned to training, the remaining clusters may be too small or too biased to serve as representative test sets. This cluster size distribution problem can be mitigated through stratified sampling within clusters or careful balancing across splits, ensuring that test sets contain sufficient examples across the label distribution.</p>
<p>Pairwise clustering can miss hidden relationships that arise through transitive homology. Protein A may share 35% identity with protein B, and protein B may share 35% identity with protein C, yet A and C share only 20% identity. If A is in training and C is in testing, B serves as an indirect bridge that allows information to leak between splits despite no direct high-identity pair spanning them. Connected component analysis or multi-step clustering can address these transitive relationships, though at increased computational cost.</p>
<p>Multi-domain proteins complicate whole-protein clustering because different domains may have different evolutionary histories. A protein may share one domain with training proteins and another domain with test proteins. Whether this represents leakage depends on the prediction task: if predicting whole-protein function, shared domains matter; if predicting domain-specific properties, they matter more acutely. Domain-aware splitting assigns domains rather than whole proteins to clusters, though this requires domain annotation that may not always be available.</p>
<p>For genomic (non-protein) sequences, repeat elements and transposable elements create analogous challenges. A model trained to predict chromatin state may learn features of LINE elements that recur throughout the genome. Excluding repetitive regions from evaluation or explicitly accounting for repeat content can clarify what the model has actually learned about regulatory sequences versus repetitive element patterns.</p>
</section>
</section>
<section id="sec-ch11-splitting-biological-axis" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="sec-ch11-splitting-biological-axis"><span class="header-section-number">12.3</span> Splitting by Biological Axis</h2>
<p>Beyond sequence homology, genomic data admits multiple axes along which splits can be constructed. The choice of axis determines what kind of generalization is being tested.</p>
<div id="fig-splitting-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-splitting-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_5/ch21/03-fig-splitting-strategies.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER</strong></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-splitting-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.3: [High] Matrix: Splitting strategies vs.&nbsp;what they test. Rows: Random, individual-aware, family-aware, chromosome, gene/protein family, cohort/site, temporal, ancestry. Columns: Sequence generalization, individual, population, temporal robustness, cross-site. Cells: check, x, tilde. Schematics for each strategy; performance drop annotations; combinable strategies note.
</figcaption>
</figure>
</div>
<section id="sec-ch11-splitting-individual" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="sec-ch11-splitting-individual"><span class="header-section-number">12.3.1</span> Splitting by Individual</h3>
<p>For tasks involving human genetic variation, ensuring that data from the same individual (or related individuals) does not appear in both training and test sets is essential. A variant effect predictor trained on variants from person A and tested on other variants from person A may learn individual-specific patterns, such as haplotype structure or ancestry-correlated allele frequencies, that do not generalize to new individuals.</p>
<p>Family structure creates subtler leakage. First-degree relatives share approximately 50% of their genomes identical by descent. Even distant relatives share genomic segments that can be exploited by sufficiently powerful models. Best practice involves computing kinship coefficients across all individuals and either excluding one member of each related pair or assigning entire family clusters to the same split. The UK Biobank provides pre-computed relatedness estimates; other cohorts may require explicit calculation using tools like KING or <code>PLINK</code>. <em>[Citation Needed]</em></p>
</section>
<section id="sec-ch11-splitting-genomic-region" class="level3" data-number="12.3.2">
<h3 data-number="12.3.2" class="anchored" data-anchor-id="sec-ch11-splitting-genomic-region"><span class="header-section-number">12.3.2</span> Splitting by Genomic Region</h3>
<p>Chromosome-based splits assign entire chromosomes to training or testing. This approach is common in regulatory genomics, where models trained on chromosomes 1-16 are tested on chromosomes 17-22 (or similar partitions). The advantage is simplicity and reproducibility; the disadvantage is that chromosomes are not independent. Chromosome 6 contains the HLA region with its unusual patterns of variation and selection; chromosome 21 is small and gene-poor; sex chromosomes have distinct biology. Results may vary substantially depending on which chromosomes are held out.</p>
<p>Region-based splits hold out contiguous segments (e.g., 1 Mb windows) distributed across the genome. This provides more uniform coverage than chromosome splits but requires careful attention to boundary effects. If a regulatory element spans the boundary between training and test regions, parts of its context may leak into training.</p>
</section>
<section id="sec-ch11-splitting-gene-family" class="level3" data-number="12.3.3">
<h3 data-number="12.3.3" class="anchored" data-anchor-id="sec-ch11-splitting-gene-family"><span class="header-section-number">12.3.3</span> Splitting by Gene or Protein Family</h3>
<p>For variant effect prediction, holding out entire genes or protein families tests whether models learn general principles versus gene-specific patterns. A model that achieves high accuracy by memorizing that <em>TP53</em> variants are often pathogenic has not demonstrated understanding of mutational mechanisms. Gene-level splits force models to generalize to genes they have never seen, providing stronger evidence of biological insight.</p>
<p>Family-level splits extend this logic to groups of related genes. Holding out all kinases or all GPCRs tests whether models can generalize across evolutionary families. This is particularly stringent for protein structure and function prediction, where family membership strongly predicts properties.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think: Choosing the Right Split
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a project to predict whether coding variants cause loss of protein function. You have variants from 1000 genes, with 50-100 variants per gene. Which splitting strategy would you choose, and why?</p>
<p>A. Random split (80/10/10) B. Chromosome-based (train on chr1-18, test on chr19-22) C. Gene-based (train on 800 genes, test on 200 held-out genes) D. Individual-based (split by patient ID)</p>
<p><em>Consider: What would each split actually test? Which shortcuts could models exploit?</em></p>
</div>
</div>
</section>
<section id="sec-ch11-splitting-experimental-context" class="level3" data-number="12.3.4">
<h3 data-number="12.3.4" class="anchored" data-anchor-id="sec-ch11-splitting-experimental-context"><span class="header-section-number">12.3.4</span> Splitting by Experimental Context</h3>
<p>Multi-task models that predict chromatin marks across cell types can be split by cell type rather than genomic position. Training on liver, lung, and brain while testing on heart and kidney assesses whether learned regulatory logic transfers across tissues. This matters because cell-type-specific factors drive much of regulatory variation; a model that has simply learned which regions are accessible in the training cell types may fail on novel cell types even when sequence features should transfer.</p>
<p>Similarly, models can be split by assay type (e.g., training on ATAC-seq, testing on DNase-seq), laboratory (to assess batch effects), or time point (for longitudinal data). Each split tests a different axis of generalization.</p>
</section>
<section id="sec-ch11-splitting-ancestry" class="level3" data-number="12.3.5">
<h3 data-number="12.3.5" class="anchored" data-anchor-id="sec-ch11-splitting-ancestry"><span class="header-section-number">12.3.5</span> Splitting by Ancestry</h3>
<p>For human genomic applications, ancestry-stratified evaluation has become essential. Models trained predominantly on European ancestry cohorts often show degraded performance in African, East Asian, South Asian, and admixed populations. This degradation reflects both differences in allele frequency spectra and differences in <strong>linkage disequilibrium</strong> patterns that affect which variants are informative.</p>
<p>Best practice reports performance separately for each major ancestry group represented in the data. When held-out ancestry groups are available (e.g., training on Europeans and testing on Africans), this provides the strongest test of cross-population generalization. When only European data are available, this limitation should be explicitly acknowledged, and claims about generalization should be appropriately modest. The confounding effects of ancestry on genomic predictions are detailed in <a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>.</p>
</section>
<section id="sec-ch11-splitting-time" class="level3" data-number="12.3.6">
<h3 data-number="12.3.6" class="anchored" data-anchor-id="sec-ch11-splitting-time"><span class="header-section-number">12.3.6</span> Splitting by Time</h3>
<p>Temporal splits assign data to training and test sets based on when observations were collected, annotations were created, or variants were classified. This strategy tests whether models generalize forward in time, the actual deployment scenario for any predictive system.</p>
<p>For variant pathogenicity prediction, temporal splits are particularly revealing. ClinVar (<a href="../part_1/p1-ch02-data.html#sec-ch02-clinvar" class="quarto-xref"><span>Section 2.8.1</span></a>) provides submission dates enabling clean temporal partitioning. Training on ClinVar annotations from 2018 and testing on variants first classified in 2022 asks whether the model can predict labels that did not yet exist during training. This avoids the circularity that arises when training and test labels were assigned by similar processes at similar times. Variants classified more recently may reflect updated curation standards, new functional evidence, or reclassifications of previously uncertain variants; a model that performs well on these genuinely new classifications demonstrates predictive validity rather than recapitulation of historical curation patterns.</p>
<p>Implementing temporal splits requires metadata that many datasets lack. ClinVar provides submission dates, enabling clean temporal partitioning. UniProt tracks annotation dates for functional assignments. Clinical cohorts with longitudinal follow-up naturally admit temporal splits based on diagnosis dates. When temporal metadata is unavailable, publication dates of source literature can serve as proxies, though these may not perfectly reflect when information became available to model developers.</p>
<p>The key limitation of temporal splits is non-stationarity. The distribution of variants classified in 2022 may differ systematically from those classified in 2018, not because biology changed but because research priorities, sequencing technologies, and ascertainment patterns evolved. Performance degradation on temporally held-out data may reflect distribution shift rather than genuine failure to generalize. Combining temporal splits with stratified analysis (performance by variant type, gene category, or evidence strength) helps disentangle these factors.</p>
</section>
</section>
<section id="sec-ch11-leakage-detection" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="sec-ch11-leakage-detection"><span class="header-section-number">12.4</span> Leakage Taxonomy and Detection</h2>
<p>Even with careful splitting, leakage can enter evaluations through multiple pathways. A variant effect predictor that achieves 0.95 auROC on held-out test data may be exploiting information that would never exist for truly novel variants, rendering the performance estimate meaningless for clinical deployment. Understanding common leakage patterns helps practitioners design cleaner evaluations and critically assess published results.</p>
<p>Genomic machine learning faces four distinct leakage types, each creating different pathways for inflated performance: label leakage, feature leakage, temporal leakage, and benchmark leakage. These categories are not mutually exclusive; a single evaluation may suffer from multiple forms simultaneously, with compounding effects on apparent performance.</p>
<div id="tbl-leakage-types" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-leakage-types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;12.1: The four major leakage types in genomic machine learning, with detection strategies for each.
</figcaption>
<div aria-describedby="tbl-leakage-types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 22%">
<col style="width: 16%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Leakage Type</th>
<th>Definition</th>
<th>Example</th>
<th>Detection Strategy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Label leakage</strong></td>
<td>Target labels derived from features the model can access</td>
<td>ClinVar classifications informed by SIFT/PolyPhen scores</td>
<td>Compare to baseline using only those features</td>
</tr>
<tr class="even">
<td><strong>Feature leakage</strong></td>
<td>Input features encode future or target information</td>
<td>Conservation scores for pathogenicity prediction</td>
<td>Ablate suspicious features; measure degradation</td>
</tr>
<tr class="odd">
<td><strong>Temporal leakage</strong></td>
<td>Using future information to predict past</td>
<td>Training on 2023 labels to predict 2020 classifications</td>
<td>Strict temporal splits with date metadata</td>
</tr>
<tr class="even">
<td><strong>Benchmark leakage</strong></td>
<td>Test set construction influenced by evaluated methods</td>
<td>Benchmark selected proteins with good sequence coverage</td>
<td>Check benchmark construction procedure</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="sec-ch11-label-leakage" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="sec-ch11-label-leakage"><span class="header-section-number">12.4.1</span> Label Leakage</h3>
<p>Label leakage occurs when target labels are derived from information that the model can access through its features. The classic example is training pathogenicity predictors on ClinVar annotations while using sequence features that contributed to those annotations. If ClinVar curators used <em>SIFT</em> and <em>PolyPhen</em> scores when classifying variants, and the new model uses similar sequence features, high performance may reflect recapitulation of curation criteria rather than independent predictive power.</p>
<p>The ClinVar circularity problem represents a particularly insidious form of label leakage. When computational predictions contributed to the pathogenicity classifications that later become training labels, new models learn to replicate their predecessors rather than discover independent signal. This circularity propagates through generations of models, each inheriting and reinforcing the biases of earlier predictors. The circularity problem for classical variant effect predictors is examined in <a href="../part_1/p1-ch04-vep-classical.html#sec-ch04-circularity" class="quarto-xref"><span>Section 4.5</span></a>, with broader treatment of how such label contamination creates confounded evaluations in <span class="quarto-unresolved-ref">?sec-ch22-label-bias</span>.</p>
<p>Expression models face analogous challenges when trained on features derived from the same samples used to define expression labels. The information flow becomes circular: labels inform features, which predict labels, creating apparent performance that would not generalize to independent samples.</p>
</section>
<section id="sec-ch11-feature-leakage" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2" class="anchored" data-anchor-id="sec-ch11-feature-leakage"><span class="header-section-number">12.4.2</span> Feature Leakage</h3>
<p>Feature leakage occurs when input features encode information about the target that would not be available at prediction time. In genomics, conservation scores are a common source. If a model uses PhyloP scores as features and the target is pathogenicity, the model may learn that conserved positions are more likely pathogenic without learning anything about variant-specific biology. This would be appropriate if conservation scores are intended to be part of the prediction pipeline, but problematic if the goal is to develop a model that predicts pathogenicity from sequence alone.</p>
<p>Similarly, population allele frequency encodes selection pressure. A model that learns “rare variants are more likely pathogenic” has discovered a useful heuristic but not necessarily mechanistic understanding. Whether this counts as leakage depends on the intended use case. For clinical variant interpretation where allele frequency is always available, exploiting this feature is appropriate. For understanding variant biology, it may mask whether the model has learned anything beyond frequency-based priors.</p>
<p>Feature leakage also arises when features encode information about data partitions or batch structure rather than biology. Coverage patterns that differ systematically between cases and controls, quality metrics that correlate with sequencing center, or variant density profiles that reflect caller-specific behavior all constitute feature leakage of this form.</p>
</section>
<section id="sec-ch11-temporal-leakage" class="level3" data-number="12.4.3">
<h3 data-number="12.4.3" class="anchored" data-anchor-id="sec-ch11-temporal-leakage"><span class="header-section-number">12.4.3</span> Temporal Leakage</h3>
<p>Temporal leakage violates the causal structure of prediction by using future information to predict past events. A model trained on ClinVar annotations from 2023 and tested on annotations that were uncertain in 2020 may perform well because new annotations were informed by model-like predictions. The apparent validation is circular: the model predicts labels that were partially derived from model-like reasoning applied after the prediction timepoint.</p>
<p>Clinical outcome prediction faces similar risks when laboratory values, imaging results, or clinical notes recorded after the prediction timepoint enter the feature set. A model predicting 30-day mortality that includes vital signs from day 15 has trivial access to outcome-correlated information. Proper temporal splits must respect not only when samples were collected but when each feature became available.</p>
<p>Training on variants classified in 2023 to predict classifications that were uncertain in 2020 allows models to learn from reclassification patterns rather than intrinsic variant properties. The model exploits the trajectory of scientific knowledge rather than the underlying biology.</p>
</section>
<section id="sec-ch11-benchmark-leakage" class="level3" data-number="12.4.4">
<h3 data-number="12.4.4" class="anchored" data-anchor-id="sec-ch11-benchmark-leakage"><span class="header-section-number">12.4.4</span> Benchmark Leakage</h3>
<p>Benchmark leakage occurs when test set construction was influenced by methods similar to those being evaluated. If a protein function benchmark was created by selecting proteins with high-confidence annotations, and those annotations were partly derived from sequence similarity searches, sequence-based models may perform well by exploiting the same similarity that guided benchmark construction.</p>
<p>Foundation models face particular challenges with benchmark leakage. If a DNA language model is pretrained on all publicly available genomic sequence including ENCODE data, and then evaluated on ENCODE-derived benchmarks, the pretraining has exposed the model to information about the test distribution even if specific test examples were held out. The model may have learned statistical patterns in ENCODE data that transfer to ENCODE benchmarks without reflecting genuine biological understanding.</p>
<p>This form of leakage is especially difficult to detect because it operates at the level of distributional overlap rather than specific example memorization. A model that has never seen a particular test sequence may still have learned the statistical regularities that make that sequence predictable within the benchmark distribution.</p>
</section>
<section id="sec-ch11-detecting-leakage" class="level3" data-number="12.4.5">
<h3 data-number="12.4.5" class="anchored" data-anchor-id="sec-ch11-detecting-leakage"><span class="header-section-number">12.4.5</span> Detecting Leakage</h3>
<p>Several strategies help detect leakage, though none provides definitive proof of its absence. These approaches complement each other; rigorous evaluation employs multiple strategies, recognizing that each catches different leakage pathways while remaining blind to others.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Leakage Detection Checklist
</div>
</div>
<div class="callout-body-container callout-body">
<p>When evaluating your own model or reviewing published results, work through these detection strategies:</p>
<ol type="1">
<li><p><strong>Baseline analysis:</strong> Does a simple model using only potentially leaky features (allele frequency, conservation) achieve similar performance?</p></li>
<li><p><strong>Feature ablation:</strong> Does removing suspicious features cause dramatic performance drops?</p></li>
<li><p><strong>Confounder analysis:</strong> Does performance remain after conditioning on potential confounders (gene, ancestry, batch)?</p></li>
<li><p><strong>Temporal validation:</strong> Does performance hold on prospectively collected data?</p></li>
<li><p><strong>Overlap audit:</strong> Has the overlap between pretraining data and benchmark test sets been documented and checked?</p></li>
</ol>
</div>
</div>
<p>Simple models that could not plausibly have learned biology provide an essential baseline analysis. If a linear model using only allele frequency achieves 0.80 auROC on a pathogenicity benchmark, and a sophisticated deep model achieves 0.82, the marginal improvement may not justify claims of biological insight. The deep model’s performance is bounded by what simple confounders already explain.</p>
<p>Systematic feature ablation removes potentially leaky features and measures performance degradation. If removing conservation scores causes a 20-point drop in auROC, the model was heavily dependent on conservation rather than learning independent predictors. This approach identifies which features drive performance but cannot distinguish legitimate signal from leakage without domain knowledge about what information should be available at prediction time.</p>
<p>Explicit confounder analysis models potential confounders and tests whether model predictions remain informative after conditioning. If a variant effect predictor’s scores become non-predictive after controlling for gene length and expression level, the model may have learned gene-level confounders rather than variant-level effects. <a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 13</span></a> examines how leakage relates to these broader confounding structures.</p>
<p>Temporal validation evaluates models on data collected after the training data was frozen. If performance degrades substantially on newer data, the model may have been fitted to temporal artifacts in the original dataset. This approach is particularly valuable for detecting temporal leakage but requires access to prospectively collected data.</p>
<p>Finally, overlap auditing explicitly checks for sequence or sample overlap between pretraining corpora and evaluation benchmarks. For foundation models, this requires documenting pretraining data composition and comparing against benchmark construction procedures. The audit may reveal that apparent generalization is actually interpolation within seen distributions.</p>
</section>
</section>
<section id="sec-ch11-metrics-genomic-tasks" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="sec-ch11-metrics-genomic-tasks"><span class="header-section-number">12.5</span> Metrics for Genomic Tasks</h2>
<p>Metrics quantify model performance but different metrics answer different questions. Choosing appropriate metrics requires clarity about what aspect of performance matters for the intended application.</p>
<div id="fig-metric-selection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-metric-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_5/ch21/04-fig-metric-selection.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER</strong></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-metric-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.4: [High] Decision flowchart. Decision points: Binary vs continuous? Balanced? Ranking vs probability? Clinical decisions? Metric recommendations at each terminal. Metric descriptions: auROC, <strong>area under the precision-recall curve (auPRC)</strong>, <strong>calibration</strong>, net benefit. Warning callouts: “auROC invariant to monotonic transforms”; “High correlation != clinically meaningful.”
</figcaption>
</figure>
</div>
<section id="sec-ch11-discrimination-metrics" class="level3" data-number="12.5.1">
<h3 data-number="12.5.1" class="anchored" data-anchor-id="sec-ch11-discrimination-metrics"><span class="header-section-number">12.5.1</span> Discrimination Metrics</h3>
<p>For binary outcomes (pathogenic versus benign, bound versus unbound, accessible versus closed), discrimination metrics assess how well the model separates classes. The auROC measures the probability that a randomly selected positive example is ranked above a randomly selected negative example. auROC is threshold-independent and widely reported but can be misleading when classes are highly imbalanced.</p>
<p>The auPRC better reflects performance when positives are rare. For variant pathogenicity prediction, where perhaps 1% of variants are truly pathogenic, a model achieving 0.95 auROC might still have poor precision at useful recall levels. auPRC directly captures the <strong>precision</strong>-<strong>recall</strong> trade-off that matters for applications requiring both high sensitivity and manageable false positive rates.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Why auROC Can Mislead
</div>
</div>
<div class="callout-body-container callout-body">
<p>The distinction between auROC and auPRC reflects a mathematical property with practical consequences:</p>
<ul>
<li><strong>auROC is invariant to class imbalance</strong>: A model’s auROC remains identical whether 1% or 50% of examples are positive, because it measures pairwise ranking between one positive and one negative.</li>
<li><strong>This invariance becomes a liability in deployment</strong>: A model with 0.95 auROC applied to a dataset where 0.1% of variants are pathogenic might flag 100 false positives for every true positive at a threshold capturing 80% of positives.</li>
</ul>
<p><strong>Rule of thumb:</strong> Report both auROC (for comparison across datasets) and auPRC (for realistic assessment of deployment utility). When in doubt, auPRC is the more honest metric for imbalanced problems.</p>
</div>
</div>
<p>This same invariance becomes a liability when evaluating for deployment. A model with 0.95 auROC applied to a dataset where 0.1% of variants are pathogenic might flag 100 false positives for every true pathogenic variant at a threshold capturing 80% of positives. The auROC provides no warning of this behavior because it treats a positive-to-negative pair the same regardless of how many negatives exist. For any application where false positives carry real costs (manual curation, clinical follow-up, unnecessary patient anxiety), auROC presents an optimistic picture that collapses upon deployment.</p>
<p>auPRC explicitly accounts for the negative class size. When positives are rare, achieving high precision requires a model that scores the vast majority of negatives lower than positives, not just a typical negative. This makes auPRC sensitive to class imbalance in exactly the way deployment is sensitive to class imbalance. A model moving from a balanced benchmark to a 1000:1 imbalanced application will show stable auROC but declining auPRC, mirroring the actual increase in false discovery rate users will experience. For this reason, auPRC (or equivalently, average precision) should be the primary metric when the deployment class distribution is known and imbalanced.</p>
<p>Threshold-dependent metrics including <strong>sensitivity</strong>, <strong>specificity</strong>, <strong>positive predictive value</strong>, and <strong>negative predictive value</strong> require specifying a decision threshold. These metrics are more interpretable for specific use cases (e.g., “the model identifies 90% of pathogenic variants while flagging only 5% of benign variants as false positives”) but require choosing thresholds that may not generalize across settings.</p>
</section>
<section id="sec-ch11-regression-correlation-metrics" class="level3" data-number="12.5.2">
<h3 data-number="12.5.2" class="anchored" data-anchor-id="sec-ch11-regression-correlation-metrics"><span class="header-section-number">12.5.2</span> Regression and Correlation Metrics</h3>
<p>For continuous predictions (expression levels, effect sizes, binding affinities), correlation metrics assess agreement between predicted and observed values. <strong>Pearson correlation</strong> measures linear association; <strong>Spearman correlation</strong> measures rank association and is robust to nonlinear relationships. The <strong>coefficient of determination (<span class="math inline">\(R^2\)</span>)</strong> measures variance explained, though interpretation requires care when baseline performance is near zero.</p>
<p>For predictions at genomic scale (e.g., predicted versus observed expression across thousands of genes), these metrics may obscure important patterns. A model might achieve high genome-wide correlation by correctly predicting which genes are highly expressed while failing on the genes where predictions matter most. Task-specific stratification, such as correlation within expression quantiles or among disease-relevant genes, provides more actionable information.</p>
</section>
<section id="sec-ch11-ranking-prioritization-metrics" class="level3" data-number="12.5.3">
<h3 data-number="12.5.3" class="anchored" data-anchor-id="sec-ch11-ranking-prioritization-metrics"><span class="header-section-number">12.5.3</span> Ranking and Prioritization Metrics</h3>
<p>Many genomic workflows care about ranking rather than absolute prediction. Variant prioritization pipelines rank candidates for follow-up; gene prioritization ranks targets for experimental validation. <strong>Top-k recall</strong> measures the fraction of true positives captured in the top <span class="math inline">\(k\)</span> predictions. <strong>Enrichment at k</strong> compares the true positive rate in the top <span class="math inline">\(k\)</span> to the background rate. <strong>Normalized discounted cumulative gain (NDCG)</strong> weights ranking quality by position, penalizing relevant items placed lower in the list more than those placed near the top.</p>
<p>These metrics align with how predictions are actually used. If experimental capacity permits validating only 20 variants per locus, top-20 recall matters more than global auROC. Reporting both global metrics and rank-aware metrics at relevant cutoffs provides a complete picture.</p>
</section>
<section id="sec-ch11-clinical-utility-metrics" class="level3" data-number="12.5.4">
<h3 data-number="12.5.4" class="anchored" data-anchor-id="sec-ch11-clinical-utility-metrics"><span class="header-section-number">12.5.4</span> Clinical Utility Metrics</h3>
<p>For clinical applications, discrimination and calibration are necessary but not sufficient. <strong>Decision curves</strong> plot net benefit across decision thresholds, where net benefit weighs the value of true positives against the cost of false positives at each threshold. A model may achieve high auROC but offer no net benefit at clinically relevant thresholds if it fails to discriminate in the region where decisions are actually made.</p>
<p><strong>Net reclassification improvement (NRI)</strong> measures how often adding genomic features to a clinical model changes risk classifications in the correct direction. This directly addresses whether genomics adds clinical value beyond existing predictors. <a href="../part_6/p6-ch27-clinical-risk.html" class="quarto-xref"><span>Chapter 27</span></a> provides detailed treatment of clinical evaluation frameworks.</p>
</section>
</section>
<section id="sec-ch11-baseline-selection" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="sec-ch11-baseline-selection"><span class="header-section-number">12.6</span> Baseline Selection</h2>
<p>Baseline comparisons determine the meaning of reported performance. A model achieving 0.85 auROC might represent a major advance if the best prior method achieved 0.70, or a trivial improvement if simple heuristics achieve 0.83. Choosing appropriate baselines is as important as choosing appropriate metrics.</p>
<section id="sec-ch11-strong-baselines" class="level3" data-number="12.6.1">
<h3 data-number="12.6.1" class="anchored" data-anchor-id="sec-ch11-strong-baselines"><span class="header-section-number">12.6.1</span> Strong Baselines, Not Straw Men</h3>
<p>The temptation to compare against weak baselines inflates apparent contributions. A deep learning model compared against a naive prior or a deliberately crippled baseline will appear impressive regardless of whether it offers genuine value. Strong baselines force honest assessment of improvement.</p>
<p>For sequence-based predictions, <strong>position weight matrices (PWMs)</strong> and k-mer logistic regression provide classical baselines that capture sequence composition without deep learning. If a convolutional model barely outperforms logistic regression on k-mer counts, the convolutional architecture may not be contributing as much as claimed.</p>
<p>For variant effect prediction, simple features like allele frequency, conservation scores, and amino acid properties provide baselines that any sophisticated model should substantially exceed. <em>CADD</em> (<a href="../part_1/p1-ch04-vep-classical.html#sec-ch04-cadd" class="quarto-xref"><span>Section 4.3</span></a>) serves as a well-calibrated baseline that combines many hand-crafted features; outperforming <em>CADD</em> demonstrates that learning provides value beyond feature engineering.</p>
<p>For foundation models, comparisons should include both randomly initialized models of similar architecture (to isolate the value of pretraining) and simpler pretrained models (to isolate the value of scale or architectural innovations). Claiming that pretraining helps requires demonstrating improvement over training from scratch on the same downstream data.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check: Baseline Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p>For each scenario, identify the appropriate baseline:</p>
<ol type="1">
<li>A new DNA language model claims to predict TF binding sites better than previous approaches. What baselines should it beat?</li>
<li>A variant pathogenicity predictor claims state-of-the-art performance. What would a “straw man” comparison look like, and what would a rigorous comparison include?</li>
<li>A foundation model claims that pretraining improves downstream performance. What comparison demonstrates the value of pretraining specifically?</li>
</ol>
</div>
</div>
</section>
<section id="sec-ch11-historical-baselines" class="level3" data-number="12.6.2">
<h3 data-number="12.6.2" class="anchored" data-anchor-id="sec-ch11-historical-baselines"><span class="header-section-number">12.6.2</span> Historical Baselines and Progress Tracking</h3>
<p>Comparing to methods from five years ago may demonstrate progress but overstates the contribution of any single method. Comparisons should include the best currently available alternatives, not just historically important ones. When prior work is not directly comparable (different data, different splits, different metrics), reimplementing baselines on common benchmarks provides fairer comparison.</p>
<p>Field-wide progress tracking benefits from persistent benchmarks with frozen test sets. Once test set results for a benchmark are published, that benchmark becomes less useful for future model development because the test set is no longer truly held out. Periodic benchmark refresh with new held-out data helps maintain evaluation integrity.</p>
</section>
<section id="sec-ch11-non-dl-baselines" class="level3" data-number="12.6.3">
<h3 data-number="12.6.3" class="anchored" data-anchor-id="sec-ch11-non-dl-baselines"><span class="header-section-number">12.6.3</span> Non-Deep-Learning Baselines</h3>
<p>Deep learning models should be compared against strong non-deep alternatives. Gradient-boosted trees, random forests, and regularized linear models often achieve competitive performance with far less computation. If a 100-million-parameter transformer barely outperforms XGBoost on tabular features, the complexity may not be justified.</p>
<p>This comparison is especially important for clinical deployment, where simpler models may be preferred for interpretability, computational efficiency, or regulatory approval. Demonstrating that deep learning provides substantial gains over strong non-deep baselines strengthens the case for adoption.</p>
</section>
</section>
<section id="sec-ch11-ablation-studies" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="sec-ch11-ablation-studies"><span class="header-section-number">12.7</span> Ablation Studies</h2>
<p>Ablation studies systematically remove or modify model components to understand their contributions. Where baselines compare across methods, ablations investigate within a method, revealing which design choices actually matter.</p>
<section id="sec-ch11-component-isolation" class="level3" data-number="12.7.1">
<h3 data-number="12.7.1" class="anchored" data-anchor-id="sec-ch11-component-isolation"><span class="header-section-number">12.7.1</span> Component Isolation</h3>
<p>Standard ablations remove individual components: attention layers, skip connections, normalization schemes, specific input features. If removing attention heads causes minimal performance degradation, the model may not be exploiting long-range dependencies as claimed. If removing a particular input modality has no effect, that modality may not be contributing useful information.</p>
<p>Ablations should be designed to test specific hypotheses. If the claim is that a foundation model learns biologically meaningful representations, ablating pretraining (comparing to random initialization) directly tests this claim. If the claim is that cross-attention between modalities enables integration, ablating cross-attention while retaining separate encoders tests whether integration provides value.</p>
</section>
<section id="sec-ch11-hyperparameter-sensitivity" class="level3" data-number="12.7.2">
<h3 data-number="12.7.2" class="anchored" data-anchor-id="sec-ch11-hyperparameter-sensitivity"><span class="header-section-number">12.7.2</span> Hyperparameter Sensitivity</h3>
<p>Reporting performance across hyperparameter ranges reveals robustness. A model that achieves state-of-the-art performance only at a narrow learning rate range with specific regularization may be overfit to the evaluation setup. Consistent performance across reasonable hyperparameter variations provides stronger evidence of genuine capability.</p>
</section>
<section id="sec-ch11-architecture-search-confounds" class="level3" data-number="12.7.3">
<h3 data-number="12.7.3" class="anchored" data-anchor-id="sec-ch11-architecture-search-confounds"><span class="header-section-number">12.7.3</span> Architecture Search Confounds</h3>
<p>When model development involves extensive architecture search, reported performance conflates the value of the final architecture with the value of search on the validation set. The validation set is no longer truly held out; it has been used to select among architectures. Final evaluation on a completely untouched test set, with the architecture fixed before test set examination, provides cleaner assessment.</p>
</section>
<section id="sec-ch11-reporting-standards" class="level3" data-number="12.7.4">
<h3 data-number="12.7.4" class="anchored" data-anchor-id="sec-ch11-reporting-standards"><span class="header-section-number">12.7.4</span> Reporting Standards</h3>
<p>Ablation tables should clearly indicate what was changed in each condition, the number of random seeds or runs, and measures of variance. Single-run ablations can produce misleading results due to training stochasticity. Reporting means and standard deviations across multiple runs reveals whether observed differences exceed random variation.</p>
</section>
</section>
<section id="sec-ch11-statistical-rigor" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="sec-ch11-statistical-rigor"><span class="header-section-number">12.8</span> Statistical Rigor</h2>
<p>Performance differences between models may reflect genuine capability differences or random variation in training and evaluation. Statistical analysis distinguishes signal from noise.</p>
<section id="sec-ch11-significance-testing" class="level3" data-number="12.8.1">
<h3 data-number="12.8.1" class="anchored" data-anchor-id="sec-ch11-significance-testing"><span class="header-section-number">12.8.1</span> Significance Testing</h3>
<p>For classification metrics, significance tests ask whether observed differences exceed what would be expected from sampling variation. <strong>Bootstrap</strong> confidence intervals resample the test set with replacement, recompute metrics on each resample, and report the distribution of metric values. Non-overlapping 95% confidence intervals suggest significant differences. <strong>Permutation tests</strong> shuffle predictions between models and measure how often shuffled differences exceed observed differences.</p>
<p>For comparing multiple models across multiple benchmarks, correction for multiple testing becomes important. Without correction, 20 pairwise comparisons will produce an expected one false positive at the 0.05 level even when all models perform equally. The <strong>Bonferroni correction</strong> divides the significance threshold by the number of tests; the <strong>Benjamini-Hochberg procedure</strong> controls false discovery rate with more power than Bonferroni. <em>[Citation Needed]</em></p>
</section>
<section id="sec-ch11-effect-sizes" class="level3" data-number="12.8.2">
<h3 data-number="12.8.2" class="anchored" data-anchor-id="sec-ch11-effect-sizes"><span class="header-section-number">12.8.2</span> Effect Sizes</h3>
<p>Statistical significance does not imply practical significance. A difference of 0.001 auROC might be statistically significant with millions of test examples while being practically meaningless. <strong>Effect sizes</strong> quantify the magnitude of differences independent of sample size. Cohen’s <em>d</em> for continuous outcomes and odds ratios for binary outcomes provide standardized measures of effect magnitude.</p>
<p>Reporting both significance tests and effect sizes provides complete information. A result that is statistically significant with a tiny effect size warrants different interpretation than one that is significant with a large effect size.</p>
</section>
<section id="sec-ch11-confidence-intervals" class="level3" data-number="12.8.3">
<h3 data-number="12.8.3" class="anchored" data-anchor-id="sec-ch11-confidence-intervals"><span class="header-section-number">12.8.3</span> Confidence Intervals on Metrics</h3>
<p>Point estimates of auROC or correlation should be accompanied by confidence intervals. DeLong’s method provides analytical confidence intervals for auROC <span class="citation" data-cites="delong_comparing_1988">(<a href="../bib/references.html#ref-delong_comparing_1988" role="doc-biblioref"><strong>delong_comparing_1988?</strong></a>)</span>; bootstrap methods provide distribution-free intervals for any metric. Reporting “auROC = <span class="math inline">\(0.85\)</span> (95% CI: <span class="math inline">\(0.82\)</span>–<span class="math inline">\(0.88\)</span>)” is more informative than “auROC = <span class="math inline">\(0.85\)</span>” alone.</p>
</section>
<section id="sec-ch11-variance-random-seeds" class="level3" data-number="12.8.4">
<h3 data-number="12.8.4" class="anchored" data-anchor-id="sec-ch11-variance-random-seeds"><span class="header-section-number">12.8.4</span> Variance Across Random Seeds</h3>
<p>Deep learning models are sensitive to initialization and optimization stochasticity. Training the same architecture with different random seeds can produce substantially different results. Best practice trains multiple runs and reports means and standard deviations. If the standard deviation across runs exceeds the difference between methods, claimed improvements may not be reproducible.</p>
</section>
</section>
<section id="sec-ch11-evaluating-fm" class="level2" data-number="12.9">
<h2 data-number="12.9" class="anchored" data-anchor-id="sec-ch11-evaluating-fm"><span class="header-section-number">12.9</span> Evaluating Foundation Models</h2>
<p>Genomic foundation models (<a href="../part_3/p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 14</span></a>) admit multiple evaluation paradigms, each testing different aspects of learned representations.</p>
<div id="fig-fm-evaluation-paradigms" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fm-evaluation-paradigms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_5/ch21/05-A-fig-fm-evaluation-paradigms.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER A</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_5/ch21/05-B-fig-fm-evaluation-paradigms.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER B</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_5/ch21/05-C-fig-fm-evaluation-paradigms.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER C</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fm-evaluation-paradigms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.5: [Enhancing] Three-column comparison. Column 1 (Zero-shot): Frozen model, direct prediction; tests alignment; <em>ESM-1v</em> log-likelihood example; pros/cons. Column 2 (Linear Probing): Frozen embeddings, linear classifier; tests linear accessibility; isolates representation quality; pros/cons. Column 3 (Fine-tuning): Gradients, adaptation; tests total potential; best performance but conflates representation with adaptation. Bottom: Data efficiency curve; “Pretraining value = gap at low data.”
</figcaption>
</figure>
</div>
<section id="sec-ch11-zero-shot-eval" class="level3" data-number="12.9.1">
<h3 data-number="12.9.1" class="anchored" data-anchor-id="sec-ch11-zero-shot-eval"><span class="header-section-number">12.9.1</span> Zero-Shot Evaluation</h3>
<p>In <strong>zero-shot evaluation</strong>, the pretrained model is applied without any task-specific training. For masked language models, this typically means using token probabilities to score variants or classify sequences. A variant that disrupts a position the model predicts with high confidence may indicate functional importance.</p>
<p>Zero-shot performance tests whether pretraining captures task-relevant structure without explicit supervision. Strong zero-shot performance suggests the pretraining objective aligned with the evaluation task; weak zero-shot performance suggests misalignment. Comparing zero-shot performance to simple baselines (e.g., conservation scores for variant effects) calibrates whether the foundation model provides value beyond what simpler approaches achieve.</p>
</section>
<section id="sec-ch11-linear-probing" class="level3" data-number="12.9.2">
<h3 data-number="12.9.2" class="anchored" data-anchor-id="sec-ch11-linear-probing"><span class="header-section-number">12.9.2</span> Linear Probing</h3>
<p><strong>Linear probing</strong> freezes the foundation model and trains only a linear classifier on extracted embeddings. This isolates representation quality from fine-tuning capacity. If a linear probe on foundation model embeddings substantially outperforms a linear probe on random embeddings, the foundation model has learned useful features.</p>
<p>Layer-wise probing reveals where information is encoded. Early layers may capture local sequence features while later layers capture more abstract patterns. If the information needed for a task is extractable from early layers, the model may not require the full depth of the architecture for that application.</p>
</section>
<section id="sec-ch11-fine-tuning-eval" class="level3" data-number="12.9.3">
<h3 data-number="12.9.3" class="anchored" data-anchor-id="sec-ch11-fine-tuning-eval"><span class="header-section-number">12.9.3</span> Fine-Tuning Evaluation</h3>
<p>Full <strong>fine-tuning</strong> adapts all model parameters to the downstream task. This provides the best performance but conflates representation quality with adaptation capacity. A foundation model might achieve high fine-tuned performance through the capacity of its architecture rather than the quality of its pretrained representations.</p>
<p>Comparing fine-tuned foundation models to equivalently architected models trained from scratch isolates the value of pretraining. If both approaches converge to similar performance given sufficient downstream data, pretraining provides label efficiency (less data needed to reach a given performance level) rather than improved final performance. Data efficiency curves, plotting performance against downstream training set size, reveal this trade-off.</p>
</section>
<section id="sec-ch11-transfer-tasks" class="level3" data-number="12.9.4">
<h3 data-number="12.9.4" class="anchored" data-anchor-id="sec-ch11-transfer-tasks"><span class="header-section-number">12.9.4</span> Transfer Across Tasks</h3>
<p>Foundation models justify their “foundation” designation by transferring to diverse downstream tasks. Evaluating on a single task, however well-designed, cannot assess breadth of transfer. Multi-task evaluation across regulatory prediction, variant effects, protein properties, and other applications reveals whether foundation models provide general-purpose representations or excel only on tasks similar to their pretraining objective.</p>
<p>Transfer across species, tissues, and experimental modalities provides additional evidence of generalization. A DNA language model that transfers from human to mouse, or from blood cells to neurons, demonstrates that its representations capture biological principles rather than species-specific or tissue-specific patterns.</p>
</section>
</section>
<section id="sec-ch11-calibration" class="level2" data-number="12.10">
<h2 data-number="12.10" class="anchored" data-anchor-id="sec-ch11-calibration"><span class="header-section-number">12.10</span> Calibration Essentials</h2>
<p>Strong discrimination does not guarantee useful probability estimates. A model achieving 0.95 auROC might assign probability 0.99 to all positive examples and 0.98 to all negatives, ranking perfectly while providing meaningless confidence values. Clinical decision-making requires both: accurate ranking to identify high-risk variants and accurate probabilities to inform the weight of computational evidence. Calibration assesses whether predicted probabilities match observed frequencies, a property essential for rational integration of model outputs into diagnostic workflows.</p>
<section id="sec-ch11-assessing-calibration" class="level3" data-number="12.10.1">
<h3 data-number="12.10.1" class="anchored" data-anchor-id="sec-ch11-assessing-calibration"><span class="header-section-number">12.10.1</span> Assessing Calibration</h3>
<p>The most intuitive assessment comes from <strong>reliability diagrams</strong>, which plot predicted probabilities against observed frequencies. The construction bins predictions into intervals (commonly ten bins spanning 0 to 0.1, 0.1 to 0.2, and so forth), computes the mean predicted probability within each bin, computes the fraction of positive examples within each bin, and plots these quantities against each other. Perfect calibration produces points along the diagonal; systematic deviations reveal overconfidence (points below the diagonal) or underconfidence (points above).</p>
<p>A single summary statistic, the <strong>expected calibration error (ECE)</strong>, captures miscalibration as the weighted average absolute difference between predicted and observed probabilities across bins. Lower ECE indicates better calibration. The metric depends on binning choices; equal-width bins may place most examples in a few bins for models with concentrated predictions, while equal-mass bins ensure each bin contains the same number of examples but may span wide probability ranges. ECE should be reported alongside reliability diagrams for interpretability.</p>
<p>Aggregate calibration metrics can mask important heterogeneity. A model might achieve low aggregate ECE while being systematically overconfident for rare variant classes and underconfident for common ones, with opposite errors canceling in the aggregate statistic. Stratified calibration analysis across ancestry groups, variant classes, and gene categories identifies these disparities. For genomic models intended for diverse populations, subgroup-stratified calibration is not optional; aggregate metrics can mask clinically significant differential performance.</p>
</section>
<section id="sec-ch11-recalibration-methods" class="level3" data-number="12.10.2">
<h3 data-number="12.10.2" class="anchored" data-anchor-id="sec-ch11-recalibration-methods"><span class="header-section-number">12.10.2</span> Recalibration Methods</h3>
<p>Post-hoc recalibration adjusts predicted probabilities without retraining the underlying model. Methods range from single-parameter approaches like <strong>temperature scaling</strong> <span class="citation" data-cites="guo_calibration_2017">(<a href="../bib/references.html#ref-guo_calibration_2017" role="doc-biblioref">Guo et al. 2017</a>)</span>, which divides logits by a learned constant to compress overconfident distributions, to non-parametric transformations like <strong>isotonic regression</strong>, which fits a monotonic function mapping raw scores to calibrated probabilities. <strong>Platt scaling</strong> <span class="citation" data-cites="platt_probabilistic_1999">(<a href="../bib/references.html#ref-platt_probabilistic_1999" role="doc-biblioref"><strong>platt_probabilistic_1999?</strong></a>)</span> fits a logistic regression from model outputs to true labels, providing intermediate flexibility. Each method makes different assumptions about the structure of miscalibration and requires different amounts of calibration data. The mathematical details, theoretical foundations, and guidance for method selection are developed in <a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-post-hoc-calibration" class="quarto-xref"><span>Section 23.3</span></a>.</p>
<p>All recalibration methods require held-out calibration data distinct from both training and test sets. Calibrating on test data and then evaluating calibration on the same test data produces overoptimistic estimates. For foundation models, the calibration set should be drawn from the deployment distribution; calibrating on ClinVar expert-reviewed variants may not transfer to variants in less-studied genes or underrepresented populations.</p>
</section>
<section id="sec-ch11-calibration-comparison" class="level3" data-number="12.10.3">
<h3 data-number="12.10.3" class="anchored" data-anchor-id="sec-ch11-calibration-comparison"><span class="header-section-number">12.10.3</span> Calibration in Model Comparison</h3>
<p>When comparing models, calibration metrics complement discrimination metrics. Two models with identical auROC may have dramatically different calibration, and the better-calibrated model will produce more reliable clinical evidence even though its ranking performance is equivalent. Reporting both discrimination (auROC, auPRC) and calibration (ECE, reliability diagrams) provides a complete picture of model performance.</p>
<p>Calibration can often be improved post-hoc without sacrificing discrimination. Temperature scaling preserves ranking while adjusting probability magnitudes, meaning a model can be recalibrated to improve ECE without changing auROC. This observation suggests that raw discrimination metrics may be more fundamental indicators of model quality, with calibration treated as an adjustable property. The comprehensive treatment of calibration theory is developed in <a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-calibration" class="quarto-xref"><span>Section 23.2</span></a>, including its relationship to uncertainty quantification (<a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-types" class="quarto-xref"><span>Section 23.1</span></a>) and methods for quantifying different sources of prediction uncertainty. Clinical deployment requires additional calibration considerations examined in <a href="../part_6/p6-ch27-clinical-risk.html#sec-ch27-calibration" class="quarto-xref"><span>Section 27.6.2</span></a>.</p>
</section>
</section>
<section id="sec-ch11-putting-together" class="level2" data-number="12.11">
<h2 data-number="12.11" class="anchored" data-anchor-id="sec-ch11-putting-together"><span class="header-section-number">12.11</span> Putting It All Together</h2>
<p>When designing or evaluating a genomic model assessment, working through a systematic checklist helps identify gaps and potential problems. The following questions organize this review, though the specific considerations will vary by application.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Evaluation Design Checklist
</div>
</div>
<div class="callout-body-container callout-body">
<p>Use this checklist when designing an evaluation or reviewing published work:</p>
<p><strong>Data Splitting</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Have individuals, genomic regions, gene families, and ancestries been appropriately separated?</label></li>
<li><label><input type="checkbox">Has homology-aware clustering been applied with appropriate identity thresholds?</label></li>
<li><label><input type="checkbox">Is there any plausible pathway for leakage or circularity?</label></li>
</ul>
<p><strong>Baselines</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Are comparisons made against the best available alternatives?</label></li>
<li><label><input type="checkbox">Do non-deep-learning baselines establish floors that justify complexity?</label></li>
<li><label><input type="checkbox">Does improvement over baselines warrant additional computational costs?</label></li>
</ul>
<p><strong>Metrics</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Are multiple metrics reported (discrimination, calibration, ranking)?</label></li>
<li><label><input type="checkbox">Are confidence intervals provided?</label></li>
<li><label><input type="checkbox">Are subgroup-stratified metrics reported for clinically relevant populations?</label></li>
</ul>
<p><strong>Ablations</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Have systematic ablations demonstrated which design choices drive performance?</label></li>
<li><label><input type="checkbox">Is performance robust across hyperparameter ranges and random seeds?</label></li>
</ul>
<p><strong>Statistical Rigor</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Are significance tests applied with appropriate correction?</label></li>
<li><label><input type="checkbox">Are effect sizes reported alongside p-values?</label></li>
</ul>
<p><strong>For Foundation Models</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Is performance reported across zero-shot, probing, and fine-tuning regimes?</label></li>
<li><label><input type="checkbox">Do data efficiency curves reveal where pretraining provides value?</label></li>
<li><label><input type="checkbox">Has transfer been tested across diverse tasks?</label></li>
</ul>
</div>
</div>
<p>First, consider the level of decision the model is intended to support. A model intended for molecular prediction faces different evaluation requirements than one designed for variant prioritization, patient risk stratification, or clinical action. Metrics should align with the actual decision context: enrichment metrics suit variant ranking, while net benefit matters for clinical decisions.</p>
<p>Second, examine whether data splits adequately prevent leakage. Are individuals, genomic regions, gene families, and ancestries appropriately separated? Has homology-aware clustering been applied with appropriate identity thresholds? Is there any plausible pathway for leakage or circularity through shared labels, features, or distributional overlap?</p>
<p>Third, assess the baseline comparisons. Are comparisons made against the best available alternatives, not just historical or deliberately weak baselines? Do non-deep-learning baselines establish floors that justify architectural complexity? Does the improvement over baselines warrant the additional computational and interpretability costs?</p>
<p>Fourth, evaluate metric selection. Are multiple metrics reported to capture discrimination, calibration, and ranking quality? Are metrics computed with confidence intervals that convey uncertainty? Are subgroup-stratified metrics reported to assess whether performance varies across clinically relevant populations?</p>
<p>Fifth, examine whether ablation studies isolate component contributions. Have systematic ablations demonstrated which design choices drive performance? Is performance robust across hyperparameter ranges and random seeds, or does it depend on specific configurations?</p>
<p>Sixth, consider statistical rigor. Are significance tests applied with appropriate correction for multiple comparisons? Are effect sizes reported alongside <em>p</em>-values to distinguish statistical from practical significance? Are confidence intervals provided for key metrics?</p>
<p>For foundation models specifically, additional considerations apply. Is performance reported across zero-shot, probing, and fine-tuning regimes? Do data efficiency curves reveal where pretraining provides value? Has transfer been tested across diverse tasks to justify the “foundation” designation?</p>
<p>Finally, assess robustness to deployment conditions. How does performance vary across cohorts, platforms, and ancestries? How does the model behave under distribution shift, missing data, or label noise? Would the evaluation translate to realistic deployment scenarios?</p>
<p>This checklist is not exhaustive but covers the most common evaluation pitfalls. Working through it systematically at the design stage can prevent problems that are difficult to fix retrospectively. Reviewers and readers can use the same checklist to critically assess published work.</p>
</section>
<section id="sec-ch11-question-behind-metric" class="level2" data-number="12.12">
<h2 data-number="12.12" class="anchored" data-anchor-id="sec-ch11-question-behind-metric"><span class="header-section-number">12.12</span> The Question Behind the Metric</h2>
<p>The question is never simply “what is the auROC?” but rather “what has been demonstrated, and how much should we trust it?” A reported metric summarizes one aspect of model behavior on one dataset under one evaluation protocol. Whether that metric predicts performance in deployment depends on details that standard reporting obscures: how data were split, whether leakage occurred, which subgroups were evaluated, what baselines were compared, and whether statistical conclusions account for multiple comparisons and estimation uncertainty.</p>
<p>The shortcuts that accelerate research in other machine learning domains produce misleading conclusions when applied to genomic data. Random train-test splits ignore homology that creates pseudo-replication. Single-metric comparisons miss failure modes in clinically relevant subgroups. Significance tests without effect sizes conflate statistical and practical importance. Benchmark evaluation without temporal awareness allows indirect leakage through shared community resources. Homology, population structure, batch effects, and label circularity create countless opportunities for self-deception, and genomic data exhibit all of these in abundance.</p>
<p>Rigorous evaluation requires sustained effort at every stage, from experimental design through statistical analysis. Confounding and leakage (<a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>) examines how <strong>population stratification</strong>, batch effects, and ascertainment bias produce results that evaporate under deployment, with specific attention to ancestry-stratified evaluation in <span class="quarto-unresolved-ref">?sec-ch22-ancestry-confounding</span> and batch effect detection in <a href="../part_4/p4-ch22-multi-omics.html#sec-ch22-batch-effects" class="quarto-xref"><span>Section 22.7.1</span></a>. Uncertainty quantification (<a href="../part_5/p5-ch23-uncertainty.html" class="quarto-xref"><span>Chapter 23</span></a>) extends calibration assessment to epistemic versus aleatoric uncertainty (<a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-types" class="quarto-xref"><span>Section 23.1</span></a>) and selective prediction (<a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-selective-prediction" class="quarto-xref"><span>Section 23.7</span></a>). Interpretability (<a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 25</span></a>) addresses whether models have learned genuine biology or exploited confounded patterns, with attribution methods in <a href="../part_5/p5-ch24-interpretability.html#sec-ch24-attribution" class="quarto-xref"><span>Section 25.1</span></a> providing specific diagnostic tools. For clinical applications specifically, risk prediction frameworks (<a href="../part_6/p6-ch27-clinical-risk.html" class="quarto-xref"><span>Chapter 27</span></a>) develop evaluation approaches tailored to decision-making, where net benefit and decision curves supplement discrimination metrics. Together, these perspectives provide the critical apparatus for engaging with genomic foundation model claims.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>This chapter covered the dual challenge of benchmarks (what to measure) and evaluation methodology (how to measure it properly).</p>
<p><strong>Key Takeaways on Benchmarks:</strong></p>
<ul>
<li><strong>Protein benchmarks</strong> (<em>TAPE</em>, <em>FLIP</em>, <em>ProteinGym</em>) are most mature, with standardized transfer learning evaluation and homology-aware splitting</li>
<li><strong>DNA/regulatory benchmarks</strong> (<em>Genomic Benchmarks</em>, <em>BEND</em>) are rapidly developing but face challenges with quantitative targets and long-range dependencies</li>
<li><strong>Variant effect benchmarks</strong> span molecular (DMS) to clinical (ClinVar) levels, with critical circularity concerns for database-derived labels</li>
<li><strong>Trait-level benchmarks</strong> (<em>TraitGym</em>, <em>EmbedGEM</em>) assess whether foundation models add value beyond classical PGS methods</li>
</ul>
<p><strong>Key Takeaways on Methodology:</strong></p>
<ul>
<li><strong>Random splits fail</strong> for genomic data because sequences share homology, individuals share ancestry, and samples share batch effects</li>
<li><strong>Homology-aware splitting</strong> (CD-HIT/MMseqs2 at appropriate thresholds) prevents the most common leakage pathway</li>
<li><strong>Four leakage types</strong> (label, feature, temporal, benchmark) require different detection strategies</li>
<li><strong>Metric selection</strong> must match deployment objectives: auPRC for imbalanced data, calibration for probability estimates, ranking metrics for prioritization</li>
<li><strong>Strong baselines</strong> and proper ablations distinguish genuine advances from benchmark-specific tuning</li>
</ul>
<p><strong>Looking Ahead:</strong> The next chapter (<a href="p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>) examines how confounding and leakage structures beyond homology create spurious performance claims, including population stratification, batch effects, and ascertainment bias.</p>
<p><strong>Connections:</strong></p>
<ul>
<li>Apply evaluation principles when assessing claims in later chapters on foundation models (<a href="../part_3/p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 14</span></a> through <a href="../part_3/p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 18</span></a>)</li>
<li>Calibration concepts developed here connect to uncertainty quantification (<a href="../part_5/p5-ch23-uncertainty.html" class="quarto-xref"><span>Chapter 23</span></a>)</li>
<li>Clinical utility metrics introduced here are expanded for clinical risk prediction (<a href="../part_6/p6-ch27-clinical-risk.html" class="quarto-xref"><span>Chapter 27</span></a>)</li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-dallago_flip_2022" class="csl-entry" role="listitem">
Dallago, Christian, Jody Mou, Kadina E. Johnston, Bruce J. Wittmann, Nicholas Bhattacharya, Samuel Goldman, Ali Madani, and Kevin K. Yang. 2022. <span>“<span>FLIP</span>: <span>Benchmark</span> Tasks in Fitness Landscape Inference for Proteins.”</span> bioRxiv. <a href="https://doi.org/10.1101/2021.11.09.467890">https://doi.org/10.1101/2021.11.09.467890</a>.
</div>
<div id="ref-gresova_genomic_2023" class="csl-entry" role="listitem">
Grešová, Katarína, Vlastimil Martinek, David Čechák, Petr Šimeček, and Panagiotis Alexiou. 2023. <span>“Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification.”</span> <em>BMC Genomic Data</em> 24 (1): 25. <a href="https://doi.org/10.1186/s12863-023-01123-8">https://doi.org/10.1186/s12863-023-01123-8</a>.
</div>
<div id="ref-guo_calibration_2017" class="csl-entry" role="listitem">
Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. <span>“On <span>Calibration</span> of <span>Modern</span> <span>Neural</span> <span>Networks</span>.”</span> In <em>Proceedings of the 34th <span>International</span> <span>Conference</span> on <span>Machine</span> <span>Learning</span></em>, 1321–30. PMLR. <a href="https://proceedings.mlr.press/v70/guo17a.html">https://proceedings.mlr.press/v70/guo17a.html</a>.
</div>
<div id="ref-landrum_clinvar_2018" class="csl-entry" role="listitem">
Landrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. <span>“<span>ClinVar</span>: Improving Access to Variant Interpretations and Supporting Evidence.”</span> <em>Nucleic Acids Research</em> 46 (D1): D1062–67. <a href="https://doi.org/10.1093/nar/gkx1153">https://doi.org/10.1093/nar/gkx1153</a>.
</div>
<div id="ref-marin_bend_2024" class="csl-entry" role="listitem">
Marin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. <span>“<span>BEND</span>: <span>Benchmarking</span> <span>DNA</span> <span>Language</span> <span>Models</span> on Biologically Meaningful Tasks.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2311.12570">https://doi.org/10.48550/arXiv.2311.12570</a>.
</div>
<div id="ref-mukherjee_embedgem_2024" class="csl-entry" role="listitem">
Mukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom Soare, Raghav Tandon, David Amar, et al. 2024. <span>“<span>EmbedGEM</span>: A Framework to Evaluate the Utility of Embeddings for Genetic Discovery.”</span> <em>Bioinformatics Advances</em> 4 (1). <a href="https://doi.org/10.1093/bioadv/vbae135">https://doi.org/10.1093/bioadv/vbae135</a>.
</div>
<div id="ref-notin_proteingym_2024" class="csl-entry" role="listitem">
Notin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. <span>“<span>ProteinGym</span>: <span>Large</span>-<span>Scale</span> <span>Benchmarks</span> for <span>Protein</span> <span>Fitness</span> <span>Prediction</span> and <span>Design</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 36 (December): 64331–79. <a href="https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html">https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html</a>.
</div>
<div id="ref-rao_evaluating_2019" class="csl-entry" role="listitem">
Rao, Roshan, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, and Yun S. Song. 2019. <span>“Evaluating <span>Protein</span> <span>Transfer</span> <span>Learning</span> with <span>TAPE</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1906.08230">https://doi.org/10.48550/arXiv.1906.08230</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_2/p2-ch10-adaptation.html" class="pagination-link" aria-label="Adaptation Strategies">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_2/p2-ch12-confounding.html" class="pagination-link" aria-label="Confounding and Data Leakage">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>