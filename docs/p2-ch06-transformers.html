<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Transformer Architecture for Genomics – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p2-ch07-foundation.html" rel="next">
<link href="./p2-ch05-tokens.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part II: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch06-transformers.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-pgs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS &amp; Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-cadd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Deleteriousness Scores</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-tokens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-transformers.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-foundation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretrain.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning &amp; Deployment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">CNN Sequence-to-Function Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RNA &amp; Transcript-Level Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-plm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-hybrid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Long-range Hybrid Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">p4–multi-modal_multi-scale.qmd</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-sc-epi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Single-Cell &amp; Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Graphs, Networks, and Biology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Multi-Omics &amp; Systems Biology</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch18-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch19-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-vep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-confound.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Interpretability &amp; Mechanisms</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch23-clinical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch24-variants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Pathogenic Variant Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-drugs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Drug Discovery &amp; Biotech</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Future Work &amp; Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-model-list.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#why-transformers-for-genomics" id="toc-why-transformers-for-genomics" class="nav-link active" data-scroll-target="#why-transformers-for-genomics"><span class="header-section-number">6.1</span> Why Transformers for Genomics?</a></li>
  <li><a href="#architectural-paradigms-for-sequences" id="toc-architectural-paradigms-for-sequences" class="nav-link" data-scroll-target="#architectural-paradigms-for-sequences"><span class="header-section-number">6.2</span> Architectural Paradigms for Sequences</a></li>
  <li><a href="#the-self-attention-mechanism" id="toc-the-self-attention-mechanism" class="nav-link" data-scroll-target="#the-self-attention-mechanism"><span class="header-section-number">6.3</span> The Self-Attention Mechanism</a></li>
  <li><a href="#positional-encoding-representing-sequence-order" id="toc-positional-encoding-representing-sequence-order" class="nav-link" data-scroll-target="#positional-encoding-representing-sequence-order"><span class="header-section-number">6.4</span> Positional Encoding: Representing Sequence Order</a></li>
  <li><a href="#the-transformer-block-architecture" id="toc-the-transformer-block-architecture" class="nav-link" data-scroll-target="#the-transformer-block-architecture"><span class="header-section-number">6.5</span> The Transformer Block Architecture</a></li>
  <li><a href="#scaling-transformers-for-genomics" id="toc-scaling-transformers-for-genomics" class="nav-link" data-scroll-target="#scaling-transformers-for-genomics"><span class="header-section-number">6.6</span> Scaling Transformers for Genomics</a></li>
  <li><a href="#transformer-variants-in-genomic-applications" id="toc-transformer-variants-in-genomic-applications" class="nav-link" data-scroll-target="#transformer-variants-in-genomic-applications"><span class="header-section-number">6.7</span> Transformer Variants in Genomic Applications</a></li>
  <li><a href="#training-considerations" id="toc-training-considerations" class="nav-link" data-scroll-target="#training-considerations"><span class="header-section-number">6.8</span> Training Considerations</a></li>
  <li><a href="#limitations-and-alternatives" id="toc-limitations-and-alternatives" class="nav-link" data-scroll-target="#limitations-and-alternatives"><span class="header-section-number">6.9</span> Limitations and Alternatives</a></li>
  <li><a href="#summary-and-forward-references" id="toc-summary-and-forward-references" class="nav-link" data-scroll-target="#summary-and-forward-references"><span class="header-section-number">6.10</span> Summary and Forward References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part II: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch06-transformers.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-transformers" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="content-visible callout callout-style-default callout-warning callout-empty-content callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>The transformer architecture revolutionized natural language processing in 2017 and rapidly spread to other domains, including genomics. Between 2018 and 2023, transformers became the dominant paradigm for modeling biological sequences, powering protein language models like ESM and genomic models like DNABERT and Nucleotide Transformer. This chapter examines the core mechanisms that make transformers effective for genomic data, their architectural components, and the practical considerations for applying them to DNA, RNA, and protein sequences.</p>
<p>We begin by motivating why transformers matter for genomics, then systematically unpack the self-attention mechanism, positional encodings, and the standard transformer block. We discuss scaling considerations specific to genomic sequences, survey transformer variants used in genomic applications, and address training challenges. Finally, we consider the limitations that led to the development of alternative architectures like state space models, which we cover in depth in Chapter <a href="p3-ch11-dna.html" class="quarto-xref"><span>Chapter 11</span></a>.</p>
<div class="content-visible callout callout-style-default callout-note callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>VISUAL SUGGESTION (chapter overview)</strong><br>
Figure: Timeline showing major transformer milestones in NLP (Vaswani 2017, BERT 2018, GPT-3 2020) alongside genomic applications (DNABERT 2020, Enformer 2021, Nucleotide Transformer 2023, Evo 2024), with arrows indicating architectural innovations flowing from NLP to genomics.</p>
</div>
</div>
<section id="why-transformers-for-genomics" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="why-transformers-for-genomics"><span class="header-section-number">6.1</span> Why Transformers for Genomics?</h2>
<p>Convolutional neural networks dominated early genomic deep learning because they naturally capture local patterns like transcription factor binding motifs, splice sites, and promoter elements. CNNs apply the same learned filters across all positions, implementing a form of translation equivariance that matches the biological intuition that a motif has similar effects regardless of where it appears in a regulatory region.</p>
<p>However, CNNs have fundamental limitations when modeling genomic regulation. Their receptive fields are inherently local. Even with stacked layers and dilated convolutions, the effective context a CNN can integrate remains bounded, often spanning only a few kilobases. Gene regulation in eukaryotes operates over vastly longer ranges. Enhancers routinely act hundreds of kilobases from their target promoters. Topologically associating domains (TADs) organize chromatin contacts at megabase scales. Distal regulatory variants identified through GWAS often lie far from coding sequences, exerting their effects through long-range interactions that CNNs struggle to capture directly.</p>
<p>Transformers address this limitation through their self-attention mechanism, which allows each position in a sequence to directly attend to all other positions in a single operation. This global receptive field enables modeling of long-range dependencies without requiring information to propagate through many layers. For genomics, this means enhancer-promoter interactions can be learned directly, regulatory context from distant elements can inform predictions at any position, and models can integrate information across entire genes or regulatory domains.</p>
<p>The attention mechanism also offers flexibility in how models aggregate information. Rather than applying fixed convolutional kernels, attention learns position-specific aggregation patterns that can adapt to the biological context. Different attention heads can specialize in different types of interactions, potentially capturing diverse regulatory mechanisms like enhancer-promoter communication, insulator boundaries, or promoter competition.</p>
<p>These properties made transformers particularly attractive for genomic foundation models. Proteins evolved through deep evolutionary time, creating complex structure-function relationships that benefit from global context integration. DNA regulatory elements interact across large genomic distances in ways that local CNNs miss. RNA secondary structure brings distant nucleotides into close three-dimensional proximity, creating long-range dependencies in the linear sequence.</p>
<div class="content-visible callout callout-style-default callout-note callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>VISUAL SUGGESTION (CNN vs transformer receptive fields)</strong><br>
Figure: Side-by-side comparison showing a CNN’s limited, layered receptive field (triangular expansion through convolution stack) versus a transformer’s global receptive field (all positions connect to all positions in attention layer). Overlay example showing enhancer-promoter pair 100kb apart that transformer captures in one layer but requires many CNN layers to connect.</p>
</div>
</div>
</section>
<section id="architectural-paradigms-for-sequences" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="architectural-paradigms-for-sequences"><span class="header-section-number">6.2</span> Architectural Paradigms for Sequences</h2>
<p>Before diving into transformer mechanics, we briefly situate them within the broader landscape of sequence modeling architectures. Three main paradigms have dominated genomic deep learning over the past decade.</p>
<p>CNNs were the first architecture to achieve strong performance on genomic tasks, pioneered by models like DeepSEA and DeepBind. Their strength lies in parameter efficiency (shared filters across positions) and the ability to learn hierarchical local features (motifs, motif combinations, regulatory grammar). Chapter 5 covers CNN-based regulatory prediction models in detail. CNNs remain competitive for tasks where local context dominates, such as splice site recognition or promoter classification. However, their limited receptive fields constrain performance on tasks requiring long-range integration.</p>
<p>Transformers emerged from natural language processing, where they demonstrated unprecedented ability to model linguistic context and semantic relationships. The core innovation is self-attention, which computes interactions between all sequence positions simultaneously. This provides global context but comes at a computational cost that scales quadratically with sequence length. Despite this expense, transformers dominated genomic modeling from roughly 2018 through 2023 because their ability to capture long-range dependencies outweighed their computational demands for many applications.</p>
<p>State space models (SSMs) represent a more recent development, offering linear computational complexity while maintaining long-range modeling capability. Architectures like Mamba have begun to challenge transformers’ dominance, particularly for ultra-long genomic contexts where transformer quadratic scaling becomes prohibitive. We defer detailed discussion of SSMs to Chapter <a href="p3-ch11-dna.html" class="quarto-xref"><span>Chapter 11</span></a>, focusing here on transformers as the foundation from which these newer architectures emerged.</p>
<p>The progression from CNNs to transformers to SSMs reflects an ongoing tension between computational efficiency and modeling capacity. Each paradigm offers distinct trade-offs that remain relevant for different genomic applications. Understanding transformers in depth provides essential context for appreciating both why they succeeded and why the field is now exploring alternatives.</p>
</section>
<section id="the-self-attention-mechanism" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="the-self-attention-mechanism"><span class="header-section-number">6.3</span> The Self-Attention Mechanism</h2>
<p>Self-attention is the defining component of transformer architecture. It allows each position in a sequence to aggregate information from all other positions through learned, input-dependent weights. This section unpacks the mathematical formulation and intuition behind self-attention.</p>
<p>At each position in the input sequence, self-attention computes three vectors: a query, a key, and a value. These are produced by multiplying the input embedding at that position by three learned weight matrices <span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>, and <span class="math inline">\(W^V\)</span>. The query represents “what this position is looking for,” the key represents “what this position offers,” and the value represents “what information this position carries.”</p>
<p>The attention mechanism then computes similarity scores between each query and all keys. Specifically, for position <span class="math inline">\(i\)</span>, we compute the dot product between its query <span class="math inline">\(q_i\)</span> and every key <span class="math inline">\(k_j\)</span> for positions <span class="math inline">\(j = 1, \ldots, L\)</span>, where <span class="math inline">\(L\)</span> is the sequence length. These scores are scaled by the square root of the key dimension <span class="math inline">\(\sqrt{d_k}\)</span> to prevent gradient instability when dimensions are large, yielding:</p>
<p><span class="math display">\[
\text{score}(q_i, k_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
\]</span></p>
<p>A softmax function converts these scores into a probability distribution over positions, producing attention weights <span class="math inline">\(\alpha_{ij}\)</span> that sum to one across all <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[
\alpha_{ij} = \frac{\exp(\text{score}(q_i, k_j))}{\sum_{j'=1}^L \exp(\text{score}(q_i, k_{j'}))}
\]</span></p>
<p>These weights determine how much each position <span class="math inline">\(i\)</span> attends to each other position <span class="math inline">\(j\)</span>. High attention weight means position <span class="math inline">\(i\)</span> strongly aggregates information from position <span class="math inline">\(j\)</span>; low weight means that position contributes little to the output at position <span class="math inline">\(i\)</span>.</p>
<p>Finally, the output at position <span class="math inline">\(i\)</span> is computed as a weighted sum of all value vectors, where the weights are the attention scores:</p>
<p><span class="math display">\[
\text{output}_i = \sum_{j=1}^L \alpha_{ij} v_j
\]</span></p>
<p>This weighted aggregation is the core of self-attention. Each output position receives a mixture of information from across the entire sequence, with the mixture proportions learned through backpropagation based on task objectives.</p>
<p>Multi-head attention extends this mechanism by running multiple attention operations in parallel, each with different learned projections. If we use <span class="math inline">\(H\)</span> heads, we split the model dimension <span class="math inline">\(d\)</span> into <span class="math inline">\(H\)</span> subspaces of dimension <span class="math inline">\(d/H\)</span>, compute separate queries, keys, and values for each head, run attention independently, then concatenate the outputs and project back to dimension <span class="math inline">\(d\)</span>. This allows different heads to capture different types of relationships. In genomic models, different heads might specialize in different regulatory patterns, such as one head attending to nearby positions (local context) while another attends to distal elements (long-range interactions).</p>
<p>The computational cost of self-attention is quadratic in sequence length because we compute <span class="math inline">\(L \times L\)</span> attention scores. For a sequence of length <span class="math inline">\(L\)</span> and model dimension <span class="math inline">\(d\)</span>, computing all queries, keys, and values requires <span class="math inline">\(O(Ld^2)\)</span> operations, while computing the <span class="math inline">\(L^2\)</span> attention scores requires <span class="math inline">\(O(L^2d)\)</span> operations. The quadratic term dominates for long sequences, making standard self-attention expensive for genomic contexts spanning hundreds of kilobases.</p>
<div class="content-visible callout callout-style-default callout-note callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>VISUAL SUGGESTION (attention computation flow)</strong><br>
Figure: Schematic showing input embeddings transformed into Q, K, V matrices, computation of attention scores as QK^T heatmap, softmax normalization, and weighted aggregation of values. Include small example with L=6 positions showing how output at position 3 aggregates information from all positions with different weights.</p>
</div>
</div>
</section>
<section id="positional-encoding-representing-sequence-order" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="positional-encoding-representing-sequence-order"><span class="header-section-number">6.4</span> Positional Encoding: Representing Sequence Order</h2>
<p>Transformers have a critical limitation: self-attention is permutation invariant. If you shuffle the order of input tokens, each position still attends to the same set of other positions with the same values, just in different order. The model has no inherent notion of sequence position. For genomic data where order matters fundamentally (5’ to 3’ directionality, strand orientation, spatial organization), this poses a serious problem.</p>
<p>Positional encodings solve this by injecting information about token positions into the model. The original Transformer <span class="citation" data-cites="vaswani_attention_2017">(<a href="references.html#ref-vaswani_attention_2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span> used sinusoidal functions with different frequencies for each dimension. For position <span class="math inline">\(pos\)</span> and dimension <span class="math inline">\(i\)</span>, the encoding is:</p>
<p><span class="math display">\[
\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
\]</span> <span class="math display">\[
\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
\]</span></p>
<p>These fixed sinusoidal patterns have useful properties: they are deterministic (same for all sequences), allow the model to learn to attend by relative positions (since <span class="math inline">\(\text{PE}(pos+k)\)</span> can be written as a linear function of <span class="math inline">\(\text{PE}(pos)\)</span>), and generalize to sequence lengths not seen during training.</p>
<p>However, many genomic models use learned positional embeddings instead. These are simply lookup tables where each position <span class="math inline">\(pos\)</span> has a learned vector <span class="math inline">\(E_{pos}\)</span> that is added to the input embedding. Learned positional embeddings offer more flexibility, allowing the model to discover position-dependent patterns specific to genomic data. The trade-off is that they must be trained and do not automatically extrapolate to longer sequences than those seen during training.</p>
<p>Relative positional encodings represent a middle ground. Rather than encoding absolute positions, these schemes encode the relative distance or relationship between positions. T5-style relative position bias adds a learnable scalar bias to attention scores based on the distance between query and key positions. This helps the model learn that nearby positions often have stronger interactions than distant ones, while remaining agnostic about absolute position.</p>
<p>Attention with Linear Biases (ALiBi) takes this further by adding a fixed linear penalty to attention scores based on distance, without any learned parameters. For a head with slope <span class="math inline">\(m\)</span>, the attention score between positions separated by distance <span class="math inline">\(|i - j|\)</span> is penalized by <span class="math inline">\(m|i - j|\)</span>. Different heads use different slopes, encouraging some to focus locally and others globally. ALiBi has shown strong generalization to longer contexts than seen during training, making it attractive for genomic applications where sequence length varies.</p>
<p>Rotary Position Embeddings (RoPE) encode positions by rotating the query and key vectors in a high-dimensional space. The rotation angle depends on position, ensuring that the dot product between query and key depends on their relative distance. RoPE has become popular in recent language models and is beginning to appear in genomic transformers because it combines the benefits of relative encoding with efficient implementation.</p>
<p>For genomics, positional encoding must respect biological semantics. DNA has strand directionality: the sequence ACGT on the forward strand has different regulatory meaning than ACGT on the reverse strand. Positional encodings should allow the model to learn strand-specific patterns. Some genomic transformers use separate positional embeddings for forward and reverse strands. Others rely on the model to learn strand orientation from sequence content itself.</p>
<p>Genomic coordinates pose another challenge. Should position 1 in the model correspond to a fixed genomic coordinate (e.g., transcription start site), or should it be relative to some local landmark? Different models make different choices based on their tasks. Models predicting regulatory activity often center sequences on gene promoters, using positions relative to the TSS. Foundation models trained on random genomic segments typically use positional encodings that reflect sequence order without reference to genomic coordinates.</p>
<div class="content-visible callout callout-style-default callout-note callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>VISUAL SUGGESTION (positional encoding schemes)</strong><br>
Figure: Four panels comparing positional encoding methods. (a) Sinusoidal: wave patterns at different frequencies. (b) Learned: discrete embedding vectors. (c) ALiBi: attention score modification with distance-dependent penalty. (d) RoPE: rotation in 2D subspace. Include small example showing how different schemes affect attention between positions separated by different distances.</p>
</div>
</div>
</section>
<section id="the-transformer-block-architecture" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="the-transformer-block-architecture"><span class="header-section-number">6.5</span> The Transformer Block Architecture</h2>
<p>A transformer model consists of stacked blocks, each containing self-attention and feed-forward components connected by residual connections and layer normalization. Understanding this block structure is essential for grasping how information flows through transformer models.</p>
<p>Each transformer block has two main components: a multi-head self-attention layer and a position-wise feed-forward network. The attention layer enables global communication across all positions, allowing each position to gather information from the entire sequence. The feed-forward network processes each position independently, applying nonlinear transformations to the aggregated information.</p>
<p>Layer normalization stabilizes training by normalizing activations across the feature dimension at each position. Two conventions exist for where to place layer normalization relative to the self-attention and feed-forward sublayers. Post-norm places normalization after each sublayer, applying it to the output before the residual connection. Pre-norm places normalization before each sublayer, normalizing the input to that sublayer. Pre-norm has become more common in recent models because it improves training stability, particularly for deep networks, though post-norm can achieve slightly better final performance with careful tuning.</p>
<p>The feed-forward network consists of two linear transformations with a nonlinearity between them. Typically, this expands the dimension by a factor of four, applies a nonlinear activation function (often GELU), then projects back to the original dimension. This expansion allows the model to process the attention-aggregated information through a high-dimensional nonlinear transformation before producing the output for the next layer.</p>
<p>Residual connections wrap around both the attention and feed-forward sublayers, adding the input directly to the output. These connections serve two critical functions. First, they provide gradient highways during backpropagation, allowing gradients to flow directly through many layers without being repeatedly transformed. This enables training of very deep networks. Second, they create an inductive bias toward small, incremental refinements of the representation at each layer, rather than forcing each layer to construct an entirely new representation.</p>
<p>The flow through a transformer block with pre-norm looks like this: the input <span class="math inline">\(X\)</span> is first normalized, then processed by multi-head attention to produce <span class="math inline">\(X'\)</span>, which is added back to the original input via residual connection, yielding <span class="math inline">\(X + X'\)</span>. This sum is again normalized, passed through the feed-forward network to produce <span class="math inline">\(X''\)</span>, and added to the input of the feed-forward layer via another residual connection, yielding the final output <span class="math inline">\((X + X') + X'' = X + X' + X''\)</span>.</p>
<p>Stacking depth determines how many times this refinement process occurs. Shallow transformers with few layers are parameter-efficient but may lack capacity for complex tasks. Deep transformers with many layers can learn more sophisticated representations but require more computation and careful optimization. Most genomic transformers use between 6 and 24 layers, though this varies by application. Models focused on short sequences (e.g., small RNA molecules) might use fewer layers, while foundation models trained on long genomic contexts often use deeper stacks.</p>
<p>The choice of depth involves balancing several considerations. Deeper networks can learn more complex functions and abstract representations, but they are harder to train, prone to overfitting without sufficient data, and more computationally expensive at both training and inference. For genomic applications, depth often correlates with the complexity of regulatory patterns being modeled. Simple motif-based tasks might benefit more from wider layers (larger <span class="math inline">\(d\)</span>) than deeper stacks, while tasks requiring integration of hierarchical regulatory information (promoters → enhancers → TAD structure) may benefit from additional depth.</p>
<div class="content-visible callout callout-style-default callout-note callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>VISUAL SUGGESTION (transformer block schematic)</strong><br>
Figure: Detailed diagram of a single transformer block showing pre-norm configuration. Input flows through LayerNorm → Multi-head Attention → Add &amp; Norm → Feed-forward (with dimension expansion) → Add &amp; Norm → Output. Annotate with typical dimensions (e.g., d=512, dff=2048) and show residual connections as bypass arrows.</p>
</div>
</div>
</section>
<section id="scaling-transformers-for-genomics" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="scaling-transformers-for-genomics"><span class="header-section-number">6.6</span> Scaling Transformers for Genomics</h2>
<p>Genomic sequences present unique scaling challenges. DNA sequences can span millions of bases, far exceeding the typical context lengths in natural language processing. Tokenization choices (covered in Chapter <a href="p2-ch05-tokens.html" class="quarto-xref"><span>Chapter 5</span></a>) interact with sequence length: single-nucleotide tokens create very long sequences, while k-mer tokens reduce length at the cost of vocabulary size. Scaling transformers to genomic applications requires careful consideration of how to balance model size, context length, and computational resources.</p>
<p>Parameter scaling in transformers comes from two main sources: width and depth. Width refers to the model dimension <span class="math inline">\(d\)</span>, which determines the size of embeddings and hidden states. Increasing width allows the model to represent more complex patterns at each position but increases the number of parameters quadratically (since weight matrices are <span class="math inline">\(d \times d\)</span>). Depth refers to the number of stacked transformer blocks. Increasing depth allows the model to learn hierarchical abstractions through repeated refinement but also increases parameters linearly with layers.</p>
<p>The transformer scaling laws studied in natural language processing suggest that model performance improves smoothly with increased parameters, data, and compute. For genomics, similar principles apply, though the optimal ratios differ. Genomic sequences are less compressible than natural language (each nucleotide carries less predictable information than words in structured text), suggesting that genomic models might benefit more from depth than width compared to language models of similar parameter count.</p>
<p>Context length scaling presents the central challenge for genomic transformers. Standard self-attention has <span class="math inline">\(O(L^2)\)</span> complexity, where <span class="math inline">\(L\)</span> is sequence length. A 10kb sequence tokenized at single-nucleotide resolution has 10,000 tokens, requiring 100 million attention computations per layer. A 200kb sequence has 200,000 tokens, requiring 40 billion attention computations per layer. This quadratic scaling rapidly becomes prohibitive.</p>
<p>Several strategies address this computational bottleneck. Sparse attention patterns restrict which positions can attend to which others, reducing the quadratic cost. For example, local windowing allows each position to attend only to positions within a fixed window, reducing complexity to <span class="math inline">\(O(Lw)\)</span> where <span class="math inline">\(w\)</span> is window size. This works well when most relevant interactions are local, as often holds for regulatory sequences where nearby elements interact more strongly than distant ones.</p>
<p>Strided attention patterns create a hierarchy where lower layers use local windows and upper layers attend to every <span class="math inline">\(k\)</span>-th position. This captures both local fine-grained patterns and global coarse-grained structure while maintaining sub-quadratic complexity. Hybrid models like Enformer (Chapter <a href="p3-ch14-hybrid.html" class="quarto-xref"><span>Chapter 14</span></a>) use this strategy, applying CNNs to downsample sequences before transformer layers.</p>
<p>Approximations to full attention offer another approach. Linformer approximates the attention matrix through low-rank decomposition, reducing complexity to linear in sequence length at the cost of some expressiveness. Performer uses random feature methods to approximate attention scores without explicitly computing the full <span class="math inline">\(L \times L\)</span> matrix. These approximations work well in practice for some tasks but may lose important long-range dependencies.</p>
<p>For genomic applications, the choice among these strategies depends on the biological context. Regulatory prediction often benefits from local windowing because nearby elements dominate enhancer-promoter interactions. Foundation models trained to predict masked tokens may require global attention because the model must integrate information from across the entire sequence to make coherent predictions. Variant effect prediction sometimes requires selective attention to specific distal elements, which sparse patterns may miss.</p>
<p>Memory requirements compound the computational challenge. Transformer training requires storing activations for backpropagation, and attention matrices can be particularly memory-intensive. Gradient checkpointing trades compute for memory by recomputing activations during the backward pass rather than storing them. This allows training larger models or longer sequences on fixed hardware at the cost of additional computation time.</p>
<p>Mixed precision training uses lower-precision (16-bit) floating point for most computations while maintaining higher precision (32-bit) for critical operations like loss computation and optimizer updates. Modern GPUs accelerate 16-bit arithmetic substantially, providing near 2× speedup with minimal precision loss for most models. Genomic transformers routinely use mixed precision to enable training on longer sequences or with larger batch sizes.</p>
<div class="content-visible callout callout-style-default callout-note callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>VISUAL SUGGESTION (scaling trade-offs)</strong><br>
Figure: Multi-panel showing scaling relationships. (a) Parameter count vs model dimension and depth, with typical genomic model configurations marked. (b) Attention complexity vs sequence length for full, windowed, and sparse attention, with genomic length scales (1kb, 10kb, 100kb, 1Mb) indicated. (c) Memory usage vs sequence length showing impact of activation checkpointing.</p>
</div>
</div>
</section>
<section id="transformer-variants-in-genomic-applications" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="transformer-variants-in-genomic-applications"><span class="header-section-number">6.7</span> Transformer Variants in Genomic Applications</h2>
<p>The transformer architecture has been adapted in diverse ways for genomic modeling. This section surveys major variants and their design rationale.</p>
<p>Standard encoder-only transformers process sequences bidirectionally, allowing each position to attend to all other positions including future positions. DNABERT and Nucleotide Transformer exemplify this architecture. They are trained with masked language modeling objectives where random tokens are masked and the model predicts them from bidirectional context. This works well for understanding genomic sequences where both upstream and downstream context matters, such as transcription factor binding sites influenced by flanking sequence or protein structures determined by both N-terminal and C-terminal residues.</p>
<p>Encoder-only transformers excel at sequence representation tasks where the goal is to learn meaningful embeddings that capture biological properties. These embeddings can be used directly for downstream tasks like variant effect prediction or as features for other models. The bidirectional context allows rich representations but makes these models unsuitable for generative tasks where we need to sample sequences autoregressively.</p>
<p>Decoder-only transformers use causal attention where each position attends only to itself and preceding positions. This enables autoregressive generation: the model generates sequences one token at a time, conditioning each new token on all previous tokens. GenSLM and other genomic foundation models trained on next-token prediction use this architecture. Causal attention is essential for generative modeling but provides less rich representations than bidirectional attention for fixed sequences because each position has access to only partial context.</p>
<p>The trade-off between encoder and decoder architectures reflects a fundamental tension in genomic modeling. Representation learning benefits from bidirectional context, while sequence generation requires causal structure. Some applications use both: a bidirectional encoder produces initial representations, then a causal decoder refines them for generation or prediction tasks.</p>
<p>Hybrid CNN-transformer architectures combine convolutional layers with transformer blocks. Models like Enformer and Borzoi (Chapter <a href="p3-ch14-hybrid.html" class="quarto-xref"><span>Chapter 14</span></a>) apply convolutional stems to long sequences, downsampling through pooling, then pass the compressed representation through transformer layers. This exploits CNNs’ efficiency for local pattern extraction while using transformers for long-range integration. The downsampling addresses transformers’ quadratic complexity by reducing sequence length before attention.</p>
<p>These hybrid models achieve state-of-the-art performance on regulatory prediction tasks but blur the line between pure transformers and other architectures. They work because genomic regulation involves both local patterns (motifs, nucleosome positioning) and long-range interactions (enhancer-promoter loops, chromatin domains). The CNN-transformer combination matches this multi-scale structure.</p>
<p>Long-range modifications adapt transformer attention for ultra-long genomic contexts. Genomic Interpreter uses 1D Swin transformers with hierarchical windowed attention, enabling megabase-scale modeling. Others use sparse attention patterns tailored to genomic structure, such as attending to fixed landmark positions (promoters, insulators) or using genomic distance-based sparsity patterns that assume nearby positions interact more strongly than distant ones.</p>
<p>These modifications share a common goal: reducing attention’s quadratic cost while preserving long-range capability. Success depends on whether the imposed structure matches actual genomic interactions. If enhancer-promoter loops occur at specific, regular spacing, fixed sparse patterns might work well. If interactions are highly variable and data-dependent, full attention or learned sparsity may be necessary.</p>
<p>Bidirectional versus causal attention has implications beyond generation. For variant effect prediction, bidirectional context allows the model to see both upstream and downstream changes when scoring a mutation. This can improve prediction quality because regulatory effects often depend on surrounding context. However, causal models may better capture directionality in processes like transcription or replication where 5’ to 3’ order matters mechanistically.</p>
<div class="content-visible callout callout-style-default callout-note callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>VISUAL SUGGESTION (transformer variants)</strong><br>
Figure: Comparison of four architectures with attention pattern diagrams and use cases. (a) Bidirectional encoder (DNABERT): full attention matrix shown as filled triangle. (b) Causal decoder (GenSLM): lower triangular attention matrix. (c) Hybrid (Enformer): CNN stem → downsampled sequence → transformer layers. (d) Sparse (Genomic Interpreter): blocked attention pattern with local and strided blocks.</p>
</div>
</div>
</section>
<section id="training-considerations" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="training-considerations"><span class="header-section-number">6.8</span> Training Considerations</h2>
<p>Training genomic transformers involves optimization algorithms, regularization strategies, and infrastructure choices that differ in important ways from natural language models.</p>
<p>Optimization for transformers typically uses Adam or AdamW, adaptive learning rate algorithms that maintain per-parameter learning rates adjusted based on gradient statistics. AdamW adds weight decay directly to the parameter updates rather than to the loss function, which improves training stability and generalization for transformers. Learning rate schedules typically use warmup for the first few thousand steps, linearly increasing the learning rate from zero to a peak value, then decay (either linear or cosine) for the remainder of training.</p>
<p>Warmup is particularly important for transformers because large learning rates early in training can cause instability when parameters are randomly initialized. Gradients in early iterations can be extremely large or small depending on initialization, and adaptive optimizers need time to build accurate gradient statistics. Warmup allows the optimizer to stabilize before applying full learning rates.</p>
<p>For genomics, learning rate tuning often requires domain-specific adjustment. Genomic sequences have different statistical properties than natural language, and the optimal learning rate may differ substantially from NLP defaults. Regulatory sequences with highly conserved motifs may require lower learning rates to avoid overfitting to these strong signals, while protein sequences with weaker conservation may benefit from higher learning rates that encourage exploration.</p>
<p>Regularization prevents overfitting, particularly important for genomic applications where training data may be limited compared to the large models we wish to train. Dropout randomly zeros out activations during training, forcing the network to learn robust features that do not depend on specific neurons. Attention dropout applies this to attention weights, randomly dropping connections between positions. This prevents the model from over-relying on specific position pairs and encourages learning of distributed representations.</p>
<p>Weight decay penalizes large parameter values, encouraging the model to use smaller, smoother weights. For transformers, weight decay is typically applied to all parameters except biases and layer normalization parameters. The weight decay coefficient must be tuned carefully: too little provides insufficient regularization, while too much overly constrains the model and reduces capacity.</p>
<p>Gradient issues plague deep network training. Vanishing gradients occur when gradients become extremely small as they backpropagate through many layers, preventing effective learning in early layers. Exploding gradients are the opposite problem where gradients grow exponentially, causing parameter updates that destabilize training. Transformers mitigate vanishing gradients through residual connections that provide direct gradient paths through the network. Exploding gradients are addressed through gradient clipping, which rescales gradients when their norm exceeds a threshold.</p>
<p>For genomic transformers, gradient issues often manifest differently than in language models. Genomic sequences have less hierarchical structure than natural language (no grammatical sentence structure), which affects gradient flow through attention layers. Imbalanced token frequencies (certain k-mers or amino acids appear much more often than others) can create gradient imbalances where common tokens receive large gradients while rare but biologically important tokens receive tiny gradients. Addressing this may require reweighting losses or using adaptive batch sampling.</p>
<p>Computational infrastructure for genomic transformer training typically requires distributed approaches. Single-GPU training suffices only for small models on short sequences. Multi-GPU data parallelism replicates the model across GPUs, splitting batches across devices and aggregating gradients. This scales well up to batch sizes limited by convergence requirements. Model parallelism splits the model itself across devices, necessary when models are too large to fit on a single GPU. Pipeline parallelism divides layers across devices and pipelines the forward and backward passes.</p>
<p>Mixed precision training (mentioned earlier) is nearly universal in genomic transformer training. Modern GPUs provide specialized tensor cores that accelerate 16-bit operations dramatically. For genomics, mixed precision typically provides 1.5-2× speedup with no loss in final model quality, though careful attention to loss scaling and overflow detection is required.</p>
<p>Batch size selection involves competing considerations. Larger batches provide more stable gradient estimates and better GPU utilization but require more memory and may reduce generalization. Genomic transformers often use gradient accumulation to simulate large batches: small batches are processed sequentially, gradients accumulated across them, then a single parameter update made. This provides the benefits of large batches without the memory cost.</p>
<div class="content-visible callout callout-style-default callout-note callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>VISUAL SUGGESTION (training dynamics)</strong><br>
Figure: Two-panel figure showing typical training curves. (a) Learning rate schedule showing warmup and decay phases. (b) Training and validation loss over time, with annotations for common issues (overfitting, underfitting, gradient instability) and when to apply different interventions (increase regularization, reduce learning rate, add gradient clipping).</p>
</div>
</div>
</section>
<section id="limitations-and-alternatives" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="limitations-and-alternatives"><span class="header-section-number">6.9</span> Limitations and Alternatives</h2>
<p>Despite their success, transformers have fundamental limitations that motivated development of alternative architectures. Understanding these limitations contextualizes recent innovations and guides architecture choice for specific genomic applications.</p>
<p>The quadratic complexity bottleneck is transformers’ most severe limitation. Computing all pairwise attention scores for a sequence of length <span class="math inline">\(L\)</span> requires <span class="math inline">\(O(L^2)\)</span> operations and memory. For genomic contexts of 100kb or more (roughly 100,000 single-nucleotide tokens), this becomes prohibitive. Even with sparse attention approximations, transformers struggle to scale to megabase-length contexts where some regulatory interactions occur.</p>
<p>Recent genomic models have pushed context lengths to impressive scales (Enformer handles 200kb, AlphaGenome approaches 1Mb), but these often rely on hybrid architectures with significant downsampling or hierarchical windowing that may miss certain long-range patterns. Pure transformers without such modifications remain limited to shorter contexts.</p>
<p>Alternative sub-quadratic architectures address this limitation directly. State space models (SSMs) like S4 and Mamba achieve linear complexity by representing sequences as continuous-time dynamical systems rather than discrete token-to-token interactions. These models maintain long-range memory through recurrent state updates while avoiding the quadratic attention bottleneck. Chapter <a href="p3-ch11-dna.html" class="quarto-xref"><span>Chapter 11</span></a> covers SSMs in detail, including Hyena DNA and the Evo family of genomic foundation models.</p>
<p>Hyena operators replace attention with long convolutions implemented efficiently in the Fourier domain, achieving sub-quadratic complexity. Mamba uses selective state space models that dynamically adjust state transitions based on input content. Both approaches show promise for ultra-long genomic contexts, potentially enabling whole-chromosome or even whole-genome modeling.</p>
<p>When should we prefer transformers over these alternatives? Transformers excel when global context matters but sequences are not extremely long (under 10-50kb depending on resources). Their attention maps provide some interpretability, showing which positions the model considers relevant for specific predictions. Transformers have extensive tooling and pretrained models available from NLP research that transfer readily to genomics.</p>
<p>CNNs remain preferable when computational efficiency is paramount and local patterns dominate. For splice site prediction or promoter classification where the relevant context spans at most a few hundred base pairs, a well-designed CNN may outperform transformers while using far fewer parameters and less compute. The inductive bias toward local patterns also provides regularization that helps with limited training data.</p>
<p>Hybrid approaches combining CNNs for local feature extraction with transformers for long-range integration often achieve the best of both worlds. As discussed in Chapter <a href="p3-ch14-hybrid.html" class="quarto-xref"><span>Chapter 14</span></a>, models like Enformer demonstrate that architectural combinations can outperform pure transformers or pure CNNs on real genomic tasks.</p>
<p>The transition to sub-quadratic architectures is ongoing. Early results suggest SSMs match or exceed transformers on some genomic benchmarks while scaling to longer contexts. However, transformers benefit from years of engineering optimization and extensive pretrained models. The genomics community is actively exploring whether SSMs’ theoretical advantages translate to practical improvements across diverse tasks.</p>
<div class="content-visible callout callout-style-default callout-note callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>VISUAL SUGGESTION (architecture comparison)</strong><br>
Figure: Comparison table or radar chart showing transformers, CNNs, and SSMs across multiple axes: sequence length capacity, parameter efficiency, interpretability, training stability, inference speed, and ecosystem maturity. Highlight trade-offs rather than declaring one superior.</p>
</div>
</div>
</section>
<section id="summary-and-forward-references" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="summary-and-forward-references"><span class="header-section-number">6.10</span> Summary and Forward References</h2>
<p>This chapter examined transformer architecture from first principles through genomic applications. Transformers introduced global self-attention that addresses CNNs’ limited receptive fields, enabling modeling of long-range genomic interactions. The core mechanisms involve queries, keys, and values computing position-dependent aggregation weights that determine how information flows through the sequence.</p>
<p>Positional encodings address transformers’ permutation invariance by injecting sequence order information. Multiple approaches exist (sinusoidal, learned, relative, RoPE), each with different properties for generalization and computational efficiency. Genomic applications must consider strand directionality and genomic coordinate systems when choosing positional encoding schemes.</p>
<p>The transformer block architecture combines multi-head attention with position-wise feed-forward networks, connected by residual connections and layer normalization. Stacking these blocks creates deep networks that learn hierarchical representations through repeated refinement. Design choices about depth, width, normalization placement, and component sizing affect model capacity and training stability.</p>
<p>Scaling transformers to genomic sequences involves trade-offs between parameters, context length, and compute. Quadratic attention complexity limits context lengths, motivating sparse attention patterns, hierarchical windowing, and hybrid architectures. Mixed precision training and gradient checkpointing enable larger models on available hardware.</p>
<p>Transformer variants for genomics include bidirectional encoders for representation learning (DNABERT, Nucleotide Transformer), causal decoders for generation (GenSLM), and hybrid architectures combining CNNs with attention (Enformer, Borzoi). Each variant suits different tasks based on whether bidirectional context or autoregressive structure is more important.</p>
<p>Training considerations specific to genomics include learning rate schedules with warmup, dropout and weight decay regularization, gradient clipping for stability, and distributed training infrastructure. Genomic sequences’ unique statistical properties (token frequency imbalance, weaker hierarchical structure than language) affect optimization in subtle ways that may require task-specific tuning.</p>
<p>Transformers’ quadratic complexity motivates alternatives, particularly state space models that achieve linear scaling while maintaining long-range capability. When to prefer transformers versus SSMs or CNNs depends on sequence length requirements, local versus global pattern importance, computational resources, and whether extensive pretrained models are available.</p>
<p>Looking forward, transformers remain central to genomic foundation models despite emerging alternatives. Chapter <a href="p3-ch11-dna.html" class="quarto-xref"><span>Chapter 11</span></a> surveys DNA language models built on transformer and SSM architectures, trained through self-supervised objectives. Chapter <a href="p3-ch13-plm.html" class="quarto-xref"><span>Chapter 13</span></a> covers protein language models where transformers have achieved remarkable success at learning structure and function from sequence alone. Chapter <a href="p3-ch14-hybrid.html" class="quarto-xref"><span>Chapter 14</span></a> examines how regulatory prediction models combine transformers with CNNs to handle long genomic contexts efficiently.</p>
<p>The transformer architecture represents a watershed moment in genomic deep learning, enabling models that genuinely capture long-range dependencies essential for understanding gene regulation. While newer architectures may supersede transformers for some applications, the conceptual framework they introduced, particularly the idea that global context aggregation through learned attention is tractable and effective, continues to shape genomic modeling.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-vaswani_attention_2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention <span>Is</span> <span>All</span> <span>You</span> <span>Need</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p2-ch05-tokens.html" class="pagination-link" aria-label="Sequence Representation &amp; Tokens">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p2-ch07-foundation.html" class="pagination-link" aria-label="Genomic Foundation Models: Concepts &amp; Taxonomy">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>