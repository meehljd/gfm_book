<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Attention and Transformers – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p2-ch08-pretraining.html" rel="next">
<link href="./p2-ch06-cnn.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part II: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch07-attention.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Attention and Transformers</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation and Tokenization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Models for Genomic Sequence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-attention.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Attention and Transformers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives and Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning and Adaptation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">The Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Long-Context Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Variant Effect Prediction with Foundation Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p4--multi-scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Multi-Scale Modeling</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RNA Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Single-Cell and Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">3D Genome and Spatial Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch18-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch19-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Evaluation Methodology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability and Mechanism</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Rare Disease and Variant Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Drug Discovery and Target Identification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch28-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Sequence Design and Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch29-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Regulatory, Ethical, and Future Considerations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Training Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-self-attention-mechanism" id="toc-the-self-attention-mechanism" class="nav-link active" data-scroll-target="#the-self-attention-mechanism"><span class="header-section-number">7.1</span> The Self-Attention Mechanism</a>
  <ul class="collapse">
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention"><span class="header-section-number">7.1.1</span> Multi-Head Attention</a></li>
  <li><a href="#computational-cost" id="toc-computational-cost" class="nav-link" data-scroll-target="#computational-cost"><span class="header-section-number">7.1.2</span> Computational Cost</a></li>
  </ul></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding"><span class="header-section-number">7.2</span> Positional Encoding</a>
  <ul class="collapse">
  <li><a href="#absolute-position-encodings" id="toc-absolute-position-encodings" class="nav-link" data-scroll-target="#absolute-position-encodings"><span class="header-section-number">7.2.1</span> Absolute Position Encodings</a></li>
  <li><a href="#relative-position-encodings" id="toc-relative-position-encodings" class="nav-link" data-scroll-target="#relative-position-encodings"><span class="header-section-number">7.2.2</span> Relative Position Encodings</a></li>
  <li><a href="#genomic-position-considerations" id="toc-genomic-position-considerations" class="nav-link" data-scroll-target="#genomic-position-considerations"><span class="header-section-number">7.2.3</span> Genomic Position Considerations</a></li>
  </ul></li>
  <li><a href="#the-transformer-block" id="toc-the-transformer-block" class="nav-link" data-scroll-target="#the-transformer-block"><span class="header-section-number">7.3</span> The Transformer Block</a>
  <ul class="collapse">
  <li><a href="#block-components" id="toc-block-components" class="nav-link" data-scroll-target="#block-components"><span class="header-section-number">7.3.1</span> Block Components</a></li>
  <li><a href="#information-flow" id="toc-information-flow" class="nav-link" data-scroll-target="#information-flow"><span class="header-section-number">7.3.2</span> Information Flow</a></li>
  </ul></li>
  <li><a href="#scaling-to-genomic-sequences" id="toc-scaling-to-genomic-sequences" class="nav-link" data-scroll-target="#scaling-to-genomic-sequences"><span class="header-section-number">7.4</span> Scaling to Genomic Sequences</a>
  <ul class="collapse">
  <li><a href="#parameter-scaling" id="toc-parameter-scaling" class="nav-link" data-scroll-target="#parameter-scaling"><span class="header-section-number">7.4.1</span> Parameter Scaling</a></li>
  <li><a href="#context-length-strategies" id="toc-context-length-strategies" class="nav-link" data-scroll-target="#context-length-strategies"><span class="header-section-number">7.4.2</span> Context Length Strategies</a></li>
  <li><a href="#memory-and-precision" id="toc-memory-and-precision" class="nav-link" data-scroll-target="#memory-and-precision"><span class="header-section-number">7.4.3</span> Memory and Precision</a></li>
  </ul></li>
  <li><a href="#architectural-variants-for-genomics" id="toc-architectural-variants-for-genomics" class="nav-link" data-scroll-target="#architectural-variants-for-genomics"><span class="header-section-number">7.5</span> Architectural Variants for Genomics</a>
  <ul class="collapse">
  <li><a href="#encoder-only-transformers" id="toc-encoder-only-transformers" class="nav-link" data-scroll-target="#encoder-only-transformers"><span class="header-section-number">7.5.1</span> Encoder-Only Transformers</a></li>
  <li><a href="#decoder-only-transformers" id="toc-decoder-only-transformers" class="nav-link" data-scroll-target="#decoder-only-transformers"><span class="header-section-number">7.5.2</span> Decoder-Only Transformers</a></li>
  <li><a href="#encoder-decoder-transformers" id="toc-encoder-decoder-transformers" class="nav-link" data-scroll-target="#encoder-decoder-transformers"><span class="header-section-number">7.5.3</span> Encoder-Decoder Transformers</a></li>
  <li><a href="#hybrid-cnn-transformer-models" id="toc-hybrid-cnn-transformer-models" class="nav-link" data-scroll-target="#hybrid-cnn-transformer-models"><span class="header-section-number">7.5.4</span> Hybrid CNN-Transformer Models</a></li>
  <li><a href="#long-context-modifications" id="toc-long-context-modifications" class="nav-link" data-scroll-target="#long-context-modifications"><span class="header-section-number">7.5.5</span> Long-Context Modifications</a></li>
  </ul></li>
  <li><a href="#training-dynamics" id="toc-training-dynamics" class="nav-link" data-scroll-target="#training-dynamics"><span class="header-section-number">7.6</span> Training Dynamics</a>
  <ul class="collapse">
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization"><span class="header-section-number">7.6.1</span> Optimization</a></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="header-section-number">7.6.2</span> Regularization</a></li>
  <li><a href="#gradient-stability" id="toc-gradient-stability" class="nav-link" data-scroll-target="#gradient-stability"><span class="header-section-number">7.6.3</span> Gradient Stability</a></li>
  <li><a href="#distributed-training" id="toc-distributed-training" class="nav-link" data-scroll-target="#distributed-training"><span class="header-section-number">7.6.4</span> Distributed Training</a></li>
  </ul></li>
  <li><a href="#limitations-and-emerging-alternatives" id="toc-limitations-and-emerging-alternatives" class="nav-link" data-scroll-target="#limitations-and-emerging-alternatives"><span class="header-section-number">7.7</span> Limitations and Emerging Alternatives</a>
  <ul class="collapse">
  <li><a href="#the-quadratic-barrier" id="toc-the-quadratic-barrier" class="nav-link" data-scroll-target="#the-quadratic-barrier"><span class="header-section-number">7.7.1</span> The Quadratic Barrier</a></li>
  <li><a href="#state-space-models" id="toc-state-space-models" class="nav-link" data-scroll-target="#state-space-models"><span class="header-section-number">7.7.2</span> State Space Models</a></li>
  <li><a href="#choosing-architectures" id="toc-choosing-architectures" class="nav-link" data-scroll-target="#choosing-architectures"><span class="header-section-number">7.7.3</span> Choosing Architectures</a></li>
  </ul></li>
  <li><a href="#from-architecture-to-learning" id="toc-from-architecture-to-learning" class="nav-link" data-scroll-target="#from-architecture-to-learning"><span class="header-section-number">7.8</span> From Architecture to Learning</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part II: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch07-attention.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Attention and Transformers</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-attention" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Attention and Transformers</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Convolutional networks excel at local patterns but fail at long-range dependencies. When a regulatory element 50 kilobases upstream controls gene expression, no practical CNN architecture can span that distance. The receptive field would require hundreds of layers and billions of parameters to propagate information across such distances. This limitation extends beyond theoretical inconvenience: enhancer-promoter interactions routinely occur at these scales, topologically associating domains organize chromatin contacts across megabases, and GWAS variants often lie far from coding sequences, exerting effects through distal regulatory mechanisms. The convolutional models examined in <a href="p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> capture motifs, chromatin marks, and local regulatory grammar with remarkable fidelity, yet they remain fundamentally blind to the long-range interactions that govern much of gene regulation.</p>
<p>The attention mechanism, introduced for machine translation in 2017 <span class="citation" data-cites="vaswani_attention_2017">(<a href="references.html#ref-vaswani_attention_2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span>, resolved this tension by abandoning sequential processing entirely. Rather than propagating information through layers of local operations, attention computes direct interactions between all positions in a single operation. A position near a gene’s promoter can attend directly to an enhancer 100 kilobases away without information passing through intermediate layers. The key insight was not incremental improvement to convolutional receptive fields but a fundamentally different approach to sequence modeling: let each position query all other positions and aggregate information based on learned relevance. This architectural shift enabled the foundation models that now dominate genomic AI, from protein language models like ESM that learn structure from sequence alone to regulatory models like Enformer that predict expression from 200-kilobase contexts.</p>
<p>This chapter examines the mechanisms that make transformers effective for genomic data. We unpack self-attention mathematically while maintaining intuition for what the computation accomplishes biologically. We address positional encodings (the solution to attention’s position-blindness), the transformer block architecture, and scaling considerations specific to genomic sequences that can span millions of bases. We survey transformer variants used across genomic applications and confront the quadratic complexity that ultimately motivates the state space models examined in <a href="p3-ch11-dna-lm.html" class="quarto-xref"><span>Chapter 11</span></a>.</p>
<section id="the-self-attention-mechanism" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="the-self-attention-mechanism"><span class="header-section-number">7.1</span> The Self-Attention Mechanism</h2>
<p>Understanding a variant’s regulatory impact often requires integrating information across thousands of base pairs. A transcription factor binding site gains meaning from the promoter it activates, the insulators that constrain its range, and the chromatin state that modulates accessibility. CNNs force this information to propagate through many layers, each aggregating only local context. Self-attention instead computes all pairwise interactions simultaneously, allowing the model to directly relate any position to any other regardless of distance. Where convolutions apply fixed filters uniformly across the sequence, attention performs dynamic routing: each position queries the entire sequence and aggregates information based on content-dependent relevance scores. The “routing” changes for every input because attention weights depend on what the sequence contains, not just where positions sit relative to each other.</p>
<p>At each position in the input sequence, self-attention computes three vectors: a query, a key, and a value. These vectors emerge from multiplying the input embedding at that position by three learned weight matrices <span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>, and <span class="math inline">\(W^V\)</span>. The query represents what information this position seeks from other positions. The key represents what information this position offers to queries from elsewhere. The value represents the actual information this position contributes when attended to. This query-key-value structure separates the question of “which positions should interact” (determined by query-key similarity) from “what information flows between them” (determined by values).</p>
<p>The attention mechanism computes similarity scores between each query and all keys. For position <span class="math inline">\(i\)</span>, we compute the dot product between its query <span class="math inline">\(q_i\)</span> and every key <span class="math inline">\(k_j\)</span> across all positions <span class="math inline">\(j = 1, \ldots, L\)</span>, where <span class="math inline">\(L\)</span> is sequence length. These scores are scaled by <span class="math inline">\(\sqrt{d_k}\)</span> (the square root of the key dimension) to prevent the dot products from growing large in high dimensions, which would push softmax outputs toward extreme values and create vanishing gradients:</p>
<p><span class="math display">\[
\text{score}(q_i, k_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
\]</span></p>
<p>A softmax function converts these scores into attention weights <span class="math inline">\(\alpha_{ij}\)</span> that form a probability distribution over positions:</p>
<p><span class="math display">\[
\alpha_{ij} = \frac{\exp(\text{score}(q_i, k_j))}{\sum_{j'=1}^L \exp(\text{score}(q_i, k_{j'}))}
\]</span></p>
<p>These weights determine how strongly position <span class="math inline">\(i\)</span> attends to each other position. High weight means position <span class="math inline">\(i\)</span> aggregates substantial information from position <span class="math inline">\(j\)</span>; low weight means position <span class="math inline">\(j\)</span> contributes little to the output at position <span class="math inline">\(i\)</span>. The final output at position <span class="math inline">\(i\)</span> is a weighted sum of all value vectors:</p>
<p><span class="math display">\[
\text{output}_i = \sum_{j=1}^L \alpha_{ij} v_j
\]</span></p>
<p>This weighted aggregation is the core of self-attention. Each output position receives a mixture of information from across the entire sequence, with mixture proportions learned through backpropagation. For genomic sequences, this means a position near a splice site can attend to both the upstream exon and downstream intron, integrating context that determines whether splicing occurs. A position in a promoter can attend to distant enhancers, learning which distal elements influence expression at this gene.</p>
<section id="multi-head-attention" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="multi-head-attention"><span class="header-section-number">7.1.1</span> Multi-Head Attention</h3>
<p>A single attention operation learns one pattern of position interactions. Multi-head attention extends this by running multiple attention operations in parallel, each with independent learned projections. If we use <span class="math inline">\(H\)</span> heads, we split the model dimension <span class="math inline">\(d\)</span> into <span class="math inline">\(H\)</span> subspaces of dimension <span class="math inline">\(d/H\)</span>, compute separate queries, keys, and values for each head, run attention independently, concatenate outputs, and project back to dimension <span class="math inline">\(d\)</span>.</p>
<p>Different heads can specialize in different interaction types. In genomic models, one head might attend to nearby positions (capturing local motif context) while another attends to positions at characteristic distances (capturing nucleosome spacing or enhancer-promoter loops). Empirically, attention heads in trained genomic models show diverse patterns: some attend locally regardless of content, others attend to specific sequence motifs, and still others show distance-dependent patterns suggestive of chromatin organization. This specialization emerges from training without explicit supervision, reflecting the model’s discovery that different types of interactions require different aggregation patterns.</p>
<p>The multi-head structure also provides redundancy that aids training. If one head fails to learn useful patterns, others can compensate. Gradient flow through multiple parallel paths stabilizes optimization. For genomic applications where training data may be limited compared to natural language, this redundancy helps prevent individual heads from overfitting to spurious correlations.</p>
</section>
<section id="computational-cost" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="computational-cost"><span class="header-section-number">7.1.2</span> Computational Cost</h3>
<p>The computational cost of self-attention scales quadratically with sequence length. Computing all pairwise attention scores requires <span class="math inline">\(L \times L\)</span> operations. For a 10-kilobase sequence tokenized at single-nucleotide resolution, this means 100 million attention computations per layer. A 200-kilobase sequence requires 40 billion computations per layer. Memory requirements scale similarly because the attention matrix must be stored for backpropagation.</p>
<p>This quadratic scaling creates a fundamental tension between context length and computational tractability. Genomic applications demand long contexts (enhancer-promoter distances, gene bodies, regulatory domains), but transformer attention becomes prohibitively expensive precisely where long contexts matter most. A 1-megabase context (modest by genomic standards) would require <span class="math inline">\(10^{12}\)</span> attention computations per layer with standard self-attention. This tension motivates both the efficient attention variants discussed later in this chapter and the state space models examined in <a href="p3-ch11-dna-lm.html" class="quarto-xref"><span>Chapter 11</span></a>.</p>
</section>
</section>
<section id="positional-encoding" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="positional-encoding"><span class="header-section-number">7.2</span> Positional Encoding</h2>
<p>Self-attention has a critical limitation: it is permutation invariant. Shuffling input token order changes nothing about how attention weights are computed because the same queries, keys, and values exist regardless of position. The model has no inherent notion of sequence order. For genomic data where position matters fundamentally (5’ to 3’ directionality, distance from transcription start sites, strand orientation), this blindness to order would be catastrophic.</p>
<p>Positional encodings inject information about token positions into the model, a challenge first introduced in <span class="quarto-unresolved-ref">?sec-embeddings</span> when discussing how models represent sequence information. The approach varies across methods, but the goal is consistent: break permutation invariance by making the model aware of where each token sits in the sequence.</p>
<section id="absolute-position-encodings" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="absolute-position-encodings"><span class="header-section-number">7.2.1</span> Absolute Position Encodings</h3>
<p>The original transformer used sinusoidal functions with different frequencies for each embedding dimension. For position <span class="math inline">\(pos\)</span> and dimension <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
\]</span></p>
<p><span class="math display">\[
\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
\]</span></p>
<p>These fixed patterns have useful properties. They are deterministic (the same for all sequences), allow the model to learn to attend by relative positions (since <span class="math inline">\(\text{PE}(pos+k)\)</span> can be expressed as a linear function of <span class="math inline">\(\text{PE}(pos)\)</span>), and generalize to sequence lengths not seen during training. The different frequencies across dimensions create a unique “fingerprint” for each position that the model can learn to decode.</p>
<p>Many genomic models use learned positional embeddings instead: lookup tables where each position has a learned vector added to the input embedding. Learned embeddings offer flexibility, allowing the model to discover position-dependent patterns specific to genomic data (such as characteristic distances for nucleosome positioning or regulatory element spacing). The trade-off is that learned embeddings require training and do not automatically extrapolate to longer sequences.</p>
</section>
<section id="relative-position-encodings" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="relative-position-encodings"><span class="header-section-number">7.2.2</span> Relative Position Encodings</h3>
<p>Absolute encodings treat position 1,000 and position 1,001 as having different representations even though their relative relationship (adjacent positions) may matter more than their absolute locations. Relative positional encodings address this by encoding distances between positions rather than absolute coordinates.</p>
<p>T5-style relative position bias adds a learnable scalar to attention scores based on the distance between query and key positions. This helps the model learn that nearby positions often interact more strongly than distant ones while remaining agnostic about absolute position in the sequence.</p>
<p>Attention with Linear Biases (ALiBi) adds a fixed linear penalty to attention scores based on distance, without learned parameters. For a head with slope <span class="math inline">\(m\)</span>, attention between positions separated by distance <span class="math inline">\(|i - j|\)</span> is penalized by <span class="math inline">\(m|i - j|\)</span>. Different heads use different slopes, encouraging some to focus locally and others globally. ALiBi generalizes well to longer contexts than seen during training because the linear penalty extrapolates naturally, making it attractive for genomic applications where sequence length varies dramatically.</p>
<p>Rotary Position Embeddings (RoPE) encode positions by rotating query and key vectors in a high-dimensional space, with rotation angle depending on position. The dot product between rotated query and key depends on their relative distance, combining benefits of relative encoding with efficient implementation. RoPE has become common in recent language models and appears increasingly in genomic transformers.</p>
</section>
<section id="genomic-position-considerations" class="level3" data-number="7.2.3">
<h3 data-number="7.2.3" class="anchored" data-anchor-id="genomic-position-considerations"><span class="header-section-number">7.2.3</span> Genomic Position Considerations</h3>
<p>Genomic sequences impose additional requirements on positional encoding. DNA has strand directionality: ACGT on the forward strand has different regulatory meaning than the same sequence on the reverse strand (which reads ACGT as TGCA from the other direction). Positional encodings should enable the model to learn strand-specific patterns. Some genomic transformers use separate embeddings for forward and reverse strands; others rely on the model learning strand orientation from sequence content.</p>
<p>Genomic coordinates pose another design choice. Should position 1 correspond to a fixed genomic landmark (transcription start site, gene start) or simply indicate sequence order without biological reference? Models predicting regulatory activity often center sequences on promoters, using positions relative to the TSS. Foundation models trained on random genomic segments typically use positional encodings reflecting sequence order without genomic coordinate reference.</p>
<p>The choice of positional encoding interacts with tokenization (<a href="p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>). K-mer tokenization reduces sequence length (and thus attention cost) but changes what “position” means: position 1 might represent nucleotides 1-6 rather than a single base. Positional encodings must be interpreted relative to the tokenization scheme, and different combinations may suit different applications.</p>
</section>
</section>
<section id="the-transformer-block" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="the-transformer-block"><span class="header-section-number">7.3</span> The Transformer Block</h2>
<p>A transformer model consists of stacked blocks, each refining the sequence representation through attention and feed-forward processing. Understanding this block structure illuminates how information flows through transformer models and why certain design choices matter.</p>
<section id="block-components" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="block-components"><span class="header-section-number">7.3.1</span> Block Components</h3>
<p>Each transformer block contains two main components: a multi-head self-attention layer and a position-wise feed-forward network. The attention layer enables global communication, allowing each position to gather information from the entire sequence. The feed-forward network processes each position independently, applying nonlinear transformations to the aggregated information.</p>
<p>The feed-forward network consists of two linear transformations with a nonlinearity between them. Typically, this expands the dimension by a factor of four (from model dimension <span class="math inline">\(d\)</span> to <span class="math inline">\(4d\)</span>), applies GELU or similar activation, then projects back to dimension <span class="math inline">\(d\)</span>. This expansion allows processing through a high-dimensional nonlinear transformation before producing output for the next layer. The position-wise nature means each position is transformed identically but independently; cross-position information flows only through attention.</p>
<p>Layer normalization stabilizes training by normalizing activations across the feature dimension at each position. Two conventions exist for placement. Post-norm places normalization after each sublayer, applying it to output before the residual connection. Pre-norm places normalization before each sublayer, normalizing input to attention or feed-forward operations. Pre-norm has become more common because it improves training stability for deep networks, though post-norm can achieve slightly better final performance with careful tuning.</p>
<p>Residual connections wrap around both attention and feed-forward sublayers, adding input directly to output. These connections serve two critical functions. First, they provide gradient highways during backpropagation, allowing gradients to flow directly through many layers without repeated transformation. This enables training of very deep networks. Second, they create an inductive bias toward incremental refinement: each layer makes small adjustments to the representation rather than constructing entirely new representations from scratch.</p>
</section>
<section id="information-flow" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="information-flow"><span class="header-section-number">7.3.2</span> Information Flow</h3>
<p>The flow through a pre-norm transformer block proceeds as follows. Input <span class="math inline">\(X\)</span> is normalized, processed by multi-head attention to produce <span class="math inline">\(X'\)</span>, and added back via residual connection, yielding <span class="math inline">\(X + X'\)</span>. This sum is normalized, passed through the feed-forward network to produce <span class="math inline">\(X''\)</span>, and added via another residual connection, yielding final output <span class="math inline">\(X + X' + X''\)</span>. Each layer thus adds refinements to the representation while preserving information from earlier processing.</p>
<p>Stacking depth determines how many times this refinement occurs. Shallow transformers (few layers) are parameter-efficient but may lack capacity for complex tasks. Deep transformers (many layers) can learn sophisticated representations but require more computation and careful optimization. Most genomic transformers use 6 to 24 layers, varying by application. Models for short sequences (small RNAs, peptides) might use fewer layers, while foundation models for long genomic contexts often use deeper stacks to build hierarchical representations.</p>
<p>The choice of depth balances capacity against trainability. Deeper networks learn more complex functions but are harder to optimize, prone to overfitting without sufficient data, and more expensive at training and inference. For genomics, depth often correlates with the complexity of patterns being modeled. Simple motif tasks might benefit more from wider layers (larger <span class="math inline">\(d\)</span>) than deeper stacks, while tasks requiring hierarchical integration (promoter-enhancer-TAD relationships) may benefit from additional depth that builds increasingly abstract representations.</p>
</section>
</section>
<section id="scaling-to-genomic-sequences" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="scaling-to-genomic-sequences"><span class="header-section-number">7.4</span> Scaling to Genomic Sequences</h2>
<p>Genomic sequences present unique scaling challenges that distinguish them from the text sequences for which transformers were originally designed. DNA sequences span millions of bases, far exceeding typical context lengths in natural language processing. Tokenization choices interact with sequence length: single-nucleotide tokens create very long sequences, while k-mer tokens reduce length at the cost of vocabulary size. Effective application of transformers to genomics requires careful balance of model size, context length, and computational resources.</p>
<section id="parameter-scaling" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="parameter-scaling"><span class="header-section-number">7.4.1</span> Parameter Scaling</h3>
<p>Transformer parameters come primarily from two sources: width and depth. Width (model dimension <span class="math inline">\(d\)</span>) determines embedding and hidden state sizes. Increasing width allows more complex pattern representation at each position but increases parameters quadratically because weight matrices scale as <span class="math inline">\(d \times d\)</span>. Depth (number of layers) determines how many refinement steps occur. Increasing depth allows hierarchical abstractions through repeated processing but increases parameters linearly.</p>
<p>Scaling laws from natural language processing suggest performance improves smoothly with increased parameters, data, and compute. Similar principles apply to genomics, though optimal ratios differ. Genomic sequences are less compressible than natural language: each nucleotide carries less predictable information than words in structured sentences. This suggests genomic models might benefit relatively more from depth (more processing of limited information per position) than from width (more dimensions per position).</p>
</section>
<section id="context-length-strategies" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="context-length-strategies"><span class="header-section-number">7.4.2</span> Context Length Strategies</h3>
<p>Standard self-attention’s <span class="math inline">\(O(L^2)\)</span> complexity becomes prohibitive for long genomic contexts. Several strategies address this bottleneck.</p>
<p>Sparse attention patterns restrict which positions attend to which others. Local windowing allows each position to attend only within a fixed window, reducing complexity to <span class="math inline">\(O(Lw)\)</span> where <span class="math inline">\(w\)</span> is window size. This works when most relevant interactions are local, as often holds for regulatory sequences where nearby elements interact more strongly than distant ones. The trade-off is missing important long-range interactions that fall outside windows.</p>
<p>Strided attention creates hierarchy: lower layers use local windows while upper layers attend to every <span class="math inline">\(k\)</span>-th position. This captures both local fine-grained patterns and global coarse-grained structure while maintaining sub-quadratic complexity. Hybrid models like Enformer (<a href="p3-ch13-regulatory.html" class="quarto-xref"><span>Chapter 13</span></a>) apply CNNs to downsample sequences before transformer layers, reducing the effective sequence length that attention must handle.</p>
<p>Approximations to full attention offer another approach. Linformer approximates the attention matrix through low-rank decomposition, reducing complexity to linear in sequence length. Performer uses random feature methods to approximate attention scores without explicitly computing the full <span class="math inline">\(L \times L\)</span> matrix. These approximations trade some expressiveness for efficiency and may miss certain long-range dependencies.</p>
</section>
<section id="memory-and-precision" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="memory-and-precision"><span class="header-section-number">7.4.3</span> Memory and Precision</h3>
<p>Memory requirements compound computational challenges. Training requires storing activations for backpropagation, and attention matrices are particularly memory-intensive. Gradient checkpointing trades compute for memory by recomputing activations during the backward pass rather than storing them. This enables training larger models or longer sequences on fixed hardware at the cost of additional computation time.</p>
<p>Mixed precision training uses 16-bit floating point for most computations while maintaining 32-bit precision for critical operations like loss computation and optimizer updates. Modern GPUs accelerate 16-bit arithmetic substantially, providing near 2× speedup with minimal precision loss. Genomic transformers routinely use mixed precision to train on longer sequences or with larger batch sizes than full precision would allow.</p>
<p>For genomic applications, the choice among scaling strategies depends on biological context. Regulatory prediction often benefits from local windowing because nearby elements dominate most enhancer-promoter interactions. Foundation models predicting masked tokens may require global attention because prediction requires integrating information from across entire sequences. Variant effect prediction sometimes requires attending to specific distal elements that sparse patterns might miss.</p>
</section>
</section>
<section id="architectural-variants-for-genomics" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="architectural-variants-for-genomics"><span class="header-section-number">7.5</span> Architectural Variants for Genomics</h2>
<p>The transformer architecture has been adapted in diverse ways for genomic modeling. Understanding these variants illuminates design trade-offs and guides architecture selection for specific applications.</p>
<section id="encoder-only-transformers" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="encoder-only-transformers"><span class="header-section-number">7.5.1</span> Encoder-Only Transformers</h3>
<p>Encoder-only transformers process sequences bidirectionally, allowing each position to attend to all other positions including those that follow in the sequence. DNABERT and Nucleotide Transformer exemplify this architecture, trained with masked language modeling objectives where random tokens are masked and predicted from bidirectional context.</p>
<p>Bidirectional attention suits tasks where both upstream and downstream context matters for understanding a position. Transcription factor binding sites are influenced by flanking sequence in both directions. Protein structures depend on residues throughout the chain, not just N-terminal sequence. Variant effects may depend on context from both sides of the mutation.</p>
<p>Encoder-only models excel at representation learning: producing embeddings that capture biological properties useful for downstream tasks. These embeddings can feed into variant effect predictors, function classifiers, or other models. The bidirectional context produces richer representations than unidirectional processing, but encoder-only architectures cannot generate sequences autoregressively because they require seeing the full sequence to produce any output.</p>
</section>
<section id="decoder-only-transformers" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="decoder-only-transformers"><span class="header-section-number">7.5.2</span> Decoder-Only Transformers</h3>
<p>Decoder-only transformers use causal attention where each position attends only to itself and preceding positions. This enables autoregressive generation: the model produces sequences one token at a time, conditioning each new token on all previous tokens. GenSLM and other genomic models trained on next-token prediction use this architecture.</p>
<p>Causal attention is essential for generation tasks but produces less rich representations for fixed sequences because each position sees only partial context. The choice between encoder and decoder architectures reflects a fundamental tension: representation learning benefits from bidirectional context, while generation requires causal structure. Some applications use both, with a bidirectional encoder producing initial representations that a causal decoder refines for generation or autoregressive prediction.</p>
</section>
<section id="encoder-decoder-transformers" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="encoder-decoder-transformers"><span class="header-section-number">7.5.3</span> Encoder-Decoder Transformers</h3>
<p>Encoder-decoder architectures combine bidirectional encoding with autoregressive decoding. The encoder processes an input sequence with full bidirectional attention, producing contextualized representations. The decoder then generates output tokens autoregressively, attending both to its own previous outputs (through causal self-attention) and to encoder representations (through cross-attention). This cross-attention allows each decoder position to query the full encoded input when generating output.</p>
<p>Encoder-decoder models suit sequence-to-sequence tasks where input and output differ in length or structure. In natural language processing, machine translation exemplifies this pattern: encode a sentence in one language, decode into another. Genomic applications are less common but include tasks like predicting protein sequences from DNA (where input and output alphabets differ) or generating variant descriptions from sequence context. Most genomic foundation models favor encoder-only or decoder-only architectures because their primary tasks (representation learning, sequence generation) align naturally with those simpler structures. Encoder-decoder overhead provides little benefit when input and output share the same sequence space.</p>
</section>
<section id="hybrid-cnn-transformer-models" class="level3" data-number="7.5.4">
<h3 data-number="7.5.4" class="anchored" data-anchor-id="hybrid-cnn-transformer-models"><span class="header-section-number">7.5.4</span> Hybrid CNN-Transformer Models</h3>
<p>Hybrid architectures combine convolutional layers with transformer blocks. Enformer and Borzoi (<a href="p3-ch13-regulatory.html" class="quarto-xref"><span>Chapter 13</span></a>) apply convolutional stems to long sequences, downsampling through pooling, then pass compressed representations through transformer layers. This exploits CNNs’ efficiency for local pattern extraction while using transformers for long-range integration.</p>
<p>These hybrids achieve state-of-the-art performance on regulatory prediction tasks. They work because genomic regulation involves both local patterns (motifs, nucleosome positioning signals) and long-range interactions (enhancer-promoter loops, TAD boundaries). The CNN-transformer combination matches this multi-scale structure: CNNs handle the local motif grammar while transformers integrate across the broader regulatory landscape.</p>
<p>Hybrid approaches also address the quadratic attention bottleneck indirectly. By downsampling sequences before transformer layers (often by factors of 128 or more), they reduce effective sequence length and thus attention cost. A 200-kilobase genomic region might be compressed to roughly 1,500 positions after CNN processing, making full attention tractable. The cost is loss of single-nucleotide resolution in the transformer layers, though the CNN stem preserves local detail.</p>
</section>
<section id="long-context-modifications" class="level3" data-number="7.5.5">
<h3 data-number="7.5.5" class="anchored" data-anchor-id="long-context-modifications"><span class="header-section-number">7.5.5</span> Long-Context Modifications</h3>
<p>Several approaches adapt transformer attention specifically for ultra-long genomic contexts. Hierarchical windowed attention (as in Swin transformers adapted for 1D sequences) applies attention within local windows at lower layers, then merges windows at higher layers to enable cross-window communication. This maintains local resolution while building global context hierarchically.</p>
<p>Other approaches use sparse attention patterns tailored to genomic structure. One might attend to fixed landmark positions (known promoters, insulators, or TAD boundaries) or use genomic distance-based sparsity that assumes nearby positions interact more strongly than distant ones. Success depends on whether imposed structure matches actual genomic interactions. If enhancer-promoter loops occur at predictable spacing, fixed sparse patterns work well. If interactions are highly variable and context-dependent, full attention or learned sparsity may be necessary.</p>
</section>
</section>
<section id="training-dynamics" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="training-dynamics"><span class="header-section-number">7.6</span> Training Dynamics</h2>
<p>Training genomic transformers involves optimization algorithms, regularization strategies, and stability considerations that differ in important ways from text-domain models.</p>
<section id="optimization" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="optimization"><span class="header-section-number">7.6.1</span> Optimization</h3>
<p>Transformers typically train with Adam or AdamW, adaptive learning rate algorithms that maintain per-parameter learning rates adjusted based on gradient statistics. AdamW applies weight decay directly to parameter updates rather than to the loss function, improving generalization and training stability. Learning rate schedules typically use warmup (linearly increasing learning rate from near-zero to peak over the first several thousand steps) followed by decay (linear or cosine decrease over the remaining training).</p>
<p>Warmup addresses a specific instability: transformers with random initialization can produce extreme gradients early in training, and adaptive optimizers need time to build accurate gradient statistics. Warmup allows the optimizer to stabilize before applying full learning rates. Skipping warmup often causes training collapse in the first few hundred steps.</p>
<p>For genomics, learning rate tuning may require adjustment from NLP defaults. Genomic sequences have different statistical properties than natural language, and optimal learning rates may differ substantially. Regulatory sequences with highly conserved motifs may require lower learning rates to avoid overfitting to these strong signals. Protein sequences with weaker positional conservation may benefit from higher rates that encourage exploration.</p>
</section>
<section id="regularization" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="regularization"><span class="header-section-number">7.6.2</span> Regularization</h3>
<p>Regularization prevents overfitting, particularly important when training data is limited relative to model size. Dropout randomly zeros activations during training, forcing the network to learn robust features independent of specific neurons. Attention dropout applies this to attention weights, randomly dropping connections between positions. This prevents over-reliance on specific position pairs and encourages distributed representations.</p>
<p>Weight decay penalizes large parameter values, encouraging smaller, smoother weights. For transformers, weight decay is typically applied to all parameters except biases and layer normalization parameters. The coefficient requires careful tuning: too little provides insufficient regularization; too much constrains capacity and reduces model expressiveness.</p>
</section>
<section id="gradient-stability" class="level3" data-number="7.6.3">
<h3 data-number="7.6.3" class="anchored" data-anchor-id="gradient-stability"><span class="header-section-number">7.6.3</span> Gradient Stability</h3>
<p>Gradient issues plague deep network training. Vanishing gradients occur when gradients become extremely small through many layers, preventing learning in early layers. Exploding gradients are the opposite, where gradients grow exponentially and destabilize training. Transformers mitigate vanishing gradients through residual connections that provide direct gradient paths. Exploding gradients are addressed through gradient clipping, which rescales gradients when their norm exceeds a threshold.</p>
<p>For genomic transformers, gradient issues manifest differently than in language models. Genomic sequences have less hierarchical structure than natural language (no grammatical sentence organization), which affects gradient flow through attention layers. Imbalanced token frequencies create gradient imbalances: common k-mers or amino acids receive large gradients while rare but biologically important tokens receive small gradients. Addressing this may require loss reweighting or adaptive sampling that ensures rare tokens appear frequently enough for effective learning.</p>
</section>
<section id="distributed-training" class="level3" data-number="7.6.4">
<h3 data-number="7.6.4" class="anchored" data-anchor-id="distributed-training"><span class="header-section-number">7.6.4</span> Distributed Training</h3>
<p>Computational infrastructure for large genomic transformers typically requires distributed approaches. Single-GPU training suffices only for small models on short sequences. Data parallelism replicates the model across GPUs, splitting batches across devices and aggregating gradients. This scales well up to batch sizes limited by convergence requirements. Model parallelism splits the model itself across devices, necessary when models exceed single-GPU memory. Pipeline parallelism divides layers across devices and pipelines forward and backward passes.</p>
<p>Batch size selection involves competing considerations. Larger batches provide more stable gradient estimates and better GPU utilization but require more memory and may reduce generalization. Genomic transformers often use gradient accumulation to simulate large batches: small batches process sequentially, gradients accumulate, then a single parameter update occurs. This provides large-batch benefits without the memory cost, though it increases training time proportionally.</p>
</section>
</section>
<section id="limitations-and-emerging-alternatives" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="limitations-and-emerging-alternatives"><span class="header-section-number">7.7</span> Limitations and Emerging Alternatives</h2>
<p>Despite their success, transformers have fundamental limitations that motivate continued architectural innovation. Understanding these limitations contextualizes recent developments and guides architecture selection for specific genomic applications.</p>
<section id="the-quadratic-barrier" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="the-quadratic-barrier"><span class="header-section-number">7.7.1</span> The Quadratic Barrier</h3>
<p>The quadratic complexity of self-attention remains transformers’ most severe limitation for genomics. Computing all pairwise attention scores requires <span class="math inline">\(O(L^2)\)</span> operations and memory. For genomic contexts exceeding 100 kilobases (roughly 100,000 single-nucleotide tokens), this becomes prohibitive. Even with sparse approximations, transformers struggle at megabase scales where some regulatory interactions occur.</p>
<p>Recent models have pushed context lengths impressively. Enformer handles 200 kilobases; emerging models approach 1 megabase. But these achievements rely on hybrid architectures with significant downsampling or hierarchical windowing that may miss certain long-range patterns. Pure transformers without such modifications remain limited to shorter contexts.</p>
</section>
<section id="state-space-models" class="level3" data-number="7.7.2">
<h3 data-number="7.7.2" class="anchored" data-anchor-id="state-space-models"><span class="header-section-number">7.7.2</span> State Space Models</h3>
<p>State space models (SSMs) address the quadratic barrier directly by achieving linear complexity while maintaining long-range modeling capability. Rather than computing all pairwise interactions, SSMs represent sequences as continuous-time dynamical systems, maintaining memory through recurrent state updates.</p>
<p>Architectures like S4, Hyena, and Mamba have demonstrated competitive or superior performance to transformers on various sequence modeling tasks while scaling to much longer contexts. For genomics, this enables whole-chromosome or potentially whole-genome modeling that remains intractable for standard transformers. <a href="p3-ch11-dna-lm.html" class="quarto-xref"><span>Chapter 11</span></a> examines these architectures in detail, including HyenaDNA and the Evo family of DNA foundation models built on SSM principles.</p>
</section>
<section id="choosing-architectures" class="level3" data-number="7.7.3">
<h3 data-number="7.7.3" class="anchored" data-anchor-id="choosing-architectures"><span class="header-section-number">7.7.3</span> Choosing Architectures</h3>
<p>When should one prefer transformers over alternatives? Transformers excel when global context matters but sequences are not extremely long (under 10-50 kilobases depending on computational resources). Attention maps provide interpretability, showing which positions the model considers relevant for predictions. Transformers benefit from extensive tooling and pretrained models from NLP that transfer readily to genomics.</p>
<p>CNNs remain preferable when computational efficiency is paramount and local patterns dominate. For splice site prediction or promoter classification where relevant context spans at most a few hundred base pairs, a well-designed CNN may outperform transformers while using far fewer parameters. The inductive bias toward local patterns also regularizes against overfitting when training data is limited.</p>
<p>Hybrid approaches often achieve the best practical results. As <a href="p3-ch13-regulatory.html" class="quarto-xref"><span>Chapter 13</span></a> demonstrates, models combining CNNs for local feature extraction with transformers for long-range integration outperform pure architectures on regulatory prediction tasks. The optimal combination depends on the specific biological question and the scale of relevant interactions.</p>
<p>The transition toward sub-quadratic architectures continues. Early results suggest SSMs match or exceed transformers on some genomic benchmarks while scaling to longer contexts. However, transformers benefit from years of engineering optimization and extensive pretrained model ecosystems. The field is actively exploring whether SSMs’ theoretical advantages translate to practical improvements across diverse genomic tasks.</p>
</section>
</section>
<section id="from-architecture-to-learning" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="from-architecture-to-learning"><span class="header-section-number">7.8</span> From Architecture to Learning</h2>
<p>The transformer architecture provides the computational substrate for modern genomic foundation models, but architecture alone does not determine what models learn. The attention mechanism enables long-range interaction modeling; position encodings break permutation invariance to preserve sequence order; stacked blocks build hierarchical representations through iterative refinement. Yet the patterns these components learn depend critically on training objectives and data.</p>
<p>Self-supervised pretraining, examined in <a href="p2-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>, provides the learning signal that transforms architectural capacity into biological knowledge. Masked language modeling teaches models to predict held-out tokens from context, implicitly learning sequence patterns and constraints. Next-token prediction in autoregressive models captures sequential dependencies. These objectives, applied to massive genomic datasets, enable transformers to learn representations that transfer across diverse downstream tasks.</p>
<p>The foundation models in <a href="p3-ch11-dna-lm.html" class="quarto-xref"><span>Chapter 11</span></a> and <a href="p3-ch12-protein-lm.html" class="quarto-xref"><span>Chapter 12</span></a> represent the culmination of transformer architecture and self-supervised learning. DNABERT applies masked language modeling to k-mer tokenized DNA, learning regulatory grammar from genomic sequence alone. ESM-2 applies similar principles to protein sequences, discovering that structure and function emerge from evolutionary patterns without explicit structural supervision. Enformer (<a href="p3-ch13-regulatory.html" class="quarto-xref"><span>Chapter 13</span></a>) combines transformer attention with convolutional processing to span the 200-kilobase contexts required for expression prediction. Each model demonstrates that transformers, properly trained on biological sequence, capture biologically meaningful patterns.</p>
<p>The quadratic attention bottleneck that limits standard transformers has spurred development of alternative architectures. State space models achieve linear complexity while maintaining long-range capability, enabling the ultra-long contexts required for genome-scale modeling. Whether these alternatives ultimately displace transformers or complement them remains an open question. What is clear is that the attention mechanism introduced a new paradigm for genomic modeling: one where global context is directly accessible, where positions communicate without mediation through local layers, and where the computational challenge shifts from “how to see far enough” to “how to attend efficiently.” This paradigm, regardless of its specific architectural instantiation, continues to shape genomic AI.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-vaswani_attention_2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention <span>Is</span> <span>All</span> <span>You</span> <span>Need</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p2-ch06-cnn.html" class="pagination-link" aria-label="Convolutional Models for Genomic Sequence">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Models for Genomic Sequence</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p2-ch08-pretraining.html" class="pagination-link" aria-label="Pretraining Objectives and Strategies">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives and Strategies</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>