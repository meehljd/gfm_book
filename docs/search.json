[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Genomic Foundation Models",
    "section": "",
    "text": "Introduction\nWe can now sequence a human genome for a few hundred dollars and store millions of genomes in a single biobank. What we cannot yet do, reliably, is tell you what most of those variants mean. The gap between sequencing capacity and interpretive capacity defines the central problem of modern genomics. It is exactly the gap that genomic foundation models aim to close.\nMeanwhile, deep learning has transformed how we represent language, proteins, and now DNA itself. Large models trained on broad sequence data can now be adapted to tasks ranging from variant interpretation to clinical risk prediction, all without retraining from scratch for each new problem.\nThis book is about that intersection: genomic foundation models (GFMs) - large, reusable models trained on genomic and related data that can be adapted to many downstream tasks. Rather than offering a general introduction to genomics or machine learning, the goal is narrower and more opinionated:\nThe chapters that follow connect classic genomics pipelines, early deep regulatory models, sequence language models, and multi-omic GFMs into a single narrative arc.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-genomic-foundation-models",
    "href": "index.html#why-genomic-foundation-models",
    "title": "Genomic Foundation Models",
    "section": "Why Genomic Foundation Models?",
    "text": "Why Genomic Foundation Models?\nTraditional genomic modeling has usually been task-specific:\n\nA variant caller tuned to distinguish sequencing errors from true variants.\n\nA supervised CNN trained to predict a fixed set of chromatin marks.\n\nA risk score fit for one trait, in one ancestry group, in one health system.\n\nThese models can work very well in the setting they were designed for, but they often do not transfer gracefully to new assays, tissues, ancestries, or institutions.\nThe foundation model paradigm takes a different view:\n\nScale\nTrain large models on massive, heterogeneous datasets, across assays, tissues, species, and cohorts, so they learn reusable structure.\nSelf-supervision\nUse objectives such as masked-token prediction, next-token modeling, or contrastive learning that do not require manual labels, allowing us to exploit unlabeled genomes, perturbation screens, and population variation.\nReusability\nTreat the model as a backbone: for new tasks, we probe, adapt, or fine-tune the same representation instead of training a new model from scratch.\n\nIn genomics, this paradigm is still evolving and far from settled. Some tasks benefit dramatically from pretraining; others barely move beyond strong classical baselines. This book leans into that tension and asking when foundation models actually help, and when simpler approaches suffice (Bommasani et al. 2022; Guo et al. 2025).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#recurring-themes",
    "href": "index.html#recurring-themes",
    "title": "Genomic Foundation Models",
    "section": "Recurring Themes",
    "text": "Recurring Themes\nSeveral threads run through the book; individual chapters can be read as different views of the same underlying questions.\n\nData and Architecture Co-evolve\nWe will see how:\n\nEarly deleteriousness scores built on hand-engineered features and shallow models.\n\nCNNs enabled direct learning of regulatory “motifs” and local grammar from raw sequence.\n\nTransformers and other long-context models opened the door to capturing broader regulatory neighborhoods and chromatin structure.\n\nGFMs push toward representations that span multiple assays, tissues, and even organisms.\n\nAt each stage, the interesting question is not “Is this model fancier?” but “How does the available data constrain what the model can sensibly learn?”\n\n\nContext Length and Genomic Geometry\nMany genomic phenomena are intrinsically non-local: enhancers regulating distant genes, looping interactions, polygenic effects spread across the genome. The book returns repeatedly to “how far” a model can see, how it represents long-range dependencies, and what is gained (and lost) as context windows and architectures scale.\n\n\nPrediction Versus Design\nMost current models are used as predictors: given sequence and context, what happens? But the same models can be embedded in design and closed-loop workflows, from variant prioritization to sequence or library design. We will explore how foundation models change the boundary between analysis and experimental planning, and what new failure modes emerge in the process.\n\n\nFrom Benchmarks to Decisions\nBenchmark scores are seductive and easy to compare. Real biological and clinical decisions are messy, multi-objective, and often constrained by data drift, bias, and poorly specified endpoints. A recurring theme is the gap between “state-of-the-art AUC” and actual impact—and how careful evaluation, confounder analysis, and calibration can narrow that gap.\n\n\nInterpretability and Mechanism\nFinally, we return often to interpretability, not as optional decoration, but as a design constraint. We will ask when saliency maps, motif extraction, or more mechanistic analyses genuinely deepen understanding, and when they simply provide a veneer of comfort over confounded or brittle models.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-the-book-is-organized",
    "href": "index.html#how-the-book-is-organized",
    "title": "Genomic Foundation Models",
    "section": "How the Book Is Organized",
    "text": "How the Book Is Organized\nThe book is organized into six parts plus three short appendices. Each part can be read on its own, but they are designed to build on one another.\n\nPart I — Data & Pre-DL Methods\nPart I lays the genomic and statistical foundation that later models rest on.\n\nChapter 1  Sequencing: From Reads to Variants introduces next-generation sequencing, alignment, and variant calling, highlighting sources of error and the evolution from hand-crafted pipelines to learned variant callers.\nChapter 2  The Genomic Data Landscape surveys the core data resources that underlie most modern work: reference genomes, population variation catalogs, clinical variant databases, and functional genomics consortia. It also discusses how they are used as training targets and evaluation benchmarks.\nChapter 3  GWAS & Polygenic Scores reviews genome-wide association studies, linkage disequilibrium, fine-mapping, and polygenic scores, emphasizing what these “variant-to-trait” associations do and do not tell us.\nChapter 4  Deleteriousness Scores covers conservation-based and machine learning-based variant effect predictors such as CADD, including their feature sets, label construction, and issues like circularity and dataset bias.\n\nTogether, Part I answers: What data and pre-deep-learning tools form the backdrop that any genomic foundation model must respect, integrate with, or improve upon?\n\n\n\nPart II — CNN Seq-to-Function Models\nPart II turns to the first wave of deep sequence-to-function models, largely built on convolutional neural networks.\n\nChapter 5  Regulatory Prediction presents CNN-based models that predict chromatin accessibility, histone marks, and related regulatory annotations directly from DNA sequence, and explores what they learn about motifs and regulatory grammar.\nChapter 6  Transcriptional Effects extends from chromatin to gene expression, showing how models combine sequence, regulatory features, and context to predict expression levels and perturbation effects.\nChapter 7  Splicing Prediction focuses on deep models of pre-mRNA splicing and splice-site choice, and how these models can be used to interpret variant effects on splicing in both research and clinical contexts.\n\n\n\n\nPart III — Transformer Models\nPart III introduces transformer-based and related architectures for representing biological sequence.\n\nChapter 8  Sequence Representation & Tokens examines how we turn genomic and protein sequences into model-compatible tokens, including k-mers, byte-pair encodings, and other schemes, and how these choices shape downstream models.\nChapter 9  Protein Language Models describes large protein language models trained on sequence databases, their emergent structure and function representations, and applications to variant effect prediction and protein design.\nChapter 10  DNA Foundation Models surveys DNA language models and other genomic foundation backbones, including their training corpora, objectives, evaluation suites, and limitations.\nChapter 11  Long-range Hybrid Model covers hybrid CNN/transformer and related architectures designed to handle long genomic contexts, such as models that predict regulatory readouts over tens to hundreds of kilobases.\n\n\n\n\nPart IV — GFMs & Multi-omics\nPart IV is the conceptual core of the book, focusing explicitly on genomic foundation models and their multi-omic extensions.\n\nChapter 12  Genomic FMs: Principles & Practice provides a working definition and taxonomy of genomic FMs, design dimensions (architecture, context length, conditioning), and practical guidance for using pretrained backbones in downstream tasks…\nChapter 13  Variant Effect Prediction recasts variant effect prediction in the foundation-model era, spanning protein and DNA-based approaches, and discusses calibration, uncertainty, and integration into existing pipelines.\nChapter 14  Multi-omics & Systems Context broadens the view from isolated sequences to multi-omic and systems-level representations, including models that integrate genomic, transcriptomic, proteomic, and phenotype data.\n\n\n\n\nPart V — Reliability & Interpretation\nPart V pulls out cross-cutting issues that apply to essentially every model in the book.\n\nChapter 15  Model Evaluation & Benchmarks develops a unified framework for evaluating models across molecular, variant-level, trait-level, and clinical tasks, and discusses data splitting, metric choice, and the link between benchmarks and real-world decisions.\nChapter 16  Confounders in Model Training details sources of confounding and data leakage, from batch effects and ancestry structure to label bias and covariate shift, and offers practical strategies for detection and mitigation.\nChapter 17  Interpretability & Mechanisms explores interpretability tools from classical motif discovery and attribution methods to emerging mechanistic approaches, and asks when these tools genuinely reveal biological mechanisms.\n\n\n\n\nPart VI — Applications\nPart VI moves from methods to end-to-end workflows in research and clinical practice.\n\nChapter 18  Clinical Risk Prediction discusses risk prediction tasks that combine genomic features (including outputs from GFMs) with clinical and environmental data, focusing on discrimination, calibration, fairness, and deployment in health systems.\nChapter 19  Pathogenic Variant Discovery examines how models fit into rare disease and cancer workflows, including variant prioritization pipelines, integration with family and tumor-normal data, and lab-in-the-loop validation.\nChapter 20  Drug Discovery & Biotech looks at how GFMs intersect with target discovery, functional genomics screens, biomarker development, and biotech/industry workflows, including build-vs-buy and organizational considerations.\n\n\n\n\nAppendices\nTwo short appendices provide background and pointers:\n\nAppendix A — Deep Learning Primer for Genomics is a compact introduction to neural networks, CNNs, transformers, training, and evaluation, aimed at genomics-first readers who want enough ML background to engage with the main chapters. :contentReferenceoaicite:2\nAppendix B — Additional Resources is a curated set textbooks, courses, software, and papers for deeper dives into genomics, statistical genetics, and deep learning.\nAppendix C — Glossary is a glossary of key terms.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#a-moving-target",
    "href": "index.html#a-moving-target",
    "title": "Genomic Foundation Models",
    "section": "A Moving Target",
    "text": "A Moving Target\nGenomic foundation models are a moving target: architectures, datasets, and evaluation suites are evolving quickly. This book is not intended as a frozen survey of “the state of the art,” but as a framework for reasoning about new models as they appear.\nIf it succeeds, you should finish able to:\n\nPlace a new model in the landscape of data, architecture, objective, and application.\n\nDesign analyses and experiments that use GFMs as components—features, priors, or simulators—without overclaiming what they can do.\n\nRecognize common pitfalls in training, evaluation, and deployment, especially in clinical and translational settings.\n\nDecide where foundation models are genuinely useful, and where simpler methods or classical baselines are sufficient.\n\nThe next chapter now turns to the foundations: how we get from raw reads to variants, and from variants to the datasets and benchmarks on which all of these models depend.\n\n\n\n\nBommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2022. “On the Opportunities and Risks of Foundation Models.” arXiv. https://doi.org/10.48550/arXiv.2108.07258.\n\n\nGuo, Fei, Renchu Guan, Yaohang Li, Qi Liu, Xiaowo Wang, Can Yang, and Jianxin Wang. 2025. “Foundation Models in Bioinformatics.” National Science Review 12 (4): nwaf028. https://doi.org/10.1093/nsr/nwaf028.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why I Wrote This Book\nWorking on genomic foundation models means context-switching constantly: debugging data artifacts one week, reproducing a transformer-based variant effect predictor the next, and arguing about clinical patient cohorts the week after. The knowledge required is scattered across textbooks, methods papers, and tribal folklore - genomics on one shelf, deep learning on another, clinical deployment in someone else’s head entirely.\nThis book is my attempt to put those pieces in one place: to connect the mature, statistically grounded tradition of human genetics with the rapidly changing ecosystem of deep learning and foundation models, and to make that transition legible for people who live in one corner of the triangle and are trying to get oriented to the others.\nI wrote it first for myself and my collaborators: as a way to organize wiki pages, markdown files, and half-finished slide decks into something coherent. Over time it became clear that turning those notes into a book might be useful to others navigating the same landscape.\nWhat I wanted, but could not find, was a conceptual throughline:\nThis book is my best attempt at answering those questions in a way that is historically grounded, technically honest, and practically oriented.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#why-i-wrote-this-book",
    "href": "preface.html#why-i-wrote-this-book",
    "title": "Preface",
    "section": "",
    "text": "How do we get from reads to variants in a way that a deep model can trust?\nHow should we think about polygenic scores, fine-mapping, and functional assays in the era of foundation models?\nWhen we say a model “understands” regulatory grammar or protein function, what does that actually mean?\nAnd what does it take to move from a promising preprint to a tool that can support decisions about real patients?",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#how-this-book-came-together",
    "href": "preface.html#how-this-book-came-together",
    "title": "Preface",
    "section": "How This Book Came Together",
    "text": "How This Book Came Together\nThe structure of the book reflects the way these ideas evolved in my own work.\nEarly sections grew out of teaching and mentoring conversations: explaining next-generation sequencing, variant calling, and pre-deep-learning interpretation methods to new team members who were strong in statistics or ML but new to genomics (and vice versa).\nThe middle sections emerged from a series of “journal club + experiments” cycles, where we:\n\nread papers on sequence-to-function CNNs, protein language models, and genomic transformers,\ntried to reproduce key results or adapt them to key datasets,\nand documented the pain points—data formats, training instabilities, evaluation pitfalls, which never quite fit into a methods section.\n\nThe later parts were shaped by collaborations around clinical prediction, variant interpretation pipelines, and larger multi-omic models. Many of the examples and caveats come directly from these projects: places where a model that looked excellent on paper behaved in surprising ways when exposed to real-world data, or where simple baselines outperformed much fancier architectures once confounding and distribution shift were handled correctly.\nBecause of that origin, the book has a particular bias: it is written from the perspective of someone who spends much of their time trying to get models to work in messy, high-stakes settings. You will see this in the emphasis on data quality, evaluation, and clinical translation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#how-to-read-this-book",
    "href": "preface.html#how-to-read-this-book",
    "title": "Preface",
    "section": "How to Read This Book",
    "text": "How to Read This Book\nThis is not a genomics textbook, a complete review of every DNA or protein model, or a deep-learning-from-scratch course. Instead, it is meant to be:\n\na roadmap to the main kinds of data, models, and objectives that matter for genomic foundation models today\na bridge between classical statistical genetics and modern representation learning\na practical guide to the kinds of failure modes and design choices that matter in real applications.\n\nYou do not need to read the book cover-to-cover in order.\n\nIf your background is in genomics or statistical genetics, you may want to skim the early deep-learning motivations and focus more on the sections that introduce convolutional models, transformers, and self-supervision, then move on to evaluation and applications.\nIf you come from machine learning, it may be more helpful to start with the genomic data and pre-deep-learning methods, then dive into the sequence-to-function and transformer-based chapters with an eye toward how the data and objectives differ from text or images.\nIf you are a clinician or translational researcher, you might care most about the reliability, confounding, and clinical deployment discussions, dipping back into the modeling parts as needed to interpret results or communicate with technical collaborators.\n\nThe book is organized into six parts:\n\nPart I introduces genomic data and pre-deep-learning interpretation methods, from sequencing and variant calling to early pathogenicity scores and polygenic models.\nPart II focuses on supervised sequence-to-function models, with an emphasis on convolutional architectures, regulatory prediction, and splicing.\nPart III turns to transformer-based models and self-supervision, covering protein and DNA language models and hybrid architectures that combine CNNs and transformers.\nPart IV discusses what makes a model a foundation model in genomics, including multi-omic architectures, variant effect modeling, and emergent capabilities.\nPart V examines reliability, evaluation, confounding, and interpretability—how we know whether a model is learning what we think it is, and how to detect when it is not.\nPart VI looks at applications: clinical and risk prediction, variant interpretation workflows, and early steps toward drug discovery and biotech use cases.\n\nWithin each part, the goal is not to catalogue every paper, but to highlight representative examples and the design principles they illustrate. References are there to give you starting points, not to serve as a comprehensive literature review.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#what-this-book-assumes-and-what-it-does-not",
    "href": "preface.html#what-this-book-assumes-and-what-it-does-not",
    "title": "Preface",
    "section": "What This Book Assumes (and What It Does Not)",
    "text": "What This Book Assumes (and What It Does Not)\nThe book assumes:\n\nbasic familiarity with probability and statistics (regression, hypothesis testing, effect sizes),\ncore genomics concepts (genes, variants, linkage disequilibrium, GWAS at a high level),\nand some exposure to machine learning ideas (training versus test data, overfitting, loss functions).\n\nIt does not assume that you have implemented deep learning models yourself, or that you are fluent in every area. When a chapter leans heavily on a particular background (for example, causal inference or modern self-supervised learning), it will either provide a brief refresher or point you to an appendix or external resource.\nIf you are missing some of this background, that is fine. The intent is for you to be able to read actively: to pause, look up side topics, and then return to the main arc without feeling lost.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#a-note-on-scope-and-opinions",
    "href": "preface.html#a-note-on-scope-and-opinions",
    "title": "Preface",
    "section": "A Note on Scope and Opinions",
    "text": "A Note on Scope and Opinions\nGenomic foundation models are evolving quickly. Any snapshot is, by definition, incomplete and slightly out of date.\nRather than chasing every new architecture or benchmark, the book focuses on durable ideas:\n\nhow different data types fit together,\nwhat kinds of objectives encourage useful representations,\nhow evaluation can fail in genomics-specific ways,\nand where deep models complement (rather than replace) classical approaches.\n\nInevitably, there are judgment calls about which papers, methods, and perspectives to emphasize. Those choices reflect my own experiences and biases. They are not an official position of any institution I work with, and they will certainly differ from other reasonable views in the field.\nYou should treat the book as one opinionated map of the landscape, not the landscape itself.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#acknowledgements",
    "href": "preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis book exists because of many generous people who shared their time, ideas, and encouragement.\nFirst, I owe a deep debt of gratitude to my colleagues in the Mayo Clinic GenAI and broader data science community. The day-to-day conversations, whiteboard sessions, and “what went wrong here?” post-mortems with this group shaped much of the perspective and many of the examples in the chapters.\nI am especially grateful to the principal investigators and clinicians whose questions kept the focus on real patients and real decisions:\nDr. Shant Ayanian, Dr. Elena Myasoedova, and Dr. Alexander Ryu.\nTo leadership at Mayo Clinic who supported the time, computing resources, and institutional patience needed for both the models and this book:\nDr. Matthew Callstrom, Dr. Panos Korfiatis, and Matt Redlon.\nTo my data science and machine learning engineering colleagues, whose work and feedback directly shaped many of the workflows and case studies:\nBridget Toomey, Carl Molnar, Zach Jensen, and Marc Blasi.\nI am also grateful for the architectural creativity, hardware insight, and willingness to experiment from our collaborators at Cerebras:\nNatalia Vassilieva, Jason Wolfe, Omid Shams Solari, Vinay Pondenkandath, Bhargav Kanakiya, and Faisal Al-khateeb.\nAnd to our collaborators at GoodFire, whose partnership helped push these ideas toward interpretable and deployable systems:\nDaniel Balsam, Nicholas Wang, Michael Pearce, and Mark Bissell.\nI would also like to thank my former colleagues at LGC for foundational work and conversations around protein language models and large-scale representation learning:\nPrasad Siddavatam and Robin Butler.\nBeyond these named groups, I owe a broader debt to the geneticists, molecular biologists, statisticians, clinicians, and engineers whose work this book draws on. The field moves forward because people share code, publish honest benchmarks, and insist that models be connected back to biologically meaningful questions. Thank you for setting that standard.\nFinally, I am grateful to my wife, Alyssa, and our two kids for their patience with the evenings and weekends this book consumed. You gave me the space to finish it and the reasons to step away from it.\nIf this book helps you connect a new model to a real biological question, design a more robust evaluation, or communicate more clearly across disciplinary boundaries then it will have done its job.\n— Josh Meehl",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html",
    "href": "p1-ch01-ngs.html",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "",
    "text": "1.1 The Challenge of NGS Data\nNext-generation sequencing (NGS) has transformed genomics by making it routine to generate tens to hundreds of gigabases of sequence from a single individual in a few days. Modern instruments produce short reads—typically 100–300 bp paired-end Illumina reads—at very high throughput, but with non-trivial error profiles including substitutions, context-specific errors, and base quality uncertainties. These reads are then aligned to imperfect reference genomes that omit structural variation and some segmental duplications (Goodwin, McPherson, and McCombie 2016).\nTurning these raw reads into a reliable list of variants is therefore not just a matter of comparing strings. Variant calling pipelines must disentangle sequencing errors (instrument noise, PCR artifacts), alignment artifacts (mis-mapping in repeats, paralogous regions, pseudogenes), and genuine biological variation (germline variants, somatic mutations, mosaicism). Historically, this was addressed by complex, modular pipelines combining probabilistic models and hand-crafted heuristics (Nielsen et al. 2011). Deep learning now plays an important role in simplifying and improving parts of this stack, but it is helpful to understand the classical pipeline first.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#targeting-strategies-panels-exomes-and-genomes",
    "href": "p1-ch01-ngs.html#targeting-strategies-panels-exomes-and-genomes",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.2 Targeting Strategies: Panels, Exomes, and Genomes",
    "text": "1.2 Targeting Strategies: Panels, Exomes, and Genomes\nNGS is not a single technology; it is deployed in different targeting strategies, each with distinct trade-offs.\n\n1.2.1 Targeted and Panel Sequencing\nTargeted gene panels capture tens to hundreds of genes selected for a clinical indication such as cardiomyopathy or hereditary cancer syndromes. These panels offer high depth in a limited region (often 200–500×), relatively low cost per sample, and simple interpretation workflows tied to well-curated gene lists. However, panels miss novel genes outside their design, most structural variants and non-coding regulatory changes, and opportunities for reanalysis as gene–disease knowledge evolves.\n\n\n1.2.2 Whole-Exome Sequencing\nWhole-exome sequencing (WES) enriches coding exons and some flanking splice regions genome-wide. Typical coverage ranges from 80–150× for exonic targets, though capture efficiency varies across GC content and repetitive exons. Non-coding regions are largely unobserved.\nWES has been especially successful for Mendelian disease gene discovery and diagnostic workflows. At the same time, it misses non-coding and structural causes, has non-uniform coverage leading to heterogeneous sensitivity across genes, and requires careful handling of capture biases and batch effects.\n\n\n1.2.3 Whole-Genome Sequencing\nWhole-genome sequencing (WGS) samples nearly all bases in the genome. Typical coverage is 30–60× across the genome, with more uniform depth than WES. Because there is no capture step, WGS produces fewer batch-specific artifacts and enables detection of non-coding variants, structural variants, and copy-number changes along with SNVs and indels.\nWGS is increasingly favored for new large cohorts and rare disease diagnostics despite higher cost, because the data are reusable for many downstream analyses (GWAS, PGS, rare variant burden tests), it simplifies pipelines by eliminating the need to track changing capture designs, and it supports more complete variant catalogs for the models discussed later in this book.\nThroughout this text, we assume a WES/WGS-style pipeline where we start from aligned reads and aim to call high-confidence SNVs and small indels.\n\n\n1.2.4 Long-Read Sequencing Technologies\nWhile short-read Illumina sequencing dominates population-scale studies, long-read technologies are increasingly important for resolving complex genomic regions and structural variation.\nPacific Biosciences (PacBio) HiFi produces reads of 10–25 kb with accuracy exceeding 99.9% through circular consensus sequencing (Wenger et al. 2019). These reads can span repetitive elements, segmental duplications, and structural variants that confound short-read alignment. Oxford Nanopore Technologies (ONT) generates ultra-long reads—routinely 10–100 kb, with some exceeding 1 Mb—at somewhat lower per-base accuracy (roughly 95–99% for recent chemistries). ONT’s portability and real-time sequencing enable novel applications ranging from field diagnostics to direct RNA sequencing (Dabernig-Heinz et al. 2024).\nLong reads transform variant calling in several ways. Structural variants—deletions, insertions, inversions, and complex rearrangements that are invisible or ambiguous in short-read data—become directly observable. Tandem repeats, segmental duplications, and transposable element insertions can be traversed rather than collapsed. A single long read can span many heterozygous sites, enabling direct, read-backed phasing over tens of kilobases. The T2T-CHM13 reference genome, completed with long reads, added approximately 200 Mb of previously unresolved sequence, including centromeres and acrocentric chromosome arms (Nurk et al. 2022).\nSpecialized variant callers have been developed for long-read data. DeepVariant includes models trained on PacBio and ONT data, while tools like Clair3 and PEPPER-Margin-DeepVariant are optimized for nanopore error profiles (Poplin et al. 2018; Zheng et al. 2022; Shafin et al. 2021). For structural variants, callers such as Sniffles, pbsv, and cuteSV exploit the unique properties of long reads (Smolka et al. 2024; “PacificBiosciences/Pbsv” 2025; Jiang et al. 2020).\nThis chapter focuses on short-read pipelines, which remain the workhorse for large cohorts and cost-sensitive applications. However, hybrid approaches—combining short-read depth with long-read phasing and structural variant detection—are increasingly common, and the models in later chapters must accommodate variants discovered by either technology.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#classical-variant-calling-pipelines",
    "href": "p1-ch01-ngs.html#classical-variant-calling-pipelines",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.3 Classical Variant Calling Pipelines",
    "text": "1.3 Classical Variant Calling Pipelines\nWhile every institution implements its own details, a classical short-read pipeline has several common stages.\nThe process begins with base calling and demultiplexing, where instrument software converts fluorescent images to base calls and quality scores, and reads are demultiplexed by barcode into sample-specific FASTQ files.\nNext comes read alignment, in which short reads are aligned to a reference genome (such as GRCh38 or T2T-CHM13) using seed-and-extend mappers such as BWA-MEM or minimap2 (Li 2013, 2018). Aligners must cope with mismatches, small indels, and repetitive sequence.\nPost-alignment processing follows, including marking or removing PCR duplicates, base quality score recalibration (BQSR) to model systematic quality score errors, and local realignment around indels in older pipelines.\nPer-sample variant calling then takes place, where tools like the Genome Analysis Toolkit (GATK) HaplotypeCaller fit local haplotypes using hidden Markov models, de Bruijn graphs, or other probabilistic frameworks (McKenna et al. 2010). These tools produce candidate variants with genotype likelihoods for each sample.\nFinally, for cohort variant calling, joint genotyping and cohort refinement recombines per-sample likelihoods to enforce a consistent set of variants across individuals. Raw calls are then filtered to distinguish true variants from artifacts. Simple hard filters apply fixed cutoffs to individual metrics, but GATK’s Variant Quality Score Recalibration (VQSR) takes a more sophisticated approach: it fits a Gaussian mixture model in the multi-dimensional space of variant annotations (depth, strand bias, mapping quality, read position bias, etc.), using variants at known polymorphic sites as a training set. Each candidate receives a recalibrated score reflecting how well it matches the learned “true variant” distribution, allowing joint consideration of multiple quality axes rather than independent hard thresholds.\nThese steps are encoded in pipelines like GATK Best Practices and similar frameworks. The key point is that each step uses hand-designed summary features and mechanistic models chosen by experts, not learned end-to-end (Van der Auwera et al. 2018).\n\n1.3.1 Probabilistic Framework\nAt the core of GATK’s HaplotypeCaller is a Bayesian genotype likelihood model. For a candidate genotype \\(G\\) at a given site, the posterior probability given the read data \\(D\\) is:\n\\[P(G \\mid D) \\propto P(G) \\prod_{r \\in \\text{reads}} P(r \\mid G)\\]\nwhere \\(P(G)\\) is a prior over genotypes (often assuming Hardy-Weinberg equilibrium) and \\(P(r \\mid G)\\) is the likelihood of observing read \\(r\\) given genotype \\(G\\). Computing \\(P(r \\mid G)\\) is non-trivial: GATK uses a pair hidden Markov model (pair-HMM) to marginalize over possible alignments between the read and candidate haplotypes, incorporating base quality scores to weight the contribution of each base.\nThis formulation assumes conditional independence of reads given the genotype—an assumption known to be violated in practice due to correlated errors from PCR duplicates, systematic instrument biases, and local sequence context (DePristo et al. 2011). DeepVariant’s CNN (see Section 1.8), by contrast, sees all reads in a pileup simultaneously and can learn to model these dependencies implicitly.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#haplotype-phasing",
    "href": "p1-ch01-ngs.html#haplotype-phasing",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.4 Haplotype Phasing",
    "text": "1.4 Haplotype Phasing\nDiploid organisms carry two copies of each autosomal chromosome—one inherited from each parent. Standard variant calling produces unphased genotypes: we know that an individual is heterozygous at two nearby sites (say, A/G and C/T), but not which alleles reside on the same physical chromosome. Phasing resolves this ambiguity by assigning each allele to a specific haplotype.\n\n1.4.1 Why Phasing Matters\nPhased haplotypes are essential for multiple applications. In recessive disease, two deleterious variants in the same gene cause disease only if they are on different haplotypes (in trans); unphased calls cannot distinguish cis from trans configurations, making compound heterozygosity detection impossible without phase information. Some regulatory and coding effects depend on the combination of alleles on a single chromosome, enabling haplotype-aware variant effect prediction. Reference panels used for genotype imputation (such as TOPMed and 1000 Genomes) are stored as phased haplotypes, and accurate phasing improves imputation quality. Finally, haplotype structure carries information about population history, recombination, and natural selection that drives ancestry and selection analyses.\n\n\n1.4.2 Phasing Methods\nPhasing can be achieved through several approaches. Read-backed phasing exploits physical linkage: when heterozygous variants are close enough to be spanned by the same sequencing read or read pair, the linkage directly reveals phase. Short-read data can phase variants within a few hundred base pairs, while long reads extend this to tens of kilobases or more.\nStatistical or population-based phasing tools such as SHAPEIT, Eagle, and Beagle infer phase by leveraging linkage disequilibrium patterns observed in reference panels (O’Connell et al. 2014; Loh et al. 2016; Browning et al. 2021). These methods are highly accurate for common variants but struggle with rare variants that lack informative LD.\nPedigree-based phasing becomes possible when parent–offspring trios or larger pedigrees are available; Mendelian inheritance rules can resolve phase with high confidence.\nLong-read and linked-read technologies provide direct observation of haplotype structure. PacBio HiFi and Oxford Nanopore reads span tens of kilobases, while linked-read methods (such as the now-discontinued 10x Genomics platform) tag short reads originating from the same long DNA molecule, providing intermediate-range phasing.\nModern pipelines often combine these approaches: statistical phasing across the genome, refined by read-backed evidence where available, and augmented by trio data when present. The result is a phased VCF with haplotype assignments (for example, 0|1 rather than 0/1), which downstream analyses can exploit.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#sources-of-error-and-uncertainty",
    "href": "p1-ch01-ngs.html#sources-of-error-and-uncertainty",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.5 Sources of Error and Uncertainty",
    "text": "1.5 Sources of Error and Uncertainty\nEven with modern pipelines, variant calls are imperfect. Understanding the important failure modes is essential for interpreting downstream analyses (Li 2014).\nMapping ambiguity arises when reads from segmental duplications, paralogous genes, and low-complexity regions are mis-aligned. Reference bias can favor the reference allele in ambiguous regions, causing systematic undercalling of alternate alleles.\nSystematic sequencing artifacts include context-specific errors in homopolymers and GC-rich regions, as well as batch effects across runs, instruments, or library preparations. These artifacts can create correlated false positives that are difficult to filter.\nLow-coverage regions present another challenge. WES capture dropouts or WGS coverage dips can create false negatives for heterozygous variants, and somatic or mosaic variants at low allele fraction can be mistaken for noise.\nComplex variants are also problematic. Small indels near homopolymers or repetitive elements are difficult to call accurately, and multi-nucleotide variants may be decomposed into multiple SNVs depending on the caller’s representation choices.\nThe deep learning models in later chapters inherit these errors as input noise. Understanding where variant calls are reliable—and where they are not—is essential when training sequence-to-function models, building polygenic scores, or interpreting predicted variant effects.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#difficult-to-call-regions",
    "href": "p1-ch01-ngs.html#difficult-to-call-regions",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.6 Difficult-to-Call Regions",
    "text": "1.6 Difficult-to-Call Regions\nNot all genomic regions are created equal. Some areas of the genome are systematically problematic for short-read variant calling due to their sequence properties.\n\n1.6.1 Segmental Duplications and Paralogs\nRegions with high sequence identity to other parts of the genome cause reads to map ambiguously. Paralogous genes—such as SMN1 and SMN2, or CYP2D6 and its pseudogenes—are particularly challenging. A read originating from one copy may align equally well to another, leading to false variant calls or missed true variants.\n\n\n1.6.2 Low-Complexity and Repetitive Sequence\nHomopolymers, short tandem repeats, and other low-complexity regions have elevated error rates on most sequencing platforms. Indel calling in these regions is especially unreliable, and many pipelines mask or flag them.\n\n\n1.6.3 The HLA Region: A Case Study\nThe human leukocyte antigen (HLA) locus on chromosome 6p21 is among the most challenging regions in the human genome—and among the most clinically important.\nHLA is difficult to call for several reasons. The HLA genes (HLA-A, HLA-B, HLA-C, HLA-DRB1, and others) are the most polymorphic coding genes in the human genome, with thousands of known alleles per gene (Robinson et al. 2020). Standard reference-based alignment struggles because reads may match the reference poorly even when they represent common, well-characterized alleles. The MHC region contains segmental duplications, copy-number variable genes (such as HLA-DRB3/4/5), and pseudogenes that confound read mapping. Different HLA alleles may differ by only a few nucleotides, making accurate allele-level typing difficult with short reads alone. Reads carrying non-reference HLA alleles may fail to align or align with low mapping quality, causing systematic undercalling of alternate alleles.\nDespite these challenges, accurate HLA typing is essential for several clinical applications. In transplantation, HLA matching between donor and recipient is critical for organ and hematopoietic stem cell transplantation outcomes. HLA alleles are the strongest genetic risk factors for many autoimmune conditions, including type 1 diabetes, rheumatoid arthritis, and multiple sclerosis; fine-mapping causal alleles and amino acid positions requires accurate genotyping (Sakaue et al. 2023; Padyukov 2022). Specific HLA alleles—such as HLA-B*57:01 for abacavir and HLA-B*15:02 for carbamazepine—are pharmacogenomic markers for severe adverse drug reactions (Mallal et al. 2008; Chung et al. 2004). HLA diversity also shapes immune responses to pathogens, including HIV, hepatitis, and SARS-CoV-2.\nBecause standard variant callers perform poorly in HLA, specialized tools have been developed. HLA imputation methods, including those available through the Michigan Imputation Server, use dense reference panels to impute HLA alleles from array genotypes, enabling large-scale association studies (Sakaue et al. 2023). Sequence-based typing tools such as T1K perform HLA and KIR (killer immunoglobulin-like receptor) genotyping directly from WES, WGS, or RNA-seq data by aligning reads against allele databases (such as IPD-IMGT/HLA) rather than the linear reference genome (Song et al. 2022). T1K is notable for its speed, accuracy across sequencing platforms, and ability to handle both DNA and RNA data. Graph-based approaches that incorporate known HLA alleles as alternate paths can also improve alignment and variant calling in this region (Garrison et al. 2018; Liao et al. 2023).\nFor the purposes of this book, HLA exemplifies a broader lesson: regions of extreme diversity, structural complexity, or clinical importance may require specialized methods beyond generic variant calling pipelines. Later chapters show how variant effect models handle these challenging regions—often by excluding them entirely or applying specialized processing.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#benchmarking-and-ground-truth",
    "href": "p1-ch01-ngs.html#benchmarking-and-ground-truth",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.7 Benchmarking and Ground Truth",
    "text": "1.7 Benchmarking and Ground Truth\nEvaluating variant callers requires high-confidence truth sets against which predictions can be compared. The Genome in a Bottle (GIAB) Consortium, coordinated by NIST, provides extensively characterized reference samples with validated variant calls across most of the genome (Zook et al. 2019).\n\n1.7.1 GIAB Reference Samples\nThe primary GIAB samples include NA12878 (also known as HG001), a well-studied female of European ancestry from the CEPH/Utah pedigree with the longest history of characterization. The collection also includes HG002 through HG007: an Ashkenazi Jewish trio (HG002–HG004) and a Han Chinese trio (HG005–HG007), providing diversity and enabling trio-based validation.\nFor each sample, GIAB provides high-confidence variant calls—consensus calls derived from multiple sequencing technologies and variant callers, representing the best current estimate of true genotypes. They also define high-confidence regions, genomic intervals where the truth set is believed to be reliable; difficult regions such as segmental duplications and centromeres are excluded. Benchmarking tools like hap.py and RTG Tools enable standardized comparison of callsets against truth, reporting precision, recall, and F1 by variant type (Krusche et al. 2019; “RealTimeGenomics/Rtg-Core” 2025).\n\n\n1.7.2 Benchmarking Metrics\nStandard metrics for variant calling include recall (sensitivity), defined as TP / (TP + FN) or the fraction of true variants detected; precision (positive predictive value), defined as TP / (TP + FP) or the fraction of called variants that are true; and the F1 score, the harmonic mean of precision and recall. These are typically reported separately for SNVs and indels and may be stratified by genomic context, such as performance inside versus outside difficult regions.\n\n\n1.7.3 Limitations of Current Benchmarks\nGIAB truth sets have known limitations. They are derived primarily from short-read data and may miss complex variants, structural variants, and variation in difficult regions. High-confidence regions cover only approximately 85–90% of the genome, so performance in excluded regions is unknown. Sample diversity is limited, and performance may differ in underrepresented populations.\nOngoing efforts—including the T2T Consortium’s complete genome assemblies and the Human Pangenome Reference Consortium’s diverse haplotype collection—are expanding the scope of benchmarking resources (Liao et al. 2023).",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#sec-deepvar",
    "href": "p1-ch01-ngs.html#sec-deepvar",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.8 DeepVariant: CNNs for Variant Calling",
    "text": "1.8 DeepVariant: CNNs for Variant Calling\nDeepVariant replaces much of the hand-engineered logic in classical pipelines with a deep convolutional neural network trained to classify candidate variants directly from read pileups (Poplin et al. 2018).\n\n1.8.1 Image-Like Pileup Representation\nAround each candidate site, DeepVariant constructs a six-channel tensor resembling an image. Each row corresponds to a read overlapping the site, with channels encoding reference match/mismatch status, Phred-scaled base quality, mapping quality, strand orientation, allele support (reference vs. alternate), and additional alignment features. The reference sequence and candidate alleles are overlaid. This representation allows the CNN to distinguish patterns consistent with true variants—balanced allele support across strands, consistent base qualities, clean alignments—from artifacts like strand-biased support or mismatches clustered at read ends.\n\n\n1.8.2 Inception-Style CNN Classifier\nDeepVariant uses an Inception-style CNN originally developed for image classification. Trained on high-confidence truth sets such as GIAB genomes, it learns to recognize true variant patterns and reject artifacts (strand bias, mapping pileups in repeats, inconsistent quality profiles). Once trained, the same architecture generalizes across whole-genome versus whole-exome data, PCR-free versus PCR-amplified libraries, and different instrument models and read lengths.\nCrucially, DeepVariant learns to weigh quality signals jointly and end-to-end, rather than relying on post-hoc recalibration. Where VQSR fits a separate model on hand-selected annotations after calling, DeepVariant integrates the raw evidence directly into its classification—the CNN sees the same strand bias and quality patterns that VQSR would use, but learns their relationship to true variant status during training rather than in a decoupled second step.\n\n\n1.8.3 Cohort Calling with DeepVariant and GLnexus\nFor cohort calling, DeepVariant can be combined with joint genotyping tools such as GLnexus to scale to tens or hundreds of thousands of samples while maintaining high accuracy (Yun et al. 2021). In this setup, DeepVariant produces per-sample gVCFs (genomic VCFs) containing genotype likelihoods at all sites, not just variant sites, and GLnexus merges gVCFs across samples to produce a cohort-wide callset.\nJoint calling matters for several reasons. It improves sensitivity for rare variants: a variant observed in only one or two individuals may have weak per-sample evidence, but by combining likelihoods across carriers, joint calling can recover true rare variants that would be filtered in single-sample analysis. Joint calling ensures consistent representation, so that the same variants are genotyped across all samples, avoiding the problem of comparing different candidate variant sites across samples. Cohort-level quality filters can identify and remove systematic artifacts that affect subsets of samples, reducing batch effects and improving allele frequency estimates for downstream GWAS and PRS accuracy.\nThis combination has become a de facto standard for large WES and WGS cohorts, including recent releases of gnomAD and UK Biobank (Karczewski et al. 2020; Bycroft et al. 2018).\n\n\n1.8.4 Comparison: Classical Pipelines vs. DeepVariant\n\n\n\n\n\n\n\n\nAspect\nGATK HaplotypeCaller\nDeepVariant\n\n\n\n\nCore approach\nPair-HMM + hand-crafted heuristics\nCNN on pileup tensors\n\n\nFeature engineering\nExpert-designed (MQ, DP, FS, etc.)\nLearned end-to-end\n\n\nRead independence\nAssumed (violated in practice)\nImplicitly models dependencies\n\n\nCalibration\nVQSR post-hoc recalibration\nWell-calibrated likelihoods\n\n\nGeneralization\nRequires species/platform tuning\nTransfers across species and platforms\n\n\nStructural variants\nLimited (SNVs/indels only)\nLimited (SNVs/indels only)\n\n\n\nBoth approaches achieve comparable accuracy on high-quality Illumina data, but DeepVariant shows advantages in difficult contexts and generalizes more readily to new sequencing technologies.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#significance-for-genomic-deep-learning",
    "href": "p1-ch01-ngs.html#significance-for-genomic-deep-learning",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.9 Significance for Genomic Deep Learning",
    "text": "1.9 Significance for Genomic Deep Learning\nNGS and variant calling set the stage for everything else in this book.\n\n1.9.1 Defining the Atoms We Model\nThe output of WES and WGS pipelines—a VCF of SNVs, indels, and increasingly structural variants—is the raw material for nearly all downstream analyses. Polygenic scores (Chapter 3) aggregate variant effects across the genome. Rare variant burden tests collapse variants by gene or functional annotation. Variant effect predictors (Chapter 4 and later chapters) learn to score individual variants for deleteriousness or functional impact. The quality of variant calls directly limits the quality of these downstream models: false positives introduce noise, while false negatives create blind spots.\n\n\n1.9.2 Constraining Downstream Models\nIf an assay never observes a class of variants, deep models cannot learn about them. Short-read WGS misses many structural variants and complex rearrangements. WES captures coding regions but ignores most non-coding regulatory variation. Difficult regions such as HLA and segmental duplications may be systematically excluded from training data. Models trained on these data inherit their biases and gaps. Chapter 16 returns to these issues when discussing confounders and dataset artifacts.\n\n\n1.9.3 Motivating End-to-End Learning\nDeepVariant is an early example of replacing a hand-designed pipeline with a learned model that operates on raw-ish data and directly optimizes accuracy. This paradigm—replacing feature engineering with learned representations—recurs throughout the book. DeepSEA and Basenji (Chapter 5) learn regulatory grammars from sequence. SpliceAI (Chapter 7) predicts splicing from local sequence context. DNA language models (Chapter 10) learn general-purpose representations from unlabeled genomes. In each case, the question is whether learned representations outperform hand-crafted features, and under what conditions.\n\n\n1.9.4 Looking Ahead\nThe remaining chapters in Part I describe how variant calls are aggregated into genome-wide association studies (Chapter 3), which identify variant-trait associations; polygenic scores (Chapter 3), which predict complex traits from many variants; and deleteriousness scores (Chapter 4), which prioritize variants by predicted pathogenicity. These serve as baselines, inputs, and evaluation targets for the deep learning models that follow.\n\n\n\n\nBrowning, Brian L., Xiaowen Tian, Ying Zhou, and Sharon R. Browning. 2021. “Fast Two-Stage Phasing of Large-Scale Sequence Data.” American Journal of Human Genetics 108 (10): 1880–90. https://doi.org/10.1016/j.ajhg.2021.08.005.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nChung, Wen-Hung, Shuen-Iu Hung, Hong-Shang Hong, Mo-Song Hsih, Li-Cheng Yang, Hsin-Chun Ho, Jer-Yuarn Wu, and Yuan-Tsong Chen. 2004. “A Marker for Stevens–Johnson Syndrome.” Nature 428 (6982): 486–86. https://doi.org/10.1038/428486a.\n\n\nDabernig-Heinz, Johanna, Mara Lohde, Martin Hölzer, Adriana Cabal, Rick Conzemius, Christian Brandt, Matthias Kohl, et al. 2024. “A Multicenter Study on Accuracy and Reproducibility of Nanopore Sequencing-Based Genotyping of Bacterial Pathogens.” Journal of Clinical Microbiology 62 (9): e00628–24. https://doi.org/10.1128/jcm.00628-24.\n\n\nDePristo, Mark A., Eric Banks, Ryan Poplin, Kiran V. Garimella, Jared R. Maguire, Christopher Hartl, Anthony A. Philippakis, et al. 2011. “A Framework for Variation Discovery and Genotyping Using Next-Generation DNA Sequencing Data.” Nature Genetics 43 (5): 491–98. https://doi.org/10.1038/ng.806.\n\n\nGarrison, Erik, Jouni Sirén, Adam M. Novak, Glenn Hickey, Jordan M. Eizenga, Eric T. Dawson, William Jones, et al. 2018. “Variation Graph Toolkit Improves Read Mapping by Representing Genetic Variation in the Reference.” Nature Biotechnology 36 (9): 875–79. https://doi.org/10.1038/nbt.4227.\n\n\nGoodwin, Sara, John D. McPherson, and W. Richard McCombie. 2016. “Coming of Age: Ten Years of Next-Generation Sequencing Technologies.” Nature Reviews Genetics 17 (6): 333–51. https://doi.org/10.1038/nrg.2016.49.\n\n\nJiang, Tao, Yongzhuang Liu, Yue Jiang, Junyi Li, Yan Gao, Zhe Cui, Yadong Liu, Bo Liu, and Yadong Wang. 2020. “Long-Read-Based Human Genomic Structural Variation Detection with cuteSV.” Genome Biology 21 (1): 189. https://doi.org/10.1186/s13059-020-02107-y.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKrusche, Peter, Len Trigg, Paul C. Boutros, Christopher E. Mason, Francisco M. De La Vega, Benjamin L. Moore, Mar Gonzalez-Porta, et al. 2019. “Best Practices for Benchmarking Germline Small Variant Calls in Human Genomes.” Nature Biotechnology 37 (5): 555–60. https://doi.org/10.1038/s41587-019-0054-x.\n\n\nLi, Heng. 2013. “Aligning Sequence Reads, Clone Sequences and Assembly Contigs with BWA-MEM.” arXiv. https://doi.org/10.48550/arXiv.1303.3997.\n\n\n———. 2014. “Towards Better Understanding of Artifacts in Variant Calling from High-Coverage Samples.” Bioinformatics 30 (20): 2843–51. https://doi.org/10.1093/bioinformatics/btu356.\n\n\n———. 2018. “Minimap2: Pairwise Alignment for Nucleotide Sequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness, Glenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome Reference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nLoh, Po-Ru, Petr Danecek, Pier Francesco Palamara, Christian Fuchsberger, Yakir A Reshef, Hilary K Finucane, Sebastian Schoenherr, et al. 2016. “Reference-Based Phasing Using the Haplotype Reference Consortium Panel.” Nature Genetics 48 (11): 1443–48. https://doi.org/10.1038/ng.3679.\n\n\nMallal, Simon, Elizabeth Phillips, Giampiero Carosi, Jean-Michel Molina, Cassy Workman, Janez Tomažič, Eva Jägel-Guedes, et al. 2008. “HLA-B*5701 Screening for Hypersensitivity to Abacavir.” New England Journal of Medicine 358 (6): 568–79. https://doi.org/10.1056/NEJMoa0706135.\n\n\nMcKenna, Aaron, Matthew Hanna, Eric Banks, Andrey Sivachenko, Kristian Cibulskis, Andrew Kernytsky, Kiran Garimella, et al. 2010. “The Genome Analysis Toolkit: A MapReduce Framework for Analyzing Next-Generation DNA Sequencing Data.” Genome Research 20 (9): 1297–1303. https://doi.org/10.1101/gr.107524.110.\n\n\nNielsen, Rasmus, Joshua S. Paul, Anders Albrechtsen, and Yun S. Song. 2011. “Genotype and SNP Calling from Next-Generation Sequencing Data.” Nature Reviews. Genetics 12 (6): 443–51. https://doi.org/10.1038/nrg2986.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V. Bzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The Complete Sequence of a Human Genome.” Science 376 (6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Connell, Jared, Deepti Gurdasani, Olivier Delaneau, Nicola Pirastu, Sheila Ulivi, Massimiliano Cocca, Michela Traglia, et al. 2014. “A General Approach for Haplotype Phasing Across the Full Spectrum of Relatedness.” PLOS Genetics 10 (4): e1004234. https://doi.org/10.1371/journal.pgen.1004234.\n\n\n“PacificBiosciences/Pbsv.” 2025. PacBio. https://github.com/PacificBiosciences/pbsv.\n\n\nPadyukov, Leonid. 2022. “Genetics of Rheumatoid Arthritis.” Seminars in Immunopathology 44 (1): 47–62. https://doi.org/10.1007/s00281-022-00912-0.\n\n\nPoplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, et al. 2018. “[DeepVariant] A Universal SNP and Small-Indel Variant Caller Using Deep Neural Networks.” Nature Biotechnology 36 (10): 983–87. https://doi.org/10.1038/nbt.4235.\n\n\n“RealTimeGenomics/Rtg-Core.” 2025. Real Time Genomics. https://github.com/RealTimeGenomics/rtg-core.\n\n\nRobinson, James, Dominic J Barker, Xenia Georgiou, Michael A Cooper, Paul Flicek, and Steven G E Marsh. 2020. “IPD-IMGT/HLA Database.” Nucleic Acids Research 48 (D1): D948–55. https://doi.org/10.1093/nar/gkz950.\n\n\nSakaue, Saori, Saisriram Gurajala, Michelle Curtis, Yang Luo, Wanson Choi, Kazuyoshi Ishigaki, Joyce B. Kang, et al. 2023. “Tutorial: A Statistical Genetics Guide to Identifying HLA Alleles Driving Complex Disease.” Nature Protocols 18 (9): 2625–41. https://doi.org/10.1038/s41596-023-00853-4.\n\n\nShafin, Kishwar, Trevor Pesout, Pi-Chuan Chang, Maria Nattestad, Alexey Kolesnikov, Sidharth Goel, Gunjan Baid, et al. 2021. “Haplotype-Aware Variant Calling with PEPPER-Margin-DeepVariant Enables High Accuracy in Nanopore Long-Reads.” Nature Methods 18 (11): 1322–32. https://doi.org/10.1038/s41592-021-01299-w.\n\n\nSmolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W. Horner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024. “Detection of Mosaic and Population-Level Structural Variants with Sniffles2.” Nature Biotechnology 42 (10): 1571–80. https://doi.org/10.1038/s41587-023-02024-y.\n\n\nSong, Li, Gali Bai, X. Shirley Liu, Bo Li, and Heng Li. 2022. “T1K: Efficient and Accurate KIR and HLA Genotyping with Next-Generation Sequencing Data.” bioRxiv. https://doi.org/10.1101/2022.10.26.513955.\n\n\nVan der Auwera, Geraldine A., Mauricio O. Carneiro, Christopher Hartl, Ryan Poplin, Guillermo del Angel, Ami Levy-Moonshine, Tadeusz Jordan, et al. 2018. “From FastQ Data to High-Confidence Variant Calls: The Genome Analysis Toolkit Best Practices Pipeline.” Current Protocols in Bioinformatics 43 (1): 11.10.1–33. https://doi.org/10.1002/0471250953.bi1110s43.\n\n\nWenger, Aaron M., Paul Peluso, William J. Rowell, Pi-Chuan Chang, Richard J. Hall, Gregory T. Concepcion, Jana Ebler, et al. 2019. “Accurate Circular Consensus Long-Read Sequencing Improves Variant Detection and Assembly of a Human Genome.” Nature Biotechnology 37 (10): 1155–62. https://doi.org/10.1038/s41587-019-0217-9.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll, and Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls Using DeepVariant and GLnexus.” Bioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam, and Ruibang Luo. 2022. “Symphonizing Pileup and Full-Alignment for Deep Learning-Based Long-Read Variant Calling.” Nature Computational Science 2 (12): 797–803. https://doi.org/10.1038/s43588-022-00387-x.\n\n\nZook, Justin M., Jennifer McDaniel, Nathan D. Olson, Justin Wagner, Hemang Parikh, Haynes Heaton, Sean A. Irvine, et al. 2019. “An Open Resource for Accurately Benchmarking Small Variant and Reference Calls.” Nature Biotechnology 37 (5): 561–66. https://doi.org/10.1038/s41587-019-0074-6.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html",
    "href": "p1-ch02-data.html",
    "title": "2  The Genomic Data Landscape",
    "section": "",
    "text": "2.1 Why Genomic Data Resources Matter\nOnce we can sequence genomes and call variants, we immediately face a new problem: interpretation. No single dataset is sufficient to decide whether a variant is benign, pathogenic, or relevant to a trait. Instead, we rely on a mosaic of complementary resources: reference genomes and gene annotations that define coordinates and consequences, population variation catalogs that reveal what survives in healthy individuals, cohort and biobank datasets that link variation to phenotypes, functional genomics atlases that map biochemical activity, and clinical databases that aggregate expert interpretations.\nThis chapter surveys these foundational resources. Later chapters draw from them repeatedly—either directly as model inputs or indirectly as labels, benchmarks, and priors. We begin with general genomic infrastructure (references, variation catalogs, cohorts) and then turn to functional and expression resources (ENCODE, GTEx-like datasets) that provide the training labels for sequence-to-function models.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#reference-genomes-and-gene-annotations",
    "href": "p1-ch02-data.html#reference-genomes-and-gene-annotations",
    "title": "2  The Genomic Data Landscape",
    "section": "2.2 Reference Genomes and Gene Annotations",
    "text": "2.2 Reference Genomes and Gene Annotations\nEvery genomic analysis begins with a coordinate system. Reference genomes define the scaffold onto which sequencing reads are mapped, while gene annotations overlay that scaffold with biological meaning, specifying where transcripts begin and end, which regions encode protein, and how exons are spliced together. These resources are so foundational that their assumptions often become invisible: a variant’s consequence, a gene’s constraint score, and a model’s training labels all depend on choices embedded in the reference assembly and annotation release. Understanding these dependencies is essential for interpreting results, recognizing systematic biases, and anticipating how analyses will generalize across datasets built on different genomic foundations.\n\n2.2.1 Reference Assemblies\nMost modern pipelines align reads to a small number of reference assemblies, predominantly GRCh38 or the newer T2T-CHM13 (Nurk et al. 2022). A reference genome is not simply a consensus sequence; it encodes a series of consequential decisions about how to represent duplications, alternate haplotypes, and unresolved gaps, all annotated with coordinates that downstream tools assume are stable.\nThe choice of reference shapes everything that follows. It determines which regions are “mappable” by short reads, how structural variants are represented, and how comparable results will be across cohorts and over time. Graph-based and pangenome references relax the assumption of a single linear reference, but the majority of datasets used in this book, and the models trained on them, are still built on GRCh37 or GRCh38 (Liao et al. 2023).\n\n\n2.2.2 Gene Models\nGene annotation databases such as GENCODE and RefSeq define the exon–intron structures, canonical and alternative transcripts, start and stop codons, and untranslated regions that allow us to interpret variants in biological context (Frankish et al. 2019; O’Leary et al. 2016). These annotations are critical for distinguishing coding from non-coding variants, identifying splice-disrupting mutations, and mapping functional genomics signals to genes.\nThe MANE Select project provides a single matched transcript per protein-coding gene that is identical between GENCODE and RefSeq, simplifying clinical interpretation but further privileging a single isoform over biological complexity (Morales et al. 2022).\nMany downstream resources, from variant effect predictors to polygenic score pipelines, implicitly assume that gene models are correct and complete. In practice, new isoforms continue to be discovered, alternative splicing remains incompletely cataloged, and cell-type-specific transcripts may be missing from bulk-derived annotations. These gaps propagate through every tool built on them.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#population-variant-catalogs-and-allele-frequencies",
    "href": "p1-ch02-data.html#population-variant-catalogs-and-allele-frequencies",
    "title": "2  The Genomic Data Landscape",
    "section": "2.3 Population Variant Catalogs and Allele Frequencies",
    "text": "2.3 Population Variant Catalogs and Allele Frequencies\nPopulation variant catalogs provide the empirical foundation for distinguishing pathogenic mutations from benign polymorphisms. Allele frequency, the proportion of chromosomes in a reference population carrying a given variant, serves as a powerful prior: variants observed at appreciable frequency in healthy individuals are unlikely to cause severe early-onset disease, while ultra-rare variants demand closer scrutiny. Beyond simple filtering, allele frequencies inform statistical frameworks for case-control association, provide training signal for deleteriousness predictors, and enable imputation of ungenotyped variants through linkage disequilibrium The catalogs described below have progressively expanded in sample size, ancestral diversity, and annotation depth, transforming variant interpretation from an ad hoc exercise into a quantitative discipline.\n\n2.3.1 dbSNP and the Variant Universe\nHistorically, dbSNP aggregated known single nucleotide polymorphisms and short indels into a single catalog, providing stable identifiers (rsIDs) that serve as common currency across tools and publications, basic frequency information where available, and a convenient handle for linking to other resources (Sherry et al. 2001). Modern whole-exome and whole-genome sequencing cohorts routinely discover millions of previously unseen variants, but dbSNP identifiers remain the standard way to refer to known polymorphisms.\n\n\n2.3.2 1000 Genomes and Early Reference Panels\nThe 1000 Genomes Project provided one of the first widely used multi-population reference panels, enabling imputation and linkage-disequilibrium-based analyses on genotyping arrays (Auton et al. 2015). Its samples continue to serve as benchmarks for variant calling performance, and its haplotype structure underlies many imputation servers and downstream analyses (Yun et al. 2021).\n\n\n2.3.3 The Genome Aggregation Database (gnomAD)\nThe Genome Aggregation Database aggregates exome and genome data from a wide array of cohorts into harmonized allele frequency resources (Karczewski et al. 2020). gnomAD provides high-resolution allele frequencies for SNVs and indels across diverse ancestries, constraint metrics such as pLI and LOEUF that summarize a gene’s intolerance to loss-of-function variation, and per-variant annotations flagging poor quality regions, low complexity, and other caveats.\nThese resources are indispensable for filtering common variants in Mendelian disease diagnostics, distinguishing extremely rare variants from recurrent ones, and providing population genetics priors used by variant effect predictors and deleteriousness scores like CADD (Rentzsch et al. 2019; Schubach et al. 2024). The constraint metrics, in particular, have become standard features in machine learning models that prioritize disease-relevant genes and variants.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#cohorts-biobanks-and-gwas-summary-data",
    "href": "p1-ch02-data.html#cohorts-biobanks-and-gwas-summary-data",
    "title": "2  The Genomic Data Landscape",
    "section": "2.4 Cohorts, Biobanks, and GWAS Summary Data",
    "text": "2.4 Cohorts, Biobanks, and GWAS Summary Data\nLarge-scale biobanks and population cohorts have transformed human genetics from a discipline reliant on family studies and candidate gene approaches into one powered by population-level statistical inference. These resources link genomic data to electronic health records, lifestyle questionnaires, imaging, and longitudinal outcomes, enabling discovery of genetic associations across thousands of traits simultaneously. However, the composition of these cohorts carries consequences: the overrepresentation of European-ancestry individuals in most major biobanks creates systematic gaps in variant discovery, effect size estimation, and polygenic score portability that propagate through downstream analyses. These ancestry biases, and strategies for addressing them, are discussed in detail in Chapter 16.\n\n2.4.1 Large Population Cohorts\nModern human genetics relies on large cohorts with genome-wide variation and rich phenotyping. UK Biobank, with approximately 500,000 participants and deep phenotyping, has become the dominant resource for methods development and benchmarking (Bycroft et al. 2018). FinnGen leverages Finland’s population history and unified healthcare records (Kurki et al. 2023). The All of Us Research Program prioritizes diversity, aiming to enroll one million participants with deliberate oversampling of historically underrepresented groups (All of Us Research Program Investigators 2019). Additional resources include the Million Veteran Program, Mexican Biobank, BioBank Japan, China Kadoorie Biobank, and emerging African genomics initiatives such as H3Africa (Sirugo, Williams, and Tishkoff 2019). Together, these efforts enable genome-wide association studies for thousands of traits, development and evaluation of polygenic scores, and fine-mapping of causal variants and genes (Marees et al. 2018; Mountjoy et al. 2021).\nWhile this book focuses on models rather than specific cohorts, it is important to recognize that most GWAS and polygenic score methods in Chapter 3 assume data from either array genotyping with imputation or whole-exome/whole-genome sequencing with joint calling, as in DeepVariant/GLnexus-style pipelines (Yun et al. 2021). The ascertainment, quality control, and population composition of these cohorts shape what signals can be detected and how well models generalize.\n\n\n2.4.2 GWAS Summary Statistics\nBeyond individual-level data, many resources distribute GWAS summary statistics: per-variant effect sizes and p-values aggregated across cohorts. The GWAS Catalog compiles published results across traits (Sollis et al. 2023), while the PGS Catalog provides curated polygenic score weights and metadata for reproducibility (Lambert et al. 2021). Frameworks like Open Targets Genetics integrate fine-mapped signals and candidate causal genes across loci (Mountjoy et al. 2021).\nThese summary data are the raw material for many polygenic score methods (Chapter 3) and statistical fine-mapping algorithms. They enable meta-analysis across cohorts, transfer of genetic findings to new populations, and integration with functional annotations to prioritize causal variants.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#functional-genomics-and-regulatory-landscapes",
    "href": "p1-ch02-data.html#functional-genomics-and-regulatory-landscapes",
    "title": "2  The Genomic Data Landscape",
    "section": "2.5 Functional Genomics and Regulatory Landscapes",
    "text": "2.5 Functional Genomics and Regulatory Landscapes\nThe vast majority of the human genome lies outside protein-coding exons, yet this non-coding space harbors the regulatory logic that governs when, where, and how much each gene is expressed. Functional genomics assays provide the experimental means to map this regulatory landscape: identifying transcription factor binding sites, nucleosome positioning, chromatin accessibility, histone modifications, and three-dimensional genome organization across cell types and conditions. For the purposes of this book, these datasets serve a dual role. First, they supply the biological vocabulary for interpreting non-coding variants, linking sequence changes to potential regulatory consequences. Second, and more directly, they provide the training labels for sequence-to-function deep learning models. When a model learns to predict chromatin accessibility or histone marks from DNA sequence alone, it is learning a compressed representation of the regulatory code implicit in thousands of functional genomics experiments.\n\n2.5.1 ENCODE, Roadmap, and Related Consortia\nProjects like ENCODE, Roadmap Epigenomics, and Gene Expression Omnibus (GEO) are primary data generation efforts that designed coordinated experimental campaigns, selected cell types and tissues for profiling, and produced comprehensive compendia of transcription factor ChIP-seq, histone modification ChIP-seq, open chromatin assays (DNase-seq, ATAC-seq), and chromatin conformation data (Hi-C and related methods) (Kagda et al. 2025; Kundaje et al. 2015; Edgar, Domrachev, and Lash 2002). These datasets map regulatory elements, chromatin states, and higher-order genome structure with tight experimental control and uniform processing pipelines.\nThe significance of these consortia for this book is less about any individual experiment than about the scale and standardization they provide. By generating hundreds of assays across dozens of cell types with consistent protocols, ENCODE and Roadmap created canonical reference datasets that define the regulatory landscape for the cell types they profiled.\n\n\n2.5.2 The Cistrome Data Browser\nWhile ENCODE and Roadmap produced authoritative datasets for their chosen cell types and factors, they represent only a fraction of publicly available functional genomics experiments. The Cistrome Data Browser addresses this gap by aggregating thousands of human and mouse ChIP-seq and chromatin accessibility datasets from ENCODE, Roadmap, GEO, and individual publications into a reprocessed, searchable repository (Zheng et al. 2019). All datasets pass through a uniform quality control and processing pipeline, enabling comparisons across experiments that were originally generated by different labs with different protocols.\nCistrome provides uniform peak calls and signal tracks, metadata for cell type, factor, and experimental conditions, and tools for motif analysis and regulatory element annotation. The tradeoff is heterogeneity: while the reprocessing harmonizes computational steps, the underlying experiments vary in sample preparation, sequencing depth, and experimental design. Cistrome thus expands coverage at the cost of the tight experimental control found in the primary consortia.\n\n\n2.5.3 From Assays to Training Labels\nSequence-to-function models transform these functional genomics resources into supervised learning problems. Models like DeepSEA (see Chapter 5) draw training labels from ENCODE, Roadmap, and Cistrome-style datasets collectively: each genomic window is associated with binary or quantitative signals indicating transcription factor binding, histone modifications, or chromatin accessibility across many assays and cell types (Zhou and Troyanskaya 2015; Zhou et al. 2018).\nThe quality, coverage, and biases of these labels directly constrain what models can learn. Cell types absent from the training compendium cannot be predicted reliably. Factors with few high-quality ChIP-seq experiments will have noisier labels. And systematic differences between assay types (peak-based binary labels versus quantitative signal tracks) shape whether models learn to predict occupancy, accessibility, or something in between. These considerations become central when we examine model architectures and training strategies in Chapter 5.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#expression-and-eqtl-resources",
    "href": "p1-ch02-data.html#expression-and-eqtl-resources",
    "title": "2  The Genomic Data Landscape",
    "section": "2.6 Expression and eQTL Resources",
    "text": "2.6 Expression and eQTL Resources\nExpression datasets link sequence variation to transcriptional consequences, providing a bridge between regulatory elements and gene-level effects. While functional genomics assays reveal where transcription factors bind and which chromatin regions are accessible, expression data answer the downstream question: does this regulatory activity actually change how much RNA a gene produces? Expression quantitative trait loci (eQTLs) formalize this relationship statistically, identifying genetic variants associated with changes in transcript abundance. For variant interpretation and genomic prediction, eQTLs offer mechanistic hypotheses connecting non-coding variants to specific genes and tissues. For model training, expression data provide quantitative labels that integrate across the many regulatory inputs converging on a single promoter. The resources below range from population-scale bulk tissue atlases to emerging single-cell datasets that resolve expression variation at cellular resolution.\n\n2.6.1 Bulk Expression Atlases\nProjects like the Genotype-Tissue Expression (GTEx) consortium provide RNA-seq expression profiles across dozens of tissues, eQTL maps linking variants to gene expression changes in cis, and splicing QTLs and other molecular QTLs (The GTEx Consortium 2020). With matched genotypes and expression data from nearly 1,000 post-mortem donors across 54 tissues, GTEx established foundational insights: most genes harbor tissue-specific eQTLs, regulatory variants typically act in cis over distances of hundreds of kilobases, and expression variation explains a meaningful fraction of complex trait heritability.\nEven when not explicitly cited, GTEx-like resources underpin expression prediction models such as PrediXcan and TWAS frameworks, colocalization analyses that ask whether a GWAS signal and an eQTL share a causal variant, and expression-based prioritization of candidate genes at trait-associated loci (Gamazon et al. 2015; Gusev et al. 2016). The GTEx design has limitations: post-mortem collection introduces agonal stress artifacts, sample sizes per tissue vary considerably, and some disease-relevant tissues (such as pancreatic islets or specific brain regions) remain undersampled. Complementary resources like the eQTLGen Consortium aggregate eQTL results from blood across larger sample sizes, trading tissue diversity for statistical power (Võsa et al. 2021).\n\n\n2.6.2 Single-Cell and Context-Specific Expression\nBulk RNA-seq averages expression across all cells in a tissue sample, obscuring the cell-type-specific programs that often mediate disease biology. Single-cell RNA-seq resolves this heterogeneity, identifying expression signatures for individual cell types, rare populations, and transitional states. Large-scale efforts like the Human Cell Atlas, Tabula Sapiens, and disease-focused single-cell consortia are building reference atlases that catalog cell types across organs and developmental stages (Regev et al. 2017; The Tabula Sapiens Consortium 2022).\nFor variant interpretation, single-cell data enable cell-type-specific eQTL mapping, revealing that a variant may influence expression in one cell type but not others within the same tissue. Spatial transcriptomics adds anatomical context, preserving tissue architecture while measuring gene expression. These technologies introduce computational challenges: sparsity from dropout, batch effects across samples and technologies, and the sheer scale of datasets with millions of cells. In this book, single-cell and spatial resources appear primarily in later chapters on multi-omics integration and systems-level models, but they represent the direction toward which expression genetics is moving, promising to connect genetic variation to cellular phenotypes with unprecedented resolution.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#variant-interpretation-databases-and-clinical-labels",
    "href": "p1-ch02-data.html#variant-interpretation-databases-and-clinical-labels",
    "title": "2  The Genomic Data Landscape",
    "section": "2.7 Variant Interpretation Databases and Clinical Labels",
    "text": "2.7 Variant Interpretation Databases and Clinical Labels\nAllele frequencies tell us what variants are tolerated in healthy populations, and functional genomics data reveal where the genome is biochemically active, but neither directly answers the clinical question: is this variant pathogenic? That determination requires integrating multiple lines of evidence, including family segregation, functional assays, computational predictions, and phenotypic observations, into a structured framework that can be applied consistently across variants, genes, and diseases. Clinical variant interpretation databases aggregate these assessments from laboratories, expert panels, and research groups, providing labels that inform diagnostic decisions, guide research, and serve as training data for machine learning models. These databases have become critical infrastructure for both clinical genomics and computational method development, though their labels carry biases and circularity that propagate through any analysis built on them.\n\n2.7.1 ClinVar and Related Resources\nClinVar aggregates assertions of variant pathogenicity from clinical laboratories and researchers, with supporting evidence and conflicting interpretations where relevant (Landrum et al. 2018). Its labels are critical for diagnostic pipelines, benchmarking variant effect predictors, and training machine learning models in clinical genomics.\nHowever, ClinVar’s labels are not collected in isolation. As discussed in Chapter 5, clinical submissions increasingly incorporate computational scores like CADD as one piece of evidence, which creates subtle circularity when those same labels are used to evaluate or train computational predictors (Schubach et al. 2024). This circularity is a recurring methodological concern throughout the book.\n\n\n2.7.2 ClinGen and Expert Curation\nThe Clinical Genome Resource (ClinGen) complements ClinVar by providing expert-curated assessments at multiple levels of granularity (Rehm et al. 2015). ClinGen expert panels evaluate gene-disease validity, asking whether variation in a particular gene can cause a specific disease, and dosage sensitivity, determining whether haploinsufficiency or triplosensitivity leads to clinical phenotypes. These evaluations build on the catalog of Mendelian phenotypes maintained by OMIM (Online Mendelian Inheritance in Man), which provides curated gene-disease associations, clinical synopses, and literature summaries that have long served as the reference for clinical genetics (Amberger et al. 2015).\nFor individual variants, ClinGen Variant Curation Expert Panels apply the ACMG/AMP criteria systematically, and the FDA has recognized these curations as a valid source of scientific evidence for clinical validity (Pejaver et al. 2022). ClinGen also develops calibrated thresholds for computational predictors like CADD and REVEL, specifying score intervals that justify different strengths of evidence for pathogenicity or benignity. These calibrated thresholds directly inform how computational scores should be incorporated into variant classification workflows.\n\n\n2.7.3 ClinPGx and Pharmacogenomics Resources\nClinPGx integrates the PharmGKB knowledge base, CPIC clinical guidelines, and PharmCAT annotation tool into a unified pharmacogenomics resource (Whirl-Carrillo et al. 2012). While most variant interpretation databases focus on disease-causing mutations, ClinPGx curates gene-drug associations that influence drug metabolism, efficacy, and adverse reactions. These pharmacogenomic variants are often common polymorphisms rather than rare pathogenic mutations, but their clinical importance for prescribing decisions makes them a distinct category of actionable genetic variation. The CPIC guidelines provide evidence-based recommendations for adjusting drug selection or dosing based on pharmacogene diplotypes, and ClinPGx-annotated FDA drug labels document the regulatory status of these associations.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#how-later-chapters-use-these-resources",
    "href": "p1-ch02-data.html#how-later-chapters-use-these-resources",
    "title": "2  The Genomic Data Landscape",
    "section": "2.8 How Later Chapters Use These Resources",
    "text": "2.8 How Later Chapters Use These Resources\nThe genomic deep learning models that follow inherit both the strengths and limitations of the data they are trained on. Chapter 3 draws on GWAS summary statistics and biobank-scale cohorts to construct polygenic scores. Chapter 5 examines how annotation-based methods compress population frequencies, conservation, and functional signals into genome-wide deleteriousness scores. Chapters 5-7 use ENCODE, Roadmap, and Cistrome-style functional data as training labels for sequence-to-function models, while Chapters 12-14 revisit these resources as inputs, labels, and priors for genomic foundation models.\nBy surveying the data landscape in one place, we establish a common reference that later chapters can build on rather than re-introducing each resource from scratch. The recurring theme is that biases, gaps, and circularity in these foundational datasets propagate through every model trained on them. A variant effect predictor trained on ClinVar labels inherits the ascertainment biases of clinical sequencing; a chromatin model trained on ENCODE cell lines may not generalize to primary tissues. Understanding these foundations is essential for interpreting what models learn and anticipating where they will fail.\n\n\n\n\nAll of Us Research Program Investigators, All of Us; 2019. “The ‘All of Us’ Research Program.” New England Journal of Medicine 381 (7): 668–76. https://doi.org/10.1056/NEJMsr1809937.\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F. Scott, and Ada Hamosh. 2015. “OMIM.org: Online Mendelian Inheritance in Man (OMIM®), an Online Catalog of Human Genes and Genetic Disorders.” Nucleic Acids Research 43 (D1): D789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAuton, Adam, Gonçalo R. Abecasis, David M. Altshuler, Richard M. Durbin, Gonçalo R. Abecasis, David R. Bentley, Aravinda Chakravarti, et al. 2015. “A Global Reference for Human Genetic Variation.” Nature 526 (7571): 68–74. https://doi.org/10.1038/nature15393.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nEdgar, Ron, Michael Domrachev, and Alex E. Lash. 2002. “Gene Expression Omnibus: NCBI Gene Expression and Hybridization Array Data Repository.” Nucleic Acids Research 30 (1): 207–10. https://doi.org/10.1093/nar/30.1.207.\n\n\nFrankish, Adam, Mark Diekhans, Anne-Maud Ferreira, Rory Johnson, Irwin Jungreis, Jane Loveland, Jonathan M Mudge, et al. 2019. “GENCODE Reference Annotation for the Human and Mouse Genomes.” Nucleic Acids Research 47 (D1): D766–73. https://doi.org/10.1093/nar/gky955.\n\n\nGamazon, Eric R., Heather E. Wheeler, Kaanan P. Shah, Sahar V. Mozaffari, Keston Aquino-Michaels, Robert J. Carroll, Anne E. Eyler, et al. 2015. “A Gene-Based Association Method for Mapping Traits Using Reference Transcriptome Data.” Nature Genetics 47 (9): 1091–98. https://doi.org/10.1038/ng.3367.\n\n\nGusev, Alexander, Arthur Ko, Huwenbo Shi, Gaurav Bhatia, Wonil Chung, Brenda W. J. H. Penninx, Rick Jansen, et al. 2016. “Integrative Approaches for Large-Scale Transcriptome-Wide Association Studies.” Nature Genetics 48 (3): 245–52. https://doi.org/10.1038/ng.3506.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data Navigation on the ENCODE Portal.” Nature Communications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. “Integrative Analysis of 111 Reference Human Epigenomes.” Nature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nKurki, Mitja I., Juha Karjalainen, Priit Palta, Timo P. Sipilä, Kati Kristiansson, Kati M. Donner, Mary P. Reeve, et al. 2023. “FinnGen Provides Genetic Insights from a Well-Phenotyped Isolated Population.” Nature 613 (7944): 508–18. https://doi.org/10.1038/s41586-022-05473-8.\n\n\nLambert, Samuel A., Laurent Gil, Simon Jupp, Scott C. Ritchie, Yu Xu, Annalisa Buniello, Aoife McMahon, et al. 2021. “The Polygenic Score Catalog as an Open Database for Reproducibility and Systematic Evaluation.” Nature Genetics 53 (4): 420–25. https://doi.org/10.1038/s41588-021-00783-5.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness, Glenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome Reference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nMarees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan, Emmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018. “[GWAS] A Tutorial on Conducting Genome-Wide Association Studies: Quality Control and Statistical Analysis.” International Journal of Methods in Psychiatric Research 27 (2): e1608. https://doi.org/10.1002/mpr.1608.\n\n\nMorales, Joannella, Shashikant Pujar, Jane E. Loveland, Alex Astashyn, Ruth Bennett, Andrew Berry, Eric Cox, et al. 2022. “A Joint NCBI and EMBL-EBI Transcript Set for Clinical Genomics and Research.” Nature 604 (7905): 310–15. https://doi.org/10.1038/s41586-022-04558-8.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy Schwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021. “An Open Approach to Systematically Prioritize Causal Variants and Genes at All Published Human GWAS Trait-Associated Loci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V. Bzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The Complete Sequence of a Human Genome.” Science 376 (6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Leary, Nuala A., Mathew W. Wright, J. Rodney Brister, Stacy Ciufo, Diana Haddad, Rich McVeigh, Bhanu Rajput, et al. 2016. “Reference Sequence (RefSeq) Database at NCBI: Current Status, Taxonomic Expansion, and Functional Annotation.” Nucleic Acids Research 44 (D1): D733–45. https://doi.org/10.1093/nar/gkv1189.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel, Sean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022. “Calibration of Computational Tools for Missense Variant Pathogenicity Classification and ClinGen Recommendations for PP3/BP4 Criteria.” American Journal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nRegev, Aviv, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe Benoist, Ewan Birney, Bernd Bodenmiller, et al. 2017. “The Human Cell Atlas.” Edited by Thomas R Gingeras. eLife 6 (December): e27041. https://doi.org/10.7554/eLife.27041.\n\n\nRehm, Heidi L., Jonathan S. Berg, Lisa D. Brooks, Carlos D. Bustamante, James P. Evans, Melissa J. Landrum, David H. Ledbetter, et al. 2015. “ClinGen — The Clinical Genome Resource.” New England Journal of Medicine 372 (23): 2235–42. https://doi.org/10.1056/NEJMsr1406261.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nSherry, S. T., M.-H. Ward, M. Kholodov, J. Baker, L. Phan, E. M. Smigielski, and K. Sirotkin. 2001. “dbSNP: The NCBI Database of Genetic Variation.” Nucleic Acids Research 29 (1): 308–11. https://doi.org/10.1093/nar/29.1.308.\n\n\nSirugo, Giorgio, Scott M. Williams, and Sarah A. Tishkoff. 2019. “The Missing Diversity in Human Genetic Studies.” Cell 177 (1): 26–31. https://doi.org/10.1016/j.cell.2019.02.048.\n\n\nSollis, Elliot, Abayomi Mosaku, Ala Abid, Annalisa Buniello, Maria Cerezo, Laurent Gil, Tudor Groza, et al. 2023. “The NHGRI-EBI GWAS Catalog: Knowledgebase and Deposition Resource.” Nucleic Acids Research 51 (D1): D977–85. https://doi.org/10.1093/nar/gkac1010.\n\n\nThe GTEx Consortium. 2020. “The GTEx Consortium Atlas of Genetic Regulatory Effects Across Human Tissues.” Science 369 (6509): 1318–30. https://doi.org/10.1126/science.aaz1776.\n\n\nThe Tabula Sapiens Consortium. 2022. “The Tabula Sapiens: A Multiple-Organ, Single-Cell Transcriptomic Atlas of Humans.” Science 376 (6594): eabl4896. https://doi.org/10.1126/science.abl4896.\n\n\nVõsa, Urmo, Annique Claringbould, Harm-Jan Westra, Marc Jan Bonder, Patrick Deelen, Biao Zeng, Holger Kirsten, et al. 2021. “Large-Scale Cis- and Trans-eQTL Analyses Identify Thousands of Genetic Loci and Polygenic Scores That Regulate Blood Gene Expression.” Nature Genetics 53 (9): 1300–1310. https://doi.org/10.1038/s41588-021-00913-z.\n\n\nWhirl-Carrillo, M, E M McDonagh, J M Hebert, L Gong, K Sangkuhl, C F Thorn, R B Altman, and T E Klein. 2012. “Pharmacogenomics Knowledge for Personalized Medicine.” Clinical Pharmacology & Therapeutics 92 (4): 414–17. https://doi.org/10.1038/clpt.2012.96.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll, and Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls Using DeepVariant and GLnexus.” Bioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei Sun, Chen-Hao Chen, et al. 2019. “Cistrome Data Browser: Expanded Datasets and New Tools for Gene Regulatory Analysis.” Nucleic Acids Research 47 (D1): D729–35. https://doi.org/10.1093/nar/gky1094.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch03-pgs.html",
    "href": "p1-ch03-pgs.html",
    "title": "3  GWAS & Polygenic Scores",
    "section": "",
    "text": "3.1 The GWAS Paradigm\nGenome-wide association studies represent the dominant paradigm for mapping genetic contributions to complex traits. The core idea is conceptually simple: test each of millions of genetic variants for statistical association with a phenotype of interest, then identify variants or genomic regions where association signals exceed stringent thresholds for multiple testing. This brute-force approach, made feasible by advances in genotyping technology and the assembly of large cohorts, has catalogued thousands of trait-associated loci across the human genome. Yet GWAS is fundamentally a statistical exercise in association, not a direct window into biological mechanism. Understanding both its power and its limitations is essential groundwork for the mechanistic models we develop in later parts of this book.\nA GWAS requires three ingredients: a large sample of genotyped or sequenced individuals, a well-defined phenotype (either binary, such as disease status, or quantitative, such as height or lipid levels), and a statistical model that relates genotype to phenotype while adjusting for confounders. In practice, the confounders typically include age, sex, and principal components of genetic ancestry that capture population structure (Marees et al. 2018). Alternative strategies for controlling confounding, including case-control matching on ancestry and other covariates, are discussed in Chapter 16.\nThe output of a GWAS is a set of associated variants and loci, not a direct map from variant to mechanism. Variants that pass the significance threshold are sometimes called “hits,” but this terminology obscures an important ambiguity: the variant with the smallest p-value at a locus is not necessarily the variant that causes the phenotypic effect. It may simply be a statistical proxy for the true causal variant, a distinction that becomes central in later sections.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS & Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-pgs.html#the-gwas-paradigm",
    "href": "p1-ch03-pgs.html#the-gwas-paradigm",
    "title": "3  GWAS & Polygenic Scores",
    "section": "",
    "text": "3.1.1 Continuous Phenotypes\nFor quantitative traits such as height, body mass index, or lipid levels, GWAS employs linear regression. At a single bi-allelic variant \\(j\\), the standard model takes the form\n\\[\ny_i = \\alpha + \\beta_j g_{ij} + \\gamma^\\top c_i + \\varepsilon_i,\n\\]\nwhere:\n\n\\(y_i\\) is the phenotype for individual \\(i\\).\n\\(\\alpha\\) is the intercept representing the baseline phenotype when genotype dosage and covariates are zero.\n\\(\\beta_j\\) is the per-allele effect size we wish to estimate.\n\\(g_{ij}\\) is the genotype dosage at variant \\(j\\), coded as 0, 1, or 2 copies of the alternative allele, or as an imputed fractional dosage.\n\\(\\gamma\\) is a vector of coefficients for the covariates.\n\\(c_i\\) is a vector of covariates.\n\\(\\varepsilon_i\\) is the residual error term.\n\n\n3.1.1.1 Effect Size\nThe coefficient \\(\\beta_j\\) has a direct interpretation: it is the expected change in phenotype per additional copy of the alternative allele, holding covariates constant. A variant with \\(\\hat\\beta_j = 0.05\\) for height (measured in centimeters) would be associated with an average increase of 0.05 cm per copy of the effect allele. These effect sizes are typically small, often explaining far less than 1% of phenotypic variance individually.\n\n3.1.1.1.1 Variance Explained and Polygenicity\nThe variance explained by a single variant depends on both its effect size and its allele frequency. For an additive model, the contribution to phenotypic variance is approximately \\(2p(1-p)\\beta_j^2\\), where \\(p\\) is the allele frequency. A variant with modest effect size but intermediate frequency will explain more variance than one with the same effect size but low frequency. Conversely, rare variants can harbor larger effect sizes while still explaining little population-level variance.\nThe distribution of effect sizes across the genome follows a characteristic pattern: most variants have negligible effects, a modest number have small but detectable effects, and very few have effects large enough to be individually meaningful. This “polygenicity” means that for most complex traits, thousands of variants contribute to heritability, each with a tiny increment. The degree of polygenicity varies by trait: height is highly polygenic, while some autoimmune diseases show more concentrated genetic architecture.\n\n\n3.1.1.1.2 Winner’s Curse\nEffect sizes estimated in discovery GWAS tend to be inflated relative to their true values, a phenomenon known as winner’s curse. Variants cross the significance threshold partly because sampling noise pushed their estimated effects upward; re-estimation in independent samples typically yields smaller effects. This inflation is most severe for variants near the significance threshold and motivates the use of independent replication cohorts.\n\n\n3.1.1.1.3 Standardized Effect Sizes\nGWAS results are often reported as standardized effect sizes rather than raw coefficients. When both the phenotype and genotype dosage are standardized to unit variance, \\(\\beta_j\\) represents the correlation between genotype and phenotype, and its square approximates the proportion of variance explained. Standardization facilitates comparison across traits measured in different units but obscures the clinically interpretable magnitude of effects. Summary statistics from large consortia typically include both raw and standardized effect sizes, or provide sufficient information to convert between them.\n\n\n\n3.1.1.2 Significance Testing and Multiple Comparisons\nThe test statistic \\(z_j = \\hat\\beta_j / \\text{SE}(\\hat\\beta_j)\\) follows approximately a standard normal distribution under the null hypothesis of no association. The corresponding p-value quantifies how unlikely the observed association would be if the variant had no true effect on the phenotype.\n\n3.1.1.2.1 Genome-Wide Significance\nTesting millions of variants simultaneously creates a severe multiple comparisons problem. At a nominal significance level of \\(\\alpha = 0.05\\), we would expect roughly 50,000 false positives among one million independent tests. The field has converged on a genome-wide significance threshold of \\(p &lt; 5 \\times 10^{-8}\\), derived from a Bonferroni correction for approximately one million independent common variants in the human genome after accounting for linkage disequilibrium (pe?’er_estimation_2008). This threshold is stringent by design: it controls the family-wise error rate, ensuring that across all tests, the probability of even one false positive remains below 0.05.\nThe Bonferroni correction is conservative when tests are correlated, as they are in GWAS due to LD. Alternative approaches, such as controlling the false discovery rate (FDR), permit more discoveries at the cost of accepting a known proportion of false positives among significant results. In practice, the \\(5 \\times 10^{-8}\\) threshold has proven robust and remains the convention for declaring genome-wide significant associations, though suggestive thresholds (often \\(p &lt; 10^{-5}\\)) are sometimes used to flag loci for replication.\n\n\n3.1.1.2.2 Significance Versus Magnitude\nThe distinction between statistical significance and effect magnitude deserves emphasis. In sufficiently large samples, even tiny effects become highly significant. A variant explaining 0.01% of phenotypic variance might achieve \\(p &lt; 10^{-50}\\) in a million-person study. Significance tells us whether an association is likely real; effect size tells us whether it matters. For polygenic score construction and biological interpretation, effect sizes are the quantities that carry scientific meaning.\n\n\n\n3.1.1.3 Covariates\nThe covariate vector \\(c_i\\) typically includes age, sex, and principal components of genetic ancestry. The ancestry components are essential for avoiding confounding due to population structure: if allele frequencies and phenotype means both vary across populations, a naive analysis will find spurious associations at variants that simply track ancestry rather than causally influencing the trait. Additional covariates such as genotyping batch, recruitment site, or technical factors may be included depending on the study design.\nThe covariate coefficients \\(\\gamma\\) have an analogous interpretation: each element represents the expected change in phenotype per unit change in the corresponding covariate, holding genotype and other covariates constant. For example, a coefficient of 0.5 for age would indicate that each additional year of age is associated with a 0.5-unit increase in the phenotype. Unlike \\(\\beta_j\\), the covariate coefficients are nuisance parameters in GWAS; they are included to avoid confounding but are not the quantities of scientific interest.\n\n\n3.1.1.4 Ancestry PCs\nAmong the covariates, principal components of genetic ancestry deserve particular explanation because they address a confounding problem specific to genetic association studies: population stratification. Human populations differ in allele frequencies due to demographic history, and these frequency differences can correlate with phenotypic differences driven by environmental or cultural factors. If a study population includes individuals from multiple ancestral backgrounds, and if ancestry correlates with the phenotype for non-genetic reasons, variants that simply differ in frequency between populations can produce spurious associations.\nPrincipal component analysis of the genotype matrix provides a standard solution (Patterson, Price, and Reich 2006; price_pca_2006?). The first few principal components of genome-wide genotype data capture axes of genetic variation that correspond primarily to continental ancestry and finer-scale population structure. Including these PCs as covariates in the regression model adjusts for ancestry-associated confounding: the GWAS tests whether a variant is associated with phenotype beyond what would be expected from shared ancestry. In practice, most studies include between 10 and 20 ancestry PCs, though the optimal number depends on the population structure present in the cohort. This approach does not eliminate all confounding, particularly from cryptic relatedness or fine-scale structure within ancestry groups, but it handles the dominant sources of stratification in typical GWAS designs. The broader implications of ancestry for model development and evaluation are discussed in Chapter 16.\n\n\n3.1.1.5 Intercept and Residuals\nIn the linear model, \\(\\alpha\\) represents the expected phenotype value when the genotype dosage is zero (homozygous reference) and all covariates are also zero. This baseline is often not directly interpretable in practice, since “zero” may not be a meaningful value for covariates like age or ancestry principal components. If covariates are mean-centered before fitting, then \\(\\alpha\\) represents the expected phenotype for a reference-homozygous individual at average covariate values, which is somewhat more intuitive. In practice, the intercept is a nuisance parameter in GWAS. The scientific focus is on \\(\\beta_j\\), the per-allele effect size, not the baseline. The intercept anchors the model but is rarely reported or interpreted in GWAS summary statistics.\nThe residual term \\(\\varepsilon_i\\) absorbs everything not captured by the modeled genotype and covariates. This includes measurement noise in the phenotype, environmental influences, gene-by-environment interactions, epistatic effects among variants, and the aggregate contribution of all other genetic variants not being tested at this particular locus. In this sense, \\(\\varepsilon_i\\) reflects both the stochastic nature of complex phenotypes and the heritability gap: even a variant with a true causal effect explains only a small fraction of phenotypic variance, leaving most variation in the residual. For highly polygenic traits, the per-variant \\(R^2\\) is typically tiny, and the residual dominates.\n\n\n\n3.1.2 Binary Phenotypes\nFor disease outcomes and other case-control phenotypes, linear regression is replaced by logistic regression. The phenotype \\(y_i\\) is now binary (1 for cases, 0 for controls), and we model the log-odds of disease:\n\\[\n\\log \\frac{P(y_i = 1)}{P(y_i = 0)} = \\alpha + \\beta_j g_{ij} + \\gamma^\\top c_i.\n\\]\nThe left-hand side is the logit of the probability of being a case. The coefficient \\(\\beta_j\\) now represents the change in log-odds per additional copy of the alternative allele, rather than a change in a continuous phenotype.\nExponentiating \\(\\beta_j\\) yields the odds ratio (OR), which is the quantity most commonly reported for binary traits. An odds ratio of 1.2 means that each copy of the effect allele multiplies the odds of disease by 1.2, or equivalently increases the odds by 20%. Most common variants identified by GWAS have odds ratios between 1.05 and 1.5, reflecting modest individual effects that accumulate across many loci to influence disease risk.\nThe odds ratio is not the same as relative risk, though the two are often conflated. For rare diseases (prevalence below roughly 10%), the odds ratio approximates the relative risk, but for common outcomes the distinction matters. An odds ratio of 2.0 does not mean the risk is doubled; it means the odds are doubled, and converting to absolute risk requires knowledge of baseline disease prevalence.\nLogistic regression shares the same covariate structure as linear regression, and the same concerns about population stratification apply. The residual error term disappears from the explicit model formulation because the outcome is binary, but the conceptual issue remains: most of the variation in disease liability is not captured by any single variant, and the per-variant contribution to overall risk discrimination is small.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS & Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-pgs.html#linkage-disequilibrium-and-association-signals",
    "href": "p1-ch03-pgs.html#linkage-disequilibrium-and-association-signals",
    "title": "3  GWAS & Polygenic Scores",
    "section": "3.2 Linkage Disequilibrium and Association Signals",
    "text": "3.2 Linkage Disequilibrium and Association Signals\nA GWAS result is not a direct readout of which variants cause a phenotype. The genome is not a collection of independent loci; nearby variants are correlated because they are inherited together on haplotypes that have not been broken up by recombination. This correlation structure, known as linkage disequilibrium, means that when a GWAS identifies an association signal at a particular variant, the true causal variant may lie anywhere within the surrounding correlated region. Distinguishing causal variants from their correlated neighbors is one of the central challenges in human genetics, and it has direct implications for how we interpret polygenic scores and, later, how we train and evaluate sequence-based models.\n\n3.2.1 Haplotype Structure and Recombination\nThe correlation between nearby variants arises from the mechanics of meiotic recombination. When chromosomes pair during meiosis, they exchange segments at crossover points, but these crossovers are relatively rare: on average, only one or two per chromosome arm per generation. Variants that are close together on a chromosome have a low probability of being separated by a crossover event, so they tend to be inherited together on the same haplotype across many generations. Variants that are farther apart, or on different chromosomes, are more likely to be shuffled independently. The result is a genome organized into regions of high correlation (sometimes called LD blocks) separated by recombination hotspots where correlation decays rapidly.\n\n\n3.2.2 Measuring Correlation: The r² Statistic\nThe standard measure of linkage disequilibrium between two biallelic variants is \\(r^2\\), the squared Pearson correlation between their genotype dosages in a population sample. Values of \\(r^2\\) near 1.0 indicate that the two variants are nearly perfect proxies for each other: knowing the genotype at one variant almost completely determines the genotype at the other. Values near zero indicate that the variants segregate independently. In practice, \\(r^2\\) decays with physical distance, but the rate of decay varies substantially across the genome depending on local recombination rates. Some regions harbor extended haplotypes where dozens or even hundreds of variants remain tightly correlated across tens or hundreds of kilobases; others show rapid LD decay within a few kilobases.\n\n\n3.2.3 Causal Versus Tag Variants\nThis correlation structure has a critical implication for GWAS interpretation. When we observe a significant association between a variant and a phenotype, we cannot immediately conclude that this variant is responsible for the phenotypic effect. The association may instead reflect the variant’s correlation with a true causal variant nearby. The tested variant is acting as a statistical proxy, or tag, for the underlying causal signal. In many GWAS loci, the variant with the smallest p-value is not the true causal variant; it is simply the most strongly associated tag in that LD block, often because it was genotyped or imputed with higher quality, or because its allele frequency happened to provide more statistical power.\nThis ambiguity motivates a terminological distinction that will recur throughout this book. A causal variant is one where changing the allele would, in the relevant biological context, change the phenotype. It exerts a direct mechanistic effect on some molecular process that ultimately influences the trait. A tag variant (or proxy variant) is statistically associated with the phenotype only because it is correlated with one or more causal variants through linkage disequilibrium. If we could somehow break the LD by examining a population with different haplotype structure, the tag variant would lose its association while the causal variant would retain it.\nIn practice, we rarely know with certainty which variants are causal. We therefore adopt two working categories. A putative causal variant is one with strong statistical evidence (such as a high posterior probability from fine-mapping) combined with supportive functional data (such as overlap with regulatory elements or experimental validation). A purely associative variant is one that achieves statistical significance in GWAS but where the weight of evidence suggests it is tagging underlying causal variation rather than contributing mechanistically. The boundary between these categories is not sharp, and many variants occupy an ambiguous middle ground. Polygenic scores, as typically constructed, do not distinguish between these categories at all: they assign weights based on statistical association, regardless of whether the variants are causal or purely associative. This limitation becomes important when we consider how scores transfer",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS & Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-pgs.html#from-association-signals-to-fine-mapping",
    "href": "p1-ch03-pgs.html#from-association-signals-to-fine-mapping",
    "title": "3  GWAS & Polygenic Scores",
    "section": "3.3 From Association Signals to Fine-Mapping",
    "text": "3.3 From Association Signals to Fine-Mapping\nGiven that GWAS associations typically implicate broad genomic regions rather than individual causal variants, a natural follow-up question is: which variant (or variants) within an associated locus is actually responsible for the phenotypic effect? Statistical fine-mapping attempts to answer this question by modeling the joint contribution of all variants in a region while accounting for their correlation structure. The output is not a single answer but a probability distribution over candidate variants, allowing us to quantify our uncertainty about which variants are causal. This probabilistic framing has important downstream consequences: it influences how we construct polygenic scores, how we prioritize variants for experimental follow-up, and how we evaluate whether deep learning models have learned biologically meaningful signals.\n\n3.3.1 Bayesian Fine-Mapping Framework\nThe conceptual shift from marginal to joint modeling lies at the heart of fine-mapping. Standard GWAS tests each variant independently, asking whether that variant’s genotype is associated with the phenotype after adjusting for covariates. This marginal approach ignores the fact that multiple variants in the same region share information through LD. If two variants are highly correlated, they will both show significant associations even if only one of them (or neither, if both are tagging a third variant) is truly causal. Fine-mapping methods instead fit models that consider all variants in a region simultaneously, asking which subset of variants best explains the observed association signal given the correlation structure among them.\nMost modern fine-mapping approaches adopt a Bayesian framework. For each variant in the region, the method estimates a posterior inclusion probability (PIP), which represents the probability that the variant is causal given the observed data and the assumed model. A variant with PIP of 0.95 has strong statistical evidence of causality; a variant with PIP of 0.05 is unlikely to be causal and is probably tagging nearby causal variation. These probabilities are not guarantees, and they depend on modeling assumptions that may not hold perfectly in practice, but they provide a principled quantification of uncertainty that point estimates from GWAS cannot offer.\nA related concept is the credible set, which is a minimal set of variants that together contain the causal variant (or variants) with high probability. A 95% credible set, for example, is constructed by ranking variants by their PIPs and including variants until their cumulative probability exceeds 0.95. In favorable cases where LD is limited and one variant stands out clearly, a credible set may contain only one or a few variants. In regions of extensive LD where many variants have similar statistical support, credible sets may contain dozens of candidates, reflecting genuine uncertainty about which is causal.\nFine-mapping methods differ in their underlying assumptions. Some assume a single causal variant per locus, which simplifies computation but may be unrealistic for complex loci harboring multiple independent signals. Others allow for multiple causal variants, at the cost of increased computational complexity and the need for additional regularization or prior assumptions. Methods also differ in their prior distributions on effect sizes: some assume that causal effect sizes follow a normal distribution, while others use spike-and-slab priors that place most probability mass on zero (reflecting the expectation that most variants are not causal) with a diffuse component for the minority that are. Finally, methods differ in their data requirements. Some operate directly on individual-level genotype and phenotype data, which provides the most information but requires access to protected datasets. Others operate on GWAS summary statistics combined with LD estimates from a reference panel, sacrificing some precision for the practical advantage of working with publicly available data (Pasaniuc and Price 2016).\n\n\n3.3.2 Applications and Multi-Ancestry Leverage\nThe outputs of fine-mapping feed into multiple downstream applications. Variants with high PIPs become candidates for experimental follow-up, whether through CRISPR perturbation, reporter assays, or other functional studies. Credible sets define the search space for identifying causal genes, often through integration with gene expression data or chromatin annotations. And PIP estimates can inform polygenic score construction: rather than weighting variants purely by their GWAS effect sizes, one can upweight variants with high posterior probability of causality and downweight those that appear to be tagging nearby causal variation. This reweighting does not dramatically improve predictive accuracy in most cases, but it can improve interpretability and, importantly, improve transferability across populations with different LD structures.\nMulti-ancestry data provides particular leverage for fine-mapping. Because LD patterns differ across populations (reflecting distinct demographic histories and recombination landscapes), a variant that is tightly linked to a causal variant in one population may be less correlated in another. When the same association signal appears across ancestries but the pattern of correlated variants differs, fine-mapping algorithms can triangulate more precisely on the likely causal variant. Large resources such as Open Targets Genetics integrate association signals, LD information, fine-mapping results, and functional annotations across multiple ancestries to prioritize likely causal variants and their target genes for thousands of traits (Mountjoy et al. 2021).\n\n\n3.3.3 Appropriate Expectations\nIt is important to maintain appropriate expectations about what fine-mapping can and cannot achieve. Statistical fine-mapping narrows the search space and quantifies uncertainty, but it does not definitively identify causal variants. Even a variant with PIP above 0.9 may not be causal if the model assumptions are violated or if the true causal variant was not included in the analysis (for example, because it is a rare variant not well captured by common variant arrays). Biological validation remains essential. What fine-mapping provides is a principled transition from the statement that something in this region is associated with the trait to the more refined statement that these few variants are the most plausible causal candidates, given current data and models.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS & Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-pgs.html#constructing-polygenic-scores",
    "href": "p1-ch03-pgs.html#constructing-polygenic-scores",
    "title": "3  GWAS & Polygenic Scores",
    "section": "3.4 Constructing Polygenic Scores",
    "text": "3.4 Constructing Polygenic Scores\nWith GWAS summary statistics in hand, the next step for many applications is to aggregate genetic effects across the genome into a single number that summarizes an individual’s genetic predisposition to a trait. This aggregation, known as a polygenic score, treats the genome as a linear sum of variant effects, weighting each variant by its estimated contribution from GWAS. The simplicity of this formulation is both its strength and its limitation: it enables straightforward computation and interpretation, but it also encodes assumptions about additivity and ignores the distinction between truly causal variants and those that are merely correlated with causal variants. Several methodological traditions have emerged for constructing polygenic scores, ranging from simple heuristics to sophisticated Bayesian models that explicitly account for linkage disequilibrium.\n\n\n\n\n\n\nNoteTerminology: PGS vs PRS\n\n\n\nBefore diving into the mechanics of genome-wide association studies and polygenic prediction, it is worth clarifying the terminology that pervades this literature. The field has accumulated several near-synonyms over the past two decades, reflecting both the evolution of methods and the expanding scope of applications from disease risk to quantitative traits. Establishing a consistent vocabulary here will prevent confusion in later chapters, where we build on these concepts to connect classical statistical genetics with deep learning approaches.\nThe literature uses several related terms:\n\nPolygenic risk score (PRS) – historically common, especially for disease endpoints\n\nPolygenic score (PGS) – more general, used for both disease risk and quantitative traits\n\nIn this book we use polygenic score (PGS) as the primary term, because many of the same methods are used for quantitative traits (e.g., height, LDL cholesterol), disease incidence (e.g., coronary artery disease), and intermediate molecular traits. When we cite work that uses “PRS,” we treat PRS and PGS as synonyms unless the distinction matters.\nThroughout, we will:\n\nUse PGS for the generic concept.\n\nUse PRS only when we are quoting or closely paraphrasing papers that do the same.\n\n\n\nThe mathematical form of a polygenic score is deceptively simple. For an individual \\(i\\), the score is computed as\n\\[\n\\text{PGS}_i = \\sum_{j=1}^M w_j \\, g_{ij},\n\\]\nwhere \\(g_{ij}\\) is the genotype dosage for individual \\(i\\) at variant \\(j\\) (typically coded as 0, 1, or 2 copies of the effect allele, or as a fractional imputed dosage), \\(w_j\\) is a weight representing the estimated per-allele effect of variant \\(j\\), and the sum runs over \\(M\\) variants included in the score. The weight \\(w_j\\) is usually derived from GWAS effect size estimates, though the precise derivation varies across methods. This formulation embodies a linear additive model: each variant contributes independently and additively to the score, with no interactions between variants and no nonlinear transformations of genotype.\nThe challenge in constructing a useful polygenic score lies in choosing which variants to include and how to set their weights. Raw GWAS effect size estimates are noisy, particularly for variants that barely reach significance or for variants in regions of extensive LD where the signal is spread across many correlated markers. Simply summing all genome-wide variants weighted by their marginal effect estimates would produce a score dominated by noise. The various methods for PGS construction can be understood as different strategies for filtering variants, shrinking effect estimates, and accounting for correlation structure.\n\n3.4.1 Clumping and Thresholding\nThe simplest approach, known as clumping and thresholding (often abbreviated C+T), applies two sequential filters to the GWAS results (Choi, Mak, and O’Reilly 2020). First, variants are filtered by p-value: only those exceeding some significance threshold are retained. This threshold might be the conventional genome-wide significance level of \\(5 \\times 10^{-8}\\), or it might be more permissive (such as \\(10^{-4}\\), \\(10^{-2}\\), or even 1.0 to include all variants) depending on the application and the polygenicity of the trait. Second, within each genomic region, variants are “clumped” by LD: the variant with the smallest p-value is retained as the index variant, and all other variants within a specified window that exceed an \\(r^2\\) threshold (commonly 0.1 or 0.2) are removed. The surviving variants are then weighted by their GWAS effect size estimates, and the score is computed as the weighted sum.\nClumping and thresholding has the virtues of simplicity and computational efficiency. It can be implemented using only summary statistics and a reference panel for LD estimation, without access to individual-level data. It produces sparse scores with interpretable variant sets. And it remains competitive with more sophisticated methods for some traits, particularly those with large-effect variants that are well captured by stringent significance thresholds.\nThe limitations of C+T stem from its heuristic nature. By retaining only one variant per LD block, it discards information: if multiple variants in a region independently contribute to the trait (allelic heterogeneity), or if the true causal variant was not the one with the smallest p-value, the score will be suboptimal. The method treats all retained variants as equally reliable, making no distinction between a variant that clearly stands alone and one that narrowly beat several nearly equivalent neighbors. And the performance depends sensitively on the choice of p-value threshold and LD parameters, which are typically tuned by grid search in a validation dataset, introducing potential overfitting and limiting generalizability.\n\n\n3.4.2 LD-Aware Bayesian Methods\nA more principled approach models the joint distribution of effect sizes across all variants while explicitly accounting for their correlation structure. The family of LD-aware Bayesian methods, exemplified by LDpred (Vilhjálmsson et al. 2015), PRS-CS, SBayesR, and lassosum, shares a common conceptual framework: treat the true effect sizes as random variables drawn from some prior distribution, observe the noisy GWAS estimates, and compute posterior effect size estimates that optimally combine prior beliefs with observed data given the LD structure.\nLDpred, for example, assumes that a fraction \\(p\\) of variants have nonzero effects drawn from a normal distribution, while the remaining \\(1-p\\) have exactly zero effect. Given GWAS summary statistics and an LD reference panel, the method computes posterior mean effect sizes by solving a system of equations that propagates information across correlated variants. Variants with strong marginal associations that are uncorrelated with other strong signals receive weights close to their GWAS estimates; variants whose associations can be explained by LD with nearby signals are shrunk toward zero. The polygenicity parameter \\(p\\) can be estimated from the data or specified based on prior knowledge about the trait.\nOther methods in this family make different modeling choices. PRS-CS uses a continuous shrinkage prior that adapts to local genetic architecture. SBayesR employs a mixture of normal distributions with different variances, allowing for a spectrum of effect sizes from large to small. Lassosum applies L1 penalization, which induces sparsity and can be computed efficiently. Despite these differences, all of these methods share the goal of producing effect size estimates that account for LD, shrink noisy estimates appropriately, and can leverage genome-wide information rather than treating each locus independently.\nCompared to clumping and thresholding, LD-aware Bayesian methods generally achieve modestly higher predictive accuracy, particularly for highly polygenic traits where thousands of variants contribute small effects. They allow multiple correlated variants to share signal rather than forcing a winner-take-all selection. And they provide a coherent probabilistic framework that can, in principle, be extended to incorporate additional information such as functional annotations or multi-ancestry data. The cost is increased computational complexity and the need for careful specification of priors, LD reference panels, and other modeling choices.\n\n\n3.4.3 Fine-Mapping-Informed Polygenic Scores\nThe methods described above derive weights from GWAS association statistics, which reflect a mixture of causal effects and LD-induced correlations. An alternative strategy incorporates fine-mapping results to emphasize variants that are more likely to be causal. If fine-mapping has produced posterior inclusion probabilities for variants across the genome, these probabilities can be integrated into PGS construction in several ways.\nOne approach filters variants by PIP, including only those above some threshold (such as 0.1 or 0.5) in the score. This produces a sparse score concentrated on high-confidence causal candidates. A related approach weights variants by their PIP, setting \\(w_j \\propto \\text{PIP}_j \\times \\hat\\beta_j\\) so that variants with low probability of causality contribute less even if their marginal associations are strong. Yet another approach operates at the level of credible sets: for each fine-mapped locus, select one representative variant (typically the one with highest PIP) or include all variants in the credible set with PIP-proportional weights.\nThese fine-mapping-informed strategies aim to shift the score away from purely associative variants toward putative causal variants. The practical benefits for prediction accuracy are often modest, since even tag variants carry predictive information as long as LD patterns are consistent between training and test populations. The more compelling advantages are interpretability and transferability. A score built from likely causal variants is easier to connect to biological mechanisms and target genes. And because causal variants should have consistent effects across populations (unlike tag variants, whose correlations with causal variants may differ), fine-mapping-informed scores may transfer better across ancestries, though this remains an active area of investigation.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS & Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-pgs.html#interpreting-polygenic-scores",
    "href": "p1-ch03-pgs.html#interpreting-polygenic-scores",
    "title": "3  GWAS & Polygenic Scores",
    "section": "3.5 Interpreting Polygenic Scores",
    "text": "3.5 Interpreting Polygenic Scores\nOnce a polygenic score has been computed for an individual, the question becomes: what does it mean? A raw score, expressed as a sum of weighted genotypes, has no inherent clinical or biological interpretation. Converting this number into something actionable, whether a relative risk compared to the population or an absolute probability of disease, requires additional modeling and calibration. Moreover, the interpretation of a polygenic score depends critically on the population in which it was derived and the population in which it is applied. Scores developed in one ancestry group often perform poorly in others, raising both scientific and ethical questions about equitable deployment.\n\n3.5.1 Relative versus Absolute Risk\nPolygenic scores are most naturally interpreted in relative terms. The raw score itself is an arbitrary number whose magnitude depends on how many variants are included, how effects are scaled, and various normalization choices. What matters is where an individual falls within the distribution of scores in a reference population. A person in the 95th percentile has a higher genetic predisposition than 95% of the reference population; a person in the 10th percentile has lower predisposition than 90% of the population. This percentile framing, or equivalently the number of standard deviations from the population mean, provides a natural way to communicate relative genetic risk.\nThe clinical literature often emphasizes tail comparisons: individuals in the top 1% or 5% of the PGS distribution compared to those in the middle or bottom of the distribution. For many common diseases, those in the top few percentiles have odds ratios of 2 to 5 (or occasionally higher) compared to population average, meaning their odds of disease are several-fold elevated. These tail effects can be clinically meaningful, particularly for diseases where early intervention or enhanced screening might benefit high-risk individuals. However, most of the population falls in the broad middle of the distribution, where the PGS provides only modest discrimination.\nTranslating a polygenic score into absolute risk, such as a statement that an individual’s 10-year probability of developing a disease is 15%, requires substantially more modeling. The PGS alone provides only relative ranking; converting to absolute probability requires knowledge of the baseline incidence rate in the relevant population, which varies by age, sex, ancestry, calendar time, and other factors. It also requires integrating the PGS with non-genetic risk factors (clinical measurements, family history, lifestyle variables) that independently contribute to disease risk. Finally, the resulting risk model must be calibrated to ensure that predicted probabilities match observed outcomes in relevant validation cohorts. A model that assigns 20% risk to a group should see approximately 20% of that group develop the disease; miscalibration undermines clinical utility and patient trust. We return to these issues of risk modeling, calibration, and clinical decision thresholds in Chapter Chapter 18, where we discuss the integration of genomic and clinical data for patient stratification.\n\n\n3.5.2 Ancestry, Linkage Disequilibrium, and Transferability\nA polygenic score derived in one population often performs substantially worse when applied to another, and this transferability problem is one of the most pressing challenges in the field. The decline in predictive accuracy is not uniform: scores developed in European-ancestry cohorts (which dominate the GWAS literature due to historical recruitment patterns) typically lose 20-80% of their predictive power when applied to African-ancestry or East Asian-ancestry populations, with the magnitude of decline varying by trait and methodology.\nSeveral factors contribute to this transferability gap. The most fundamental is differences in linkage disequilibrium structure across populations. Human populations have distinct demographic histories involving bottlenecks, expansions, and admixture events that have shaped their haplotype patterns. A variant that tags a causal variant through tight LD in European populations may be only weakly correlated with that same causal variant in African populations, where LD blocks tend to be shorter due to greater ancestral diversity. If a PGS relies heavily on such tag variants rather than causal variants, it will perform well only in populations with similar LD structure.\nAllele frequency differences compound this problem. A variant that is common in one population may be rare or absent in another, and vice versa. GWAS statistical power depends on allele frequency, so the set of variants that reach significance (and thus enter into PGS) is shaped by the allele frequency spectrum of the discovery population. Effect size estimates are also noisier for rarer variants, so weights learned in one population may not transfer accurately even for shared variants.\nBeyond these genetic factors, environmental and gene-by-environment interactions may differ across populations. The phenotypic consequence of a genetic variant can depend on diet, pathogen exposure, healthcare access, and countless other environmental variables that vary geographically and socioeconomically. A variant that increases cardiovascular risk in the context of a Western diet may have different effects in other dietary contexts. These interactions are rarely modeled explicitly in standard GWAS and PGS frameworks, contributing to transferability failures that cannot be explained by LD and allele frequency differences alone.\nLarge biobank efforts that recruit diverse populations, such as the Million Veteran Program, have brought these issues into sharp focus (Verma et al. 2024). Studies in these cohorts consistently find that PGS developed in European-ancestry samples explain less phenotypic variance in other ancestry groups, sometimes dramatically so. This disparity has direct implications for health equity: if genomic medicine tools work well only for populations that are already overrepresented in research, their clinical deployment risks widening rather than narrowing health disparities.\nSeveral strategies can mitigate transferability problems. Multi-ancestry GWAS meta-analyses increase power to detect variants with consistent effects across populations while down-weighting ancestry-specific signals. Fine-mapping, as discussed earlier, can identify putative causal variants that should have consistent effects regardless of local LD patterns. Functional annotations from resources like ENCODE and GTEx (described in Chapter Chapter 2) can prioritize variants in regulatory regions with evidence of molecular function, on the theory that biologically active variants are more likely to be causal and thus more likely to transfer. And methods that explicitly model LD differences across populations, or that leverage admixed individuals who carry haplotypes from multiple ancestral backgrounds, can improve cross-population prediction. None of these approaches fully solves the transferability problem, but together they point toward a future where PGS reflect shared human biology rather than ancestry-specific statistical artifacts.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS & Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-pgs.html#limitations-of-gwas-and-pgs-and-the-case-for-mechanistic-models",
    "href": "p1-ch03-pgs.html#limitations-of-gwas-and-pgs-and-the-case-for-mechanistic-models",
    "title": "3  GWAS & Polygenic Scores",
    "section": "3.6 Limitations of GWAS and PGS, and the Case for Mechanistic Models",
    "text": "3.6 Limitations of GWAS and PGS, and the Case for Mechanistic Models\nDespite their success in identifying thousands of trait-associated loci, GWAS and polygenic scores have fundamental limitations that motivate the rest of this book. They are tools of statistical association, not biological mechanism. They rely on linkage disequilibrium patterns that vary across populations. They are dominated by noncoding variants whose functional effects are difficult to interpret. And they provide no information about how genetic risk might interact with environment, treatment, or disease stage. These limitations are not merely technical inconveniences; they represent gaps in our understanding that sequence-based deep learning models are uniquely positioned to address. The chapters that follow will show how models that learn directly from DNA sequence can complement and extend the classical GWAS framework, moving from association toward mechanism.\n\n3.6.1 Achievements and the Clinical Adoption Gap\nThe achievements of GWAS and polygenic scores should not be understated. Over the past two decades, GWAS has produced a systematic catalog of genetic associations for thousands of human traits and diseases, transforming our understanding of the genetic architecture of complex phenotypes. For some traits, particularly highly heritable quantitative phenotypes like height and lipid levels, polygenic scores explain a meaningful fraction of phenotypic variance and can identify individuals at substantially elevated risk. The methodology is mature, the computational pipelines are well-established, and GWAS summary statistics are increasingly available as public resources that enable secondary analyses without access to protected individual-level data.\nYet despite these successes, polygenic scores have seen limited adoption in routine clinical practice. This gap between research promise and clinical implementation reflects the accumulated weight of the limitations discussed throughout this chapter. Clinicians and healthcare systems have proven cautious about integrating PGS into care pathways, and for understandable reasons: the scores provide probabilistic stratification rather than actionable diagnosis, their performance varies across the diverse patient populations that healthcare systems serve, and the path from a percentile ranking to a clinical decision remains unclear for most conditions. The enthusiasm that greeted early PGS publications has given way to a more sober assessment of what these tools can and cannot deliver in their current form.\n\n\n3.6.2 Association Without Mechanism\nThe first fundamental limitation is that GWAS and PGS operate at the level of statistical association rather than biological mechanism. A polygenic score tells us that certain variants are correlated with disease risk in the populations studied, but it does not tell us why. It does not identify which gene is affected, what molecular pathway is perturbed, or how the genetic signal might interact with therapeutic interventions. Two variants with identical weights in a PGS may have entirely different biological stories: one might directly disrupt a protein coding sequence, while another might be a tag variant in weak LD with an unknown regulatory element. This mechanistic opacity limits both scientific interpretation and clinical utility. Without understanding mechanism, we cannot easily move from risk prediction to risk modification.\n\n\n3.6.3 Population Transferability\nThe second limitation is the dependence on linkage disequilibrium patterns and the resulting problems with portability across populations. As discussed in the previous section, PGS developed in European-ancestry cohorts often perform substantially worse in other ancestry groups. This is not merely a statistical inconvenience; it raises serious questions about equitable deployment. A healthcare system that offers PGS-based risk stratification to patients will systematically provide less accurate information to patients from underrepresented populations. The fact that most GWAS have been conducted in European-ancestry samples is a historical artifact of recruitment patterns and funding priorities, but its consequences propagate forward into any clinical tool built on those data.\n\n\n3.6.4 The Noncoding Variant Challenge\nThe third limitation concerns the noncoding nature of most GWAS signals. The majority of trait-associated variants fall outside protein-coding regions, in the vast genomic territory devoted to gene regulation, chromatin organization, and functions we do not yet fully understand. Interpreting these noncoding variants is far more difficult than interpreting coding variants. A missense mutation that changes an amino acid can be evaluated with structural models, evolutionary conservation, and biochemical assays. A variant in an intergenic region might affect an enhancer active only in a specific cell type at a specific developmental stage, or it might have no functional consequence at all and simply tag a causal variant nearby. Understanding noncoding variation requires models of regulatory grammar that traditional GWAS does not provide.\n\n\n3.6.5 Static Scores in a Dynamic Context\nThe fourth limitation is the static nature of conventional PGS. The score is computed once from germline genotypes and treated as a fixed quantity, but disease risk is not static. It changes with age, accumulates through environmental exposures, responds to medications, and evolves through disease progression. A polygenic score for cardiovascular disease does not account for whether the patient is taking statins, has changed their diet, or has already experienced a myocardial infarction. Integrating genetic risk with the rich longitudinal data available in electronic health records, including laboratory values, imaging, medications, and clinical notes, is essential for genomic prediction that is truly useful in clinical contexts.\n\n\n3.6.6 Missing Heritability\nA foundational limitation predates the transferability concerns discussed above: the gap between heritability estimated from family studies and the variance explained by GWAS-identified variants. Twin and family studies consistently estimate that common complex traits have substantial heritability, often 40% to 80% for traits like height, BMI, and psychiatric conditions. Yet early GWAS, despite identifying dozens or hundreds of associated loci, could account for only a small fraction of this heritability. This discrepancy, termed the “missing heritability” problem, prompted extensive methodological development and debate (manolio_missing_2009?).\nSeveral factors contribute to the gap. Common variants with effect sizes too small to reach genome-wide significance individually may collectively explain substantial heritability, a possibility confirmed by methods that estimate heritability from all SNPs rather than just significant hits (yang_common_2010?). Rare variants, poorly tagged by genotyping arrays, likely contribute additional signal. Structural variants, gene-gene interactions, and gene-environment interactions are largely invisible to standard GWAS designs. And some portion of twin-study heritability may reflect shared environment or assortative mating rather than additive genetic effects. While methodological advances have closed much of the gap for some traits, the phenomenon illustrates a core limitation: GWAS-based approaches capture only a subset of genetic architecture, and the portion they miss may be precisely where mechanistic insight is most needed.\n\n\n3.6.7 Toward Mechanistic Models\nThese limitations collectively motivate the approaches that form the core of this book. Sequence-based deep learning models offer a path from association toward mechanism by learning the relationship between DNA sequence and molecular function directly. Convolutional neural networks trained on regulatory assay data, such as DeepSEA and its successors, can predict how sequence changes affect transcription factor binding, chromatin accessibility, and gene expression (Zhou and Troyanskaya 2015; Zhou et al. 2018). Splicing models can predict how variants affect pre-mRNA processing (Chapter Chapter 7). These predictions are mechanistic in a way that GWAS effect sizes are not: they make claims about molecular function that can be tested experimentally.\nVariant effect predictions from deep learning models can complement GWAS and fine-mapping to prioritize putative causal variants at trait-associated loci. If a fine-mapped credible set contains ten variants with similar posterior probabilities, but only one of them is predicted to substantially alter enhancer activity in a disease-relevant cell type, that variant becomes a higher-priority candidate for experimental follow-up. Resources like Open Targets Genetics already integrate such predictions alongside association statistics and fine-mapping results (Mountjoy et al. 2021).\nBeyond prioritization, mechanistic predictions can be incorporated into polygenic score frameworks themselves. Rather than weighting variants purely by their GWAS associations, one can use predicted functional effects as priors, features, or reweighting factors. A variant predicted to disrupt a splice site or abolish transcription factor binding might receive greater weight than a variant with similar association statistics but no predicted functional consequence. This integration of statistical association with mechanistic prediction represents a promising direction for building scores that are more interpretable, more transferable across populations, and potentially more amenable to therapeutic intervention.\nThe genomic foundation models discussed in Part IV extend these ideas further. By training on massive corpora of sequence data with self-supervised objectives, these models learn representations that capture evolutionary constraints, regulatory syntax, and sequence-function relationships at a scale and generality that task-specific models cannot match. The goal is not to replace GWAS but to complement it: to provide the mechanistic context that association studies lack, to enable predictions for rare variants and understudied populations, and ultimately to close the gap between statistical genetics and biological understanding.\nIn later chapters we will see how multi-omics integration (Chapter Chapter 14) and clinical modeling (Chapter Chapter 18) build on these foundations to combine genetic, molecular, and clinical data for robust and equitable genomic prediction. For now, the key takeaway is that polygenic scores, as powerful as they are for certain applications, remain fundamentally associative tools. They summarize correlation patterns in training populations without capturing the biological mechanisms that generate those patterns. Understanding LD, fine-mapping, and the distinction between causal and purely associative variants is essential background not only for interpreting classical PGS but also for appreciating what sequence-based deep learning models aim to achieve and how they might eventually transform genomic medicine.\n\n\n\n\nChoi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O’Reilly. 2020. “[PRS] Tutorial: A Guide to Performing Polygenic Risk Score Analyses.” Nature Protocols 15 (9): 2759–72. https://doi.org/10.1038/s41596-020-0353-1.\n\n\nMarees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan, Emmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018. “[GWAS] A Tutorial on Conducting Genome-Wide Association Studies: Quality Control and Statistical Analysis.” International Journal of Methods in Psychiatric Research 27 (2): e1608. https://doi.org/10.1002/mpr.1608.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy Schwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021. “An Open Approach to Systematically Prioritize Causal Variants and Genes at All Published Human GWAS Trait-Associated Loci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nPasaniuc, Bogdan, and Alkes L. Price. 2016. “Dissecting the Genetics of Complex Traits Using Summary Association Statistics.” Nature Reviews Genetics 18 (2): 117–27. https://doi.org/10.1038/nrg.2016.142.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006. “Population Structure and Eigenanalysis.” PLOS Genetics 2 (12): e190. https://doi.org/10.1371/journal.pgen.0020190.\n\n\nVerma, Anurag, Jennifer E. Huffman, Alex Rodriguez, Mitchell Conery, Molei Liu, Yuk-Lam Ho, Youngdae Kim, et al. 2024. “Diversity and Scale: Genetic Architecture of 2068 Traits in the VA Million Veteran Program.” Science 385 (6706): eadj1182. https://doi.org/10.1126/science.adj1182.\n\n\nVilhjálmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev, Sara Lindström, Stephan Ripke, Giulio Genovese, et al. 2015. “Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores.” American Journal of Human Genetics 97 (4): 576–92. https://doi.org/10.1016/j.ajhg.2015.09.001.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS & Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch04-cadd.html",
    "href": "p1-ch04-cadd.html",
    "title": "4  Deleteriousness Scores",
    "section": "",
    "text": "4.1 The Variant Prioritization Challenge\nA typical human genome contains approximately four to five million genetic variants relative to the reference assembly. The vast majority of these are functionally neutral, representing the accumulated diversity of human evolution and population history. For any individual with a suspected genetic condition, the central interpretive challenge is to identify the handful of variants that plausibly contribute to disease from this enormous background of benign variation.\nThe data resources surveyed in Chapter 2 provide multiple complementary views of variant function, each with distinct strengths and limitations. Population frequency databases such as gnomAD reveal which variants survive in large cohorts of ostensibly healthy individuals, offering a powerful filter for identifying rare, potentially deleterious alleles (“The Genome Aggregation Database (gnomAD)” n.d.). Functional genomics consortia including ENCODE and the Roadmap Epigenomics Project indicate which genomic regions show evidence of biochemical activity across diverse cell types and developmental contexts. Clinical databases such as ClinVar and HGMD collect expert-curated variant classifications drawn from case reports and diagnostic laboratories, providing ground truth labels for known pathogenic and benign variants.\nEach of these sources is partial in important ways. Population databases are dominated by common variants, which are mostly tolerated by virtue of their high frequency. Functional genomics data is inherently noisy and often context-specific: a region active in liver hepatocytes may be quiescent in neurons, and vice versa. Clinical databases are sparse and heavily biased toward well-studied genes and variant types, leaving vast swaths of the genome without reliable clinical annotations. Moreover, the annotation density varies dramatically across the genome: protein-coding exons are densely labeled relative to deep intronic and intergenic sequences.\nBefore deep learning, variant effect predictors typically tackled this problem by focusing on one narrow signal. Conservation-based methods such as phyloP and GERP score each position according to its evolutionary constraint across multi-species alignments, under the logic that positions conserved over hundreds of millions of years are likely functionally important (Siepel et al. 2005; Davydov et al. 2010). Protein-level tools such as SIFT and PolyPhen predict the impact of amino acid substitutions based on sequence homology, physicochemical properties, and structural features (Ng and Henikoff 2003; Adzhubei et al. 2010). Positional annotations capture simple features like distance to splice sites or proximity to known regulatory elements. Each of these approaches captures a real biological signal, but each is also incomplete: conservation scores miss recently evolved functional elements, protein-level tools are blind to non-coding variants, and positional annotations lack the resolution to distinguish causal variants from linked neutral neighbors.\nCombined Annotation-Dependent Depletion (CADD) represented a fundamental shift in this landscape (Rentzsch et al. 2019). Rather than relying on a single predictive signal, CADD defined a general framework for genome-wide variant prioritization that integrates dozens of heterogeneous annotations and uses evolutionary depletion as a proxy training label. The key insight was to avoid training directly on small sets of known pathogenic versus benign variants, which are scarce and biased toward certain genes and variant types. Instead, CADD contrasts variants that have survived purifying selection in the human lineage with matched simulated variants that could have occurred but did not. This evolutionary proxy strategy yields an enormous training set, enables genome-wide coverage, and produces scores that generalize across coding and non-coding regions alike.\nThis chapter focuses on the CADD framework because it establishes design patterns that recur throughout the deep learning models covered in subsequent chapters: proxy labels derived from evolutionary signals, large-scale training on millions of examples, integration of diverse features into unified scores, and genome-wide precomputation for downstream reuse.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch04-cadd.html#the-evolutionary-proxy-training-strategy",
    "href": "p1-ch04-cadd.html#the-evolutionary-proxy-training-strategy",
    "title": "4  Deleteriousness Scores",
    "section": "4.2 The Evolutionary Proxy Training Strategy",
    "text": "4.2 The Evolutionary Proxy Training Strategy\nCADD’s most important conceptual contribution was to reframe variant effect prediction as a large-scale machine learning problem with labels derived from evolutionary signal rather than clinical curation. The scarcity and bias of known pathogenic variants has long limited supervised approaches to variant interpretation. ClinVar and similar databases contain tens of thousands of labeled variants, but these are concentrated in a small fraction of genes, skewed toward certain variant types (nonsense, frameshift, canonical splice site), and subject to ascertainment bias from clinical referral patterns. Training directly on such labels tends to produce models that perform well on variants similar to the training set but generalize poorly to the broader genome.\nCADD sidesteps this problem by constructing a synthetic classification task: can a model distinguish variants that are actually observed in human populations from matched simulated variants that have not survived evolution? The observed variants serve as proxies for tolerated alleles, while the simulated variants serve as proxies for potentially deleterious alleles. This framing yields a training set of tens of millions of examples, far exceeding what clinical curation can provide, and covers the full spectrum of variant types and genomic contexts.\n\n4.2.1 Proxy-Neutral Variants\nThe proxy-neutral class consists of variants that are actually observed in human populations. CADD draws these from large sequencing datasets such as the 1000 Genomes Project and early gnomAD-like resources, including both single nucleotide variants (SNVs) and short insertions and deletions (indels). The selection criteria typically favor variants with high derived allele frequency, reflecting the assumption that alleles which have drifted to appreciable frequency in human populations are unlikely to be strongly deleterious over recent evolutionary timescales.\nThis is not a perfect proxy for benign variants. Some observed alleles are genuinely pathogenic, particularly those with incomplete penetrance, late onset, or context-dependent effects. Variants under weak negative selection may persist at low to moderate frequencies for thousands of generations before eventual elimination. Population-specific bottlenecks and founder effects can elevate the frequency of otherwise deleterious alleles in particular groups. Despite these caveats, the proxy-neutral class is, on average, substantially enriched for tolerated alleles relative to a random sample of possible mutations. The key statistical insight is that systematic enrichment, even if imperfect at the individual variant level, provides a useful training signal when aggregated across millions of examples.\n\n\n4.2.2 Proxy-Deleterious Variants\nThe proxy-deleterious class is constructed by simulating mutations across the genome according to realistic mutational processes. The simulation matches local sequence context, typically using trinucleotide frequencies to capture the strong dependence of mutation rates on the identity of flanking bases. CpG dinucleotides, for example, have elevated mutation rates due to spontaneous deamination of methylated cytosines, and the simulation accounts for this by generating more CpG transitions in the proxy-deleterious set. Regional variation in mutation rates, driven by factors including replication timing, chromatin state, and local sequence composition, is similarly incorporated by scaling mutation counts within genomic windows (Rentzsch et al. 2019; Schubach et al. 2024).\nThe logic underlying this construction is subtle but powerful. Simulated variants represent changes that could plausibly occur under human mutational processes but are generally not observed at high frequency in population databases. Many of these simulated variants would in fact be neutral if they were to arise; the simulation makes no attempt to identify truly deleterious mutations at the individual level. However, the proxy-deleterious class as a whole is enriched for alleles that are disfavored by selection, because the set of possible mutations includes many that disrupt conserved elements, alter protein function, or perturb regulatory sequences. By contrasting this set with the proxy-neutral class, CADD learns to recognize the annotation signatures that distinguish variants under purifying selection from those that have been tolerated.\n\n\n4.2.3 Training Objective\nWith proxy-neutral and proxy-deleterious classes in hand, CADD trains a binary classifier to distinguish between them. The input to this classifier is a feature vector describing each variant, encompassing the diverse annotations surveyed in the following section: gene model features, conservation scores, epigenetic signals, protein-level predictions, and more. The label is simply whether the variant was simulated (proxy-deleterious) or observed (proxy-neutral). The objective is to learn a scoring function that assigns higher values to simulated variants, reflecting their predicted deleteriousness.\nEarly CADD versions employed linear support vector machines trained on approximately 30 million simulated versus observed variants with 63 annotation features plus selected interaction terms (Rentzsch et al. 2019). This relatively simple architecture was sufficient to capture the main structure of the problem, in part because the features themselves encode substantial biological knowledge. Later versions, including CADD v1.7, employ logistic regression-style models with expanded feature sets, retaining the same fundamental paradigm of contrasting simulated and observed variants while accommodating richer annotations (Schubach et al. 2024).\nThis evolutionary depletion framework anticipates several themes that recur in modern self-supervised learning. The labels are not clinical ground truth but derived from a proxy signal (survival under selection) that is abundant and covers the entire genome. The training set is extremely large, enabling complex decision boundaries and robust generalization. The resulting scores are precomputed genome-wide and reused for diverse downstream tasks, from rare disease gene discovery to variant filtration pipelines to evaluation baselines for newer models. In this sense, CADD can be understood as an early example of pretraining on a large-scale proxy task followed by transfer to clinical applications, a pattern that defines modern foundation models.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch04-cadd.html#integration-of-diverse-annotations",
    "href": "p1-ch04-cadd.html#integration-of-diverse-annotations",
    "title": "4  Deleteriousness Scores",
    "section": "4.3 Integration of Diverse Annotations",
    "text": "4.3 Integration of Diverse Annotations\nCADD’s second conceptual pillar is the integration of many weak, noisy annotations into a single composite score. Where earlier variant effect predictors typically relied on one or a few signals, CADD combines more than 60 features in its original incarnation and substantially more in version 1.7 (Rentzsch et al. 2019; Schubach et al. 2024). This integrative approach recognizes that no single annotation captures the full complexity of variant function. Conservation scores miss recently evolved functional elements. Protein-level predictions are uninformative for non-coding variants. Regulatory annotations are noisy and incomplete. By learning optimal weights for combining these diverse signals, CADD achieves performance that exceeds any individual component.\nBecause Chapter 2 already surveys the underlying data resources in detail, this section focuses on the categories of features and how they function within the CADD framework. For specifics on individual databases such as ENCODE, Roadmap, gnomAD, and ClinVar, readers should consult the earlier chapter.\n\n4.3.1 Gene Model Annotations\nGene model annotations describe the local transcript and coding context of each variant. The most fundamental is the predicted sequence consequence: whether a variant is synonymous, missense, nonsense, frameshift, splice-site disrupting, or located in untranslated or intronic regions. These consequence categories capture qualitatively different modes of disruption, from silent changes that preserve protein sequence to truncating mutations that eliminate large portions of the gene product.\nBeyond simple consequence, CADD incorporates positional features such as distance to exon-intron boundaries and proximity to canonical splice sites. Variants near splice junctions have elevated potential to disrupt splicing even if they do not directly alter the canonical GT-AG dinucleotides. Distance to the start and stop codons provides additional context, as does the position within the reading frame for coding variants.\nGene-level attributes further enrich the annotation set. Constraint metrics derived from population data, such as the probability of loss-of-function intolerance (pLI) and the loss-of-function observed/expected upper bound fraction (LOEUF), quantify how tolerant each gene is to damaging variation (“The Genome Aggregation Database (gnomAD)” n.d.). Variants in highly constrained genes receive elevated deleteriousness scores, reflecting the empirical observation that such genes are enriched for disease associations. Known disease gene status from OMIM and similar resources provides complementary information about genes with established pathogenic roles.\nThese gene model features allow CADD to make biologically meaningful distinctions. A synonymous variant in a tolerant gene with high LOEUF receives a very different score than a truncating variant in a highly constrained developmental regulator. The model learns these distinctions from the differential representation of variant types across the proxy-neutral and proxy-deleterious training classes.\n\n\n4.3.2 Conservation and Constraint\nEvolutionary conservation provides some of the strongest signals for variant deleteriousness, particularly in non-coding regions where direct functional labels are scarce. CADD incorporates multiple conservation metrics computed from multi-species alignments spanning mammals, vertebrates, and more distant taxa.\nBase-level conservation scores such as GERP (Genomic Evolutionary Rate Profiling) and phyloP quantify the deviation of observed substitution rates from neutral expectation (Davydov et al. 2010; Siepel et al. 2005). Positions with strong negative GERP scores show substitution rates far below neutral, indicating purifying selection has maintained these bases across tens or hundreds of millions of years of evolution. PhastCons provides a complementary view by identifying conserved elements, contiguous regions with elevated conservation that likely correspond to functional units. PhyloP scores individual positions without the smoothing implicit in element-based approaches, capturing both conservation (slow evolution) and acceleration (fast evolution) relative to neutral models.\nRegional measures of constraint complement base-level scores. These capture broader patterns of evolutionary pressure that may not be evident at single positions but emerge when considering larger windows. Mutation rate estimates, derived from substitution patterns in presumably neutral regions such as ancestral repeats, allow the model to distinguish true constraint from low mutation rate.\nConservation features are particularly valuable for non-coding variant interpretation, where biochemical annotations are often incomplete or absent. A deeply conserved non-coding position is likely functional even if no enhancer or promoter annotation overlaps it. Conversely, lack of conservation provides evidence (though not proof) that a position is tolerant to variation.\n\n\n4.3.3 Epigenetic and Regulatory Activity\nCADD incorporates regulatory annotations derived from functional genomics assays to capture the chromatin and regulatory context of each variant. These features draw primarily from large-scale consortium efforts including ENCODE and the Roadmap Epigenomics Project, which have profiled chromatin accessibility, histone modifications, and transcription factor binding across hundreds of cell types and tissues.\nDNase I hypersensitivity and ATAC-seq peaks identify regions of open chromatin, marking active regulatory elements including promoters, enhancers, and insulators. ChIP-seq signals for histone modifications provide additional context: H3K4me3 marks active promoters, H3K27ac marks active enhancers, H3K36me3 spans transcribed gene bodies, and H3K27me3 marks polycomb-repressed regions. Transcription factor ChIP-seq directly identifies binding sites for specific regulators, though coverage varies considerably across factors and cell types.\nChromatin state segmentations integrate multiple histone marks and accessibility signals into discrete functional categories. These segmentations, produced by algorithms such as ChromHMM, assign each genomic position to states like “active promoter,” “strong enhancer,” “weak enhancer,” “transcribed region,” or “heterochromatin.” By including these aggregate states alongside raw signals, CADD can capture combinatorial patterns that distinguish functional regulatory elements from background.\nThese epigenomic features help prioritize non-coding variants that disrupt active regulatory regions. A variant falling within an active enhancer marked by H3K27ac and DNase hypersensitivity in a relevant tissue receives elevated deleteriousness scores, even if its conservation is modest. The tissue specificity of regulatory annotations presents both opportunity and challenge: a variant may be highly consequential in one cellular context while neutral in another, and CADD’s genome-wide scores necessarily average across this heterogeneity.\n\n\n4.3.4 Additional Features\nBeyond the major annotation categories, CADD incorporates features capturing local sequence context and genomic architecture. GC content and CpG dinucleotide density affect mutation rates, chromatin structure, and gene regulation. Segmental duplications and low-complexity regions flag positions where mapping uncertainty may confound variant calls or where duplicated sequences complicate interpretation. Distance to telomeres and centromeres provides coarse chromosomal context.\nFor coding variants, CADD includes protein-level features beyond simple consequence. Amino acid physicochemical properties, including size, charge, hydrophobicity, and polarity, inform predictions about whether a substitution is conservative or radical. Legacy variant effect scores such as SIFT and PolyPhen are included as features, allowing CADD to leverage decades of prior work on protein variant interpretation (Ng and Henikoff 2003; Adzhubei et al. 2010). Grantham distances quantify the biochemical dissimilarity between amino acid pairs, while secondary structure and domain annotations from databases like Pfam provide structural context.\nNot every individual annotation is informative in isolation. Many features are noisy, incomplete, or redundant with one another. The power of CADD lies in learning how to weight and combine these heterogeneous signals, up-weighting annotations that distinguish proxy-deleterious from proxy-neutral variants and down-weighting those that do not. This learned integration is more powerful than any manually specified combination rule and adapts automatically as new features are added in subsequent versions.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch04-cadd.html#model-architecture-and-scoring",
    "href": "p1-ch04-cadd.html#model-architecture-and-scoring",
    "title": "4  Deleteriousness Scores",
    "section": "4.4 Model Architecture and Scoring",
    "text": "4.4 Model Architecture and Scoring\n\n4.4.1 Machine Learning Framework\nCADD’s classifier operates on a high-dimensional feature vector assembled for each variant. The input representation concatenates all annotations described in the previous section: gene model features, conservation scores, epigenomic signals, sequence context, and protein-level predictions where applicable. For a typical variant, this yields a vector of several dozen to over a hundred features, depending on the CADD version and whether the variant falls in coding or non-coding sequence.\nEarly CADD versions employed a linear support vector machine (SVM) trained on approximately 30 million observed and simulated variants with 63 annotation features plus selected interaction terms (Rentzsch et al. 2019). The choice of a linear model was deliberate: with tens of millions of training examples and dozens of features, a linear SVM is computationally tractable while still capturing the main structure of the classification problem. The linearity also provides some interpretability, as feature weights indicate which annotations most strongly distinguish the proxy classes.\nIn CADD v1.7, the framework transitions to a logistic regression-style model with an expanded annotation set exceeding 100 features (Schubach et al. 2024). The fundamental paradigm remains unchanged: contrast simulated and observed variants using a discriminative classifier. The expanded feature set incorporates new annotations including protein language model scores and regulatory CNN predictions (discussed in the following section), while the logistic formulation provides well-calibrated probability estimates that facilitate downstream score transformations.\nConceptually, the classifier learns a scoring function \\(s(x)\\) such that large positive values indicate variants whose annotation profiles resemble the proxy-deleterious class, while large negative values indicate profiles resembling the proxy-neutral class. Variants with intermediate scores occupy an ambiguous middle ground where the annotation evidence does not clearly favor either class. The raw output of this classifier is often referred to as the C-score or raw CADD score.\n\n\n4.4.2 PHRED-Scaled Scores\nRaw CADD scores are not directly interpretable as probabilities or biological effect sizes. The scale depends on model architecture, feature normalization, and training set composition, all of which vary across CADD versions. To provide a more intuitive and stable scoring system, CADD defines PHRED-like scaled scores based on the rank of each variant among all possible single-nucleotide substitutions in the reference genome (Rentzsch et al. 2019; Schubach et al. 2024).\nThe PHRED scaling follows the same logarithmic convention used in sequencing quality scores. A scaled score of 10 indicates that a variant falls in the top 10% of predicted deleteriousness among all possible substitutions. A score of 20 indicates the top 1%, and a score of 30 indicates the top 0.1%. More generally, a scaled score of \\(n\\) corresponds to the top \\(10^{-n/10}\\) fraction of the deleteriousness distribution. This transformation compresses the raw scores into a 1-99 range that reflects percentile rank rather than absolute effect size.\nThis rank-based transformation has several practical consequences. First, it provides a simple interpretation: users can immediately understand that a variant with scaled score 20 is predicted to be more deleterious than 99% of possible substitutions. Second, it ensures comparability across CADD versions. Because the scaled score is defined relative to the full distribution of possible variants, a score of 20 always means “top 1%” even as the underlying model, features, and raw score distributions change between releases. Third, it sacrifices resolution in the bulk of the distribution. Most variants are predicted to be relatively benign, and these cluster in the low-score range where differences are difficult to interpret. The scaling concentrates dynamic range in the high-score tail where clinical interpretation typically focuses.\nIn rare disease pipelines, CADD scaled scores are commonly used as filters to enrich for potentially pathogenic variants before detailed interpretation. Typical thresholds range from 15 (top 3%) to 20 (top 1%) or higher, depending on the stringency required and the downstream analysis workflow. These filters are not intended as definitive pathogenicity calls but rather as prioritization tools that reduce the variant burden to a manageable number for expert review.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch04-cadd.html#cadd-v1.7-integration-of-deep-learning-predictions",
    "href": "p1-ch04-cadd.html#cadd-v1.7-integration-of-deep-learning-predictions",
    "title": "4  Deleteriousness Scores",
    "section": "4.5 CADD v1.7: Integration of Deep Learning Predictions",
    "text": "4.5 CADD v1.7: Integration of Deep Learning Predictions\nCADD v1.7 demonstrates how the original annotation-integration framework naturally accommodates deep learning outputs and modern sequence models (Schubach et al. 2024). Rather than replacing CADD’s architecture with an end-to-end neural network, the developers adopted a pragmatic strategy: treat deep learning predictions as additional features within the existing integrative framework. This approach preserves CADD’s interpretable structure while benefiting from the representational power of large pretrained models.\n\n4.5.1 Protein Language Model Features\nFor protein-coding variants, CADD v1.7 integrates variant effect scores from protein language models (PLMs), particularly ESM-1v (Meier et al. 2021). These models represent a paradigm shift in protein sequence analysis. Trained self-supervised on hundreds of millions of protein sequences using masked language modeling objectives, PLMs learn contextual embeddings that capture the evolutionary constraints and functional requirements shaping protein sequences. The resulting representations encode information about secondary structure, domain boundaries, binding interfaces, and catalytic sites without explicit supervision on any of these properties.\nESM-1v provides per-variant scores by comparing the log-likelihood of the reference and alternate amino acids at each position. Positions where the model confidently predicts the reference residue and assigns low probability to the alternate receive large effect scores, indicating the substitution violates learned sequence constraints. These scores correlate strongly with experimental measurements of variant effects from deep mutational scanning assays, demonstrating that PLMs capture genuine functional information.\nBy embedding ESM-1v-derived features into its annotation set, CADD v1.7 effectively delegates part of the representation learning to a large foundational protein model, then uses its own classifier to recalibrate and integrate these signals with other annotations (Schubach et al. 2024). This division of labor plays to each model’s strengths: the PLM learns rich sequence representations from massive protein databases, while CADD’s integrative framework combines these representations with genomic context, conservation, and regulatory features that protein-only models cannot access.\n\n\n4.5.2 Regulatory CNN Predictions\nFor non-coding variants, CADD v1.7 incorporates regulatory variant effect predictions from sequence-based convolutional neural networks trained on chromatin accessibility and related assays (Zhou and Troyanskaya 2015; Schubach et al. 2024). These CNNs, exemplified by DeepSEA and similar architectures covered in Chapters 5 and 6, take raw DNA sequence as input and predict a battery of chromatin features including transcription factor binding, histone modifications, and DNase hypersensitivity across diverse cell types.\nThe variant effect predictions are computed as delta scores: the difference in predicted regulatory activity between reference and alternate alleles. Large magnitude deltas indicate variants predicted to substantially alter local chromatin state or transcription factor occupancy. These predictions provide a learned, sequence-based view of regulatory impact that complements the annotation-based epigenomic features derived from experimental data.\nBy incorporating CNN-derived regulatory predictions, CADD v1.7 uses early sequence-to-function deep learning models as feature generators within its broader integrative framework. This represents an important architectural pattern that recurs throughout genomic deep learning: pretrained sequence models provide representations or predictions that are then combined with other information sources in downstream tasks. The pretrained models capture sequence-intrinsic patterns, while the integrative framework adds genomic context and cross-annotation calibration.\n\n\n4.5.3 Extended Conservation Scores\nCADD v1.7 updates its conservation and mutation-rate features to incorporate advances in comparative genomics and population genetics. Deeper mammalian alignments from projects like Zoonomia, which sequenced over 200 mammalian species, provide substantially improved resolution for identifying constrained positions, particularly in non-coding regions where earlier alignments had limited power (Schubach et al. 2024). The expanded phylogenetic scope allows detection of constraint that is specific to mammals or particular mammalian clades, complementing the broader vertebrate and eukaryotic conservation captured by earlier alignments.\nImproved models of genome-wide mutation rates sharpen the distinction between true evolutionary constraint and regions with inherently low mutation rates. Earlier approaches sometimes conflated these signals: a region might appear conserved simply because few mutations arise there rather than because mutations are selectively removed. By incorporating refined mutation rate estimates derived from de novo mutation studies and population polymorphism patterns, CADD v1.7 can better distinguish these scenarios and assign appropriate deleteriousness scores.\nThese updates are particularly valuable for non-coding variant interpretation, where conservation signals are often the strongest available evidence for function. Improved detection of mammal-specific regulatory elements and better calibration against local mutation rates help identify pathogenic non-coding variants that earlier versions might have missed.\n\n\n4.5.4 Performance Improvements\nCADD v1.7 is evaluated on several benchmark datasets that span different variant types and functional readouts (Schubach et al. 2024). Clinical variant benchmarks drawn from ClinVar and gnomAD compare pathogenic and benign variant sets, providing a coarse approximation of the clinical classification task that motivates CADD’s development. Deep mutational scanning (DMS) assays, summarized in resources like ProteinGym, offer experimentally measured variant effects for thousands of mutations across dozens of proteins, enabling evaluation against direct functional measurements rather than clinical labels (Notin et al. 2023). Saturation mutagenesis reporter assays for promoters and enhancers capture regulatory variant effects with nucleotide resolution, testing CADD’s performance on the non-coding variants that are often most challenging to interpret.\nAcross these benchmarks, incorporating PLM scores, regulatory CNN predictions, and updated conservation features yields consistent improvements in classification and ranking performance compared to earlier CADD versions. The gains are particularly pronounced for missense variants, where ESM-1v features provide substantial additional signal, and for non-coding variants in active regulatory regions, where CNN predictions complement annotation-based features. These improvements validate the strategy of incorporating deep learning outputs as features while maintaining CADD’s interpretable integrative framework.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch04-cadd.html#benchmarking-against-alternative-approaches",
    "href": "p1-ch04-cadd.html#benchmarking-against-alternative-approaches",
    "title": "4  Deleteriousness Scores",
    "section": "4.6 Benchmarking Against Alternative Approaches",
    "text": "4.6 Benchmarking Against Alternative Approaches\n\n4.6.1 Coding Variants\nFor coding variants, CADD exists within a crowded landscape of deleteriousness predictors spanning four decades of methodological development. Legacy tools such as SIFT and PolyPhen pioneered sequence-based and structure-based prediction of amino acid substitution effects, using evolutionary conservation and physicochemical properties to identify potentially damaging missense variants (Ng and Henikoff 2003; Adzhubei et al. 2010). Ensemble methods such as REVEL, MetaLR, and M-CAP combine predictions from multiple individual tools, using machine learning to weight and integrate their outputs. Modern deep learning approaches exploit protein language models, structure prediction from AlphaFold, and end-to-end neural architectures that learn directly from sequence.\nIn systematic benchmarks across clinically annotated variants and deep mutational scanning datasets, CADD’s combination of evolutionary, protein-level, and gene-context features yields performance that is competitive with or superior to many specialized scores for Mendelian disease variant prioritization (Rentzsch et al. 2019; Schubach et al. 2024). The integration of ESM-1v features in version 1.7 closes much of the gap with pure PLM-based methods while retaining CADD’s advantages in interpretability and genome-wide coverage. CADD’s performance is particularly strong when variants must be ranked across diverse genes and consequence types, a setting that favors integrative approaches over methods tuned for specific protein families or variant classes.\nHowever, for focused applications within specific protein families or functional classes, specialized methods may outperform CADD. Tools optimized for loss-of-function variant interpretation may capture nuances that CADD’s genome-wide training misses. Structure-based methods incorporating AlphaFold predictions can model three-dimensional context that sequence-based features cannot fully capture. The appropriate choice of variant effect predictor depends on the specific application, available data, and interpretability requirements.\n\n\n4.6.2 Non-coding Variants\nNon-coding variant interpretation presents fundamentally greater challenges than coding variant prediction. Ground-truth pathogenic non-coding variants are far rarer in clinical databases and heavily biased toward a small number of well-studied regulatory elements, particularly canonical splice sites and a handful of characterized enhancers. The vast majority of the non-coding genome lacks reliable pathogenicity labels, making supervised approaches difficult and benchmark construction problematic.\nFunctional genomics assays provide an alternative view of non-coding function, but their interpretation is complicated by noise, cell-type specificity, and the uncertain relationship between biochemical activity and phenotypic consequence. A variant may alter transcription factor binding in a reporter assay yet have no detectable effect on gene expression or organismal phenotype. Conversely, subtle regulatory perturbations may have profound effects in specific developmental contexts that are not captured by standard assays.\nWithin this challenging landscape, CADD’s integration of regulatory annotations and conservation allows it to rank plausible non-coding candidates genome-wide, particularly in promoters and enhancers covered by ENCODE and Roadmap data (Rentzsch et al. 2019). The addition of regulatory CNN predictions in version 1.7 provides learned sequence-based features that extend beyond annotation coverage. However, CADD’s performance on non-coding variants depends heavily on the availability and quality of underlying annotations. Variants in poorly annotated regions, including many distal enhancers and non-coding RNAs, receive scores driven primarily by conservation, which may miss recently evolved or lineage-specific functional elements.\n\n\n4.6.3 Population Frequency Correlation\nBecause CADD uses evolutionary depletion as its training signal, its scores naturally correlate with population allele frequencies. Common variants in gnomAD tend to have low CADD scores, reflecting the expectation that alleles reaching high frequency have survived purifying selection. Very rare variants, particularly singletons observed in only one individual, show a broad distribution of scores with a substantial fraction in the high-score tail (Rentzsch et al. 2019; “The Genome Aggregation Database (gnomAD)” n.d.).\nThis correlation is useful for many applications. High CADD scores often highlight variants under purifying selection, which are enriched for functional and potentially pathogenic alleles. The relationship provides a sanity check: if CADD assigned high scores to common variants, something would be wrong with either the model or the frequency data.\nHowever, this correlation also means that CADD partially recapitulates frequency-based filtering. In downstream pipelines, it is important not to double-count this signal by applying both aggressive frequency cutoffs and strict CADD thresholds. Such redundant filtering can exclude variants that fail one criterion but might be genuinely pathogenic. The optimal strategy depends on the application: for highly penetrant Mendelian variants, frequency filtering alone may suffice; for variants with incomplete penetrance or population-specific effects, CADD provides complementary information beyond frequency.\n\n\n4.6.4 Limitations and Circularity with ClinVar\nCADD is now deeply embedded in variant interpretation workflows worldwide, used by clinical laboratories, research groups, and diagnostic pipelines as a standard prioritization tool. This success raises an important methodological concern: potential circularity between CADD scores and clinical databases such as ClinVar.\nTwo forms of circularity are particularly relevant. First, evaluation circularity arises when CADD is assessed on benchmark datasets that were themselves influenced by CADD. ClinVar submissions increasingly incorporate in silico evidence, including CADD scores, as part of their classification process. When we evaluate CADD on post-2014 ClinVar variants after clinical curation has already used CADD, we risk overestimating performance because the model is partially being judged against labels it helped create (Schubach et al. 2024). Variants with high CADD scores are more likely to be classified as pathogenic, and variants classified as pathogenic form the positive evaluation set, creating a feedback loop that inflates apparent performance.\nSecond, broader sociotechnical feedback affects model development even if CADD’s core training labels derive from simulated versus observed variants rather than clinical databases. ClinVar and related resources still influence feature engineering, threshold selection, and choice of evaluation benchmarks. Over time, variants consistently prioritized by CADD are more likely to receive follow-up investigation, be published, and enter ClinVar as likely pathogenic, reinforcing the underlying signal. This feedback is not unique to CADD but affects any widely used predictive tool in genomics and medicine.\nThese circularity concerns motivate several best practices for evaluation. Benchmarks should include datasets independent of clinical curation pipelines, such as deep mutational scanning experiments, reporter assays, and population-based burden tests where labels derive from experimental measurement rather than clinical judgment. Performance should be reported separately on pre-CADD and post-CADD ClinVar subsets when temporal stratification is possible. ClinVar-based evaluation should be treated as a sanity check confirming that CADD captures clinically relevant signals, not as the primary or sole measure of model quality.\nThese concerns foreshadow similar issues we will encounter in later chapters when genomic foundation models are evaluated on benchmarks that themselves rely on older predictive tools or clinical databases shaped by those tools.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch04-cadd.html#significance-for-genomic-deep-learning",
    "href": "p1-ch04-cadd.html#significance-for-genomic-deep-learning",
    "title": "4  Deleteriousness Scores",
    "section": "4.7 Significance for Genomic Deep Learning",
    "text": "4.7 Significance for Genomic Deep Learning\nCADD occupies an important historical position at the junction between hand-crafted feature integration and modern deep, self-supervised representation learning. Several aspects of its design resonate throughout the models and methods covered in subsequent chapters, making it a valuable conceptual anchor for understanding the field’s evolution.\nThe first connection is between annotation integration and multi-task deep models. CADD’s strategy of combining dozens of heterogeneous annotations into a single score anticipates the multi-task learning frameworks that define modern genomic deep learning. Models like DeepSEA, Basset, and Enformer, covered in Chapters 5 through 7 and revisited in Chapter 11, predict hundreds of functional genomics readouts from sequence and then reuse these predictions as building blocks for downstream tasks. The conceptual structure is similar: learn to predict many weak signals, then combine them for variant interpretation. In CADD v1.7, the boundary between these approaches blurs as deep networks including ESM-1v and regulatory CNNs provide features that CADD integrates (Meier et al. 2021; Zhou and Troyanskaya 2015; Schubach et al. 2024). The distinction between “annotation-based” and “deep learning-based” methods becomes one of degree rather than kind.\nThe second connection is between evolutionary proxy labels and self-supervised learning. CADD’s training on simulated versus observed variants uses the signature of selection as a rich, weak supervisory signal available across the entire genome (Rentzsch et al. 2019). This strategy is conceptually parallel to the masked language modeling objectives that define modern protein and DNA language models. In both cases, the labels derive not from expert curation but from statistical regularities in large datasets: which tokens (amino acids, nucleotides, or variants) are observed versus which are plausible but absent. The resulting models learn representations that transfer to diverse downstream tasks, from variant effect prediction to structure determination to regulatory sequence design. Chapters 8 through 10 develop this connection in detail for transformer-based foundation models.\nThe third connection concerns genome-wide coverage and scalability. By precomputing scores for all possible single-nucleotide substitutions in the reference genome, CADD demonstrated the feasibility and utility of generating genome-wide variant annotations for downstream reuse. Users need not run the full model for each query; they simply look up precomputed scores from distributed files. Many genomic foundation models now follow an analogous pattern, precomputing embeddings or predictions for every base or variant and exposing them as reusable resources. The infrastructure for distributing and querying such precomputed annotations has become a standard component of genomic analysis pipelines.\nThe fourth connection is composability with deep learning. CADD is not a direct competitor to modern sequence-based deep models but rather an integrative framework that increasingly incorporates them as features. This “deep features plus shallow integrator” pattern appears repeatedly in practical deployments where interpretability, calibration, or computational constraints favor hybrid approaches over end-to-end neural networks. Clinical variant interpretation pipelines, in particular, often combine CADD-style integrative scores with deep learning predictions and expert review, leveraging the strengths of each approach.\nAs we move into the CNN-based sequence-to-function models of Part II and the transformer-based genomic foundation models of Parts III and IV, it is helpful to remember that CADD solved a difficult problem using tools available at the time. The challenge of variant prioritization under data scarcity and annotation heterogeneity does not disappear with more powerful models. The deep learning systems that follow expand on CADD’s core ideas by learning representations directly from sequence and tying those representations to richer experimental readouts. Yet they still rely on many of the same data resources surveyed in Chapter 2 and confront many of the same challenges around evaluation bias, label circularity, and the fundamental difficulty of inferring causality from correlation. Understanding CADD’s solutions and limitations provides essential context for appreciating both the advances and the persistent challenges in genomic deep learning.\n\n\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky, Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev. 2010. “A Method and Server for Predicting Damaging Missense Mutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper, Arend Sidow, and Serafim Batzoglou. 2010. “Identifying a High Fraction of the Human Genome to Be Under Selective Constraint Using GERP++.” PLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT: Predicting Amino Acid Changes That Affect Protein Function.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. “ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design.” Advances in Neural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs, Minmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005. “[PhastCons] Evolutionarily Conserved Elements in Vertebrate, Insect, Worm, and Yeast Genomes.” Genome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.\n\n\n“The Genome Aggregation Database (gnomAD).” n.d. Accessed July 3, 2025. https://www.nature.com/immersive/d42859-020-00002-x/index.html.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "p2-ch05-reg.html",
    "href": "p2-ch05-reg.html",
    "title": "5  Regulatory Prediction",
    "section": "",
    "text": "5.1 The Noncoding Variant Challenge\nThe vast majority of disease-associated variants identified by GWAS lie in noncoding regions of the genome. Across thousands of loci mapped to complex traits, only a small minority directly alter protein-coding sequences; the remainder fall in introns, intergenic regions, and putative regulatory elements where their functional consequences are far less obvious. This presents both an interpretive challenge and an opportunity. If we could predict how noncoding variants affect gene regulation, we would have a powerful tool for moving from statistical association to biological mechanism.\nYet in 2015, the field lacked systematic methods to predict how noncoding variants affect regulatory activity. Existing approaches relied on overlap with known annotations: if a variant fell within a ChIP-seq peak or DNase hypersensitive site, it might be flagged as potentially functional. This strategy had obvious appeal, since it grounded predictions in experimental observations, but it suffered from fundamental limitations. Overlap-based annotation offered no mechanism for predicting the direction or magnitude of a variant’s effect on regulatory activity. A variant might fall within an enhancer, but would it strengthen or weaken the enhancer? By how much? These questions could not be answered by checking whether genomic coordinates intersected. Furthermore, overlap-based methods could not score variants in regions lacking experimental coverage, which was problematic given that functional genomics experiments, despite their scale, still covered only a fraction of cell types and conditions.\nDeepSEA, introduced by Zhou and Troyanskaya in 2015, fundamentally changed this paradigm by learning to predict chromatin features directly from DNA sequence (Zhou and Troyanskaya 2015). Rather than asking “does this variant overlap a known regulatory element?”, DeepSEA asks “what regulatory activities does this sequence encode, and how would a mutation change them?” This shift from annotation lookup to sequence-based prediction opened a new chapter in computational genomics, one where deep neural networks could learn the relationship between DNA sequence and molecular function without requiring hand-crafted features or explicit motif definitions.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch05-reg.html#the-core-innovation-learning-regulatory-code-from-sequence",
    "href": "p2-ch05-reg.html#the-core-innovation-learning-regulatory-code-from-sequence",
    "title": "5  Regulatory Prediction",
    "section": "5.2 The Core Innovation: Learning Regulatory Code from Sequence",
    "text": "5.2 The Core Innovation: Learning Regulatory Code from Sequence\nDeepSEA’s central insight was that deep convolutional networks could learn the sequence patterns underlying regulatory activity without explicit feature engineering. Previous methods like gapped k-mer SVMs (gkm-SVM) required defining sequence features a priori—specifying which k-mers to count and how to weight them. DeepSEA instead learned relevant sequence features automatically from data.\n\n5.2.1 Architecture\nThe original DeepSEA architecture comprised:\n\nInput layer: 1000 bp DNA sequence, one-hot encoded (4 channels × 1000 positions)\nThree convolutional layers: Each followed by ReLU activation and max pooling, learning increasingly abstract sequence features\nFully connected layer: Integrating learned features across the sequence\nOutput layer: 919 sigmoid outputs predicting chromatin profile probabilities\n\nThe convolutional layers function analogously to motif scanners, but with crucial differences: they learn motifs from data rather than requiring predefined position weight matrices, and deeper layers can learn combinations of motifs (regulatory “grammar”) rather than just individual binding sites.\n\n\n5.2.2 Training Data\nDeepSEA was trained on 919 chromatin profiles from ENCODE and Roadmap Epigenomics:\n\n\n\nProfile Type\nCount\nExamples\n\n\n\n\nTranscription factor binding\n690\nCTCF, p53, GATA1\n\n\nHistone modifications\n104\nH3K4me3, H3K27ac\n\n\nDNase I hypersensitivity\n125\nOpen chromatin across cell types\n\n\n\nFor each 1000 bp sequence, the model predicts the probability that the central 200 bp region exhibits each chromatin feature. Training used sequences from the human genome with chromosome 8 held out for testing.\n\n\n5.2.3 Multi-Task Learning\nA key architectural decision was predicting all 919 features simultaneously rather than training separate models. This multi-task learning approach offers several advantages:\n\nShared representations: Early convolutional layers learn general sequence features (e.g., GC content, common motifs) useful across tasks\nRegularization: Jointly predicting correlated features prevents overfitting to any single task\nEfficiency: One model serves all prediction tasks",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch05-reg.html#predicting-variant-effects",
    "href": "p2-ch05-reg.html#predicting-variant-effects",
    "title": "5  Regulatory Prediction",
    "section": "5.3 Predicting Variant Effects",
    "text": "5.3 Predicting Variant Effects\nWith a trained model that maps sequence to chromatin profiles, variant effect prediction becomes straightforward in principle: predict chromatin profiles for both reference and alternative allele sequences, then compute the difference. This produces a 919-dimensional vector describing how the variant is predicted to alter regulatory activity across all profiled features. A variant might be predicted to increase CTCF binding while decreasing DNase accessibility, or to have no effect on any chromatin feature, depending on where it falls and what sequence context it disrupts or creates.\nThis approach has a crucial property: it requires no training on variant data. The model learns to predict chromatin profiles from sequence during training, using only reference genome sequences and their experimentally measured chromatin states. Variant effect prediction is then a form of transfer: the model applies what it learned about sequence-function relationships to score mutations it has never seen. This ab initio capability distinguishes sequence-based models from approaches that learn directly from observed variant effects, which are inevitably biased toward common variants where statistical power exists.\n\n5.3.1 Single-Nucleotide Sensitivity\nFor the approach to work, the model must achieve single-nucleotide sensitivity: changing one base must be capable of substantially altering predictions. This is not guaranteed. A model could achieve good performance on chromatin prediction by learning only coarse sequence features (GC content, repeat density) that are insensitive to point mutations. Such a model would be useless for variant interpretation.\nDeepSEA achieves genuine single-nucleotide sensitivity, and the authors validated this using allelic imbalance data from digital genomic footprinting. For 57,407 variants showing allele-specific DNase I sensitivity across 35 cell types, DeepSEA predictions correlated strongly with the experimentally observed allelic bias. Variants predicted to increase chromatin accessibility tended to show higher accessibility on the corresponding allele, and vice versa. This correlation would not exist if the model were insensitive to point mutations.\nThe validation is particularly compelling because allelic imbalance represents an independent experimental readout. The model was not trained to predict allelic imbalance; it was trained to predict chromatin profiles from reference sequences. That it correctly predicts the direction of allelic effects demonstrates that the learned sequence-function relationships capture genuine biology rather than spurious correlations.\n\n\n5.3.2 In Silico Saturation Mutagenesis\nBeyond scoring individual variants, DeepSEA enables a powerful computational experiment: in silico saturation mutagenesis (ISM). By systematically predicting effects of all possible single-nucleotide substitutions within a sequence, one can identify which positions are most critical for regulatory function. At each position, three alternative nucleotides can be substituted, and the predicted change in chromatin profiles can be computed for each. Positions where substitutions produce large predicted effects are presumably functionally constrained, while positions tolerant of substitution are less critical.\nISM analysis of regulatory elements reveals sequence positions where mutations would most strongly perturb function. These critical positions often correspond to transcription factor binding motifs learned by the model. When the predicted effects are visualized along a regulatory sequence, clear patterns emerge: core motif positions show strong predicted effects, while flanking positions are more tolerant. This provides a form of motif discovery that emerges from the model’s learned representations rather than from explicit motif searching.\nThe computational cost of ISM is linear in sequence length: for a 1000 bp sequence, 3000 forward passes are required (three substitutions per position). This is tractable for individual regions of interest, and precomputed ISM scores for the entire genome can be generated with sufficient computational resources.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch05-reg.html#functional-variant-prioritization",
    "href": "p2-ch05-reg.html#functional-variant-prioritization",
    "title": "5  Regulatory Prediction",
    "section": "5.4 Functional Variant Prioritization",
    "text": "5.4 Functional Variant Prioritization\nBeyond predicting chromatin effects for individual variants, DeepSEA introduced a framework for prioritizing likely functional variants among large sets of candidates. This addresses a practical problem in human genetics: GWAS and sequencing studies identify many variants in a region, most of which are not causal. Which variants should be prioritized for follow-up?\n\n5.4.1 eQTL Prioritization\nExpression quantitative trait loci (eQTLs) represent variants statistically associated with gene expression changes. However, most eQTL signals reflect linkage disequilibrium rather than direct causation. A lead eQTL variant may simply be correlated with the true causal variant, which could be any of dozens of SNPs in the same LD block. Distinguishing causal variants from their correlated neighbors is essential for understanding regulatory mechanisms and for transferring findings across populations where LD patterns differ.\nDeepSEA demonstrated improved ability to distinguish likely causal eQTL variants from nearby non-causal variants compared to overlap-based methods. The intuition is straightforward: if a variant is truly causal, it should disrupt a sequence feature that matters for gene regulation. A variant that happens to be in LD with the causal variant but does not itself disrupt regulatory sequences should have minimal predicted effect. By ranking variants according to predicted regulatory impact, DeepSEA can prioritize those most likely to be causal.\n\n\n5.4.2 GWAS Variant Prioritization\nSimilarly, for GWAS-identified disease associations, DeepSEA helped prioritize which variants in LD blocks were most likely causal. The model outperformed contemporary methods including GWAVA (which was trained on known regulatory mutations) on held-out benchmarks. This was notable because DeepSEA was not trained on variant data at all; its variant prioritization ability emerged from learning sequence-chromatin relationships, not from learning which variants are pathogenic.\n\n\n5.4.3 Comparison to Prior Methods\nDeepSEA’s performance advantage over gkm-SVM was particularly notable for transcription factor binding prediction. The deep CNN achieved higher AUC for nearly all transcription factors tested. More revealing was the pattern with respect to sequence context: gkm-SVM showed no improvement when given longer input sequences (extending context from 200 bp to 500 bp to 1000 bp), while DeepSEA performance improved substantially with additional context.\nThis difference reflects the fundamental limitation of gapped k-mer methods. By counting k-mers and learning weights for them, gkm-SVM can capture the presence of individual motifs but struggles to learn relationships between motifs at different positions. The same k-mers in different spatial arrangements contribute identically to the score. Deep convolutional networks, by contrast, learn hierarchical representations where deeper layers can capture spatial dependencies between features detected by earlier layers. Additional sequence context provides more opportunities for these dependencies to inform predictions.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch05-reg.html#evolution-of-the-deepsea-framework",
    "href": "p2-ch05-reg.html#evolution-of-the-deepsea-framework",
    "title": "5  Regulatory Prediction",
    "section": "5.5 Evolution of the DeepSEA Framework",
    "text": "5.5 Evolution of the DeepSEA Framework\nThe original DeepSEA established the sequence-to-chromatin prediction paradigm. Subsequent work from the same research group expanded and refined this approach, building a lineage of models with progressively greater scope and sophistication.\n\n5.5.1 DeepSEA Beluga (2018)\nExPecto, published in 2018, included an updated chromatin prediction model nicknamed “Beluga” that served as the foundation for tissue-specific expression prediction (Zhou et al. 2018). Beluga incorporated several architectural improvements over the original DeepSEA. The number of predicted chromatin profiles expanded from 919 to 2,002, covering additional transcription factors and histone modifications across more cell types. The architecture deepened, adding additional convolutional layers with residual connections that facilitated training and improved gradient flow. The input context expanded from 1000 bp to 2000 bp, allowing the model to capture longer-range sequence dependencies.\nThese improvements were motivated by the downstream application: predicting gene expression requires integrating regulatory signals across tens of kilobases around each transcription start site. A more capable chromatin prediction model, applied at multiple positions around a gene, provides richer features for expression prediction. The Beluga chromatin model is discussed further in Chapter 6, where it forms the first component of the ExPecto expression prediction framework.\n\n\n5.5.2 Sei (2022)\nSei represents the current state of the DeepSEA lineage, predicting 21,907 chromatin profiles, a 24-fold expansion over the original (Chen et al. 2022). This dramatic scaling required both more training data (from expanded ENCODE and Roadmap datasets) and architectural innovations to handle the increased output dimensionality efficiently.\nThe Sei architecture introduces dual linear and nonlinear paths: parallel convolution blocks, one with activation functions and one without, allowing the model to learn both complex nonlinear patterns and simpler linear relationships. This design reflects the observation that some chromatin features depend on subtle nonlinear combinations of sequence features, while others are well predicted by simpler linear combinations. Dilated convolutions expand the receptive field without reducing spatial resolution, allowing the network to integrate information across longer distances without aggressive pooling. Spatial basis functions provide a memory-efficient mechanism for integrating information across positions, reducing the parameter count that would otherwise grow prohibitively with the number of output features.\nSei improved over Beluga by 19% on average (measured by AUROC/(1-AUROC), a metric that emphasizes improvements at high performance levels) on the 2,002 profiles predicted by both models. Beyond raw prediction performance, Sei introduced sequence class annotations that cluster the 21,907 chromatin predictions into interpretable regulatory categories, facilitating biological interpretation of model outputs.\n\n\n\nModel\nYear\nChromatin Targets\nInput Length\nArchitecture\n\n\n\n\nDeepSEA\n2015\n919\n1000 bp\n3 conv + FC\n\n\nBeluga\n2018\n2,002\n2000 bp\nDeep residual CNN\n\n\nSei\n2022\n21,907\n4000 bp\nDual-path + dilated conv",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch05-reg.html#what-deepsea-learns",
    "href": "p2-ch05-reg.html#what-deepsea-learns",
    "title": "5  Regulatory Prediction",
    "section": "5.6 What DeepSEA Learns",
    "text": "5.6 What DeepSEA Learns\nAnalyzing what neural networks learn is notoriously difficult, but several approaches have been applied to DeepSEA and its successors, revealing that the models capture biologically meaningful sequence patterns.\n\n5.6.1 Motif Discovery\nThe first convolutional layer of DeepSEA contains filters that scan the input sequence for local patterns. By visualizing these filters as position weight matrices (treating the learned weights as log-odds scores) or by identifying sequences that maximally activate each filter, researchers can examine what patterns the network has learned to detect.\nAnalysis of DeepSEA’s first-layer filters reveals learned sequence patterns corresponding to known transcription factor binding motifs. Many filters match canonical motifs from databases like JASPAR, indicating that the network has independently discovered the sequence preferences of well-characterized transcription factors. This is reassuring: it confirms that the network is learning biologically relevant patterns rather than spurious correlations.\nDeeper layers capture more complex patterns that do not correspond to individual motifs. These representations are harder to interpret but presumably encode combinations of motifs and spatial arrangements that predict chromatin state.\n\n\n5.6.2 Regulatory Grammar\nBeyond individual motifs, DeepSEA implicitly learns aspects of regulatory “grammar,” the rules governing how motifs combine to produce regulatory activity. This includes motif spacing requirements (some transcription factor pairs require specific distances between their binding sites for cooperative function), motif orientation preferences (certain motifs function only in specific orientations relative to each other or to the gene), and combinatorial logic (multiple weak motifs can synergize, or overlapping sites can create competition between factors).\nThese grammatical rules are not explicitly represented in the model architecture; they emerge from learning to predict chromatin profiles from sequence. The deep architecture provides the representational capacity to encode complex dependencies, and the training procedure discovers whatever dependencies best predict the training labels.\nHowever, the original DeepSEA architecture’s limited receptive field constrained its ability to learn long-range dependencies. Max pooling after each convolutional layer progressively reduces spatial resolution, and the fully connected layer can only integrate information from the resulting compressed representation. Dependencies spanning hundreds or thousands of base pairs, such as enhancer-promoter communication, are difficult to capture in this framework. This limitation motivated later architectures with expanded context windows, culminating in models like Enformer (Chapter 11) with effective receptive fields spanning hundreds of kilobases.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch05-reg.html#limitations-and-considerations",
    "href": "p2-ch05-reg.html#limitations-and-considerations",
    "title": "5  Regulatory Prediction",
    "section": "5.7 Limitations and Considerations",
    "text": "5.7 Limitations and Considerations\nDeepSEA represented a major advance, but understanding its limitations is essential for appropriate application and for appreciating the motivations behind subsequent developments.\n\n5.7.1 Cell Type Specificity\nDeepSEA predicts chromatin profiles for specific cell types included in training, but the same sequence may have different regulatory activity in cell types not represented. The model cannot extrapolate to novel cell types without relevant training data. If a user wants to predict regulatory activity in a cell type that was not profiled by ENCODE or Roadmap, DeepSEA provides no principled way to do so. The prediction would either be unavailable or would require assuming that a related profiled cell type is a reasonable proxy.\nThis limitation is intrinsic to the supervised learning framework: the model learns input-output mappings for the cell types present in training data. Extending predictions to new cell types would require either profiling those cell types experimentally (creating new training labels) or developing methods that transfer learned representations across cell types, an active area of research in subsequent work.\n\n\n5.7.2 Context Independence\nThe model treats each input sequence independently, without considering the broader genomic or cellular context in which that sequence operates. Three important contextual factors are absent from the model.\nThree-dimensional chromatin structure brings distant genomic sequences into spatial proximity, allowing enhancers to regulate promoters located hundreds of kilobases away on the linear chromosome. DeepSEA sees only the linear sequence within its 1000 bp window; it cannot know whether distant regulatory elements are spatially proximal in the nucleus.\nThe current transcriptional state of the cell affects chromatin accessibility and transcription factor availability. A sequence might have regulatory potential that is realized only when certain factors are expressed. DeepSEA predicts potential regulatory activity based on sequence alone, not actual activity conditioned on cellular state.\nOther variants in the same individual (epistasis) may modify the effect of any single variant. DeepSEA predicts effects for each variant in isolation, against the reference genome background. In reality, individuals carry thousands of variants, some of which may interact.\n\n\n5.7.3 Quantitative Accuracy\nWhile DeepSEA accurately predicts the binary presence or absence of chromatin features, its quantitative predictions of signal strength are less reliable. The model outputs probabilities that a region exhibits each feature, but these probabilities do not directly correspond to the magnitude of ChIP-seq or DNase-seq signal intensity. A region might be correctly predicted as bound by a transcription factor, but the model provides limited information about whether binding is strong or weak.\nLater models addressed this limitation by predicting continuous coverage tracks rather than binary peaks. Basenji, introduced in 2018, predicted normalized read coverage across the genome, providing quantitative predictions that could be directly compared to experimental measurements (kelley_basenji_2018?). This shift from classification to regression enabled more nuanced variant effect predictions, where the question becomes not just “does this variant disrupt binding?” but “by how much does this variant change binding affinity?”",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch05-reg.html#significance-for-the-field",
    "href": "p2-ch05-reg.html#significance-for-the-field",
    "title": "5  Regulatory Prediction",
    "section": "5.8 Significance for the Field",
    "text": "5.8 Significance for the Field\nDeepSEA established several paradigms that shaped subsequent genomic deep learning. These contributions extend beyond the specific model to influence how the field approaches sequence-to-function prediction more broadly.\nThe “sequence-in, function-out” paradigm treats DNA sequence as the sole input and molecular function as the output, learning the mapping without hand-engineered features. This end-to-end learning approach allows the model to discover relevant patterns from data rather than encoding assumptions about what patterns matter. Subsequent models have extended this paradigm to predict increasingly complex functional readouts, from expression levels to splicing outcomes to three-dimensional chromatin organization.\nMulti-task chromatin prediction, jointly modeling many related tasks, proved both more efficient and more effective than training separate models. The shared representations and implicit regularization that emerge from multi-task learning have become standard in genomic deep learning. Modern models routinely predict hundreds or thousands of outputs simultaneously, leveraging correlations between tasks to improve predictions for each.\nVariant effect prediction via sequence comparison, scoring variants by comparing predictions for reference and alternative alleles, provided a general framework for interpreting genetic variation. This approach extends naturally to any sequence-based model: if the model predicts molecular function from sequence, it can predict how mutations alter that function. The ab initio nature of this prediction, requiring no training on variant data, enables scoring of rare variants where population data is sparse.\nThe approach demonstrated that deep learning could extract biologically meaningful patterns from raw sequence data at scale. Convolutional filters learn motifs, deeper layers learn combinations, and the resulting representations support accurate prediction and variant interpretation. This opened the door to increasingly sophisticated sequence-to-function models predicting not just chromatin state, but gene expression (ExPecto, Chapter 6), splicing (SpliceAI, Chapter 7), and eventually long-range regulatory interactions (Enformer, Chapter 11).\nDeepSEA’s public web server (http://deepsea.princeton.edu/) and code release also established a model for making genomic deep learning tools accessible to the broader research community. Rather than keeping trained models proprietary, the authors provided both a web interface for casual users and downloadable code and weights for computational researchers. This practice of open release has become standard in the field, accelerating progress by allowing others to build on published work rather than reimplementing from scratch.\nThe model’s success also catalyzed interest in deep learning among genomics researchers who had previously worked with simpler statistical methods. By demonstrating that neural networks could learn interpretable and useful representations of regulatory sequence, DeepSEA helped establish genomics as a legitimate application domain for deep learning and attracted researchers from both communities to the intersection.\n\n\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html",
    "href": "p2-ch06-transc.html",
    "title": "6  Transcriptional Effects",
    "section": "",
    "text": "6.1 From Chromatin to Expression\nDeepSEA (Chapter 5) demonstrated that deep learning could predict chromatin features from DNA sequence alone. Yet chromatin accessibility and transcription factor binding are intermediate phenotypes. The ultimate functional readout for most regulatory variants is their effect on gene expression. A variant might disrupt a transcription factor binding site, but does that binding site actually regulate a nearby gene? In which tissues? By how much?\nExPecto, introduced by Zhou et al. in 2018, addressed these questions by extending the sequence-to-chromatin paradigm to predict tissue-specific gene expression levels (Zhou et al. 2018). The framework’s name reflects its core capability: expression prediction. Rather than stopping at chromatin predictions, ExPecto integrates predicted regulatory signals across a 40 kb promoter-proximal region to predict absolute expression levels in 218 tissues and cell types.\nCritically, ExPecto predicts expression effects ab initio from sequence, without training on any variant data. This enables scoring of rare variants, de novo mutations, and even hypothetical mutations never observed in any population.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html#the-modular-architecture",
    "href": "p2-ch06-transc.html#the-modular-architecture",
    "title": "6  Transcriptional Effects",
    "section": "6.2 The Modular Architecture",
    "text": "6.2 The Modular Architecture\nExPecto comprises three sequential components, each addressing a distinct computational challenge.\n\n6.2.1 Component 1: Epigenomic Effects Model (Beluga CNN)\nThe first component is an enhanced version of DeepSEA, predicting 2,002 chromatin profiles (histone marks, transcription factor binding, and DNase hypersensitivity) across more than 200 cell types. Key architectural improvements over the original DeepSEA include expanded chromatin targets (from 919 to 2,002), a wider input window (from 1,000 bp to 2,000 bp), deeper architecture (six convolutional layers with residual connections rather than three), and broader cell type coverage (over 200 cell types compared to approximately 125).\n\n\n\nFeature\nDeepSEA (2015)\nExPecto/Beluga (2018)\n\n\n\n\nChromatin targets\n919\n2,002\n\n\nInput window\n1,000 bp\n2,000 bp\n\n\nConvolution layers\n3\n6 (with residual connections)\n\n\nCell types\n~125\n&gt;200\n\n\n\nThe CNN scans the 40 kb region surrounding each transcription start site (TSS) with a moving window (200 bp step size), generating chromatin predictions at 200 spatial positions. For each gene, this produces 2,002 × 200 = 400,400 features representing the predicted spatial chromatin organization around the TSS.\n\n\n6.2.2 Component 2: Spatial Feature Transformation\nThe 400,400-dimensional feature space poses optimization challenges for downstream expression prediction. ExPecto addresses this through spatial transformation, a biologically motivated dimensionality reduction that captures the known distance-dependent relationship between regulatory elements and their target promoters.\nThe transformation applies ten exponential decay functions separately to upstream and downstream regions. The full model specification is:\n\\[\n\\text{expression} = \\sum_{i,k} \\left( \\beta_{ik}^{\\text{up}} \\cdot \\mathbf{1}(t_d &lt; 0) + \\beta_{ik}^{\\text{down}} \\cdot \\mathbf{1}(t_d &gt; 0) \\right) \\cdot \\sum_{d \\in D} p_{id} \\cdot e^{-a_k \\cdot |t_d|}\n\\]\nwhere \\(p_{id}\\) is the predicted probability for chromatin feature \\(i\\) at spatial bin \\(d\\), \\(t_d\\) is the mean distance to TSS for bin \\(d\\), and \\(a_k\\) represents decay constants (0.01, 0.02, 0.05, 0.1, 0.2). The indicator functions \\(\\mathbf{1}(\\cdot)\\) allow separate coefficients for upstream (\\(\\beta^{\\text{up}}\\)) and downstream (\\(\\beta^{\\text{down}}\\)) regions.\nThis transformation reduces dimensionality 20-fold (to 20,020 features) while preserving spatial information. Features with higher decay rates are concentrated near the TSS, while lower decay rates capture more distal signals. The transformation is not learned but prespecified, equivalent to constraining the model to learn smooth spatial patterns as linear combinations of basis functions.\n\n\n6.2.3 Component 3: Tissue-Specific Linear Models\nThe final component comprises 218 L2-regularized linear regression models (one per tissue), each predicting log RPKM expression from spatially-transformed features. Linear models were chosen deliberately: they provide interpretability, prevent overfitting given the high-dimensional feature space, and enable straightforward coefficient analysis to identify which chromatin features drive expression in each tissue.\nTraining used gradient boosting with L2 regularization (λ=100, shrinkage η=0.01), with chromosome 8 held out for evaluation (990 genes). The chromosome-level holdout prevents data leakage through overlapping regulatory regions and sequence homology.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html#expression-prediction-performance",
    "href": "p2-ch06-transc.html#expression-prediction-performance",
    "title": "6  Transcriptional Effects",
    "section": "6.3 Expression Prediction Performance",
    "text": "6.3 Expression Prediction Performance\nExPecto achieved 0.819 median Spearman correlation between predicted and observed expression (log RPKM) across 218 tissues and cell types, a substantial improvement over prior sequence-based expression models, which were typically limited to narrower regulatory regions (&lt;2 kb) and fewer cell types.\n\n6.3.1 Tissue Specificity\nBeyond predicting absolute expression levels, ExPecto captures tissue-specific expression patterns. Expression predictions correlate more strongly with experimental measurements from the matching tissue than from other tissues, indicating the model learns tissue-specific regulatory logic rather than generic sequence features.\nAnalysis of model coefficients reveals automatic learning of cell-type-relevant features without explicit tissue labels. The liver expression model assigns top weights to seven transcription factors profiled in HepG2 (liver-derived) cells. The breast tissue model weights estrogen receptor (ER-α) and glucocorticoid receptor (GR) features from breast cancer cell lines T-47D and ECC-1 most heavily among its positive coefficients. Blood cell expression models derive their top five predictive features from blood cell lines and erythroblast cells. These patterns emerge purely from learning to predict expression, without any tissue identity information provided to the chromatin features.\n\n\n6.3.2 Feature Importance\nModel coefficients also reveal the relative contributions of different chromatin feature types to expression prediction. Transcription factors and histone marks receive consistently higher weights, reflecting their direct mechanistic roles in transcriptional regulation. DNase I features receive significantly lower weights (p = 6.9×10⁻²⁵, Wilcoxon rank sum test) despite indicating regulatory activity. This discrepancy likely reflects that DNase hypersensitivity marks the presence of regulatory activity without specifying its type (activating versus repressing) or its causal relationship to expression.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html#variant-effect-prediction",
    "href": "p2-ch06-transc.html#variant-effect-prediction",
    "title": "6  Transcriptional Effects",
    "section": "6.4 Variant Effect Prediction",
    "text": "6.4 Variant Effect Prediction\nExPecto’s expression predictions enable scoring variant effects through in silico mutagenesis: predict expression with reference allele, predict with alternative allele, and compute the difference. Because the model never trains on variant data, predictions are unconfounded by linkage disequilibrium, a fundamental advantage over statistical eQTL approaches.\n\n6.4.1 Computing Variant Effects\nFor any variant, ExPecto computes effects by comparing predictions:\n\\[\n\\Delta \\text{expression} = f(\\text{sequence}_{\\text{alt}}) - f(\\text{sequence}_{\\text{ref}})\n\\]\nThis approach predicts the direction and magnitude of expression change in each of 218 tissues for any single nucleotide variant within the 40 kb promoter region.\n\n\n6.4.2 eQTL Validation\nExPecto correctly predicted the direction of expression change for 92% of the top 500 strongest-effect GTEx eQTL variants. Prediction accuracy increases with predicted effect magnitude: variants with stronger predicted effects show higher eQTL direction concordance, consistent with the expectation that true causal variants should have larger predicted effects.\nUnlike traditional eQTL studies, which are biased toward common variants with sufficient statistical power, ExPecto predictions work equally well across the allele frequency spectrum. This makes the framework particularly valuable for rare variant interpretation where population data is sparse.\n\n\n6.4.3 Advantages Over eQTL Mapping\nTraditional eQTL studies face fundamental limitations. Linkage disequilibrium confounds causal inference: only 3.5 to 11.7% of GTEx lead variants are estimated to be truly causal, meaning fewer than 1% of all reported eQTL variants directly affect expression. Allele frequency creates power imbalances, as rare variants lack the sample sizes required for detection. Tissue availability constrains what can be studied, since eQTL mapping requires large sample sizes in the tissue of interest.\nExPecto’s sequence-based predictions sidestep all three limitations. The model scores variants based on predicted functional impact rather than population associations, works identically for any allele frequency, and leverages expression training data from many tissues even when eQTL data is unavailable.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html#gwas-causal-variant-prioritization",
    "href": "p2-ch06-transc.html#gwas-causal-variant-prioritization",
    "title": "6  Transcriptional Effects",
    "section": "6.5 GWAS Causal Variant Prioritization",
    "text": "6.5 GWAS Causal Variant Prioritization\nA major application of ExPecto is prioritizing causal variants within GWAS-identified loci, where LD typically prevents identification of the true functional variant.\n\n6.5.1 Systematic Prioritization\nZhou et al. applied ExPecto to prioritize variants from approximately 3,000 GWAS studies. GWAS loci with stronger predicted effect variants were significantly more likely to replicate in independent studies (p = 6.3×10⁻¹⁸⁹, Wald test with logistic regression). Stronger predicted effect variants were also more likely to be the exact replicated variant (p = 5.6×10⁻¹⁴).\nThe framework can identify causal variants that statistical association alone cannot distinguish. For example, an early venous thromboembolism GWAS identified rs3756008 as the lead variant near the F11 locus. ExPecto prioritized a different LD variant, rs4253399, which was subsequently discovered as the true association in a larger cohort study.\n\n\n6.5.2 Experimental Validation\nThe authors experimentally validated three top-ranked ExPecto predictions for immune-related diseases using luciferase reporter assays. In all cases, the ExPecto-prioritized variants showed significant allele-specific regulatory activity, while the original GWAS lead variants showed no differential activity.\n\n\n\n\n\n\n\n\n\n\n\nDisease\nExPecto-Prioritized SNP\nGene\nReporter Effect\np-value\nGWAS Lead SNP\n\n\n\n\nCrohn’s disease / IBD\nrs1174815\nIRGM\nDecreased expression\n3×10⁻⁶\nNot significant\n\n\nBehçet’s disease\nrs147398495\nCCR1\nChanged activity\n7×10⁻¹⁰\nNot significant\n\n\nChronic HBV infection\nrs381218\nHLA-DOA\n4-fold change\n1×10⁻⁹\nNot significant\n\n\n\nExPecto correctly predicted the direction of expression change for all three validated variants. These results demonstrate that sequence-based expression models can identify functional variants that statistical association studies cannot distinguish from linked non-functional variants.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html#in-silico-saturation-mutagenesis",
    "href": "p2-ch06-transc.html#in-silico-saturation-mutagenesis",
    "title": "6  Transcriptional Effects",
    "section": "6.6 In Silico Saturation Mutagenesis",
    "text": "6.6 In Silico Saturation Mutagenesis\nThe computational efficiency of ExPecto enables exhaustive characterization of the regulatory mutation space. The authors computed predicted effects for all possible single nucleotide substitutions within ±1 kb of each TSS, covering over 140 million mutations across 23,779 human Pol II-transcribed genes. This identified more than 1.1 million mutations with strong predicted expression effects.\n\n6.6.1 Variation Potential\nFor each gene, the comprehensive mutagenesis profile defines its “variation potential” (VP), the collective effects of all possible mutations on that gene’s expression. VP reflects the regulatory sensitivity of each gene. Genes with high VP have expression that is easily perturbed by sequence changes, with regulatory regions densely packed with functional elements. Genes with low VP show expression robust to mutations, potentially indicating fewer regulatory constraints or more redundant regulatory architecture.\nVP correlates with known biological properties: tissue-specific genes show lower VP than broadly expressed genes, and genes under stronger evolutionary constraint tend to have higher VP.\n\n\n6.6.2 Constraint Violation Scores\nBy comparing predicted mutational effects to observed population variation, ExPecto enables inference of evolutionary constraints. A “constraint violation score” measures whether observed variants push expression in the “wrong” direction relative to inferred evolutionary constraint. Genes with negative VP directionality (mutations tend to reduce expression) are typically actively expressed, where loss-of-function mutations are deleterious. Genes with positive VP directionality (mutations tend to increase expression) are typically repressed, where gain-of-expression mutations are deleterious.\nThis framework successfully predicts GWAS risk alleles without any prior variant-disease association data. Positive violation scores are significantly associated with alternative alleles being risk alleles (p = 0.002, Wilcoxon rank sum test, AUC = 0.67), demonstrating potential for ab initio disease variant identification.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html#the-40-kb-regulatory-window",
    "href": "p2-ch06-transc.html#the-40-kb-regulatory-window",
    "title": "6  Transcriptional Effects",
    "section": "6.7 The 40 kb Regulatory Window",
    "text": "6.7 The 40 kb Regulatory Window\nExPecto’s ±20 kb window around each TSS represents an empirically optimized trade-off. Smaller windows decreased prediction performance, while larger windows (50 to 200 kb) showed negligible performance improvement.\nThis suggests that most regulatory information for promoter-proximal expression lies within 40 kb of the TSS, at least within the linear modeling framework employed by ExPecto. Distal enhancers beyond this window, while biologically important, likely require more sophisticated integration approaches to capture. Enformer (Chapter 11), with its 200 kb effective receptive field, addresses this limitation.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html#relationship-to-the-deepsea-lineage",
    "href": "p2-ch06-transc.html#relationship-to-the-deepsea-lineage",
    "title": "6  Transcriptional Effects",
    "section": "6.8 Relationship to the DeepSEA Lineage",
    "text": "6.8 Relationship to the DeepSEA Lineage\nExPecto represents a conceptual extension of the DeepSEA framework. DeepSEA (2015) predicts 919 chromatin profiles from a 1 kb context window. ExPecto/Beluga (2018) predicts gene expression across 218 tissues from a 40 kb context window. Sei (2022) predicts 21,907 chromatin profiles plus sequence classes from a 4 kb window.\n\n\n\nModel\nYear\nPrimary Output\nContext Window\n\n\n\n\nDeepSEA\n2015\n919 chromatin profiles\n1 kb\n\n\nExPecto/Beluga\n2018\nGene expression (218 tissues)\n40 kb\n\n\nSei\n2022\n21,907 chromatin profiles + sequence classes\n4 kb\n\n\n\nWhile DeepSEA predicts regulatory intermediate phenotypes, ExPecto predicts the downstream transcriptional consequence. For GWAS variant prioritization, ExPecto predictions proved more effective than DeepSEA alone. Variants may alter chromatin features without affecting expression, but expression effects are more directly tied to phenotypic consequences.\nThe chromatin prediction component of ExPecto (Beluga) became the foundation for Sei (discussed in Chapter 5), which expanded chromatin targets to 21,907 profiles and introduced sequence class annotations for interpretability.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html#limitations-and-considerations",
    "href": "p2-ch06-transc.html#limitations-and-considerations",
    "title": "6  Transcriptional Effects",
    "section": "6.9 Limitations and Considerations",
    "text": "6.9 Limitations and Considerations\n\n6.9.1 Linear Expression Model\nWhile the chromatin CNN captures nonlinear sequence patterns, the final expression model is linear. This prevents modeling of complex regulatory logic such as synergistic interactions between elements, competitive binding or mutual exclusion, and threshold effects where element contributions are context-dependent. The choice was pragmatic: linear models require less data and offer interpretability, but may sacrifice predictive power for genes with complex regulatory logic.\n\n\n6.9.2 Context Window Constraints\nThe 40 kb promoter-proximal window misses distal enhancers operating over hundreds of kilobases, three-dimensional chromatin interactions that bring distant elements into proximity, and enhancer-promoter specificity (which enhancer regulates which gene among nearby alternatives).\n\n\n6.9.3 TSS-Centric Framework\nExPecto requires a defined TSS for each gene, potentially limiting predictions for genes with multiple alternative promoters, novel or unannotated transcription start sites, and tissue-specific promoter usage.\n\n\n6.9.4 Training Data Biases\nExpression models trained on GTEx, Roadmap, and ENCODE data inherit their biases. These include ancestry composition (GTEx is primarily European), tissue representation (some tissues well-covered, others sparse), and cell line artifacts (immortalized cells may not reflect primary tissue biology).",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch06-transc.html#significance-for-the-field",
    "href": "p2-ch06-transc.html#significance-for-the-field",
    "title": "6  Transcriptional Effects",
    "section": "6.10 Significance for the Field",
    "text": "6.10 Significance for the Field\nExPecto established several paradigms that influenced subsequent genomic deep learning.\nThe modular sequence-to-expression prediction architecture demonstrated the value of decomposing the problem into chromatin prediction, spatial integration, and expression modeling. This separation enables interpretability and component-wise improvement.\nAb initio variant effect prediction, achieved by training without variant data, avoids LD confounding and enables causal inference rather than mere association. This principle carries forward to later expression and variant effect models.\nScalable in silico mutagenesis showed that computational efficiency enables exhaustive characterization of mutational effects at genome scale, a capability that would be impossible experimentally.\nThe framework’s tissue-specific regulatory learning demonstrated that models can learn tissue-relevant regulatory features without explicit tissue labels for chromatin inputs, relying instead on the structure of expression data.\nFinally, the experimental validation standard set by ExPecto, demonstrating functional validation of computational predictions with reporter assays, established expectations for the field.\nThe framework demonstrated that deep learning could move beyond predicting intermediate molecular phenotypes (chromatin state) to predict cellular phenotypes (expression levels) directly from sequence. This progression from sequence to chromatin to expression to disease prefigured the increasingly ambitious goals of later genomic foundation models.\nExPecto’s public web portal (http://hb.flatironinstitute.org/expecto) and code release (https://github.com/FunctionLab/ExPecto) maintained the field’s norm of open tool availability established by DeepSEA. The framework continues to serve as a baseline for expression prediction methods and as a component in variant prioritization pipelines.\n\n\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transcriptional Effects</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html",
    "href": "p2-ch07-splice.html",
    "title": "7  Splicing Prediction",
    "section": "",
    "text": "7.1 The Splicing Challenge\nWhile DeepSEA and ExPecto (Chapters 5–6) addressed chromatin state and gene expression, a distinct class of functional variants operates through a different mechanism: disruption of pre-mRNA splicing. The spliceosome—the cellular machinery that removes introns and joins exons—achieves remarkable precision, recognizing the correct splice sites among millions of potential candidates in the human transcriptome. Yet the sequence determinants underlying this specificity remained incompletely understood, limiting interpretation of variants that might alter splicing.\nSpliceAI, introduced by Jaganathan et al. in 2019, demonstrated that deep neural networks could learn the sequence rules governing splicing with near-spliceosomal precision (Jaganathan et al. 2019). The model predicts splice site locations directly from pre-mRNA sequence, enabling identification of “cryptic splice” variants—mutations that create novel splice sites or disrupt existing ones in ways that evade traditional annotation-based detection.\nThe clinical implications are substantial: SpliceAI estimates that 9–11% of pathogenic mutations in rare genetic disorders act through cryptic splicing, representing a previously underappreciated class of disease variation.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#prior-approaches-and-limitations",
    "href": "p2-ch07-splice.html#prior-approaches-and-limitations",
    "title": "7  Splicing Prediction",
    "section": "7.2 Prior Approaches and Limitations",
    "text": "7.2 Prior Approaches and Limitations\nBefore SpliceAI, splice site prediction relied on methods with limited context:\n\nMaxEntScan: Models core splice motifs using maximum entropy, limited to ~9 bp context around donor/acceptor sites\nGeneSplicer: Combines Markov models with decision trees\nNNSplice: Early neural network approach with narrow receptive fields\n\nThese methods captured the essential GT (donor) and AG (acceptor) dinucleotides and surrounding consensus sequences, but could not model the long-range determinants—exon/intron length constraints, branch points, enhancers, and silencers—that contribute to splicing specificity. As a result, they produced many false positive predictions and missed variants acting through distal mechanisms.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#the-spliceai-architecture",
    "href": "p2-ch07-splice.html#the-spliceai-architecture",
    "title": "7  Splicing Prediction",
    "section": "7.3 The SpliceAI Architecture",
    "text": "7.3 The SpliceAI Architecture\nSpliceAI employs an ultra-deep residual convolutional network that integrates information across 10,000 nucleotides of sequence context—orders of magnitude more than prior methods.\n\n7.3.1 Residual Block Design\nThe architecture’s fundamental unit is the residual block, comprising batch normalization, ReLU activation, and dilated convolutions. Residual connections address the vanishing gradient problem that had limited earlier deep networks:\n\\[\n\\text{output} = \\text{input} + F(\\text{input})\n\\]\nwhere \\(F\\) represents the transformation learned by the convolutional layers. Skip connections from every fourth residual block feed directly to the penultimate layer, accelerating training convergence.\n\n\n7.3.2 Dilated Convolutions for Long-Range Context\nEach residual block uses dilated (atrous) convolutions parameterized by:\n\n\\(N\\): Number of convolutional kernels\n\\(W\\): Window size\n\\(D\\): Dilation rate\n\nA kernel with window size \\(W\\) and dilation rate \\(D\\) spans \\((W-1) \\cdot D\\) neighboring positions. The total receptive field \\(S\\) of the network is:\n\\[\nS = \\sum_{i=1}^{K} 2 \\cdot (W_i - 1) \\cdot D_i\n\\]\nwhere \\(K\\) is the number of residual blocks. By progressively increasing dilation rates through the network, SpliceAI achieves a 10,000 bp receptive field without the computational cost of processing 10,000 positions at full resolution.\n\n\n7.3.3 Architecture Variants\nFour architectures were developed with different context windows:\n\n\n\nModel\nFlanking Sequence\nTotal Context\nResidual Blocks\n\n\n\n\nSpliceAI-80nt\n40 bp each side\n80 bp\n4\n\n\nSpliceAI-400nt\n200 bp each side\n400 bp\n8\n\n\nSpliceAI-2k\n1,000 bp each side\n2,000 bp\n16\n\n\nSpliceAI-10k\n5,000 bp each side\n10,000 bp\n32\n\n\n\nThe 32-layer SpliceAI-10k model substantially outperformed shorter-context variants, demonstrating that long-range sequence features contribute meaningfully to splice site prediction.\n\n\n7.3.4 Output Format\nFor each nucleotide position, SpliceAI outputs three probabilities summing to one:\n\nProbability of being a splice acceptor (first nucleotide of an exon)\nProbability of being a splice donor (last nucleotide of an exon)\nProbability of being neither\n\nThe model operates in sequence-to-sequence mode: given an input of length \\(S/2 + l + S/2\\), it outputs predictions for the central \\(l\\) positions. This enables efficient batch processing where overlapping computations are shared.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#training-and-evaluation",
    "href": "p2-ch07-splice.html#training-and-evaluation",
    "title": "7  Splicing Prediction",
    "section": "7.4 Training and Evaluation",
    "text": "7.4 Training and Evaluation\n\n7.4.1 Training Data\nSpliceAI was trained on 20,287 protein-coding genes from GENCODE V24, selecting principal transcripts when multiple isoforms existed. The training/test split used odd versus even chromosomes:\n\nTraining: Chromosomes 2, 4, 6, 8, 10–22, X, Y (13,384 genes, 130,796 donor-acceptor pairs)\nTesting: Chromosomes 1, 3, 5, 7, 9—excluding genes with paralogs on training chromosomes (1,652 genes, 14,289 donor-acceptor pairs)\n\nThe paralog exclusion prevents information leakage through sequence homology.\nFor variant effect prediction, training was augmented with novel splice junctions commonly observed in GTEx RNA-seq data (adding ~67,000 donor and ~63,000 acceptor annotations), improving sensitivity for detecting splice-altering variants, particularly in deep intronic regions.\n\n\n7.4.2 Splice Site Prediction Performance\nSpliceAI-10k achieved:\n\nTop-k accuracy: 95% (at threshold where predicted sites equal actual sites)\nPR-AUC: 0.98\n\nFor comparison, MaxEntScan achieved only 57% top-k accuracy under equivalent conditions. The dramatic improvement reflects SpliceAI’s ability to reject false positive splice sites by considering sequence context beyond the core motif.\nNotably, performance improved substantially with context length (80 bp → 400 bp → 2,000 bp → 10,000 bp), confirming that distal sequence features contribute to splice site recognition.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#variant-effect-prediction",
    "href": "p2-ch07-splice.html#variant-effect-prediction",
    "title": "7  Splicing Prediction",
    "section": "7.5 Variant Effect Prediction",
    "text": "7.5 Variant Effect Prediction\n\n7.5.1 The Delta Score\nSpliceAI predicts variant effects by comparing splice site predictions for reference and alternative sequences:\n\\[\n\\Delta\\text{score} = \\max_{|p - v| \\leq 50} \\left| P_{\\text{alt}}(p) - P_{\\text{ref}}(p) \\right|\n\\]\nwhere \\(v\\) is the variant position and \\(p\\) ranges over positions within 50 bp of the variant. The maximum change across all positions captures variants that strengthen existing sites, weaken existing sites, or create entirely new splice sites.\nCritically, the model was trained only on reference transcript sequences and splice junction annotations—it never saw variant data during training. Variant effect prediction is thus a challenging test of whether the network learned genuine sequence determinants of splicing.\n\n\n7.5.2 Cryptic Splice Variant Classes\nSpliceAI detects several classes of splice-altering variants:\n\nDonor/acceptor loss: Disruption of annotated splice sites\nDonor/acceptor gain: Creation of novel splice sites\nExon skipping: Variants causing an exon to be spliced out\nIntron retention: Variants causing an intron to remain in mature mRNA\nCryptic exon activation: Deep intronic variants creating novel exons\n\nTraditional annotation-based methods can identify variants in the essential GT/AG dinucleotides but miss the broader landscape of cryptic splice variants operating through more subtle mechanisms.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#validation-on-gtex-rna-seq",
    "href": "p2-ch07-splice.html#validation-on-gtex-rna-seq",
    "title": "7  Splicing Prediction",
    "section": "7.6 Validation on GTEx RNA-seq",
    "text": "7.6 Validation on GTEx RNA-seq\nThe authors validated SpliceAI predictions using RNA-seq data from 149 GTEx individuals with matched whole-genome sequencing. Private variants (present in only one individual) predicted to alter splicing were tested for association with aberrant splice junctions.\n\n7.6.1 Validation Rates\nAt a Δ score threshold of ≥0.5, cryptic splice variants validated at three-quarters the rate of essential GT/AG splice disruptions:\n\n\n\nVariant Class\nValidation Rate\n\n\n\n\nEssential GT/AG disruption\n~100% (by definition)\n\n\nCryptic splice (Δ ≥ 0.8)\n~85%\n\n\nCryptic splice (Δ ≥ 0.5)\n~75%\n\n\nCryptic splice (Δ ≥ 0.2)\n~50%\n\n\n\nValidation rate and effect size both tracked closely with Δ score, confirming that the model’s confidence correlates with functional impact.\n\n\n7.6.2 Position-Dependent Sensitivity\nSensitivity varied by genomic location:\n\nNear exons (≤50 bp from exon-intron boundaries): 71% sensitivity at Δ ≥ 0.5\nDeep intronic (&gt;50 bp from boundaries): 41% sensitivity at Δ ≥ 0.5\n\nDeep intronic variants are more challenging because intronic regions contain fewer of the specificity determinants selected to be present near exons. Nevertheless, SpliceAI substantially outperformed prior methods in both regions.\n\n\n7.6.3 Comparison to Prior Methods\nBenchmarking against MaxEntScan, GeneSplicer, and NNSplice demonstrated SpliceAI’s superior performance across all operating points. At matched sensitivity, SpliceAI achieved higher validation rates; at matched validation rates, SpliceAI achieved higher sensitivity.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#population-genetics-evidence",
    "href": "p2-ch07-splice.html#population-genetics-evidence",
    "title": "7  Splicing Prediction",
    "section": "7.7 Population Genetics Evidence",
    "text": "7.7 Population Genetics Evidence\nBeyond RNA-seq validation, the authors assessed whether predicted cryptic splice variants show signatures of negative selection in human populations.\n\n7.7.1 Allele Frequency Depletion\nUsing ExAC/gnomAD data, high-confidence cryptic splice variants (Δ ≥ 0.8) showed 78% depletion at common allele frequencies compared to expectation—comparable to the 82% depletion observed for frameshift, stop-gain, and essential splice-disrupting variants. This indicates that most confidently predicted cryptic splice variants are functional and deleterious.\nThe depletion was stronger for variants predicted to cause frameshifts versus in-frame alterations, consistent with the expectation that frameshift-causing splice variants have more severe fitness consequences.\n\n\n7.7.2 Rare Variant Burden\nThe average human genome carries approximately:\n\n11 rare protein-truncating variants (allele frequency &lt;0.1%)\n5 rare functional cryptic splice variants\n\nCryptic splice variants outnumber essential GT/AG splice-disrupting variants roughly 2:1, highlighting the substantial mutational target space beyond canonical splice sites.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#de-novo-mutations-in-rare-disease",
    "href": "p2-ch07-splice.html#de-novo-mutations-in-rare-disease",
    "title": "7  Splicing Prediction",
    "section": "7.8 De Novo Mutations in Rare Disease",
    "text": "7.8 De Novo Mutations in Rare Disease\nThe central clinical finding of SpliceAI is that cryptic splice mutations constitute a major, previously underappreciated cause of rare genetic disorders.\n\n7.8.1 Case-Control Analysis\nThe authors analyzed de novo mutations in:\n\n4,293 individuals with intellectual disability (Deciphering Developmental Disorders cohort)\n3,953 individuals with autism spectrum disorders (Simons Simplex Collection + Autism Sequencing Consortium)\n2,073 unaffected sibling controls\n\nDe novo mutations predicted to disrupt splicing (Δ ≥ 0.1) were significantly enriched in affected individuals:\n\n\n\nCohort\nEnrichment vs. Controls\np-value\n\n\n\n\nIntellectual disability (DDD)\n1.51-fold\n4.2×10⁻⁴\n\n\nAutism spectrum disorder\n1.30-fold\n0.020\n\n\n\nThe enrichment remained significant when restricting to synonymous and intronic mutations, excluding the possibility that results were driven solely by variants with dual protein-coding and splicing effects.\n\n\n7.8.2 Fraction of Pathogenic Mutations\nBased on the excess of de novo mutations in cases versus controls:\n\n9% of pathogenic de novo mutations in intellectual disability act through cryptic splicing\n11% of pathogenic de novo mutations in autism act through cryptic splicing\n\nIn absolute terms, ~250 cases across the cohorts could be explained by de novo cryptic splice mutations, compared to ~909 cases explained by de novo protein-truncating variants.\n\n\n7.8.3 Clinical Penetrance\nCryptic splice mutations showed roughly 50% of the clinical penetrance of classic protein-truncating mutations (stop-gain, frameshift, essential splice). This reduced penetrance reflects that many cryptic splice variants are hypomorphic—producing a mixture of normal and aberrant transcripts rather than complete loss of function.\nWell-characterized examples from Mendelian disease support this interpretation: the c.315-48T&gt;C variant in FECH and c.-32-13T&gt;G in GAA are both hypomorphic cryptic splice alleles associated with milder phenotype or later age of onset.\n\n\n7.8.4 Novel Gene Discovery\nIncluding cryptic splice mutations in gene discovery analyses identified:\n\n5 additional candidate genes for intellectual disability\n2 additional candidate genes for autism\n\nThese genes would have fallen below the discovery threshold (FDR &lt;0.01) when considering only protein-coding mutations.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#experimental-validation-in-autism-patients",
    "href": "p2-ch07-splice.html#experimental-validation-in-autism-patients",
    "title": "7  Splicing Prediction",
    "section": "7.9 Experimental Validation in Autism Patients",
    "text": "7.9 Experimental Validation in Autism Patients\nTo directly validate predicted cryptic splice effects, the authors performed deep RNA-seq (~350 million reads per sample, ~10× GTEx coverage) on lymphoblastoid cell lines from 36 autism probands harboring predicted de novo cryptic splice mutations.\n\n7.9.1 Validation Results\nAmong 28 cases with adequate RNA-seq coverage at the gene of interest:\n\n21 (75%) showed unique aberrant splicing events associated with the predicted de novo variant\n7 (25%) showed no aberrant splicing in lymphoblastoid cells\n\nThe 75% validation rate is remarkable given that the relevant tissue (developing brain) was not accessible—some cryptic splice effects may be tissue-specific and not observable in blood-derived cells.\n\n\n7.9.2 Aberrant Splicing Classes\nAmong the 21 validated cases:\n\n\n\nSplicing Aberration\nCount\n\n\n\n\nNovel junction creation\n9\n\n\nExon skipping\n8\n\n\nIntron retention\n4\n\n\n\nThese aberrant events were absent from all other samples (the remaining 35 probands and 149 GTEx individuals), confirming their association with the predicted de novo variants.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#what-spliceai-learns",
    "href": "p2-ch07-splice.html#what-spliceai-learns",
    "title": "7  Splicing Prediction",
    "section": "7.10 What SpliceAI Learns",
    "text": "7.10 What SpliceAI Learns\nAnalysis of SpliceAI’s learned representations revealed that the network captures known splicing biology:\n\n7.10.1 Core Splice Motifs\nThe model correctly learned the essential GT donor and AG acceptor dinucleotides, plus surrounding consensus sequences. In silico mutagenesis of these positions produced the largest predicted effects.\n\n\n7.10.2 Branch Point Recognition\nIntroducing the optimal branch point sequence (TACTAAC) at varying distances from splice acceptors showed that SpliceAI learned the expected distance constraints (20–45 bp upstream of acceptors). At distances &lt;20 bp, the branch point disrupts the polypyrimidine tract, and SpliceAI correctly predicted reduced acceptor strength.\n\n\n7.10.3 Exonic Splicing Enhancers\nThe SR-protein binding motif GAAGAA, introduced at various positions, enhanced splice site strength when placed in expected locations within exons, demonstrating that SpliceAI learned the contribution of exonic splicing enhancers.\n\n\n7.10.4 Nucleosome Positioning\nNovel exon-creation events (where variants activate cryptic exons in introns) were significantly associated with existing nucleosome positioning, supporting a causal role for nucleosome occupancy in exon definition. SpliceAI implicitly captures this relationship despite not being trained on chromatin data.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#limitations-and-considerations",
    "href": "p2-ch07-splice.html#limitations-and-considerations",
    "title": "7  Splicing Prediction",
    "section": "7.11 Limitations and Considerations",
    "text": "7.11 Limitations and Considerations\n\n7.11.1 Tissue Specificity\nSpliceAI predicts splice sites based on sequence alone, without modeling tissue-specific alternative splicing. The same variant may have different effects across tissues depending on the expression of splicing factors and regulatory RNAs.\n\n\n7.11.2 Incomplete Penetrance\nMany cryptic splice variants produce partial shifts in splicing (alternative splicing) rather than complete disruption. The Δ score correlates with penetrance, but precise quantification of isoform ratios requires experimental validation.\n\n\n7.11.3 Deep Intronic Predictions\nWhile SpliceAI substantially improves deep intronic variant prediction over prior methods, sensitivity remains lower than for variants near exons. The 41% sensitivity (Δ ≥ 0.5) in deep intronic regions suggests that additional sequence features beyond the 10 kb context may contribute to splicing.\n\n\n7.11.4 Training on Canonical Transcripts\nTraining on principal transcripts may not fully capture the diversity of alternative splicing. Augmentation with RNA-seq-derived junctions improved performance, suggesting that expanded training data could further enhance predictions.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p2-ch07-splice.html#significance-for-the-field",
    "href": "p2-ch07-splice.html#significance-for-the-field",
    "title": "7  Splicing Prediction",
    "section": "7.12 Significance for the Field",
    "text": "7.12 Significance for the Field\nSpliceAI established several important contributions:\n\nClinical impact quantification: The estimate that 9–11% of pathogenic mutations act through cryptic splicing fundamentally changed understanding of the noncoding disease mutation landscape\nDeep context matters: The 32-layer, 10 kb context architecture demonstrated that splicing involves long-range sequence integration, motivating similar approaches in other genomic prediction tasks\nGenome-wide variant scoring: Precomputed Δ scores for all possible single nucleotide substitutions (available at https://github.com/Illumina/SpliceAI) enable routine clinical annotation\nValidation standards: The combination of RNA-seq validation, population genetics evidence, and case-control analysis established a rigorous framework for evaluating variant effect predictors\nSpecialized versus general models: SpliceAI’s success demonstrated that task-specific deep learning models could outperform general-purpose approaches by focusing computational capacity on a well-defined prediction problem\n\nSpliceAI has become a standard component of clinical variant interpretation pipelines, complementing protein-effect predictors and regulatory variant scores. The approach has influenced subsequent work on tissue-specific splicing prediction and integration of splicing effects into comprehensive variant effect models like Borzoi (Chapter 11).\nThe model’s code and precomputed scores are publicly available (https://github.com/Illumina/SpliceAI), enabling widespread adoption in both research and clinical settings.\n\n\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Splicing Prediction</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html",
    "href": "p3-ch08-tokens.html",
    "title": "8  Sequence Representation & Tokens",
    "section": "",
    "text": "8.1 From Sequence to Model: The Representation Problem\nEvery genomic deep learning model must answer a fundamental question: how should DNA sequence be represented as numerical input? The previous chapters employed one-hot encoding—a simple, lossless representation where each nucleotide becomes a 4-dimensional binary vector. This approach worked well for CNN-based models like DeepSEA (Chapter 5) and SpliceAI (Chapter 7), but the emergence of transformer-based language models introduced new considerations around tokenization, vocabulary design, and the trade-offs between sequence compression and resolution.\nThis chapter examines the evolution of sequence representation strategies, from one-hot encoding through k-mer tokenization to modern approaches including Byte Pair Encoding (BPE), single-nucleotide tokens, and biologically-informed tokenization schemes. The choice of representation profoundly affects what a model can learn, how efficiently it trains, and what context lengths it can practically achieve.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#one-hot-encoding-the-cnn-baseline",
    "href": "p3-ch08-tokens.html#one-hot-encoding-the-cnn-baseline",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.2 One-Hot Encoding: The CNN Baseline",
    "text": "8.2 One-Hot Encoding: The CNN Baseline\n\n8.2.1 Representation\nOne-hot encoding represents each nucleotide as a sparse binary vector:\n\n\n\nNucleotide\nVector\n\n\n\n\nA\n[1, 0, 0, 0]\n\n\nC\n[0, 1, 0, 0]\n\n\nG\n[0, 0, 1, 0]\n\n\nT\n[0, 0, 0, 1]\n\n\n\nA sequence of length \\(L\\) becomes a matrix of dimensions \\(4 \\times L\\), interpretable as 4 “channels” (like RGB channels in images, plus one).\n\n\n8.2.2 Advantages\nOne-hot encoding offers several properties that made it the default for CNN-based genomic models:\n\nLossless: No information is discarded; every nucleotide is explicitly represented\nSingle-nucleotide resolution: Enables detection of effects from individual SNPs\nTranslation equivariance: Convolutional filters learn position-invariant motifs\nSimplicity: No preprocessing, vocabulary construction, or tokenizer training required\n\n\n\n8.2.3 Limitations\nFor transformer architectures, one-hot encoding presents challenges:\n\nSequence length: A 10 kb sequence requires 10,000 tokens, straining attention’s \\(O(L^2)\\) complexity\nNo learned embeddings: Each nucleotide has a fixed, sparse representation rather than a learned dense embedding\nContext constraints: Practical transformer context windows of 512–4,096 tokens translate to only 512–4,096 bp—a tiny fraction of genes or regulatory regions",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#k-mer-tokenization-dnaberts-approach",
    "href": "p3-ch08-tokens.html#k-mer-tokenization-dnaberts-approach",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.3 K-mer Tokenization: DNABERT’s Approach",
    "text": "8.3 K-mer Tokenization: DNABERT’s Approach\n\n8.3.1 Concept\nK-mer tokenization treats overlapping subsequences of length \\(k\\) as tokens, analogous to words in natural language. DNABERT (2021) pioneered this approach for genomic transformers, using 6-mers (Ji et al. (2021)].\nFor a 6-mer vocabulary: - Vocabulary size: \\(4^6 = 4,096\\) possible tokens - Each token represents 6 consecutive nucleotides - Tokens overlap by \\(k-1 = 5\\) positions\n\n\n8.3.2 Overlapping vs. Non-Overlapping\nDNABERT used overlapping k-mers: for a sequence ACGTACGT, the 3-mer tokens would be:\nPosition:  1   2   3   4   5   6\nSequence:  A   C   G   T   A   C   G   T\n3-mers:   ACG CGT GTA TAC ACG CGT\nThis preserves positional information but creates computational redundancy—the sequence length in tokens equals the sequence length in nucleotides (minus \\(k-1\\)).\n\n\n8.3.3 Problems with K-mer Tokenization\nDNABERT-2 (2024) identified fundamental limitations of k-mer tokenization (Zhou et al. (2024)]:\n\nNo sequence compression: Overlapping k-mers don’t reduce sequence length, so context window limitations persist\nTokenization ambiguity: A single sequence position contributes to \\(k\\) different tokens, complicating variant effect interpretation\nSample inefficiency: The model must learn that overlapping tokens share nucleotides, rather than this being encoded in the representation\nComputational overhead: Processing \\(L\\) overlapping tokens for an \\(L\\)-bp sequence is no more efficient than one-hot encoding\nFixed vocabulary: The \\(4^k\\) vocabulary doesn’t adapt to corpus statistics; frequent and rare k-mers receive equal representation capacity",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#byte-pair-encoding-learning-the-vocabulary",
    "href": "p3-ch08-tokens.html#byte-pair-encoding-learning-the-vocabulary",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.4 Byte Pair Encoding: Learning the Vocabulary",
    "text": "8.4 Byte Pair Encoding: Learning the Vocabulary\n\n8.4.1 The BPE Algorithm\nByte Pair Encoding, originally a data compression algorithm, constructs a vocabulary by iteratively merging the most frequent adjacent token pairs in the training corpus:\n\nInitialize vocabulary with single nucleotides: {A, C, G, T}\nCount all adjacent token pairs in the corpus\nMerge the most frequent pair into a new token\nRepeat until desired vocabulary size is reached\n\nThis produces variable-length tokens that capture frequently occurring sequence patterns, achieving genuine sequence compression.\n\n\n8.4.2 DNABERT-2’s BPE Implementation\nDNABERT-2 replaced 6-mer tokenization with BPE, demonstrating substantial improvements (Zhou et al. (2024)]:\n\n21× fewer parameters than comparable k-mer models\n92× less GPU time in pretraining\nNon-overlapping tokens: Actual sequence compression, enabling longer effective context\n\nThe BPE vocabulary learns corpus statistics—repetitive elements, common motifs, and frequent sequence patterns receive dedicated tokens, while rare sequences are represented as shorter subunits.\n\n\n8.4.3 GROVER’s Custom BPE\nGROVER (Genome Rules Obtained Via Extracted Representations) trained BPE specifically on the human genome and selected vocabulary using a custom next-k-mer prediction task (Sanabria et al. (2024)]. Analysis revealed that learned token embeddings encode:\n\nFrequency: Common tokens cluster separately from rare ones\nSequence content: GC-rich versus AT-rich tokens segregate\nLength: Token length correlates with embedding dimensions\nGenomic localization: Some tokens appear primarily in repeats; others distribute broadly",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#single-nucleotide-tokenization-hyenadna",
    "href": "p3-ch08-tokens.html#single-nucleotide-tokenization-hyenadna",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.5 Single-Nucleotide Tokenization: HyenaDNA",
    "text": "8.5 Single-Nucleotide Tokenization: HyenaDNA\n\n8.5.1 The Case for Maximum Resolution\nWhile k-mer and BPE tokenization compress sequences, they sacrifice single-nucleotide resolution. A single nucleotide polymorphism (SNP) can completely alter protein function, yet multi-nucleotide tokens obscure the precise position and identity of variants.\nHyenaDNA (2023) took the opposite approach: single-nucleotide tokens with no compression (Nguyen et al. (2023)]. Each nucleotide (A, C, G, T) is a separate token, preserving:\n\nFull resolution: Every nucleotide is independently represented\nVariant precision: SNP effects can be isolated to specific tokens\nNo tokenization artifacts: No ambiguity about which token contains a variant\n\n\n\n8.5.2 Scaling to 1 Million Base Pairs\nThe challenge with single-nucleotide tokens is sequence length. A 1 Mb region requires 1 million tokens—far beyond standard transformer capacity. HyenaDNA addresses this through the Hyena architecture, which replaces attention with implicit convolutions that scale sub-quadratically:\n\n\n\nModel\nArchitecture\nMax Context\nComplexity\n\n\n\n\nDNABERT\nTransformer\n512 bp\n\\(O(L^2)\\)\n\n\nNucleotide Transformer\nTransformer\n6 kb\n\\(O(L^2)\\)\n\n\nHyenaDNA\nHyena\n1 Mb\n\\(O(L \\log L)\\)\n\n\n\nHyenaDNA achieved a 500× increase in context length over dense attention models while maintaining single-nucleotide resolution.\n\n\n8.5.3 Performance Characteristics\nOn Nucleotide Transformer benchmarks, HyenaDNA reached state-of-the-art on 12 of 18 datasets with orders of magnitude fewer parameters and less pretraining data. On GenomicBenchmarks, it surpassed prior state-of-the-art on 7 of 8 datasets by an average of +10 accuracy points.\nNotably, HyenaDNA demonstrated the first use of in-context learning in genomics—performing tasks based on examples provided in the context window without fine-tuning.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#biologically-informed-tokenization",
    "href": "p3-ch08-tokens.html#biologically-informed-tokenization",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.6 Biologically-Informed Tokenization",
    "text": "8.6 Biologically-Informed Tokenization\n\n8.6.1 The Central Dogma as Tokenization Guide: Life-Code\nStandard tokenization treats DNA as a homogeneous string, ignoring the biological reality that different genomic regions serve different functions. Coding sequences follow codon structure (3-nucleotide units encoding amino acids), while noncoding regions have no such constraint.\nLife-Code (2025) proposed codon-aware tokenization that respects the central dogma of molecular biology (Liu et al. (2025)]:\n\nCoding regions: Tokenized by codons (3-mers in reading frame)\nNoncoding regions: Tokenized by learned patterns\nIntegration: Unified framework spanning DNA, RNA, and protein\n\nThis approach enables Life-Code to: - Learn protein structure through knowledge distillation from protein language models - Capture interactions between coding and noncoding regions - Achieve state-of-the-art results across DNA, RNA, and protein tasks\n\n\n8.6.2 BioToken: Encoding Genomic Annotations\nBioToken (2025) extends tokenization beyond sequence content to include genomic structural annotations (Medvedev et al. (2025)]:\n\nVariant encoding: Tokens that explicitly represent SNPs, insertions, and deletions\nRegulatory annotations: Encoding of known regulatory elements\nFunctional context: Integration of gene structure, chromatin state, and other annotations\n\nBy incorporating biological inductive biases directly into the token representation, BioToken’s associated model (BioFM) achieves competitive or superior performance to specialized models (Enformer, SpliceAI) with significantly fewer parameters (265M).",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#the-context-length-evolution",
    "href": "p3-ch08-tokens.html#the-context-length-evolution",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.7 The Context Length Evolution",
    "text": "8.7 The Context Length Evolution\nThe history of genomic deep learning shows a consistent trend toward longer sequence context:\n\n\n\nEra\nRepresentative Models\nMax Context\nTokenization\n\n\n\n\n2015–2017\nDeepSEA, DeepBind\n1 kb\nOne-hot\n\n\n2018–2020\nExPecto, SpliceAI\n10–40 kb\nOne-hot\n\n\n2021\nDNABERT, Enformer\n512 bp – 200 kb\nK-mer / One-hot\n\n\n2022–2023\nNucleotide Transformer\n6 kb\nK-mer\n\n\n2023–2024\nHyenaDNA, Caduceus\n1 Mb\nSingle-nucleotide\n\n\n2025\nEvo 2\n1 Mb\nSingle-nucleotide (BPE)\n\n\n\nThis expansion reflects biological reality: regulatory elements can influence genes from hundreds of kilobases away, and understanding genome function requires integrating information across these distances.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#trade-offs-in-tokenization-design",
    "href": "p3-ch08-tokens.html#trade-offs-in-tokenization-design",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.8 Trade-offs in Tokenization Design",
    "text": "8.8 Trade-offs in Tokenization Design\n\n8.8.1 Compression vs. Resolution\n\n\n\nStrategy\nCompression\nResolution\nVariant Handling\n\n\n\n\nOne-hot\nNone\nSingle-nucleotide\nPrecise\n\n\nOverlapping k-mer\nNone\nK-nucleotide\nAmbiguous\n\n\nNon-overlapping k-mer\n~K×\nK-nucleotide\nFrame-dependent\n\n\nBPE\nVariable\nVariable\nContext-dependent\n\n\nSingle-nucleotide\nNone\nSingle-nucleotide\nPrecise\n\n\n\nHigher compression enables longer context but loses precision for variant effects. BPE offers a middle ground with adaptive compression, but variant positions relative to token boundaries can affect predictions.\n\n\n8.8.2 Vocabulary Size Considerations\n\n\n\nTokenization\nTypical Vocabulary Size\n\n\n\n\nOne-hot / Single-nucleotide\n4 (+ special tokens)\n\n\n6-mer\n4,096\n\n\nBPE (DNABERT-2)\n4,096–32,000\n\n\nCodon-aware\n~64 (codons) + noncoding\n\n\n\nLarger vocabularies increase embedding table size but may capture more complex patterns. Smaller vocabularies are parameter-efficient but require the model to learn compositional structure.\n\n\n8.8.3 Computational Efficiency\nFor a sequence of length \\(L\\) bp:\n\n\n\nTokenization\nTokens\nAttention Cost\n\n\n\n\nOne-hot\n\\(L\\)\n\\(O(L^2)\\)\n\n\nNon-overlapping k-mer\n\\(L/k\\)\n\\(O(L^2/k^2)\\)\n\n\nBPE (average compression \\(c\\))\n\\(L/c\\)\n\\(O(L^2/c^2)\\)\n\n\n\nBPE’s variable compression can achieve substantial speedups, but the benefit depends on corpus statistics and vocabulary size.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#implications-for-variant-effect-prediction",
    "href": "p3-ch08-tokens.html#implications-for-variant-effect-prediction",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.9 Implications for Variant Effect Prediction",
    "text": "8.9 Implications for Variant Effect Prediction\nTokenization choice directly affects variant effect prediction:\n\n8.9.1 Single-Nucleotide Tokens (HyenaDNA, Evo 2)\n\nReference and alternate alleles occupy the same token position\nEffects are precisely localized\nNo tokenization artifacts\n\n\n\n8.9.2 K-mer Tokens\n\nA single SNP changes \\(k\\) overlapping tokens\nMust aggregate effects across affected tokens\nBoundary effects if variant is at token junction\n\n\n\n8.9.3 BPE Tokens\n\nVariant may fall within a token or at token boundary\nEffect interpretation depends on token segmentation\nRe-tokenization may be needed for alternate allele\n\nFor clinical variant interpretation, single-nucleotide resolution is often preferred despite computational costs, as subtle genetic variations can have major phenotypic consequences.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#the-emerging-consensus",
    "href": "p3-ch08-tokens.html#the-emerging-consensus",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.10 The Emerging Consensus",
    "text": "8.10 The Emerging Consensus\nRecent developments suggest convergence toward:\n\nSingle-nucleotide resolution for maximum precision, enabled by sub-quadratic architectures (Hyena, Mamba, state space models)\nLearned embeddings rather than fixed one-hot vectors, allowing the model to discover meaningful nucleotide representations\nBiologically-informed augmentation where appropriate—encoding codons in coding regions, incorporating annotations, or using species-specific vocabularies\nHybrid approaches combining the efficiency of compression with resolution where needed\n\nThe choice ultimately depends on the task: variant effect prediction demands high resolution, while tasks like species classification or repeat annotation may benefit from compression.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch08-tokens.html#references-in-context",
    "href": "p3-ch08-tokens.html#references-in-context",
    "title": "8  Sequence Representation & Tokens",
    "section": "8.11 References in Context",
    "text": "8.11 References in Context\nThe models discussed in this chapter set the stage for the genomic language models covered in Chapter 10. Understanding tokenization choices clarifies why models like the Nucleotide Transformer use 6-mers (Dalla-Torre et al. 2023), why DNABERT-2 switched to BPE, and why HyenaDNA’s single-nucleotide approach enabled unprecedented context lengths. The hybrid architectures of Chapter 11 (Enformer, Borzoi) largely retained one-hot encoding for its precision, while the long-range models of Chapter 12 explore how sub-quadratic architectures enable single-nucleotide tokenization at genomic scale.\n\n\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sequence Representation & Tokens</span>"
    ]
  },
  {
    "objectID": "p3-ch09-plm.html",
    "href": "p3-ch09-plm.html",
    "title": "9  Protein Language Models",
    "section": "",
    "text": "9.1 Evolutionary Sequences as Natural Language\nBefore transformers revolutionized genomic sequence modeling, they first transformed our ability to model proteins. The success of protein language models (PLMs) established a paradigm that would later inspire genomic foundation models: treat biological sequences as a form of natural language, train large transformer models on massive unlabeled sequence databases, and extract functional knowledge through self-supervised learning.\nThe analogy between protein sequences and natural language runs deeper than mere metaphor. Both encode complex information in linear strings of discrete tokens (amino acids or words). Both exhibit hierarchical structure—motifs combine into domains as words combine into phrases. Both have syntax (structural constraints) and semantics (functional meaning). And crucially, both are shaped by evolutionary pressure: natural selection filters protein sequences just as cultural selection shapes language.\nThis chapter examines how protein language models pioneered biological foundation modeling, from the ESM family’s demonstration that transformers can learn protein structure and function from sequence alone, to their application in variant effect prediction and structure determination. Understanding PLMs provides essential context for the genomic language models covered in subsequent chapters, as many architectural choices and training strategies transfer directly from proteins to DNA.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch09-plm.html#the-esm-model-family",
    "href": "p3-ch09-plm.html#the-esm-model-family",
    "title": "9  Protein Language Models",
    "section": "9.2 The ESM Model Family",
    "text": "9.2 The ESM Model Family\n\n9.2.1 ESM-1b: Establishing the Paradigm\nThe Evolutionary Scale Modeling (ESM) project, developed at Meta AI Research, demonstrated that transformer language models trained on protein sequences learn biologically meaningful representations without explicit supervision (Rives et al. 2021).\nTraining data: ESM-1b was trained on UniRef50, a clustered database of ~33 million protein sequences covering the known diversity of protein families. UniRef50 clusters sequences at 50% identity, providing broad coverage while reducing redundancy.\nArchitecture: ESM-1b uses a BERT-style bidirectional transformer with 650 million parameters:\n\n\n\nComponent\nSpecification\n\n\n\n\nLayers\n33\n\n\nHidden dimension\n1,280\n\n\nAttention heads\n20\n\n\nParameters\n650M\n\n\nMax sequence length\n1,024 amino acids\n\n\n\nTraining objective: Masked language modeling (MLM)—the model learns to predict randomly masked amino acids given surrounding context. This is analogous to BERT’s masked token prediction, but operating on amino acids rather than words.\n\n\n9.2.2 What ESM Learns\nDespite never seeing structural or functional labels during training, ESM learns representations that capture:\nSecondary structure: Attention patterns in ESM correlate with alpha helices and beta sheets. The model implicitly learns that certain amino acid patterns form specific structural elements.\nContact prediction: ESM’s attention heads capture residue-residue contacts—amino acids that are distant in sequence but close in 3D space. This emergent capability suggests the model learns aspects of protein folding from sequence statistics alone.\nEvolutionary conservation: Masked token predictions correlate with position-specific conservation scores from multiple sequence alignments. ESM effectively learns which positions tolerate variation and which are constrained.\nFunctional sites: Attention concentrates on catalytic residues, binding sites, and other functionally important positions, even without explicit functional annotation.\n\n\n9.2.3 ESM-2: Scaling Up\nESM-2 extended the ESM approach with larger models and improved training (Lin et al. 2022):\n\n\n\nModel\nParameters\nLayers\nPerformance\n\n\n\n\nESM-2 (8M)\n8M\n6\nBaseline\n\n\nESM-2 (35M)\n35M\n12\n+5% contact prediction\n\n\nESM-2 (150M)\n150M\n30\n+8% contact prediction\n\n\nESM-2 (650M)\n650M\n33\n+12% contact prediction\n\n\nESM-2 (3B)\n3B\n36\n+15% contact prediction\n\n\nESM-2 (15B)\n15B\n48\nState-of-the-art\n\n\n\nPerformance scales smoothly with model size across structure prediction, contact prediction, and variant effect tasks—a phenomenon mirroring the scaling laws observed in natural language processing.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch09-plm.html#prottrans-alternative-architectures",
    "href": "p3-ch09-plm.html#prottrans-alternative-architectures",
    "title": "9  Protein Language Models",
    "section": "9.3 ProtTrans: Alternative Architectures",
    "text": "9.3 ProtTrans: Alternative Architectures\nThe ProtTrans family explored multiple transformer architectures for protein sequences:\nProtBERT: BERT-style bidirectional encoder trained on BFD (Big Fantastic Database), comprising ~2.1 billion protein sequences.\nProtT5: Encoder-decoder architecture based on T5, enabling both understanding and generation tasks.\nProtXLNet: XLNet-style permutation language modeling, capturing bidirectional context without the [MASK] token artifact.\nProtTrans models demonstrated that the protein language modeling paradigm generalizes across architectures. The choice between encoder-only (BERT-style) and encoder-decoder (T5-style) models depends on the downstream application: encoders excel at classification and embedding tasks, while encoder-decoders enable sequence generation.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch09-plm.html#esm-1v-zero-shot-variant-effect-prediction",
    "href": "p3-ch09-plm.html#esm-1v-zero-shot-variant-effect-prediction",
    "title": "9  Protein Language Models",
    "section": "9.4 ESM-1v: Zero-Shot Variant Effect Prediction",
    "text": "9.4 ESM-1v: Zero-Shot Variant Effect Prediction\nA critical application of protein language models is predicting the effects of amino acid substitutions—missense variants that are the most common type of protein-coding mutation.\n\n9.4.1 The Zero-Shot Approach\nESM-1v (2021) demonstrated that PLMs can predict variant effects without any training on variant labels. The approach exploits masked language modeling: for a variant at position \\(i\\) changing amino acid \\(a\\) to \\(b\\), compute:\n\\[\\Delta \\text{score} = \\log P(b | \\text{context}) - \\log P(a | \\text{context})\\]\nIf the model assigns higher probability to the mutant amino acid than the wild-type, the variant is predicted benign; if lower, deleterious. This “zero-shot” prediction requires no labeled training data—the model’s evolutionary knowledge, learned from sequence databases, directly informs variant interpretation.\n\n\n9.4.2 Genome-Wide Prediction\nBrandes et al. (2023) applied ESM-1b to predict effects for all ~450 million possible missense variants in the human genome (Brandes et al. 2023):\nScale: Every position × every possible substitution across all human proteins\nPerformance on ClinVar: ESM-1b outperformed existing methods in classifying ~150,000 ClinVar/HGMD missense variants as pathogenic or benign\nDeep mutational scanning: Strong correlation with experimental measurements across 28 DMS datasets\nIsoform-specific effects: ~2 million variants annotated as damaging only in specific protein isoforms, highlighting the importance of considering alternative splicing\nThis work established PLMs as practical tools for clinical variant interpretation, capable of scoring variants that lack experimental characterization or evolutionary homologs.\n\n\n9.4.3 Benchmarking on ProteinGym\nProteinGym provides a comprehensive benchmark for variant effect predictors, aggregating 217 deep mutational scanning assays covering diverse proteins (Notin et al. 2023):\n\n\n\nMethod\nMean Spearman ρ\n\n\n\n\nESM-1v\n0.48\n\n\nEVE (evolutionary model)\n0.46\n\n\nDeepSequence\n0.44\n\n\nPolyPhen-2\n0.32\n\n\nSIFT\n0.30\n\n\n\nPLMs achieve competitive or superior performance to methods that explicitly model evolutionary conservation from multiple sequence alignments, despite using only single sequences as input.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch09-plm.html#esmfold-structure-from-sequence",
    "href": "p3-ch09-plm.html#esmfold-structure-from-sequence",
    "title": "9  Protein Language Models",
    "section": "9.5 ESMFold: Structure from Sequence",
    "text": "9.5 ESMFold: Structure from Sequence\n\n9.5.1 From Language Model to Structure Predictor\nThe most dramatic demonstration of PLM capabilities came with ESMFold, which predicts protein 3D structure directly from ESM-2 embeddings (Lin et al. 2022).\nTraditional structure prediction (including AlphaFold2) relies heavily on multiple sequence alignments (MSAs)—computationally expensive searches against sequence databases that can take hours per protein. ESMFold eliminates this requirement:\nArchitecture: ESMFold couples ESM-2 (15B parameters) with a structure module adapted from AlphaFold2. The language model embeddings replace MSA-derived features.\nSpeed: ~60× faster than AlphaFold2 for typical proteins, enabling metagenomic-scale structure prediction\nAccuracy: Achieves atomic-level accuracy for many proteins, though slightly below AlphaFold2 for proteins that benefit from MSA information\n\n\n9.5.2 What This Reveals About PLMs\nESMFold’s success demonstrates that ESM-2’s internal representations encode sufficient information to determine 3D structure. The language model has learned not just local sequence patterns but global folding principles—what makes a sequence fold into a particular shape.\nThis has profound implications: the “attention” that transformers pay to distant sequence positions during masked prediction is, in some sense, learning the physics of protein folding. Residues that need to be close in 3D space attend to each other in the transformer’s attention matrices.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch09-plm.html#transfer-to-genomics-cadd-and-alphamissense",
    "href": "p3-ch09-plm.html#transfer-to-genomics-cadd-and-alphamissense",
    "title": "9  Protein Language Models",
    "section": "9.6 Transfer to Genomics: CADD and AlphaMissense",
    "text": "9.6 Transfer to Genomics: CADD and AlphaMissense\n\n9.6.1 CADD v1.7: PLM Features for Variant Prioritization\nThe Combined Annotation Dependent Depletion (CADD) framework integrates diverse annotations to score variant deleteriousness (Chapter 3). CADD v1.7 incorporated ESM-1v predictions as features (Schubach et al. 2024):\nIntegration approach: ESM-1v scores are computed for all missense variants and included alongside conservation scores, functional annotations, and regulatory predictions.\nPerformance gains:\n\n\n\nBenchmark\nCADD v1.6\nCADD v1.7\nImprovement\n\n\n\n\nClinVar pathogenic vs. common\n0.94\n0.95\n+1%\n\n\nDeep mutational scanning (31 datasets)\n0.78\n0.81\n+3%\n\n\n\nThe PLM features particularly improve scoring for variants in regions with limited evolutionary conservation data, where traditional methods struggle.\n\n\n9.6.2 AlphaMissense: Combining PLM and Structure\nAlphaMissense represents the state-of-the-art in missense variant effect prediction, combining PLM representations with structural context (Cheng et al. 2023):\nArchitecture: AlphaMissense adapts AlphaFold’s architecture, fine-tuning on human and primate variant population frequency databases. The model learns to predict pathogenicity by combining:\n\nSequence embeddings from ESM-style language modeling\nStructural context from predicted protein structures\nEvolutionary information from cross-species comparisons\n\nTraining data: Population frequency databases (gnomAD) provide weak labels—common variants are likely benign, absent variants may be deleterious. Critically, AlphaMissense never trains on clinical pathogenicity labels (ClinVar), yet achieves state-of-the-art performance on clinical benchmarks.\nScale: Predictions for all ~71 million possible single amino acid substitutions across the human proteome\nClassification: 89% of missense variants classified as either likely benign or likely pathogenic, providing actionable predictions for the vast majority of possible variants\n\n\n9.6.3 Performance Comparison\n\n\n\nMethod\nClinVar AUC\nDMS Correlation\nTraining Data\n\n\n\n\nSIFT\n0.78\n0.30\nConservation\n\n\nPolyPhen-2\n0.82\n0.32\nConservation + structure\n\n\nCADD v1.7\n0.95\n0.81\nMulti-feature integration\n\n\nESM-1v\n0.89\n0.48\nSequence only (zero-shot)\n\n\nAlphaMissense\n0.94\n0.52\nPLM + structure + population\n\n\n\nAlphaMissense achieves top performance by integrating the strengths of multiple approaches: PLM-derived sequence understanding, AlphaFold-derived structural context, and population genetics-derived evolutionary constraint signals.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch09-plm.html#lessons-for-genomic-language-models",
    "href": "p3-ch09-plm.html#lessons-for-genomic-language-models",
    "title": "9  Protein Language Models",
    "section": "9.7 Lessons for Genomic Language Models",
    "text": "9.7 Lessons for Genomic Language Models\nThe success of protein language models established several principles that inform genomic foundation modeling:\n\n9.7.1 Self-Supervision Works\nPLMs demonstrated that massive amounts of biological knowledge can be learned from unlabeled sequences. The same evolutionary pressures that shape proteins also shape DNA—purifying selection removes deleterious variants, leaving statistical signatures in sequence databases.\n\n\n9.7.2 Scale Matters\nPerformance improves predictably with model size, motivating the development of larger genomic models. The 8M → 15B parameter progression in ESM-2 showed consistent gains across tasks.\n\n\n9.7.3 Transfer Learning is Effective\nRepresentations learned for one task (masked token prediction) transfer to other tasks (structure prediction, variant effects). This suggests that self-supervised pretraining captures fundamental biological knowledge rather than task-specific shortcuts.\n\n\n9.7.4 Architecture Choices\nThe BERT-style bidirectional encoder proved highly effective for proteins, where the entire sequence context is available. However, genomic sequences present different challenges: much longer lengths (genes span kilobases, genomes span gigabases), different information density (proteins are information-dense, intergenic regions less so), and different symmetries (DNA has reverse-complement structure absent in proteins).\n\n\n9.7.5 Integration with Other Modalities\nAlphaMissense showed that PLM embeddings combine effectively with structural information. Similarly, genomic models benefit from integration with epigenomic data, gene annotations, and other biological context.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch09-plm.html#limitations-and-ongoing-challenges",
    "href": "p3-ch09-plm.html#limitations-and-ongoing-challenges",
    "title": "9  Protein Language Models",
    "section": "9.8 Limitations and Ongoing Challenges",
    "text": "9.8 Limitations and Ongoing Challenges\n\n9.8.1 Sequence Length\nMost PLMs handle sequences up to ~1,000–2,000 amino acids. While sufficient for most individual proteins, this limits modeling of large protein complexes and doesn’t directly transfer to the much longer sequences in genomics.\n\n\n9.8.2 Orphan Proteins\nPLMs struggle with proteins that have few homologs in training databases. “Orphan” or “dark” proteins—those unique to specific lineages—lack the evolutionary signal that PLMs exploit.\n\n\n9.8.3 Epistasis\nMost variant effect predictions assume independence—the effect of mutation A doesn’t depend on whether mutation B is present. Real proteins exhibit epistasis, where variant effects depend on sequence context.\n\n\n9.8.4 Interpretability\nWhile attention patterns correlate with biological features, understanding exactly what PLMs learn remains challenging. The field is developing interpretation methods (Chapter 17), but PLMs remain partially “black box.”",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch09-plm.html#significance",
    "href": "p3-ch09-plm.html#significance",
    "title": "9  Protein Language Models",
    "section": "9.9 Significance",
    "text": "9.9 Significance\nProtein language models established that transformer architectures can learn deep biological knowledge from sequence data alone. ESM’s ability to predict structure, function, and variant effects without explicit labels demonstrated the power of self-supervised learning on evolutionary data.\nThis success directly motivated the development of genomic language models. If proteins are a language that transformers can learn, perhaps DNA is too. The genomic language models covered in Chapter 10 adapt PLM architectures and training strategies to the distinct challenges of DNA sequences—longer contexts, different alphabets, and the full complexity of gene regulation.\nThe integration path also continues: just as CADD v1.7 and AlphaMissense incorporate PLM predictions, future models will integrate genomic and proteomic language models into unified frameworks (Chapter 13, Chapter 14). The central dogma of molecular biology—DNA → RNA → protein—suggests that models capturing all three modalities may achieve the deepest understanding of how genomes encode life.\n\n\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. “ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design.” Advances in Neural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html",
    "href": "p3-ch10-glm.html",
    "title": "10  DNA Foundation Models",
    "section": "",
    "text": "10.1 From Supervised CNNs to Self-Supervised Genomic LMs\nGenomic language models extend the ideas of protein language models (Chapter 9) to the DNA level: they treat genomes themselves as a “corpus,” learn statistical regularities through self-supervision, and reuse those representations for many downstream tasks.\nWhere Chapters 5–7 focused on supervised sequence-to-function CNNs and specialized architectures, and Chapter 8 focused on representation and tokenization, this chapter turns to DNA foundation models—large, often transformer-based or hybrid architectures trained on unlabeled genomic sequence at scale.\nThese models aim to provide a single, reusable backbone for tasks ranging from regulatory annotation and variant effect prediction to cross-species transfer and clinical prioritization. They mark the transition from “a model per dataset” to general-purpose genomic backbones analogous to BERT, GPT, and ESM in natural and protein language modeling.\nThe CNN era (DeepSEA, ExPecto, SpliceAI; Chapters 5–7) shared a common pattern:\nThis approach achieved remarkable performance but suffers from three fundamental constraints:\nProtein language models (Chapter 9) showed a different route: self-supervised learning on unlabeled sequences, with downstream tasks solved by probing or fine-tuning. Genomic language models import this recipe to DNA:\nThe promise is that once a sufficiently powerful backbone is trained, it becomes the default starting point for nearly any DNA-level prediction problem.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html#from-supervised-cnns-to-self-supervised-genomic-lms",
    "href": "p3-ch10-glm.html#from-supervised-cnns-to-self-supervised-genomic-lms",
    "title": "10  DNA Foundation Models",
    "section": "",
    "text": "Inputs: One-hot encoded DNA sequence around a locus\n\nTargets: Task-specific labels (chromatin marks, expression, splice junctions)\n\nObjective: Predict those labels using supervised loss functions\n\n\n\nLabel dependence – Every new assay, cell type, or phenotype requires new labeled data.\nTask coupling – Model design is tightly coupled to the task (e.g., splice-aware architectures, expression-distance kernels).\nLimited reuse – Features learned for one problem do not automatically transfer to others.\n\n\n\nData: Large collections of genomic sequences across species, individuals, or functional regions.\nObjectives:\n\nMasked language modeling (MLM): predict masked bases or tokens.\nNext-token or sequence modeling: predict the next token in a sequence.\nHybrid tasks: combine MLM with auxiliary objectives (e.g., predicting annotations).\n\nUsage modes:\n\nFreeze the model and train light-weight probes for specific tasks.\nFine-tune the entire model (or adapters) for specialized downstream tasks.\nUse zero-shot or few-shot scoring by comparing log-likelihoods of alternative sequences or alleles.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html#early-genomic-transformers-dnabert-and-dnabert-2",
    "href": "p3-ch10-glm.html#early-genomic-transformers-dnabert-and-dnabert-2",
    "title": "10  DNA Foundation Models",
    "section": "10.2 Early Genomic Transformers: DNABERT and DNABERT-2",
    "text": "10.2 Early Genomic Transformers: DNABERT and DNABERT-2\n\n10.2.1 DNABERT — BERT for k-merized DNA\nDNABERT applied the BERT masked language modeling framework to genomic sequences, using overlapping k-mers (e.g., 6-mers) as tokens and training on human reference sequences (Ji et al. 2021). As discussed in Chapter 8, this design had several defining characteristics:\n\nTokenization: Overlapping k-mers created a discrete vocabulary of size \\(4^k\\).\nObjective: Masked token prediction, exactly as in BERT.\nInput scale: Context windows of a few hundred base pairs (e.g., 512 tokens).\nDownstream evaluation: Fine-tuned on tasks such as promoter classification, splice site prediction, and transcription factor binding.\n\nDNABERT provided proof of concept that:\n\nSelf-supervised pretraining on raw DNA can improve performance over training from scratch.\nLearned embeddings capture biologically meaningful regularities, even when trained only on the reference genome.\nBERT-style architectures can be re-used across multiple downstream tasks.\n\nHowever, the k-mer design also introduced the limitations detailed in Chapter 8:\n\nNo true sequence compression—overlapping k-mers do not reduce sequence length.\nAmbiguity in positional interpretation—each nucleotide participates in multiple tokens.\nLimited context and scaling, due to quadratic attention and redundant overlapping tokens.\n\n\n\n10.2.2 DNABERT-2 — Toward Better Tokenization and Efficiency\nDNABERT-2 revisited both tokenization and architecture, highlighting how much representation matters for genomic LMs (Zhou et al. 2024).\nKey differences relative to DNABERT:\n\nTokenization: Improved schemes (e.g., BPE-style merges) that better compress redundancies and reduce sequence length.\nEfficiency: Models that scale to larger contexts and corpora without prohibitive memory costs.\nPerformance: Consistent gains on a range of seq2label genomics benchmarks over both DNABERT and non-pretrained baselines.\n\nDNABERT and DNABERT-2 collectively established that:\n\nSelf-supervision on DNA works and is competitive with hand-engineered pipelines.\nTokenization choices (Chapter 8) have large practical consequences.\nMasked LM training can produce reusable representations for diverse sequence tasks.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html#scaling-context-and-diversity-nucleotide-transformer",
    "href": "p3-ch10-glm.html#scaling-context-and-diversity-nucleotide-transformer",
    "title": "10  DNA Foundation Models",
    "section": "10.3 Scaling Context and Diversity: Nucleotide Transformer",
    "text": "10.3 Scaling Context and Diversity: Nucleotide Transformer\nDNABERT showed feasibility, but its context windows and training data were modest relative to the scale of genomes. Nucleotide Transformer pushed much further, emphasizing scale and diversity (Dalla-Torre et al. 2023):\n\nCorpus: Genomic data spanning multiple species and populations.\nModels: Transformer encoders of various sizes, from moderate to very large parameter counts.\nContext length: Up to ~6 kb per input sequence—an order-of-magnitude jump over DNABERT while still using dense attention (Chapter 8).\nTraining objective: Masked language modeling on subsequences sampled from genomes.\n\nThe Nucleotide Transformer work contributed several important ideas:\n\nCross-species pretraining\nTraining on many genomes (rather than a single reference) exposes the model to:\n\nDiverse sequence patterns and GC content.\nDifferent regulatory architectures.\nEvolutionary constraints that recur across lineages.\n\nThis mirrors the use of large multi-species multiple sequence alignments in protein LMs (Chapter 9) but operates at the raw DNA level.\nBenchmark suite\nTo quantify representation quality, Nucleotide Transformer introduced a benchmark panel of genomic tasks, commonly referred to in later work as the Nucleotide Transformer benchmarks (Dalla-Torre et al. 2023). Typical tasks include:\n\nPromoter and enhancer classification.\nHistone mark and chromatin accessibility prediction.\nVariant/pathogenicity proxies.\nRegulatory element type classification.\n\nModels are evaluated via linear probes, shallow classifiers, or light fine-tuning, providing a standard yardstick for later DNA LMs.\nScaling trends\nAs with protein and natural-language models, performance improves predictably with:\n\nLarger models.\nMore pretraining data.\nLonger context windows.\n\nThese scaling curves help forecast the returns from investing in even larger genomic LMs.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html#beyond-dense-attention-hyenadna-and-1-mb-context",
    "href": "p3-ch10-glm.html#beyond-dense-attention-hyenadna-and-1-mb-context",
    "title": "10  DNA Foundation Models",
    "section": "10.4 Beyond Dense Attention: HyenaDNA and 1 Mb Context",
    "text": "10.4 Beyond Dense Attention: HyenaDNA and 1 Mb Context\nQuadratic attention limits transformer context length to tens of kilobases at best, even with aggressive engineering. HyenaDNA replaces attention with a Hyena long-convolution / state-space architecture that scales sub-quadratically and can process sequences up to 1 Mb (Nguyen et al. 2023).\nAs summarized in Chapter 8:\n\n\n\nModel\nArchitecture\nMax context\nComplexity\n\n\n\n\nDNABERT\nTransformer\n512 bp\n\\(O(L^2)\\)\n\n\nNucleotide Transformer\nTransformer\n6 kb\n\\(O(L^2)\\)\n\n\nHyenaDNA\nHyena\n1 Mb\n\\(O(L \\log L)\\)\n\n\n\nHyenaDNA introduced several qualitative advances:\n\nMegabase-scale context\nProcessing 1 Mb windows allows the model to “see”:\n\nEntire gene bodies plus flanking regulatory regions.\nLong-range enhancer–promoter interactions.\nTopologically associating domain (TAD)-scale structure.\n\nThis aligns better with biological reality, where regulatory interactions often span tens to hundreds of kilobases.\nSingle-nucleotide resolution\nDespite its long context, HyenaDNA maintains base-level resolution, so single-nucleotide variants can be evaluated in the context of megabases of surrounding sequence.\nIn-context learning signals\nOn Nucleotide Transformer benchmarks and additional tasks, HyenaDNA shows in-context learning behaviors—performance improves when examples are included in the input context without updating model weights (Nguyen et al. 2023). This suggests that at sufficient scale, DNA models can adapt to new tasks or distributions via prompts rather than fine-tuning, mirroring phenomena seen in large language models.\nState-of-the-art performance\nOn GenomicBenchmarks and related evaluations, HyenaDNA achieves state-of-the-art results on the majority of tasks, often by large margins, illustrating that architectural innovations can yield both longer context and better predictive accuracy (Nguyen et al. 2023).",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html#generative-regulatory-foundation-models-grover",
    "href": "p3-ch10-glm.html#generative-regulatory-foundation-models-grover",
    "title": "10  DNA Foundation Models",
    "section": "10.5 Generative Regulatory Foundation Models: GROVER",
    "text": "10.5 Generative Regulatory Foundation Models: GROVER\nMost models above focus on sequence-level objectives (masked or next-token). GROVER shifts attention from sequence to regulatory tracks (Sanabria et al. 2024):\n\nInputs/outputs: GROVER is trained on multi-track functional genomics signals (e.g., ATAC-seq, histone marks) across many cell types and tissues instead of raw sequence alone.\nObjective: Predict masked or held-out regulatory profiles conditioned on neighboring tracks, cell-type embeddings, or limited sequence context.\nArchitecture: A transformer-style backbone tailored to spatiotemporal grids of genomic positions × assays × cell types.\n\nGROVER plays a role analogous to self-supervised vision models for images:\n\nIt treats regulatory profiles as a high-dimensional “image” over the genome.\nIt learns rich representations of regulatory states at each position.\nIt supports tasks like imputation of missing assays, denoising, and cell-type-specific activity prediction.\n\nWhile not a pure DNA language model, GROVER-style systems complement sequence-based LMs:\n\nDNA LMs capture what the genome can do (the encoded potential).\nRegulatory LMs like GROVER capture what the genome is actually doing in specific contexts (cell types, conditions).\n\nLater chapters (Part IV) explore how sequence-based and regulatory foundation models can be combined—e.g., using DNA LMs to parameterize sequence priors and regulatory LMs for context-specific readouts (Sanabria et al. 2024).",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html#central-dogma-aware-and-annotation-enriched-models",
    "href": "p3-ch10-glm.html#central-dogma-aware-and-annotation-enriched-models",
    "title": "10  DNA Foundation Models",
    "section": "10.6 Central-Dogma-Aware and Annotation-Enriched Models",
    "text": "10.6 Central-Dogma-Aware and Annotation-Enriched Models\nChapter 8 discussed how tokenization can encode biological structure. Some recent models push this further by integrating central dogma and genomic annotations directly into the modeling framework.\n\n10.6.1 Life-Code: Central Dogma as an Inductive Bias\nLife-Code proposes codon-aware, central-dogma-informed tokenization to bridge DNA, RNA, and protein within a single language-modeling framework (Liu et al. 2025):\n\nCoding regions: Tokenized as codons (3-mers in frame), reflecting the genetic code.\nNoncoding regions: Tokenized via learned subword units.\nIntegration: Unified representations span DNA → RNA → protein, enabling knowledge sharing across modalities.\n\nLife-Code uses distillation from protein LMs (Chapter 9) to:\n\nImport protein-level structural knowledge into DNA/RNA sequence representations.\nImprove performance on tasks involving coding sequence, such as predicting missense effects or expression changes.\nAchieve competitive or state-of-the-art results on tasks across the full central dogma (DNA, RNA, protein) (Liu et al. 2025).\n\n\n\n10.6.2 BioToken: Encoding Variants and Structure as Tokens\nBioToken extends tokenization beyond nucleotide content to include explicit genomic annotations (Medvedev et al. 2025):\n\nVariant-aware tokens: Encode SNPs, insertions, and deletions as distinct tokens rather than implicit changes in sequence.\nStructural annotations: Incorporate information about exons, introns, UTRs, promoters, enhancers, and other regulatory elements.\nFunctional context: Include signals such as chromatin state, conservation scores, or known regulatory motifs.\n\nThis design moves toward fully structured genomic LMs, where:\n\nThe input “sentence” is not only DNA bases but also position-specific metadata.\nRepresentations can directly integrate sequence, structure, and functional annotations.\n\nLife-Code and BioToken foreshadow the multi-modal, multi-omic foundation models discussed in Part IV, where sequence is only one of many integrated information streams.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html#using-genomic-lms-in-practice",
    "href": "p3-ch10-glm.html#using-genomic-lms-in-practice",
    "title": "10  DNA Foundation Models",
    "section": "10.7 Using Genomic LMs in Practice",
    "text": "10.7 Using Genomic LMs in Practice\nJust as protein LMs can be used in different modes (frozen embeddings, fine-tuning, zero-shot scoring; Chapter 9), genomic LMs have multiple usage patterns.\n\n10.7.1 Embeddings as Universal Features\nThe simplest way to use a genomic LM is to:\n\nExtract embeddings for windows around loci of interest (e.g., 1–6 kb segments).\nPool or select positions relevant to the task (e.g., promoters, candidate enhancers, variant sites).\nTrain a light-weight downstream model (linear layer, small MLP, or logistic regression).\n\nApplications include:\n\nRegulatory element classification: Distinguishing promoters, enhancers, silencers, and insulators.\nChromatin state prediction: Predicting ATAC-seq or histone mark presence from sequence alone, as an alternative to models like DeepSEA (Chapter 5).\nVariant effect scoring: Replacing or augmenting hand-crafted features in frameworks like CADD with LM-derived features (analogous to CADD v1.7’s use of PLM features; Chapter 9; Schubach et al. (2024)).\nSplicing and transcript modeling: Combining LM embeddings with splice-aware architectures like SpliceAI (Chapter 7).\n\nBecause the LM is frozen, this approach is computationally efficient and avoids catastrophic forgetting when new tasks are added.\n\n\n10.7.2 Fine-Tuning and Task-Specific Heads\nWhen more labels are available, fine-tuning can significantly improve performance:\n\nFull fine-tuning: Update all LM parameters for a specific task.\nAdapter-based tuning: Insert small bottleneck modules into each layer and update only those, keeping the backbone mostly frozen.\nPrompt-based tuning: Learn task-specific prompts or prefix embeddings that steer the model’s behavior without changing its core weights.\n\nFine-tuning is especially valuable for:\n\nHigh-stakes clinical tasks where every percentage point matters.\nTasks that probe very specific sequence-function relationships (e.g., particular TF binding specificities).\nScenarios where domain shift is large (e.g., applying a cross-species LM to a previously unseen clade).\n\n\n\n10.7.3 Zero-Shot and Few-Shot Variant Scoring\nAnalogous to protein models like ESM-1v and AlphaMissense (Chapter 9; Brandes et al. (2023); Cheng et al. (2023)), genomic LMs can be used to compute zero-shot variant scores by:\n\nComparing the log-likelihood (or pseudo-likelihood) of sequences with reference vs alternate alleles.\nMeasuring changes in masked-token prediction probabilities at variant positions.\nEvaluating the impact of a variant on internal representations (e.g., vector distances between reference and variant embeddings).\n\nThese approaches can:\n\nProvide rapid prioritization of novel variants without task-specific training.\nComplement supervised classifiers trained on clinical or functional labels (e.g., ClinVar, curated datasets).\nOffer a starting point for more specialized models (e.g., exon-specific splice models, enhancer-specific expression predictors).",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html#evaluation-benchmarks-and-pitfalls",
    "href": "p3-ch10-glm.html#evaluation-benchmarks-and-pitfalls",
    "title": "10  DNA Foundation Models",
    "section": "10.8 Evaluation, Benchmarks, and Pitfalls",
    "text": "10.8 Evaluation, Benchmarks, and Pitfalls\nAs genomic LMs proliferate, evaluation practices become crucial.\n\n10.8.1 Benchmark Suites\nNucleotide Transformer introduced a widely used benchmark panel (Dalla-Torre et al. 2023), and later work, including HyenaDNA and Life-Code, also reports results on GenomicBenchmarks and related collections (Nguyen et al. 2023). Common traits of these suites include:\n\nMultiple task families:\n\nPromoter/enhancer classification.\nTF binding prediction.\nChromatin accessibility and histone marks.\nSplicing, TSS/TES prediction, or other sequence-label tasks.\n\nStandardized splits:\n\nTrain/validation/test partitions.\nConsistent evaluation metrics (AUROC, AUPRC, accuracy).\n\nBaseline comparisons:\n\nNon-pretrained CNNs and transformers.\nEarlier models like DeepSEA, ExPecto, and SpliceAI.\nPreviously published genomic LMs.\n\n\nThese benchmarks help separate true representational gains from gains due to dataset choice or training tricks.\n\n\n10.8.2 Distribution Shift, Data Leakage, and Overfitting\nGenomic LMs are especially vulnerable to distribution shift: they may be pretrained on one mix of species, assays, or cohorts and then applied to a very different context (new genome builds, experimental protocols, or patient populations). The general principles and evaluation strategies for robustness to these shifts are covered in detail in Chapter 15; here we mainly note that GLM benchmarks should explicitly include “out-of-domain” settings (e.g., new tissues or cohorts) rather than only i.i.d. held-out sequences.\nBecause many genomic resources are reused across pretraining, fine-tuning, and evaluation, data leakage and overfitting can easily inflate retrospective performance, especially for GLMs trained on massive unlabeled corpora and then evaluated on derived benchmarks. Systematic treatment of leakage paths, circularity, and confounding—along with practical mitigation strategies—is given in Chapter 16.\nFinally, when GLM-derived features feed into clinical prediction pipelines (e.g., risk scores or variant prioritization tools), the relevant notion of “performance” becomes clinical: discrimination, calibration, and net benefit for specific decisions. These clinical evaluation criteria, and how to connect model scores to real-world decisions, are discussed in Chapter 18.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch10-glm.html#lessons-and-outlook",
    "href": "p3-ch10-glm.html#lessons-and-outlook",
    "title": "10  DNA Foundation Models",
    "section": "10.9 Lessons and Outlook",
    "text": "10.9 Lessons and Outlook\nDNA language models bring the “foundation model” paradigm to the genome. Several themes emerge:\n\nRepresentation is central\nTokenization and context length (Chapter 8) are not superficial implementation details—they determine what patterns a model can see and how efficiently it can learn. Life-Code and BioToken show that biologically informed tokenization and annotations can serve as powerful inductive biases (Liu et al. 2025; Medvedev et al. 2025).\nScale and diversity matter\nNucleotide Transformer and HyenaDNA demonstrate that performance improves with both model size and training data diversity (Dalla-Torre et al. 2023; Nguyen et al. 2023). Including multiple species, populations, and genomic contexts yields more robust representations.\nLong-range context is biologically necessary\nMany regulatory phenomena operate at tens to hundreds of kilobases. Megabase-scale models like HyenaDNA show that we can finally begin to model these interactions at single-base resolution in a single forward pass.\nSelf-supervision and supervision are complementary\nSelf-supervised LMs excel at learning broad, reusable features, but they do not automatically solve every downstream problem. Specialized architectures and supervised objectives (e.g., Enformer and related models in Chapter 11) remain crucial for accurate quantitative prediction of complex genomic readouts.\nIntegration with other modalities is the next frontier\nModels like GROVER, Life-Code, and BioToken hint at a future where DNA LMs are one part of larger multi-modal genomic foundation models that integrate:\n\nSequence (DNA, RNA, protein).\nRegulatory profiles (chromatin, expression).\n3D genome organization.\nPopulation genetics, phenotypes, and clinical data.\n\n\nThis chapter has focused on sequence-centric DNA LMs and their immediate extensions. In Chapter 11, we turn to Enformer and related long-range sequence-to-function models that explicitly predict molecular readouts from sequence, closing the loop between self-supervised sequence understanding and supervised functional prediction.\n\n\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>DNA Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-hybrid.html",
    "href": "p3-ch11-hybrid.html",
    "title": "11  Long-range Hybrid Model",
    "section": "",
    "text": "11.1 Why Expression Needs Long-Range Models\nExPecto (Chapter 6) showed that gene expression can be predicted ab initio from sequence by combining a CNN-based chromatin model (Beluga) with a separate regression layer mapping chromatin features to expression across tissues (Zhou et al. 2018). This modular strategy worked surprisingly well, but it inherited two key limitations from its DeepSEA-style backbone (Chapter 5):\nAs genomic datasets grew (ENCODE, Roadmap, FANTOM, GTEx, and others; Chapter 2), it became clear that:\nPure CNN architectures can expand their receptive field using dilated convolutions and pooling, but doing so at single-nucleotide resolution quickly becomes parameter- and memory-intensive. On the other hand, classic transformer architectures can model long-range dependencies via attention, but their \\(O(L^2)\\) runtime and memory makes naïve application to 200 kb sequences infeasible (Chapter 10).\nHybrid architectures like Enformer and Borzoi emerged as a compromise:\nThis chapter focuses on these hybrid designs—particularly Enformer (Avsec et al. 2021) and Borzoi (Linder et al. 2025)—and how they changed what “sequence-to-expression” models can do.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Long-range Hybrid Model</span>"
    ]
  },
  {
    "objectID": "p3-ch11-hybrid.html#why-expression-needs-long-range-models",
    "href": "p3-ch11-hybrid.html#why-expression-needs-long-range-models",
    "title": "11  Long-range Hybrid Model",
    "section": "",
    "text": "Restricted context: A 40 kb input window captures proximal promoters and some nearby enhancers, but many regulatory interactions span 100 kb or more.\nTwo-stage learning: Chromatin prediction and expression prediction are trained separately, leaving no opportunity for the expression objective to shape the representation of sequence.\n\n\n\nEnhancers can regulate genes hundreds of kilobases away.\neQTLs often sit outside promoter windows traditionally used for expression models.\nChromatin conformation (loops, TADs) introduces non-local dependencies between DNA segments.\n\n\n\n\nUse convolutions to extract local motif features and progressively downsample the sequence into a manageable number of latent positions.\nApply self-attention over this compressed representation to capture long-range regulatory interactions across ~100–200 kb.\nPredict many signals at once (chromatin profiles, transcription start site activity, RNA-seq coverage), enabling multi-task learning and rich variant effect prediction.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Long-range Hybrid Model</span>"
    ]
  },
  {
    "objectID": "p3-ch11-hybrid.html#problem-setting-sequence-to-expression-at-scale",
    "href": "p3-ch11-hybrid.html#problem-setting-sequence-to-expression-at-scale",
    "title": "11  Long-range Hybrid Model",
    "section": "11.2 Problem Setting: Sequence-to-Expression at Scale",
    "text": "11.2 Problem Setting: Sequence-to-Expression at Scale\nThe models in this chapter tackle a demanding version of the classic problem:\n\nGiven a long DNA sequence window around a genomic locus, predict a rich set of regulatory and transcriptional readouts across many cell types.\n\n\n11.2.1 Inputs\n\nDNA sequence: One-hot encoded sequence:\n\nLength: typically ~200 kb centered on a candidate promoter or gene.\nAlphabet: A/C/G/T (N masked or handled by special channels).\n\nPositional indexing:\n\nThe model must know where promoter-proximal bases and distal elements are, relative to each other.\nPositional information is encoded via convolutional receptive fields and/or explicit positional embeddings for the attention layers.\n\n\n\n\n11.2.2 Outputs\nEnformer and Borzoi are both multi-task, multi-position sequence-to-signal models:\n\nMultiple assays:\n\nDNase/ATAC-seq (chromatin accessibility)\nHistone marks (e.g., H3K4me3, H3K27ac, etc.)\nCAGE or RNA-seq signals related to transcription and expression.\n\nMultiple cell types / conditions:\n\nHundreds of tracks, each representing a signal in a particular cell type or experimental context.\n\nMultiple positions along the window:\n\nPredictions are made at fixed strides across the input window (e.g., every 128 or 256 bp), yielding a coverage track rather than a single scalar.\n\n\n\n\n11.2.3 Loss Functions\nTypical objective:\n\nPer-track, per-position regression:\n\nOften a Poisson or negative binomial likelihood on read counts.\nSometimes log-transformed counts with a mean-squared error loss.\n\nMulti-task weighting:\n\nAll tracks contribute to the loss.\nSome models tune weights to prevent abundant assays (e.g., DNase) from dominating scarce but important ones (e.g., rare histone marks).\n\n\nThe learning problem is thus:\n\\[\nf_\\theta: \\text{DNA sequence (≈200 kb)} \\rightarrow \\text{[Tracks × Positions] continuous outputs}\n\\]\nwith \\(\\theta\\) shared across assays, cell types, and genomic loci.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Long-range Hybrid Model</span>"
    ]
  },
  {
    "objectID": "p3-ch11-hybrid.html#enformer-cnn-attention-for-200-kb-context",
    "href": "p3-ch11-hybrid.html#enformer-cnn-attention-for-200-kb-context",
    "title": "11  Long-range Hybrid Model",
    "section": "11.3 Enformer: CNN + Attention for 200 kb Context",
    "text": "11.3 Enformer: CNN + Attention for 200 kb Context\nEnformer (Avsec et al. 2021) is a landmark model that directly integrates long-range sequence context with cell-type-specific expression prediction, using a hybrid CNN–transformer architecture.\n\n11.3.1 Architectural Overview\nConceptually, Enformer has three stages:\n\nConvolutional stem:\nExtract local motifs and progressively downsample the sequence.\nTransformer trunk:\nApply self-attention to model long-range dependencies between downsampled positions.\nHeads for multi-task outputs:\nDecode the attended representation into assay- and cell-type-specific coverage tracks.\n\nA high-level architecture table is:\n\n\n\n\n\n\n\n\nStage\nFunction\nKey Characteristics\n\n\n\n\nCNN stem\nLocal motif extraction, downsampling\nResidual + dilated convs, pooling\n\n\nTransformer blocks\nLong-range dependency modeling\nMulti-head self-attention, MLPs\n\n\nOutput heads\nPredict assays across positions & cells\nTask-specific linear projections\n\n\n\n\n11.3.1.1 1. Convolutional Stem\nThe convolutional front-end:\n\nTakes ~200 kb one-hot sequence as input.\nApplies stacked conv–norm–nonlinearity–pooling layers.\nExpands receptive field while downsampling length by a large factor (e.g., 128–256×).\n\nThe resulting representation can be viewed as:\n\nA sequence of \\(L'\\) latent tokens (\\(L' \\ll L\\)), each summarizing a multi-kilobase region.\nEach token encodes local motif configurations and short-range regulatory patterns.\n\nThis step solves the “attention on raw nucleotides” problem by:\n\nReducing a 200,000 bp sequence into, say, ~1,000–2,000 tokens.\nAllowing attention to operate at a much lower effective resolution.\n\n\n\n11.3.1.2 2. Transformer Trunk\nEnformer then applies several transformer blocks over the compressed sequence:\n\nMulti-head self-attention:\n\nEvery downsampled position can attend to every other position.\nCaptures relationships between distant enhancers and promoters, or between multiple regulatory elements.\n\nFeed-forward networks (MLPs):\n\nNonlinear mixing of information at each position.\n\nResidual connections and normalization:\n\nStabilize training and enable deep stacks.\n\n\nIntuitively:\n\nConvolution layers answer:\n“What motifs and local patterns exist in this region?”\nAttention layers answer:\n“How do these regions interact across the 200 kb window to shape regulatory activity?”\n\n\n\n11.3.1.3 3. Multi-Task Output Heads\nAfter attention, Enformer:\n\nApplies task-specific heads to each position in the latent sequence.\nProduces coverage predictions for each assay × cell type combination.\n\nFor CAGE-based transcription start site (TSS) activity:\n\nThe model predicts coverage around TSS positions.\nGene-level expression metrics can be obtained by aggregating predictions at positions near annotated TSSs (e.g., summing or averaging log counts across a small window).\n\n\n\n\n11.3.2 Training Data and Objective\nEnformer is trained on a large collection of human and mouse regulatory datasets:\n\nHuman:\n\nDNase, histone ChIP-seq, and CAGE across many cell types.\n\nMouse:\n\nAnalogous assays used for cross-species learning.\n\n\nKey design choices:\n\nJoint human–mouse training:\n\nEncourages the model to learn regulatory principles conserved across mammals.\nEnables zero-shot transfer between species for some tasks.\n\nChromosome holdout:\n\nEntire chromosomes held out for evaluation to avoid overly optimistic performance via local sequence similarity.\n\n\nThe loss aggregates over:\n\nAll targets (tracks).\nAll positions in the output window.\nAll training loci.\n\n\n\n11.3.3 Enformer as a Variant Effect Predictor\nLike DeepSEA, Enformer can be used for in silico variant effect prediction:\n\nExtract a 200 kb window around a locus from the reference genome.\nRun Enformer to obtain predicted coverage tracks.\nIntroduce an alternative allele (e.g., SNP) into the window.\nRe-predict coverage and compute Δ-prediction:\n\\[\n\\Delta \\text{signal} = f_\\theta(\\text{alt sequence}) - f_\\theta(\\text{ref sequence})\n\\]\nAggregate Δ-predictions around TSSs to quantify predicted expression change for genes in each cell type.\n\nThis approach allows:\n\nFine-grained assessment of how a variant might alter promoter-proximal signals and distal enhancer contributions.\nIntegration into downstream tools (e.g., fine-mapping pipelines) that require variant-level scores.\n\n\n\n11.3.4 eQTL Validation via GTEx\nEnformer’s variant effect predictions were systematically evaluated using GTEx eQTL data (Chapter 2):\n\nFor each gene–tissue pair:\n\nKnown eQTLs (lead variants) and non-eQTL variants in LD were compared.\n\nSigned LD profile (SLDP) regression:\n\nCorrelates predicted expression effects with observed eQTL effect sizes, accounting for LD structure.\n\nFindings (Avsec et al. 2021):\n\nEnformer’s predictions showed stronger alignment with observed eQTLs than prior models like Basenji2 (a purely convolutional long-range model).\nImprovement was especially notable at distal regulatory variants, where long-range attention is crucial.\n\n\nIn practice, this means Enformer:\n\nCan prioritize variants likely to be causal eQTLs.\nProvides cell-type-specific effect predictions, which are critical for interpreting tissues with sparse experimental data.\n\n\n\n11.3.5 Interpretation and Mechanistic Insight\nWhile Enformer is a complex model, several interpretation strategies provide mechanistic insight:\n\nGradient-based attribution:\n\nCompute gradients of gene-level expression predictions with respect to input sequence.\nHighlight bases or motifs that drive the predicted expression of a gene in a specific cell type.\n\nIn silico mutagenesis:\n\nSystematically mutate bases to estimate their impact on a target gene or track.\nIdentify enhancers and key transcription factor binding sites controlling expression.\n\nPerturbation of attention:\n\nAnalyze which positions attend most strongly to a promoter, revealing candidate long-range enhancers.\n\n\nThese tools have been used to:\n\nMap promoter–enhancer interactions directly from sequence.\nSuggest causal regulatory elements for disease-associated variants.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Long-range Hybrid Model</span>"
    ]
  },
  {
    "objectID": "p3-ch11-hybrid.html#borzoi-transcriptome-centric-hybrid-modeling",
    "href": "p3-ch11-hybrid.html#borzoi-transcriptome-centric-hybrid-modeling",
    "title": "11  Long-range Hybrid Model",
    "section": "11.4 Borzoi: Transcriptome-Centric Hybrid Modeling",
    "text": "11.4 Borzoi: Transcriptome-Centric Hybrid Modeling\nEnformer is primarily trained on chromatin and CAGE profiles. Borzoi (Linder et al. 2025) extends the hybrid architecture paradigm to model the RNA transcriptome itself, with an emphasis on finer-grained transcriptional features.\n\n11.4.1 Motivation\nRNA-seq data carries richer information than a single expression scalar per gene:\n\nCoverage along exons and introns:\n\nReflects transcription initiation, elongation, and termination.\n\nSplice junction usage:\n\nReveals alternative splicing patterns (complementing Chapter 7’s SpliceAI).\n\nPolyadenylation and 3′ UTR usage:\n\nImpacts mRNA stability, localization, and translation.\n\n\nA general-purpose model that predicts base-level RNA-seq read coverage from DNA sequence could:\n\nProvide a unified framework for transcript-level variant effect prediction (transcription, splicing, polyadenylation).\nOffer mechanistic insight into how regulatory sequence features shape the full life cycle of transcripts.\n\n\n\n11.4.2 Architectural Highlights\nBorzoi builds on the Enformer-style backbone:\n\nConvolutional front-end:\n\nProcesses long DNA windows (on the order of ~100–200 kb).\nLearns local motifs and regulatory patterns at single-nucleotide or modestly downsampled resolution.\n\nHybrid long-range module:\n\nUses attention and/or long-range convolutions to integrate information across the entire context.\nExplicitly designed to capture relationships between promoters, internal exons, and distal elements.\n\nMulti-layer output heads:\n\nPredict RNA-seq coverage tracks across the window.\nOutput separate tracks for:\n\nSense vs antisense transcription.\nSplice junction signals.\nPolyA-related coverage around 3′ ends.\n\n\n\nLike Enformer, Borzoi is trained in a multi-task regime, but with a stronger emphasis on RNA-related readouts.\n\n\n11.4.3 From Chromatin Signals to RNA Readouts\nConceptually, Borzoi closes the loop:\n\nDeepSEA/Beluga/Enformer:\nSequence → chromatin + transcription start activity\nBorzoi:\nSequence → full transcriptome coverage\n\nThis supports several analyses:\n\nPromoter usage:\n\nDistinguish alternative promoter TSSs based on coverage patterns.\n\nAlternative splicing:\n\nPredict differential exon inclusion or skipping, complementing specialized models like SpliceAI.\n\n3′ UTR and polyA site choice:\n\nModel coverage drop-offs and polyA-linked patterns.\n\n\nVariant effect prediction follows similar steps as with Enformer:\n\nPredict transcriptome outputs for reference and alternate sequences.\nCompute Δ-coverage at exons, splice junctions, and 3′ ends.\nAggregate into variant-level scores for tasks like eQTL or sQTL prioritization.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Long-range Hybrid Model</span>"
    ]
  },
  {
    "objectID": "p3-ch11-hybrid.html#what-hybrid-models-changed",
    "href": "p3-ch11-hybrid.html#what-hybrid-models-changed",
    "title": "11  Long-range Hybrid Model",
    "section": "11.5 What Hybrid Models Changed",
    "text": "11.5 What Hybrid Models Changed\nHybrid CNN–transformer sequence models like Enformer and Borzoi introduced several conceptual advances over earlier architectures.\n\n11.5.1 1. Explicit Long-Range Modeling\nBy combining convolutional downsampling with attention over latent tokens, these models:\n\nAchieve hundreds of kilobases of effective context with manageable compute.\nAllow all positions in the compressed representation to interact, approximating many possible promoter–enhancer relationships.\n\nThis is crucial for:\n\nCapturing distal enhancers that sit far from genes.\nModeling complex regulatory architectures where multiple enhancers and silencers integrate to control expression.\n\n\n\n11.5.2 2. Unified Multi-Task Learning Across Modalities\nHybrid models jointly predict:\n\nChromatin accessibility.\nHistone marks.\nTranscriptional activity (CAGE, RNA-seq).\n\nThe result:\n\nShared representations that capture general regulatory logic.\nRegularization across assays and cell types, reducing overfitting to any single dataset.\nA pathway to transfer learning, where a single pretrained model can be adapted to downstream tasks.\n\n\n\n11.5.3 3. Improved Variant Effect Prediction for Expression\nCompared to earlier CNN-only models (DeepSEA, Beluga, ExPecto, Basenji2):\n\nEnformer demonstrated stronger eQTL concordance and better performance on expression-related benchmarks (Avsec et al. 2021).\nHybrid designs can identify distal causal variants more reliably, because their architecture naturally encodes long-range dependencies.\n\nBorzoi takes this further by providing detailed transcriptome-level readouts, enabling:\n\nCombined assessment of transcription, splicing, and polyadenylation for each variant.\nA richer mechanistic understanding of how sequence variation impacts the full RNA life cycle.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Long-range Hybrid Model</span>"
    ]
  },
  {
    "objectID": "p3-ch11-hybrid.html#limitations-and-failure-modes",
    "href": "p3-ch11-hybrid.html#limitations-and-failure-modes",
    "title": "11  Long-range Hybrid Model",
    "section": "11.6 Limitations and Failure Modes",
    "text": "11.6 Limitations and Failure Modes\nDespite their power, hybrid long-range models are not omniscient and introduce new challenges.\n\n11.6.1 Data and Label Limitations\n\nBiased training data:\n\nENCODE/Roadmap assays focus on specific cell types, conditions, and regions.\nGTEx eQTLs are enriched for certain ancestries (Chapter 2).\n\nMissing modalities:\n\nMany regulatory phenomena (e.g., RNA binding protein effects, 3D structure beyond contact frequency) are only partially captured by the available assays.\n\n\nAs a result, the models may:\n\nUnderperform in cell types or ancestries not well represented in the training data.\nMisinterpret patterns that are confounded by technical artifacts (batch effects, mapping biases).\n\n\n\n11.6.2 Sequence Context and Generalization\n\nEnformer and Borzoi are trained on fixed window sizes around annotated loci:\n\nBehavior outside those canonical windows may be less reliable.\n\nTraining focuses on reference genome context:\n\nLarge indels, structural variants, or rearrangements may be poorly modeled.\n\nThe models assume linear genomic context:\n\n3D chromatin architecture is only indirectly captured via sequence patterns correlated with looping; explicit Hi-C or Micro-C integration is limited.\n\n\n\n\n11.6.3 Interpretability and Trust\nAlthough attribution methods exist:\n\nAttention weights and gradient-based scores are not direct causal evidence.\nAttributions can be noisy and sensitive to how targets are aggregated.\nFor clinical use, predictions often require orthogonal validation, e.g., CRISPR perturbation or allele-specific expression assays.\n\nThese issues are part of the broader interpretability challenges discussed in later chapters on evaluation and confounders.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Long-range Hybrid Model</span>"
    ]
  },
  {
    "objectID": "p3-ch11-hybrid.html#role-in-the-gfm-landscape",
    "href": "p3-ch11-hybrid.html#role-in-the-gfm-landscape",
    "title": "11  Long-range Hybrid Model",
    "section": "11.7 Role in the GFM Landscape",
    "text": "11.7 Role in the GFM Landscape\nHybrid architectures like Enformer and Borzoi occupy an interesting middle ground between task-specific CNNs and general-purpose genomic foundation models:\n\nCompared to earlier CNN systems:\n\nThey model much longer context and support richer multi-modal outputs.\nThey offer significantly improved expression-related variant effect prediction.\n\nCompared to modern GFMs (Chapters 12–13):\n\nThey are specialized and supervised on particular assays, not trained with broad self-supervision on raw genomes.\nTheir architecture is hand-crafted for specific tasks (chromatin + expression), rather than serving as a universal pretraining backbone.\n\n\nIn practice, they serve as:\n\nHigh-performance baselines for variant effect prediction tasks, especially when expression or RNA readouts are primary endpoints.\nPretraining sources: Representations learned by Enformer-like trunks can be adapted for downstream tasks or combined with pretrained language models over DNA.\nDesign templates: Many newer architectures borrow the “conv stem + long-range module + multi-task heads” pattern, swapping attention for alternative long-range mechanisms (e.g., state space models, Hyena, Mamba; Chapter 12).\n\nAs the field moves toward large, multi-modal genomic foundation models that integrate sequence, chromatin, expression, and 3D structure, Enformer and Borzoi represent key waypoints—demonstrating that:\n\nLong-range context is essential for accurate expression prediction.\nHybrid architectures can make such context computationally tractable.\nMulti-task supervision across regulatory layers is an effective path from raw DNA to clinically relevant variant effect predictions.\n\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.",
    "crumbs": [
      "Part III: Transformers Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Long-range Hybrid Model</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html",
    "href": "p4-ch12-principles.html",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "",
    "text": "12.1 From Task-Specific Models to Genomic Foundation Models\nGenomic foundation models (GFMs) are the culmination of several threads developed across the earlier parts of this book: high-fidelity variant calling, regulatory sequence-to-function prediction, protein language models, and long-context transformers for DNA. They extend these ideas into models that are general-purpose, pretrained at scale, and reusable across a wide range of genomic and genetic tasks.\nThis chapter steps back from individual architectures to define what it means for a model to be a genomic foundation model, organizes the emerging ecosystem into a practical taxonomy, and distills design principles that will guide the rest of Part IV.\nThe earlier chapters traced a fairly linear progression:\nFoundation models build on these ingredients but change the contract:\nHyenaDNA is a canonical example: a genomic foundation model pretrained on the human reference genome with context lengths up to 1M tokens at single-nucleotide resolution using a Hyena-based long-range architecture. DNABERT-2, Nucleotide Transformer V2, Caduceus-Ph, GROVER and related models form a parallel family of transformer-style DNA FMs. A recent benchmark comparing these five models across diverse tasks (classification, gene expression prediction, variant effect quantification, TAD recognition) illustrates both the promise and the limitations of current DNA FMs (Manzo, Borkowski, and Ovcharenko 2025).\nAt a high level, we can view GFMs as extending the “pretrain → finetune” paradigm from natural language and protein modeling into genomics, but with domain-specific constraints (extreme context lengths, single-nucleotide sensitivity, strong mechanistic priors).",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html#from-task-specific-models-to-genomic-foundation-models",
    "href": "p4-ch12-principles.html#from-task-specific-models-to-genomic-foundation-models",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "",
    "text": "Hand-crafted scores and shallow models such as CADD and early pathogenicity predictors (Rentzsch et al. 2019; Schubach et al. 2024).\nTask-specific deep models such as DeepSEA, ExPecto, Sei, Enformer and SpliceAI, which learn regulatory or splicing effects directly from sequence (J. Zhou and Troyanskaya 2015; J. Zhou et al. 2018; Chen et al. 2022; Avsec et al. 2021; Jaganathan et al. 2019).\nSequence language models over proteins and DNA (ESM, DNABERT, Nucleotide Transformer, HyenaDNA, GROVER) that learn general sequence representations via self-supervision (Rives et al. 2021; Lin et al. 2022; Brandes et al. 2023; Ji et al. 2021; Dalla-Torre et al. 2023; Nguyen et al. 2023; Sanabria et al. 2024).\n\n\n\nThe primary “product” of a GFM is not a task-specific prediction head, but a reusable representation (and sometimes a general interface) that can be adapted to many downstream tasks with modest additional supervision.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html#what-makes-a-model-a-genomic-foundation-model",
    "href": "p4-ch12-principles.html#what-makes-a-model-a-genomic-foundation-model",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "12.2 What Makes a Model a Genomic Foundation Model?",
    "text": "12.2 What Makes a Model a Genomic Foundation Model?\nThe term “foundation model” is sometimes used loosely in the genomics literature. For practical purposes, it is useful to define working criteria that separate GFMs from ordinary deep models.\n\n12.2.1 Working definition\nA genomic foundation model is a pre-trained model that:\n\nLearns from large-scale genomic data with minimal task-specific supervision\n\nPretraining on entire genomes (or large portions) across species or populations.\nObjectives such as masked language modeling, next-token prediction, denoising, or multi-task sequence-to-function prediction.\n\nProduces general-purpose representations\n\nEmbeddings of sequences, variants, loci, or genes that are useful across many downstream tasks.\nRepresentations can be extracted and reused with light adapters or linear probes.\n\nIs designed for broad transfer\n\nSupports many downstream tasks without retraining the full model.\nTransfer across assays (e.g., from chromatin marks to gene expression), tissues, species, or variant types.\n\nScales along at least one dimension\n\nContext length (e.g., HyenaDNA’s million-token window).\nParameter count (e.g., ESM and Nucleotide Transformer families).\nData diversity (e.g., pan-genomic pretraining, cross-species corpora).\n\nExposes a relatively standardized interface\n\nA common API for embeddings, sequence scoring, and mask-based perturbation.\nOften distributed via model hubs (e.g., Hugging Face) with documented downstream recipes.\n\n\nMany excellent deep models for genomics (e.g., early DeepSEA or SpliceAI) fail one or more of these criteria: they were trained for a specific assay or task, use narrowly scoped inputs/outputs, and are not designed for broad reuse.\n\n\n12.2.2 GFMs vs “just big models”\nScale alone does not make a model a foundation model. A very large Enformer-like model trained solely on human chromatin tracks is powerful but still strongly bound to a specific prediction interface (e.g., sequence → fixed set of chromatin tracks). By contrast, a DNA LM like HyenaDNA or DNABERT-2 is explicitly trained to model raw sequence using a general objective, and is naturally repurposed as an embedding engine.\nThe distinction matters because it affects:\n\nEvaluation: GFMs must be assessed across families of tasks (e.g., TraitGym, ProteinGym) (Benegas, Eraslan, and Song 2025; Notin et al. 2023).\nDeployment: GFMs are infrastructure that many downstream teams can reuse; task-specific models are closer to “applications.”",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html#taxonomy-of-genomic-foundation-models",
    "href": "p4-ch12-principles.html#taxonomy-of-genomic-foundation-models",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "12.3 Taxonomy of Genomic Foundation Models",
    "text": "12.3 Taxonomy of Genomic Foundation Models\nWe will use a pragmatic taxonomy based on input modality and pretraining objective rather than architecture alone.\n\n12.3.1 DNA language models\nThese models treat DNA as a “language” and learn to predict masked or next tokens. Representative examples include:\n\nDNABERT / DNABERT-2: k-mer and nucleotide-level transformers trained with masked language modeling on large genomic corpora (Ji et al. 2021; Z. Zhou et al. 2024).\nNucleotide Transformer: large-scale transformer LMs trained across multiple species, with variants V1/V2 differing in context length and pretraining data (Dalla-Torre et al. 2023).\nHyenaDNA: a long-range genomic FM using Hyena operators (implicit convolutions) with sub-quadratic scaling, trained on human reference with up to 1M-token contexts and single-nucleotide vocabulary (Nguyen et al. 2023).\nGROVER: an autoregressive DNA LM that learns rich sequence context and shows strong performance on annotation and variant tasks (Sanabria et al. 2024).\n\nStrengths\n\nNatural fit for representation learning: the main output is a contextual embedding for each nucleotide or token.\nFlexible adaptation: any task that can be phrased as “score a sequence or variant” can be built on top.\nCompatible with in-context learning and soft prompting (see HyenaDNA) for some tasks.\n\nLimitations\n\nIndirect modeling of quantitative functional readouts (e.g., expression, epigenetic signal).\nDifficult to interpret mechanistically compared to sequence-to-function models that predict explicit assays.\n\n\n\n12.3.2 Regulatory sequence-to-function GFMs\nBuilding on DeepSEA, ExPecto, Sei, and Enformer (J. Zhou and Troyanskaya 2015; J. Zhou et al. 2018; Chen et al. 2022; Avsec et al. 2021), newer models aim to:\n\nPredict hundreds to thousands of chromatin marks, TF binding profiles, and accessibility tracks from raw sequence.\nOperate over longer context windows (100 kb or more).\nProvide variant effect scores by computing \\(\\Delta\\)-predictions between reference and alternative alleles.\n\nWhile some of these models were originally trained for specific assays, they approximate GFMs when:\n\nThe output space is sufficiently broad (e.g., a panel of assays spanning many cell types).\nTheir internal representations are reused for tasks beyond the original assay set, such as gene expression prediction, enhancer–promoter linking, or variant prioritization.\n\nEnformer is a prototypical example of a sequence-to-function model that has been widely reused as a feature extractor for downstream tasks, including gene expression prediction and fine-mapping of regulatory variants (Avsec et al. 2021).\n\n\n12.3.3 Variant-centric GFMs and trait models\nA third class of GFMs focuses not on raw sequence but on genetic variants as the fundamental unit. These models often:\n\nEmbed variants using contextual information from local sequence, gene structure, and external annotations.\nPredict variant pathogenicity, molecular consequences, or trait-level effect sizes.\n\nExamples in this space include:\n\nCADD and its deep-learning-enhanced successor models, which integrate annotations and sequence features for broad variant pathogenicity scoring (Rentzsch et al. 2019; Schubach et al. 2024).\nAlphaMissense, which repurposes ESM-style protein LMs to predict missense pathogenicity at scale (Cheng et al. 2023).\nDelphi, MIFM, and related models that couple GFMs with polygenic score (PGS) estimation for complex traits (Georgantas, Kutalik, and Richiardi 2024; Rakowski and Lippert 2025; Wu et al. 2024).\nEmerging variant representation learning datasets and benchmarks (e.g., GV-Rep) that explicitly probe how well GFMs represent genetic variants and clinical annotations.\n\nVariant-centric GFMs blur the line between feature extractors and trait models: their predictions can be plugged directly into PGS pipelines, risk stratification tools, or rare disease interpretation workflows.\n\n\n12.3.4 Multi-omic and cross-modal GFMs\nFinally, a growing set of models aim to natively integrate multiple modalities:\n\nDNA sequence, chromatin state, and gene expression.\nSequence and 3D genome structure (Hi-C, Micro-C).\nDNA with non-sequence modalities such as images or free text.\n\nRecent work (e.g., Omni-DNA) explores transformer-based auto-regressive models that jointly handle DNA and task-specific tokens, enabling multi-task learning over sequence, epigenetic marks, and even textual descriptions of function. These models move GFMs closer to a unified interface for genome biology, at the cost of more complex training objectives and data engineering.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html#design-dimensions-of-genomic-foundation-models",
    "href": "p4-ch12-principles.html#design-dimensions-of-genomic-foundation-models",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "12.4 Design Dimensions of Genomic Foundation Models",
    "text": "12.4 Design Dimensions of Genomic Foundation Models\nWhen designing or choosing a GFM, it is helpful to think in terms of several orthogonal design dimensions.\n\n12.4.1 Data: what does the model “see”?\nKey data decisions include:\n\nSpecies coverage\n\nHuman-only: focused on clinical and human genetics applications.\nCross-species: pretraining on dozens or hundreds of species (as in Nucleotide Transformer and many protein LMs) encourages discovery of conserved regulatory code and better out-of-domain generalization (Dalla-Torre et al. 2023; Rives et al. 2021).\n\nAssay diversity\n\nFor sequence-to-function GFMs: which epigenomic assays, cell types, and perturbation datasets are included (e.g., Cistrome-like collections (Zheng et al. 2019)).\nFor variant-centric GFMs: which clinical databases, experimental screens, and population cohorts are integrated.\n\nPopulation diversity\n\nInclusion of genomes from diverse ancestries is crucial to avoid embedding population-specific biases into GFMs and downstream risk scores.\nEarly deep PGS models such as Delphi and MIFM explicitly tackle ancestry-aware evaluation (Georgantas, Kutalik, and Richiardi 2024; Rakowski and Lippert 2025; Wu et al. 2024).\n\nContext length and sampling\n\nRandom slicing of long chromosomes into training windows (HyenaDNA).\nTargeted sampling around genes, enhancers, or known variants.\nWarm-up schedules that gradually increase context length to stabilize training.\n\n\n\n\n12.4.2 Architecture: how does the model process sequence?\nCommon architectural families include:\n\nTransformers\n\nEncoder-only (BERT-style; DNABERT, Nucleotide Transformer).\nDecoder-only (GPT-style; GROVER, some Omni-DNA models).\nEncoder–decoder hybrids for tasks requiring explicit outputs (e.g., sequence→text explanations).\n\nAttention-free long-range models\n\nHyena-based models (HyenaDNA): implicit convolutions with sub-quadratic complexity.\nState space models and related architectures that trade exact attention for scalable long-range interactions.\n\nDense-attention long-range transformers\n\nModels like Gene42 show that with careful engineering and context extension schedules, dense-attention transformers can also reach ~200 kb contexts at single-nucleotide resolution.\n\nHybrid architectures\n\nCNN + transformer stacks (e.g., local convolutions followed by global attention, as seen in some Enformer-like models (Avsec et al. 2021)).\nCross-attention between DNA and auxiliary modalities (e.g., chromatin, 3D contacts).\n\n\nArchitecture choices primarily determine:\n\nMaximum practical context length.\nMemory and compute requirements.\nEase of adaptation (e.g., decoder-only models are often easier to use for generative tasks, transformers easier for cross-modal fusion).\n\n\n\n12.4.3 Objectives: what does the model learn to predict?\nTypical pretraining objectives include:\n\nMasked token prediction\n\nRandomly mask nucleotides or k-mers and predict them given context (DNABERT, DNABERT-2, many transformers) (Ji et al. 2021; Z. Zhou et al. 2024).\nEncourages the model to capture local and medium-range dependencies.\n\nNext-token prediction\n\nAutoregressive LM objective (GROVER, HyenaDNA).\nNaturally aligns with generative tasks and in-context learning, and leverages techniques from large language models.\n\nDenoising and span corruptions\n\nReplace or permute spans of sequence and train the model to reconstruct them.\nEncourages robustness to small perturbations and focus on long-range structure.\n\nMulti-task sequence-to-function prediction\n\nPredict chromatin profiles, TF binding, accessibility, expression, etc., directly from sequence (DeepSEA, Enformer, Sei) (J. Zhou and Troyanskaya 2015; Avsec et al. 2021; Chen et al. 2022).\nFunctions as a powerful regularizer and a direct bridge between sequence patterns and molecular readouts.\n\nCross-modal objectives\n\nJointly predict sequence, epigenetic tracks, and textual/function labels (e.g., in Omni-DNA-like architectures).\nContrastive alignment between DNA slices and other modalities (e.g., 3D contacts, histone marks).\n\n\n\n\n12.4.4 Tokenization and representations\nTokenization is non-trivial for DNA:\n\nCharacter-level (single nucleotide): simplest and compatible with single-nucleotide resolution, used by HyenaDNA and many sequence-to-function models (Nguyen et al. 2023).\nk-mer tokenization (e.g., 3–6-mers) reduces sequence length and helps transformers reach longer effective contexts, at the cost of some resolution (Ji et al. 2021).\nLearned tokenization (e.g., BioToken-style approaches) which discover sub-sequence units optimized for downstream performance (Medvedev et al. 2025).\n\nInternally, GFMs typically produce:\n\nPer-position embeddings \\(h_i \\in \\mathbb{R}^d\\) for each nucleotide or token.\nPooled sequence embeddings (mean, CLS token, learned pooling) that summarize an entire region.\nVariant embeddings, constructed by contrasting reference vs alternative alleles, sometimes augmented with structural context.\n\nThe choice of pooling strategy can significantly influence downstream performance; benchmarking studies have found that simple mean pooling of per-token embeddings often outperforms more elaborate strategies across many tasks (Manzo, Borkowski, and Ovcharenko 2025).",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html#evaluating-genomic-foundation-models",
    "href": "p4-ch12-principles.html#evaluating-genomic-foundation-models",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "12.5 Evaluating Genomic Foundation Models",
    "text": "12.5 Evaluating Genomic Foundation Models\nBecause GFMs are meant to be foundations, evaluation must be broader than single-task metrics.\n\n12.5.1 Downstream task suites and benchmarks\nEmerging benchmark suites provide structured evaluations:\n\nProteinGym: variant effect prediction across many proteins for protein LMs (Notin et al. 2023).\nTraitGym: trait-level performance of regulatory and genomic models across complex trait prediction tasks (Benegas, Eraslan, and Song 2025).\nComparative evaluations of DNA LMs and regulatory models, such as the work by Manzo et al. comparing sequence models across regulatory genomics tasks (Manzo, Borkowski, and Ovcharenko 2025).\nDNA FM benchmarks that systematically compare models like DNABERT-2, Nucleotide Transformer V2, HyenaDNA, Caduceus-Ph, and GROVER across classification, variant effect, and TAD tasks.\nVariant-centric benchmarks like GV-Rep, probing GFMs’ ability to represent clinical variants and their contexts.\n\nA key lesson from these benchmarks is that no single model dominates all tasks: general-purpose DNA FMs often perform well but may lag specialized architectures for gene expression and QTL prediction, while excelling for variant prioritization and regulatory element annotation.\n\n\n12.5.2 Evaluation modes: zero-shot, linear probe, fine-tune\nGFMs can be evaluated in several regimes:\n\nZero-shot evaluation\n\nUse frozen embeddings with simple operations (similarity, clustering) or predefined scoring rules.\nExample: using HyenaDNA embeddings directly for in-context learning on simple motif tasks.\n\nLinear probes\n\nTrain shallow linear or logistic regression heads on top of frozen embeddings.\nProvides a quick measure of how easily information is linearly decodable from GFM representations.\n\nLight-weight adaptation\n\nLow-rank adaptation (LoRA), prompt tuning, or small MLP heads fine-tuned on specific tasks.\nBalances performance with computational cost and stability.\n\nFull-model finetuning\n\nFinetune all parameters for high-stakes tasks where maximal performance is critical and data is abundant.\nRisk of catastrophic forgetting or overfitting, especially when downstream data is limited.\n\n\nThe right regime depends on data size, computational budget, and the sensitivity of the application (e.g., rare disease diagnosis vs exploratory motif discovery).",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html#using-gfms-in-practice",
    "href": "p4-ch12-principles.html#using-gfms-in-practice",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "12.6 Using GFMs in Practice",
    "text": "12.6 Using GFMs in Practice\nFrom the vantage point of a working computational biologist, the most pressing questions are “Which model should I use?” and “How do I plug it into my pipeline?”\n\n12.6.1 Typical usage patterns\nCommon ways to use GFMs include:\n\nEmbedding-based pipelines\n\nExtract per-base or pooled embeddings for loci of interest.\nTrain simple downstream models (e.g., gradient-boosted trees, small neural nets) on these embeddings.\nEvaluate on held-out datasets or across cohorts.\n\nVariant effect scoring\n\nUse sequence-to-function GFMs (Enformer-like) to compute \\(\\Delta\\)-predictions between reference and alternate alleles.\nFeed variant-level scores into downstream calibration layers or PGS models (Avsec et al. 2021; Georgantas, Kutalik, and Richiardi 2024; Rakowski and Lippert 2025).\n\nFeature augmentation\n\nCombine GFM-derived features with classical annotations (conservation, CADD scores, functional genomics tracks) (Rentzsch et al. 2019; Schubach et al. 2024).\nParticularly useful for rare variant interpretation where each evidence source is sparse.\n\nCross-modal linking\n\nUse GFMs as common embedding spaces linking sequence with expression, imaging, or textual annotations (e.g., variant→phenotype descriptions).\n\n\n\n\n12.6.2 Choosing a model for your use case\nA simple decision guide:\n\nNeed long-range context (&gt;100 kb)?\n\nConsider models like HyenaDNA or long-context dense-attention models such as Gene42.\n\nFocus on regulatory variant interpretation near genes?\n\nStart with Enformer-like or DeepSEA-like GFMs and compare against DNA LMs working via embeddings (Avsec et al. 2021; J. Zhou and Troyanskaya 2015; Chen et al. 2022; Ji et al. 2021).\n\nTrait-level prediction with large cohorts?\n\nExplore PGS pipelines that incorporate GFM-based variant priors such as Delphi or MIFM (Georgantas, Kutalik, and Richiardi 2024; Rakowski and Lippert 2025; Wu et al. 2024).\n\nMethod development / benchmarking?\n\nUse standardized benchmarks (TraitGym, ProteinGym, GV-Rep, DNA FM suites) to ensure your comparisons are meaningful (Benegas, Eraslan, and Song 2025; Notin et al. 2023; Manzo, Borkowski, and Ovcharenko 2025).",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html#safety-robustness-and-responsible-use",
    "href": "p4-ch12-principles.html#safety-robustness-and-responsible-use",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "12.7 Safety, Robustness, and Responsible Use",
    "text": "12.7 Safety, Robustness, and Responsible Use\nAs GFMs become infrastructure for clinical and research pipelines, safety and robustness are not optional extras.\n\n12.7.1 Robustness and adversarial sensitivity\nRecent work such as SafeGenes highlights that genomic FMs (including ESM1b-like and other GFMs) can be surprisingly sensitive to adversarial perturbations—both at the input sequence level and through soft prompts in embedding space. Even when perturbations are hardly biologically plausible, they reveal:\n\nFragility of decision boundaries in high-dimensional representation space.\nPotential failure modes where small spurious changes strongly impact pathogenicity or variant effect predictions.\n\nThis suggests that:\n\nAdversarial testing should become part of GFM validation, especially for clinical use cases.\nRobust training (e.g., via data augmentation, adversarial objectives, or distributionally robust optimization) may be needed for high-stakes tasks.\n\n\n\n12.7.2 12.7.2 Bias, fairness, and ancestry\nGFMs trained predominantly on reference genomes or Euro-centric cohorts risk encoding biased priors:\n\nUnderestimation of risk in underrepresented ancestries.\nMisclassification of benign variants that are common in certain populations but rare in training data.\n\nDeep PGS and variant interpretation pipelines that incorporate GFMs should:\n\nPerform ancestry-stratified evaluation (Georgantas, Kutalik, and Richiardi 2024; Rakowski and Lippert 2025; Wu et al. 2024).\nConsider explicit debiasing (e.g., reweighting) and careful calibration.\n\n\n\n12.7.3 Data governance and privacy\nBecause GFMs are often trained on large collections of genomic sequences:\n\nData use agreements and privacy protections must be respected; some cohort-level datasets cannot be used for unrestricted pretraining.\nEven when training on reference genomes, leakage from labeled clinical datasets into training may complicate downstream evaluation.\n\nTo date, most published GFMs emphasize training on public reference genomes or synthetic benchmarks, but clinical deployment will require stronger guarantees.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html#open-challenges-and-future-directions",
    "href": "p4-ch12-principles.html#open-challenges-and-future-directions",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "12.8 Open Challenges and Future Directions",
    "text": "12.8 Open Challenges and Future Directions\nGenomic foundation models are still in their early days. Several open challenges stand out.\n\n12.8.1 Toward unified multi-omic GFMs\nCurrent GFMs are still fragmented:\n\nDNA-only LMs.\nSequence-to-function models tied to specific assays.\nVariant-centric pathogenicity models.\nProtein and RNA LMs.\n\nA major frontier is unified multi-omic GFMs that:\n\nJointly model DNA, RNA, protein, chromatin, and 3D genome structure.\nSupport cross-modal queries such as “given this variant, what is the likely impact on TF binding, chromatin accessibility, and gene expression in a given cell type?”\nProvide interpretable pathways connecting sequence variation to phenotypes.\n\nModels such as Omni-DNA are first steps in this direction, showing that multi-task, cross-modal training is feasible at scale.\n\n\n12.8.2 Integrating causal and mechanistic structure\nMost GFMs are trained with purely predictive objectives. Incorporating more causal structure could:\n\nImprove robustness to distribution shift (e.g., between cell types or interventions).\nEnable counterfactual reasoning (“what if we knock out this enhancer?”).\n\nPotential routes include:\n\nCausal representation learning on top of GFM embeddings.\nMechanistic constraints derived from gene regulatory networks or biochemical kinetics.\nJoint modeling of perturbation data (CRISPR screens, gene knockouts) with observational genomics.\n\n\n\n12.8.3 Efficient and accessible deployment\nEven if GFMs train on large clusters, their deployment should be feasible in typical research labs and clinical environments:\n\nDistillation into smaller student models.\nEfficient inference via sparsity, quantization, and hardware-aware architectures.\nTask-specific adapters that keep the frozen backbone small enough for on-premise use.\n\nThe long-range efficiency of architectures like HyenaDNA and the emergence of dense-attention models like Gene42 suggest multiple viable paths to deployable GFMs.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch12-principles.html#summary",
    "href": "p4-ch12-principles.html#summary",
    "title": "12  Genomic FMs: Principles & Practice",
    "section": "12.9 Summary",
    "text": "12.9 Summary\nIn this chapter, we:\n\nDefined what it means for a model to be a genomic foundation model, emphasizing scale, generality, and reusability.\nProposed a practical taxonomy: DNA language models, sequence-to-function GFMs, variant-centric GFMs, and emerging multi-omic models.\nSurveyed core design dimensions: data, architecture, objectives, and tokenization.\nDiscussed evaluation regimes and benchmark suites that assess GFMs across diverse tasks.\nOutlined how practitioners can integrate GFMs into variant interpretation, regulatory genomics, and trait prediction pipelines.\nHighlighted emerging concerns around robustness, bias, and responsible deployment.\n\nThe remaining chapters of Part IV will dive deeper into specific application domains—clinical interpretation, population-scale trait modeling, and multi-omics integration—using the conceptual framework established here to organize a rapidly evolving ecosystem of genomic foundation models.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025. “[TraitGym] Benchmarking DNA Sequence Models for Causal Regulatory Variant Prediction in Human Genetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. “Delphi: A Deep-Learning Method for Polygenic Risk Prediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nManzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025. “Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory Variants.” bioRxiv: The Preprint Server for Biology, June, 2025.05.19.654920. https://doi.org/10.1101/2025.05.19.654920.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. “ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design.” Advances in Neural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray, Peter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping Improves Identification of Causal Variants.” Research Square, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.\n\n\nZheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei Sun, Chen-Hao Chen, et al. 2019. “Cistrome Data Browser: Expanded Datasets and New Tools for Gene Regulatory Analysis.” Nucleic Acids Research 47 (D1): D729–35. https://doi.org/10.1093/nar/gky1094.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Genomic FMs: Principles & Practice</span>"
    ]
  },
  {
    "objectID": "p4-ch13-vep.html",
    "href": "p4-ch13-vep.html",
    "title": "13  Variant Effect Prediction",
    "section": "",
    "text": "13.1 From Handcrafted Scores to Foundation Models\nVariant effect prediction (VEP) sits at the heart of modern genomics. Most variants discovered in clinical sequencing are rare and lack direct experimental evidence; yet clinicians still need to decide whether they’re benign, pathogenic, or somewhere in between. Earlier in this book we saw:\nThe frontier today is shaped by foundation models that combine:\nThis chapter surveys four landmark systems:\nTogether, they preview what “Genomic Foundation Models” look like when specialized for variant interpretation.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p4-ch13-vep.html#from-handcrafted-scores-to-foundation-models",
    "href": "p4-ch13-vep.html#from-handcrafted-scores-to-foundation-models",
    "title": "13  Variant Effect Prediction",
    "section": "",
    "text": "Conservation and heuristic scores (e.g., traditional tools like SIFT (Ng and Henikoff (2003)), PolyPhen (Adzhubei et al. (2010)), CADD (Rentzsch et al. (2019))), which combine evolutionary constraint and manually engineered features.\nSequence-to-function CNNs like DeepSEA and ExPecto (Chapters 5–6), which predict chromatin and expression to estimate regulatory effects.\nSpecialized architectures like SpliceAI (Chapter 7), which target specific mechanisms such as splicing.\nProtein language models (Chapter 9), which learn rich representations from large-scale protein sequences and can be adapted for missense VEP.\n\n\n\nMassive pretraining (proteome- or genome-scale),\nLong-range context (from kilobases to megabases),\nMultiple sources of information (sequence, structure, multi-species alignments, multi-omic outputs).\n\n\n\nAlphaMissense – proteome-wide missense pathogenicity predictions.\nGPN-MSA – a DNA language model over multi-species alignments for genome-wide VEP.\nEvo 2 – a generalist genomic language model spanning all domains of life.\nAlphaGenome – a unified, megabase-scale sequence-to-function model with state-of-the-art regulatory VEP.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p4-ch13-vep.html#alphamissense-proteome-wide-missense-pathogenicity",
    "href": "p4-ch13-vep.html#alphamissense-proteome-wide-missense-pathogenicity",
    "title": "13  Variant Effect Prediction",
    "section": "13.2 AlphaMissense: Proteome-Wide Missense Pathogenicity",
    "text": "13.2 AlphaMissense: Proteome-Wide Missense Pathogenicity\nAlphaMissense, developed by DeepMind, provides precomputed pathogenicity scores for ~71 million possible human missense variants, covering almost every single–amino acid change in the proteome.\n\n13.2.1 Inputs: Combining Sequence and Structure\nAlphaMissense builds on two pillars:\n\nProtein language modeling\n\nA transformer-based model is trained on massive multiple sequence alignments (MSAs), learning which amino acids tend to appear at each position across evolution.\n\nFrom this, the model infers how “surprising” a given amino acid substitution is in its evolutionary context.\n\nPredicted 3D structure from AlphaFold2\n\nStructural context (packing, secondary structure, local interactions) helps distinguish tolerated changes (e.g., on solvent-exposed loops) from disruptive ones (e.g., in tightly packed cores or active sites).\n\n\nFor each variant, AlphaMissense ingests:\n\nThe wild-type sequence,\nThe substitution position and amino-acid change,\nSequence context from the MSA,\nStructural environment derived from AlphaFold2.\n\nThese features are fed into a neural network that outputs a pathogenicity probability between 0 and 1.\n\n\n13.2.2 Training and Calibration\nAlphaMissense’s training is hybrid:\n\nSelf-supervised pretraining learns general sequence and structural representations from evolutionary data.\nSupervised calibration uses:\n\nClinVar and similar databases for labeled pathogenic/benign variants,\nPopulation frequencies (e.g., gnomAD) under the assumption that common variants are more likely benign.\n\n\nThe model’s raw scores are calibrated so that:\n\nScores near 0 behave like “likely benign,”\n\nScores near 1 behave like “likely pathogenic,”\n\nIntermediate scores capture uncertainty and ambiguous cases.\n\nIn practice, AlphaMissense adopts score cutoffs that approximately map to “likely benign,” “uncertain,” and “likely pathogenic” categories used in clinical interpretation frameworks.\n\n\n13.2.3 Performance and Clinical Utility\nAcross diverse benchmarks—ClinVar, curated expert panels, and multiplexed assays of variant effect (MAVEs)—AlphaMissense:\n\nAchieves state-of-the-art AUROC and AUPRC for missense VEP.\nGeneralizes across many genes, including those with little prior annotation.\nProduces scores that are more consistent with experimental functional readouts than many earlier predictors.\n\nAs a result, AlphaMissense scores have already been integrated into:\n\nClinical re-annotation of exomes,\n\nReclassification of variants of uncertain significance (VUS),\n\nGene-specific studies where high-throughput functional assays are impractical.\n\n\n\n13.2.4 Limitations and Caveats\nDespite its impressive performance, AlphaMissense has important limitations:\n\nMissense-only: It does not natively handle nonsense, frameshift, regulatory, or deep intronic variants.\nSingle-variant focus: It scores one substitution at a time, ignoring combinations of variants (compound heterozygosity, epistasis).\nDependent on training labels: Any biases in ClinVar or population data (e.g., ancestry representation) can propagate into scores.\nInterpretability: While attention maps and feature attributions can be examined, the reasoning for a particular score is often opaque.\n\nFor these reasons, guidelines recommend treating AlphaMissense as supporting evidence to be combined with segregation, functional data, and population frequencies—not as a standalone decision-maker.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p4-ch13-vep.html#gpn-msa-genome-wide-variant-effect-prediction-from-msas",
    "href": "p4-ch13-vep.html#gpn-msa-genome-wide-variant-effect-prediction-from-msas",
    "title": "13  Variant Effect Prediction",
    "section": "13.3 GPN-MSA: Genome-Wide Variant Effect Prediction from MSAs",
    "text": "13.3 GPN-MSA: Genome-Wide Variant Effect Prediction from MSAs\nWhile AlphaMissense focuses on proteins, GPN-MSA tackles the harder problem of genome-wide variant effect prediction in complex genomes such as human, directly at the DNA level.\n\n13.3.1 Alignment-Based DNA Language Model\nGPN-MSA extends earlier Genomic Pre-trained Network (GPN) models by operating on multi-species genome alignments:\n\nInput: a stack of aligned sequences from multiple species (e.g., human plus dozens of mammals).\nRepresentation: the model sees both:\n\nThe reference sequence (e.g., human), and\n\nAuxiliary features encoding how each aligned species matches, mismatches, or gaps at each base.\n\n\nThe model is trained with a masked language modeling (MLM) objective:\n\nRandomly mask nucleotides in the reference sequence,\nPredict the masked base given the surrounding context and the aligned sequences.\n\nThis encourages the model to learn evolutionary constraints: positions where substitutions are strongly disfavored across species get very confident predictions; unconstrained positions allow more flexibility.\n\n\n13.3.2 Variant Scoring Strategies\nGPN-MSA supports several ways to derive variant effect scores:\n\nLikelihood-based scoring: compare the model’s log-likelihood (or probability) of the reference vs. alternate allele at the variant position.\nEmbedding distance: compute embeddings for reference and alternate sequences and use their difference (e.g., Euclidean distance) as an effect magnitude.\nInfluence scores: quantify how much a variant perturbs the model’s outputs across the surrounding genomic context.\n\nBecause the model operates on whole-genome alignments, it can score:\n\nCoding and noncoding variants,\nRegulatory elements, introns, UTRs, and intergenic regions,\nVariants in regions with complex conservation patterns, where simple phyloP-like scores Siepel et al. (2005) struggle.\n\n\n\n13.3.3 Benchmarking and Applications\nGPN-MSA demonstrates strong performance on:\n\nGenome-wide pathogenic vs. benign classification datasets,\nVariant sets from genome-wide association studies,\nFunctional readouts from high-throughput reporter assays.\n\nPractically, GPN-MSA is useful for:\n\nGenome-wide prefiltering: prioritizing candidate causal variants in regulatory regions.\nComplementing protein-focused tools: supplying information where AlphaMissense is blind (deep noncoding, intronic, intergenic).\n\nIts key limitation is dependency on high-quality multi-species alignments; coverage and quality drop in repetitive, structurally complex, or poorly aligned regions.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p4-ch13-vep.html#evo-2-a-generalist-genomic-language-model",
    "href": "p4-ch13-vep.html#evo-2-a-generalist-genomic-language-model",
    "title": "13  Variant Effect Prediction",
    "section": "13.4 Evo 2: A Generalist Genomic Language Model",
    "text": "13.4 Evo 2: A Generalist Genomic Language Model\nEvo 2 pushes the foundation-model paradigm to the extreme: it is a genome-scale language model trained across all domains of life—bacteria, archaea, eukaryotes, and phages—on &gt;9 trillion DNA tokens.\n\n13.4.1 Scale and Architecture\nKey features of Evo 2 include:\n\nAutoregressive training on DNA: predict the next base given the preceding context, analogous to next-token prediction in text LLMs.\nA StripedHyena 2 architecture, blending convolutional and attention mechanisms to support:\n\nContext windows up to 1 million base pairs,\n\nEfficient long-range modeling.\n\nMultiple model sizes (e.g., 7B and 40B parameters) with open-source weights, training code, and the OpenGenome2 dataset.\n\nEvo 2 is designed as a generalist: it is not trained specifically for VEP, but rather to model genomic sequences broadly.\n\n\n13.4.2 Zero-Shot Variant Effect Scoring\nRemarkably, Evo 2 can be used for zero-shot variant interpretation:\n\nFor a given locus, compute the model’s sequence likelihood (or log-probability) for the reference allele.\nThen compute the likelihood for the alternate allele (or sequence containing it).\nThe difference in likelihood provides a variant effect score—variants that strongly reduce probability are inferred to be more disruptive.\n\nIn benchmarks reported in the preprint and follow-up analyses:\n\nEvo 2 achieves competitive or state-of-the-art accuracy for pathogenic vs. benign classification across multiple variant types (coding and noncoding), even without variant-specific supervised training.\nA simple supervised classifier built on Evo 2 embeddings reaches state-of-the-art performance on tasks like BRCA1 VUS classification.\n\n\n\n13.4.3 Cross-Species Variant Interpretation\nBecause Evo 2 is trained across diverse species:\n\nIt naturally supports variant effect prediction in non-model organisms (e.g., livestock, crops).\nIt can help quantify mutation load, prioritize variants for breeding programs, and guide genome editing designs across species.\n\nHowever, its generality comes with trade-offs:\n\nDomain-specific models (like AlphaMissense for human missense or AlphaGenome for regulatory variants) may still outperform Evo 2 on certain human-centric tasks.\nCareful calibration and benchmarking are required before clinical use.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p4-ch13-vep.html#alphagenome-unified-megabase-scale-regulatory-modeling",
    "href": "p4-ch13-vep.html#alphagenome-unified-megabase-scale-regulatory-modeling",
    "title": "13  Variant Effect Prediction",
    "section": "13.5 AlphaGenome: Unified Megabase-Scale Regulatory Modeling",
    "text": "13.5 AlphaGenome: Unified Megabase-Scale Regulatory Modeling\nWhere Evo 2 is generalist and sequence-only, AlphaGenome is explicitly designed as a multimodal regulatory model of the human genome, with a focus on variant effect prediction across many functional readouts.\n\n13.5.1 Architecture: CNNs + Transformers over 1 Mbp\nAlphaGenome takes as input 1 megabase (1 Mb) of DNA sequence and produces predictions at single-base resolution for a large set of genomic “tracks,” including:\n\nChromatin accessibility and histone marks,\nTranscription factor binding,\nGene expression (e.g., CAGE-like signals),\n3D genome contacts,\nSplicing features (junctions and splice-site usage).\n\nArchitecturally:\n\nConvolutional layers detect local sequence motifs.\nTransformer blocks propagate information across the full megabase context.\nTask-specific heads output different experimental modalities across many tissues and cell types.\n\nThis design generalizes earlier models like Basenji/Enformer (for regulatory tracks) and SpliceAI (for splicing) into a single, unified model.\n\n\n13.5.2 Variant Effect Prediction Across Modalities\nGiven a reference sequence and a candidate variant, AlphaGenome scores variant effects by:\n\nPredicting genome-wide functional tracks for the reference sequence.\nPredicting the same tracks for the sequence bearing the variant.\nComparing predictions to obtain delta signals across:\n\nRegulatory elements (promoters, enhancers, insulators),\nSplicing patterns (gain/loss of splice junctions),\nGene expression levels,\n3D contact maps affecting enhancer–promoter communication.\n\n\nOn extensive benchmarks, AlphaGenome:\n\nAchieves state-of-the-art accuracy in predicting unseen functional genomics tracks.\nShows strong performance on diverse variant effect tasks (e.g., noncoding disease variants, splicing disruptions, regulatory MPRA data).\nProvides mechanistic hypotheses (which tracks/tissues are disrupted) rather than only a single scalar risk score.\n\nAn API makes AlphaGenome accessible to the research community, enabling large-scale variant scoring without local training infrastructure.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p4-ch13-vep.html#comparing-design-choices-across-modern-vep-models",
    "href": "p4-ch13-vep.html#comparing-design-choices-across-modern-vep-models",
    "title": "13  Variant Effect Prediction",
    "section": "13.6 Comparing Design Choices Across Modern VEP Models",
    "text": "13.6 Comparing Design Choices Across Modern VEP Models\nThe models in this chapter span different points in the design space:\n\n\n\n\n\n\n\n\n\n\n\nModel\nInput Modality\nContext Length\nPretraining Data\nVariant Types\nPrimary Outputs\n\n\n\n\nAlphaMissense\nProtein sequence + structure\nProtein-length\nMSAs + structural environment\nMissense only\nPathogenicity probability\n\n\nGPN-MSA\nMulti-species DNA alignments\nkb-scale windows\nWhole-genome MSAs (multiple species)\nCoding + noncoding\nLikelihood / embedding-based scores\n\n\nEvo 2\nRaw DNA sequence\nUp to ~1 Mb\nOpenGenome2 (all domains of life)\nAll variant types\nZero-shot likelihood-based scores\n\n\nAlphaGenome\nRaw DNA sequence\n1 Mb\nHuman genome + multi-omic tracks\nAll variant types\nMulti-omic tracks + delta effects\n\n\n\nKey contrasts:\n\nScope\n\nAlphaMissense is human-missense-specific, with deep clinical calibration.\n\nGPN-MSA and AlphaGenome are human-genome-centric, spanning coding and regulatory variants.\n\nEvo 2 is cross-species and general-purpose.\n\nContext and long-range effects\n\nAlphaMissense operates at protein scale.\n\nGPN-MSA uses modest windows centered on the variant.\n\nEvo 2 and AlphaGenome support megabase-scale context, capturing long-range regulatory interactions.\n\nOutputs\n\nAlphaMissense and GPN-MSA primarily output scalar scores.\n\nEvo 2 outputs likelihoods/embeddings that require task-specific postprocessing.\n\nAlphaGenome outputs rich functional profiles, enabling mechanistic hypotheses.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p4-ch13-vep.html#practical-use-choosing-and-interpreting-modern-vep-tools",
    "href": "p4-ch13-vep.html#practical-use-choosing-and-interpreting-modern-vep-tools",
    "title": "13  Variant Effect Prediction",
    "section": "13.7 Practical Use: Choosing and Interpreting Modern VEP Tools",
    "text": "13.7 Practical Use: Choosing and Interpreting Modern VEP Tools\nIn realistic workflows, these models are complementary rather than competing.\n\n13.7.1 Coding Missense Variants\nFor human missense variants:\n\nUse AlphaMissense as a high-coverage, clinically calibrated score.\nComplement with:\n\nProtein language model embeddings (Chapter 9) for gene- or domain-specific modeling,\nConservation and population data (e.g., GPN-MSA in coding regions, gnomAD frequencies),\nGene-level context (constraint metrics, disease association).\n\n\n\n\n13.7.2 Noncoding and Regulatory Variants\nFor regulatory variation (promoters, enhancers, introns, intergenic):\n\nUse AlphaGenome to obtain:\n\nTissue-specific changes in chromatin and expression,\nSplicing consequences (especially for intronic and exonic variants),\nPotential disruption of long-range enhancer–promoter interactions.\n\nUse GPN-MSA when:\n\nYou want a conservation-grounded score,\nHigh-quality multi-species alignments are available,\nYou’re scanning broad regions genome-wide.\n\n\n\n\n13.7.3 Cross-Species and Large-Scale Modeling\nFor non-human organisms, or when building general-purpose genomic tools:\n\nLeverage Evo 2 for:\n\nZero-shot variant scoring in poorly annotated species,\nDesigning or screening edits (e.g., CRISPR designs),\nServing as a feature extractor feeding downstream supervised models.\n\n\n\n\n13.7.4 Score Interpretation and Calibration\nRegardless of the model:\n\nTreat scores as probabilistic evidence, not binary labels.\nConsider:\n\nCalibration (does a score of 0.9 truly correspond to ~90% pathogenic variants?),\nDistribution of scores within a gene (outliers are more suspect),\nConsistency across tools (agreement between AlphaMissense, GPN-MSA, AlphaGenome, Evo 2, and simpler conservation metrics strengthens confidence).\n\n\nWhere possible, tie predictions back to:\n\nMechanistic hypotheses (splice site disruption, enhancer–promoter rewiring),\nExperimental follow-up (targeted assays, MPRA, CRISPR screens).",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p4-ch13-vep.html#open-challenges-and-future-directions",
    "href": "p4-ch13-vep.html#open-challenges-and-future-directions",
    "title": "13  Variant Effect Prediction",
    "section": "13.8 Open Challenges and Future Directions",
    "text": "13.8 Open Challenges and Future Directions\nEven these state-of-the-art systems leave major gaps:\n\nAncestry and population bias\nTraining data and labels remain skewed toward certain ancestries, raising concerns about performance and calibration in underrepresented populations.\nComplex variant patterns\nMost models focus on single-base or single-amino-acid changes. Systematic handling of:\n\nHaplotypes,\n\nIndels and structural variants,\n\nEpistatic interactions across distant loci\nis still in its infancy.\n\nIntegrating multi-omics and longitudinal data\nAlphaGenome marks a step toward unified multi-omic prediction, but dynamic phenomena (developmental trajectories, environment, time-series responses) are only lightly modeled.\nInterpretability and clinical communication\nTranslating high-dimensional predictions into explanations that clinicians and patients can understand—and that map onto emerging guidelines for AI-assisted variant interpretation—remains a human-factor challenge.\nSafe deployment and continual learning\nAs more functional datasets and clinical labels accumulate, models will need continual updating without catastrophic forgetting, along with governance frameworks to track model versions and provenance.\n\nIn the next chapters, we will connect these VEP systems to broader issues in evaluation, bias, and multi-omics integration, positioning them within the broader landscape of Genomic Foundation Models. This chapter’s models illustrate how the building blocks from earlier chapters—NGS, functional genomics, CNNs, transformers, protein and DNA language models—coalesce into powerful, end-to-end systems for variant interpretation.\n\n\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky, Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev. 2010. “A Method and Server for Predicting Damaging Missense Mutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT: Predicting Amino Acid Changes That Affect Protein Function.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs, Minmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005. “[PhastCons] Evolutionarily Conserved Elements in Vertebrate, Insect, Worm, and Yeast Genomes.” Genome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html",
    "href": "p4-ch14-omics.html",
    "title": "14  Multi-omics & Systems Context",
    "section": "",
    "text": "14.1 Why Single-omics Models Are Not Enough\nModern genomic foundation models (GFMs) excel at learning from sequences, structures, or single-omic profiles in isolation. Yet most complex traits arise from systems-level interactions: genetic variants perturb molecular networks; networks span multiple omics layers; and these layers interact with environment, development, and clinical context. A model that sees only one layer rarely captures the full story.\nThis chapter surveys how deep learning extends beyond single-omics to integrate methylation, chromatin, expression, protein, and clinical data into unified representations. Within the book structure defined by the Quarto project, this is the final chapter of Part IV and serves as a bridge from model-centric architecture design to systems-level, clinically grounded applications.\nWe focus on several archetypal systems:\nTogether, these approaches illustrate emerging design patterns for systems-aware GFMs that move from single sequences to whole-patient representations.\nEarlier chapters emphasized how sequence-based models can predict variant effects from local DNA or protein context. These models already improve causal variant prioritization and polygenic risk scoring. However, they typically assume a narrow view of biology:\nReal diseases violate all three assumptions:\nChapter 3 highlighted the “missing heritability” and limited portability of traditional GWAS and linear PGS, motivating sequence-based deep learning. Here we take the next step: combining sequence-derived features with multi-omics and systems-level models that better reflect biological organization.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#why-single-omics-models-are-not-enough",
    "href": "p4-ch14-omics.html#why-single-omics-models-are-not-enough",
    "title": "14  Multi-omics & Systems Context",
    "section": "",
    "text": "Single layer: A CNN or transformer may see only DNA sequence or only expression.\n\nAdditive effects: Many downstream uses still treat variant effects as additively summing across loci.\n\nStatic context: Models rarely account for dynamic state (cell type, developmental stage, environment).\n\n\n\nRegulation is multi-layered: genetic variants alter chromatin accessibility and DNA methylation, which modulate transcription, splicing, translation, and protein modification.\n\nEffects are context-dependent: the same variant can be benign in one tissue and pathogenic in another.\n\nRisk is combinatorial: epistasis and pathway-level perturbations play a significant role in many complex traits.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#foundations-of-multi-omics-integration",
    "href": "p4-ch14-omics.html#foundations-of-multi-omics-integration",
    "title": "14  Multi-omics & Systems Context",
    "section": "14.2 Foundations of Multi-omics Integration",
    "text": "14.2 Foundations of Multi-omics Integration\nMulti-omics data come in several flavors:\n\nBulk-level profiles (e.g., GWAS variants, bulk RNA-seq, bulk proteomics)\n\nSingle-cell modalities (scRNA-seq, scATAC-seq, multiome, spatial omics)\n\nEpigenetic readouts (DNA methylation, histone marks, chromatin conformation)\n\nClinical and environmental covariates (EHR, labs, lifestyle)\n\nIntegration strategies typically fall into three categories:\n\nEarly fusion (feature-level)\n\nConcatenate normalized features from multiple omics and feed them into a single model.\n\nStraightforward but sensitive to scaling, missing data, and modality imbalance.\n\nIntermediate fusion (shared latent space)\n\nLearn modality-specific encoders that map each omic into a common latent space.\n\nAlign latent spaces via reconstruction losses, contrastive terms, or graph constraints.\n\nThis is the dominant design in modern multi-omics deep learning.\n\nLate fusion (prediction-level)\n\nTrain separate models per modality; combine outputs via ensemble or meta-model.\n\nRobust to missing modalities but may underutilize cross-omic structure.\n\n\nModern frameworks like GLUE and multi-omics GNNs adopt intermediate fusion, often with graphs encoding known or inferred relationships (e.g., gene–peak, gene–TF, protein–protein, or sample similarity networks). The rest of this chapter traces how these design choices implement systems-level reasoning in practice.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#cpgpt-a-foundation-model-for-dna-methylation",
    "href": "p4-ch14-omics.html#cpgpt-a-foundation-model-for-dna-methylation",
    "title": "14  Multi-omics & Systems Context",
    "section": "14.3 CpGPT: A Foundation Model for DNA Methylation",
    "text": "14.3 CpGPT: A Foundation Model for DNA Methylation\n\n14.3.1 Motivation: Methylation as a Systems Hub\nDNA methylation sits at a crucial junction between genotype, environment, and phenotype:\n\nIt integrates genetic, developmental, and environmental influences.\n\nIt encodes cell type and cell state information.\n\nIt is predictive of aging, mortality, and disease risk.\n\nTraditional methylation models are task-specific (e.g., age clocks, mortality predictors). CpGPT reframes methylation as a foundation modeling problem, using large-scale pretraining to unlock downstream tasks.\n\n\n14.3.2 Architecture and Pretraining\nCpGPT (Cytosine-phosphate-Guanine Pretrained Transformer) is trained on large-scale collections of whole-genome and array-based methylation profiles. Conceptually, CpGPT treats methylomes as sequences or sets of CpG sites, and uses transformer-style self-attention to model:\n\nLocal CpG correlations (e.g., CpG islands)\n\nLong-range coordination across genomic regions\n\nGlobal sample-level variation (e.g., age, disease status)\n\nKey aspects:\n\nMasked modeling objectives: Learn to reconstruct held-out CpG values from context.\n\nMulti-task pretraining: Auxiliary tasks like array conversion or reference mapping encourage robust representations.\n\nSample embeddings: The [CLS]-like embedding for each sample acts as a compact, task-agnostic representation of its methylome.\n\n\n\n14.3.3 Zero-shot and Fine-tuned Tasks\nBecause CpGPT is trained on diverse cohorts, it exhibits zero-shot or few-shot generalization to new tasks:\n\nImputation and array conversion: Fill in missing CpGs or harmonize different methylation platforms.\n\nChronological age and mortality prediction: Yield clocks that match or exceed specialized models.\n\nSample classification: Distinguish tissues, disease states, or exposure profiles.\n\nIn a multi-omics context, CpGPT-derived embeddings can serve as:\n\nInputs to downstream predictors (e.g., risk scores, prognosis models).\n\nOne modality in a shared latent space (with expression, proteomics, etc.).\n\nA way to inject epigenetic state into otherwise sequence-centric GFMs.\n\nConceptually, CpGPT is an example of a single-omic foundation model that is designed to plug into multi-omics architectures.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#glue-graph-linked-unified-embedding-for-single-cell-multi-omics",
    "href": "p4-ch14-omics.html#glue-graph-linked-unified-embedding-for-single-cell-multi-omics",
    "title": "14  Multi-omics & Systems Context",
    "section": "14.4 GLUE: Graph-linked Unified Embedding for Single-cell Multi-omics",
    "text": "14.4 GLUE: Graph-linked Unified Embedding for Single-cell Multi-omics\n\n14.4.1 The Unpaired Single-cell Integration Problem\nSingle-cell experiments often profile different modalities in different cells—for instance:\n\nSome cells with scRNA-seq only\n\nOther cells with scATAC-seq only\n\nSometimes a small subset with both (multiome) or additional modalities (e.g., protein, methylation)\n\nThe central challenge: build a unified atlas that aligns these cells in a common space, recovers cell types and trajectories, and infers regulatory networks.\nGLUE (Graph-Linked Unified Embedding) addresses this by combining modality-specific encoders with a graph of biological prior knowledge linking features across omics.\n\n\n14.4.2 Architecture\nGLUE consists of three key components:\n\nModality-specific variational autoencoders (VAEs)\n\nEach omic (e.g., RNA, ATAC) has its own encoder–decoder pair.\n\nEncoders map cells to a low-dimensional latent embedding; decoders reconstruct modality-specific features.\n\nFeature graph and SCGLUE\n\nFeatures (genes, peaks, motifs) form a graph whose edges capture biological relationships: e.g., a peak linked to a gene’s promoter or enhancer, or TF binding motifs affecting genes.\n\nA graph neural network (GNN) learns feature embeddings consistent with this graph.\n\nAlignment objectives\n\nLoss terms encourage the cell latent spaces to align (so RNA-only and ATAC-only cells with similar biology end up near each other).\n\nThe feature embeddings are tied to the cell latents via generative decoders, enforcing consistency between data and prior graph.\n\n\nThe result is a unified embedding in which cells from multiple modalities can be jointly clustered, visualized, and used for downstream tasks.\n\n\n14.4.3 Applications\nThe GLUE framework has demonstrated:\n\nMulti-omics integration (RNA, ATAC, methylation or protein) at single-cell resolution.\n\nRegulatory network inference by linking chromatin features to gene expression through the feature graph.\n\nAtlas construction over large cohorts, correcting earlier annotation errors and unifying datasets across labs.\n\nFrom the perspective of GFMs, GLUE exemplifies graph-guided multi-modal pretraining: modality-specific encoders learn a shared latent space regularized by biological networks, enabling reuse across tasks and tissues.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#gnn-based-multi-omics-cancer-subtyping-mogcn-cgmega-and-beyond",
    "href": "p4-ch14-omics.html#gnn-based-multi-omics-cancer-subtyping-mogcn-cgmega-and-beyond",
    "title": "14  Multi-omics & Systems Context",
    "section": "14.5 GNN-based Multi-omics Cancer Subtyping: MoGCN, CGMega, and Beyond",
    "text": "14.5 GNN-based Multi-omics Cancer Subtyping: MoGCN, CGMega, and Beyond\nCancer is inherently multi-omic: driver mutations, copy number changes, epigenetic reprogramming, and transcriptional rewiring jointly define tumor subtypes. Multi-omics cancer subtyping models increasingly rely on graph neural networks to capture this complexity.\n\n14.5.1 MoGCN: Patient Graphs from Multi-omics\nMoGCN is a graph-convolutional framework for cancer subtype classification that integrates genomics, transcriptomics, and proteomics.\nDesign:\n\nEach patient is a node in a graph; edges encode similarity (e.g., based on expression or multi-omics features).\n\nFor each omic, a GCN learns modality-specific latent representations.\n\nThese representations are concatenated into a joint embedding per patient.\n\nA classifier operating on node embeddings predicts cancer subtypes (e.g., BRCA subtypes).\n\nBenefits:\n\nCaptures non-linear relationships between patients in a data-driven graph.\n\nNaturally integrates multiple omics via multi-view GCNs.\n\nEnables subtype discovery and interpretation via graph structure and learned embeddings.\n\n\n\n14.5.2 CGMega: Multi-omics Cancer Gene Modules\nWhere MoGCN focuses on patient-level graphs, CGMega operates on gene-level graphs:\n\nNodes represent genes; edges capture multi-omics relationships (expression, copy number, methylation, 3D genome contacts, etc.).\n\nA graph attention network learns cancer gene modules—subsets of genes that co-vary across omics and are associated with phenotypes.\n\nThis module-centric view aligns with systems biology: instead of single-gene markers, CGMega identifies network-level signatures that better reflect pathway dysregulation.\n\n\n14.5.3 Design Patterns and Alternatives\nA growing ecosystem of multi-omics subtyping methods uses related patterns:\n\nContrastive learning for multi-omics sample embeddings.\n\nGenerative models (e.g., GAN-based subtyping) that jointly model multiple omics for unsupervised clustering.\n\nTransformer-based hybrids that blend MLPs and transformer blocks for high-dimensional omics.\n\nCommon themes:\n\nModality-specific encoders with shared latent spaces\n\nGraphs capturing patient–patient or gene–gene relationships\n\nEmphasis on interpretability via clusters, modules, or attention over features\n\nThese cancer models illustrate how multi-omics integration naturally leads to graph-structured GFMs, where sequences, epigenetics, and expression are all nodes in a learned biological network.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#rare-variants-and-epistasis-in-systems-context",
    "href": "p4-ch14-omics.html#rare-variants-and-epistasis-in-systems-context",
    "title": "14  Multi-omics & Systems Context",
    "section": "14.6 Rare Variants and Epistasis in Systems Context",
    "text": "14.6 Rare Variants and Epistasis in Systems Context\nChapter 3 discussed how standard PGS methods often ignore rare variants and epistasis, despite their importance for individual-level risk and disease mechanism. Multi-omics and systems models offer a framework to incorporate these effects more effectively.\n\n14.6.1 DeepRVAT: Set-based Rare Variant Burden Modeling\nDeepRVAT (Deep Rare Variant Association Testing) learns gene-level impairment scores from rare variant annotations and genotypes using set neural networks.\nKey properties:\n\nTreats each gene’s rare variants as an unordered set.\n\nLearns a trait-agnostic gene impairment score that generalizes across traits.\n\nImproves both gene discovery and detection of high-risk individuals across many complex traits.\n\nConceptually, DeepRVAT bridges the gap between variant-level annotations (e.g., VEP, conservation, structure-based predictions) and gene-level burden, making it naturally compatible with sequence-based variant effect models introduced earlier in the book.\n\n\n14.6.2 NeEDL: Network-based Epistasis Detection\nNeEDL (Network-based Epistasis Detection via Local search) uses network medicine and quantum-inspired optimization to identify epistatic interactions among SNPs.\nCore ideas:\n\nBuild a network of SNPs and genes based on biological priors and GWAS signals.\n\nUse local search strategies to explore combinations of variants that jointly influence disease.\n\nPrioritize interpretable epistatic modules that map onto pathways and cellular processes.\n\nNeEDL does not yet operate as a full GFM, but it points toward systems-level combinatorial reasoning that future GFMs will need to support.\n\n\n14.6.3 G2PT: Hierarchical Genotype-to-Phenotype Transformers\nG2PT (Genotype-to-Phenotype Transformer) explicitly models hierarchical structure:\n\nVariant-level signals aggregate into genes.\n\nGenes aggregate into systems (e.g., pathways, tissues).\n\nSystems collectively determine phenotypes and polygenic risk.\n\nArchitecturally:\n\nUses transformer blocks to model interactions at each level.\n\nIncorporates prior knowledge (e.g., gene–pathway membership) to structure attention patterns.\n\nProvides explanations by attributing risk to specific variants, genes, and systems.\n\nG2PT can be viewed as an early example of a systems-aware GFM for genotype data, unifying additive and interaction effects within a single deep model.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#deep-learning-enhanced-polygenic-risk-and-fine-mapping",
    "href": "p4-ch14-omics.html#deep-learning-enhanced-polygenic-risk-and-fine-mapping",
    "title": "14  Multi-omics & Systems Context",
    "section": "14.7 Deep Learning-enhanced Polygenic Risk and Fine-mapping",
    "text": "14.7 Deep Learning-enhanced Polygenic Risk and Fine-mapping\nChapter 3 framed PGS as linear weighted sums of SNP effects. Deep learning extends this paradigm by:\n\nModeling non-linear interactions and context dependence\n\nIntegrating multi-omics features as priors or inputs\n\nSharing information across ancestries and cohorts\n\n\n14.7.1 Deep-learning PGS (e.g., Delphi-like frameworks)\nDeep-learning PGS frameworks learn complex functions of genotype and covariates, rather than relying on additive SNP weights.\nKey contributions:\n\nIncorporate non-genetic risk factors alongside genome-wide variants.\n\nLearn non-linear functions that can capture dominance, epistasis, and interactions with covariates.\n\nDemonstrate improved discrimination over traditional PGS across several traits.\n\nFrom a systems perspective, these models represent a move toward whole-patient risk modeling, albeit still primarily from genotype + covariates, without explicit multi-omics integration.\n\n\n14.7.2 MIFM and Multi-ancestry Fine-mapping\nMultiple-instance fine-mapping frameworks (MIFM-like methods) address a key bottleneck: lack of per-variant labels. Instead, we often know only that some variant(s) in a locus are causal. This is formulated as a multiple-instance learning problem:\n\nEach locus is a “bag” of variants.\n\nLoci with significant GWAS signals form positive bags; others form negative bags.\n\nA deep model learns to assign high scores to causal variants within positive bags.\n\nRelated methods in multi-ancestry contexts combine signals across cohorts and ancestries, leveraging divergent LD patterns to refine causal inference.\nConnections to earlier chapters:\n\nVariant effect predictors (Chapters 5–7, 13) can supply per-variant features.\n\nMulti-omics models (this chapter) provide functional priors (e.g., regulatory activity, methylation, chromatin accessibility).\n\nMIFM-type frameworks integrate these priors with GWAS evidence to produce more accurate, ancestry-aware fine-mapping.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#design-patterns-for-multi-omics-and-systems-gfms",
    "href": "p4-ch14-omics.html#design-patterns-for-multi-omics-and-systems-gfms",
    "title": "14  Multi-omics & Systems Context",
    "section": "14.8 Design Patterns for Multi-omics and Systems GFMs",
    "text": "14.8 Design Patterns for Multi-omics and Systems GFMs\nPulling these examples together, several design patterns emerge for systems-level GFMs:\n\nModality-specific encoders + shared latent space\n\nCpGPT, GLUE, and many multi-omics subtyping models use separate encoders for each omic, aligned in a common embedding space.\n\nThis design supports flexible inference with missing modalities and incremental addition of new data types.\n\nGraph-guided integration\n\nGLUE’s feature graph, CGMega’s gene modules, and NeEDL’s epistasis networks all use prior or learned graphs to structure learning.\n\nGNNs, graph transformers, and attention over graph edges are natural tools for encoding biological networks.\n\nHierarchical modeling\n\nG2PT formalizes the hierarchy from variants → genes → systems → phenotypes.\n\nSimilar hierarchies can be defined for omics layers: sequence → chromatin → methylation → expression → protein → clinical traits.\n\nSet- and bag-based learning\n\nDeepRVAT and MIFM treat variants or loci as sets/bags with permutation-invariant architectures.\n\nThis is crucial when sample sizes are large, labels are sparse, and order is biologically meaningless.\n\nFoundation pretraining + task-specific adaptation\n\nCpGPT is pretrained on massive methylation datasets and then adapted to tasks like aging clocks, mortality prediction, or disease classification.\n\nFuture models may pretrain jointly on sequence, chromatin, methylation, expression, and clinical data, then specialize for specific traits.\n\n\nThese patterns collectively point toward general-purpose systems GFMs that can ingest heterogeneous biological data and output risk predictions, mechanistic hypotheses, or treatment recommendations.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#practical-pitfalls-and-considerations",
    "href": "p4-ch14-omics.html#practical-pitfalls-and-considerations",
    "title": "14  Multi-omics & Systems Context",
    "section": "14.9 Practical Pitfalls and Considerations",
    "text": "14.9 Practical Pitfalls and Considerations\nDespite impressive progress, multi-omics and systems GFMs are especially vulnerable to confounding and overinterpretation—issues examined in depth in Chapter 16. Key challenges include:\n\nBatch effects and platform heterogeneity\n\nDifferent omics layers often come from different assays, labs, or time points.\n\nIntegration methods can inadvertently encode batch structure rather than biology if not properly corrected.\n\nSample size and missingness\n\nMulti-omics datasets are typically smaller than single-omic datasets.\n\nMany samples lack certain modalities, requiring robust handling of missing data.\n\nPopulation diversity and fairness\n\nAs highlighted for PGS, representation of diverse ancestries is essential.\n\nMulti-omics GFMs risk amplifying disparities if trained primarily on European-ancestry or high-resource cohorts.\n\nEvaluation complexity\n\nMulti-omics models can be evaluated at many levels: predictive performance, biological consistency, network plausibility, and clinical utility.\n\nOverfitting to proxy metrics (e.g., clustering quality) may not translate to actionable biology.\n\nInterpretability and causal inference\n\nAttention or feature importance scores are not guarantees of causal mechanism.\n\nIntegrating deep models with perturbation data (e.g., CRISPR screens) and robust causal frameworks remains an open frontier.\n\n\nCareful experimental design, thoughtful validation, and transparent reporting are therefore especially crucial for multi-omics GFMs.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p4-ch14-omics.html#outlook-toward-whole-patient-foundation-models",
    "href": "p4-ch14-omics.html#outlook-toward-whole-patient-foundation-models",
    "title": "14  Multi-omics & Systems Context",
    "section": "14.10 Outlook: Toward Whole-patient Foundation Models",
    "text": "14.10 Outlook: Toward Whole-patient Foundation Models\nThe methods in this chapter sketch an endgame for genomic deep learning:\n\nGenome-wide variant and sequence representation via hybrid CNN/transformer/SSM architectures (Chapters 10–13).\n\nMulti-omics integration through graph-guided latent spaces (CpGPT, GLUE, MoGCN, CGMega).\n\nSystems-level reasoning about rare variants and epistasis (DeepRVAT, NeEDL, G2PT).\n\nClinically oriented risk modeling with deep PGS and fine-mapping (Delphi-like and MIFM-like frameworks).\n\nA future whole-patient foundation model might:\n\nJointly encode genotype, methylome, chromatin state, expression, proteomics, imaging, and EHR data.\n\nProvide unified representations across tissues, cell types, and time points.\n\nOffer calibrated, equitable predictions of disease risk and treatment response.\n\nSupport mechanistic queries like “which pathways mediate this variant’s effect in this tissue?” or “which interventions counteract this rare variant burden in this patient?”\n\nRealizing this vision will require advances in data sharing, privacy-preserving learning, scalable architecture design, and causal validation. But the methods surveyed here show that moving beyond single-omics is not just incremental—it fundamentally changes what kinds of questions genomic models can answer, bringing us closer to truly systems-level, clinically actionable genomics.",
    "crumbs": [
      "Part IV: GFMs & Multi-omics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multi-omics & Systems Context</span>"
    ]
  },
  {
    "objectID": "p5-ch15-eval.html",
    "href": "p5-ch15-eval.html",
    "title": "15  Model Evaluation & Benchmarks",
    "section": "",
    "text": "15.1 Evaluation as a Multi-Scale Problem\nBy now, we have seen genomic models at almost every scale:\nEach chapter introduced local metrics and benchmarks. What has been missing is a single place to answer:\nThis chapter provides that unifying view. We:\nThroughout, the theme is that architecture and scale matter, but evaluation choices often matter more.\nGenomic models are deployed at very different scales. It helps to keep a simple mental pyramid in mind:\nGood evaluation starts from the intended level of action:\nThe rest of the chapter climbs this pyramid, while keeping a few core metric families in view.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Evaluation & Benchmarks</span>"
    ]
  },
  {
    "objectID": "p5-ch15-eval.html#evaluation-as-a-multi-scale-problem",
    "href": "p5-ch15-eval.html#evaluation-as-a-multi-scale-problem",
    "title": "15  Model Evaluation & Benchmarks",
    "section": "",
    "text": "Molecular / regulatory\n\nInputs: local sequence, epigenomic context.\n\nOutputs: chromatin accessibility, histone marks, TF binding, splicing outcomes, expression levels.\n\nExamples: DeepSEA-style chromatin models, SpliceAI, Enformer.\n\nVariant-level\n\nInputs: a specific variant (SNV, indel, structural variant) and its context.\n\nOutputs: pathogenicity scores, predicted molecular impact, fine-mapping posterior probabilities.\n\nExamples: CADD-style deleteriousness scores, AlphaMissense-like VEPs, MIFM fine-mapping posteriors.\n\nTrait / individual-level\n\nInputs: a person’s genotype/sequence plus other features.\n\nOutputs: risk scores for complex traits, predicted phenotypes, endophenotypes.\n\nExamples: classical PGS, GFM-augmented risk scores (Chapter 3; Chapter 18).\n\nClinical / decision-level\n\nInputs: model predictions plus context (guidelines, utility assumptions).\n\nOutputs: decisions (treat vs not treat, screen vs not screen), enriched cohorts, trial eligibility.\n\nExamples: screening strategies, clinical decision support tools.\n\n\n\n\nIf the goal is variant prioritization in a rare disease pipeline, improvement in AUROC on a chromatin benchmark is only indirectly relevant.\n\nIf the goal is clinical risk stratification, better perplexity on a DNA language model test set is useful only insofar as it leads to more discriminative, better calibrated risk scores.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Evaluation & Benchmarks</span>"
    ]
  },
  {
    "objectID": "p5-ch15-eval.html#metric-families-across-genomic-tasks",
    "href": "p5-ch15-eval.html#metric-families-across-genomic-tasks",
    "title": "15  Model Evaluation & Benchmarks",
    "section": "15.2 Metric Families Across Genomic Tasks",
    "text": "15.2 Metric Families Across Genomic Tasks\nMost evaluation in this book falls into four broad metric families.\n\n15.2.1 Classification Metrics\nFor binary or multi-class outputs (e.g., pathogenic vs benign; open vs closed chromatin; presence vs absence of a histone mark):\n\nAUROC (AUC) – probability that a randomly chosen positive is ranked above a randomly chosen negative.\n\nAUPRC – precision–recall area; more informative when positives are rare (e.g., pathogenic variants among many benign ones).\n\nAccuracy, sensitivity, specificity – intuitive but sensitive to class imbalance and thresholds.\n\nTypical patterns:\n\nVariant effect predictors and clinical risk models report AUROC/AUPRC for prioritization.\n\nRegulatory prediction models often report per-task AUROC averaged over hundreds of chromatin assays.\n\n\n\n15.2.2 Regression and Correlation Metrics\nFor continuous outputs (expression levels, log-odds of accessibility, quantitative traits):\n\nPearson correlation $ r $ – linear association between predicted and observed values.\n\nSpearman correlation $ $ – rank-based association; robust to monotone transformations.\n\n$ R^2 $ – fraction of variance explained; often computed against a simple baseline (e.g., mean-only model).\n\nSequence-to-expression and multi-omics models frequently use correlation between predicted and observed tracks (e.g., Enformer-like evaluations), while PGS performance is often reported as incremental $ R^2 $ over clinical covariates.\n\n\n15.2.3 Ranking and Prioritization Metrics\nMany genomics workflows are fundamentally about ranking:\n\nPrioritizing variants in a locus for follow-up.\n\nRanking genes or targets for experimental validation.\n\nSelecting individuals at highest risk for screening.\n\nIn addition to AUROC/AUPRC, useful ranking metrics include:\n\nTop-k recall / enrichment – fraction of true positives in the top $ k $ predictions.\n\nEnrichment over baseline – how much more likely a high-scoring bucket is to contain true positives compared to random.\n\nNormalized Discounted Cumulative Gain (NDCG) – emphasizes getting highly relevant items near the top.\n\nThese metrics better align with practical questions such as “how many real Mendelian variants would land in the top 20 candidates?”\n\n\n15.2.4 Generative and Language-Model Metrics\nSelf-supervised genomic language models (Chapter 10) introduce their own metrics:\n\nPerplexity / cross-entropy on masked-token reconstruction tasks.\n\nBits-per-base for next-token prediction or compression-style objectives.\n\nThese are important for representation quality and for comparing pretraining runs, but:\n\nThey are distribution-specific (tied to the pretraining corpus).\n\nImprovements in perplexity do not automatically translate into better variant or trait predictions.\n\nAs a result, generative metrics should be paired with downstream task metrics (classification, regression, ranking) to assess real utility.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Evaluation & Benchmarks</span>"
    ]
  },
  {
    "objectID": "p5-ch15-eval.html#levels-of-evaluation-from-base-pairs-to-bedside",
    "href": "p5-ch15-eval.html#levels-of-evaluation-from-base-pairs-to-bedside",
    "title": "15  Model Evaluation & Benchmarks",
    "section": "15.3 Levels of Evaluation: From Base Pairs to Bedside",
    "text": "15.3 Levels of Evaluation: From Base Pairs to Bedside\nWe now walk through the pyramid from molecular readouts to clinical decisions, focusing on what “good evaluation” looks like at each level.\n\n15.3.1 Molecular and Regulatory-Level Evaluation\nTasks:\n\nPredicting chromatin accessibility, histone marks, TF binding profiles.\n\nPredicting splicing outcomes (e.g., psi values) or transcription start/termination sites.\n\nPredicting MPRA readouts or CRISPR perturbation effects.\n\nCommon evaluation setups:\n\nMulti-task classification: AUROC/AUPRC per assay, then averaged (with or without weighting).\n\nTrack-wise regression: Pearson/Spearman correlation between predicted and observed signal profiles.\n\nOut-of-cell-type prediction: training on some cell types and testing on others.\n\nKey design choices:\n\nGranularity of labels – base-resolution vs binning (e.g., 128 bp bins).\n\nContext windows – do we test long-range generalization (Enformer-like) or local contexts?\n\nHeld-out biology – new TFs, new cell types, new loci (see splits below).\n\nPitfalls:\n\nOverfitting to specific assays or idiosyncratic lab protocols.\n\nInadvertent leakage when nearby genomic regions or replicate experiments are split across train and test.\n\n\n\n15.3.2 Variant-Level Evaluation\nTasks:\n\nClassifying variants as pathogenic vs benign, or damaging vs tolerated.\n\nPredicting functional impact (e.g., effect on splicing, expression, protein stability).\n\nFine-mapping: assigning posterior probabilities of causality to variants in associated loci.\n\nCommon benchmarks:\n\nClinical labels: ClinVar, HGMD, curated variant sets from diagnostic labs.\n\nPopulation-based labels: allele frequency strata (e.g., ultra-rare vs common) in gnomAD-like resources.\n\nFunctional assays: saturation mutagenesis, MPRAs, deep mutational scanning.\n\nMetrics:\n\nAUROC/AUPRC on binary labels (e.g., pathogenic vs benign).\n\nCorrelation or rank metrics against experimental effect sizes.\n\nCalibration-style metrics for probabilistic outputs (e.g., reliability diagrams for pathogenicity probabilities or fine-mapping posteriors).\n\nDesign questions:\n\nWhat is the negative class? Common, presumably benign variants; frequency-matched controls; synonymous variants; or synthetic negatives as in CADD (Chapter 4).\n\nWhat is held out? Genes, loci, or variant types unseen during training.\n\nHow are multiple variants per locus handled? Evaluating top-k recall of causal variants per risk locus is often more informative than global AUC.\n\nThis level is also where issues like circularity—scores trained on ClinVar then evaluated on overlapping variants—are especially acute; we return to this in Chapter 16.\n\n\n15.3.3 Trait- and Individual-Level Evaluation\nTasks:\n\nPredicting quantitative traits (e.g., LDL, height, eGFR) from genotypes and other features.\n\nCase–control risk prediction for complex diseases (e.g., CAD, T2D).\n\nMulti-trait and multi-task risk modeling.\n\nMetrics:\n\nIncremental $ R^2 $ for quantitative traits – variance explained by genomic features over clinical covariates.\n\nAUROC/AUPRC / C-index for binary or time-to-event outcomes.\n\nNet reclassification improvement (NRI) – how often individuals are moved across clinically meaningful risk thresholds in the correct direction.\n\nImportant evaluation settings:\n\nWithin-ancestry vs cross-ancestry performance (building on Chapter 3).\n\nWithin-cohort vs external validation (e.g., train in one biobank, test in another).\n\nJoint vs marginal contribution of genetics when combined with EHR and multi-omics (Chapter 14).\n\nEven for purely “research” models, reporting absolute performance (e.g., AUROC) alongside incremental gain over strong baselines is essential for understanding real impact.\n\n\n15.3.4 Clinical and Decision-Level Evaluation\nClinical risk models, treatment response predictors, and trial enrichment models (Chapter 18) ultimately need to be evaluated in terms of decisions, not just scores.\nBeyond discrimination and calibration, important concepts include:\n\nDecision curves and net benefit – compare different thresholds or policies by weighting true positives vs false positives based on clinical utilities.\n\nCost-sensitive and utility-aware evaluation – different misclassification costs (e.g., missing a high-risk patient vs unnecessary screening).\n\nProspective and interventional evaluation – randomized trials, pragmatic trials, and observational implementations with careful monitoring.\n\nThis chapter gives only a high-level overview; Chapter 18 goes deeper into clinical metrics and deployment, and Chapter 19 discusses evaluation of variant-centric discovery workflows.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Evaluation & Benchmarks</span>"
    ]
  },
  {
    "objectID": "p5-ch15-eval.html#data-splits-leakage-and-robustness",
    "href": "p5-ch15-eval.html#data-splits-leakage-and-robustness",
    "title": "15  Model Evaluation & Benchmarks",
    "section": "15.4 Data Splits, Leakage, and Robustness",
    "text": "15.4 Data Splits, Leakage, and Robustness\nMetrics mean little without well-designed splits. In genomics, the usual “random 80/10/10” split often fails to test the generalization we care about.\n\n15.4.1 Axes of Splitting\nCommon axes along which we can (and often should) split:\n\nBy individual – ensure that genomes from the same person or family do not appear in both training and test sets.\n\nBy locus / region – hold out contiguous genomic regions (e.g., specific chromosomes or megabase windows).\n\nBy gene / target – for VEP and protein models, hold out entire genes or protein families.\n\nBy assay / cell type / tissue – train on some contexts and test on unseen ones.\n\nBy ancestry / cohort – train in one ancestry or cohort and evaluate on others.\n\nDifferent scientific questions imply different splits:\n\n“Can this model generalize to new loci in the same cell type?” → locus or chromosome-based splits.\n\n“Can it generalize to new cell types?” → cell-type splits.\n\n“Can it generalize to different populations or clinics?” → ancestry and cohort splits.\n\n\n\n15.4.2 Types of Leakage\nLeakage arises when information about the test set sneaks into training:\n\nDuplicate or near-duplicate sequences across splits (e.g., overlapping windows around the same variant).\n\nShared individuals or families across train and test.\n\nBenchmark construction leakage – when labels are derived from resources that also guided model design or pretraining.\n\nHyperparameter tuning leakage – repeatedly evaluating on the test set while choosing checkpoints.\n\nChapter 16 focuses on confounders and leakage as sources of biased performance estimates; here, the takeaway is practical:\n\nAlways define the split to match the generalization you care about, then audit for potential linkage/dataset overlap.\n\n\n\n15.4.3 Robustness and Distribution Shift\nRobustness is evaluated by deliberately shifting the data distribution:\n\nTechnical shifts – new sequencing platforms, coverage levels, or assay protocols.\n\nBiological shifts – new species, tissues, disease subtypes, or ancestries.\n\nClinical shifts – new hospitals, care patterns, or time periods.\n\nRobustness evaluations often look like:\n\nTraining on one platform or cohort and testing on another.\n\nComparing performance across subgroups (e.g., ancestry-stratified AUROC).\n\nStress-testing models under label noise or missingness.\n\nThese experiments often reveal that performance on curated, i.i.d. benchmarks overestimates usefulness in messy real-world settings, especially for high-stakes clinical decisions.\nA model that performs well on curated benchmarks may still struggle in real-world scenarios:\n\nPopulation diversity: Training corpora may underrepresent certain ancestries, leading to biased variant scoring (Chapter 2).\nAssay heterogeneity: Experimental conditions, labs, and technologies differ from the curated datasets used in training.\nPhenotypic complexity: Many clinically relevant phenotypes involve long causal chains—from variant to molecular consequence to tissue-level effect to disease.\n\nThus, genomic LM evaluation increasingly includes:\n\nCross-population robustness.\nOut-of-distribution testing (new tissues, cell types, or species).\nEnd-to-end evaluations on clinically relevant endpoints (e.g., disease risk prediction, rare disease diagnosis), often in combination with traditional statistical genetics tools.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Evaluation & Benchmarks</span>"
    ]
  },
  {
    "objectID": "p5-ch15-eval.html#benchmarks-leaderboards-and-their-limits",
    "href": "p5-ch15-eval.html#benchmarks-leaderboards-and-their-limits",
    "title": "15  Model Evaluation & Benchmarks",
    "section": "15.5 Benchmarks, Leaderboards, and Their Limits",
    "text": "15.5 Benchmarks, Leaderboards, and Their Limits\nBenchmark suites—such as those introduced for Nucleotide Transformer and related genomic LMs—serve important roles:\n\nProvide standardized datasets, metrics, and splits.\n\nEnable apples-to-apples comparisons between architectures.\n\nEncourage reproducibility and shared baselines.\n\nHowever, benchmark-centric culture has pitfalls:\n\nOverfitting to the benchmark – models tuned aggressively on a small panel may degrade on new tasks.\n\nNarrow task coverage – many suites focus on chromatin and TF binding, under-representing splicing, structural variation, or clinical endpoints.\n\nMisaligned incentives – chasing fractional improvements in AUROC can overshadow more important gains (e.g., robustness, calibration, fairness).\n\nGood practice:\n\nTreat benchmark scores as necessary but not sufficient.\n\nComplement them with task-specific evaluations that mirror the intended downstream usage.\n\nPeriodically refresh benchmarks to include new assays, ancestries, and edge cases.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Evaluation & Benchmarks</span>"
    ]
  },
  {
    "objectID": "p5-ch15-eval.html#evaluating-foundation-models-zero-shot-probing-and-fine-tuning",
    "href": "p5-ch15-eval.html#evaluating-foundation-models-zero-shot-probing-and-fine-tuning",
    "title": "15  Model Evaluation & Benchmarks",
    "section": "15.6 Evaluating Foundation Models: Zero-Shot, Probing, and Fine-Tuning",
    "text": "15.6 Evaluating Foundation Models: Zero-Shot, Probing, and Fine-Tuning\nGenomic foundation models (Chapter 12) complicate evaluation because there are multiple ways to use them.\n\n15.6.1 Zero-Shot and Few-Shot Evaluation\nIn zero-shot settings, we apply the pretrained model without task-specific training, e.g.:\n\nUsing masked-token probabilities to rank variants by predicted deleteriousness.\n\nUsing embedding similarities to cluster sequences or annotate motifs.\n\nEvaluation focuses on:\n\nHow well these “raw” scores correlate with functional or clinical labels.\n\nWhether few-shot adaptation (e.g., small linear heads trained on limited labeled data) already yields strong performance.\n\nZero-shot performance is a stress test of representation quality and inductive biases.\n\n\n15.6.2 Probing and Linear Evaluation\nA common pattern is to:\n\nFreeze the foundation model.\n\nExtract embeddings for sequences, variants, or loci.\n\nTrain simple probes (linear models, shallow MLPs) on downstream labels.\n\nKey evaluation questions:\n\nHow much label efficiency do we gain vs training from scratch?\n\nHow stable are probe results across random seeds and small dataset variations?\n\nDo probes perform well across diverse tasks, or only on those similar to pretraining objectives?\n\nThis regime isolates the usefulness of learned representations.\n\n\n15.6.3 Full Fine-Tuning and Task-Specific Heads\nFor high-value tasks, we often fine-tune the foundation model end-to-end:\n\nAdding task-specific heads (classification, regression, ranking).\n\nAdapting to new modalities (e.g., multi-omics fusion in Chapter 14) or clinical contexts (Chapter 18).\n\nEvaluation then looks similar to “classic” deep models, but with additional questions:\n\nTransfer vs from-scratch baselines: does fine-tuning a GFM meaningfully outperform training a comparable architecture from scratch on the same data?\n\nCatastrophic forgetting: does fine-tuning degrade performance on other tasks, and does that matter for intended use?\n\nRobustness and fairness: do foundation model features inherit or amplify biases (Chapter 16)?\n\nAcross all regimes, it is helpful to report:\n\nAbsolute performance, delta vs strong baselines, and data efficiency curves (performance vs labeled data size).",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Evaluation & Benchmarks</span>"
    ]
  },
  {
    "objectID": "p5-ch15-eval.html#uncertainty-calibration-and-reliability",
    "href": "p5-ch15-eval.html#uncertainty-calibration-and-reliability",
    "title": "15  Model Evaluation & Benchmarks",
    "section": "15.7 Uncertainty, Calibration, and Reliability",
    "text": "15.7 Uncertainty, Calibration, and Reliability\nMetrics like AUROC summarize ranking, but they say little about how trustworthy individual predictions are.\nKey concepts:\n\nCalibration – predicted probabilities match observed frequencies (e.g., variants scored at 0.8 pathogenic are truly ~80% pathogenic).\n\nEpistemic vs aleatoric uncertainty – model uncertainty due to limited data vs inherent noise in the problem.\n\nSelective prediction / abstention – models that can say “I don’t know” when confidence is low.\n\nEvaluation tools:\n\nReliability diagrams and Brier scores for calibration.\n\nCalibration curves stratified by subgroup (ancestry, sex, site) for fairness.\n\nCoverage vs accuracy curves for selective prediction: “If the model only predicts on the 50% most confident samples, how accurate is it?”\n\nFor clinical risk models, Chapter 18 covers calibration and uncertainty in more depth. For variant-centric tasks, similar tools apply to pathogenicity probabilities or fine-mapping posteriors, which must be interpreted cautiously in light of confounders (Chapter 16).",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Evaluation & Benchmarks</span>"
    ]
  },
  {
    "objectID": "p5-ch15-eval.html#putting-it-all-together-an-evaluation-checklist",
    "href": "p5-ch15-eval.html#putting-it-all-together-an-evaluation-checklist",
    "title": "15  Model Evaluation & Benchmarks",
    "section": "15.8 Putting It All Together: An Evaluation Checklist",
    "text": "15.8 Putting It All Together: An Evaluation Checklist\nWhen designing or reviewing an evaluation for a genomic model, it can help to walk through a simple checklist:\n\nWhat level is the decision?\n\nMolecular assay design, variant prioritization, patient risk, or clinical action?\n\nEnsure metrics align with that level (e.g., enrichment for variant ranking, net benefit for clinical decisions).\n\nWhat are the baselines?\n\nStrong non-deep baselines (logistic regression, classical PGS).\n\nPrior deep models (e.g., DeepSEA, SpliceAI, Enformer, earlier GFMs).\n\nReport both absolute performance and gains over these baselines.\n\nHow are splits designed?\n\nAre individuals, loci, genes, assays, and ancestries appropriately separated?\n\nIs there any plausible path for leakage or circularity?\n\nHow robust is performance?\n\nAcross cohorts, ancestries, platforms, and time.\n\nUnder label noise or missingness.\n\nAre uncertainty and calibration evaluated?\n\nFor probabilistic outputs, are calibration and decision-level trade-offs reported?\n\nAre subgroup-specific metrics examined?\n\nHow does the model behave across usage regimes?\n\nZero-shot, probing, fine-tuning for GFMs.\n\nData efficiency: does pretraining help when labeled data are scarce?\n\nWhat is the story beyond the benchmark?\n\nDoes improved performance change downstream decisions or experimental design?\n\nAre there plans for prospective or interventional evaluation when clinical deployment is envisioned?\n\n\nSubsequent chapters flesh out specific aspects of reliability:\n\nChapter 16 digs into confounders, bias, and fairness, showing how evaluation can mislead when data are structured in problematic ways.\n\nChapter 17 focuses on interpretability and mechanisms, turning models from black boxes into testable biological hypotheses.\n\nTogether, these chapters aim to help you read the rest of the book—and the emerging literature on genomic foundation models—with a critical eye toward what has really been demonstrated and how much you should trust it.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Evaluation & Benchmarks</span>"
    ]
  },
  {
    "objectID": "p5-ch16-confound.html",
    "href": "p5-ch16-confound.html",
    "title": "16  Confounders in Model Training",
    "section": "",
    "text": "16.1 Why Confounders Are Ubiquitous in Genomic ML\nIn previous chapters, we treated model performance curves and ROC–AUC numbers as if they transparently reflected how well a model learns biology. In practice, genomic data is riddled with structure that makes it dangerously easy for models—especially large, overparameterized ones—to exploit shortcuts.\nPopulation structure, technical batch effects, benchmark leakage, and label noise can all inflate headline metrics while leaving real-world performance and clinical reliability largely unchanged. These issues are not unique to deep learning; they affect traditional statistics and GWAS as well. But the scale, flexibility, and opacity of modern genomic foundation models (GFMs) make them particularly susceptible.\nThis chapter surveys the main confounders that arise when training and evaluating genomic models, and outlines practical strategies to detect, mitigate, and transparently report them. We focus on five recurring themes:\nThroughout, the key message is simple: architecture advances are only as meaningful as the datasets and evaluation protocols that support them.\nA confounder is a variable that influences both the features (e.g., genotypes, readouts) and the labels (e.g., case/control status, functional effect), creating spurious associations. In genomics, confounders abound because:\nDeep models are powerful pattern detectors. If confounders produce consistent patterns that correlate with labels, models will happily learn those shortcuts instead of the causal biology we care about. The result is impressive performance on held-out data that share the same hidden structure, but brittle behavior as soon as we change ancestry, institution, assay, or time period.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch16-confound.html#why-confounders-are-ubiquitous-in-genomic-ml",
    "href": "p5-ch16-confound.html#why-confounders-are-ubiquitous-in-genomic-ml",
    "title": "16  Confounders in Model Training",
    "section": "",
    "text": "Data are observational, not randomized. Disease labels, population sampling, and technical pipelines are all determined by real-world constraints and historical biases.\n\nPopulation structure is strong and multi-layered. Ancestry, relatedness, and local adaptation affect allele frequencies throughout the genome.\n\nTechnical pipelines are complex. Each step—sample collection, library prep, sequencing, alignment, variant calling, QC—can introduce systematic differences between cohorts.\n\nLabels are noisy. Clinical databases (e.g., ClinVar) and high-throughput assays contain uncertain and sometimes incorrect annotations.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch16-confound.html#ancestry-stratification-and-population-bias",
    "href": "p5-ch16-confound.html#ancestry-stratification-and-population-bias",
    "title": "16  Confounders in Model Training",
    "section": "16.2 Ancestry Stratification and Population Bias",
    "text": "16.2 Ancestry Stratification and Population Bias\n\n16.2.1 How ancestry becomes a shortcut\nHuman genetic variation is structured by ancestry: allele frequencies and haplotype patterns differ across populations due to demographic history, drift, and selection. Disease prevalence, environmental exposures, and health-care access are also ancestry- and region-dependent.\nThis creates a classic confounding scenario:\n\nFeatures: Genotypes or sequence variants reflect ancestry.\n\nLabels: Case/control status, disease subtype, or even “pathogenic vs. benign” annotations can vary with ancestry.\n\nIf a case cohort is primarily of one ancestry and controls are primarily of another, a model can achieve high predictive performance by acting as an ancestry classifier rather than a disease predictor. The same issue arises for variant effect prediction: variants common in one ancestry but rare in another can be spuriously tagged as pathogenic or benign because of how databases were curated.\n\n\n16.2.2 Manifestations in genomic models\nSome common ways ancestry confounding shows up:\n\nCase/control imbalance across ancestries. For example, cases over-representing individuals of European ancestry, controls over-representing other groups.\n\nReference database bias. Variant annotations derived mostly from European-ancestry cohorts; “benign” often means “common in Europeans”.\n\nImplicit ancestry markers. Cryptic relatedness, shared haplotypes, and local LD patterns let models recover ancestry even when explicit labels are removed.\n\nFor high-capacity models such as transformer-based GFMs, even subtle ancestry differences are enough to support a shortcut.\n\n\n16.2.3 Detecting ancestry confounding\nPractical diagnostics include:\n\nPCA or UMAP of genotypes/embeddings. If cases and controls cluster by ancestry, that’s a red flag.\n\nStratified performance. Evaluate metrics separately within each ancestry group; large performance drops or reversals across groups suggest confounding.\n\nAncestry-only baselines. Fit a simple classifier on ancestry PCs or self-identified ancestry alone. If this baseline approaches your model’s performance, your model is likely exploiting similar information.\n\nPermutation tests within ancestry strata. Shuffling labels within ancestry groups should destroy performance for a truly disease-specific signal, but not for models relying on cross-ancestry differences.\n\n\n\n16.2.4 Mitigating ancestry bias\nMitigation is imperfect, but several strategies help:\n\nBalanced study design. Wherever possible, recruit cases and controls with similar ancestry distributions, or match controls to cases.\n\nWithin-ancestry evaluation. Report metrics for each ancestry separately; use training–validation splits that preserve within-group structure.\n\nCovariate adjustment. Include ancestry PCs, kinship matrices, or mixed-model random effects in simpler models; for deep models, condition on or adversarially remove ancestry signals from learned embeddings.\n\nMulti-ancestry training. Train on diverse populations rather than restricting to a single ancestry, and explicitly model ancestry as a domain variable.\n\nFairness-aware objectives. Introduce regularizers or constraints that penalize performance disparities across ancestry groups, especially in clinical deployment contexts.\n\nIn later chapters on PGS and multi-omics, careful ancestry handling will be essential for equitable risk prediction.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch16-confound.html#benchmark-leakage-and-traintest-overlap",
    "href": "p5-ch16-confound.html#benchmark-leakage-and-traintest-overlap",
    "title": "16  Confounders in Model Training",
    "section": "16.3 Benchmark Leakage and Train/Test Overlap",
    "text": "16.3 Benchmark Leakage and Train/Test Overlap\nEven with perfectly balanced ancestries, evaluation can be misleading if information “leaks” from training to test sets. Leakage is especially insidious in genomics because:\n\nThe genome is highly structured and redundant.\n\nPublic datasets and benchmarks are heavily reused.\n\nMany papers do not fully specify how splits were constructed.\n\n\n16.3.1 Forms of leakage\nCommon leakage patterns include:\n\nIndividual overlap. The same person (or close relative) appears in both train and test sets, directly or via related cohorts.\n\nVariant overlap. Exact variants, or near-identical ones at the same locus, appear in both splits; this can happen when different datasets are merged.\n\nLocus-level overlap. Variants in the same gene, regulatory element, or LD block are split between train and test. A model may learn locus-specific idiosyncrasies instead of general rules.\n\nDatabase reuse leakage. Benchmarks constructed from ClinVar, gnomAD, or other public databases but evaluated against an external set that partially overlaps those sources.\n\nTime-based leakage. Models trained on data that include later submissions of the same variants or patients that are used as “future” test examples.\n\nFor large models, even very small overlaps can inflate metrics, particularly when test sets are small.\n\n\n16.3.2 Safer splitting strategies\nTo reduce leakage:\n\nIndividual-level splits. Ensure that no individual (or closely related individuals, if kinship is known) appears in both train and test sets.\n\nLocus- or gene-level splits. For variant effect prediction, split at the gene, enhancer, or genomic region level so that test loci are unseen.\n\nChromosome-based splits. For genome-wide tasks, hold out entire chromosomes or chromosome arms. This is not perfect but greatly reduces local dependency leakage.\n\nTime-based splits. Train on data up to a cutoff date and test on later data, mimicking realistic deployment.\n\nTransparent data provenance. Track the origin of each sample and variant (e.g., database version, submission ID) to avoid accidental reuse.\n\n\n\n16.3.3 Evaluation design and reporting\nBeyond the split itself, evaluation design matters:\n\nReport both in-distribution performance (same cohort) and out-of-distribution performance (new cohorts, ancestries, or technical pipelines).\n\nWhenever possible, include cross-cohort benchmarks: train on one cohort, test on another with different recruitment or sequencing characteristics.\n\nShare code and detailed recipes for dataset construction so that others can reproduce and critique splitting choices.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch16-confound.html#technical-artifacts-batch-effects-and-platform-differences",
    "href": "p5-ch16-confound.html#technical-artifacts-batch-effects-and-platform-differences",
    "title": "16  Confounders in Model Training",
    "section": "16.4 Technical Artifacts: Batch Effects and Platform Differences",
    "text": "16.4 Technical Artifacts: Batch Effects and Platform Differences\nWhile ancestry and population structure reflect biological reality, batch effects are artifacts of the measurement process. In genomics, differences in:\n\nSample collection protocols\n\nLibrary preparation kits\n\nSequencing platforms and chemistry versions\n\nRead length, depth, and coverage\n\nAlignment and variant calling pipelines\n\ncan all introduce systematic shifts in feature distributions.\n\n16.4.1 How batch effects confound models\nTechnical batches often correlate with labels:\n\nA case cohort may be sequenced at one institution on one platform, while controls are sequenced elsewhere with different protocols.\n\nA longitudinal study might switch from one capture kit or sequencer to another halfway through, coinciding with changes in enrollment criteria.\n\nPublic datasets may aggregate studies with very different technical characteristics.\n\nIn such settings, a model can achieve high accuracy by recognizing batch signatures (e.g., patterns of missingness, depth, noise spectra) rather than bona fide biological signals.\n\n\n16.4.2 Diagnosing technical confounders\nCommon diagnostics include:\n\nEmbedding visualization by batch. Project learned embeddings or expression/coverage profiles via PCA or UMAP, then color points by batch, platform, or institution. Strong clustering by these variables suggests technical structure.\n\nBatch-only baselines. Train a classifier using only batch labels or simple technical covariates (e.g., read depth, platform indicators). High baseline performance is a warning sign.\n\nNegative controls. Evaluate models on samples where labels should be uncorrelated with batch (e.g., technical replicates, randomized subsets).\n\nReplicate consistency. Examine consistency of predictions across technical replicates processed in different batches.\n\n\n\n16.4.3 Mitigating batch effects\nMitigation is an active research area; common approaches include:\n\nCareful study design. Randomize cases and controls across batches whenever possible; avoid systematic alignment between batch and outcome.\n\nPreprocessing harmonization. Use standardized pipelines for alignment and variant calling; reprocess raw data when feasible to reduce inter-study differences.\n\nStatistical batch correction. Methods such as ComBat, Harmony, and related approaches can reduce batch effects in expression or chromatin data; similar ideas can be applied to embeddings from GFMs.\n\nDomain adaptation and adversarial training. Train representations that are predictive of labels while being invariant to batch or platform (e.g., via gradient reversal layers or distribution matching objectives).\n\nExplicit multi-domain modeling. Treat each batch or platform as a domain and learn domain-conditional parameters or mixture-of-experts models.\n\nEven with aggressive correction, residual batch structure typically remains; transparent reporting and robustness checks are essential.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch16-confound.html#label-noise-and-ground-truth-uncertainty",
    "href": "p5-ch16-confound.html#label-noise-and-ground-truth-uncertainty",
    "title": "16  Confounders in Model Training",
    "section": "16.5 Label Noise and Ground-Truth Uncertainty",
    "text": "16.5 Label Noise and Ground-Truth Uncertainty\nLarge-scale genomic models rely on labels from:\n\nClinical variant interpretation databases (e.g., pathogenic vs. benign)\n\nGWAS-derived case/control status\n\nHigh-throughput functional screens (e.g., MPRA, saturation mutagenesis, CRISPR screens)\n\nCurated “gold-standard” sets for VEP, splicing predictions, or PGS\n\nThese labels are not error-free. Sources of label noise include:\n\nConflicting annotations. ClinVar often contains variants with conflicting interpretations or uncertain significance; criteria for pathogenicity change over time.\n\nAscertainment bias. Variants labeled as “benign” may simply be common in some populations; variants labeled as “pathogenic” may be enriched in clinically ascertained cohorts.\n\nMeasurement noise in functional assays. High-throughput experiments have variable reproducibility across labs, conditions, and replicates. Thresholding continuous scores into discrete classes compounds the issue.\n\nPhenotyping noise. Clinical case/control labels may be inaccurate due to misdiagnosis, incomplete records, or heterogeneous disease definitions.\n\n\n16.5.1 Consequences for models\nLabel noise can:\n\nLimit achievable performance, especially for tasks with overlapping phenotype definitions.\n\nEncourage models to learn spurious proxies that correlate with annotation errors.\n\nBias calibration and decision thresholds, particularly in imbalanced settings.\n\nIn some scenarios, training on noisy labels still improves performance if noise is roughly symmetric or if the dataset is very large. However, for rare disease variants and high-stakes predictions, even small fractions of mislabeled examples can be problematic.\n\n\n16.5.2 Strategies for robust learning with noisy labels\nApproaches to deal with label noise include:\n\nCurated subsets. Restrict training and evaluation to high-confidence annotations (e.g., ClinVar “Pathogenic” and “Benign” with multiple submitters and no conflicts), even at the cost of reduced size.\n\nSoft labels and uncertainty modeling. Use probabilistic labels derived from inter-rater disagreement, confidence scores, or continuous assay measurements rather than hard 0/1 labels.\n\nRobust losses. Employ loss functions less sensitive to mislabeled points (e.g., label smoothing, margin-based losses, or methods that down-weight high-loss outliers).\n\nNoise-aware training. Explicitly model label noise (e.g., via a noise transition matrix or latent variable models) and jointly infer true labels.\n\nConsensus across modalities. Combine evidence from protein structure, evolutionary conservation, regulatory context, and clinical data; treat disagreements as signals of uncertainty.\n\nMechanistic interpretability can also help flag model predictions that disagree with known biology, potentially identifying mislabeled examples.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch16-confound.html#cross-ancestry-pgs-transferability-and-model-fairness",
    "href": "p5-ch16-confound.html#cross-ancestry-pgs-transferability-and-model-fairness",
    "title": "16  Confounders in Model Training",
    "section": "16.6 Cross-Ancestry PGS Transferability and Model Fairness",
    "text": "16.6 Cross-Ancestry PGS Transferability and Model Fairness\nPolygenic scores (PGS) and other genome-wide predictors have gained traction as potential tools for early disease risk stratification. However, many PGS have been developed primarily in individuals of European ancestry, raising concerns about:\n\nReduced predictive accuracy in underrepresented ancestries.\n\nBiased calibration, where risk is systematically over- or under-estimated in certain groups.\n\nDownstream disparities if PGS-informed clinical decisions (e.g., screening recommendations) are applied uniformly.\n\n\n16.6.1 Why transferability fails\nReasons for poor cross-ancestry transfer include:\n\nAllele frequency differences. Effect estimates calibrated in one population may not generalize when allele frequencies change.\n\nLD pattern differences. Tagging SNPs used in PGS may capture causal variants in one ancestry but not another.\n\nGene–environment interaction. Environmental exposures and lifestyle factors that interact with genetic risk differ across populations.\n\nAscertainment and recruitment biases. Early GWAS datasets often oversampled certain ancestries, clinical populations, or socioeconomic strata.\n\nThese issues carry over to deep learning–based PGS and GFMs fine-tuned for disease prediction. Even if the underlying model is trained on diverse genomes in a self-supervised fashion, the supervised fine-tuning and evaluation data can reintroduce bias.\n\n\n16.6.2 Towards more equitable models\nApproaches to improve cross-ancestry performance and fairness include:\n\nMulti-ancestry GWAS and training data. Include diverse cohorts at the design stage rather than as an afterthought.\n\nAncestry-aware modeling. Condition effect sizes or model parameters on ancestry, or learn ancestry-invariant representations coupled with ancestry-specific calibration.\n\nTransfer learning and fine-tuning. Adapt models from ancestries with large datasets to those with smaller datasets using domain adaptation techniques.\n\nFairness metrics. Report group-wise calibration, sensitivity, specificity, and decision-curve analyses, not just overall AUC.\n\nStakeholder engagement. Work with clinicians, ethicists, and affected communities to decide when and how PGS should be used, and what constitutes acceptable performance gaps.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch16-confound.html#from-cautionary-tales-to-best-practices",
    "href": "p5-ch16-confound.html#from-cautionary-tales-to-best-practices",
    "title": "16  Confounders in Model Training",
    "section": "16.7 From Cautionary Tales to Best Practices",
    "text": "16.7 From Cautionary Tales to Best Practices\nModern genomic foundation models promise impressive capabilities: genome-scale variant effect prediction, cross-species transfer, multi-omics integration, and clinically actionable risk scores. Yet without rigorous attention to confounders, these capabilities can be overstated or misapplied.\nEmerging work on genomic evaluation frameworks emphasizes:\n\nData documentation. Detailed datasheets for datasets and benchmarks, including recruitment, ancestry composition, technical pipelines, and label provenance.\n\nRobust evaluation protocols. Cross-cohort, cross-ancestry, and time-split evaluations that stress-test models beyond their training distribution.\n\nConfounder-aware training. Explicit modeling of ancestry, batch, and label uncertainty, and the use of adversarial or domain-adaptation techniques.\n\nTransparent reporting. Clear communication of limitations, potential failure modes, and groups for whom the model has not been validated.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch16-confound.html#a-practical-checklist-for-confounder-resilient-genomic-modeling",
    "href": "p5-ch16-confound.html#a-practical-checklist-for-confounder-resilient-genomic-modeling",
    "title": "16  Confounders in Model Training",
    "section": "16.8 A Practical Checklist for Confounder-Resilient Genomic Modeling",
    "text": "16.8 A Practical Checklist for Confounder-Resilient Genomic Modeling\nTo close, here is a concise checklist you can apply when designing, training, and evaluating genomic models:\n\nPopulation structure\n\nHave you quantified ancestry and relatedness (e.g., via PCs or kinship)?\n\nAre cases and controls balanced within ancestry groups?\n\nDo you report performance stratified by ancestry?\n\nData splits and leakage\n\nAre individuals, families, and closely related samples confined to a single split?\n\nDo you split at the locus, gene, or chromosome level where appropriate?\n\nHave you checked for overlap with external databases used in evaluation?\n\nBatch and platform effects\n\nAre technical variables (batch, platform, institution) correlated with labels?\n\nHave you visualized embeddings colored by batch?\n\nDo you use harmonization, batch correction, or domain adaptation as needed?\n\nLabel quality\n\nHow are labels defined, and what is their uncertainty?\n\nDo you filter to high-confidence subsets for primary evaluation?\n\nDo you employ robust training strategies to handle label noise?\n\nCross-group performance and fairness\n\nDo you report metrics for each ancestry and relevant subgroup?\n\nAre risk scores calibrated across groups, or is group-specific calibration required?\n\nHave you considered the ethical and clinical implications of residual performance gaps?\n\nReproducibility and transparency\n\nAre dataset construction and splitting procedures fully documented and shareable?\n\nAre code and evaluation pipelines available for independent verification?\n\n\nBy systematically addressing these points, we can ensure that the gains from modern architectures—transformers, SSMs, and GFMs—translate into trustworthy advances in genomic science and medicine, rather than brittle models that merely reflect quirks of our data and history.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html",
    "href": "p5-ch17-interp.html",
    "title": "17  Interpretability & Mechanisms",
    "section": "",
    "text": "17.1 Why Interpretability Matters for Genomic Models\nDeep learning models in genomics increasingly operate as systems-level surrogates for biology: they predict chromatin features, gene expression, or variant effects directly from sequence. When such models drive mechanistic hypotheses or clinical decisions, how they make predictions becomes as important as how well they perform.\nInterpretability in this context serves at least four roles:\nThis chapter surveys the main interpretability tools developed for genomic models, from convolutional filters and saliency maps to global regulatory vocabularies and attention patterns in genomic language models (gLMs) and transformer-based regulatory models. Throughout, the emphasis is on mechanistic interpretability: moving from “what correlates with the prediction?” to “what regulatory hypothesis does the model imply?”",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#why-interpretability-matters-for-genomic-models",
    "href": "p5-ch17-interp.html#why-interpretability-matters-for-genomic-models",
    "title": "17  Interpretability & Mechanisms",
    "section": "",
    "text": "Mechanistic insight\n\nExtract sequence motifs (putative TF binding sites), regulatory grammars, and long-range interaction patterns directly from trained models.\n\nTurn “black-box” predictions into candidate mechanisms that can be tested experimentally.\n\nModel debugging and confounder detection\n\nReveal when models rely on artifacts (e.g., GC content, mappability, batch-specific motifs) instead of bona fide regulatory signals.\n\nComplement Chapter 16’s focus on data and evaluation confounders by interrogating model internals.\n\nClinical and translational trust\n\nSupport variant interpretation workflows by explaining why specific rare or de novo variants are predicted to be damaging.\n\nProvide interpretable axes of variation (e.g., motif disruptions, regulatory “sequence classes”) that can be combined with orthogonal evidence.\n\nScientific communication\n\nCondense high-dimensional latent representations into human-readable abstractions—motifs, regulatory classes, or interaction graphs—that can be shared across labs and applications.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#interpreting-convolutional-filters-as-motifs",
    "href": "p5-ch17-interp.html#interpreting-convolutional-filters-as-motifs",
    "title": "17  Interpretability & Mechanisms",
    "section": "17.2 Interpreting Convolutional Filters as Motifs",
    "text": "17.2 Interpreting Convolutional Filters as Motifs\nConvolutional neural networks (CNNs) remain a workhorse for modeling cis-regulatory sequence (Chapters 5–7). In many of these models, first-layer convolutional filters act as motif detectors:\n\nA filter slides along the one-hot encoded sequence (Chapter 8).\n\nAt each position, it computes a dot product between its weights and the local sequence window.\n\nHigh activation indicates that the subsequence closely matches the filter’s preferred pattern.\n\n\n17.2.1 From Filters to Motif Logos\nA common workflow to interpret filters:\n\nCollect high-activation instances\n\nRun the trained model on a large sequence set (e.g., training data or genome tiles).\n\nFor each filter, record positions where its activation exceeds a threshold.\n\nExtract and align subsequences\n\nPull out fixed-length windows around those positions.\n\nAlign them and compute base frequencies at each position.\n\nBuild a position weight matrix (PWM)\n\nConvert base frequencies to log-odds scores relative to a background distribution.\n\nVisualize as a sequence logo.\n\nMatch to known motif databases\n\nCompare PWMs to JASPAR or HOCOMOCO TF motif libraries using similarity scores.\n\nAnnotate filters with candidate TF identities (“this filter resembles CTCF”).\n\n\nThis procedure has been applied to models like DeepSEA and its successors to demonstrate that early layers learn motifs for canonical TFs and chromatin-associated patterns, validating that models are discovering biologically meaningful sequence features rather than arbitrary patterns.\n\n\n17.2.2 Beyond First-Layer Filters\nDeeper convolutional layers aggregate lower-level motifs:\n\nCombinatorial motifs: Filters that respond to pairs or clusters of TF motifs.\n\nGrammar patterns: Distance or orientation constraints (e.g., “ETS motif ~10 bp upstream of GATA motif”).\n\nContextual preferences: Filters that fire only in particular GC contexts or nucleosome positioning patterns.\n\nHowever, directly interpreting deeper layers becomes challenging because receptive fields expand and nonlinearities accumulate. This motivates attribution-based approaches that connect predictions back to individual bases.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#attribution-methods-connecting-bases-to-predictions",
    "href": "p5-ch17-interp.html#attribution-methods-connecting-bases-to-predictions",
    "title": "17  Interpretability & Mechanisms",
    "section": "17.3 Attribution Methods: Connecting Bases to Predictions",
    "text": "17.3 Attribution Methods: Connecting Bases to Predictions\nAttribution methods assign an “importance score” to each input base (or k-mer), reflecting how much it contributes to a prediction for a specific task and sequence.\nLet $ f(x) $ be a model predicting some output (e.g., chromatin accessibility, gene expression, or variant effect) from sequence $ x $. Attribution methods estimate the contribution of each base $ x_i $ to $ f(x) $, often for a specific output neuron (e.g., a particular cell type).\n\n17.3.1 In Silico Mutagenesis (ISM)\nIn silico mutagenesis is conceptually straightforward and model-agnostic:\n\nFor each position $ i $ and base $ b $, create a mutated sequence $ x^{(i b)} $.\n\nCompute the change in prediction\n\\[\n\\Delta f_{i,b} = f(x^{(i \\rightarrow b)}) - f(x).\n\\]\nAggregate these changes (e.g., max across non-reference alleles) to obtain a per-base importance score.\n\nVariants:\n\nSingle-nucleotide ISM: Mutate each base individually; expensive but faithful.\n\nSaturation mutagenesis: Explore all possible oligos in a window to probe combinatorial effects and grammar.\n\nVariant-specific scoring: Evaluate $ f() - f() $ for a particular SNV or indel.\n\nStrengths:\n\nTrue “what-if” causal perturbations under the model.\n\nWorks for any differentiable or non-differentiable model (including ensembles and post-processed scores).\n\nLimitations:\n\nComputationally expensive: $ O(L ||) $ forward passes for sequence length $ L $ and alphabet size $ || $.\n\nCaptures local effects; may miss distributed interactions if not designed carefully.\n\n\n\n17.3.2 Gradient-Based Methods\nGradient-based methods approximate “how much would the prediction change if we nudged this base?” via backpropagation.\n\n17.3.2.1 Vanilla Gradient / Saliency\nCompute the gradient of the output with respect to the input:\n\\[\ns_i = \\frac{\\partial f(x)}{\\partial x_i}.\n\\]\nWith one-hot encoding, this gradient can be interpreted as the sensitivity to changing the nucleotide at position $ i $. A common variant multiplies the gradient by the input (“gradient × input”).\nPros:\n\nRequires a single backward pass per sequence.\n\nEasy to implement and integrate into training workflows.\n\nCons:\n\nSusceptible to gradient saturation (zero gradients in regions where the model is already confident).\n\nNoisy saliency maps often require smoothing or aggregation across multiple noisy inputs.\n\n\n\n17.3.2.2 DeepLIFT\nDeepLIFT (Deep Learning Important FeaTures) compares neuron activations between an input and a reference (or baseline) sequence, distributing differences back to inputs using layer-wise rules rather than raw gradients. It aims to:\n\nAvoid gradient saturation.\n\nEnforce a consistency constraint: the sum of input contributions matches the difference in output between input and reference.\n\nDeepLIFT has been widely used for genomic models, particularly in conjunction with TF-MoDISco (next section), where its base-level importance scores serve as inputs for motif discovery.\n\n\n17.3.2.3 Integrated Gradients (IG)\nIntegrated Gradients compute the path integral of gradients along a linear interpolation from a reference $ x’ $ to the input $ x $:\n\\[\n\\text{IG}_i(x) = (x_i - x'_i) \\int_{\\alpha=0}^1 \\frac{\\partial f\\left(x' + \\alpha(x - x')\\right)}{\\partial x_i} d\\alpha.\n\\]\nIn practice, this integral is approximated via a Riemann sum over discrete steps. IG satisfies desirable axioms (e.g., sensitivity, implementation invariance) and tends to be less noisy than raw gradients.\nKey design considerations for all gradient-based methods:\n\nChoice of reference:\n\nRandom genomic background, dinucleotide-shuffled sequence, or an “average” non-functional sequence.\n\nDifferent references emphasize different aspects of the signal.\n\nOutput selection:\n\nSingle-task models: directly attribute the scalar output.\n\nMulti-task models: choose a specific track (e.g., H3K27ac in one cell type) or aggregate across tasks.\n\nPost-processing:\n\nSmooth along the sequence (e.g., average in sliding windows).\n\nAggregate over channels or strands.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#from-attributions-to-motifs-tf-modisco",
    "href": "p5-ch17-interp.html#from-attributions-to-motifs-tf-modisco",
    "title": "17  Interpretability & Mechanisms",
    "section": "17.4 From Attributions to Motifs: TF-MoDISco",
    "text": "17.4 From Attributions to Motifs: TF-MoDISco\nAttribution maps highlight where the model focuses, but they do not automatically yield consistent motifs or regulatory grammars. TF-MoDISco (Transcription Factor Motif Discovery from Importance Scores) was developed to bridge this gap.\n\n17.4.1 Core Idea\nRather than performing motif discovery on raw sequences, TF-MoDISco operates on base-level importance scores:\n\nCompute importance scores\n\nUse DeepLIFT, ISM, IG, or similar methods on many sequences.\n\nObtain an importance score for each base and strand.\n\nExtract “seqlets”\n\nIdentify local windows where the total importance exceeds a threshold.\n\nTreat each window (seqlet) as a candidate motif instance.\n\nCluster seqlets\n\nCompare seqlets using similarity metrics that consider both sequence and importance scores.\n\nCluster into groups corresponding to putative motifs.\n\nBuild consolidated motifs\n\nAlign seqlets within each cluster.\n\nConstruct PWMs and importance-weighted logos.\n\nOptionally match to known TF motifs.\n\nReport motif instances and grammar\n\nMap motifs back onto the genome.\n\nAnalyze co-occurrence, spacing, and orientation rules.\n\n\nWhen applied to models like BPNet, TF-MoDISco has recovered known TF motifs, discovered novel variants, and revealed grammars (e.g., directional spacing constraints) that can be validated with synthetic reporter assays.\nIn the context of genomic foundation models, an analogous workflow can be applied:\n\nUse a GFM or transformer-based model to produce base-level attributions for a specific downstream task (e.g., chromatin accessibility).\n\nRun TF-MoDISco to extract a task-specific motif vocabulary.\n\nAnalyze how motif usage changes across cell types, conditions, or species.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#interpreting-attention-and-long-range-context",
    "href": "p5-ch17-interp.html#interpreting-attention-and-long-range-context",
    "title": "17  Interpretability & Mechanisms",
    "section": "17.5 Interpreting Attention and Long-Range Context",
    "text": "17.5 Interpreting Attention and Long-Range Context\nTransformer-based models (Chapters 8–11) use self-attention to mix information across long genomic contexts, enabling them to capture distal regulatory interactions and genomic organization. Interpretability here often centers on attention patterns and long-range attribution.\n\n17.5.1 Genomic Language Models and Operon Structure (gLM)\nGenomic language models (gLMs) treat genes or genomic tokens as a sequence and train transformers to predict masked tokens, analogous to protein or text LMs. Work on gLMs trained on millions of metagenomic scaffolds shows that these models learn non-trivial genomic structure:\n\nAttention heads mark operons and co-regulated modules\n\nCertain heads specialize in connecting genes that are part of the same operon or functional module.\n\nAttention maps reveal networks of co-regulated genes, often aligning with known operon boundaries.\n\nFunctional semantics and taxonomic signals\n\nLatent representations cluster by enzymatic function and gene ontology.\n\nAttention patterns can separate clades and capture clade-specific gene neighborhoods.\n\nMechanistic interpretation\n\nThese patterns suggest the model has inferred a “syntax” of gene neighborhoods: which genes tend to co-occur and in what order, conditioned on phylogenetic context.\n\n\nWhile attention is not universally a faithful explanation of model decisions, attention analysis in gLM reveals emergent mechanistic structure that is consistent with biological organization.\n\n\n17.5.2 Distal Regulatory Elements in Enformer-Like Models\nEnformer and related models predict chromatin features and gene expression from large genomic windows (e.g., 100 kb+) by combining convolutional layers with transformer blocks.\nKey interpretability questions:\n\nWhich distal enhancers drive the predicted expression at a given transcription start site (TSS)?\n\nHow do variants in distal elements propagate to gene-level outputs?\n\nInterpretability strategies include:\n\nGradient-based attributions over long windows\n\nCompute attributions of a gene’s expression output with respect to input bases across the entire window.\n\nVisualize importance tracks to highlight putative enhancers and silencers.\n\nAttention pattern analysis\n\nIdentify attention heads that consistently link distal positions to TSS regions.\n\nRelate high-attention edges to Hi-C contact maps or chromatin interaction data.\n\nIn silico perturbation of regulatory elements\n\nDelete or scramble candidate enhancers and recompute gene expression predictions.\n\nInsert synthetic motifs or enhance motif scores to gauge dose–response relationships.\n\n\nThese analyses can reveal candidate enhancer–promoter links and TF motifs that the model deems critical for gene regulation, helping translate raw attention weights and attributions into mechanistic hypotheses.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#global-regulatory-vocabularies-sei-sequence-classes",
    "href": "p5-ch17-interp.html#global-regulatory-vocabularies-sei-sequence-classes",
    "title": "17  Interpretability & Mechanisms",
    "section": "17.6 Global Regulatory Vocabularies: Sei Sequence Classes",
    "text": "17.6 Global Regulatory Vocabularies: Sei Sequence Classes\nMost motif-based interpretation operates at the local level. Sei takes a complementary global approach by learning a vocabulary of regulatory sequence classes that summarize a vast array of chromatin profiles.\n\n17.6.1 The Sei Framework\nSei trains a deep sequence model to predict tens of thousands of chromatin profiles (TF binding, histone marks, accessibility) across many cell types directly from DNA sequence. The key interpretability step is to compress these thousands of outputs into a few dozen “sequence classes”, each representing a characteristic regulatory activity pattern:\n\nPromoter-like classes (e.g., H3K4me3-rich, TSS-proximal).\n\nEnhancer-like classes (H3K27ac, H3K4me1).\n\nRepressive classes (H3K27me3, H3K9me3).\n\nCell-type- or lineage-specific modules (e.g., neuronal, immune).\n\nEach input sequence (or variant) is assigned a score for each sequence class, effectively mapping it to a point in a low-dimensional “regulatory activity space”.\n\n\n17.6.2 Interpretation and Applications\nA regulatory vocabulary like Sei’s supports several interpretability goals:\n\nIntermediate, human-interpretable features\n\nInstead of raw high-dimensional outputs, one can reason in terms of “promoter-like,” “B-cell enhancer,” or “polycomb-repressed” scores.\n\nVariant interpretation\n\nVariants can be summarized by their shifts in sequence-class scores, yielding concise descriptions like “increases neuronal enhancer activity while decreasing repressive marks.”\n\nTrait and disease enrichment\n\nGWAS loci can be enriched for specific sequence classes, revealing tissues and regulatory programs most relevant to disease.\n\n\nThis notion of a regulatory vocabulary parallels word embeddings or topics in NLP and provides a bridge between highly multivariate model outputs and mechanistically interpretable axes of variation.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#case-study-from-base-pair-attributions-to-regulatory-grammar",
    "href": "p5-ch17-interp.html#case-study-from-base-pair-attributions-to-regulatory-grammar",
    "title": "17  Interpretability & Mechanisms",
    "section": "17.7 Case Study: From Base-Pair Attributions to Regulatory Grammar",
    "text": "17.7 Case Study: From Base-Pair Attributions to Regulatory Grammar\nPutting the pieces together, a typical mechanistic interpretability pipeline for a CNN or transformer-based regulatory model might look like:\n\nTrain a predictive model\n\nFor example, predict chromatin accessibility or TF ChIP-seq tracks from sequence.\n\nCompute base-level attributions\n\nUse DeepLIFT or IG for positive predictions in a target cell type.\n\nDiscover motifs with TF-MoDISco\n\nExtract seqlets from high-attribution regions, cluster, and derive motifs.\n\nMatch motifs to known TFs and identify novel ones.\n\nInfer grammar from motif instances\n\nAnalyze motif co-occurrence, spacing, and orientation in high-scoring sequences.\n\nUse knock-in/knock-out in silico experiments to confirm dependencies (e.g., both motifs needed, order matters).\n\nRelate motifs to sequence classes or attention patterns\n\nMap motif-rich regions to Sei sequence classes or Enformer attributions.\n\nConnect local motif grammar to global regulatory context (e.g., distal enhancer–promoter linkages, cell-type specificity).\n\nValidate with experiments or external datasets\n\nCheck whether motif disruptions align with reporter assay effects or allelic imbalance.\n\nCompare inferred enhancer–promoter links to Hi-C or CRISPR perturbation screens.\n\n\nThis integrated approach moves beyond “pretty saliency maps” toward testable hypotheses about regulatory logic.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#evaluating-interpretations-faithfulness-vs-plausibility",
    "href": "p5-ch17-interp.html#evaluating-interpretations-faithfulness-vs-plausibility",
    "title": "17  Interpretability & Mechanisms",
    "section": "17.8 Evaluating Interpretations: Faithfulness vs Plausibility",
    "text": "17.8 Evaluating Interpretations: Faithfulness vs Plausibility\nNot all explanations are equally trustworthy. Effective interpretability work must grapple with the distinction between:\n\nPlausibility: Does the explanation “look” biological (e.g., known motifs, enhancer marks)?\n\nFaithfulness: Does the explanation accurately reflect the internal computation of the model?\n\nPotential pitfalls:\n\nAttention as explanation\n\nHigh attention weights need not correspond to large changes in output; they may reflect information routing rather than causal influence.\n\nCombining attention with attribution or perturbation analyses yields more reliable insights.\n\nAttribution noise and saturation\n\nGradient-based methods can produce noisy maps or miss important features in saturated regions.\n\nUse multiple methods (ISM, DeepLIFT, IG) and check for consistency.\n\nShortcut features\n\nModels may rely on dataset-specific artifacts (e.g., barcode k-mers, GC content) that produce clean motifs but are not mechanistically meaningful.\n\n\nRecommended practices:\n\nSanity checks\n\nRandomize model weights: attributions should degrade to noise.\n\nRandomize labels: derived motifs should disappear or lose predictive power.\n\nCounterfactual tests\n\nDelete or scramble high-attribution regions and confirm that predictions drop accordingly.\n\nInsert discovered motifs into neutral backgrounds to test gain-of-function effects.\n\nBenchmarking interpretability methods\n\nUse synthetic datasets with known ground-truth grammar.\n\nCompare methods on their ability to recover planted motifs and interactions.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#a-practical-interpretability-toolbox-for-genomic-foundation-models",
    "href": "p5-ch17-interp.html#a-practical-interpretability-toolbox-for-genomic-foundation-models",
    "title": "17  Interpretability & Mechanisms",
    "section": "17.9 A Practical Interpretability Toolbox for Genomic Foundation Models",
    "text": "17.9 A Practical Interpretability Toolbox for Genomic Foundation Models\nFor practitioners working with genomic foundation models (GFMs) and their fine-tuned derivatives, a practical toolbox might include:\n\nLocal effect estimation\n\nFor variant effect prediction: use ref/alt scoring and small-window ISM around variants.\n\nAggregate per-base attributions into per-variant or per-motif scores.\n\nMotif and grammar discovery\n\nCompute base-level attributions for high-confidence predictions.\n\nRun TF-MoDISco or similar algorithms to build a motif vocabulary.\n\nAnalyze motif grammars across tasks (e.g., multiple cell types or assays).\n\nGlobal context visualization\n\nFor transformer-based GFMs: inspect attention patterns to identify heads that track operons, gene neighborhoods, or enhancer–promoter loops.\n\nFor models like Enformer: combine long-range attributions with contact maps to hypothesize regulatory architectures.\n\nRegulatory vocabularies and embeddings\n\nUse frameworks like Sei to project sequences into a low-dimensional regulatory activity space.\n\nCluster variants, enhancers, or genomic regions by their sequence-class profiles to reveal shared regulatory programs.\n\nModel and dataset auditing\n\nUse interpretability tools to identify reliance on confounded or undesirable features.\n\nCross-reference with Chapter 16’s confounder taxonomy (ancestry stratification, batch effects) to design deconfounded training and evaluation.\n\nHuman-in-the-loop analysis\n\nIntegrate motif and sequence-class outputs into visualization tools (e.g., genome browsers with attribution tracks, motif tracks, and class scores).\n\nEnable domain experts to iteratively refine hypotheses.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p5-ch17-interp.html#outlook-from-explanations-to-mechanistic-models",
    "href": "p5-ch17-interp.html#outlook-from-explanations-to-mechanistic-models",
    "title": "17  Interpretability & Mechanisms",
    "section": "17.10 Outlook: From Explanations to Mechanistic Models",
    "text": "17.10 Outlook: From Explanations to Mechanistic Models\nInterpretability in genomic deep learning is evolving from post hoc explanation toward model-assisted mechanistic discovery:\n\nFoundation models provide rich latent spaces and long-range context.\n\nAttribution and motif discovery tools translate those representations into candidate regulatory grammars.\n\nGlobal vocabularies like Sei’s sequence classes offer interpretable axes spanning thousands of assays.\n\nAttention analysis in genomic language models reveals emergent gene-level organization, hinting at scalable ways to capture systems-level biology.\n\nThe next frontier is to close the loop:\n\nUse insights from interpretability (motifs, grammars, sequence classes) to design better architectures and training objectives.\n\nFeed experimentally validated grammars back into models as inductive biases.\n\nDevelop evaluation frameworks where success is measured not only by predictive accuracy but also by mechanistic fidelity—how well model-derived hypotheses align with the causal structure of regulatory biology.\n\nIn this sense, interpretability is not just a diagnostic for black-box models. It is a central tool for turning genomic foundation models into engines of biological discovery, capable of bridging the gap between sequence-level predictions and the mechanistic understanding that underpins robust clinical translation.",
    "crumbs": [
      "Part V: Reliability & Interpretation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretability & Mechanisms</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html",
    "href": "p6-ch18-clinical.html",
    "title": "18  Clinical Risk Prediction",
    "section": "",
    "text": "18.1 Problem Framing: What Is Clinical Risk Prediction?\nModern genomic foundation models (GFMs) give us increasingly rich representations of DNA, RNA, proteins, and multi-omic context (Parts II–IV). The natural next question is: how do we turn these representations into actionable predictions for individual patients?\nThis chapter focuses on clinical risk prediction and decision support—models that estimate the probability, timing, or trajectory of outcomes such as incident disease, progression, recurrence, or adverse drug reactions. We emphasize how GFMs and related deep models:\nWe end with case studies in cardiometabolic risk, oncology risk and recurrence, and pharmacogenomics / adverse drug reactions, illustrating how GFMs move from bench to bedside.\nClinical risk prediction is the task of mapping patient data—including genotypes, family history, clinical measurements, imaging, and environmental factors—to probabilistic statements about future outcomes. Concretely, a model might answer questions like:\nIn practice, these tasks fall into a few archetypes:\nGFMs enter these problems as feature generators: they transform raw genomic and multi-omic data into structured embeddings, variant effect scores, or region-level functional annotations. These representations then feed classic supervised learning tasks—classification, regression, and survival modeling—alongside clinical covariates.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html#problem-framing-what-is-clinical-risk-prediction",
    "href": "p6-ch18-clinical.html#problem-framing-what-is-clinical-risk-prediction",
    "title": "18  Clinical Risk Prediction",
    "section": "",
    "text": "What is this patient’s 10-year risk of coronary artery disease if treated with standard of care?\n\nGiven current tumor characteristics and therapy, what is the hazard of recurrence within 2 years?\n\nIf we start this medication, what is the probability of a severe adverse drug reaction (ADR) in the next 6 months?\n\n\n\nIndividual-level incident risk\n\nWill a currently disease-free individual develop disease within a specified time window (e.g., 10-year type 2 diabetes risk)?\n\n\nProgression and complication risk\n\nAmong individuals with an existing condition, who will develop complications (e.g., nephropathy in diabetes, heart failure after myocardial infarction)?\n\n\nPrognosis and survival\n\nTime-from-baseline to events such as death, recurrence, or transplant, often with censoring and competing risks.\n\n\nTreatment response and toxicity\n\nWill a patient benefit from therapy A vs B, and what is their risk of severe toxicity or ADR?",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html#task-types-and-loss-functions",
    "href": "p6-ch18-clinical.html#task-types-and-loss-functions",
    "title": "18  Clinical Risk Prediction",
    "section": "18.2 Task Types and Loss Functions",
    "text": "18.2 Task Types and Loss Functions\nAlthough GFMs provide sophisticated inputs, the prediction tasks themselves often re-use well-understood statistical frameworks.\n\n18.2.1 Binary and Multi-label Classification\nFor many screening or triage problems, risk prediction is posed as:\n\nWill outcome Y occur within horizon T?\n\nExamples include incident atrial fibrillation within 5 years, or hospitalization for heart failure in the next 12 months. Models output a risk score $ = P(Y=1 x) $, trained with cross-entropy or focal losses.\nExtensions:\n\nMulti-label classification: Predict multiple outcomes (e.g., myocardial infarction, stroke, heart failure) simultaneously; share a common representation but separate output heads.\n\nOrdinal endpoints: Disease stages or severity scores modeled with ordinal losses instead of strictly binary outcomes.\n\nGFMs contribute by providing richer genetic features than traditional hand-crafted burden scores or PGS (e.g., variant-level embeddings from Nucleotide Transformer, GPN, or regulatory LMs) (Dalla-Torre et al. 2023; Benegas, Batra, and Song 2023).\n\n\n18.2.2 Survival and Time-to-Event Modeling\nRisk is often more naturally expressed as time-to-event:\n\nTime from baseline to myocardial infarction or revascularization.\n\nTime from surgery to cancer recurrence.\n\nTime from first exposure to drug to severe toxicity.\n\nThese require models that handle censoring (patients lost to follow-up or event-free at study end). Approaches include:\n\nCox proportional hazards models with genomic and GFM-derived features as covariates.\n\nDeep survival models that use neural networks to parameterize hazard functions, survival curves, or discrete-time hazards.\n\nCompeting risks models for mutually exclusive outcomes (e.g., cancer-specific vs non-cancer mortality).\n\nGFMs naturally provide high-dimensional, possibly non-linear features; deep survival architectures can exploit these features while learning flexible hazard structures.\n\n\n18.2.3 Multi-task Risk and Shared Representations\nLarge health systems increasingly estimate risk for dozens of outcomes simultaneously (e.g., hospital readmission, multiple cardiovascular endpoints, medication-specific ADRs). This motivates multi-task frameworks:\n\nA shared encoder (combining EHR, genomic, and multi-omic encoders) produces a patient-level embedding.\n\nMultiple output heads estimate risks for different endpoints or time horizons.\n\nSuch models can exploit cross-task correlations and share statistical strength (e.g., overlapping genetic architectures between lipids, CAD, and stroke). Deep polygenic architectures like Delphi and G2PT already adopt multi-trait ideas for genomic risk representations (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025).",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html#from-pgs-to-gfm-enabled-risk-scores",
    "href": "p6-ch18-clinical.html#from-pgs-to-gfm-enabled-risk-scores",
    "title": "18  Clinical Risk Prediction",
    "section": "18.3 From PGS to GFM-Enabled Risk Scores",
    "text": "18.3 From PGS to GFM-Enabled Risk Scores\nPolygenic scores (PGS) are a natural starting point for genomically informed clinical prediction.\n\n18.3.1 Classical PGS: Strengths and Limitations\nTraditional PGS typically:\n\nUse GWAS summary statistics to estimate per-variant weights.\n\nConstruct a score $ S = _j w_j g_j $, where $ g_j $ is genotype at variant $ j $ and $ w_j $ is its estimated effect size.\n\nPlug PGS into regression or survival models alongside clinical covariates (age, sex, BMI, labs).\n\nDespite many successes, classical PGS have well-known limitations:\n\nLimited modeling of epistasis and non-linearities: Additive models struggle with higher-order interactions and context-dependent effects.\n\nChallenge in integrating functional priors: Annotation-aware methods exist, but rarely leverage full GFMs.\n\nPortability gaps: Performance often drops in under-represented ancestries due to LD structure and GWAS ascertainment.\n\nThese limitations motivate deep learning-based PGS that better exploit structure in both genotype and functional annotation space.\n\n\n18.3.2 Deep Polygenic Risk: Delphi and G2PT\nRecent methods push beyond additive scores by using deep sequence and genotype models:\n\nDelphi: A deep-learning method for polygenic risk prediction that jointly models variant-level features and higher-order patterns across the genome (Georgantas, Kutalik, and Richiardi 2024).\n\nCan incorporate variant annotations and linkage structure more flexibly than linear PGS.\n\nSupports multi-phenotype prediction, effectively performing task-conditioned PGS.\n\nG2PT (Genotype-to-Phenotype Transformer): A transformer-based architecture that treats an individual’s genotype as a sequence of variant “tokens” and learns polygenic risk representations with attention-based context (Lee et al. 2025).\n\nNaturally captures epistatic interactions via attention, not just additive effects.\n\nEmphasizes interpretability by tying attention patterns back to loci and pathways.\n\n\nBoth systems can optionally use GFM-derived variant features (e.g., scores from sequence-level LMs such as Nucleotide Transformer, HyenaDNA, or GPN) (Dalla-Torre et al. 2023; Nguyen et al. 2023; Benegas, Batra, and Song 2023). In this view:\n\nA GFM maps variant and local sequence context to variant effect features (e.g., predicted impact on chromatin, expression, motifs).\n\nA polygenic risk model (Delphi, G2PT, or related) aggregates these features across the genome to produce a patient-level risk embedding.\n\nA clinical head uses this embedding plus EHR covariates to output risk for specific outcomes or time horizons.\n\n\n\n18.3.3 Fine-mapping and Causal Variants: MIFM\nPolygenic risk ultimately hinges on causal variants, not just associated markers. MIFM (Multiple Instance Fine-Mapping) exemplifies how deep sequence models can refine the link between variant effects and risk:\n\nMIFM uses a deep sequence model in a multiple-instance learning framework to identify causal regulatory variants within associated loci (Rakowski and Lippert 2025).\n\nBy modeling sets (bags) of variants per locus, it distinguishes causal variants from passengers in tight LD blocks.\n\nThe outputs—posterior probabilities or importance scores for candidate variants—can inform both mechanistic studies and more parsimonious, interpretable PGS.\n\nTogether, Delphi, G2PT, and MIFM illustrate a pattern that recurs throughout this chapter:\n\nUse GFMs and deep sequence models to transform raw genotype into rich, structured features, then plug those features into prediction and decision-support architectures that live closer to the clinic.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html#beyond-genotype-fusing-gfms-with-ehr-and-multi-omics",
    "href": "p6-ch18-clinical.html#beyond-genotype-fusing-gfms-with-ehr-and-multi-omics",
    "title": "18  Clinical Risk Prediction",
    "section": "18.4 Beyond Genotype: Fusing GFMs with EHR and Multi-omics",
    "text": "18.4 Beyond Genotype: Fusing GFMs with EHR and Multi-omics\nClinical risk prediction rarely depends on genetics alone. Real-world deployment typically requires fusing genomic features with EHR, imaging, and other omics, mirroring the multi-omics integration strategies from Chapter 14.\n\n18.4.1 Feature Sources\n\nGenomics and regulatory features\n\nZero-shot variant scores from DNA GFMs (e.g., Nucleotide Transformer, HyenaDNA, GPN, Grover) (Dalla-Torre et al. 2023; Nguyen et al. 2023; Benegas, Batra, and Song 2023).\n\nCoding variant scores from protein LMs (e.g., AlphaMissense-like systems; see earlier chapters).\n\nFine-mapped causal variant probabilities from MIFM or similar (Rakowski and Lippert 2025).\n\nMulti-omics and systems context\n\nCell-type–resolved epigenomic and transcriptomic embeddings from GLUE/SCGLUE and CpGPT (Cao and Gao 2022; Camillo et al. 2024).\n\nRare-variant burden and pathway-level representations from DeepRVAT and related models (Clarke et al. 2024).\n\nTumor-level representations from models such as SetQuence/SetOmic or GNN-based cancer subtypers (Jurenaite et al. 2024; X. Li et al. 2022; H. Li et al. 2024).\n\nClinical covariates and EHR\n\nDemographics, vitals, lab results, medication history.\n\nProblem lists, procedures, imaging-derived features.\n\nTime-varying trajectories of biomarkers (e.g., eGFR, HbA1c, tumor markers).\n\n\n\n\n18.4.2 Fusion Patterns\nArchitecturally, risk models usually adopt one of the fusion strategies echoed from Chapter 14:\n\nEarly fusion\n\nConcatenate GFM-derived genomic embeddings with static clinical covariates and feed into a single MLP or survival model.\n\nSimple but sensitive to scaling, missingness, and modality imbalance.\n\nIntermediate fusion\n\nSeparate encoders for genomics, EHR, and multi-omics produce modality-specific embeddings.\n\nA fusion layer (attention, cross-modal transformer, or graph-based integration) combines them into a patient embedding, which downstream heads use for risk prediction.\n\nLate fusion / ensembling\n\nIndependent models per modality (e.g., a PGS-only model, an EHR-only model).\n\nMeta-model or decision rule combines predictions (e.g., “treat if either PGS or EHR risk is high”).\n\n\nFrom a practical standpoint, intermediate fusion is often most attractive: it allows modularity (swap encoders as GFMs improve) while enabling cross-modal interactions.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html#evaluation-discrimination-calibration-and-clinical-utility",
    "href": "p6-ch18-clinical.html#evaluation-discrimination-calibration-and-clinical-utility",
    "title": "18  Clinical Risk Prediction",
    "section": "18.5 Evaluation: Discrimination, Calibration, and Clinical Utility",
    "text": "18.5 Evaluation: Discrimination, Calibration, and Clinical Utility\nHigh performance on test sets is not enough for clinical deployment. Risk models must be discriminative, well-calibrated, robust, and clinically useful.\n\n18.5.1 Discrimination\nDiscrimination measures how well the model ranks individuals by risk:\n\nAUROC (AUC) for binary endpoints.\n\nAUPRC when outcomes are rare (e.g., severe ADRs).\n\nC-index and time-dependent AUC for survival tasks.\n\nStrong discrimination is necessary but not sufficient; poorly calibrated models can still achieve high AUROC.\nFor a cross-cutting discussion of how these metrics are used across molecular, variant, and trait-level tasks, see Chapter 15.\n\n\n18.5.2 Calibration and Risk Stratification\nCalibration asks whether predicted probabilities match observed frequencies:\n\nIf a group of patients is assigned ~20% risk of an event, do ~20% actually experience it?\n\nCalibration is assessed with calibration plots, Hosmer–Lemeshow tests, and Brier scores, often stratified by subgroups (e.g., ancestry, sex, age).\n\nFor PGS-informed models, calibration is especially important because:\n\nRaw PGS are often centered and scaled rather than calibrated; mapping PGS to absolute risk usually requires post-hoc models that incorporate baseline incidence and covariates.\n\nGFMs can shift score distributions as architectures evolve; recalibration may be required when swapping or updating encoders.\n\n\n\n18.5.3 Uncertainty Estimation and “When Not to Predict”\nIn high-stakes settings, models should know when they do not know. Common strategies include:\n\nEnsemble variance or Monte Carlo dropout as uncertainty proxies.\n\nConformal prediction to output risk intervals or prediction sets with guaranteed coverage.\n\nSelective prediction / abstention: allow models to abstain on cases where uncertainty is high or inputs are out-of-distribution (e.g., rare ancestries missing from training, novel tumor subtypes).\n\nFor GFM-based systems, uncertainty can be decomposed:\n\nGenomic uncertainty: confidence in variant effect predictions or fine-mapping (e.g., MIFM probabilities).\n\nClinical uncertainty: extrapolation to new care settings, practice patterns, or patient populations.\n\nCommunicating uncertainty transparently is a core part of decision support.\n\n\n18.5.4 Fairness, Bias, and Health Equity\nMany genomic and EHR datasets reflect historical and structural inequities. Risk models can amplify these biases if not carefully evaluated.\nKey considerations:\n\nAncestry and PGS portability: Classical PGS underperform in under-represented ancestries due to GWAS design; GFM-based methods such as Delphi and G2PT have the opportunity—but not the guarantee—to improve this by leveraging functional priors and cross-ancestry information (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025).\n\nMeasurement and access bias: EHR-derived features may differ systematically across groups (e.g., who gets genotyped, which labs are ordered).\n\nGroup-wise calibration: Evaluate calibration and discrimination separately by ancestry, sex, socio-economic proxies, and care site.\n\nFairness metrics and constraints: When necessary, enforce group-level constraints (e.g., equalized odds) or design affirmative models targeting historically disadvantaged groups.\n\nEquity is not an afterthought; for GFMs, it should inform what data to pretrain on, which benchmarks to report, and how to deploy models in practice.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html#prospective-validation-trials-and-regulation",
    "href": "p6-ch18-clinical.html#prospective-validation-trials-and-regulation",
    "title": "18  Clinical Risk Prediction",
    "section": "18.6 Prospective Validation, Trials, and Regulation",
    "text": "18.6 Prospective Validation, Trials, and Regulation\nRetrospective AUCs are not enough to justify clinical use. Clinical risk models typically require:\n\nProspective validation: Evaluate model performance in a temporally held-out cohort, ideally in multiple health systems with different population structures and practice patterns.\n\nImpact studies: Measure whether using the model actually changes clinician behavior and improves outcomes (e.g., better statin targeting, fewer ADRs, reduced unnecessary imaging).\n\nRandomized or pragmatic trials when models materially influence treatment decisions, to guard against hidden confounding in observational evaluations.\n\nRegulatory landscapes (e.g., device approvals, software-as-a-medical-device frameworks) increasingly recognize learning systems and continuous updates. GFMs complicate this further:\n\nA “fixed” risk model may rely on a GFM backbone that improves over time; updates may change risk rankings and calibration.\n\nRegulatory strategies include locked models with explicit versions, change control plans, or adaptive approvals for constrained forms of continual learning.\n\nRegardless of the framework, clear documentation of data provenance, GFM versions, training procedures, and validation results is essential.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html#monitoring-drift-and-continual-learning",
    "href": "p6-ch18-clinical.html#monitoring-drift-and-continual-learning",
    "title": "18  Clinical Risk Prediction",
    "section": "18.7 Monitoring, Drift, and Continual Learning",
    "text": "18.7 Monitoring, Drift, and Continual Learning\nOnce deployed, GFMs and downstream risk models operate in non-stationary environments:\n\nClinical practice patterns change (new treatments, guidelines).\n\nPatient populations drift (e.g., new screening programs).\n\nLab assays and sequencing pipelines evolve.\n\nMonitoring should track:\n\nInput distributions (e.g., genotype frequencies, EHR feature patterns).\n\nOutput distributions (risk score histograms, fraction of patients above decision thresholds).\n\nPerformance over time (calibration, discrimination), often via rolling windows or periodic audits.\n\nWhen drift is detected:\n\nRecalibration may suffice (e.g., refitting a calibration layer to current data).\n\nPartial retraining of heads or fusion layers can adapt to new environments while keeping GFM weights fixed.\n\nFull continual learning—including updating GFM backbones—requires careful safeguards to avoid catastrophic forgetting and maintain regulatory compliance.\n\nDesign patterns from Chapter 14’s systems models (e.g., modular encoders, robust interfaces between GFMs and clinical layers) are crucial for maintainable, updatable decision support.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html#case-studies",
    "href": "p6-ch18-clinical.html#case-studies",
    "title": "18  Clinical Risk Prediction",
    "section": "18.8 Case Studies",
    "text": "18.8 Case Studies\nTo make these ideas concrete, we outline three stylized case studies that build on models and concepts from earlier chapters.\n\n18.8.1 Cardiometabolic Risk Stratification\nGoal: Identify individuals at high risk of major adverse cardiovascular events (MACE)—e.g., myocardial infarction, stroke, cardiovascular death—over a 10-year horizon.\nInputs:\n\nGenotype: Biobank-scale genotyping or WGS data.\n\nGFM features: Variant effect scores from DNA GFMs (Nucleotide Transformer, HyenaDNA, GPN) (Dalla-Torre et al. 2023; Nguyen et al. 2023; Benegas, Batra, and Song 2023).\n\nPolygenic model: Delphi or G2PT to produce a polygenic risk embedding for cardiometabolic traits (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025).\n\nClinical data: Age, sex, BMI, blood pressure, lipids, smoking, diabetes status, medications (e.g., statins, antihypertensives).\n\nModel design:\n\nUse a DNA GFM to compute variant-level annotations (e.g., predicted enhancer disruption in cardiomyocyte or hepatocyte contexts).\n\nFeed annotations and genotypes into Delphi or G2PT to obtain a patient-level genomics embedding tuned for cardiometabolic outcomes.\n\nFuse the genomics embedding with EHR covariates via an intermediate fusion network (e.g., MLP or transformer over structured features).\n\nTrain the model to predict 10-year MACE risk using survival or discrete-time hazard losses.\n\nClinical use:\n\nStratify patients into risk categories (e.g., low, intermediate, high) that inform statin initiation, PCSK9 inhibitor consideration, or intensive lifestyle intervention.\n\nProvide individual-level explanations: highlight variants and pathways (via G2PT attention or Delphi variant contributions) that most contributed to risk—bridging Chapters 9 and 15.\n\nEvaluate equity: ensure performance and calibration hold across ancestries and care sites.\n\n\n\n18.8.2 Oncology: Risk and Recurrence Prediction\nGoal: Predict recurrence risk and treatment benefit for patients with solid tumors after surgery or first-line therapy.\nInputs:\n\nSomatic landscapes from whole-exome or whole-genome tumor sequencing.\n\nTumor representations from deep set or transformer architectures such as SetQuence/SetOmic (Jurenaite et al. 2024).\n\nMulti-omics: tumor expression, methylation, and chromatin accessible from integrated frameworks (GLUE, CpGPT) (Cao and Gao 2022; Camillo et al. 2024).\n\nGNN-based subtyping: embeddings or cluster assignments from cancer subtyping models like MoGCN and CGMega (X. Li et al. 2022; H. Li et al. 2024).\n\nClinical features: stage, grade, performance status, treatment regimen.\n\nModel design:\n\nEncode somatic mutation sets with SetQuence/SetOmic to obtain tumor-variant embeddings (Jurenaite et al. 2024).\n\nIntegrate transcriptomic and epigenomic profiles via GLUE-like latent spaces and CpGPT methylation embeddings (Cao and Gao 2022; Camillo et al. 2024).\n\nCombine these with GNN-based subtype embeddings (MoGCN/CGMega) to capture tumor–microenvironment and histopathological context (X. Li et al. 2022; H. Li et al. 2024).\n\nFuse tumor-level representations with clinical features in a time-to-recurrence model (e.g., flexible deep survival network).\n\nClinical use:\n\nProvide risk estimates that guide adjuvant therapy decisions (e.g., intensifying chemotherapy or adding targeted agents for high-risk patients).\n\nSuggest candidate biomarkers or pathways for trial stratification, based on GFM-derived importance scores and attention maps.\n\nMonitor drift as treatment standards evolve; update models to reflect new targeted therapies and immune checkpoint inhibitors.\n\n\n\n18.8.3 Pharmacogenomics and Adverse Drug Reaction Risk\nGoal: Predict which patients are at high risk of severe ADRs (e.g., myopathy on statins, severe cutaneous reactions to certain drugs, cardiotoxicity of oncology agents).\nInputs:\n\nGermline variation in pharmacogenes (e.g., CYP family, HLA alleles) and broader genome.\n\nVariant effect scores from both DNA and protein LMs for coding and regulatory variants in drug metabolism and immune genes (see Chapters 2–3, 9–10).\n\nClinical context: co-medications, comorbidities, organ function (liver, kidney), prior adverse reactions.\n\nModel design:\n\nUse GFMs to derive mechanistically meaningful features for variants in pharmacogenes (e.g., predicted impact on protein stability, binding, or gene regulation).\n\nAggregate these features across loci into a pharmacogenomic risk embedding, possibly using a G2PT-style transformer restricted to relevant genes (Lee et al. 2025).\n\nCombine this with EHR data in a multi-task classification model that predicts ADR risk for multiple drugs or drug classes.\n\nClinical use:\n\nFlag patients at high risk before initiating therapy, prompting genotype-guided drug choice or dose adjustment.\n\nGenerate reports that tie risk back to specific variants and pharmacogenes, aligned with existing clinical pharmacogenomics guidelines.\n\nEvaluate performance across ancestries to avoid exacerbating disparities in access to safe and effective therapy.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch18-clinical.html#practical-design-patterns-and-outlook",
    "href": "p6-ch18-clinical.html#practical-design-patterns-and-outlook",
    "title": "18  Clinical Risk Prediction",
    "section": "18.9 Practical Design Patterns and Outlook",
    "text": "18.9 Practical Design Patterns and Outlook\nAcross these examples, several design patterns for GFM-enabled clinical prediction recur:\n\nTreat GFMs as modular feature extractors:\n\nKeep a clear separation between foundation encoders and clinical prediction heads, easing updates and regulatory management.\n\nEmbrace multi-modal fusion:\n\nCombine genotype, multi-omics, and EHR, taking advantage of architectures discussed in Chapters 10 and 16 (Dalla-Torre et al. 2023; Cao and Gao 2022).\n\nPrioritize calibration, uncertainty, and fairness as first-class citizens, not post-hoc add-ons.\nBridge interpretability and mechanism:\n\nUse tools from Chapter 14 to connect individual risk predictions to variants, regions, and pathways, enabling mechanistic hypotheses and clinician trust.\n\nDesign for continual learning and monitoring:\n\nAssume that clinical practice and data distributions will change; build pipelines that can adapt responsibly.\n\n\nIn the broader story of this book, clinical risk prediction and decision support represent a key translation layer: they connect the representational gains of genomic foundation models to the realities of patient care. The next chapters will extend these ideas to other application domains (e.g., rare disease diagnosis, discovery of pathogenic variants, and drug/biotech innovation), further exploring how GFMs reshape translational genomics.\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “[GPN] DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T. Higgins-Chen, Steve Horvath, and Bo Wang. 2024. “CpGPT: A Foundation Model for DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024. “[DeepRVAT] Integration of Variant Annotations Using Deep Set Networks Boosts Rare Variant Association Testing.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. “Delphi: A Deep-Learning Method for Polygenic Risk Prediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nJurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge, and René Jäkel. 2024. “SetQuence & SetOmic: Deep Set Transformers for Whole Genome and Exome Tumour Analysis.” BioSystems 235 (January): 105095. https://doi.org/10.1016/j.biosystems.2023.105095.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam, Amit R. Majithia, and Trey Ideker. 2025. “[G2PT] A Genotype-Phenotype Transformer to Assess and Explain Polygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Hao, Zebei Han, Yu Sun, Fu Wang, Pengzhen Hu, Yuang Gao, Xuemei Bai, et al. 2024. “CGMega: Explainable Graph Neural Network Framework with Attention Mechanisms for Cancer Gene Module Dissection.” Nature Communications 15 (1): 5997. https://doi.org/10.1038/s41467-024-50426-6.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and Yunping Zhu. 2022. “MoGCN: A Multi-Omics Integration Method Based on Graph Convolutional Network for Cancer Subtype Analysis.” Frontiers in Genetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch19-variants.html",
    "href": "p6-ch19-variants.html",
    "title": "19  Pathogenic Variant Discovery",
    "section": "",
    "text": "19.1 From Variant Effect Prediction to Prioritization\nClinical genetics ultimately cares about specific variants and genes: which changes in a patient’s genome plausibly explain their phenotype, and which loci are compelling targets for follow-up in the lab. The previous chapters focused on foundation models for variant effect prediction (Chapter 13), multi-omics integration (Chapter 14), and clinical risk prediction (Chapter 18). This chapter shifts the emphasis from prediction to discovery workflows.\nThe central question is:\nWe will treat “pathogenic” broadly—covering both Mendelian variants with large effects and complex trait variants that modulate risk more subtly. GFMs appear at multiple stages of these pipelines:\nWe will walk through these roles from locus-level variant ranking, to Mendelian disease diagnostics, to graph-based gene prioritization, and finally to closed-loop “hypothesis factory” workflows that blend GFMs with systematic perturbation experiments.\nChapter 13 surveyed state-of-the-art variant effect prediction (VEP) systems. Models such as AlphaMissense, GPN-MSA, Evo 2, and AlphaGenome assign each variant a score reflecting predicted impact on protein function, regulatory activity, or multi-omic phenotypes Z. Avsec, Latysheva, and Cheng (2025). In isolation, these scores are powerful but not yet a full prioritization pipeline.\nIn practice, discovery workflows require several additional steps:\nIn other words, GFMs provide high-resolution local perturbation scores, but the art of discovery is in wiring those scores into larger decision frameworks.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Pathogenic Variant Discovery</span>"
    ]
  },
  {
    "objectID": "p6-ch19-variants.html#from-variant-effect-prediction-to-prioritization",
    "href": "p6-ch19-variants.html#from-variant-effect-prediction-to-prioritization",
    "title": "19  Pathogenic Variant Discovery",
    "section": "",
    "text": "Contextualizing the score\nA raw VEP score has different implications depending on:\n\nVariant class (missense, splice, promoter, enhancer, UTR, intronic).\n\nGene context (constraint, tissue-specific expression, pathway membership).\n\nClinical or experimental question (dominant Mendelian disease, recessive disease, modifier of complex trait).\n\nFor example, a moderately damaging missense variant in a highly constrained gene expressed in the relevant tissue may be more compelling than a strongly damaging variant in a gene with no supporting biology.\nAggregation from variants to loci and genes\nDiscovery problems often operate at locus or gene level, requiring some aggregation of variant scores. Common strategies include:\n\nMax or top-k pooling – Focus on the worst predicted variant per gene or locus.\n\nBurden-style aggregation – Sum or average the predicted impact of all rare variants in a gene, possibly weighted by allele frequency and predicted effect.\n\nMechanism-aware aggregation – Separate coding vs regulatory, or promoter vs distal enhancer contributions, using tissue-specific scores from models like Enformer or AlphaGenome Z. Avsec, Latysheva, and Cheng (2025).\n\nCombining VEP with orthogonal evidence\nVEP is rarely used alone. Modern pipelines combine:\n\nPopulation data – Allele frequency and constraint (pLI, LOEUF, missense and LoF intolerance).\n\nClinical databases – ClinVar classifications, disease-gene catalogs (OMIM, HGMD).\n\nFunctional annotations – Chromatin state, conservation (PhyloP, PhastCons), known regulatory elements (Siepel et al. 2005).\n\nPathway and network context – Membership in pathways enriched for the trait, or centrality in relevant biological networks.\n\nGFMs enter as feature providers in this stack, often replacing or augmenting hand-crafted features.\nCalibration and interpretability\nFor prioritization, ranking may matter more than perfectly calibrated probabilities, but interpretable risk categories are crucial in clinical and experimental settings. This pushes towards:\n\nScore thresholds with empirical positive predictive value (PPV) estimates.\n\nQualitative explanations (e.g., “strong disruption of a conserved splice donor in a haploinsufficient gene”).\n\nVisualizations of attention maps, saliency, or motif-level contributions (Chapter 17).",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Pathogenic Variant Discovery</span>"
    ]
  },
  {
    "objectID": "p6-ch19-variants.html#integrating-vep-with-gwas-fine-mapping-and-burden-tests",
    "href": "p6-ch19-variants.html#integrating-vep-with-gwas-fine-mapping-and-burden-tests",
    "title": "19  Pathogenic Variant Discovery",
    "section": "19.2 Integrating VEP with GWAS, Fine-Mapping, and Burden Tests",
    "text": "19.2 Integrating VEP with GWAS, Fine-Mapping, and Burden Tests\nGenome-wide association studies (GWAS) identify statistical associations between variants and traits. However, GWAS hits are often:\n\nNoncoding – Located in enhancers or other regulatory elements.\n\nIn linkage disequilibrium (LD) – Dozens of variants in a region share similar association statistics.\n\nMechanistically opaque – Even the top GWAS SNP may not be truly causal.\n\n\n19.2.1 VEP as a prior for fine-mapping\nFine-mapping methods aim to assign each variant in a locus a posterior probability of causality, usually by combining LD patterns, effect-size estimates, and sometimes functional annotations Wu et al. (2024). GFMs naturally provide functional priors:\n\nRegulatory sequence models such as Enformer and AlphaGenome predict how a variant perturbs gene expression or chromatin landscapes Z. Avsec, Latysheva, and Cheng (2025).\n\nGenome-scale LMs like GPN-MSA and Evo 2 estimate the likelihood or impact of nucleotide substitutions in their genomic context Brixi et al. (2025).\n\nSpecialized models like TREDNet and MIFM directly target causal variant prediction at GWAS loci Rakowski and Lippert (2025).\n\nFrom a Bayesian perspective, these models provide a functional prior $ _j $ for each variant $ j $ in the locus. Fine-mapping frameworks can then:\n\nUpweight variants predicted to have large regulatory or coding effects.\n\nDownweight variants with benign or neutral predictions.\n\nSupport multi-variant configurations, where multiple causal variants exist at the same locus.\n\nRecent benchmarks like TraitGym systematically evaluate how well various genomic LMs and VEP models serve as fine-mapping priors across traits and tissues (Benegas, Eraslan, and Song 2025).\n\n\n19.2.2 Rare variant association and DeepRVAT-style models\nFor rare variants, single-variant tests have limited power. Instead, gene- or region-based burden tests aggregate rare variants across individuals to detect association. Here, VEP plays two key roles:\n\nVariant weighting and filtering\nClassical burden tests often restrict to “damaging” variants using simple filters (e.g., predicted LoF, CADD &gt; threshold). GFMs provide richer filters and weights, enabling:\n\nFine-grained distinctions among missense variants (e.g., using AlphaMissense scores (Cheng et al. 2023)).\n\nInclusion of regulatory variants predicted to modulate gene expression.\n\nContinuous weights reflecting predicted effect size, rather than binary include/exclude decisions.\n\nEnd-to-end deep set models\nDeepRVAT exemplifies a newer paradigm: instead of hand-engineered burden summaries, a deep set network ingests per-variant features (including GFM-derived VEP scores) and learns to aggregate them into a gene-level risk signal (Clarke et al. 2024). This approach:\n\nSupports heterogeneous variant classes within a gene.\n\nLearns flexible aggregation functions (e.g., non-additive interactions) while preserving permutation invariance.\n\nAccommodates multiple phenotypes and covariates within a single model.\n\n\nAs more cohorts with whole-exome or whole-genome sequencing become available, these GFM-enhanced burden frameworks blur the line between GWAS and rare variant analysis, providing a continuum of variant discovery tools.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Pathogenic Variant Discovery</span>"
    ]
  },
  {
    "objectID": "p6-ch19-variants.html#mendelian-disease-gene-and-variant-discovery",
    "href": "p6-ch19-variants.html#mendelian-disease-gene-and-variant-discovery",
    "title": "19  Pathogenic Variant Discovery",
    "section": "19.3 Mendelian Disease Gene and Variant Discovery",
    "text": "19.3 Mendelian Disease Gene and Variant Discovery\nIn Mendelian disease genetics, the questions tend to be more concrete: Which variant explains this patient’s phenotype? Which gene is implicated? WES/WGS of trios and families produces thousands of variants per individual. The standard pipeline includes:\n\nQuality control and filtering\n\nRemove low-quality calls and technical artifacts.\n\nFilter by allele frequency (e.g., &lt;0.1% in population databases), inheritance mode (de novo, recessive, X-linked), and variant type (LoF, missense, splice, structural).\n\nGene-centric ranking\n\nAggregate candidate variants per gene, using constraint metrics and known disease-gene catalogs.\n\nIntegrate phenotype similarity (e.g., HPO-based matching between patient and known gene syndromes).\n\nManual curation\n\nExpert review of gene function, expression patterns, animal models, and literature.\n\nAssessment of segregation in the family, de novo status, and evidence of pathogenic mechanism.\n\n\n\n19.3.1 GFMs in Mendelian variant prioritization\nGFMs reshape several stages of this process:\n\nRicher coding impact scores\nAlphaMissense provides proteome-wide missense pathogenicity estimates with continuous scores that often outperform traditional tools (Cheng et al. 2023). Coding-aware foundation models (cdsFM and related systems) further capture codon-level context and co-evolutionary patterns (Naghipourfar et al. 2024).\nRegulatory and splice prediction\nGenome-wide models like GPN-MSA, Evo 2, and AlphaGenome estimate the effect of noncoding and splice-proximal variants, filling a gap for Mendelian variants outside exons Z. Avsec, Latysheva, and Cheng (2025).\nCombined variant–gene scoring\nFor each gene, we can aggregate:\n\nMax or weighted VEP score across all candidate variants.\n\nSeparate tallies for LoF, missense, regulatory, and splice variants.\n\nGene-level features (constraint, expression, pathways) and phenotype similarity.\n\nA simple model might compute a composite gene score as a learned function of these features, trained on cohorts with labeled diagnoses.\n\n\n\n19.3.2 Rare disease association at scale\nBeyond single-family diagnostics, large consortia collect rare disease cohorts where the goal is to discover new gene–disease associations. DeepRVAT-style models provide one blueprint:\n\nRepresent each individual as a set of rare variants with multi-dimensional VEP features (from GFMs and traditional tools).\n\nUse deep set networks to map from per-variant features to individual-level phenotype predictions or gene-level association signals (Clarke et al. 2024).\n\nIncorporate multi-omics context (e.g., tissue-specific expression, chromatin accessibility from GLUE-like models) as additional features (Cao and Gao 2022).\n\nThis pushes Mendelian discovery closer to the foundation model paradigm: instead of hand-designed burden statistics, we train flexible architectures that learn how to combine variant-level representations into gene- and phenotype-level insights.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Pathogenic Variant Discovery</span>"
    ]
  },
  {
    "objectID": "p6-ch19-variants.html#graph-based-prioritization-of-disease-genes",
    "href": "p6-ch19-variants.html#graph-based-prioritization-of-disease-genes",
    "title": "19  Pathogenic Variant Discovery",
    "section": "19.4 Graph-Based Prioritization of Disease Genes",
    "text": "19.4 Graph-Based Prioritization of Disease Genes\nMany discovery problems are inherently network-structured. Genes interact through pathways, protein–protein interaction (PPI) networks, co-expression modules, regulatory networks, and knowledge graphs. GNNs offer a natural way to fuse:\n\nNode features from GFMs (e.g., aggregated VEP scores, expression profiles).\n\nGraph structure capturing biological relationships.\n\nLabels such as disease associations, essentiality, or cancer driver status.\n\n\n19.4.1 Multi-omics and cancer gene modules\nGLUE (and SCGLUE) frame multi-omics integration as a graph-linked embedding problem, connecting cells and features across modalities (Cao and Gao 2022). Inspired by this, GNN frameworks like MoGCN and CGMega build:\n\nGene-level graphs combining expression, methylation, copy number, and other omics layers H. Li et al. (2024).\n\nAttention mechanisms to highlight important neighbors and pathways in cancer gene modules.\n\nPredictive models for cancer subtypes, driver genes, and prognostic signatures.\n\nGFMs can enhance these systems by supplying:\n\nVariant-aware gene features (e.g., aggregated predicted impact of observed somatic mutations).\n\nRegulatory context via sequence-based predictions of expression and chromatin (Enformer, Borzoi, AlphaGenome) Z. Avsec, Latysheva, and Cheng (2025).\n\n\n\n19.4.2 Knowledge graphs and essential gene prediction\nKnowledge graphs like PrimeKG aggregate heterogeneous biomedical entities—genes, diseases, drugs, pathways, and phenotypes—into a unified relational structure (Chandak, Huang, and Zitnik 2023). GNNs on such graphs can be trained to:\n\nPrioritize disease genes based on graph proximity to known genes.\n\nSuggest drug repurposing candidates by connecting genetic evidence to drug targets.\n\nDiscover modules linked to therapeutic response or adverse effects.\n\nBingo provides a related example, combining a large language model (LLM) with GNNs to predict essential genes from protein-level data (Ma et al. 2023). In principle, the node features in such systems could incorporate:\n\nGene-level embeddings derived from protein LMs (Chapter 9).\n\nAggregated variant effect embeddings from genomic LMs (Chapter 10 and Chapter 13).\n\nMulti-omic signatures from GLUE-like integrative models (Cao and Gao 2022).\n\nTogether, these approaches illustrate a broader trend: GFMs rarely act alone. Instead, they supply dense, information-rich features to graph-based models that reason over the network context where disease mechanisms actually play out.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Pathogenic Variant Discovery</span>"
    ]
  },
  {
    "objectID": "p6-ch19-variants.html#experimental-follow-up-and-closed-loop-refinement",
    "href": "p6-ch19-variants.html#experimental-follow-up-and-closed-loop-refinement",
    "title": "19  Pathogenic Variant Discovery",
    "section": "19.5 Experimental Follow-Up and Closed-Loop Refinement",
    "text": "19.5 Experimental Follow-Up and Closed-Loop Refinement\nComputational prioritization is only half of discovery. Ultimately, we need experimental validation: does perturbing a candidate variant or gene alter the relevant molecular or cellular phenotype?\n\n19.5.1 Designing CRISPR and MPRA experiments with GFMs\nHigh-throughput perturbation assays such as:\n\nMassively parallel reporter assays (MPRAs) targeting many regulatory variants.\n\nCRISPR tiling and base editing screens across enhancers, promoters, and coding regions.\n\nPerturb-seq linking genetic perturbations to single-cell transcriptomes.\n\nare expensive and capacity-limited. GFMs help prioritize and design these experiments:\n\nSequence-to-expression models like Enformer and Borzoi can identify regions and variants with large predicted regulatory effects, guiding where to tile and which alleles to test Linder et al. (2025).\n\nGenome-scale generative models like Evo 2 can propose counterfactual edits that maximize predicted effect, enabling focused exploration of regulatory landscapes (Brixi et al. 2025).\n\nVariant effect models can suggest multiplexed libraries that systematically probe key motifs, splice sites, or codon usage patterns.\n\nInstead of brute-force tiling every base pair, we can use GFMs to bias the library toward informative perturbations, effectively turning them into experiment design engines.\n\n\n19.5.2 Using functional data to retrain and recalibrate models\nThe feedback loop goes in the other direction as well. Functional genomics screens produce rich labeled datasets:\n\nMPRA readouts of allele-specific regulatory activity.\n\nCRISPR screen scores for gene essentiality or drug sensitivity.\n\nSingle-cell perturbation responses across cell states.\n\nThese can be used to:\n\nRefine model heads for specific tasks (e.g., fine-tune a GFM to predict MPRA outcomes in a particular cell type).\n\nCalibrate scores so that predicted effect magnitudes align with measured changes.\n\nDiscover failure modes, such as motifs or chromatin contexts where current models systematically mispredict.\n\nSome recent systems explicitly design closed-loop pipelines, where model predictions drive experiments, which then feed back to improve the model and inform the next round of design Rakowski and Lippert (2025). In the limit, we approach a semi-automated “hypothesis factory”:\n\nStart from GWAS, rare variant, or tumor sequencing data.\n\nUse GFMs plus graphs to prioritize candidate variants and genes.\n\nDesign perturbation experiments guided by model predictions.\n\nUpdate the models with new functional data.\n\nIterate, progressively sharpening our understanding of the underlying mechanisms.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Pathogenic Variant Discovery</span>"
    ]
  },
  {
    "objectID": "p6-ch19-variants.html#case-studies-and-practical-considerations",
    "href": "p6-ch19-variants.html#case-studies-and-practical-considerations",
    "title": "19  Pathogenic Variant Discovery",
    "section": "19.6 Case Studies and Practical Considerations",
    "text": "19.6 Case Studies and Practical Considerations\nTo ground these ideas, consider two representative application areas.\n\n19.6.1 Rare disease diagnosis pipelines leveraging VEP scores\nModern rare disease centers increasingly adopt GFM-enhanced diagnostic workflows:\n\nVariant filtering and annotation\n\nStandard QC and frequency filters.\n\nAnnotation with GFM-based VEP scores (coding, regulatory, splice), constraint, and ClinVar evidence.\n\nGene-ranking model\n\nPer-gene aggregation of variant scores and features.\n\nA trained model that predicts the likelihood of each gene being causal, based on retrospective cohorts with known diagnoses.\n\nPhenotype integration\n\nHPO-based similarity to known gene syndromes.\n\nNetwork-based propagation of phenotype associations using knowledge graphs like PrimeKG (Chandak, Huang, and Zitnik 2023).\n\nExpert review\n\nGeneticists and clinicians inspect the top-ranked genes and variants, cross-checking against patient phenotypes, family segregation, and literature.\n\n\nCompared to traditional pipelines, the GFM-enhanced version tends to:\n\nSurface non-obvious candidates, such as noncoding or splice variants with strong predicted functional effects.\n\nProvide more nuanced prioritization among multiple missense variants in the same gene.\n\nOffer richer mechanistic hypotheses to guide follow-up experiments.\n\n\n\n19.6.2 Cancer driver mutation discovery (coding and noncoding)\nIn cancer genomics, the goal is to distinguish driver mutations from a large background of passenger mutations. GFMs and graph-based models contribute at multiple levels:\n\nVariant-level scoring\n\nUse coding VEP (e.g., AlphaMissense, cdsFM-like models) for missense drivers Naghipourfar et al. (2024).\n\nUse regulatory sequence models (Enformer, AlphaGenome, TREDNet) to evaluate noncoding mutations in promoters and enhancers Hudaiberdiev et al. (2023).\n\nGene- and module-level aggregation\n\nAggregate somatic variants per gene, weighted by predicted functional impact.\n\nApply GNNs such as MoGCN and CGMega to identify driver gene modules that are recurrently perturbed across patients H. Li et al. (2024).\n\nUse set-based models (akin to DeepRVAT) to relate patient-specific variant sets to tumor subtypes or outcomes (Clarke et al. 2024).\n\nFunctional follow-up\n\nDesign focused CRISPR tiling screens around candidate regulatory elements, prioritized by GFMs.\n\nValidate predicted driver genes in cell line or organoid models, integrating transcriptional responses with multi-omic readouts (Chapter 14).\n\n\nThese pipelines exemplify multi-scale integration: GFMs for variant-level effects, GNNs for network-level reasoning, and high-throughput perturbations for experimental validation.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Pathogenic Variant Discovery</span>"
    ]
  },
  {
    "objectID": "p6-ch19-variants.html#outlook-towards-end-to-end-discovery-systems",
    "href": "p6-ch19-variants.html#outlook-towards-end-to-end-discovery-systems",
    "title": "19  Pathogenic Variant Discovery",
    "section": "19.7 Outlook: Towards End-to-End Discovery Systems",
    "text": "19.7 Outlook: Towards End-to-End Discovery Systems\nBiomedical discovery of pathogenic variants is moving from manual, hypothesis-driven workflows toward data- and model-driven pipelines where GFMs act as a central substrate:\n\nThey turn raw sequence variation into rich, context-aware variant embeddings.\n\nThey provide priors and features for fine-mapping, rare variant association, and gene prioritization.\n\nThey guide the design of targeted perturbation experiments, which in turn provide new data to refine the models.\n\nAt the same time, several challenges remain:\n\nRobustness and generalization across ancestries, tissues, and disease cohorts.\n\nCalibration and interpretability suitable for clinical and experimental decision-making.\n\nEvaluation frameworks (like TraitGym) that fairly compare models and reveal domain gaps (Benegas, Eraslan, and Song 2025).\n\nEthical and regulatory considerations around automated variant classification and gene discovery in sensitive contexts.\n\nIn the next chapter, we zoom out to the broader drug discovery and biotech landscape (Chapter 20), where many of these discovery building blocks are embedded in industrial-scale pipelines that span from genetic association to target validation, biomarker discovery, and eventually clinical translation.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025. “[TraitGym] Benchmarking DNA Sequence Models for Causal Regulatory Variant Prediction in Human Genetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nChandak, Payal, Kexin Huang, and Marinka Zitnik. 2023. “[PrimeKG] Building a Knowledge Graph to Enable Precision Medicine.” Scientific Data 10 (1): 67. https://doi.org/10.1038/s41597-023-01960-3.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024. “[DeepRVAT] Integration of Variant Annotations Using Deep Set Networks Boosts Rare Variant Association Testing.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nHudaiberdiev, Sanjarbek, D. Leland Taylor, Wei Song, Narisu Narisu, Redwan M. Bhuiyan, Henry J. Taylor, Xuming Tang, et al. 2023. “[TREDNet] Modeling Islet Enhancers Using Deep Learning Identifies Candidate Causal Variants at Loci Associated with T2D and Glycemic Traits.” Proceedings of the National Academy of Sciences 120 (35): e2206612120. https://doi.org/10.1073/pnas.2206612120.\n\n\nLi, Hao, Zebei Han, Yu Sun, Fu Wang, Pengzhen Hu, Yuang Gao, Xuemei Bai, et al. 2024. “CGMega: Explainable Graph Neural Network Framework with Attention Mechanisms for Cancer Gene Module Dissection.” Nature Communications 15 (1): 5997. https://doi.org/10.1038/s41467-024-50426-6.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and Yunping Zhu. 2022. “MoGCN: A Multi-Omics Integration Method Based on Graph Convolutional Network for Cancer Subtype Analysis.” Frontiers in Genetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nMa, Jiani, Jiangning Song, Neil D. Young, Bill C. H. Chang, Pasi K. Korhonen, Tulio L. Campos, Hui Liu, and Robin B. Gasser. 2023. “’Bingo’-a Large Language Model- and Graph Neural Network-Based Workflow for the Prediction of Essential Genes from Protein Data.” Briefings in Bioinformatics 25 (1): bbad472. https://doi.org/10.1093/bib/bbad472.\n\n\nNaghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B. Macdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow Coyote-Maestas, and Hani Goodarzi. 2024. “[cdsFM - EnCodon/DeCodon] A Suite of Foundation Models Captures the Contextual Interplay Between Codons.” bioRxiv. https://doi.org/10.1101/2024.10.10.617568.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs, Minmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005. “[PhastCons] Evolutionarily Conserved Elements in Vertebrate, Insect, Worm, and Yeast Genomes.” Genome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray, Peter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping Improves Identification of Causal Variants.” Research Square, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Pathogenic Variant Discovery</span>"
    ]
  },
  {
    "objectID": "p6-ch20-drugs.html",
    "href": "p6-ch20-drugs.html",
    "title": "20  Drug Discovery & Biotech",
    "section": "",
    "text": "20.1 Where Genomics Touches the Drug Discovery Pipeline\nGenomic foundation models (GFMs) are built to turn raw sequence and multi-omic data into reusable biological representations and fine-grained predictions (Chapter 12). In previous chapters you saw how these models improve variant effect prediction (Chapters 10, 11, 13), long-range regulatory modeling (Chapters 8, 11, 12), and disease genetics workflows (Chapters 14–16).\nThis chapter zooms out to ask a more translational question:\nRather than walking step-by-step through a single therapeutic program, this chapter offers a compact, high-level map of where GFMs are already useful—or plausibly soon will be. The focus is on three broad roles:\nThroughout, the aim is not to promise “end-to-end AI drug discovery,” but to show pragmatic ways that genomic foundation models can reduce risk, prioritize hypotheses, and make experiments more informative, especially when coupled to high-quality human data.\nThe canonical small-molecule or biologics pipeline is often summarized as:\nGenomics most directly enters at three points:\nOther AI-for-drug-discovery efforts focus on molecular design, docking, or protein structure; those are largely beyond the scope of this book. Here we stay close to the DNA- and RNA-centric capabilities you’ve seen earlier: variant effect prediction, regulatory modeling, and multi-omics integration.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Drug Discovery & Biotech</span>"
    ]
  },
  {
    "objectID": "p6-ch20-drugs.html#where-genomics-touches-the-drug-discovery-pipeline",
    "href": "p6-ch20-drugs.html#where-genomics-touches-the-drug-discovery-pipeline",
    "title": "20  Drug Discovery & Biotech",
    "section": "",
    "text": "Target identification and validation\n\nHit finding and lead optimization\n\nPreclinical characterization (safety, PK/PD, tox)\n\nClinical trials (Phase I–III) and post-marketing\n\n\n\nEarly-stage target discovery and validation\n\nHuman genetic associations (GWAS, rare-variant burden, somatic mutation landscapes) point to potential targets.\n\nVariant-level effect predictions and gene-level constraint metrics help de-prioritize potentially unsafe or non-causal signals.\n\nBiomarker discovery and patient stratification\n\nGenetic risk scores, regulatory embeddings, and multi-omic signatures define patient subgroups and endpoints for trials.\n\nEmbeddings from GFMs make it easier to find molecularly coherent patient strata beyond traditional clinical labels.\n\nMechanism-of-action (MoA) and resistance\n\nFunctional genomics screens and perturbation assays help dissect how a compound perturbs cellular networks.\n\nGFMs can predict which perturbations matter and suggest follow-up experiments.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Drug Discovery & Biotech</span>"
    ]
  },
  {
    "objectID": "p6-ch20-drugs.html#target-discovery-and-genetic-validation",
    "href": "p6-ch20-drugs.html#target-discovery-and-genetic-validation",
    "title": "20  Drug Discovery & Biotech",
    "section": "20.2 Target Discovery and Genetic Validation",
    "text": "20.2 Target Discovery and Genetic Validation\nHuman genetics provides some of the strongest evidence that modulating a particular target can safely change disease risk. GFMs don’t replace classical statistical genetics, but they provide richer priors and more mechanistic features for identifying and validating targets.\n\n20.2.1 From variant-level scores to gene-level targets\nVariant effect prediction (VEP) models provide a natural starting point. Earlier chapters introduced:\n\nGenome-wide deleteriousness scores such as CADD, which integrate diverse annotations and—more recently—deep and foundation-model features (Rentzsch et al. 2019; Schubach et al. 2024).\nProtein-centric VEP GFMs, including AlphaMissense, GPN-MSA, and AlphaGenome, which combine protein language models, structure, and long-range context to score coding variants (Cheng et al. 2023; Benegas, Albors, et al. 2024; Z. Avsec, Latysheva, and Cheng 2025; Brandes et al. 2023).\nSequence-to-function models such as Enformer and long-context DNA LMs (e.g., Nucleic Transformer, HyenaDNA), which predict regulatory outputs from large genomic windows (Ž. Avsec et al. 2021; He et al. 2023; Nguyen et al. 2023; Trop et al. 2024).\n\nDrug target teams rarely care about individual variants per se; they care about genes and pathways. The key move is therefore to aggregate variant-level information into gene-level evidence:\n\nCoding variant aggregation\n\nSummarize missense and predicted loss-of-function (pLoF) variants in each gene using VEP scores.\n\nPartition variants by predicted functional category (e.g. likely loss-of-function vs. benign missense) and by allele frequency.\n\nDerive gene-level metrics such as “burden of predicted damaging variants in cases vs controls.”\n\nNoncoding and regulatory evidence\n\nAggregate variant effect predictions on enhancers, promoters, and splice sites that link (via chromatin interaction maps or models like Enformer) to a candidate gene (Ž. Avsec et al. 2021; He et al. 2023).\n\nUse long-range GFMs to connect distal regulatory elements to target loci across 100 kb–1 Mb.\n\nConstraint and intolerance\n\nCombine VEP-informed burden with gene constraint measures (as used implicitly in CADD and downstream tools) to identify genes that are highly intolerant to damaging variation (Rentzsch et al. 2019; Schubach et al. 2024).\n\nExtremely constrained genes may be risky targets (essentiality/toxicity), while “dose-sensitive” but not lethal genes may present more attractive opportunities.\n\n\nFrom a GFM perspective, the core idea is to treat gene-level evidence as an aggregation problem over high-dimensional variant embeddings. Instead of manually defining a handful of summary statistics, teams can feed variant embeddings or predicted functional profiles into downstream models that learn which patterns matter most for disease.\n\n\n20.2.2 Linking genetic evidence to target safety and efficacy\nClassical human genetics has established several now-standard heuristics for target selection:\n\n“Human knockout” individuals (carrying biallelic LoF variants) provide a natural experiment on what happens when a gene is effectively inactivated.\n\nProtective variants that reduce disease risk suggest directionality of effect (e.g. partial inhibition of a protein is beneficial rather than harmful).\n\nPleiotropy—associations with many unrelated traits—may signal safety liabilities.\n\nGFMs reinforce and extend these ideas by:\n\nImproving causal variant identification\n\nFine-mapping methods and multiple-instance models like MIFM can distinguish truly causal regulatory variants from correlated passengers (Wu et al. 2024; Rakowski and Lippert 2025).\n\nCombining these with regulatory GFMs tightens the map from GWAS locus → variant → target gene.\n\nRefining effect direction and magnitude\n\nVEP scores from protein and regulatory GFMs can approximate effect sizes (e.g. how “severe” a missense change is, or how strongly a regulatory variant alters expression) (Cheng et al. 2023; Benegas, Albors, et al. 2024; Z. Avsec, Latysheva, and Cheng 2025).\n\nThis can help differentiate subtle modulators from catastrophic LoF.\n\nHighlighting mechanism-enriched loci\n\nGFMs provide multi-task predictions (chromatin marks, TF binding, expression, splicing) that make it easier to interpret how a risk locus affects biology (Ž. Avsec et al. 2021; Benegas, Ye, et al. 2024).\n\n\nIn practice, a target discovery workflow might:\n\nStart from GWAS summary statistics or rare variant analyses.\n\nApply fine-mapping (e.g. MIFM) to identify candidate causal variants (Wu et al. 2024; Rakowski and Lippert 2025).\n\nScore candidate variants with VEP GFMs (both protein and regulatory).\n\nMap variants to genes using long-range regulatory models (Enformer, Nucleic Transformer, HyenaDNA) (Ž. Avsec et al. 2021; He et al. 2023; Nguyen et al. 2023).\n\nAggregate signals into gene-level “genetic support” scores, incorporating constraint and pleiotropy information.\n\nThe result is a ranked list of candidate targets with structured evidence that can be compared across diseases and programs.\n\n\n20.2.3 Evolving from hand-curated to model-centric target triage\nHistorically, target triage relied heavily on manual curation:\n\nExperts would review GWAS hits, literature, and pathway diagrams.\n\nLimited quantitative information was available for most genes, especially in non-classical pathways.\n\nGFMs shift this towards a model-centric, continuously updated view:\n\nNew data (e.g. biobank sequencing, single-cell atlases) can be fed through trained GFMs to update variant and gene evidence.\n\nThe same underlying model suite can support many disease programs, enabling consistent cross-portfolio comparisons.\n\nBenchmark frameworks like TraitGym emphasize standardized evaluation of genotype-phenotype modeling, helping teams choose appropriate model stacks for a given trait (Benegas, Eraslan, and Song 2025).\n\nThe limiting factor becomes less “do we have an annotation?” and more “can we interpret the model’s representation and connect it to biological plausibility and druggability?”—a theme echoed in Chapters 13 and 15.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Drug Discovery & Biotech</span>"
    ]
  },
  {
    "objectID": "p6-ch20-drugs.html#functional-genomics-screens-in-drug-discovery",
    "href": "p6-ch20-drugs.html#functional-genomics-screens-in-drug-discovery",
    "title": "20  Drug Discovery & Biotech",
    "section": "20.3 Functional Genomics Screens in Drug Discovery",
    "text": "20.3 Functional Genomics Screens in Drug Discovery\nWhile human genetics offers observational evidence, drug discovery also relies heavily on perturbation experiments:\n\nCRISPR knockout/knockdown/activation screens.\n\nBase-editing or saturation mutagenesis around key domains.\n\nMPRA and massively parallel promoter/enhancer assays.\n\nPerturb-seq and other high-throughput transcriptomic readouts.\n\nGenomic foundation models are well positioned to both design and interpret such screens.\n\n20.3.1 Designing smarter perturbation libraries\nTraditional pooled screens often rely on simple design rules (e.g. one sgRNA per exon, or tiling a region at fixed spacing). GFMs enable more information-dense designs:\n\nSequence-to-function priors\n\nModels like DeepSEA, Enformer, and related CNN/transformer architectures predict which bases are most functionally critical for regulatory outputs (Zhou and Troyanskaya 2015; Ž. Avsec et al. 2021; Benegas, Ye, et al. 2024).\n\nLibrary design can focus perturbations on high-sensitivity sites—predicted TF motifs, splice junctions, or enhancer “hotspots.”\n\nVariant prioritization for saturation mutagenesis\n\nProtein and DNA GFMs can prioritize substitutions expected to span a wide range of predicted fitness, enabling better estimation of quantitative genotype–phenotype maps (Cheng et al. 2023; Marquet et al. 2024).\n\nThis is especially useful for deep mutational scanning near active sites or in regulatory domains.\n\nOff-target and safety considerations\n\nSequence models can help filter sgRNA designs with high predicted off-target binding, or prioritize guide positions that minimize unintended regulatory disruption.\n\n\nThe overarching idea is to maximize the information gained per experimental budget by letting GFMs suggest where to perturb in sequence space.\n\n\n20.3.2 Interpreting screen readouts with GFMs\nOnce a screen has been run, GFMs can assist in several ways:\n\nEmbedding perturbations and outcomes\n\nEncode each perturbed sequence (e.g. enhancer variant, gene knockout) using a DNA or protein GFM, and represent each experimental condition as the combination of its embedding and observed phenotype (e.g. expression profile).\n\nThis enables manifold learning over perturbations, in which clusters correspond to shared mechanism-of-action.\n\nMapping hits back to pathways\n\nCombine GFMs with graph-based models over protein–protein interaction networks and regulatory networks to identify enriched pathways (Gao et al. 2023; Yuan and Duren 2025).\n\nLearned embeddings help propagate signal to weakly observed genes or variants.\n\nClosing the loop with model retraining\n\nUse screen outcomes as labeled examples to fine-tune sequence-to-function models in the relevant cell type or context.\n\nThis “lab-in-the-loop” refinement turns generic GFMs into highly tuned models for the cell system of interest.\n\n\nFor example, an MPRA that assays thousands of enhancer variants can yield sequence–activity pairs that dramatically improve expression-prediction GFMs in that locus or tissue. Conversely, model predictions can suggest follow-up experiments (additional variants, cell types, or perturbation strengths) that would be maximally informative given previous data.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Drug Discovery & Biotech</span>"
    ]
  },
  {
    "objectID": "p6-ch20-drugs.html#biomarker-discovery-patient-stratification-and-trial-design",
    "href": "p6-ch20-drugs.html#biomarker-discovery-patient-stratification-and-trial-design",
    "title": "20  Drug Discovery & Biotech",
    "section": "20.4 Biomarker Discovery, Patient Stratification, and Trial Design",
    "text": "20.4 Biomarker Discovery, Patient Stratification, and Trial Design\nEven when a target is well validated, many programs fail in late-stage trials because the right patients, endpoints, or biomarkers were not selected. GFMs, combined with large cohorts, offer new tools for defining and validating biomarkers.\n\n20.4.1 From polygenic scores to GFM-informed biomarkers\nClassical polygenic scores (PGS) summarize the additive effect of many common variants on disease risk. Deep learning methods such as Delphi extend this idea by learning non-linear genotype–phenotype mappings directly from genome-wide data (Georgantas, Kutalik, and Richiardi 2024).\nGFMs can enhance these approaches by:\n\nProviding richer genetic features\n\nInstead of raw genotypes, models can use VEP-derived scores, variant embeddings, or gene-level features produced by GFMs.\n\nThis can capture non-additive effects, regulatory architecture, and variant-level biology in a more compact representation.\n\nTransferring knowledge across traits and ancestries\n\nFoundation models trained across diverse genomes (e.g. Nucleotide Transformer, GENA-LM, HyenaDNA) provide features that may generalize more robustly across populations than trait-specific models (Dalla-Torre et al. 2023; Fishman et al. 2025; Nguyen et al. 2023).\n\nFine-mapping–aware approaches like MIFM further reduce dependence on linkage disequilibrium patterns (Wu et al. 2024; Rakowski and Lippert 2025).\n\nDistinguishing risk and progression\n\nBy integrating regulatory and expression predictions, risk models can differentiate genetic influences on disease onset vs progression, enabling more targeted enrichment strategies.\n\n\nIn trial design, such models can be used to:\n\nEnrich for high-risk individuals (in prevention trials).\n\nDefine genetic subtypes that may respond differently to the same mechanism.\n\nConstruct composite biomarkers that mix genetics with conventional clinical features.\n\n\n\n20.4.2 Multi-omic and single-cell biomarker discovery\nBeyond DNA variation, drug development increasingly leverages multi-omic and single-cell readouts:\n\nWhole-genome/exome tumor sequencing combined with expression, methylation, and copy-number profiling.\n\nSingle-cell multiome datasets (RNA + ATAC) that characterize cell-state landscapes in disease (Jurenaite et al. 2024; Yuan and Duren 2025).\n\nMicrobiome sequencing for host–microbe interplay and response to therapy (Yan et al. 2025).\n\nGFMs and related architectures can help here in several ways:\n\nSet-based and graph-based encoders\n\nModels like SetQuence/SetOmic treat heterogeneous genomic features for each tumor as a set, using deep set transformers to extract predictive representations (Jurenaite et al. 2024).\n\nGRN inference models such as LINGER leverage atlas-scale multiome data to infer regulatory networks that can serve as biomarkers of pathway activity (Yuan and Duren 2025).\n\nMulti-scale integration\n\nDNA and RNA GFMs can be combined with graph neural networks over gene and protein networks to build end-to-end predictors that map from genotype + cell state to clinical endpoints (Gao et al. 2023; Benegas, Ye, et al. 2024).\n\nEmbeddings from protein LMs (e.g. ESM-2-based variant models) provide additional structure for coding variants (Brandes et al. 2023; Marquet et al. 2024).\n\nBiomarker discovery workflows\n\nUse GFMs to generate rich embeddings for patients (e.g. from tumor genomes, germline variation, or multi-omic profiles).\n\nCluster or perform supervised learning to identify molecular subgroups with differential prognosis or treatment response.\n\nValidate candidate biomarkers on held-out cohorts or external datasets before deploying them in a trial.\n\n\nThe key shift is that biomarkers are no longer limited to a handful of hand-picked variants or expression markers: they become functions over high-dimensional genomic and multi-omic embeddings, learned in a data-driven way yet grounded in biological priors from GFMs.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Drug Discovery & Biotech</span>"
    ]
  },
  {
    "objectID": "p6-ch20-drugs.html#biotech-workflows-and-infrastructure-for-gfms",
    "href": "p6-ch20-drugs.html#biotech-workflows-and-infrastructure-for-gfms",
    "title": "20  Drug Discovery & Biotech",
    "section": "20.5 Biotech Workflows and Infrastructure for GFMs",
    "text": "20.5 Biotech Workflows and Infrastructure for GFMs\nFor pharma and biotech organizations, the primary challenge is not “can we train a big model?” so much as “how do we integrate GFMs into existing data platforms, governance, and decision-making?”\n\n20.5.1 GFMs as shared infrastructure\nIn a mature organization, GFMs should be treated as shared infrastructure, not ad hoc scripts:\n\nModel catalog\n\nDNA LMs (e.g. Nucleic Transformer, HyenaDNA, GENA-LM) (He et al. 2023; Nguyen et al. 2023; Fishman et al. 2025).\n\nSequence-to-function models (e.g. Enformer, Genomic Interpreter) (Ž. Avsec et al. 2021; Li et al. 2023).\n\nVariant effect predictors (AlphaMissense, GPN-MSA, AlphaGenome, CADD v1.7) (Rentzsch et al. 2019; Schubach et al. 2024; Cheng et al. 2023; Benegas, Albors, et al. 2024; Z. Avsec, Latysheva, and Cheng 2025).\n\nFeature services\n\nCentralized APIs that take as input variants, genomic intervals, or genes and return embeddings, predicted functional profiles, or risk features.\n\nLogging and versioning so that analyses can be reproduced even as models and data evolve.\n\nData governance\n\nClear separation between models trained on public data vs. sensitive internal cohorts.\n\nGuardrails around where internal data can be used for fine-tuning and how resulting models can be shared.\n\n\nEmbedding GFMs in this way allows multiple teams—target ID, biomarker discovery, clinical genetics—to reuse the same core representations rather than each building bespoke models.\n\n\n20.5.2 Build vs buy vs fine-tune\nOrganizations face three strategic options:\n\nUse external GFMs “as-is”\n\nPros: Low up-front cost; benefits from community benchmarking (e.g. TraitGym for genotype–phenotype modeling (Benegas, Eraslan, and Song 2025)).\n\nCons: May not capture organization-specific populations, assays, or traits.\n\nFine-tune open-source GFMs on internal data\n\nPros: Retains powerful general representations while adapting to local distribution.\n\nCons: Requires careful privacy controls and computational investment.\n\nTrain bespoke internal GFMs\n\nPros: Maximum control; can align pretraining exactly with available data and target use cases.\n\nCons: Expensive, complex MLOps; risk of overfitting to narrow datasets if not complemented by broader pretraining.\n\n\nIn practice, many groups adopt a hybrid strategy:\n\nStart with public GFMs for early exploration and non-sensitive tasks.\n\nGradually fine-tune on internal biobank or trial data when added value is clear.\n\nMaintain lightweight model-serving infrastructure for latency-sensitive applications (e.g. clinical decision support) and heavier offline systems for large-scale research workloads.\n\n\n\n20.5.3 IP, collaboration, and regulatory considerations\nGFMs also raise new questions around:\n\nIntellectual property\n\nModels trained on proprietary data can be valuable IP assets but are hard to patent directly.\n\nDownstream discoveries (targets, biomarkers) derived from GFMs must be carefully documented for freedom-to-operate.\n\nData sharing and federated approaches\n\nJoint training or evaluation across institutions may require federated learning or model-to-data paradigms, especially for patient-level data.\n\nRegulatory expectations\n\nFor biomarkers used in pivotal trials, regulators will expect transparent documentation of model training, validation, and performance across subgroups.\n\nChapters 14 and 15 highlight confounding and interpretability challenges that become even more acute when models inform trial inclusion or primary endpoints.\n\n\nOverall, leveraging GFMs in biotech is as much an organizational and regulatory engineering problem as a technical one.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Drug Discovery & Biotech</span>"
    ]
  },
  {
    "objectID": "p6-ch20-drugs.html#forward-look-toward-lab-in-the-loop-gfms",
    "href": "p6-ch20-drugs.html#forward-look-toward-lab-in-the-loop-gfms",
    "title": "20  Drug Discovery & Biotech",
    "section": "20.6 Forward Look: Toward Lab-in-the-Loop GFMs",
    "text": "20.6 Forward Look: Toward Lab-in-the-Loop GFMs\nA recurring theme across this book is moving from static models to closed loops that integrate:\n\nFoundational representation learning on large unlabeled datasets (genomes, multi-omics).\n\nTask-specific supervision (disease status, expression, variant effects).\n\nExperimental feedback from perturbation assays, functional screens, and clinical trials.\n\nIn the drug discovery context, this suggests an evolution toward lab-in-the-loop GFMs:\n\nHypothesis generation\n\nGFMs identify promising targets, variants, and regulatory regions.\n\nGraph and set-based models suggest network-level interventions (Jurenaite et al. 2024; Gao et al. 2023; Yuan and Duren 2025).\n\nExperiment design\n\nModels propose perturbation libraries (CRISPR, MPRA) that maximize expected information gain.\n\nSafety and off-target predictions help filter risky designs.\n\nEvidence integration and model refinement\n\nScreen results feed back into GFMs, improving their local accuracy in disease-relevant regions of sequence space.\n\nClinical trial outcomes update biomarker models and risk predictors for future trials.\n\nPortfolio-level decision support\n\nGenetic and functional evidence from GFMs is combined with classical pharmacology to prioritize or deprioritize programs.\n\nUncertainty estimates and model critique (Chapter 17) help avoid over-confidence in purely model-driven recommendations.\n\n\nRealizing this vision will require:\n\nBetter calibration and uncertainty quantification in GFMs.\n\nStronger causal reasoning to distinguish correlation from intervention-worthiness.\n\nCareful ethical and equity considerations, especially when models influence who gets access to trials or targeted therapies (Chapter 16).\n\nYet even in the near term, GFMs already offer tangible value in de-risking targets, enriching cohorts, and interpreting complex functional data. When combined with rigorous experimental design and domain expertise, they can act not as oracle decision-makers, but as force multipliers for human scientists and clinicians.\n\nIn summary, this chapter has sketched how genomic foundation models extend beyond academic benchmarks into practical levers for drug discovery and biotech:\n\nTurning variant and regulatory predictions into target discovery and validation pipelines.\n\nDesigning and interpreting functional genomics screens that probe mechanism and vulnerability.\n\nBuilding richer biomarkers and patient stratification schemes for trials.\n\nEmbedding GFMs into industrial data platforms and MLOps.\n\nSubsequent chapters in Part V can zoom into specific application domains—clinical risk prediction (Chapter 18) and pathogenic variant discovery (Chapter 19)—using the conceptual toolkit laid out here.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025. “[TraitGym] Benchmarking DNA Sequence Models for Causal Regulatory Variant Prediction in Human Genetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun S. Song. 2024. “Genomic Language Models: Opportunities and Challenges.” arXiv. https://doi.org/10.48550/arXiv.2407.11435.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nFishman, Veniamin, Yuri Kuratov, Aleksei Shmelev, Maxim Petrov, Dmitry Penzar, Denis Shepelin, Nikolay Chekanov, Olga Kardymon, and Mikhail Burtsev. 2025. “GENA-LM: A Family of Open-Source Foundational DNA Language Models for Long Sequences.” Nucleic Acids Research 53 (2): gkae1310. https://doi.org/10.1093/nar/gkae1310.\n\n\nGao, Ziqi, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li, Peilin Zhao, Huanming Yang, Yong Huang, and Jia Li. 2023. “[HIGH-PPI] Hierarchical Graph Learning for Protein–Protein Interaction.” Nature Communications 14 (1): 1093. https://doi.org/10.1038/s41467-023-36736-1.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. “Delphi: A Deep-Learning Method for Polygenic Risk Prediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nHe, Shujun, Baizhen Gao, Rushant Sabnis, and Qing Sun. 2023. “Nucleic Transformer: Classifying DNA Sequences with Self-Attention and Convolutions.” ACS Synthetic Biology 12 (11): 3205–14. https://doi.org/10.1021/acssynbio.3c00154.\n\n\nJurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge, and René Jäkel. 2024. “SetQuence & SetOmic: Deep Set Transformers for Whole Genome and Exome Tumour Analysis.” BioSystems 235 (January): 105095. https://doi.org/10.1016/j.biosystems.2023.105095.\n\n\nLi, Zehui, Akashaditya Das, William A. V. Beardall, Yiren Zhao, and Guy-Bart Stan. 2023. “Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer.” arXiv. https://doi.org/10.48550/arXiv.2306.05143.\n\n\nMarquet, Céline, Julius Schlensok, Marina Abakarova, Burkhard Rost, and Elodie Laine. 2024. “[VespaG] Expert-Guided Protein Language Models Enable Accurate and Blazingly Fast Fitness Prediction.” Bioinformatics 40 (11): btae621. https://doi.org/10.1093/bioinformatics/btae621.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nTrop, Evan, Yair Schiff, Edgar Mariano Marroquin, Chia Hsiang Kao, Aaron Gokaslan, McKinley Polen, Mingyi Shao, et al. 2024. “The Genomics Long-Range Benchmark: Advancing DNA Language Models,” October. https://openreview.net/forum?id=8O9HLDrmtq.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray, Peter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping Improves Identification of Causal Variants.” Research Square, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.\n\n\nYan, Binghao, Yunbi Nam, Lingyao Li, Rebecca A. Deek, Hongzhe Li, and Siyuan Ma. 2025. “Recent Advances in Deep Learning and Language Models for Studying the Microbiome.” Frontiers in Genetics 15 (January). https://doi.org/10.3389/fgene.2024.1494474.\n\n\nYuan, Qiuyue, and Zhana Duren. 2025. “[LINGER] Inferring Gene Regulatory Networks from Single-Cell Multiome Data Using Atlas-Scale External Data.” Nature Biotechnology 43 (2): 247–57. https://doi.org/10.1038/s41587-024-02182-7.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part VI: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Drug Discovery & Biotech</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky,\nAnna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev.\n2010. “A Method and Server for Predicting Damaging Missense\nMutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nAll of Us Research Program Investigators, All of Us; 2019. “The\n‘All of Us’ Research\nProgram.” New England Journal of Medicine\n381 (7): 668–76. https://doi.org/10.1056/NEJMsr1809937.\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F.\nScott, and Ada Hamosh. 2015. “OMIM.org:\nOnline Mendelian Inheritance in\nMan (OMIM®), an Online Catalog of Human Genes\nand Genetic Disorders.” Nucleic Acids Research 43 (D1):\nD789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAuton, Adam, Gonçalo R. Abecasis, David M. Altshuler, Richard M. Durbin,\nGonçalo R. Abecasis, David R. Bentley, Aravinda Chakravarti, et al.\n2015. “A Global Reference for Human Genetic Variation.”\nNature 526 (7571): 68–74. https://doi.org/10.1038/nature15393.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A.\nGrabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet\nKohli, and David R. Kelley. 2021. “[Enformer]\nEffective Gene Expression Prediction from Sequence by\nIntegrating Long-Range Interactions.” Nature Methods 18\n(October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025.\n“AlphaGenome: AI for Better\nUnderstanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S.\nSong. 2024. “GPN-MSA: An Alignment-Based\nDNA Language Model for Genome-Wide Variant Effect\nPrediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023.\n“[GPN] DNA Language Models Are Powerful\nPredictors of Genome-Wide Variant Effects.” Proceedings of\nthe National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025.\n“[TraitGym] Benchmarking\nDNA Sequence Models for\nCausal Regulatory Variant\nPrediction in Human\nGenetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun\nS. Song. 2024. “Genomic Language Models:\nOpportunities and Challenges.” arXiv.\nhttps://doi.org/10.48550/arXiv.2407.11435.\n\n\nBommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran\nArora, Sydney von Arx, Michael S. Bernstein, et al. 2022. “On the\nOpportunities and Risks of\nFoundation Models.” arXiv. https://doi.org/10.48550/arXiv.2108.07258.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and\nVasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant\nEffects with a Deep Protein Language Model.” Nature\nGenetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg\nBrockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025.\n“[Evo 2] Genome Modeling and Design\nAcross All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nBrowning, Brian L., Xiaowen Tian, Ying Zhou, and Sharon R. Browning.\n2021. “Fast Two-Stage Phasing of Large-Scale Sequence\nData.” American Journal of Human Genetics 108 (10):\n1880–90. https://doi.org/10.1016/j.ajhg.2021.08.005.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T.\nElliott, Kevin Sharp, Allan Motyer, et al. 2018. “The\nUK Biobank Resource with Deep Phenotyping and\nGenomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T.\nHiggins-Chen, Steve Horvath, and Bo Wang. 2024.\n“CpGPT: A Foundation Model\nfor DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE]\nMulti-Omics Single-Cell Data Integration and Regulatory\nInference with Graph-Linked Embedding.” Nature\nBiotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nChandak, Payal, Kexin Huang, and Marinka Zitnik. 2023.\n“[PrimeKG] Building a Knowledge Graph to\nEnable Precision Medicine.” Scientific Data 10 (1): 67.\nhttps://doi.org/10.1038/s41597-023-01960-3.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou.\n2022. “[DeepSEA Sei] A\nSequence-Based Global Map of Regulatory Activity for Deciphering Human\nGenetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė,\nTaylor Applebaum, Alexander Pritzel, et al. 2023.\n“[AlphaMissense] Accurate Proteome-Wide\nMissense Variant Effect Prediction with\nAlphaMissense.” Science 381 (6664):\neadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nChoi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O’Reilly. 2020.\n“[PRS] Tutorial: A Guide to Performing\nPolygenic Risk Score Analyses.” Nature Protocols 15 (9):\n2759–72. https://doi.org/10.1038/s41596-020-0353-1.\n\n\nChung, Wen-Hung, Shuen-Iu Hung, Hong-Shang Hong, Mo-Song Hsih, Li-Cheng\nYang, Hsin-Chun Ho, Jer-Yuarn Wu, and Yuan-Tsong Chen. 2004. “A\nMarker for Stevens–Johnson Syndrome.”\nNature 428 (6982): 486–86. https://doi.org/10.1038/428486a.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus\nWahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024.\n“[DeepRVAT] Integration of Variant\nAnnotations Using Deep Set Networks Boosts Rare Variant Association\nTesting.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nDabernig-Heinz, Johanna, Mara Lohde, Martin Hölzer, Adriana Cabal, Rick\nConzemius, Christian Brandt, Matthias Kohl, et al. 2024. “A\nMulticenter Study on Accuracy and Reproducibility of Nanopore\nSequencing-Based Genotyping of Bacterial Pathogens.” Journal\nof Clinical Microbiology 62 (9): e00628–24. https://doi.org/10.1128/jcm.00628-24.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez\nCarranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago,\net al. 2023. “Nucleotide Transformer: Building and\nEvaluating Robust Foundation Models for Human Genomics.”\nNature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper,\nArend Sidow, and Serafim Batzoglou. 2010. “Identifying a\nHigh Fraction of the Human\nGenome to Be Under Selective\nConstraint Using GERP++.”\nPLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nDePristo, Mark A., Eric Banks, Ryan Poplin, Kiran V. Garimella, Jared R.\nMaguire, Christopher Hartl, Anthony A. Philippakis, et al. 2011.\n“A Framework for Variation Discovery and Genotyping Using\nNext-Generation DNA Sequencing Data.” Nature\nGenetics 43 (5): 491–98. https://doi.org/10.1038/ng.806.\n\n\nEdgar, Ron, Michael Domrachev, and Alex E. Lash. 2002. “Gene\nExpression Omnibus: NCBI Gene\nExpression and Hybridization Array Data Repository.” Nucleic\nAcids Research 30 (1): 207–10. https://doi.org/10.1093/nar/30.1.207.\n\n\nFishman, Veniamin, Yuri Kuratov, Aleksei Shmelev, Maxim Petrov, Dmitry\nPenzar, Denis Shepelin, Nikolay Chekanov, Olga Kardymon, and Mikhail\nBurtsev. 2025. “GENA-LM: A Family of\nOpen-Source Foundational DNA Language Models for Long\nSequences.” Nucleic Acids Research 53 (2): gkae1310. https://doi.org/10.1093/nar/gkae1310.\n\n\nFrankish, Adam, Mark Diekhans, Anne-Maud Ferreira, Rory Johnson, Irwin\nJungreis, Jane Loveland, Jonathan M Mudge, et al. 2019.\n“GENCODE Reference Annotation for the Human and Mouse\nGenomes.” Nucleic Acids Research 47 (D1): D766–73. https://doi.org/10.1093/nar/gky955.\n\n\nGamazon, Eric R., Heather E. Wheeler, Kaanan P. Shah, Sahar V.\nMozaffari, Keston Aquino-Michaels, Robert J. Carroll, Anne E. Eyler, et\nal. 2015. “A Gene-Based Association Method for Mapping Traits\nUsing Reference Transcriptome Data.” Nature Genetics 47\n(9): 1091–98. https://doi.org/10.1038/ng.3367.\n\n\nGao, Ziqi, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li,\nPeilin Zhao, Huanming Yang, Yong Huang, and Jia Li. 2023.\n“[HIGH-PPI] Hierarchical\nGraph Learning for Protein–Protein Interaction.” Nature\nCommunications 14 (1): 1093. https://doi.org/10.1038/s41467-023-36736-1.\n\n\nGarrison, Erik, Jouni Sirén, Adam M. Novak, Glenn Hickey, Jordan M.\nEizenga, Eric T. Dawson, William Jones, et al. 2018. “Variation\nGraph Toolkit Improves Read Mapping by Representing Genetic Variation in\nthe Reference.” Nature Biotechnology 36 (9): 875–79. https://doi.org/10.1038/nbt.4227.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024.\n“Delphi: A Deep-Learning\nMethod for Polygenic Risk\nPrediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nGoodwin, Sara, John D. McPherson, and W. Richard McCombie. 2016.\n“Coming of Age: Ten Years of Next-Generation Sequencing\nTechnologies.” Nature Reviews Genetics 17 (6): 333–51.\nhttps://doi.org/10.1038/nrg.2016.49.\n\n\nGuo, Fei, Renchu Guan, Yaohang Li, Qi Liu, Xiaowo Wang, Can Yang, and\nJianxin Wang. 2025. “Foundation Models in Bioinformatics.”\nNational Science Review 12 (4): nwaf028. https://doi.org/10.1093/nsr/nwaf028.\n\n\nGusev, Alexander, Arthur Ko, Huwenbo Shi, Gaurav Bhatia, Wonil Chung,\nBrenda W. J. H. Penninx, Rick Jansen, et al. 2016. “Integrative\nApproaches for Large-Scale Transcriptome-Wide Association\nStudies.” Nature Genetics 48 (3): 245–52. https://doi.org/10.1038/ng.3506.\n\n\nHe, Shujun, Baizhen Gao, Rushant Sabnis, and Qing Sun. 2023.\n“Nucleic Transformer: Classifying\nDNA Sequences with\nSelf-Attention and\nConvolutions.” ACS Synthetic Biology 12\n(11): 3205–14. https://doi.org/10.1021/acssynbio.3c00154.\n\n\nHudaiberdiev, Sanjarbek, D. Leland Taylor, Wei Song, Narisu Narisu,\nRedwan M. Bhuiyan, Henry J. Taylor, Xuming Tang, et al. 2023.\n“[TREDNet] Modeling Islet Enhancers\nUsing Deep Learning Identifies Candidate Causal Variants at Loci\nAssociated with T2D and Glycemic Traits.”\nProceedings of the National Academy of Sciences 120 (35):\ne2206612120. https://doi.org/10.1073/pnas.2206612120.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F.\nMcRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A.\nKosmicki, et al. 2019. “[SpliceAI]\nPredicting Splicing from Primary\nSequence with Deep\nLearning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021.\n“DNABERT: Pre-Trained Bidirectional\nEncoder Representations from\nTransformers Model for DNA-Language in\nGenome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nJiang, Tao, Yongzhuang Liu, Yue Jiang, Junyi Li, Yan Gao, Zhe Cui,\nYadong Liu, Bo Liu, and Yadong Wang. 2020. “Long-Read-Based Human\nGenomic Structural Variation Detection with cuteSV.” Genome Biology 21 (1):\n189. https://doi.org/10.1186/s13059-020-02107-y.\n\n\nJurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge,\nand René Jäkel. 2024. “SetQuence &\nSetOmic: Deep Set Transformers for Whole\nGenome and Exome Tumour Analysis.” BioSystems 235\n(January): 105095. https://doi.org/10.1016/j.biosystems.2023.105095.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A.\nSloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data\nNavigation on the ENCODE Portal.” Nature\nCommunications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B.\nCummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020.\n“The Mutational Constraint Spectrum Quantified from Variation in\n141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKrusche, Peter, Len Trigg, Paul C. Boutros, Christopher E. Mason,\nFrancisco M. De La Vega, Benjamin L. Moore, Mar Gonzalez-Porta, et al.\n2019. “Best Practices for Benchmarking\nGermline Small Variant\nCalls in Human Genomes.”\nNature Biotechnology 37 (5): 555–60. https://doi.org/10.1038/s41587-019-0054-x.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela\nYen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015.\n“Integrative Analysis of 111 Reference Human Epigenomes.”\nNature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nKurki, Mitja I., Juha Karjalainen, Priit Palta, Timo P. Sipilä, Kati\nKristiansson, Kati M. Donner, Mary P. Reeve, et al. 2023.\n“FinnGen Provides Genetic Insights from a\nWell-Phenotyped Isolated Population.” Nature 613 (7944):\n508–18. https://doi.org/10.1038/s41586-022-05473-8.\n\n\nLambert, Samuel A., Laurent Gil, Simon Jupp, Scott C. Ritchie, Yu Xu,\nAnnalisa Buniello, Aoife McMahon, et al. 2021. “The\nPolygenic Score Catalog as an\nOpen Database for Reproducibility and Systematic Evaluation.”\nNature Genetics 53 (4): 420–25. https://doi.org/10.1038/s41588-021-00783-5.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen\nChao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018.\n“ClinVar: Improving Access to Variant Interpretations\nand Supporting Evidence.” Nucleic Acids Research 46\n(D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam,\nAmit R. Majithia, and Trey Ideker. 2025. “[G2PT]\nA Genotype-Phenotype Transformer to Assess and Explain\nPolygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Hao, Zebei Han, Yu Sun, Fu Wang, Pengzhen Hu, Yuang Gao, Xuemei Bai,\net al. 2024. “CGMega: Explainable Graph Neural\nNetwork Framework with Attention Mechanisms for Cancer Gene Module\nDissection.” Nature Communications 15 (1): 5997. https://doi.org/10.1038/s41467-024-50426-6.\n\n\nLi, Heng. 2013. “Aligning Sequence Reads, Clone Sequences and\nAssembly Contigs with BWA-MEM.” arXiv.\nhttps://doi.org/10.48550/arXiv.1303.3997.\n\n\n———. 2014. “Towards Better Understanding\nof Artifacts in Variant Calling\nfrom High-Coverage\nSamples.” Bioinformatics 30 (20): 2843–51.\nhttps://doi.org/10.1093/bioinformatics/btu356.\n\n\n———. 2018. “Minimap2: Pairwise Alignment for Nucleotide\nSequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and\nYunping Zhu. 2022. “MoGCN: A\nMulti-Omics Integration\nMethod Based on Graph\nConvolutional Network for Cancer\nSubtype Analysis.” Frontiers in\nGenetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nLi, Zehui, Akashaditya Das, William A. V. Beardall, Yiren Zhao, and\nGuy-Bart Stan. 2023. “Genomic Interpreter:\nA Hierarchical Genomic\nDeep Neural Network with\n1D Shifted Window\nTransformer.” arXiv. https://doi.org/10.48550/arXiv.2306.05143.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness,\nGlenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome\nReference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting\nLu, Allan dos Santos Costa, et al. 2022. “[ESM-2]\nLanguage Models of Protein Sequences at the Scale of\nEvolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and\nDavid R. Kelley. 2025. “[Borzoi]\nPredicting RNA-Seq Coverage from\nDNA Sequence as a Unifying Model of Gene\nRegulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang,\nYucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025.\n“Life-Code: Central Dogma\nModeling with Multi-Omics\nSequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nLoh, Po-Ru, Petr Danecek, Pier Francesco Palamara, Christian\nFuchsberger, Yakir A Reshef, Hilary K Finucane, Sebastian Schoenherr, et\nal. 2016. “Reference-Based Phasing Using the\nHaplotype Reference Consortium\nPanel.” Nature Genetics 48 (11): 1443–48. https://doi.org/10.1038/ng.3679.\n\n\nMa, Jiani, Jiangning Song, Neil D. Young, Bill C. H. Chang, Pasi K.\nKorhonen, Tulio L. Campos, Hui Liu, and Robin B. Gasser. 2023.\n“’Bingo’-a Large Language Model- and Graph Neural\nNetwork-Based Workflow for the Prediction of Essential Genes from\nProtein Data.” Briefings in Bioinformatics 25 (1):\nbbad472. https://doi.org/10.1093/bib/bbad472.\n\n\nMallal, Simon, Elizabeth Phillips, Giampiero Carosi, Jean-Michel Molina,\nCassy Workman, Janez Tomažič, Eva Jägel-Guedes, et al. 2008.\n“HLA-B*5701 Screening for\nHypersensitivity to Abacavir.” New\nEngland Journal of Medicine 358 (6): 568–79. https://doi.org/10.1056/NEJMoa0706135.\n\n\nManzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025.\n“Comparative Analysis of Deep\nLearning Models for Predicting\nCausative Regulatory\nVariants.” bioRxiv: The Preprint Server for\nBiology, June, 2025.05.19.654920. https://doi.org/10.1101/2025.05.19.654920.\n\n\nMarees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan,\nEmmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018.\n“[GWAS] A Tutorial on Conducting\nGenome-Wide Association Studies: Quality Control and\nStatistical Analysis.” International Journal of Methods in\nPsychiatric Research 27 (2): e1608. https://doi.org/10.1002/mpr.1608.\n\n\nMarquet, Céline, Julius Schlensok, Marina Abakarova, Burkhard Rost, and\nElodie Laine. 2024. “[VespaG]\nExpert-Guided Protein Language Models Enable Accurate and\nBlazingly Fast Fitness Prediction.” Bioinformatics 40\n(11): btae621. https://doi.org/10.1093/bioinformatics/btae621.\n\n\nMcKenna, Aaron, Matthew Hanna, Eric Banks, Andrey Sivachenko, Kristian\nCibulskis, Andrew Kernytsky, Kiran Garimella, et al. 2010. “The\nGenome Analysis Toolkit:\nA MapReduce Framework for Analyzing\nNext-Generation DNA Sequencing Data.” Genome\nResearch 20 (9): 1297–1303. https://doi.org/10.1101/gr.107524.110.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill\nVishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel,\nRonnie Rajan, and Shadab Khan. 2025. “BioToken and\nBioFM – Biologically-Informed\nTokenization Enables Accurate and\nEfficient Genomic Foundation\nModels.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and\nAlexander Rives. 2021. “[ESM-1v]\nLanguage Models Enable Zero-Shot Prediction of the Effects\nof Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nMorales, Joannella, Shashikant Pujar, Jane E. Loveland, Alex Astashyn,\nRuth Bennett, Andrew Berry, Eric Cox, et al. 2022. “A Joint\nNCBI and EMBL-EBI Transcript Set\nfor Clinical Genomics and Research.” Nature 604 (7905):\n310–15. https://doi.org/10.1038/s41586-022-04558-8.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy\nSchwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021.\n“An Open Approach to Systematically Prioritize Causal Variants and\nGenes at All Published Human GWAS Trait-Associated\nLoci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nNaghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B.\nMacdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow\nCoyote-Maestas, and Hani Goodarzi. 2024. “[cdsFM - EnCodon/DeCodon]\nA Suite of Foundation\nModels Captures the Contextual\nInterplay Between Codons.”\nbioRxiv. https://doi.org/10.1101/2024.10.10.617568.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT:\nPredicting Amino Acid Changes That Affect Protein\nFunction.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum\nBirch-Sykes, Michael Wornow, Aman Patel, et al. 2023.\n“HyenaDNA: Long-Range\nGenomic Sequence Modeling at\nSingle Nucleotide\nResolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nNielsen, Rasmus, Joshua S. Paul, Anders Albrechtsen, and Yun S. Song.\n2011. “Genotype and SNP Calling from Next-Generation\nSequencing Data.” Nature Reviews. Genetics 12 (6):\n443–51. https://doi.org/10.1038/nrg2986.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk,\nSteffanie Paul, Han Spinner, Nathan Rollins, et al. 2023.\n“ProteinGym: Large-Scale\nBenchmarks for Protein Fitness\nPrediction and Design.” Advances in\nNeural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V.\nBzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The\nComplete Sequence of a Human Genome.” Science 376\n(6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Connell, Jared, Deepti Gurdasani, Olivier Delaneau, Nicola Pirastu,\nSheila Ulivi, Massimiliano Cocca, Michela Traglia, et al. 2014. “A\nGeneral Approach for Haplotype\nPhasing Across the Full Spectrum\nof Relatedness.” PLOS Genetics 10 (4):\ne1004234. https://doi.org/10.1371/journal.pgen.1004234.\n\n\nO’Leary, Nuala A., Mathew W. Wright, J. Rodney Brister, Stacy Ciufo,\nDiana Haddad, Rich McVeigh, Bhanu Rajput, et al. 2016. “Reference\nSequence (RefSeq) Database at NCBI: Current\nStatus, Taxonomic Expansion, and Functional Annotation.”\nNucleic Acids Research 44 (D1): D733–45. https://doi.org/10.1093/nar/gkv1189.\n\n\n“PacificBiosciences/Pbsv.” 2025. PacBio. https://github.com/PacificBiosciences/pbsv.\n\n\nPadyukov, Leonid. 2022. “Genetics of Rheumatoid Arthritis.”\nSeminars in Immunopathology 44 (1): 47–62. https://doi.org/10.1007/s00281-022-00912-0.\n\n\nPasaniuc, Bogdan, and Alkes L. Price. 2016. “Dissecting the\nGenetics of Complex Traits Using Summary Association Statistics.”\nNature Reviews Genetics 18 (2): 117–27. https://doi.org/10.1038/nrg.2016.142.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006.\n“Population Structure and\nEigenanalysis.” PLOS Genetics 2 (12): e190.\nhttps://doi.org/10.1371/journal.pgen.0020190.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel,\nSean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022.\n“Calibration of Computational Tools for Missense Variant\nPathogenicity Classification and ClinGen Recommendations\nfor PP3/BP4 Criteria.” American\nJournal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nPoplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas\nColthurst, Alexander Ku, Dan Newburger, et al. 2018.\n“[DeepVariant] A Universal\nSNP and Small-Indel Variant Caller Using Deep Neural\nNetworks.” Nature Biotechnology 36 (10): 983–87. https://doi.org/10.1038/nbt.4235.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025.\n“[MIFM] Multiple Instance Fine-Mapping:\nPredicting Causal Regulatory Variants with a Deep Sequence\nModel.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\n“RealTimeGenomics/Rtg-Core.” 2025. Real Time\nGenomics. https://github.com/RealTimeGenomics/rtg-core.\n\n\nRegev, Aviv, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe\nBenoist, Ewan Birney, Bernd Bodenmiller, et al. 2017. “The\nHuman Cell Atlas.” Edited\nby Thomas R Gingeras. eLife 6 (December): e27041. https://doi.org/10.7554/eLife.27041.\n\n\nRehm, Heidi L., Jonathan S. Berg, Lisa D. Brooks, Carlos D. Bustamante,\nJames P. Evans, Melissa J. Landrum, David H. Ledbetter, et al. 2015.\n“ClinGen — The Clinical\nGenome Resource.” New England\nJournal of Medicine 372 (23): 2235–42. https://doi.org/10.1056/NEJMsr1406261.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and\nMartin Kircher. 2019. “CADD: Predicting the\nDeleteriousness of Variants Throughout the Human Genome.”\nNucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin,\nJason Liu, Demi Guo, et al. 2021. “[ESM-1b]\nBiological Structure and Function Emerge from Scaling\nUnsupervised Learning to 250 Million Protein Sequences.”\nProceedings of the National Academy of Sciences of the United States\nof America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nRobinson, James, Dominic J Barker, Xenia Georgiou, Michael A Cooper,\nPaul Flicek, and Steven G E Marsh. 2020.\n“IPD-IMGT/HLA\nDatabase.” Nucleic Acids Research 48 (D1):\nD948–55. https://doi.org/10.1093/nar/gkz950.\n\n\nSakaue, Saori, Saisriram Gurajala, Michelle Curtis, Yang Luo, Wanson\nChoi, Kazuyoshi Ishigaki, Joyce B. Kang, et al. 2023. “Tutorial: A\nStatistical Genetics Guide to Identifying HLA Alleles\nDriving Complex Disease.” Nature Protocols 18 (9):\n2625–41. https://doi.org/10.1038/s41596-023-00853-4.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch.\n2024. “[GROVER] DNA Language Model\nGROVER Learns Sequence Context in the Human Genome.”\nNature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and\nMartin Kircher. 2024. “CADD V1.7: Using Protein\nLanguage Models, Regulatory CNNs and Other Nucleotide-Level\nScores to Improve Genome-Wide Variant Predictions.” Nucleic\nAcids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nShafin, Kishwar, Trevor Pesout, Pi-Chuan Chang, Maria Nattestad, Alexey\nKolesnikov, Sidharth Goel, Gunjan Baid, et al. 2021.\n“Haplotype-Aware Variant Calling with\nPEPPER-Margin-DeepVariant Enables\nHigh Accuracy in Nanopore Long-Reads.” Nature Methods 18\n(11): 1322–32. https://doi.org/10.1038/s41592-021-01299-w.\n\n\nSherry, S. T., M.-H. Ward, M. Kholodov, J. Baker, L. Phan, E. M.\nSmigielski, and K. Sirotkin. 2001. “dbSNP: The NCBI Database of Genetic\nVariation.” Nucleic Acids Research 29 (1): 308–11. https://doi.org/10.1093/nar/29.1.308.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs,\nMinmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005.\n“[PhastCons] Evolutionarily Conserved\nElements in Vertebrate, Insect, Worm, and Yeast Genomes.”\nGenome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.\n\n\nSirugo, Giorgio, Scott M. Williams, and Sarah A. Tishkoff. 2019.\n“The Missing Diversity in\nHuman Genetic Studies.”\nCell 177 (1): 26–31. https://doi.org/10.1016/j.cell.2019.02.048.\n\n\nSmolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W.\nHorner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024.\n“Detection of Mosaic and Population-Level Structural Variants with\nSniffles2.” Nature Biotechnology 42 (10):\n1571–80. https://doi.org/10.1038/s41587-023-02024-y.\n\n\nSollis, Elliot, Abayomi Mosaku, Ala Abid, Annalisa Buniello, Maria\nCerezo, Laurent Gil, Tudor Groza, et al. 2023. “The\nNHGRI-EBI GWAS\nCatalog: Knowledgebase and Deposition Resource.”\nNucleic Acids Research 51 (D1): D977–85. https://doi.org/10.1093/nar/gkac1010.\n\n\nSong, Li, Gali Bai, X. Shirley Liu, Bo Li, and Heng Li. 2022.\n“T1K: Efficient and Accurate KIR and\nHLA Genotyping with Next-Generation Sequencing\nData.” bioRxiv. https://doi.org/10.1101/2022.10.26.513955.\n\n\n“The Genome Aggregation\nDatabase (gnomAD).” n.d.\nAccessed July 3, 2025. https://www.nature.com/immersive/d42859-020-00002-x/index.html.\n\n\nThe GTEx Consortium. 2020. “The GTEx\nConsortium Atlas of Genetic Regulatory Effects Across Human\nTissues.” Science 369 (6509): 1318–30. https://doi.org/10.1126/science.aaz1776.\n\n\nThe Tabula Sapiens Consortium. 2022. “The Tabula\nSapiens: A Multiple-Organ, Single-Cell\nTranscriptomic Atlas of Humans.” Science 376 (6594):\neabl4896. https://doi.org/10.1126/science.abl4896.\n\n\nTrop, Evan, Yair Schiff, Edgar Mariano Marroquin, Chia Hsiang Kao, Aaron\nGokaslan, McKinley Polen, Mingyi Shao, et al. 2024. “The\nGenomics Long-Range\nBenchmark: Advancing DNA\nLanguage Models,” October. https://openreview.net/forum?id=8O9HLDrmtq.\n\n\nVan der Auwera, Geraldine A., Mauricio O. Carneiro, Christopher Hartl,\nRyan Poplin, Guillermo del Angel, Ami Levy-Moonshine, Tadeusz Jordan, et\nal. 2018. “From FastQ Data to\nHigh-Confidence Variant\nCalls: The Genome\nAnalysis Toolkit Best\nPractices Pipeline.” Current\nProtocols in Bioinformatics 43 (1): 11.10.1–33. https://doi.org/10.1002/0471250953.bi1110s43.\n\n\nVerma, Anurag, Jennifer E. Huffman, Alex Rodriguez, Mitchell Conery,\nMolei Liu, Yuk-Lam Ho, Youngdae Kim, et al. 2024. “Diversity and\nScale: Genetic Architecture of 2068 Traits in the\nVA Million Veteran\nProgram.” Science 385 (6706): eadj1182. https://doi.org/10.1126/science.adj1182.\n\n\nVilhjálmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev,\nSara Lindström, Stephan Ripke, Giulio Genovese, et al. 2015.\n“Modeling Linkage Disequilibrium\nIncreases Accuracy of Polygenic\nRisk Scores.” American Journal of\nHuman Genetics 97 (4): 576–92. https://doi.org/10.1016/j.ajhg.2015.09.001.\n\n\nVõsa, Urmo, Annique Claringbould, Harm-Jan Westra, Marc Jan Bonder,\nPatrick Deelen, Biao Zeng, Holger Kirsten, et al. 2021.\n“Large-Scale Cis- and Trans-eQTL\nAnalyses Identify Thousands of Genetic Loci and Polygenic Scores That\nRegulate Blood Gene Expression.” Nature Genetics 53 (9):\n1300–1310. https://doi.org/10.1038/s41588-021-00913-z.\n\n\nWenger, Aaron M., Paul Peluso, William J. Rowell, Pi-Chuan Chang,\nRichard J. Hall, Gregory T. Concepcion, Jana Ebler, et al. 2019.\n“Accurate Circular Consensus Long-Read Sequencing Improves Variant\nDetection and Assembly of a Human Genome.” Nature\nBiotechnology 37 (10): 1155–62. https://doi.org/10.1038/s41587-019-0217-9.\n\n\nWhirl-Carrillo, M, E M McDonagh, J M Hebert, L Gong, K Sangkuhl, C F\nThorn, R B Altman, and T E Klein. 2012. “Pharmacogenomics\nKnowledge for Personalized\nMedicine.” Clinical Pharmacology &\nTherapeutics 92 (4): 414–17. https://doi.org/10.1038/clpt.2012.96.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray,\nPeter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping\nImproves Identification of Causal Variants.” Research\nSquare, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.\n\n\nYan, Binghao, Yunbi Nam, Lingyao Li, Rebecca A. Deek, Hongzhe Li, and\nSiyuan Ma. 2025. “Recent Advances in Deep Learning and Language\nModels for Studying the Microbiome.” Frontiers in\nGenetics 15 (January). https://doi.org/10.3389/fgene.2024.1494474.\n\n\nYuan, Qiuyue, and Zhana Duren. 2025. “[LINGER]\nInferring Gene Regulatory Networks from Single-Cell\nMultiome Data Using Atlas-Scale External Data.” Nature\nBiotechnology 43 (2): 247–57. https://doi.org/10.1038/s41587-024-02182-7.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll,\nand Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls\nUsing DeepVariant and GLnexus.”\nBioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei\nSun, Chen-Hao Chen, et al. 2019. “Cistrome Data\nBrowser: Expanded Datasets and New Tools for Gene\nRegulatory Analysis.” Nucleic Acids Research 47 (D1):\nD729–35. https://doi.org/10.1093/nar/gky1094.\n\n\nZheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam,\nand Ruibang Luo. 2022. “Symphonizing Pileup and Full-Alignment for\nDeep Learning-Based Long-Read Variant Calling.” Nature\nComputational Science 2 (12): 797–803. https://doi.org/10.1038/s43588-022-00387-x.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K.\nWong, and Olga G. Troyanskaya. 2018. “[Expecto]\nDeep Learning Sequence-Based Ab Initio Prediction of\nVariant Effects on Expression and Disease Risk.” Nature\nGenetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA]\nPredicting Effects of Noncoding Variants with Deep\nLearning–Based Sequence Model.” Nature Methods 12 (10):\n931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and\nHan Liu. 2024. “DNABERT-2: Efficient\nFoundation Model and Benchmark\nFor Multi-Species\nGenome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.\n\n\nZook, Justin M., Jennifer McDaniel, Nathan D. Olson, Justin Wagner,\nHemang Parikh, Haynes Heaton, Sean A. Irvine, et al. 2019. “An\nOpen Resource for Accurately Benchmarking Small Variant and Reference\nCalls.” Nature Biotechnology 37 (5): 561–66. https://doi.org/10.1038/s41587-019-0074-6.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "app-a-dl.html",
    "href": "app-a-dl.html",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "",
    "text": "A.1 From Linear Models to Deep Networks\nThis appendix gives a compact introduction to deep learning for readers who are comfortable with genomics but less familiar with modern neural networks. The goal is not to replace a full machine learning textbook, but to provide enough background to make the models in Chapters 5–19 feel intuitive rather than magical.\nWe focus on:\nWhere possible, we connect directly to the genomic case studies in the main text (DeepSEA, ExPecto, SpliceAI, Enformer, genomic language models, and GFMs).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#from-linear-models-to-deep-networks",
    "href": "app-a-dl.html#from-linear-models-to-deep-networks",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "",
    "text": "A.1.1 Models as Functions\nAt its core, a predictive model is just a function:\n\\[\nf_\\theta: x \\mapsto \\hat{y}\n\\tag{A.1}\\]\nwhere:\n\n\\(x\\) is an input (e.g., a one-hot encoded DNA sequence, variant-level features, or a patient feature vector).\n\n\\(\\hat{y}\\) is a prediction (e.g., probability of a histone mark, gene expression level, disease risk).\n\n\\(\\theta\\) are the parameters (weights) of the model.\n\nIn classical genomics workflows, \\(f_\\theta\\) might be:\n\nLogistic regression (for case–control status)\n\nLinear regression (for quantitative traits)\n\nRandom forests or gradient boosting (for variant pathogenicity scores)\n\nDeep learning keeps the same basic structure but allows \\(f_\\theta\\) to be a much more flexible, high-capacity function built by composing many simple operations.\n\n\nA.1.2 Linear Models vs Neural Networks\nA simple linear model for classification looks like:\n\\[\n\\hat{y} = \\sigma(w^\\top x + b),\n\\]\nwhere \\(w\\) and \\(b\\) are parameters and \\(\\sigma(\\cdot)\\) is a squashing nonlinearity (e.g., the logistic function). The model draws a single separating hyperplane in feature space.\nA neural network generalizes this by stacking multiple linear transformations with nonlinear activation functions:\n\\[\n\\begin{aligned}\nh_1 &= \\phi(W_1 x + b_1) \\\\\nh_2 &= \\phi(W_2 h_1 + b_2) \\\\\n&\\vdots \\\\\n\\hat{y} &= g(W_L h_{L-1} + b_L)\n\\end{aligned}\n\\]\nwhere:\n\nEach \\(W_\\ell, b_\\ell\\) is a layer’s weight matrix and bias.\n\n\\(\\phi(\\cdot)\\) is a nonlinear activation (e.g., ReLU).\n\n\\(g(\\cdot)\\) is a final activation (e.g., sigmoid for probabilities, identity for regression).\n\nThe key idea:\n\nBy composing many simple nonlinear transformations, deep networks can approximate very complex functions.\n\nIn Chapters 5–7, DeepSEA, ExPecto, and SpliceAI implement exactly this pattern, but with convolutional layers (Section 4) tailored to 1D DNA sequence instead of dense matrix multiplications (Zhou and Troyanskaya 2015; Zhou et al. 2018; Jaganathan et al. 2019).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#training-deep-models",
    "href": "app-a-dl.html#training-deep-models",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "A.2 Training Deep Models",
    "text": "A.2 Training Deep Models\n\nA.2.1 Data, Labels, and Loss Functions\nTo train a model, we need:\n\nA dataset of examples \\(\\{(x_i, y_i)\\}_{i=1}^N\\)\n\nA model \\(f_\\theta\\)\n\nA loss function \\(L(\\hat{y}, y)\\) that measures how wrong a prediction is\n\nCommon loss functions:\n\nBinary cross-entropy (for yes/no labels, e.g., “is this ChIP–seq peak present?”):\n\\[\nL(\\hat{p}, y) = -\\big(y \\log \\hat{p} + (1-y)\\log(1-\\hat{p})\\big)\n\\]\nMulticlass cross-entropy (for one-of-K labels)\n\nMean squared error (MSE) (for continuous outputs, e.g., gene expression)\n\nThe training objective is to find \\(\\theta\\) that minimizes the average loss:\n\\[\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N L\\big(f_\\theta(x_i), y_i\\big).\n\\]\n\n\nA.2.2 2.2 Gradient-Based Optimization\nDeep networks may have millions to billions of parameters. We can’t search over all possibilities, but we can follow the gradient of the loss with respect to \\(\\theta\\):\n\nGradient descent updates: \\[\n\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta),\n\\] where \\(\\eta\\) is the learning rate.\n\nIn practice, we use:\n\nMini-batch stochastic gradient descent (SGD): Compute gradients on small batches of examples (e.g., 128 sequences at a time) for efficiency and better generalization.\nAdaptive optimizers like Adam, which adjust learning rates per parameter.\n\nYou never compute gradients by hand; modern frameworks (PyTorch, JAX, TensorFlow) use automatic differentiation to efficiently compute \\(\\nabla_\\theta \\mathcal{L}\\) even for very complex architectures.\n\n\nA.2.3 Backpropagation in One Sentence\nBackpropagation is just the chain rule of calculus applied efficiently through the layers of a network. It propagates “blame” from the output back to each weight, telling us how changing that weight would change the loss.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#generalization-overfitting-and-evaluation",
    "href": "app-a-dl.html#generalization-overfitting-and-evaluation",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "A.3 Generalization, Overfitting, and Evaluation",
    "text": "A.3 Generalization, Overfitting, and Evaluation\n\nA.3.1 Train / Validation / Test Splits\nDeep networks can memorize training data if we’re not careful. To evaluate generalization, we typically split data into:\n\nTraining set – used to fit parameters\n\nValidation set – used to tune hyperparameters (learning rate, depth, etc.) and perform early stopping\n\nTest set – held out until the end to estimate performance on new data\n\nIn genomics, how we split matters as much as how much data we have:\n\nSplitting by locus or chromosome (to test cross-locus generalization)\n\nSplitting by individual or cohort (to avoid leakage between related samples)\n\nSplitting by species or ancestry when evaluating transfer\n\nThese issues are developed in more depth in the evaluation and confounding chapters (Chapter 15 and Chapter 16).\n\n\nA.3.2 Overfitting and Regularization\nSigns of overfitting:\n\nTraining loss keeps decreasing, but validation loss starts increasing.\n\nMetrics like AUROC or AUPRC plateau or drop on validation data even as they improve on training data.\n\nCommon regularization techniques:\n\nWeight decay / L2 regularization – penalize large weights.\n\nDropout – randomly zero out activations during training.\n\nEarly stopping – stop training when validation performance stops improving.\n\nData augmentation – generate more training examples by transforming inputs, e.g.:\n\nReverse-complement augmentation for DNA sequences (treat sequence and its reverse complement as equivalent).\n\nWindow jittering: randomly shifting the sequence window around a target site.\n\n\n\n\nA.3.3 Basic Metrics\nYou’ll encounter metrics such as:\n\nAUROC (Area Under the ROC Curve) – how well the model ranks positives above negatives.\n\nAUPRC (Area Under the Precision–Recall Curve) – more informative when positives are rare.\n\nCalibration metrics (e.g., Brier score) and reliability diagrams – especially for clinical risk prediction (Chapter 18).\n\nThe model and application chapters provide details about which metrics are appropriate for which tasks. See Chapter 15 for more on evaluation metrics.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#convolutional-networks-for-genomic-sequences",
    "href": "app-a-dl.html#convolutional-networks-for-genomic-sequences",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "A.4 Convolutional Networks for Genomic Sequences",
    "text": "A.4 Convolutional Networks for Genomic Sequences\nConvolutional neural networks (CNNs) are the workhorse architecture in early genomic deep learning models like DeepSEA, ExPecto, and SpliceAI (Zhou and Troyanskaya 2015; Zhou et al. 2018; Jaganathan et al. 2019).\n\nA.4.1 1D Convolutions as Motif Detectors\nFor a 1D DNA sequence encoded as a matrix \\(X \\in \\mathbb{R}^{L \\times 4}\\) (length \\(L\\), 4 nucleotides), a convolutional layer applies a set of filters (kernels) of width \\(k\\):\n\nEach filter is a small matrix \\(K \\in \\mathbb{R}^{k \\times 4}\\).\n\nAt each position, the filter computes a dot product between \\(K\\) and the corresponding \\(k\\)-length chunk of \\(X\\).\n\nSliding the filter along the sequence creates an activation map that is high wherever the motif encoded by \\(K\\) is present.\n\nIntuitively:\n\nA 1D convolutional filter learns to recognize sequence motifs (e.g., transcription factor binding sites) directly from data.\n\n\n\nA.4.2 Stacking Layers and Receptive Fields\nDeeper convolutional layers allow the model to “see” longer-range patterns:\n\nFirst layer: short motifs (e.g., 8–15 bp).\n\nHigher layers: combinations of motifs, motif spacing, and local regulatory grammar.\n\nPooling layers (e.g., max pooling) reduce spatial resolution while aggregating features, increasing the receptive field.\n\nIn DeepSEA, stacked convolutions and pooling allow the model to use hundreds of base pairs of context around a locus to predict chromatin state (Zhou and Troyanskaya 2015). ExPecto extends this idea by mapping sequence to tissue-specific expression predictions (Zhou et al. 2018). SpliceAI uses very deep dilated convolutions to reach ~10 kb of context for splicing (Jaganathan et al. 2019).\n\n\nA.4.3 Multi-Task Learning\nEarly sequence-to-function CNNs are almost always multi-task:\n\nA single input sequence is used to predict many outputs simultaneously (e.g., hundreds of TF ChIP–seq peaks, histone marks, DNase hypersensitivity tracks).\n\nShared convolutional layers learn common features, while the final layer has many output units (one per task).\n\nBenefits:\n\nEfficient use of data and compute\n\nBetter regularization: related tasks constrain each other\n\nNatural interface for variant effect prediction: you can see how a mutation affects many functional readouts at once",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#beyond-cnns-recurrent-networks-briefly",
    "href": "app-a-dl.html#beyond-cnns-recurrent-networks-briefly",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "A.5 Beyond CNNs: Recurrent Networks (Briefly)",
    "text": "A.5 Beyond CNNs: Recurrent Networks (Briefly)\nBefore Transformers dominated sequence modeling, recurrent neural networks (RNNs)—especially LSTMs and GRUs—were the default architecture for language and time series.\nConceptually:\n\nAn RNN processes a sequence one position at a time.\n\nIt maintains a hidden state that is updated as it moves along the sequence.\n\nIn principle, it can capture arbitrarily long-range dependencies.\n\nIn practice, for genomic sequences:\n\nVery long-range dependencies (tens to hundreds of kilobases) are difficult to learn with standard RNNs.\n\nTraining can be slow and unstable on very long sequences.\n\nCNNs and attention-based models have largely displaced RNNs in genomic applications.\n\nYou may still see RNNs in some multi-modal or temporal settings (e.g., modeling longitudinal clinical data), but they are not central to this book’s architectures.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#transformers-and-self-attention",
    "href": "app-a-dl.html#transformers-and-self-attention",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "A.6 Transformers and Self-Attention",
    "text": "A.6 Transformers and Self-Attention\nTransformers, introduced in natural language processing, have become the dominant architecture for sequence modeling. In this book, they underpin protein language models, DNA language models (DNABERT and successors), and long-range models like Enformer (Ji et al. 2021; Avsec et al. 2021).\n\nA.6.1 The Idea of Self-Attention\nIn a self-attention layer, each position in a sequence can directly “look at” and combine information from every other position.\nFor an input sequence represented as vectors \\(\\{x_1, \\dots, x_L\\}\\):\n\nEach position is mapped to query (\\(q_i\\)), key (\\(k_i\\)), and value (\\(v_i\\)) vectors via learned linear projections.\n\nThe attention weight from position \\(i\\) to position \\(j\\) is:\n\\[\n\\alpha_{ij} \\propto \\exp\\left(\\frac{q_i^\\top k_j}{\\sqrt{d}}\\right),\n\\]\nfollowed by normalization so that \\(\\sum_j \\alpha_{ij} = 1\\).\nThe new representation of position \\(i\\) is a weighted sum of all value vectors:\n\\[\nz_i = \\sum_{j=1}^L \\alpha_{ij} v_j.\n\\]\n\nKey properties:\n\nContent-based: Interactions are determined by similarity of representations, not just distance.\n\nGlobal context: Each position can, in principle, attend to any other position.\n\nPermutation-aware via positional encodings: Additional information (sinusoidal or learned) encodes position so the model knows order.\n\n\n\nA.6.2 Multi-Head Attention and Transformer Blocks\nReal Transformer layers use multi-head attention:\n\nThe model runs self-attention in parallel with multiple sets of \\((Q,K,V)\\) projections (heads).\n\nDifferent heads can specialize in different patterns (e.g., local motif combinations, long-range enhancer–promoter contacts).\n\nA typical Transformer block has:\n\nMulti-head self-attention\n\nAdd & layer normalization\n\nPosition-wise feed-forward network\n\nAnother add & layer normalization\n\nStacking many blocks yields a deep Transformer.\n\n\nA.6.3 Computational Cost and Long-Range Genomics\nNaive self-attention has \\(O(L^2)\\) cost in sequence length \\(L\\). For genomic sequences, where we might want 100 kb–1 Mb contexts, this is expensive.\nLong-range genomic models like Enformer and HyenaDNA address this with:\n\nHybrid designs (CNNs + attention) to reduce sequence length before applying global attention (Avsec et al. 2021).\n\nStructured state space models (SSMs) and related architectures that scale more gracefully with length (Nguyen et al. 2023).\n\nThese details are treated in depth in the long-range modeling chapters; here it suffices to know that Transformers give flexible global context at the cost of higher computational complexity.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#self-supervised-learning-and-pretraining",
    "href": "app-a-dl.html#self-supervised-learning-and-pretraining",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "A.7 Self-Supervised Learning and Pretraining",
    "text": "A.7 Self-Supervised Learning and Pretraining\nA central theme of this book is pretraining: training a large model once on a broad, unlabeled or weakly-labeled task, then re-using it for many downstream problems.\n\nA.7.1 Supervised vs Self-Supervised\n\nSupervised learning: Each input \\(x\\) comes with a label \\(y\\). Examples:\n\nPredicting chromatin marks from sequence (DeepSEA).\n\nPredicting splice junctions (SpliceAI).\n\nPredicting disease risk from features (Chapter 18).\n\nSelf-supervised learning: The model learns from raw input data without explicit labels, using some pretext task constructed from the data itself. Examples:\n\nMasked token prediction (BERT-style): hide some nucleotides and train the model to predict them from surrounding context.\n\nNext-token prediction (GPT-style): predict the next base given previous ones.\n\nDenoising or reconstruction tasks.\n\n\nIn genomics, self-supervised models treat DNA sequences as a language and learn from the vast amount of genomic sequence without needing curated labels.\n\n\nA.7.2 Masked Language Modeling on DNA\nDNABERT applied BERT-style masked language modeling to DNA sequences tokenized as overlapping k-mers (Ji et al. 2021). The model:\n\nReads sequences as k-mer tokens.\n\nRandomly masks a subset of tokens.\n\nLearns to predict the masked tokens given surrounding context.\n\nBenefits:\n\nUses essentially unlimited unlabeled genomic data.\n\nLearns rich representations that can be fine-tuned for tasks like promoter prediction, splice site detection, and variant effect prediction.\n\nChapter 10 generalizes this story to broader DNA foundation models, including alternative tokenization schemes and architectures.\n\n\nA.7.3 Pretraining, Fine-Tuning, and Probing\nAfter pretraining, we can use a model in several ways:\n\nFine-tuning: Initialize with pretrained weights, then continue training on a specific downstream task with task-specific labels.\n\nLinear probing: Freeze the pretrained model, extract embeddings, and train a simple linear classifier on top.\n\nPrompting / adapters: Add small task-specific modules (adapters) while keeping most of the model fixed.\n\nThese patterns reappear across protein LMs, DNA LMs, variant effect models, and GFMs in Chapters 9–16.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#foundations-for-evaluation-and-reliability",
    "href": "app-a-dl.html#foundations-for-evaluation-and-reliability",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "A.8 Foundations for Evaluation and Reliability",
    "text": "A.8 Foundations for Evaluation and Reliability\nWhile the main book has dedicated chapters for evaluation (Chapter 15), confounding (Chapter 16), and clinical metrics (Chapter 18), it’s useful to have a few basic concepts in mind.\n\nA.8.1 Distribution Shift\nA model is trained under some data distribution (e.g., certain assays, cohorts, ancestries) and then deployed under another (e.g., a different hospital system or population). When these differ, we have distribution shift, which can degrade performance.\nTypical genomic shifts include:\n\nNew sequencing technologies or lab protocols\n\nNew ancestries or populations\n\nNew tissues, diseases, or phenotypes\n\n\n\nA.8.2 Data Leakage\nData leakage occurs when information from the test set “leaks” into training (e.g., through overlapping loci or related individuals), leading to overly optimistic estimates of performance. Chapter 15 and Chapter 16 discuss strategies for leak-resistant splits in detail.\n\n\nA.8.3 Calibration and Uncertainty\nFor many applications, especially in the clinic, we care not just about whether the model is correct, but whether its probabilities are well calibrated and whether we know when the model is uncertain. Calibration and uncertainty quantification are covered in Chapter 18; here, the main takeaway is that perfect AUROC does not imply perfect clinical utility.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#a-minimal-recipe-for-a-genomic-deep-learning-project",
    "href": "app-a-dl.html#a-minimal-recipe-for-a-genomic-deep-learning-project",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "A.9 A Minimal Recipe for a Genomic Deep Learning Project",
    "text": "A.9 A Minimal Recipe for a Genomic Deep Learning Project\nTo make the abstractions more concrete, here is a lightweight “recipe” that roughly mirrors what the case-study chapters do.\n\nDefine the prediction problem\n\nInput: e.g., 1 kb sequence around a variant, or patient-level features.\n\nOutput: e.g., presence of a chromatin mark, change in expression, disease risk.\n\nChoose an input representation\n\nOne-hot encoding or tokenization scheme for sequences (see Chapter 8).\n\nEncodings for variants, genes, or patients (e.g., aggregate from per-variant features).\n\nPick a model family\n\nCNN for local sequence-to-function (Chapters 5–7).\n\nTransformer or SSM for long-range or language model-style tasks (Chapters 8–11).\n\nPretrained GFM + small task-specific head (Chapters 12–16).\n\nSpecify the loss and metrics\n\nCross-entropy for binary classification, MSE for regression, etc.\n\nMetrics like AUROC, AUPRC, correlation, calibration.\n\nSet up data splits and evaluation\n\nDecide whether to split by locus, individual, cohort, or species.\n\nHold out a test set and use validation data to tune hyperparameters.\n\nTrain with regularization and monitoring\n\nUse an optimizer (SGD or Adam-like) with a learning rate schedule.\n\nApply regularization (dropout, weight decay, augmentation).\n\nMonitor training and validation curves for overfitting.\n\nInspect and stress-test\n\nCheck performance across subgroups (e.g., ancestries, assays, cohorts).\n\nUse interpretability tools (Chapter 17) to see what patterns the model is using.\n\nRun robustness checks and ablations.\n\nIterate\n\nAdjust architecture, add more data, refine labels, or incorporate pretrained backbones.\n\nMove from model-centric tuning to system-level considerations (data quality, deployment environment, feedback loops).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#how-this-primer-connects-to-the-rest-of-the-book",
    "href": "app-a-dl.html#how-this-primer-connects-to-the-rest-of-the-book",
    "title": "Appendix A — Deep Learning Primer for Genomics",
    "section": "A.10 How This Primer Connects to the Rest of the Book",
    "text": "A.10 How This Primer Connects to the Rest of the Book\nThis appendix gives you the minimum vocabulary to navigate the rest of the text:\n\nChapters 5–7 show how CNNs on one-hot sequence learn regulatory code, expression, and splicing.\n\nChapters 8–11 extend these ideas to richer sequence representations, Transformers, and long-range sequence models.\n\nChapters 12–16 frame these models as genomic foundation models, introduce evaluation, interpretability, and multi-omics.\n\nChapters 17–19 show how these ingredients are assembled into clinical, discovery, and biotech applications.\n\nYou don’t need to internalize every detail here. The goal is simply that when you see terms like “convolution,” “attention,” “pretraining,” or “fine-tuning” in the main chapters, they feel like familiar tools rather than mysterious jargon.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer for Genomics</span>"
    ]
  },
  {
    "objectID": "app-b-resources.html",
    "href": "app-b-resources.html",
    "title": "Appendix B — Additional Resources",
    "section": "",
    "text": "B.1 Genomics & Human Genetics",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "app-b-resources.html#genomics-human-genetics",
    "href": "app-b-resources.html#genomics-human-genetics",
    "title": "Appendix B — Additional Resources",
    "section": "",
    "text": "Thompson & Thompson Genetics and Genomics in Medicine (9th ed.)\nRonald Cohn, Stephen Scherer, Ada Hamosh. Clinical-focused overview of human genetics and genomics for medicine, great for grounding in clinical genomics.\nHuman Molecular Genetics (5th ed.)\nTom Strachan, Andrew Read. Higher-level molecular genetics/genomics text with strong coverage of mechanisms, technologies, and disease applications.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "app-b-resources.html#immunology",
    "href": "app-b-resources.html#immunology",
    "title": "Appendix B — Additional Resources",
    "section": "B.2 Immunology",
    "text": "B.2 Immunology\n\nJaneway’s Immunobiology (10th ed.)\nKenneth M. Murphy, Casey Weaver, Leslie J. Berg. Standard comprehensive immunology textbook, excellent for understanding immune system biology relevant to genomics and disease.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "app-b-resources.html#machine-learning-deep-learning",
    "href": "app-b-resources.html#machine-learning-deep-learning",
    "title": "Appendix B — Additional Resources",
    "section": "B.3 Machine Learning & Deep Learning",
    "text": "B.3 Machine Learning & Deep Learning\n\nDeep Learning\nIan Goodfellow, Yoshua Bengio, Aaron Courville. Comprehensive deep learning textbook; free online: https://www.deeplearningbook.org/\nDive into Deep Learning (D2L)\nAston Zhang et al. Interactive deep learning book with Jupyter notebooks and multi-framework code; free online: https://d2l.ai/\nAn Introduction to Statistical Learning (ISLR, 2nd ed.)\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Gentle introduction to statistical learning methods used in ML, available free online: https://www.statlearning.com/\nThe Elements of Statistical Learning (ESL)\nTrevor Hastie, Robert Tibshirani, Jerome Friedman. More advanced, theory-heavy companion to ISLR; free PDF: https://hastie.su.domains/ElemStatLearn/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html",
    "href": "app-c-glossary.html",
    "title": "Appendix C — Glossary",
    "section": "",
    "text": "C.1 CH 01",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-01",
    "href": "app-c-glossary.html#ch-01",
    "title": "Appendix C — Glossary",
    "section": "",
    "text": "C.1.1 Sequencing Technologies & Data\n\nNext-generation sequencing (NGS)\nHigh-throughput DNA sequencing technologies that allow rapid…stretches of DNA, producing millions of short reads in parallel….\n\n\nIllumina sequencing\nA widely used NGS technology that utilizes reversible dye-terminators to sequence DNA by synthesis\n\n\nShort reads / Paired-end reads\nDNA sequences generated by NGS technologies, typically rangi… a DNA fragment, providing additional information for alignment.\n\n\nLong-read sequencing (PacBio HiFi, Oxford Nanopore)\nDNA sequencing technologies that produce longer reads, typic…ases, allowing for better resolution of complex genomic regions.\n\n\nCircular consensus sequencing\nA sequencing method used by PacBio to generate highly accura… long reads by repeatedly sequencing the same DNA molecule.\n\n\nBase calling\nThe process of determining the nucleotide sequence from raw sequencing data.\n\n\nFASTQ\nA file format that stores both nucleotide sequences and their corresponding quality scores.\n\n\nRead depth / Coverage (e.g., 30×, 100×)\nThe number of times a particular nucleotide is sequenced, indicating the reliability of the sequencing data.\n\n\n\nC.1.2 Targeting Strategies\n\nTargeted gene panel\nA sequencing approach that focuses on a specific set of gene…or cost-effective analysis of known disease-associated variants.\n\n\nWhole-exome sequencing (WES)\nA sequencing approach that targets all protein-coding region…the genome, providing a comprehensive view of coding variants.\n\n\nWhole-genome sequencing (WGS)\nA sequencing approach that captures the entire genome, inclu…ng regions, providing the most comprehensive view of genetic var\n\n\nCapture efficiency\nThe effectiveness of a targeted sequencing approach in enric…interest, impacting the overall quality and coverage of the data.\n\n\n\nC.1.3 Alignment & Processing\n\nRead alignment / Mapping\nThe process of aligning sequencing reads to a reference genome to determine their genomic origin.\n\n\nSeed-and-extend alignment\nAn algorithmic approach for read alignment that first identi…ences (seeds) and then extends the alignment around these seeds.\n\n\nPCR duplicates\nIdentical sequencing reads that originate from the same DNA …esulting from PCR amplification, which can bias variant calling.\n\n\nBase quality score recalibration (BQSR)\nA process that adjusts the quality scores of sequencing read…or systematic errors made by the sequencer.\n\n\nMapping quality\nA measure of the confidence that a read is correctly aligned to the reference genome.\n\n\nReference bias\nThe tendency for sequencing and alignment processes to preferentially detect alleles present in the reference genome.\n\n\n\nC.1.4 Variant Calling\n\nVariant calling\nThe process of identifying variants from sequencing data by comparing it to a reference genome.\n\n\nGenotype likelihood\nThe probability of observing the sequencing data given a particular genotype.\n\n\nPair-HMM (pair hidden Markov model)\nA statistical model used in variant calling to calculate the likelihood of different alignments between reads and the reference genome.\n\n\nJoint genotyping / Cohort calling\nThe process of simultaneously calling variants across multiple samples to improve accuracy and consistency.\n\n\ngVCF (genomic VCF)\nA variant call format that includes information about both va…sites, allowing for joint genotyping.\n\n\nVCF (variant call format)\nA standardized file format for storing variant information, including SNPs, indels, and structural variants.\n\n\nVQSR (Variant Quality Score Recalibration)\nA method for improving the accuracy of variant calls by mode…lationship between variant quality scores and various annotatio\n\n\nPileup\nA summary of the base calls at each position in a set of align…ing reads, used for variant calling and visualization.\n\n\n\nC.1.5 Phasing\n\nHaplotype phasing\nThe process of determining which variants are inherited together on the same chromosome.\n\n\nRead-backed phasing\nA method of phasing that uses sequencing reads that span multiple variants to determine their phase.\n\n\nStatistical phasing\nA method of phasing that uses population-level genotype data and statistical models to infer haplotypes.\n\n\nCompound heterozygosity\nThe presence of two different variants at a particular gene locus, one on each chromosome of a pair.\n\n\nCis vs. trans configuration\nDescribes the relative arrangement of two variants on the same chromosome (cis) or on different chromosomes (trans).\n\n\n\nC.1.6 Variant Types\n\nSNV (single nucleotide variant)\nA variation in a single nucleotide that occurs at a specific position in the genome.\n\n\nIndel\nAn insertion or deletion of bases in the genome of an organism.\n\n\nStructural variant\nA large-scale alteration in the genome, such as a deletion, duplication, inversion, or translocation.\n\n\nMulti-nucleotide variant (MNV)\nA variation that affects multiple consecutive nucleotides in the genome.\n\n\nMosaic variant\nA genetic variant that is present in some but not all cells of an organism, often arising during development.\n\n\nSomatic variant\nA genetic variant that occurs in non-germline cells and is not inherited, often associated with cancer.\n\n\nGermline variant\nA genetic variant that is present in the egg or sperm and can be passed on to offspring.\n\n\nDe novo variant\nA genetic variant that arises spontaneously in an individual and is not inherited from either parent.\n\n\n\nC.1.7 Difficult Regions\n\nSegmental duplication\nLarge, highly similar sequences in the genome that can complicate read alignment and variant calling.\n\n\nParalog / Paralogous gene\nA gene that is related to another gene in the same organism due to a duplication event.\n\n\nHomopolymer\nA sequence of identical nucleotides in a row, which can be prone to sequencing errors.\n\n\nLow-complexity region\nA region of the genome with a simple sequence composition, which can be challenging for alignment and variant calling.\n\n\nHLA region / MHC\nThe human leukocyte antigen (HLA) region, also known as the major histocompatibility complex (MHC), is a highly polymorphic region involved in immune response.\n\n\n\nC.1.8 Benchmarking\n\nPrecision (positive predictive value)\nThe proportion of true positive variant calls among all positive calls.\n\n\nRecall (sensitivity)\nThe proportion of true positive variant calls detected among all actual variants.\n\n\nF1 score\nThe harmonic mean of precision and recall, providing a single metric for evaluating variant calling performance.\n\n\nTrue positive (TP) / False positive (FP) / False negative (FN)\nMetrics used to evaluate the accuracy of variant calls, where TP represents correctly identified variants, FP represents incorrectly identified variants, and FN represents missed variants.\n\n\nHigh-confidence region\nA region of the genome where variant calls are considered to b… reliable, often used for benchmarking and validation.\n\n\n\nC.1.9 Key Resources/Tools (may warrant brief glossary entries)\n\nGIAB (Genome in a Bottle)\nA consortium that develops reference materials and data for benchmarking genome sequencing and variant calling.\n\n\nDeepVariant\nA deep learning-based variant caller developed by Google that identifies genetic variants from sequencing data.\n\n\nGLnexus\nA tool for joint variant calling across multiple samples, designed to work with DeepVariant outputs.\n\n\nHaplotypeCaller\nA variant caller from the Genome Analysis Toolkit (GATK) that uses local de-novo assembly of haplotypes to call variants.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-02",
    "href": "app-c-glossary.html#ch-02",
    "title": "Appendix C — Glossary",
    "section": "C.2 CH 02",
    "text": "C.2 CH 02\n\nC.2.1 Reference & Coordinate Systems\n\nReference genome/assembly\nA digital nucleic acid sequence database, assembled as a repre…example of a species’ set of genes. Multiple versions exist.\n\n\nGRCh37\nThe 37th version of the Genome Reference Consortium human genome assembly.\n\n\nGRCh38\nThe 38th version of the Genome Reference Consortium human genome assembly.\n\n\nT2T-CHM13\nThe Telomere-to-Telomere (T2T) CHM13 human genome assembly, r…ting a complete, gapless sequence of a human genome.\n\n\nPangenome reference\nA reference that represents the genetic diversity of a species, rather than a single individual.\n\n\nGene model\nA representation of the structure of a gene, including its exons, introns, and regulatory elements.\n\n\nCanonical transcript\nThe most biologically relevant transcript of a gene, often used as the reference for annotation.\n\n\nAlternative transcript/isoform\nDifferent versions of a transcript produced from the same gene due to alternative splicing or other mechanisms.\n\n\nMANE Select\nMatched Annotation from NCBI and EMBL-EBI (MANE) Select is a …cripts that are consistently annotated across databases.\n\n\n\nC.2.2 Variant Types & Properties\n\nAllele frequency\nThe proportion of a specific allele among all alleles of a gene in a population.\n\n\nMAF (minor allele frequency)\nThe frequency at which the less common allele occurs in a given population.\n\n\nrsID\nA unique identifier assigned to a single nucleotide polymorphism (SNP) in the dbSNP database.\n\n\nLoss-of-function (LoF) variant\nA genetic variant that results in reduced or abolished protein function.\n\n\nUltra-rare variant\nA genetic variant that is extremely uncommon in the population, often with a frequency of less than 0.01%.\n\n\n\nC.2.3 Population Genetics Metrics\n\nLinkage disequilibrium\nA non-random association of alleles at different loci in a given population.\n\n\npLI (probability of being loss-of-function intolerant)\nA metric that estimates the likelihood that a gene is intolerant to loss-of-function variants.\n\n\nLOEUF (loss-of-function observed/expected upper bound fraction)\nA metric that quantifies the observed versus expected number of loss-of-function variants in a gene.\n\n\nConstraint metrics\nMetrics that assess the tolerance of a gene to functional genetic variation.\n\n\nImputation\nThe process of inferring unobserved genotypes in a study sample based on observed genotypes and a reference panel.\n\n\n\nC.2.4 Functional Genomics\n\nChIP-seq\nChromatin Immunoprecipitation followed by sequencing, a method used to analyze protein-DNA interactions.\n\n\nDNase-seq\nA method to identify regions of open chromatin by sequencing DNA fragments generated by DNase I digestion.\n\n\nATAC-seq\nAssay for Transposase-Accessible Chromatin using sequencing, a technique to study chromatin accessibility.\n\n\nHi-C\nA method to study the three-dimensional architecture of genomes by capturing chromatin interactions.\n\n\nChromatin accessibility\nThe degree to which DNA is exposed and available for binding by proteins, often assessed by DNase-seq or ATAC-seq.\n\n\nHistone modification\nChemical modifications to histone proteins that can influence chromatin structure and gene expression.\n\n\nPeak calling\nThe process of identifying regions of the genome with signific…ment of sequencing reads, often used in ChIP-seq and ATAC-seq analyses.\n\n\nSignal track\nA graphical representation of sequencing data across the genom… intensity of signals such as read coverage or enrichment.\n\n\n\nC.2.5 Expression Genetics\n\neQTL (expression quantitative trait locus)\nA genomic locus that explains variation in gene expression levels.\n\n\nSplicing QTL\nA genomic locus that affects the splicing of pre-mRNA.\n\n\nMolecular QTL\nA quantitative trait locus that influences molecular traits such as gene expression, protein levels, or metabolite concentrations.\n\n\nCis-regulatory\nReferring to regulatory elements, such as promoters or enhanc…ated on the same molecule of DNA as the gene they regulate.\n\n\nColocalization\nThe occurrence of two or more genetic signals at the same genomic location, suggesting a shared causal variant.\n\n\nDropout (single-cell context)\nThe failure to detect a transcript in a single-cell RNA-seq ex…often due to low mRNA capture efficiency.\n\n\n\nC.2.6 Clinical Interpretation\n\nACMG/AMP criteria\nA set of guidelines developed by the American College of Medic…ogy (AMP) for the interpretation of sequence variants. These c…vide a framework for classifying variants into categories such path\n\n\nPathogenicity\nThe ability of a genetic variant to cause disease.\n\n\nHaploinsufficiency\nA condition in which a single functional copy of a gene is in…maintain normal function, leading to a disease phenotype.\n\n\nTriplosensitivity\nA condition in which an extra copy of a gene leads to a disease phenotype.\n\n\nGene-disease validity\nThe strength of evidence supporting a relationship between a gene and a disease.\n\n\nPharmacogenomics\nThe study of how genetic variation affects an individual’s response to drugs.\n\n\nDiplotype\nThe combination of alleles at multiple loci on a single chromosome that are inherited together.\n\n\n\nC.2.7 Study Designs & Statistics\n\nGWAS summary statistics\nAggregated data from genome-wide association studies, typicall…ormation on the association between genetic variants and traits across the genome.\n\n\nFine-mapping\nThe process of identifying the specific causal variants within…omic region associated with a trait.\n\n\nEffect size\nA measure of the strength of the relationship between a genetic variant and a trait.\n\n\nAscertainment bias\nA systematic distortion in the estimation of genetic effects d…non-random sampling of individuals or variants.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-03",
    "href": "app-c-glossary.html#ch-03",
    "title": "Appendix C — Glossary",
    "section": "C.3 CH 03",
    "text": "C.3 CH 03",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-04",
    "href": "app-c-glossary.html#ch-04",
    "title": "Appendix C — Glossary",
    "section": "C.4 CH 04",
    "text": "C.4 CH 04",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-05",
    "href": "app-c-glossary.html#ch-05",
    "title": "Appendix C — Glossary",
    "section": "C.5 CH 05",
    "text": "C.5 CH 05",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-06",
    "href": "app-c-glossary.html#ch-06",
    "title": "Appendix C — Glossary",
    "section": "C.6 CH 06",
    "text": "C.6 CH 06",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-07",
    "href": "app-c-glossary.html#ch-07",
    "title": "Appendix C — Glossary",
    "section": "C.7 CH 07",
    "text": "C.7 CH 07",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-08",
    "href": "app-c-glossary.html#ch-08",
    "title": "Appendix C — Glossary",
    "section": "C.8 CH 08",
    "text": "C.8 CH 08",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-09",
    "href": "app-c-glossary.html#ch-09",
    "title": "Appendix C — Glossary",
    "section": "C.9 CH 09",
    "text": "C.9 CH 09",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-10",
    "href": "app-c-glossary.html#ch-10",
    "title": "Appendix C — Glossary",
    "section": "C.10 CH 10",
    "text": "C.10 CH 10",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-11",
    "href": "app-c-glossary.html#ch-11",
    "title": "Appendix C — Glossary",
    "section": "C.11 CH 11",
    "text": "C.11 CH 11",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-12",
    "href": "app-c-glossary.html#ch-12",
    "title": "Appendix C — Glossary",
    "section": "C.12 CH 12",
    "text": "C.12 CH 12",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-13",
    "href": "app-c-glossary.html#ch-13",
    "title": "Appendix C — Glossary",
    "section": "C.13 CH 13",
    "text": "C.13 CH 13",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-14",
    "href": "app-c-glossary.html#ch-14",
    "title": "Appendix C — Glossary",
    "section": "C.14 CH 14",
    "text": "C.14 CH 14",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-15",
    "href": "app-c-glossary.html#ch-15",
    "title": "Appendix C — Glossary",
    "section": "C.15 CH 15",
    "text": "C.15 CH 15",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-16",
    "href": "app-c-glossary.html#ch-16",
    "title": "Appendix C — Glossary",
    "section": "C.16 CH 16",
    "text": "C.16 CH 16",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-17",
    "href": "app-c-glossary.html#ch-17",
    "title": "Appendix C — Glossary",
    "section": "C.17 CH 17",
    "text": "C.17 CH 17",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-18",
    "href": "app-c-glossary.html#ch-18",
    "title": "Appendix C — Glossary",
    "section": "C.18 CH 18",
    "text": "C.18 CH 18",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-19",
    "href": "app-c-glossary.html#ch-19",
    "title": "Appendix C — Glossary",
    "section": "C.19 CH 19",
    "text": "C.19 CH 19",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#ch-20",
    "href": "app-c-glossary.html#ch-20",
    "title": "Appendix C — Glossary",
    "section": "C.20 CH 20",
    "text": "C.20 CH 20",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#apx-a",
    "href": "app-c-glossary.html#apx-a",
    "title": "Appendix C — Glossary",
    "section": "C.21 APX A",
    "text": "C.21 APX A",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-c-glossary.html#apx-b",
    "href": "app-c-glossary.html#apx-b",
    "title": "Appendix C — Glossary",
    "section": "C.22 APX B",
    "text": "C.22 APX B",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "p1-ch04-cadd.html#the-variant-prioritization-challenge",
    "href": "p1-ch04-cadd.html#the-variant-prioritization-challenge",
    "title": "4  - condense PLM and CNN sections?",
    "section": "5.1 The Variant Prioritization Challenge",
    "text": "5.1 The Variant Prioritization Challenge\nA typical human genome contains approximately four to five million genetic variants relative to the reference assembly. The vast majority of these are functionally neutral, representing the accumulated diversity of human evolution and population history. For any individual with a suspected genetic condition, the central interpretive challenge is to identify the handful of variants that plausibly contribute to disease from this enormous background of benign variation.\nThe data resources surveyed in Chapter 2 provide multiple complementary views of variant function, each with distinct strengths and limitations. Population frequency databases such as gnomAD reveal which variants survive in large cohorts of ostensibly healthy individuals, offering a powerful filter for identifying rare, potentially deleterious alleles (“The Genome Aggregation Database (gnomAD)” n.d.). Functional genomics consortia including ENCODE and the Roadmap Epigenomics Project indicate which genomic regions show evidence of biochemical activity across diverse cell types and developmental contexts. Clinical databases such as ClinVar and HGMD collect expert-curated variant classifications drawn from case reports and diagnostic laboratories, providing ground truth labels for known pathogenic and benign variants.\nEach of these sources is partial in important ways. Population databases are dominated by common variants, which are mostly tolerated by virtue of their high frequency. Functional genomics data is inherently noisy and often context-specific: a region active in liver hepatocytes may be quiescent in neurons, and vice versa. Clinical databases are sparse and heavily biased toward well-studied genes and variant types, leaving vast swaths of the genome without reliable clinical annotations. Moreover, the annotation density varies dramatically across the genome: protein-coding exons are densely labeled relative to deep intronic and intergenic sequences.\nBefore deep learning, variant effect predictors typically tackled this problem by focusing on one narrow signal. Conservation-based methods such as phyloP and GERP score each position according to its evolutionary constraint across multi-species alignments, under the logic that positions conserved over hundreds of millions of years are likely functionally important (Siepel et al. 2005; Davydov et al. 2010). Protein-level tools such as SIFT and PolyPhen predict the impact of amino acid substitutions based on sequence homology, physicochemical properties, and structural features (Ng and Henikoff 2003; Adzhubei et al. 2010). Positional annotations capture simple features like distance to splice sites or proximity to known regulatory elements. Each of these approaches captures a real biological signal, but each is also incomplete: conservation scores miss recently evolved functional elements, protein-level tools are blind to non-coding variants, and positional annotations lack the resolution to distinguish causal variants from linked neutral neighbors.\nCombined Annotation-Dependent Depletion (CADD) represented a fundamental shift in this landscape (Rentzsch et al. 2019). Rather than relying on a single predictive signal, CADD defined a general framework for genome-wide variant prioritization that integrates dozens of heterogeneous annotations and uses evolutionary depletion as a proxy training label. The key insight was to avoid training directly on small sets of known pathogenic versus benign variants, which are scarce and biased toward certain genes and variant types. Instead, CADD contrasts variants that have survived purifying selection in the human lineage with matched simulated variants that could have occurred but did not. This evolutionary proxy strategy yields an enormous training set, enables genome-wide coverage, and produces scores that generalize across coding and non-coding regions alike.\nThis chapter focuses on the CADD framework because it establishes design patterns that recur throughout the deep learning models covered in subsequent chapters: proxy labels derived from evolutionary signals, large-scale training on millions of examples, integration of diverse features into unified scores, and genome-wide precomputation for downstream reuse.",
    "crumbs": [
      "Part I: Data & Pre-DL Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>- condense PLM and CNN sections?</span>"
    ]
  },
  {
    "objectID": "p2-ch05-reg.html#learning-regulatory-code-from-sequence",
    "href": "p2-ch05-reg.html#learning-regulatory-code-from-sequence",
    "title": "5  Regulatory Prediction",
    "section": "5.2 Learning Regulatory Code from Sequence",
    "text": "5.2 Learning Regulatory Code from Sequence\nDeepSEA’s central insight was that deep convolutional networks could learn the sequence patterns underlying regulatory activity without explicit feature engineering. This represented a departure from earlier computational approaches to regulatory genomics, which typically required defining sequence features a priori. Methods like gapped k-mer SVMs (gkm-SVM) required specifying which k-mers to count and how to weight them. Position weight matrices for transcription factor binding sites required curating motif databases like JASPAR or TRANSFAC. These approaches worked, but they encoded human assumptions about what sequence features mattered and could not easily discover novel patterns or complex dependencies.\nDeepSEA instead learned relevant sequence features automatically from data. The convolutional layers of the network function analogously to motif scanners, detecting local sequence patterns that correlate with regulatory activity. But unlike predefined motif scanners, these filters are learned during training, allowing the network to discover whatever patterns best predict the training labels. Deeper layers in the network can then learn combinations of these patterns, capturing regulatory “grammar” such as motif spacing, orientation preferences, and cooperative binding arrangements. The network does not know in advance which patterns matter; it learns them by optimizing predictions on hundreds of thousands of genomic sequences with experimentally measured chromatin profiles.\n\n5.2.1 Architecture\nThe original DeepSEA architecture was deliberately simple by modern standards, comprising a stack of convolutional layers followed by fully connected layers that integrate information across the sequence.\nThe input to the network is a 1000 bp DNA sequence, one-hot encoded into a binary matrix with four channels (one per nucleotide) and 1000 positions. This representation treats sequence as a signal to be processed by convolution, analogous to how image recognition networks process pixel values. Each position in the sequence is represented by exactly one active channel, encoding which nucleotide (A, C, G, or T) is present.\nThe network processes this input through three convolutional layers, each followed by ReLU activation and max pooling. The first convolutional layer uses 320 filters of width 8, scanning the sequence for local patterns roughly the size of transcription factor binding sites. Max pooling after each convolution reduces the spatial dimension, progressively compressing the 1000-position input into a more compact representation. The second and third convolutional layers use 480 and 960 filters respectively, with narrower widths (8 and 8) applied to the already-pooled representation. These deeper layers can learn combinations of the patterns detected by earlier layers, building increasingly abstract representations of sequence features.\nAfter the convolutional stack, a fully connected layer with 925 units integrates information across all positions in the compressed representation. This layer allows the network to learn relationships between sequence features at different positions, capturing spatial dependencies that pure convolution cannot represent. Finally, an output layer with 919 sigmoid units produces independent probability predictions for each chromatin profile.\nThe total number of parameters is modest by contemporary standards, approximately 60 million, but was substantial for genomics applications at the time. Training used stochastic gradient descent with momentum on sequences sampled from the human genome, with chromosome 8 held out for testing.\n\n\n5.2.2 Training Data\nDeepSEA was trained on 919 chromatin profiles compiled from ENCODE and Roadmap Epigenomics, two consortium efforts that had systematically mapped the epigenomic landscape across diverse human cell types and tissues (encode_integrated_2012?; roadmap_integrative_2015?). These profiles represented three major categories of regulatory annotation.\nTranscription factor binding profiles, numbering 690 in total, captured the genomic locations where specific proteins bind DNA. These were derived from ChIP-seq experiments targeting factors like CTCF (a ubiquitous insulator protein), p53 (a tumor suppressor), and GATA1 (a hematopoietic transcription factor). Each profile represents a binary classification problem: for a given sequence, is the central region bound by this factor in this cell type?\nHistone modification profiles, numbering 104, captured the locations of specific chemical modifications to histone proteins. Marks like H3K4me3 (trimethylation of lysine 4 on histone H3) are associated with active promoters, while H3K27ac (acetylation of lysine 27) marks active enhancers. H3K27me3 marks repressed regions through Polycomb-mediated silencing. These modifications do not directly encode regulatory logic but reflect the functional state of chromatin and correlate with gene expression.\nDNase I hypersensitivity profiles, numbering 125, captured regions of open chromatin across cell types. DNase hypersensitive sites mark regions where DNA is accessible to regulatory proteins, identifying potential regulatory elements regardless of which specific factors bind there. Unlike transcription factor ChIP-seq, DNase-seq provides a relatively unbiased view of regulatory potential.\nFor each 1000 bp input sequence, the model predicts the probability that the central 200 bp region exhibits each of these 919 chromatin features. The narrower prediction window relative to the input window allows the network to use flanking sequence as context for predicting the central region’s activity. Training used sequences sampled from the human genome, excluding chromosome 8 which was reserved for evaluation. This chromosome-level holdout prevents overfitting to sequence homology or LD patterns that might leak between training and test sets.\n\n\n5.2.3 Multi-Task Learning\nA key architectural decision was predicting all 919 features simultaneously rather than training separate models for each. This multi-task learning approach offers several advantages that compound as the number of tasks increases.\nShared representations in early layers benefit all tasks. The first convolutional layer learns general sequence features such as GC content, dinucleotide frequencies, and common motifs that are useful across many prediction problems. By sharing these representations, the network amortizes the cost of learning basic sequence features across all tasks rather than relearning them independently.\nJoint prediction provides regularization. Predicting many correlated features simultaneously prevents overfitting to any single task. If a convolutional filter becomes overly specific to one transcription factor, it will harm predictions for other related factors, providing a pressure toward learning generalizable representations. This implicit regularization is particularly valuable when some tasks have limited training data.\nEfficiency gains are substantial. One model serving all 919 prediction tasks requires far less computation than training and maintaining 919 separate models. This matters not only for initial training but for deployment, where a single forward pass produces all predictions.\nThe multi-task framework also reveals relationships between chromatin features. Weights connecting shared representations to different output tasks can be analyzed to understand which features rely on similar sequence patterns. This provides a form of interpretability that separate models would not offer.",
    "crumbs": [
      "Part II: CNN Seq-to-Function Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regulatory Prediction</span>"
    ]
  }
]