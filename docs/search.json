[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Genomic Foundation Models",
    "section": "",
    "text": "Introduction\nEvery human cell contains the same three billion nucleotides, yet that identical sequence produces over two hundred distinct cell types, each with characteristic patterns of gene expression, chromatin accessibility, and regulatory state. The instructions for this differentiation are written in the genome itself: in enhancers and silencers distributed across hundreds of megabases, in splice sites that determine which exons join to form mature transcripts, in three-dimensional chromatin contacts that bring distant regulatory elements together. A typical genome harbors roughly three million variants distinguishing it from the reference, but clinicians can confidently interpret fewer than a hundred. The gap between reading sequence and understanding function remains one of the central unsolved problems in biology, and it is precisely the gap that genomic foundation models aim to close.\nThe interpretive bottleneck is not simply a matter of missing labels or insufficient training data. The human genome encodes regulatory logic distributed across millions of base pairs, with enhancers acting on genes tens of kilobases away, chromatin structure gating access to transcription machinery, and splicing decisions depending on sequence contexts that span entire introns. Classical approaches attacked these problems piecemeal: one model for splice sites, another for transcription factor binding, a third for variant pathogenicity. Each required hand-crafted features, curated training sets, and careful validation within a narrow domain. The result was a fragmented landscape where transferring insights from one task to another demanded starting nearly from scratch.\nThe foundation model paradigm offers a different approach. Rather than training specialized models for each downstream task, foundation models learn general-purpose representations from massive, heterogeneous datasets using self-supervised objectives such as masked-token prediction or next-token modeling (Bommasani et al. 2022). These representations can then be adapted to new problems through fine-tuning, probing, or zero-shot inference. In natural language processing and computer vision, this paradigm has proven transformative. In genomics, the question is whether similar approaches can bridge the gap between sequence and function, connecting raw nucleotides to molecular phenotypes, disease risk, and therapeutic targets (Guo et al. 2025).\nThis book examines that intersection. Genomic foundation models (GFMs) are large, reusable models trained on genomic and related biological data that can be adapted to many downstream tasks. The term encompasses DNA language models trained on reference genomes and metagenomes, protein language models that learn evolutionary constraints from sequence databases, RNA structure predictors, and multi-omic architectures that integrate across data modalities. What unites them is the core insight that pretraining on broad biological data produces representations that transfer to specialized tasks more effectively than training narrow models from scratch.\nThe book does not offer a general introduction to genomics or machine learning. Readers seeking comprehensive coverage of molecular biology should consult Molecular Biology of the Cell; those wanting deep learning fundamentals will find better treatments in dedicated textbooks. The goal here is narrower and more specific: to provide a conceptual and practical map of how modern deep models for DNA, RNA, and proteins are built, what they actually learn, and how they can be used responsibly in research and clinical workflows. The chapters connect classical genomics pipelines, early deep regulatory models, sequence language models, and multi-omic GFMs into a single narrative arc that emphasizes both capabilities and limitations.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-foundation-models-for-genomics",
    "href": "index.html#why-foundation-models-for-genomics",
    "title": "Genomic Foundation Models",
    "section": "Why Foundation Models for Genomics?",
    "text": "Why Foundation Models for Genomics?\nTraditional genomic modeling has been overwhelmingly task-specific. A variant caller is tuned to distinguish sequencing errors from true variants in a particular sequencing platform and sample type. A supervised convolutional network predicts a fixed set of chromatin marks for a specific cell line. A polygenic risk score is fit for one trait, in one ancestry group, using data from one biobank. These models can achieve excellent performance in the settings they were designed for, but they often transfer poorly to new assays, tissues, ancestries, or institutions. When the input distribution shifts, whether because of a new sequencing chemistry, a different population, or a novel cell type, performance degrades in ways that are difficult to anticipate.\nFoundation models address this fragility through three interrelated strategies. First, they leverage scale: training on massive, heterogeneous datasets spanning multiple assays, tissues, species, and cohorts forces the model to learn representations that capture shared biological structure rather than dataset-specific artifacts. Second, they employ self-supervised objectives that do not require manual labels, allowing them to exploit the vast quantities of unlabeled sequence data, perturbation screens, and population variation that genomics generates. Third, they are designed for reusability: rather than training a new model for each task, practitioners probe, adapt, or fine-tune a shared backbone, amortizing the cost of representation learning across many downstream applications.\nThe extent to which this paradigm delivers on its promises in genomics remains an active research question. Some tasks benefit dramatically from pretrained representations; others show marginal improvement over strong classical baselines. Transfer across species, cell types, and assays works better in some settings than others. The computational costs of training and deploying large models create practical constraints that vary across research and clinical environments. This book does not assume that foundation models are the answer to every genomic problem. It aims instead to equip readers with the frameworks to evaluate when these approaches help, when simpler methods suffice, and how to design analyses that exploit the strengths of modern architectures while remaining alert to their limitations.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#recurring-themes",
    "href": "index.html#recurring-themes",
    "title": "Genomic Foundation Models",
    "section": "Recurring Themes",
    "text": "Recurring Themes\nSeveral threads run through the book, and individual chapters can be read as different perspectives on the same underlying questions.\nThe co-evolution of data and architecture is one such thread. Early variant effect predictors relied on hand-engineered features and shallow models trained on modest curated datasets. Convolutional networks enabled direct learning of regulatory motifs and local grammar from raw sequence, but their fixed receptive fields limited their reach. Transformers and other long-context architectures opened the door to capturing broader regulatory neighborhoods and chromatin structure. Foundation models push toward representations that span multiple assays, tissues, and organisms. At each stage, the question is not simply whether the model is more sophisticated, but how the available data constrain what the model can sensibly learn.\nScaling laws and emergent capabilities represent a related concern. As models grow larger and train on more data, certain capabilities appear discontinuously rather than gradually. The relationship between parameters, training data, and compute follows predictable patterns that inform practical decisions about model development. Understanding these scaling dynamics helps practitioners decide when to train larger models, when existing models suffice, and what capabilities to expect at different scales.\nContext length and genomic geometry form another recurring thread. Many genomic phenomena are intrinsically non-local: enhancers regulate genes across hundreds of kilobases, chromatin loops bring distal elements into contact, and polygenic effects distribute risk across thousands of variants genome-wide. The book returns repeatedly to how models represent these long-range dependencies, what architectural choices enable or constrain their reach, and what is gained or lost as context windows scale.\nThe distinction between prediction and design cuts across multiple chapters. Most current models are used as predictors: given a sequence and context, what molecular or phenotypic outcome is expected? The same models can also be embedded in design workflows, from variant prioritization and library construction to therapeutic sequence optimization. Foundation models change where the boundary lies between analysis and experimental planning, and they introduce new failure modes when generative or optimization objectives are misspecified.\nEvaluation connects benchmark performance to real-world decisions. Benchmark scores are seductive and easy to compare, but biological and clinical decisions are messy, multi-objective, and constrained by data drift, confounding, and poorly specified endpoints. A recurring theme is the gap between state-of-the-art metrics on held-out test sets and actual impact in research or clinical deployment. Careful evaluation, confounder analysis, uncertainty quantification, and calibration can narrow that gap, but only when practitioners understand what their metrics actually measure.\nInterpretability and mechanism form a final thread. The book treats interpretability not as optional decoration but as a design constraint that shapes how models should be built and evaluated. Saliency maps, motif extraction, and mechanistic analyses can deepen understanding of what a model has learned, but they can also provide false comfort when applied to confounded or brittle representations. Distinguishing genuine biological insight from pattern-matching artifacts requires both technical tools and careful experimental design.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-the-book-is-organized",
    "href": "index.html#how-the-book-is-organized",
    "title": "Genomic Foundation Models",
    "section": "How the Book Is Organized",
    "text": "How the Book Is Organized\nThe book is organized into six parts containing twenty-nine chapters, plus six appendices. Each part can be read on its own, but the parts are designed to build on one another.\nPart I: Genomic Foundations lays the genomic and statistical groundwork that later models rest on. 1  Sequencing: From Reads to Variants introduces next-generation sequencing, alignment, and variant calling, highlighting sources of error and the evolution from hand-crafted pipelines to learned variant callers. 2  The Genomic Data Landscape surveys the core data resources that underlie most modern work: reference genomes, population variation catalogs, clinical variant databases, and functional genomics consortia such as ENCODE and GTEx. 3  GWAS and Polygenic Scores reviews genome-wide association studies, linkage disequilibrium, fine-mapping, and polygenic scores, emphasizing what these variant-to-trait associations do and do not tell us about mechanism. 4  Classical Variant Effect Prediction covers conservation-based and machine-learning-based variant effect predictors such as CADD, including their feature sets, label construction, and issues of circularity and dataset bias. Together, these chapters answer a foundational question: what data and pre-deep-learning tools form the backdrop that any genomic foundation model must respect, integrate with, or improve upon?\nPart II: Deep Learning for Sequences introduces the conceptual and technical foundations of modern sequence modeling. 5  Sequence Representation and Tokenization examines how genomic and protein sequences are converted into model-compatible representations, covering one-hot encodings, k-mers, byte-pair encodings, learned embeddings, and position encodings, showing how these choices shape downstream model behavior. 6  Convolutional Models for Genomic Sequence examines convolutional approaches that established the field of genomic deep learning, including DeepSEA, Basset, and SpliceAI, analyzing what they learn about motifs and regulatory grammar and where their fixed receptive fields impose limitations that motivate attention-based architectures. 7  Attention and Transformers provides a detailed treatment of attention mechanisms, position encodings, and transformer architectures, with emphasis on how these ideas translate from language to biological sequence. 8  Pretraining Objectives and Strategies covers pretraining objectives, from masked language modeling and next-token prediction to contrastive and generative approaches, examining how self-supervision extracts structure from unlabeled biological data. 9  Transfer Learning and Adaptation addresses transfer learning, domain adaptation, and few-shot learning, asking when and how pretrained representations generalize to new tasks, species, and data modalities.\nPart III: Foundation Models for Biology surveys the major foundation model families, organized by modality, and establishes variant effect prediction as the integrating application. 10  The Foundation Model Paradigm develops a working definition and taxonomy of foundation models in genomics, distinguishing them from earlier supervised approaches and examining scaling laws that characterize how model capabilities change with size and data. 11  DNA Language Models covers DNA language models such as DNABERT, Nucleotide Transformer, HyenaDNA, and Evo, tracing their training corpora, objectives, evaluation suites, and current capabilities. 12  Protein Language Models describes large protein language models trained on evolutionary sequence databases, their emergent structure and function representations, and applications to structure prediction and design. 13  Long-Context Regulatory Models covers hybrid CNN-transformer and related architectures designed for long genomic contexts, such as Enformer and Borzoi, which predict regulatory readouts over tens to hundreds of kilobases. 14  Variant Effect Prediction with Foundation Models serves as a capstone that integrates these model families, examining how protein-based approaches such as AlphaMissense and DNA-based approaches such as splicing and regulatory models combine to address variant effect prediction, the central interpretive challenge that motivates the field.\nPart IV: Multi-Scale and Systems Modeling examines how foundation model principles extend beyond one-dimensional sequence to embrace cellular and systems-level biology. 15  RNA Models extends beyond splicing to RNA structure prediction and RNA foundation models, examining how secondary structure and functional context inform representation learning. 16  Single-Cell and Epigenomic Models covers foundation models for single-cell transcriptomics and epigenomics, showing how transformer architectures adapt to the unique characteristics of these data types. 17  3D Genome and Spatial Models addresses the three-dimensional organization of the genome, from chromatin loops and TAD boundaries to emerging spatial transcriptomics foundation models, examining how 3D structure provides the missing link between sequence and regulatory function. Revised Chapter: Networks and Graph-Based Reasoning turns to graph neural networks and network-based approaches, framing these not as alternatives to sequence models but as higher-level reasoning systems that consume foundation model embeddings as node features. 19  Multi-Omics Integration broadens the view to multi-omics integration, exploring how models can jointly represent genomic, transcriptomic, proteomic, and clinical information to connect sequence variation to phenotype across multiple layers of biological organization.\nPart V: Evaluation and Reliability develops frameworks for assessing what models actually learn and how reliably they perform. 20  Benchmarks for Genomic AI surveys existing benchmarks for genomic foundation models, analyzing their construction, coverage, and limitations. 21  Evaluation Methodology presents evaluation principles and proper methodology, covering data splitting, metric choice, and the link between benchmark performance and real-world utility. 22  Confounders in Model Training details sources of confounding and data leakage, from batch effects and ancestry structure to label bias and covariate shift, offering practical strategies for detection and mitigation. 23  Uncertainty Quantification addresses uncertainty quantification, examining calibration, epistemic versus aleatoric uncertainty, and practical methods such as ensembles and conformal prediction that help models express when they do not know. 24  Interpretability and Mechanism explores interpretability tools from classical motif discovery and attribution methods to emerging mechanistic approaches, asking when these tools reveal genuine biological mechanisms and when they provide false comfort.\nPart VI: Translation moves from methods to end-to-end workflows in research and clinical practice. ?sec-clinical-risk discusses clinical risk prediction that combines genomic features with electronic health records and environmental data, focusing on discrimination, calibration, fairness, and deployment in health systems. 26  Rare Disease and Variant Interpretation examines how foundation models fit into rare disease and cancer workflows, including variant prioritization pipelines, integration with family and tumor-normal data, and laboratory validation. 27  Drug Discovery and Target Identification looks at how GFMs intersect with target discovery, functional genomics screens, and biomarker development in pharmaceutical and biotechnology settings. 28  Sequence Design and Engineering covers generative applications, from protein design and therapeutic sequence optimization to synthetic biology and bioengineering workflows. 29  Regulatory, Ethical, and Future Considerations concludes with regulatory and ethical considerations, open problems, emerging directions, and considerations for responsible development and deployment of genomic AI systems.\nSix appendices provide supporting material. Appendix A — Deep Learning Primer offers a compact introduction to neural networks, CNNs, transformers, training, and evaluation for readers who want enough machine learning background to engage with the main chapters without consulting external references. Appendix B — Model Deployment covers practical considerations for deploying genomic foundation models, including computational requirements, hardware selection, and infrastructure concerns. Appendix C — Training Data Curation provides guidance on constructing training datasets, covering data sources, quality filtering, deduplication, and contamination detection. Appendix D — Referenced Models provides a comprehensive reference table of models discussed throughout the book, with architecture summaries, training data, and key citations. Appendix E — Additional Resources offers a curated collection of datasets, software tools, courses, and papers for deeper exploration. Appendix F — Glossary defines key terms spanning genomics, machine learning, and clinical applications.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-this-book-aims-to-provide",
    "href": "index.html#what-this-book-aims-to-provide",
    "title": "Genomic Foundation Models",
    "section": "What This Book Aims to Provide",
    "text": "What This Book Aims to Provide\nGenomic foundation models represent a moving target: architectures, datasets, and evaluation suites evolve rapidly. This book is not intended as a frozen survey of the current state of the art but as a framework for reasoning about new models as they appear.\nReaders who work through the material should be able to place a new model in the landscape of data, architecture, objective, and application. They should be equipped to design analyses and experiments that use GFMs as components, whether as feature extractors, priors, or simulators, without overclaiming what the models can do. They should recognize common pitfalls in training, evaluation, and deployment, especially in clinical and translational settings where errors have real consequences. And they should be able to decide where foundation models are genuinely useful and where simpler methods or classical baselines remain sufficient.\nThe next chapter turns to the foundations: how raw reads become variants, how variants become the datasets and benchmarks on which all subsequent models depend, and where the errors in this upstream process create systematic challenges that propagate through everything that follows.\n\n\n\n\nBommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2022. “On the Opportunities and Risks of Foundation Models.” arXiv. https://doi.org/10.48550/arXiv.2108.07258.\n\n\nGuo, Fei, Renchu Guan, Yaohang Li, Qi Liu, Xiaowo Wang, Can Yang, and Jianxin Wang. 2025. “Foundation Models in Bioinformatics.” National Science Review 12 (4): nwaf028. https://doi.org/10.1093/nsr/nwaf028.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why I Wrote This Book\nWorking on genomic foundation models means context-switching constantly: debugging data artifacts one week, reproducing a transformer-based variant effect predictor the next, and arguing about clinical patient cohorts the week after. The knowledge required is scattered across textbooks, methods papers, and tribal folklore - genomics on one shelf, deep learning on another, clinical deployment in someone else’s head entirely.\nThis book is my attempt to put those pieces in one place: to connect the mature, statistically grounded tradition of human genetics with the rapidly changing ecosystem of deep learning and foundation models, and to make that transition legible for people who live in one corner of the triangle and are trying to get oriented to the others.\nI wrote it first for myself and my collaborators: as a way to organize wiki pages, markdown files, and half-finished slide decks into something coherent. Over time it became clear that turning those notes into a book might be useful to others navigating the same landscape.\nWhat I wanted, but could not find, was a conceptual throughline:\nThis book is my best attempt at answering those questions in a way that is historically grounded, technically honest, and practically oriented.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#why-i-wrote-this-book",
    "href": "preface.html#why-i-wrote-this-book",
    "title": "Preface",
    "section": "",
    "text": "How do we get from reads to variants in a way that a deep model can trust?\nHow should we think about polygenic scores, fine-mapping, and functional assays in the era of foundation models?\nWhen we say a model “understands” regulatory grammar or protein function, what does that actually mean?\nAnd what does it take to move from a promising preprint to a tool that can support decisions about real patients?",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#how-this-book-came-together",
    "href": "preface.html#how-this-book-came-together",
    "title": "Preface",
    "section": "How This Book Came Together",
    "text": "How This Book Came Together\nThe structure of the book reflects the way these ideas evolved in my own work.\nEarly sections grew out of teaching and mentoring conversations: explaining next-generation sequencing, variant calling, and pre-deep-learning interpretation methods to new team members who were strong in statistics or ML but new to genomics (and vice versa).\nThe middle sections emerged from a series of “journal club + experiments” cycles, where we:\n\nread papers on sequence-to-function CNNs, protein language models, and genomic transformers,\ntried to reproduce key results or adapt them to key datasets,\nand documented the pain points—data formats, training instabilities, evaluation pitfalls, which never quite fit into a methods section.\n\nThe later parts were shaped by collaborations around clinical prediction, variant interpretation pipelines, and larger multi-omic models. Many of the examples and caveats come directly from these projects: places where a model that looked excellent on paper behaved in surprising ways when exposed to real-world data, or where simple baselines outperformed much fancier architectures once confounding and distribution shift were handled correctly.\nBecause of that origin, the book has a particular bias: it is written from the perspective of someone who spends much of their time trying to get models to work in messy, high-stakes settings. You will see this in the emphasis on data quality, evaluation, and clinical translation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#how-to-read-this-book",
    "href": "preface.html#how-to-read-this-book",
    "title": "Preface",
    "section": "How to Read This Book",
    "text": "How to Read This Book\nThis is not a genomics textbook, a complete review of every DNA or protein model, or a deep-learning-from-scratch course. Instead, it is meant to be:\n\na roadmap to the main kinds of data, models, and objectives that matter for genomic foundation models today\na bridge between classical statistical genetics and modern representation learning\na practical guide to the kinds of failure modes and design choices that matter in real applications.\n\nYou do not need to read the book cover-to-cover in order.\n\nIf your background is in genomics or statistical genetics, you may want to skim the early deep-learning motivations and focus more on the sections that introduce convolutional models, transformers, and self-supervision, then move on to evaluation and applications.\nIf you come from machine learning, it may be more helpful to start with the genomic data and pre-deep-learning methods, then dive into the sequence-to-function and transformer-based chapters with an eye toward how the data and objectives differ from text or images.\nIf you are a clinician or translational researcher, you might care most about the reliability, confounding, and clinical deployment discussions, dipping back into the modeling parts as needed to interpret results or communicate with technical collaborators.\n\nThe book is organized into six parts:\n\nPart I introduces genomic data and pre-deep-learning interpretation methods, from sequencing and variant calling to early pathogenicity scores and polygenic models.\nPart II focuses on supervised sequence-to-function models, with an emphasis on convolutional architectures, regulatory prediction, and splicing.\nPart III turns to transformer-based models and self-supervision, covering protein and DNA language models and hybrid architectures that combine CNNs and transformers.\nPart IV discusses what makes a model a foundation model in genomics, including multi-omic architectures, variant effect modeling, and emergent capabilities.\nPart V examines reliability, evaluation, confounding, and interpretability—how we know whether a model is learning what we think it is, and how to detect when it is not.\nPart VI looks at applications: clinical and risk prediction, variant interpretation workflows, and early steps toward drug discovery and biotech use cases.\n\nWithin each part, the goal is not to catalogue every paper, but to highlight representative examples and the design principles they illustrate. References are there to give you starting points, not to serve as a comprehensive literature review.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#what-this-book-assumes-and-what-it-does-not",
    "href": "preface.html#what-this-book-assumes-and-what-it-does-not",
    "title": "Preface",
    "section": "What This Book Assumes (and What It Does Not)",
    "text": "What This Book Assumes (and What It Does Not)\nThe book assumes:\n\nbasic familiarity with probability and statistics (regression, hypothesis testing, effect sizes),\ncore genomics concepts (genes, variants, linkage disequilibrium, GWAS at a high level),\nand some exposure to machine learning ideas (training versus test data, overfitting, loss functions).\n\nIt does not assume that you have implemented deep learning models yourself, or that you are fluent in every area. When a chapter leans heavily on a particular background (for example, causal inference or modern self-supervised learning), it will either provide a brief refresher or point you to an appendix or external resource.\nIf you are missing some of this background, that is fine. The intent is for you to be able to read actively: to pause, look up side topics, and then return to the main arc without feeling lost.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#a-note-on-scope-and-opinions",
    "href": "preface.html#a-note-on-scope-and-opinions",
    "title": "Preface",
    "section": "A Note on Scope and Opinions",
    "text": "A Note on Scope and Opinions\nGenomic foundation models are evolving quickly. Any snapshot is, by definition, incomplete and slightly out of date.\nRather than chasing every new architecture or benchmark, the book focuses on durable ideas:\n\nhow different data types fit together,\nwhat kinds of objectives encourage useful representations,\nhow evaluation can fail in genomics-specific ways,\nand where deep models complement (rather than replace) classical approaches.\n\nInevitably, there are judgment calls about which papers, methods, and perspectives to emphasize. Those choices reflect my own experiences and biases. They are not an official position of any institution I work with, and they will certainly differ from other reasonable views in the field.\nYou should treat the book as one opinionated map of the landscape, not the landscape itself.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#acknowledgements",
    "href": "preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis book exists because of many generous people who shared their time, ideas, and encouragement.\nFirst, I owe a deep debt of gratitude to my colleagues in the Mayo Clinic GenAI and broader data science community. The day-to-day conversations, whiteboard sessions, and “what went wrong here?” post-mortems with this group shaped much of the perspective and many of the examples in the chapters.\nI am especially grateful to the principal investigators and clinicians whose questions kept the focus on real patients and real decisions:\nDr. Shant Ayanian, Dr. Elena Myasoedova, and Dr. Alexander Ryu.\nTo leadership at Mayo Clinic who supported the time, computing resources, and institutional patience needed for both the models and this book:\nDr. Matthew Callstrom, Dr. Panos Korfiatis, and Matt Redlon.\nTo my data science and machine learning engineering colleagues, whose work and feedback directly shaped many of the workflows and case studies:\nBridget Toomey, Carl Molnar, Zach Jensen, and Marc Blasi.\nI am also grateful for the architectural creativity, hardware insight, and willingness to experiment from our collaborators at Cerebras:\nNatalia Vassilieva, Jason Wolfe, Omid Shams Solari, Vinay Pondenkandath, Bhargav Kanakiya, and Faisal Al-khateeb.\nAnd to our collaborators at GoodFire, whose partnership helped push these ideas toward interpretable and deployable systems:\nDaniel Balsam, Nicholas Wang, Michael Pearce, and Mark Bissell.\nI would also like to thank my former colleagues at LGC for foundational work and conversations around protein language models and large-scale representation learning:\nPrasad Siddavatam and Robin Butler.\nBeyond these named groups, I owe a broader debt to the geneticists, molecular biologists, statisticians, clinicians, and engineers whose work this book draws on. The field moves forward because people share code, publish honest benchmarks, and insist that models be connected back to biologically meaningful questions. Thank you for setting that standard.\nFinally, I am grateful to my wife, Alyssa, and our two kids for their patience with the evenings and weekends this book consumed. You gave me the space to finish it and the reasons to step away from it.\nIf this book helps you connect a new model to a real biological question, design a more robust evaluation, or communicate more clearly across disciplinary boundaries then it will have done its job.\n— Josh Meehl",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "p1--foundations.html",
    "href": "p1--foundations.html",
    "title": "Part I: Foundations",
    "section": "",
    "text": "Before deep learning can predict variant effects, model regulatory grammars, or guide clinical decisions, it must contend with the raw materials of modern genomics: sequences, variants, and the biological and statistical frameworks used to interpret them. Part I establishes this foundation, surveying the technologies, datasets, and pre-deep-learning methods that any genomic foundation model must respect, integrate with, or improve upon.\nThe chapters that follow trace a natural arc from data generation to interpretation. 1  Sequencing: From Reads to Variants introduces next-generation sequencing and variant calling, the processes that transform biological samples into the VCF files of single-nucleotide variants and indels that serve as inputs to nearly all downstream analysis. Understanding these technologies reveals both their remarkable power and their systematic blind spots, from reference bias to missing structural variants, limitations that propagate into every model trained on their outputs.\n2  The Genomic Data Landscape surveys the public resources that underpin modern computational genomics: reference genomes, population variation catalogs like gnomAD, functional genomics consortia such as ENCODE and Roadmap Epigenomics, and biobank-scale cohorts including the UK Biobank and GTEx. These resources serve simultaneously as training data, evaluation benchmarks, and sources of prior biological knowledge. Their coverage and biases shape what models can learn and what questions remain out of reach.\nFrom individual variants, 3  GWAS and Polygenic Scores moves to the statistical machinery of genome-wide association studies and polygenic scores. GWAS identify variant-trait associations across populations, while polygenic scores aggregate thousands of small effects into genome-wide predictors of complex traits. These methods define the classical approach to connecting genotype with phenotype, providing both baselines against which deep models are measured and conceptual frameworks that inform their design.\nFinally, 4  Classical Variant Effect Prediction examines pre-deep-learning variant effect prediction through the lens of CADD, the Combined Annotation Dependent Depletion framework. CADD represents a mature approach to variant prioritization through careful feature engineering, combining conservation, regulatory annotations, and population constraints into a single deleteriousness score. Its strengths and limitations, including subtle circularities with clinical databases, motivate the learned representations that subsequent chapters develop.\nTogether, these four chapters answer a foundational question: what data and methods form the backdrop against which genomic foundation models are built, trained, and evaluated? The models that follow do not emerge in a vacuum. They inherit the biases of sequencing technologies, learn from consortia-scale functional annotations, compete against GWAS-derived baselines, and face evaluation on benchmarks shaped by decades of prior computational and clinical work. Understanding this landscape is prerequisite to understanding what foundation models can and cannot accomplish.",
    "crumbs": [
      "Part I: Foundations"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html",
    "href": "p1-ch01-ngs.html",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "",
    "text": "1.1 The Challenge of NGS Data\nModern genomics rests on a paradox. We can sequence a human genome for a few hundred dollars and store millions of genomes in continental biobanks, yet we cannot reliably interpret most of the variants we discover. A newborn screening program can identify thousands of rare variants in an infant’s genome before discharge, but clinicians can confidently act on fewer than a hundred. The flood of sequence data vastly outpaces our ability to distinguish true biological variation from technical artifact, let alone to predict which variants matter for disease or drug response. Every analysis downstream, from polygenic scores to clinical variant interpretation, assumes that this upstream conversion from sequencing reads to variant calls has already succeeded. When it fails, every subsequent model inherits those errors.\nThis asymmetry between data generation and interpretation defines the central challenge of genomic medicine. Variant calls represent the atoms that later models operate on: polygenic risk scores treat variants as weighted features, regulatory sequence models learn from patterns around variant sites, and clinical interpretation systems classify individual variants as pathogenic or benign. None of these systems question whether the variants they receive are real. They assume the upstream pipeline has already solved the problem of separating signal from noise, true variation from systematic artifact, inherited germline mutations from somatic changes acquired during life. Understanding where that assumption breaks down, and how deep learning has begun to address those failures, establishes the foundation for everything that follows in this book.\nThe human genome contains approximately three billion base pairs, yet no instrument can read this sequence in one continuous stretch. Next-generation sequencing (NGS) instead fragments DNA molecules into short pieces, sequences each fragment independently, and produces tens to hundreds of gigabases of sequence data per run (Goodwin, McPherson, and McCombie 2016). The typical output consists of paired-end Illumina reads spanning 100 to 300 base pairs each, with each base assigned a quality score reflecting the instrument’s confidence in that call. This abundance comes at a cost: every read carries non-trivial measurement uncertainty, including substitutions from miscalled bases, context-specific errors near homopolymers, and quality degradation toward read ends.\nLong-read technologies from Pacific Biosciences and Oxford Nanopore extend the observable space dramatically, producing reads of 10 kilobases to over a megabase in length (Wenger et al. 2019; Dabernig-Heinz et al. 2024). These platforms access genomic territory invisible to short reads, including complex structural variants, segmental duplications, and repetitive regions. They carry their own characteristic error profiles, however, and the choice of sequencing platform fundamentally shapes which variants are discoverable and which systematic biases enter downstream analyses. A variant residing within a repetitive element may be invisible to short reads but readily detected by long reads that span the entire repeat.\nThe central problem is deceptively simple in statement but profound in consequence: how do we turn raw reads into a reliable list of genomic variants? Answering this question requires disentangling three fundamentally different sources of signal that manifest identically as mismatches between reads and reference. Sequencing errors arise from instrument noise and PCR artifacts during library preparation, creating false variants that never existed in the original DNA. Alignment artifacts occur when reads are mapped to incorrect genomic locations, particularly in repetitive regions and paralogous gene families, causing true variants to appear at wrong positions or disappear entirely. Genuine biological variation encompasses germline variants inherited from parents, somatic mutations acquired during cellular division, and mosaicism where only a fraction of cells carry a particular change. Historically, complex modular pipelines combining probabilistic models and hand-crafted heuristics addressed this separation (Nielsen et al. 2011). Deep learning now plays an important role in simplifying and improving parts of this stack, but understanding the classical pipeline remains essential for interpreting what downstream models actually learn.\nThe scope of this chapter centers on germline variant calling in human whole-exome sequencing (WES) and whole-genome sequencing (WGS) data, the core technical challenge underlying most genomic deep learning applications. Somatic variant calling in cancer and RNA-seq-specific variant calling share many parallels but require additional considerations that fall outside Part I of this book.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#the-challenge-of-ngs-data",
    "href": "p1-ch01-ngs.html#the-challenge-of-ngs-data",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "",
    "text": "Warning\n\n\n\nVisual TODO: Overview schematic of an NGS variant-calling workflow, from DNA sample → library prep → sequencer (short vs. long read) → FASTQ → alignment → duplicate marking → BQSR → variant calling → filtering → VCF.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#targeting-strategies-panels-exomes-and-genomes",
    "href": "p1-ch01-ngs.html#targeting-strategies-panels-exomes-and-genomes",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.2 Targeting Strategies: Panels, Exomes, and Genomes",
    "text": "1.2 Targeting Strategies: Panels, Exomes, and Genomes\nDifferent clinical and scientific goals demand different sequencing strategies. A patient presenting with sudden cardiac arrest at age 35 needs deep, reliable coverage of KCNQ1, KCNH2, SCN5A, and other ion channel genes associated with long QT syndrome; sequencing her entire genome to find these variants would waste resources and delay clinical decisions. A biobank building training data for polygenic risk scores across hundreds of thousands of participants needs genome-wide coverage, even if individual sites have modest depth. A family searching for the cause of their child’s undiagnosed developmental delay needs comprehensive coverage that leaves no coding exon unexamined. These competing demands drive the choice between targeted panels, whole-exome sequencing, and whole-genome sequencing.\n\n1.2.1 Targeted and Panel Sequencing\nWhen clinicians already know which genes to examine, targeted gene panels capture tens to hundreds of genes selected for a specific clinical indication. Panels for cardiomyopathy, hereditary cancer syndromes, or epilepsy restrict sequencing to regions of known clinical relevance. By limiting the target to a small number of loci, panels achieve very deep coverage (often exceeding 500×) at modest cost, enabling sensitive detection of rare variants and some degree of mosaicism.\nThe narrow scope of panels limits their utility for deep learning and population-scale analysis. Panels miss novel disease genes outside their predefined targets, cannot be easily repurposed for new traits, and often have heterogeneous content across laboratories that complicates data aggregation. For large-scale genomic foundation models, panel data serve better as richly phenotyped anchors than as primary training material: they provide clean labels for specific variants but sparse genomic coverage overall.\n\n\n1.2.2 Whole-Exome Sequencing\nProtein-coding sequence represents approximately 1 to 2 percent of the genome but harbors a disproportionate share of variants with known functional consequences. Whole-exome sequencing (WES) enriches coding exons and some splice-adjacent regions through hybridization probes that pull down targeted DNA, followed by short-read sequencing. Typical coverage ranges from 80 to 150× for exonic targets, sufficient for confident heterozygous variant calling in most regions.\nWES has driven Mendelian disease gene discovery for over a decade and powered early biobank-scale efforts, including the exome subsets of gnomAD and many hospital-based cohorts (Karczewski et al. 2020). The capture-based approach introduces systematic biases that propagate into downstream analyses, however. Certain exons consistently fail to capture efficiently, particularly those with extreme GC content, high repetitive content, or unusual length. A variant in the first exon of HTT (the Huntington disease gene) might be missed entirely due to extreme GC richness, and a variant effect predictor trained on WES data will never encounter variants in poorly captured regions. These blind spots are invisible in standard benchmarks but can have substantial clinical consequences. Batch effects tied to reagent lots and evolving panel designs further complicate multi-cohort analyses.\n\n\n1.2.3 Whole-Genome Sequencing\nNoncoding variants contribute substantially to human disease, and structural variants often span boundaries between exonic and intronic sequence. Whole-genome sequencing (WGS) samples nearly all bases in the genome at typical coverage of 30 to 60×, encompassing both coding and noncoding regions without the biases introduced by capture chemistry. Because there is no enrichment step, WGS produces more uniform depth than WES and enables detection of noncoding regulatory variants, structural variants, and copy-number changes alongside single nucleotide variants (SNVs) and indels (insertions and deletions).\nWGS has become increasingly favored for new large cohorts and rare disease studies. The UK Biobank’s release of 500,000 whole genomes and gnomAD’s expansion to include diverse populations both rely on WGS as the primary data type (Bycroft et al. 2018; Karczewski et al. 2020). The data are reusable for many downstream analyses, including GWAS, polygenic score development, and rare variant burden tests, and the simplified pipeline eliminates the need to track changing capture designs across time and centers. When subsequent chapters refer to “whole-genome models,” they implicitly assume access to WGS-based variant calls, even when actual training sets combine WES and WGS data for practical reasons.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Comparative table or radar plot for panels, WES, WGS (and possibly long-read WGS) summarizing target size, typical coverage depth, per-sample cost, ability to detect rare variants, and suitability for large-cohort foundation model training.\n\n\n\n\n1.2.4 Long-Read Sequencing Technologies\nShort reads face a fundamental limitation rooted in information theory: sequences shorter than local repeats, segmental duplications, or structural variants cannot unambiguously resolve these features. Consider the SMN1 and SMN2 genes, which differ by only five nucleotides across their entire coding regions. Distinguishing them is clinically critical for diagnosing spinal muscular atrophy, yet short reads routinely fail this task because a 150-bp read maps equally well to either genomic location.\nPacific Biosciences (PacBio) HiFi sequencing produces reads of 10 to 25 kilobases with per-base accuracy exceeding 99.9% through circular consensus sequencing, where the same molecule is read multiple times to correct random errors (Wenger et al. 2019). Oxford Nanopore Technologies (ONT) instruments generate reads ranging from a few kilobases to over a megabase in length, with rapidly improving raw accuracy and unique capabilities including portable sequencers suitable for field deployment, direct RNA sequencing without reverse transcription, and real-time base calling during sequencing (Dabernig-Heinz et al. 2024). These technologies played central roles in the telomere-to-telomere (T2T) assembly of a complete human genome and in emerging human pangenome references that capture population diversity beyond what any single linear reference can represent (Nurk et al. 2022; Liao et al. 2023).\nLong reads transform variant calling by traversing low-complexity and repetitive regions essentially invisible to short-read technologies. Dedicated variant callers such as PEPPER-Margin-DeepVariant (Shafin et al. 2021), Clair3 (Zheng et al. 2022), Sniffles2 (Smolka et al. 2024), pbsv (“PacificBiosciences/Pbsv” 2025), and cuteSV (Jiang et al. 2020) exploit read length and alignment patterns to detect insertions, deletions, inversions, and complex rearrangements. Single molecules spanning multiple heterozygous sites provide direct phasing information for haplotype resolution without statistical inference. Long reads also inform graph-based references and pangenomes that better represent population diversity than traditional linear references (Liao et al. 2023).\nShort-read pipelines remain the workhorse for large human cohorts due to cost and throughput advantages that will persist for years. The models discussed in later chapters must accommodate variants discovered by either technology and must be evaluated on composite callsets that integrate short- and long-read information.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#classical-variant-calling-pipelines",
    "href": "p1-ch01-ngs.html#classical-variant-calling-pipelines",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.3 Classical Variant Calling Pipelines",
    "text": "1.3 Classical Variant Calling Pipelines\nUnderstanding classical approaches matters not merely for historical completeness but because deep learning models like DeepVariant still operate within this overall framework. They replace specific components rather than rebuilding from scratch. The GATK Best Practices, first formalized around 2011 and refined continuously since, represent accumulated wisdom from a decade of methodological development (DePristo et al. 2011; Van der Auwera et al. 2018). These pipelines encode expert intuition about which quality metrics matter, how to balance sensitivity against specificity, and when borderline evidence should be trusted. Modern deep learning approaches inherit this structure even as they replace individual components with learned alternatives.\n\n1.3.1 From Sequencer to Aligned Reads\nThe journey from DNA sample to variant calls begins when instrument software converts fluorescent images or electrical signals to base calls and quality scores through base calling. Reads are demultiplexed by sample barcode into FASTQ files, each containing millions of short sequences with associated Phred-scaled quality scores. These files serve as the raw material for all subsequent analysis, encoding both the sequence content and the instrument’s confidence in each base.\nRead alignment maps each short read to its most likely position in a reference genome using seed-and-extend algorithms implemented in tools such as BWA-MEM and minimap2 (Li 2013, 2018). The current standard references include GRCh38 (the traditional linear reference) and T2T-CHM13 (the first complete telomere-to-telomere assembly). The alignment challenge is substantial: algorithms must cope with mismatches arising from both true variants and sequencing errors, small indels that shift the alignment frame, and repetitive sequences where multiple genomic locations match equally well. When a read could plausibly originate from several locations, the aligner must either choose one (potentially incorrectly), report multiple candidates, or assign a mapping quality score reflecting its uncertainty about the true origin.\nPost-alignment processing addresses systematic artifacts that would otherwise corrupt variant calls. PCR duplicates arise when multiple reads are amplified from the same original DNA fragment during library preparation; these inflate apparent coverage and can amplify sequencing errors into false variants that appear well-supported. Duplicate marking identifies and flags these reads based on identical alignment coordinates. Base quality score recalibration (BQSR) models systematic quality score errors by comparing observed mismatches to databases of known variants, adjusting quality scores to better reflect true error rates in each sequence context (DePristo et al. 2011). Older pipelines also performed local realignment around indels, though modern callers have largely internalized this step.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Block diagram of a short-read variant calling pipeline, with each step (alignment, duplicate marking, BQSR, per-sample calling, joint genotyping, filtering) shown as a box and data formats (FASTQ, BAM/CRAM, gVCF, VCF) annotated along the arrows.\n\n\n\n\n1.3.2 Per-Sample Variant Calling\nAt each position in the genome, the fundamental question is: given the reads overlapping this site, what is the most likely genotype? For diploid humans at biallelic sites, three possibilities exist: homozygous reference (0/0), heterozygous (0/1), or homozygous alternate (1/1). The task is to compute genotype likelihoods that quantify the probability of the observed read data under each possible genotype, then combine these likelihoods with prior expectations to estimate posterior probabilities.\nGATK HaplotypeCaller approaches this by first identifying regions with evidence of variation, then locally assembling candidate haplotypes from the reads spanning that region, and finally computing likelihoods for each possible diploid genotype. The core calculation uses a pair hidden Markov model (pair-HMM) to marginalize over possible alignments between each read and each candidate haplotype, incorporating base quality scores to weight the contribution of each base (DePristo et al. 2011; Li 2014).\nThe mathematical framework is Bayesian. At a given site, the posterior probability of genotype \\(G\\) given read data \\(D\\) follows from Bayes’ theorem:\n\\[\nP(G \\mid D) \\propto P(G) \\prod_{r \\in \\text{reads}} P(r \\mid G)\n\\]\nThe prior \\(P(G)\\) often assumes Hardy-Weinberg equilibrium with a specified allele frequency, while the likelihood \\(P(r \\mid G)\\) captures the probability of observing read \\(r\\) given that the true genotype is \\(G\\). This formulation assumes conditional independence of reads given the genotype, an assumption violated in practice by systematic sequencing errors, read pair correlations, and library-level artifacts that create dependencies among observations. Classical pipelines attempt to correct for these violations through BQSR and ad hoc filters. Deep learning-based callers can learn these dependencies implicitly by processing entire pileups simultaneously, one of their key advantages.\nThe per-read likelihoods aggregate into genotype likelihoods, which combine with priors to yield posterior probabilities. These posteriors become the genotype quality (GQ) scores that downstream analyses often treat as ground truth. Per-sample results are output as gVCF files encoding both variant calls and “reference blocks” with estimated confidence at non-variant positions, enabling later joint analysis across samples.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Small schematic showing reads aligned at a single site, illustrating how each read contributes to the likelihood term in the Bayesian genotype model (e.g., three reads supporting the alternate allele with different base qualities).\n\n\n\n\n1.3.3 Cohort Calling and Filtering\nIndividual samples rarely provide sufficient information for confident rare variant calling. A variant observed in only one sample with modest supporting reads might be a true rare variant or a systematic artifact; examining a single sample cannot distinguish these possibilities. Examining the same site across thousands of samples resolves this ambiguity: true variants appear in multiple individuals following population genetic expectations, while artifacts show patterns inconsistent with inheritance and population structure.\nJoint genotyping combines gVCFs across many samples to produce a multi-sample VCF. This process ensures that all samples are evaluated at the same candidate sites, avoiding the problem of comparing different variant lists, and pools information across carriers to improve sensitivity for rare variants. A variant with marginal evidence in three individuals gains credibility when those individuals share ancestry and the variant frequency matches population expectations from external databases.\nFiltering strategies separate high-confidence variants from probable artifacts. Early approaches applied independent thresholds on quality metrics such as depth, mapping quality, and strand bias, but these hard filters poorly captured the complex, multivariate patterns distinguishing true variants from errors. Variant Quality Score Recalibration (VQSR) instead trains a Gaussian mixture model on known true positives from validated resources (HapMap, 1000 Genomes) and likely false positives, learning a composite quality score that integrates multiple annotation dimensions (DePristo et al. 2011). This approach dominated large-scale variant calling for a decade before machine learning methods began to replace it.\n\n\n1.3.4 Sample-Level Quality Control\nBefore any downstream analysis or model training, variant callsets must pass through sample-level quality control. Sex checks compare reported sex to X chromosome heterozygosity and Y chromosome coverage to detect sample swaps or sex chromosome aneuploidy. Contamination analysis estimates whether DNA from multiple individuals mixed during sample preparation, which would create apparent heterozygosity at sites where the individual is actually homozygous. Relatedness detection identifies unexpected relatives or duplicate sequencing of the same individual, both of which confound association analyses and inflate apparent sample sizes. Ancestry inference estimates genetic ancestry using principal component analysis or model-based clustering, which matters for controlling population stratification in downstream analyses (Chapter 22).\nThese QC steps determine which samples enter training sets, how models are stratified by ancestry, and which samples must be excluded due to technical artifacts. When subsequent chapters refer to “a callset,” they implicitly assume that careful QC has already been applied.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#haplotype-phasing",
    "href": "p1-ch01-ngs.html#haplotype-phasing",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.4 Haplotype Phasing",
    "text": "1.4 Haplotype Phasing\nThe clinical stakes of phasing emerge clearly in compound heterozygosity. Consider a child who inherits two rare, potentially pathogenic variants in CFTR, the cystic fibrosis gene. If both variants reside on the chromosome inherited from the mother (in cis), the child retains one functional copy from the father and may be unaffected or merely a carrier. If the variants are on opposite chromosomes (in trans), no functional copy exists and the child will develop cystic fibrosis. Standard VCF genotypes cannot distinguish these scenarios, encoding only that heterozygous genotypes exist at two positions without specifying which alleles travel together on the same physical chromosome. The clinical implications are entirely different, yet the data appear identical without phase information.\nDiploid organisms carry two copies of each autosomal chromosome, one inherited from each parent. Haplotype phasing resolves the ambiguity in unphased genotype calls by assigning each allele to a specific parental chromosome, transforming genotypes such as 0/1 into phased representations like 0|1 or 1|0 where the delimiter indicates that phase has been determined.\n\n1.4.1 Clinical and Analytical Importance\nThe distinction between cis and trans configurations drives clinical decisions for recessive conditions across hundreds of disease genes, from metabolic disorders to hearing loss to retinal degeneration. Beyond compound heterozygosity, phased haplotypes enable several critical analyses. Haplotype-specific expression studies reveal allelic imbalance where one parental copy is preferentially transcribed, a phenomenon with implications for imprinting disorders and variable penetrance. Accurate modeling of linkage disequilibrium (LD) structure in population genetics depends on knowing which alleles are inherited together. Reference panels used for genotype imputation are stored as phased haplotypes; inaccurate phasing in these panels propagates errors to every study that uses them for imputation.\nFor deep learning applications, phasing determines whether models receive unordered genotype pairs or structured, haplotype-resolved representations. This choice affects model architecture, training procedures, and ultimately performance. A model that processes phased haplotypes can learn patterns spanning multiple variant sites that would appear as noise in unphased data.\n\n\n1.4.2 Phasing Methods\nDifferent data types enable different phasing strategies, each with characteristic strengths and resolution. Read-backed phasing uses sequencing reads that span multiple heterozygous sites to assign alleles to the same physical molecule. Short reads typically phase variants within tens to hundreds of base pairs, limited by fragment length. Long reads extend this range to tens of kilobases or more, providing direct physical evidence of haplotype structure.\nStatistical phasing tools such as SHAPEIT, Eagle, and Beagle use reference panels of previously phased haplotypes combined with linkage disequilibrium patterns to infer phase across much longer distances (O’Connell et al. 2014; Loh et al. 2016; Browning et al. 2021). When a study sample shares long haplotype segments with individuals in the reference panel, the algorithm can confidently assign alleles to chromosomes even without direct read evidence. These methods work well for common variation where LD is informative but struggle with rare variants that lack haplotype context in reference panels.\nPedigree-based phasing becomes possible when parent-offspring trios or larger families are available. Mendelian inheritance rules resolve phase with high confidence: an allele present in the child and one parent but absent in the other must have been inherited from that parent. The deterministic nature of this inference makes it the gold standard when family data exist.\nModern pipelines often combine these approaches, using statistical phasing anchored by a large reference panel, augmented by read-backed evidence where available, and refined by trio data when present. The resulting phase accuracy varies by variant frequency, local recombination rate, and representation in reference panels.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Table or multi-panel cartoon comparing phasing strategies: axes for input data (short reads, long reads, trios, population reference), typical genomic span per phased block, and typical use cases (imputation, rare disease, population genetics).\n\n\n\n\n1.4.3 Genotype Imputation\nSequencing every individual at high coverage is expensive, but statistical inference from population structure can fill gaps at much lower cost. Genotype imputation matches a cohort with incomplete genotype data (from array genotyping or low-coverage sequencing) against a reference panel of densely phased haplotypes. Statistical models infer missing genotypes and refine uncertain calls by leveraging LD patterns and shared haplotype segments with individuals in the reference (Browning et al. 2021).\nTwo related processes deserve distinction. Genotype refinement improves quality at sites where genotypes were already measured but with uncertainty, particularly useful in low-coverage WGS or WES where stochastic sampling creates noisy calls. Imputation of untyped variants infers genotypes at positions not directly observed in the study cohort but present in the reference panel, dramatically increasing variant density without additional sequencing.\nFor downstream deep learning, imputation has several important consequences. It increases the number of variants available as input features for genotype-based models. It produces well-calibrated genotype probabilities (dosages) that probabilistic models can exploit rather than forcing hard calls. It also ties model performance to the composition and ancestry representation of the reference panel: imputation errors are systematically larger when target individuals come from populations underrepresented in the panel, reinforcing themes of bias and confounding addressed in Chapter 22.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Cartoon showing how genotype imputation and boosting fill in missing genotypes and refine uncertain calls using a reference panel of phased haplotypes, with before/after views of a sparse vs. dense genotype matrix.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#sources-of-error-and-uncertainty",
    "href": "p1-ch01-ngs.html#sources-of-error-and-uncertainty",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.5 Sources of Error and Uncertainty",
    "text": "1.5 Sources of Error and Uncertainty\nEven with sophisticated pipelines, variant calls remain imperfect measurements of biological reality. These errors concentrate in specific genomic contexts and variant types, creating systematic blind spots in training data that downstream models inherit without warning (Li 2014). Understanding where errors arise, and why they cluster where they do, is essential for interpreting model performance and designing robust training strategies.\n\n1.5.1 Mapping Ambiguity and Reference Bias\nWhen reads align almost equally well to multiple genomic locations, no algorithm can confidently determine their true origin. Segmental duplications, paralogous gene families, and repetitive elements create these ambiguous contexts throughout the genome. The consequences flow in both directions: misassigned reads create false positive variants at incorrect locations, while correctly placed reads may be discarded or down-weighted due to mapping uncertainty, creating false negatives.\nReference bias compounds these problems by systematically favoring detection of reference alleles over alternate alleles. A read carrying a non-reference variant may align slightly worse than an identical read matching the reference due to the mismatch penalty, leading to preferential retention of reference-supporting evidence. This bias causes systematic undercalling of alternate alleles, particularly in highly polymorphic regions or for variants that substantially alter local sequence context. Populations divergent from the reference genome experience more severe reference bias, creating ancestry-correlated error patterns.\n\n\n1.5.2 Systematic Sequencing Artifacts\nSequencing chemistry introduces predictable error patterns that differ qualitatively from random noise. Homopolymer runs (stretches of identical nucleotides such as AAAAAAA or GGGGGG) cause polymerase slippage during synthesis, generating false indels at rates far exceeding substitution errors. Certain sequence motifs, particularly those with extreme GC content, exhibit systematically elevated error rates that persist even at high coverage. PCR amplification during library preparation can introduce errors early in the process; these errors then propagate into multiple reads, creating correlated false positives that appear well-supported by independent evidence.\nIndex hopping occurs when sample barcodes are misassigned during multiplexed sequencing, causing variants from one sample to appear spuriously in others sharing the same flow cell. Strand bias, where variant-supporting reads cluster on one strand orientation, often indicates systematic artifact rather than true variation. These patterns create correlated errors that cluster by batch, lane, or library preparation method, and they are difficult to distinguish from rare true variants precisely because they can appear in multiple reads with reasonable quality scores.\n\n\n1.5.3 Coverage Gaps and Allelic Imbalance\nStochastic sampling means some genomic regions receive fewer reads than average purely by chance, even when capture or sequencing is nominally uniform. In these low-coverage regions, one or both alleles may be missed entirely, and allelic balance can deviate substantially from the expected 50:50 ratio in heterozygotes. A heterozygous site with 20× coverage and 10 reads supporting each allele is confidently called; the same site with 4× coverage might show 4 reference reads and 0 alternate reads by chance alone, leading to a false homozygous reference call with no indication that an allele was missed.\nSomatic mosaic variants present at low allele fractions face similar detection challenges. A variant present in 10% of cells produces reads indistinguishable in individual quality from sequencing errors at typical coverage depths, requiring specialized statistical methods or very deep sequencing to detect reliably.\n\n\n1.5.4 Complex Variants and Representation\nSmall indels near homopolymers, multi-nucleotide variants (MNVs), and overlapping indels present representation challenges beyond simple detection. The same biological event can often be encoded in multiple equivalent ways depending on alignment and normalization conventions. The variant chr1:100 AT&gt;A might alternatively appear as chr1:101 T&gt;-, with different callers and normalization tools potentially choosing different representations for the identical underlying mutation. These equivalent representations complicate comparisons across pipelines and benchmarks; two callsets may disagree on representation while agreeing on biology, or may appear to agree while representing different events.\nThe deep learning models discussed in later chapters inherit all these errors and uncertainties as their input. If a variant never enters the VCF, no model trained on VCFs can learn its effect. If genotype qualities are miscalibrated, models trained on hard calls may be systematically overconfident in regions where input data are fundamentally noisy. These inherited limitations propagate silently through the analysis chain.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#difficult-regions-the-limits-of-short-read-calling",
    "href": "p1-ch01-ngs.html#difficult-regions-the-limits-of-short-read-calling",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.6 Difficult Regions: The Limits of Short-Read Calling",
    "text": "1.6 Difficult Regions: The Limits of Short-Read Calling\nCertain genomic regions resist accurate variant calling regardless of algorithmic sophistication, with their difficulty stemming from fundamental properties of sequence structure that challenge alignment and assembly. These regions are disproportionately responsible for discordant calls between pipelines and technologies (Li 2014). Their clinical importance often exceeds their representation in training data.\n\n1.6.1 Segmental Duplications and Gene Families\nThe CYP2D6 gene illustrates how sequence complexity creates clinical blind spots. This gene encodes a cytochrome P450 enzyme responsible for metabolizing approximately 25% of clinically used drugs, including codeine, tamoxifen, and many antidepressants. It resides in a complex genomic region alongside two pseudogenes (CYP2D7 and CYP2D8) sharing over 90% sequence identity. Short reads from one copy map almost equally well to another, producing ambiguous alignments that either receive arbitrarily assigned positions or inflated mapping quality scores that mask the underlying uncertainty.\nVariant callers operating in this region face an impossible choice between sensitivity and specificity. Conservative approaches undercall true variation to avoid false positives; aggressive approaches call spurious variants in the wrong paralog. A patient’s CYP2D6 metabolizer status, critical for drug dosing decisions that can mean the difference between therapeutic efficacy and serious adverse events, may be incorrectly inferred from short-read data alone.\n\n\n1.6.2 Low-Complexity and Repetitive Sequence\nHomopolymers, short tandem repeats (STRs), and other low-complexity regions challenge both sequencing chemistry and alignment algorithms. Indel error rates are especially elevated in these contexts, and many pipelines mask or flag these regions as low confidence. Yet variation in repeats can be biologically critical. Triplet repeat expansion disorders including Huntington disease, fragile X syndrome, and myotonic dystrophy arise from unstable repeat sequences that standard short-read pipelines handle poorly. Models trained on callsets that exclude these regions inherit blind spots at clinically important loci.\n\n\n1.6.3 The HLA Region: A Case Study in Complexity\nThe human leukocyte antigen (HLA) locus on chromosome 6p21 exemplifies both the biological importance and technical difficulty of complex genomic regions. HLA genes including HLA-A, HLA-B, HLA-C, and HLA-DRB1 encode proteins central to immune recognition and represent some of the most polymorphic sequences in the human genome. The region spans several megabases of near-identical sequences interspersed with gene conversions, copy number variation, and pseudogenes.\nStandard reference-based alignment fails in HLA because the extreme polymorphism means reads carrying common, well-characterized alleles may match the linear reference genome poorly. A read from the HLA-B*57:01 allele (clinically important for predicting abacavir hypersensitivity in HIV treatment) may fail to align or align with low mapping quality, causing systematic undercalling of this medically actionable variant (Mallal et al. 2008). The same problems affect HLA typing for transplant matching, autoimmune disease association studies, and pharmacogenomic testing across diverse therapeutic areas (Robinson et al. 2020; Sakaue et al. 2023).\nSpecialized tools address these challenges through alternative strategies. HLA imputation methods use dense reference panels to infer HLA alleles from array genotypes, enabling large-scale association studies that would otherwise require expensive targeted sequencing (Sakaue et al. 2023). Sequence-based typing tools such as T1K perform HLA and KIR (killer immunoglobulin-like receptor) genotyping directly from WES, WGS, or RNA-seq data by aligning reads against allele databases rather than the linear reference (Song et al. 2022). Graph-based approaches incorporate known HLA alleles as alternate paths through the region, improving both alignment and variant calling (Garrison et al. 2018; Liao et al. 2023).\nHLA exemplifies a broader principle: regions that are biologically rich and clinically actionable are often technically difficult. Deep models trained on callsets that downweight or exclude these regions inherit their absence, creating blind spots precisely where accurate genotyping matters most.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Zoomed-in view of the HLA region comparing a linear reference vs. graph/pangenome representation, with multiple alternative haplotypes and structural variants, highlighting why linear alignment fails and how graph alignment improves recall.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#benchmarking-and-ground-truth",
    "href": "p1-ch01-ngs.html#benchmarking-and-ground-truth",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.7 Benchmarking and Ground Truth",
    "text": "1.7 Benchmarking and Ground Truth\nEvaluating variant callers requires high-confidence truth sets and standardized comparison tools. The challenge is that “ground truth” for variant calling is not actually true in any absolute sense; it represents consensus derived from multiple imperfect observations using different technologies and algorithms. Without careful benchmarking design, it is easy to overfit to specific datasets, underestimate errors in difficult regions, or misinterpret the practical significance of small metric improvements.\n\n1.7.1 GIAB Reference Samples\nThe Genome in a Bottle (GIAB) Consortium, coordinated by NIST, provides extensively characterized reference samples with validated variant calls across most of the genome (Zook et al. 2019). The primary sample is NA12878 (also known as HG001), a female of European ancestry from the CEPH/Utah pedigree with the longest history of multi-platform characterization. Additional samples span ancestral diversity and family structures: HG002 through HG004 comprise an Ashkenazi Jewish trio enabling trio-based validation, while HG005 through HG007 provide a Han Chinese trio.\nFor each sample, GIAB provides high-confidence variant calls representing consensus from multiple sequencing technologies and variant callers that constitute the best current estimate of true genotypes. Equally important are the high-confidence regions, genomic intervals where the truth set is believed to be reliable. Performance outside these regions remains formally unmeasured. Benchmarking tools such as hap.py and RTG Tools enable standardized comparison of test callsets against truth, implementing reproducible calculation of precision, recall, and F1 metrics by variant type (Krusche et al. 2019; “RealTimeGenomics/Rtg-Core” 2025).\n\n\n1.7.2 Metrics and Their Meaning\nStandard metrics for variant calling include recall (sensitivity), the fraction of true variants in the benchmark successfully identified by the caller; precision (positive predictive value), the fraction of called variants that are present in the benchmark truth set; and F1 score, the harmonic mean of precision and recall providing a single summary when both matter equally. These metrics are typically reported separately for SNVs and indels and may be stratified by genomic context to reveal where performance degrades.\nMetrics can be defined at different levels: per-variant (did we identify the correct alternate allele?), per-genotype (did we correctly determine zygosity?), or per-site (did we recognize variation at this position regardless of allele?). For downstream models, genotype-level accuracy and sample-level completeness often matter more than simply counting variant matches. A model that receives incorrect genotypes at common regulatory variants will learn corrupted associations even if overall variant-level metrics appear strong.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Example precision-recall or ROC curves for two variant callers evaluated on a GIAB-like benchmark, plus a small confusion-matrix-style illustration showing how false positives and false negatives map to the metrics.\n\n\n\n\n1.7.3 Limitations of Benchmarks\nGIAB truth sets derive primarily from a small number of deeply sequenced samples, predominantly of European ancestry in early releases, and initially focused on genomic regions where high confidence was achievable. High-confidence regions cover approximately 85 to 90 percent of the genome, leaving performance in excluded regions formally unknown. Performance in underrepresented ancestries, in complex structural variant regions, and for novel variant classes may differ substantially from headline GIAB metrics (Zook et al. 2019; Liao et al. 2023).\nWhen benchmarks are reused extensively for method development, the risk of overfitting to benchmark-specific patterns becomes substantial. Pipelines may be tuned to maximize F1 on GIAB-like samples without improving performance on real-world cohorts with different ancestry composition, sequencing protocols, or variant spectra. For deep learning-based callers with large capacity to absorb quirks in training data, this risk is especially acute. Later chapters revisit similar themes for benchmarking and evaluation of deep models more broadly (Chapter 20, Chapter 21).\nOngoing efforts from the T2T Consortium and the Human Pangenome Reference Consortium are expanding benchmark scope to include complete genome assemblies and diverse haplotype collections that better represent human genetic diversity (Nurk et al. 2022; Liao et al. 2023).",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#sec-deepvar",
    "href": "p1-ch01-ngs.html#sec-deepvar",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.8 DeepVariant: Variant Calling as Image Classification",
    "text": "1.8 DeepVariant: Variant Calling as Image Classification\nClassical variant calling pipelines encode accumulated expert intuition through hand-crafted features and carefully tuned heuristics developed over years of experience with sequencing data. DeepVariant, introduced by Google in 2018, posed a different question: what if we let the model learn these patterns directly from data? The key insight was not better probabilistic modeling of sequencing errors but rather a reformulation of the problem itself. Variant calling becomes image classification, and convolutional neural networks learn to distinguish true variants from artifacts through the same pattern recognition that enables them to classify natural images (Poplin et al. 2018).\n\n1.8.1 Pileup Images as Input\nAround each candidate variant site, DeepVariant constructs a multi-channel tensor resembling an image. Each row corresponds to a read overlapping the site, with columns indexing positions relative to the candidate variant. Channels encode multiple features: match or mismatch with the reference, Phred-scaled base quality, mapping quality, strand orientation, support for different alleles, and additional alignment characteristics. The reference sequence and candidate alleles are overlaid as additional channels providing context.\nThis representation transforms the variant calling problem fundamentally. Rather than computing summary statistics (depth, allelic balance, strand bias) and feeding them to a classifier with predefined decision rules, DeepVariant presents the raw evidence to a neural network. The model learns that strand-biased support clustered at read ends looks different from balanced support distributed across read positions without anyone explicitly defining these features or their relative importance. Patterns invisible to hand-crafted heuristics become learnable.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Example DeepVariant-style pileup image with color channels labeled (reference, base identity, base quality, strand, mapping quality), showing how reads stack around a potential variant site.\n\n\n\n\n1.8.2 Architecture and Training\nDeepVariant uses an Inception-style CNN architecture originally developed for natural image classification. The network processes the pileup tensor through multiple convolutional layers, pooling operations, and nonlinearities, outputting posterior probabilities over three genotype classes (homozygous reference, heterozygous, homozygous alternate) for each candidate site (Poplin et al. 2018).\nTraining uses high-confidence truth sets such as GIAB genomes. The model observes many examples of true variants and non-variants along with their associated pileup images, learning complex decision boundaries that integrate base quality, mapping quality, local sequence context, and read-level patterns. Where VQSR fits a separate model on hand-selected annotations after initial calling, DeepVariant processes raw evidence directly during the primary classification step.\nThe end-to-end training produces well-calibrated genotype likelihoods across a range of sequencing chemistries, instruments, and read lengths, particularly when fine-tuned for specific experimental contexts (Yun et al. 2021). Once trained, the same architecture generalizes across whole-genome versus whole-exome data, PCR-free versus PCR-amplified libraries, and different sequencing instruments. This adaptability contrasts with classical pipelines where calibration is often a separate, post hoc step requiring platform-specific tuning by experts.\n\n\n1.8.3 Cohort Calling with GLnexus\nDeepVariant operates primarily at the per-sample level, producing a gVCF of genotype likelihoods for each individual sample. To generate a multi-sample VCF suitable for population-scale analysis, these per-sample results must be combined through joint genotyping.\nGLnexus provides this cohort-level integration for DeepVariant gVCFs (Yun et al. 2021). The system merges per-sample likelihoods, applies cohort-level priors informed by observed allele frequencies, and performs multi-sample genotype refinement and filtering. Together, DeepVariant and GLnexus form a modular pipeline where deep learning replaces the per-sample likelihood engine while the overall architecture (per-sample calls, joint genotyping, cohort filtering) remains structurally similar to classical approaches.\nJoint calling improves sensitivity for rare variants by pooling evidence across carriers, ensures consistent variant representation across all samples in a cohort, and enables cohort-level quality filters that identify systematic artifacts visible only across many samples. This combination has become a de facto standard for large WES and WGS projects, including recent releases from gnomAD and the UK Biobank (Karczewski et al. 2020; Bycroft et al. 2018).\n\n\n1.8.4 Comparison with Classical Approaches\nThe fundamental difference between DeepVariant and classical pipelines lies in how evidence is combined. HaplotypeCaller uses pair-HMM models with explicit assumptions about read independence and then applies VQSR to recalibrate quality scores using hand-selected annotation features. DeepVariant processes entire pileups simultaneously, implicitly learning correlations among reads that violate the independence assumptions built into classical probabilistic models.\nThis end-to-end approach offers several practical advantages. Calibration emerges from training rather than requiring separate recalibration steps with their own parameter tuning. Transfer across platforms and even species often succeeds with modest fine-tuning rather than complete redevelopment. The model can detect subtle artifact patterns that escape hand-crafted filters, learning representations of error modes that human experts never explicitly described.\nBoth approaches share important limitations. Neither handles structural variants well; both focus primarily on SNVs and small indels. Both operate within the same overall pipeline framework: alignment, duplicate marking, and joint genotyping remain largely unchanged regardless of the per-sample caller used. DeepVariant is best understood as a drop-in replacement for the per-sample calling step, not a complete reimagining of variant discovery from raw data.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Side-by-side schematic of a classical GATK-style pipeline vs. a DeepVariant-enhanced pipeline, emphasizing which components are hand-crafted vs. learned, and where deep learning models plug into the overall workflow.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch01-ngs.html#implications-for-genomic-deep-learning",
    "href": "p1-ch01-ngs.html#implications-for-genomic-deep-learning",
    "title": "1  Sequencing: From Reads to Variants",
    "section": "1.9 Implications for Genomic Deep Learning",
    "text": "1.9 Implications for Genomic Deep Learning\nNGS and variant calling establish the foundation for everything else in this book. They determine what data downstream models receive, where coverage exists, and where systematic blind spots remain hidden. Understanding how variants are called, and where that process fails, is essential for interpreting the performance and limitations of every model built on this foundation.\n\n1.9.1 Variants as Atomic Units\nThe output of WES and WGS pipelines (a VCF of SNVs, indels, and inferred genotypes) defines the atomic units that many downstream models operate on. Polygenic risk scores treat variants as weighted features summed across the genome. GWAS summary statistics quantify associations at individual variant positions. Variant annotation tools classify each site by predicted functional consequence. Foundation models that operate on genotypes rather than raw sequence inherit the variant catalog as their effective vocabulary.\nIf a variant is never called, it cannot appear in training data, and no model can learn its effect. False positives introduce noise into labels and features, teaching models to associate spurious variants with phenotypes. False negatives create blind spots where models must extrapolate from incomplete information, often without any indication that data are missing. Choices about phasing, imputation, and variant representation determine whether models see haplotype-structured inputs, unordered genotypes, or scalar dosage summaries. The quality of variant calls directly limits the quality of everything built upon them.\n\n\n1.9.2 Inherited Biases and Blind Spots\nUpstream decisions constrain what downstream models can learn. If an assay rarely observes indels in certain repeat classes, models trained on those callsets effectively learn a world where such variants do not exist. If certain ancestries are underrepresented in reference panels or truth sets, models may perform poorly for those populations while appearing well-calibrated in benchmarks dominated by European samples. High-confidence region definitions determine which variants enter training sets; variants in excluded regions are invisible to models regardless of their biological importance.\nFor regulatory sequence models and variant effect predictors (Chapter 11, ?sec-reg, ?sec-splice, Chapter 4), upstream variant calling determines which sites appear as candidates and how often certain sequence patterns are observed in association with functional outcomes. The HLA blind spot in short-read calling means that models trained primarily on short-read callsets will systematically underperform for immune-related variants despite their substantial clinical importance for autoimmune disease, transplant rejection, and drug hypersensitivity.\n\n\n1.9.3 Effect Sizes Across the Frequency Spectrum\nVariant calling quality modulates the effective effect sizes that downstream models can detect, with different dynamics for common and rare variation. For common variants contributing to highly polygenic traits, modest genotype error acts as additional measurement noise that attenuates effect size estimates without creating spurious large effects. Improving variant calling in already “easy” genomic regions yields diminishing returns compared to simply increasing sample size.\nFor rare variants with large individual effects, the dynamics change substantially. Loss-of-function variants, damaging missense mutations, and splice-altering changes can have substantial effects on disease risk. Here, false negatives dominate the problem: if the variant is never called, its effect is invisible to association tests and to models trained on called genotypes. Small improvements in recall for clinically important rare variants can have outsized impact on gene discovery and interpretation.\nImputed variants introduce their own effect size modulation. The squared correlation between true and imputed genotypes acts as an attenuation factor: an association with true effect size \\(\\beta\\) behaves as if the effect were approximately \\(r^2 \\beta\\) in downstream analyses using imputed dosages. Improvements in imputation quality, particularly for underrepresented ancestries where current panels perform poorly, directly scale the effective signals that models can learn.\nClinically critical loci often present the most challenging technical contexts, creating a systematic mismatch between importance and data quality. Pharmacogenomic variants in CYP gene families, immune-related variants in HLA, and many other medically actionable sites reside in regions where standard pipelines perform poorly. Global accuracy metrics may change only slightly when these regions improve, but the clinical impact can be substantial.\n\n\n1.9.4 Toward End-to-End Learning\nDeepVariant exemplifies a broader paradigm that recurs throughout this book: replacing hand-engineered components with learned models. DeepSEA and Basenji learn regulatory grammars directly from sequence (?sec-reg). SpliceAI predicts splicing outcomes from local sequence context (?sec-splice). DNA language models learn representations from unlabeled genomes (Chapter 11). In each case, the central question is whether learned representations outperform hand-crafted features, and under what conditions the additional model capacity provides genuine benefit versus overfitting to training artifacts.\nSome recent work pushes further upstream, modeling raw sequencing signal (nanopore current traces or minimally processed reads) with the goal of learning alignment and variant calling jointly rather than as separate pipeline stages. Others propose models that reason directly over reads and phenotypes, bypassing intermediate variant calls entirely. These directions blur the boundary between variant calling and downstream modeling, foreshadowing the joint, multi-scale foundation models discussed in later parts of this book.\nThe models that follow operate within the constraints established here: variant calls whose reliability varies systematically across the genome, across populations, and across the allele frequency spectrum from common to rare. This foundation shapes everything built upon it.\n\n\n\n\n\n\nWarning\n\n\n\nVisual TODO: Conceptual “stack” figure showing raw sequence/signal → variant calling → variant-level representations (VCF, annotations) → genomic foundation models → downstream tasks (PGS, regulatory prediction, variant interpretation), with arrows indicating how errors/choices propagate upward.\n\n\n\n\n\n\nBrowning, Brian L., Xiaowen Tian, Ying Zhou, and Sharon R. Browning. 2021. “Fast Two-Stage Phasing of Large-Scale Sequence Data.” American Journal of Human Genetics 108 (10): 1880–90. https://doi.org/10.1016/j.ajhg.2021.08.005.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nDabernig-Heinz, Johanna, Mara Lohde, Martin Hölzer, Adriana Cabal, Rick Conzemius, Christian Brandt, Matthias Kohl, et al. 2024. “A Multicenter Study on Accuracy and Reproducibility of Nanopore Sequencing-Based Genotyping of Bacterial Pathogens.” Journal of Clinical Microbiology 62 (9): e00628–24. https://doi.org/10.1128/jcm.00628-24.\n\n\nDePristo, Mark A., Eric Banks, Ryan Poplin, Kiran V. Garimella, Jared R. Maguire, Christopher Hartl, Anthony A. Philippakis, et al. 2011. “A Framework for Variation Discovery and Genotyping Using Next-Generation DNA Sequencing Data.” Nature Genetics 43 (5): 491–98. https://doi.org/10.1038/ng.806.\n\n\nGarrison, Erik, Jouni Sirén, Adam M. Novak, Glenn Hickey, Jordan M. Eizenga, Eric T. Dawson, William Jones, et al. 2018. “Variation Graph Toolkit Improves Read Mapping by Representing Genetic Variation in the Reference.” Nature Biotechnology 36 (9): 875–79. https://doi.org/10.1038/nbt.4227.\n\n\nGoodwin, Sara, John D. McPherson, and W. Richard McCombie. 2016. “Coming of Age: Ten Years of Next-Generation Sequencing Technologies.” Nature Reviews Genetics 17 (6): 333–51. https://doi.org/10.1038/nrg.2016.49.\n\n\nJiang, Tao, Yongzhuang Liu, Yue Jiang, Junyi Li, Yan Gao, Zhe Cui, Yadong Liu, Bo Liu, and Yadong Wang. 2020. “Long-Read-Based Human Genomic Structural Variation Detection with cuteSV.” Genome Biology 21 (1): 189. https://doi.org/10.1186/s13059-020-02107-y.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKrusche, Peter, Len Trigg, Paul C. Boutros, Christopher E. Mason, Francisco M. De La Vega, Benjamin L. Moore, Mar Gonzalez-Porta, et al. 2019. “Best Practices for Benchmarking Germline Small Variant Calls in Human Genomes.” Nature Biotechnology 37 (5): 555–60. https://doi.org/10.1038/s41587-019-0054-x.\n\n\nLi, Heng. 2013. “Aligning Sequence Reads, Clone Sequences and Assembly Contigs with BWA-MEM.” arXiv. https://doi.org/10.48550/arXiv.1303.3997.\n\n\n———. 2014. “Towards Better Understanding of Artifacts in Variant Calling from High-Coverage Samples.” Bioinformatics 30 (20): 2843–51. https://doi.org/10.1093/bioinformatics/btu356.\n\n\n———. 2018. “Minimap2: Pairwise Alignment for Nucleotide Sequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness, Glenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome Reference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nLoh, Po-Ru, Petr Danecek, Pier Francesco Palamara, Christian Fuchsberger, Yakir A Reshef, Hilary K Finucane, Sebastian Schoenherr, et al. 2016. “Reference-Based Phasing Using the Haplotype Reference Consortium Panel.” Nature Genetics 48 (11): 1443–48. https://doi.org/10.1038/ng.3679.\n\n\nMallal, Simon, Elizabeth Phillips, Giampiero Carosi, Jean-Michel Molina, Cassy Workman, Janez Tomažič, Eva Jägel-Guedes, et al. 2008. “HLA-B*5701 Screening for Hypersensitivity to Abacavir.” New England Journal of Medicine 358 (6): 568–79. https://doi.org/10.1056/NEJMoa0706135.\n\n\nNielsen, Rasmus, Joshua S. Paul, Anders Albrechtsen, and Yun S. Song. 2011. “Genotype and SNP Calling from Next-Generation Sequencing Data.” Nature Reviews. Genetics 12 (6): 443–51. https://doi.org/10.1038/nrg2986.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V. Bzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The Complete Sequence of a Human Genome.” Science 376 (6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Connell, Jared, Deepti Gurdasani, Olivier Delaneau, Nicola Pirastu, Sheila Ulivi, Massimiliano Cocca, Michela Traglia, et al. 2014. “A General Approach for Haplotype Phasing Across the Full Spectrum of Relatedness.” PLOS Genetics 10 (4): e1004234. https://doi.org/10.1371/journal.pgen.1004234.\n\n\n“PacificBiosciences/Pbsv.” 2025. PacBio. https://github.com/PacificBiosciences/pbsv.\n\n\nPoplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, et al. 2018. “[DeepVariant] A Universal SNP and Small-Indel Variant Caller Using Deep Neural Networks.” Nature Biotechnology 36 (10): 983–87. https://doi.org/10.1038/nbt.4235.\n\n\n“RealTimeGenomics/Rtg-Core.” 2025. Real Time Genomics. https://github.com/RealTimeGenomics/rtg-core.\n\n\nRobinson, James, Dominic J Barker, Xenia Georgiou, Michael A Cooper, Paul Flicek, and Steven G E Marsh. 2020. “IPD-IMGT/HLA Database.” Nucleic Acids Research 48 (D1): D948–55. https://doi.org/10.1093/nar/gkz950.\n\n\nSakaue, Saori, Saisriram Gurajala, Michelle Curtis, Yang Luo, Wanson Choi, Kazuyoshi Ishigaki, Joyce B. Kang, et al. 2023. “Tutorial: A Statistical Genetics Guide to Identifying HLA Alleles Driving Complex Disease.” Nature Protocols 18 (9): 2625–41. https://doi.org/10.1038/s41596-023-00853-4.\n\n\nShafin, Kishwar, Trevor Pesout, Pi-Chuan Chang, Maria Nattestad, Alexey Kolesnikov, Sidharth Goel, Gunjan Baid, et al. 2021. “Haplotype-Aware Variant Calling with PEPPER-Margin-DeepVariant Enables High Accuracy in Nanopore Long-Reads.” Nature Methods 18 (11): 1322–32. https://doi.org/10.1038/s41592-021-01299-w.\n\n\nSmolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W. Horner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024. “Detection of Mosaic and Population-Level Structural Variants with Sniffles2.” Nature Biotechnology 42 (10): 1571–80. https://doi.org/10.1038/s41587-023-02024-y.\n\n\nSong, Li, Gali Bai, X. Shirley Liu, Bo Li, and Heng Li. 2022. “T1K: Efficient and Accurate KIR and HLA Genotyping with Next-Generation Sequencing Data.” bioRxiv. https://doi.org/10.1101/2022.10.26.513955.\n\n\nVan der Auwera, Geraldine A., Mauricio O. Carneiro, Christopher Hartl, Ryan Poplin, Guillermo del Angel, Ami Levy-Moonshine, Tadeusz Jordan, et al. 2018. “From FastQ Data to High-Confidence Variant Calls: The Genome Analysis Toolkit Best Practices Pipeline.” Current Protocols in Bioinformatics 43 (1): 11.10.1–33. https://doi.org/10.1002/0471250953.bi1110s43.\n\n\nWenger, Aaron M., Paul Peluso, William J. Rowell, Pi-Chuan Chang, Richard J. Hall, Gregory T. Concepcion, Jana Ebler, et al. 2019. “Accurate Circular Consensus Long-Read Sequencing Improves Variant Detection and Assembly of a Human Genome.” Nature Biotechnology 37 (10): 1155–62. https://doi.org/10.1038/s41587-019-0217-9.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll, and Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls Using DeepVariant and GLnexus.” Bioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam, and Ruibang Luo. 2022. “Symphonizing Pileup and Full-Alignment for Deep Learning-Based Long-Read Variant Calling.” Nature Computational Science 2 (12): 797–803. https://doi.org/10.1038/s43588-022-00387-x.\n\n\nZook, Justin M., Jennifer McDaniel, Nathan D. Olson, Justin Wagner, Hemang Parikh, Haynes Heaton, Sean A. Irvine, et al. 2019. “An Open Resource for Accurately Benchmarking Small Variant and Reference Calls.” Nature Biotechnology 37 (5): 561–66. https://doi.org/10.1038/s41587-019-0074-6.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sequencing: From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html",
    "href": "p1-ch02-data.html",
    "title": "2  The Genomic Data Landscape",
    "section": "",
    "text": "2.1 Why Genomic Data Resources Matter\nWe can sequence a human genome for a few hundred dollars and store the resulting terabytes of data for less than a month’s streaming subscription, yet most variants discovered in that genome remain uninterpretable. A clinical laboratory receiving a patient’s whole-genome sequence will identify four to five million positions that differ from the reference, but can confidently classify fewer than a hundred of those variants as medically actionable. The remaining millions occupy a gray zone where available evidence is insufficient to distinguish benign polymorphism from pathogenic mutation. This asymmetry between variant discovery and variant interpretation defines the central challenge that the data resources in this chapter were built to address.\nNo single dataset can resolve this gap. Instead, modern genomics depends on a mosaic of complementary resources: reference genomes and gene annotations that define coordinates and consequences, population variant catalogs that reveal what survives in healthy individuals, cohort and biobank datasets that link variation to phenotypes, functional genomics atlases that map biochemical activity, and clinical databases that aggregate expert interpretations. Each resource contributes a different type of evidence; together, they form the empirical foundation that deep learning models compress, combine, and learn from.\nThe models throughout this book inherit both the power and the limitations of these foundational resources. A variant effect predictor trained on ClinVar labels learns the biases embedded in clinical ascertainment. A chromatin accessibility model trained on ENCODE cell lines may not generalize to primary tissues absent from the training compendium. A constraint metric derived from European-ancestry cohorts will be less calibrated for variants private to other populations. Understanding what these resources contain, and what they systematically miss, is essential for interpreting what models learn and anticipating where they will fail.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#why-genomic-data-resources-matter",
    "href": "p1-ch02-data.html#why-genomic-data-resources-matter",
    "title": "2  The Genomic Data Landscape",
    "section": "",
    "text": "Warning\n\n\n\nTODO (chapter-level): - Somewhere in project, discuss correlated but distinct rare variants vs gene-lethal variants vs late-onset disease variants in more depth, ideally with a simple schematic. - Ensure consistent coverage of multi-species genomes and pangenomes (including Zoonomia-style mammalian constraint resources). - Coordinate where deep mutational scanning (DMS) / multiplexed assays (ProteinGym, TraitGym) live across chapters so this chapter and the protein-focused chapters reference each other cleanly.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: High-level “subway map” of the data landscape, showing how reads → reference genomes → variant catalogs → cohorts → functional genomics → expression → clinical resources connect, with arrows indicating where later chapters plug in (e.g., ?sec-reg, Chapter 3, Chapter 4).",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#reference-genomes-and-gene-annotations",
    "href": "p1-ch02-data.html#reference-genomes-and-gene-annotations",
    "title": "2  The Genomic Data Landscape",
    "section": "2.2 Reference Genomes and Gene Annotations",
    "text": "2.2 Reference Genomes and Gene Annotations\nA variant’s biological consequence depends entirely on where it falls in the genome, and “where” is defined by coordinates that only exist relative to a reference assembly. When we say a mutation disrupts the third exon of BRCA1, that statement presupposes agreement on which sequence is the reference, which positions constitute exon boundaries, and which transcript model defines the canonical gene structure. Reference genomes and gene annotations are so foundational that their assumptions often become invisible, yet every downstream analysis inherits the choices embedded in these resources. A model cannot learn about a regulatory element for a transcript that does not exist in the annotation.\n\n2.2.1 Reference Assemblies\nMost modern pipelines align reads to a small number of reference assemblies, predominantly GRCh38 or the newer T2T-CHM13 (Nurk et al. 2022). A reference genome is not simply a consensus sequence; it encodes a series of consequential decisions about how to represent duplications, alternate haplotypes, and unresolved gaps. The choice of reference determines which regions are mappable by short reads, how structural variants are represented, and how comparable results will be across cohorts built on different assemblies.\nGraph-based and pangenome references relax the assumption of a single linear reference, representing multiple haplotypes and ancestries within a unified coordinate system (Liao et al. 2023). Comparative multi-species references, such as those used in mammalian constraint maps from the Zoonomia consortium (Sullivan et al. 2023), extend this idea across species, providing evolutionary conservation scores that feed directly into deleteriousness predictors and gene-level constraint metrics.\nFor most datasets used in this book, the practical reality is still GRCh37 or GRCh38 coordinates, often with incremental patches. Models trained on these resources therefore inherit their blind spots: incomplete or collapsed segmental duplications, underrepresented ancestries in pangenome construction, and uneven quality across chromosomes and regions. These limitations concentrate in precisely the regions where variant interpretation matters most (such as the HLA locus, pharmacogenes with structural variation, and segmental duplications harboring disease genes), creating a systematic mismatch between clinical importance and reference quality.\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: Side-by-side schematic of (1) a linear reference, (2) a graph/pangenome reference, and (3) a conservation track derived from multi-species alignment, with callouts indicating how each affects variant calling and model labels.\n\n\n\n\n2.2.2 Gene Models\nGene annotation databases such as GENCODE and RefSeq define the biological vocabulary overlaid on reference coordinates: exon-intron structures, canonical and alternative transcripts, start and stop codons, and untranslated regions (Frankish et al. 2019; O’Leary et al. 2016). These annotations distinguish coding from non-coding variants, identify splice-disrupting mutations, and map functional genomics signals to genes. They also establish the units (genes, transcripts, exons) that downstream models implicitly operate on.\nThe MANE Select project provides a single matched transcript per protein-coding gene that is identical between GENCODE and RefSeq, simplifying clinical interpretation and variant reporting (Morales et al. 2022). This standardization makes variant descriptions consistent across laboratories, yet it privileges a single isoform over biological complexity. In contexts where tissue-specific or developmentally regulated isoforms drive disease (alternative splicing in muscular dystrophies, isoform-specific expression in neuropsychiatric conditions), the canonical transcript may miss the relevant biology.\nIn practice, new isoforms continue to be discovered, alternative splicing remains incompletely cataloged, and cell-type-specific transcripts may be absent from bulk-derived annotations. Non-coding RNA genes and pseudogenes are even more unevenly annotated. These gaps propagate through every tool built on them: variant effect predictors cannot score consequences for transcripts that do not exist in their reference annotation, and expression models cannot predict isoforms they were never trained on.\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: Genome-browser-style panel showing (1) reference sequence, (2) two different transcript models for the same gene (GENCODE vs RefSeq), and (3) the MANE Select transcript, with a highlighted variant whose consequence changes depending on the chosen transcript.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#population-variant-catalogs-and-allele-frequencies",
    "href": "p1-ch02-data.html#population-variant-catalogs-and-allele-frequencies",
    "title": "2  The Genomic Data Landscape",
    "section": "2.3 Population Variant Catalogs and Allele Frequencies",
    "text": "2.3 Population Variant Catalogs and Allele Frequencies\nPopulation variant catalogs answer a question that no amount of sequence analysis alone can resolve: has this variant been observed in healthy individuals, and at what frequency? Allele frequency, the proportion of chromosomes in a reference population carrying a given variant, serves as one of the most powerful priors in variant interpretation. A missense variant observed in 5% of a population is unlikely to cause severe early-onset disease; a variant never seen across hundreds of thousands of sequenced individuals demands closer scrutiny.\nBeyond simple filtering, allele frequencies inform statistical frameworks for case-control association, provide training signal for deleteriousness predictors, and enable imputation of ungenotyped variants through linkage disequilibrium. The catalogs described below have progressively expanded in sample size, ancestral diversity, and annotation depth, transforming variant interpretation from an ad hoc exercise into a quantitative discipline.\nA crucial nuance shapes everything that follows: these catalogs record variants that are compatible with being sampled in the first place. Gene-lethal variants that cause embryonic death or severe childhood disease rarely appear, even when they are biologically informative. Variants causing late-onset conditions (Alzheimer’s risk alleles, adult-onset cancer predisposition) can persist at appreciable frequencies because selection has not had time to remove them. Throughout this book, models trained on population data can only learn from variants present in these catalogs, which means they systematically underrepresent the most severe loss-of-function mutations.\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: Allele-frequency spectrum for (1) common polymorphisms, (2) rare but observed variants, and (3) hypothetical gene-lethal variants that never appear, annotated with examples of disease categories and how different filters are applied in clinical interpretation.\n\n\n\n2.3.1 dbSNP and the Variant Universe\nThe database of Single Nucleotide Polymorphisms (dbSNP) historically aggregated known single nucleotide polymorphisms (SNPs) and short insertions/deletions into a single catalog, providing stable identifiers (rsIDs) that serve as common currency across tools and publications (Sherry et al. 2001). Modern whole-exome and whole-genome sequencing routinely discovers millions of previously unseen variants per large cohort, but dbSNP identifiers remain the standard way to reference known polymorphisms and link disparate resources. When a GWAS publication reports an association at rs12345, that identifier traces back to dbSNP and enables integration with functional annotations, clinical databases, and other catalogs.\n\n\n2.3.2 1000 Genomes and Early Reference Panels\nThe 1000 Genomes Project provided one of the first widely used multi-population reference panels, sampling individuals from African, European, East Asian, South Asian, and admixed American populations (Auton et al. 2015). Its haplotype structure underlies many imputation servers and downstream analyses, enabling genotyping arrays with millions of markers to impute tens of millions of untyped variants through linkage disequilibrium (Yun et al. 2021). Although its sample size (approximately 2,500 individuals) is modest by current standards, 1000 Genomes established the template for how to build and distribute multi-population reference panels, and its samples continue to serve as benchmarks for variant calling performance.\n\n\n2.3.3 The Genome Aggregation Database (gnomAD)\nThe Genome Aggregation Database aggregates exome and genome sequencing data from research and clinical cohorts worldwide into harmonized allele frequency resources spanning hundreds of thousands of individuals (Karczewski et al. 2020). gnomAD provides high-resolution allele frequencies stratified by genetic ancestry, enabling population-matched filtering that accounts for variants common in one ancestry but rare in others.\nBeyond frequencies, gnomAD introduced constraint metrics that have become standard features in variant prioritization. The probability of loss-of-function intolerance (pLI) and loss-of-function observed/expected upper bound fraction (LOEUF) summarize how depleted a gene is for protein-truncating variants relative to expectation. Genes essential for viability show far fewer loss-of-function variants than neutral mutation rates would predict; this depletion provides evidence of selective constraint that transfers to variant interpretation. A novel truncating variant in a highly constrained gene warrants more concern than the same variant class in an unconstrained gene.\nThese resources are indispensable for filtering common variants in Mendelian disease diagnostics, distinguishing ultra-rare variants from recurrent ones, and providing population genetics priors for deleteriousness scores like CADD (Rentzsch et al. 2019; Schubach et al. 2024). At the same time, they reflect the composition of the cohorts they aggregate: ancestry representation remains uneven despite ongoing efforts, structural variants and repeat expansions are less completely cataloged than SNVs and short indels, and individuals with severe early-onset disease are underrepresented by design. These biases propagate into every model that uses gnomAD frequencies or constraint scores as features.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#cohorts-biobanks-and-gwas-summary-data",
    "href": "p1-ch02-data.html#cohorts-biobanks-and-gwas-summary-data",
    "title": "2  The Genomic Data Landscape",
    "section": "2.4 Cohorts, Biobanks, and GWAS Summary Data",
    "text": "2.4 Cohorts, Biobanks, and GWAS Summary Data\nVariant interpretation at the population level requires linking genetic variation to phenotypes at scale. A variant’s association with disease risk, its effect on a quantitative trait, or its role in drug response can only be discovered when genotypes and phenotypes are measured together across thousands or hundreds of thousands of individuals. Yet assembling such cohorts introduces its own biases: participants must consent, provide samples, and have phenotypes recorded in standardized ways. The populations enrolled in major biobanks reflect patterns of healthcare access, research infrastructure, and historical priorities that do not represent global genetic diversity.\nThe overrepresentation of European-ancestry individuals in most major biobanks creates systematic gaps in variant discovery, effect-size estimation, and polygenic score portability that propagate through downstream analyses (Sirugo, Williams, and Tishkoff 2019). A variant common in West African populations may be absent or rare in European-dominated catalogs, rendering it invisible to association studies and underrepresented in predictive models. These ancestry biases, and strategies for addressing them, are discussed in detail in Chapter 22.\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: Flow diagram for a typical GWAS/PGS pipeline: cohort enrollment → genotyping/sequencing → QC → imputation → association testing → summary statistics → polygenic scores, with callouts for where bias enters (e.g., ancestry composition).\n\n\n\n2.4.1 Large Population Cohorts\nUK Biobank, with approximately 500,000 participants and deep phenotyping across thousands of traits, has become a dominant resource for methods development and benchmarking (Bycroft et al. 2018). FinnGen leverages Finland’s population history and unified healthcare records for large-scale disease association discovery (Kurki et al. 2023). The All of Us Research Program prioritizes diversity, aiming to enroll one million participants with deliberate oversampling of historically underrepresented groups (All of Us Research Program Investigators 2019). Additional resources include the Million Veteran Program, Mexican Biobank, BioBank Japan, China Kadoorie Biobank, and emerging African genomics initiatives such as H3Africa (Sirugo, Williams, and Tishkoff 2019).\nTogether, these efforts enable genome-wide association studies (GWAS) for thousands of traits, development and evaluation of polygenic scores, and fine-mapping of causal variants and genes (Marees et al. 2018; Mountjoy et al. 2021). From a modeling perspective, they provide the large-scale genotype-phenotype matrices that power architectures ranging from classical linear mixed models to foundation models trained on biobank-scale data. The practical reality for most GWAS and polygenic score methods in Chapter 3 is data from either array genotyping with imputation or whole-exome/whole-genome sequencing with joint calling, as in DeepVariant/GLnexus-style pipelines (Yun et al. 2021).\n\n\n2.4.2 GWAS Summary Statistics\nBeyond individual-level data, many resources distribute GWAS summary statistics: per-variant effect sizes, standard errors, and p-values aggregated across cohorts. The GWAS Catalog compiles published results across thousands of traits (Sollis et al. 2023), while the PGS Catalog provides curated polygenic score weights and metadata for reproducibility (Lambert et al. 2021). Frameworks like Open Targets Genetics integrate fine-mapped signals with functional annotations to prioritize candidate causal genes at associated loci (Mountjoy et al. 2021).\nSummary statistics enable meta-analysis across cohorts without sharing individual-level data, transfer of genetic findings to new populations through methods like PRS-CSx, and integration with functional annotations to distinguish causal variants from linked bystanders. For deep learning, summary statistics provide a sparse, trait-level view of the genome that can be combined with richer sequence-based or functional labels, though the sparsity and noise in GWAS signals pose challenges that differ from the dense labels available in functional genomics.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#functional-genomics-and-regulatory-landscapes",
    "href": "p1-ch02-data.html#functional-genomics-and-regulatory-landscapes",
    "title": "2  The Genomic Data Landscape",
    "section": "2.5 Functional Genomics and Regulatory Landscapes",
    "text": "2.5 Functional Genomics and Regulatory Landscapes\nProtein-coding exons constitute roughly 1.5% of the human genome, yet most disease-associated variants from GWAS fall outside coding regions. Understanding these non-coding variants requires mapping the regulatory logic that governs when, where, and how much each gene is expressed. Functional genomics assays provide this map: identifying transcription factor binding sites, nucleosome positioning, chromatin accessibility, histone modifications, and three-dimensional genome organization across cell types and conditions.\nFor this book, functional genomics datasets serve a dual role. First, they supply the biological vocabulary for interpreting non-coding variants, linking sequence changes to potential regulatory consequences. Second, and more directly, they provide the training labels for sequence-to-function deep learning models. When a model learns to predict chromatin accessibility or histone marks from DNA sequence alone, it compresses into its parameters the regulatory code implicit in thousands of functional genomics experiments.\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: Multi-track genome browser view showing DNA sequence, chromatin accessibility, histone marks, and transcription factor ChIP-seq peaks across two cell types, annotated to show how a sequence window becomes a multi-task training label vector for a model like DeepSEA.\n\n\n\n2.5.1 ENCODE, Roadmap, and Related Consortia\nThe Encyclopedia of DNA Elements (ENCODE) and Roadmap Epigenomics consortia designed coordinated experimental campaigns that profiled transcription factor binding (ChIP-seq), histone modifications, chromatin accessibility (DNase-seq, ATAC-seq), and chromatin conformation (Hi-C) across cell lines and primary tissues (Kagda et al. 2025; Kundaje et al. 2015). Gene Expression Omnibus (GEO) archives these and many other functional genomics datasets with standardized metadata (Edgar, Domrachev, and Lash 2002).\nThe significance of these consortia lies less in any individual experiment than in the scale and standardization they provide. By generating hundreds of assays across dozens of cell types with consistent protocols, ENCODE and Roadmap created canonical reference datasets that define the regulatory landscape for the cell types they profiled. Many regulatory deep learning models in ?sec-reg and ?sec-splice are effectively trained on these resources, learning to predict multi-task label vectors where each task corresponds to a ChIP-seq or accessibility experiment.\n\n\n2.5.2 The Cistrome Data Browser\nENCODE and Roadmap provide authoritative datasets for their chosen cell types and factors, but they represent only a fraction of publicly available functional genomics experiments. The Cistrome Data Browser addresses this gap by aggregating thousands of human and mouse ChIP-seq and chromatin accessibility datasets from ENCODE, Roadmap, GEO, and individual publications into a uniformly reprocessed repository (Zheng et al. 2019). All datasets pass through standardized quality control and peak calling, enabling comparisons across experiments originally generated with different protocols.\nCistrome provides uniform peak calls, signal tracks, and metadata for cell type, factor, and experimental conditions. The tradeoff is heterogeneity: while reprocessing harmonizes computational steps, the underlying experiments vary in sample preparation, antibody quality, sequencing depth, and experimental design. Cistrome expands coverage at the cost of the tight experimental control found in the primary consortia, a tradeoff that matters when models learn from noisy or inconsistent labels.\n\n\n2.5.3 From Assays to Training Labels\nSequence-to-function models transform functional genomics resources into supervised learning problems. Models like DeepSEA (see ?sec-reg) draw training labels from ENCODE, Roadmap, and Cistrome-style datasets: each genomic window is associated with binary or quantitative signals indicating transcription factor binding, histone modifications, or chromatin accessibility across hundreds of assays and cell types (Zhou and Troyanskaya 2015; Zhou et al. 2018).\nThe quality, coverage, and biases of these labels directly constrain what models can learn. Cell types absent from the training compendium cannot be predicted reliably. Factors with few high-quality ChIP-seq experiments will have noisier labels. Systematic differences between assay types (binary peak calls versus quantitative signal tracks) shape whether models learn to predict occupancy, accessibility, or something in between. These considerations become central when examining model architectures and training strategies in ?sec-reg.\n\n\n2.5.4 Deep Mutational Scanning and Multiplexed Variant Assays\nA parallel ecosystem has emerged for deep mutational scanning (DMS) and other multiplexed assays that measure the fitness or functional impact of thousands of protein or regulatory variants in a single experiment. Benchmarks such as ProteinGym compile large DMS datasets across proteins to evaluate variant effect predictors (Notin et al. 2023), while TraitGym curates multiplexed reporter assays and other high-throughput readouts of regulatory variant effects (Benegas, Eraslan, and Song 2025).\nThese resources sit at the interface between genomic and protein-level modeling. They provide dense, quantitative labels for synthetic or near-saturated variant libraries, complementing the sparse, naturally occurring variation in gnomAD and biobanks. DMS data differ fundamentally from population catalogs: they measure functional impact directly under controlled conditions rather than inferring it from population survival. Later chapters on protein sequence models and regulatory variant prediction return to these DMS-style datasets as key benchmarks and training sources.\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: Cartoon of a DMS/multiplexed assay pipeline: mutagenesis → pooled library → selection/screen → sequencing → fitness score per variant, with arrows indicating how these are aggregated into ProteinGym/TraitGym-style benchmarks.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#expression-and-eqtl-resources",
    "href": "p1-ch02-data.html#expression-and-eqtl-resources",
    "title": "2  The Genomic Data Landscape",
    "section": "2.6 Expression and eQTL Resources",
    "text": "2.6 Expression and eQTL Resources\nFunctional genomics assays reveal where transcription factors bind and which chromatin regions are accessible, but they do not directly answer the downstream question: does regulatory activity actually change how much RNA a gene produces? Expression datasets complete this link, measuring transcript abundance across tissues, cell types, and genetic backgrounds.\nExpression quantitative trait loci (eQTLs) formalize the genotype-expression relationship statistically, identifying genetic variants associated with changes in transcript levels. For variant interpretation, eQTLs offer mechanistic hypotheses connecting non-coding variants to specific genes and tissues: if a GWAS signal colocalizes with an eQTL for a nearby gene in a disease-relevant tissue, that gene becomes a candidate effector. For model training, expression data provide quantitative labels that integrate across many regulatory inputs converging on a single promoter.\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: Schematic of eQTL mapping: genotype matrix + expression matrix → association scan → locus plot linking variant, regulatory element, and target gene, with tissue labels.\n\n\n\n2.6.1 Bulk Expression Atlases\nThe Genotype-Tissue Expression (GTEx) consortium provides the most comprehensive resource linking genetic variation to gene expression across human tissues, with RNA-seq profiles from 948 post-mortem donors across 54 tissues (The GTEx Consortium 2020). GTEx established foundational insights that inform models throughout this book: most genes harbor tissue-specific eQTLs, regulatory variants typically act in cis over distances of hundreds of kilobases, and expression variation explains a meaningful fraction of complex trait heritability.\nGTEx underlies expression prediction models such as PrediXcan, which trains tissue-specific models to impute gene expression from genotypes alone (Gamazon et al. 2015). Transcriptome-wide association studies (TWAS) extend this idea to associate imputed expression with phenotypes (Gusev et al. 2016). Colocalization methods ask whether a GWAS signal and an eQTL share the same causal variant, providing evidence that the associated gene mediates the trait effect.\nThe GTEx design has limitations. Post-mortem collection introduces agonal stress artifacts that may not reflect living tissue biology. Sample sizes vary considerably across tissues (hundreds for some, dozens for others), affecting statistical power. Some disease-relevant tissues, such as pancreatic islets or specific brain subregions, remain undersampled. Complementary resources like the eQTLGen Consortium aggregate eQTL results from blood across much larger sample sizes, trading tissue diversity for statistical power (Võsa et al. 2021).\n\n\n2.6.2 Single-Cell and Context-Specific Expression\nBulk RNA-seq averages expression across all cells in a tissue sample, obscuring the cell-type-specific programs that often mediate disease biology. A bulk eQTL in brain tissue might reflect astrocytes, neurons, microglia, or oligodendrocytes, and the causal cell type matters for understanding mechanism. Single-cell RNA-seq resolves this heterogeneity, identifying expression signatures for individual cell types, rare populations, and transitional states.\nLarge-scale efforts including the Human Cell Atlas and Tabula Sapiens are building reference atlases that catalog cell types across organs and developmental stages (Regev et al. 2017; The Tabula Sapiens Consortium 2022). For variant interpretation, single-cell data enable cell-type-specific eQTL mapping, revealing that a variant may influence expression in one cell type but not others within the same tissue. Spatial transcriptomics adds anatomical context, preserving tissue architecture while measuring gene expression.\nThese technologies introduce computational challenges: sparsity from dropout effects, batch variation across samples and technologies, and massive scale with millions of cells per study. They also offer an increasingly fine-grained view of the link between genotype, regulatory state, and cellular phenotype. Single-cell and spatial resources appear primarily in later chapters on multi-omics integration and systems-level models.\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: Pair of panels: (1) bulk expression averaging across mixed cell types vs (2) a UMAP of single-cell profiles colored by cell type, with an inset showing a cell-type-specific eQTL that would be invisible in bulk data.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#variant-interpretation-databases-and-clinical-labels",
    "href": "p1-ch02-data.html#variant-interpretation-databases-and-clinical-labels",
    "title": "2  The Genomic Data Landscape",
    "section": "2.7 Variant Interpretation Databases and Clinical Labels",
    "text": "2.7 Variant Interpretation Databases and Clinical Labels\nAllele frequencies tell us what variants survive in healthy populations, and functional genomics data reveal where the genome is biochemically active, but neither directly answers the clinical question: is this variant pathogenic? That determination requires integrating multiple lines of evidence (family segregation, functional assays, computational predictions, phenotypic observations) into a structured framework that can be applied consistently.\nClinical variant interpretation databases aggregate these assessments from laboratories, expert panels, and research groups, providing labels that inform diagnostic decisions and serve as training data for machine learning models. These databases have become critical infrastructure for both clinical genomics and computational method development, though their labels carry biases and circularity that propagate through any analysis built on them.\n\n2.7.1 ClinVar and Related Resources\nClinVar aggregates assertions of variant pathogenicity from clinical laboratories and researchers worldwide (Landrum et al. 2018). It provides standardized classifications following ACMG/AMP guidelines (pathogenic, likely pathogenic, benign, likely benign, variant of uncertain significance) that are central to diagnostic pipelines and to benchmarking variant effect predictors.\n\n2.7.1.1 Strengths and Limitations of ClinVar\nClinVar has become the de facto reference for variant pathogenicity labels, but its contents reflect systematic biases that affect any downstream use.\nSubmission heterogeneity poses a fundamental challenge. Annotations come from diverse submitters, including diagnostic laboratories, research groups, expert panels, and database exports. Submitters apply varying evidentiary standards; some provide detailed supporting evidence while others offer only assertions. Conflicting interpretations are common, particularly for variants of uncertain significance (VUS).\nVersion sensitivity means that classifications evolve as evidence accumulates. A variant classified as VUS in 2018 may be reclassified as likely pathogenic by 2023 based on new functional studies or additional patient observations. Models trained on historical ClinVar snapshots may learn outdated classifications. When reporting performance, specifying the ClinVar version used is essential for reproducibility.\nAncestry and gene coverage biases create uneven representation. Variants in well-studied populations (particularly European ancestry) and well-characterized disease genes are heavily overrepresented. Variants from underrepresented populations are more likely to remain classified as VUS due to insufficient evidence. This creates feedback loops: predictive models perform better on European-ancestry variants because training data is richer, reinforcing the disparity (Landrum et al. 2018).\nCircularity with computational predictors represents a subtle but important concern. Clinical submissions increasingly incorporate computational scores like CADD, REVEL, and AlphaMissense as supporting evidence for pathogenicity classification. When these same ClinVar labels are then used to train or evaluate computational predictors, circularity emerges (Schubach et al. 2024). If a laboratory used a high CADD score as supporting evidence for classifying a variant as likely pathogenic, and that variant later appears as a positive label in ClinVar, models trained on ClinVar may partly learn to reproduce CADD itself rather than discovering independent signal.\nThis circularity operates at two levels. Evaluation circularity occurs when models are assessed on benchmarks influenced by the model’s own predictions. Training circularity occurs when features used in training derive from the same underlying information as the labels. Both forms inflate apparent performance without demonstrating genuine predictive power. We return to these issues in Chapter 4 and Chapter 22.\nVariants of uncertain significance (VUS) constitute the majority of rare variant classifications, reflecting genuinely limited evidence. These variants are both targets for predictive modeling (can computational methods resolve uncertainty?) and potential pitfalls (models trained only on confidently classified variants may not generalize to VUS with different characteristics).\nDespite these limitations, ClinVar remains invaluable. The key is using it appropriately: recognizing biases when training models, accounting for version differences when comparing studies, stratifying performance by ancestry and gene coverage, and treating computational predictions as one line of evidence rather than definitive classifications.\n\n\n\n2.7.2 ClinGen and Expert Curation\nThe Clinical Genome Resource (ClinGen) complements ClinVar by providing expert-curated assessments at multiple levels (Rehm et al. 2015). ClinGen expert panels evaluate gene-disease validity (whether variation in a gene can cause a specific disease) and dosage sensitivity (whether haploinsufficiency or triplosensitivity leads to clinical phenotypes). These evaluations build on the catalog of Mendelian phenotypes maintained by OMIM, which provides curated gene-disease associations and clinical synopses (Amberger et al. 2015).\nFor individual variants, ClinGen Variant Curation Expert Panels apply ACMG/AMP criteria systematically, assigning levels of evidence for pathogenicity or benignity. The FDA has recognized these curations as valid scientific evidence for clinical validity (Pejaver et al. 2022). ClinGen also develops calibrated thresholds for computational predictors, specifying score intervals that justify different strengths of evidence (supporting, moderate, strong) for pathogenicity or benignity. These calibrations directly inform how computational scores should be incorporated into variant classification workflows.\n\n\n\n\n\n\nWarning\n\n\n\nVisual suggestion: Decision-graph for variant interpretation showing evidence types (population frequency, functional assays, computational scores, segregation, pharmacogenomics) converging into ClinVar/ClinGen/ClinPGx labels, with circularity highlighted (e.g., CADD used both as input evidence and training target).\n\n\n\n\n2.7.3 ClinPGx and Pharmacogenomics Resources\nClinPGx integrates the PharmGKB knowledge base, CPIC clinical guidelines, and PharmCAT annotation tool into a unified pharmacogenomics resource (Whirl-Carrillo et al. 2012). While most variant interpretation databases focus on rare disease-causing mutations, pharmacogenomics curates gene-drug associations that influence metabolism, efficacy, and adverse reactions.\nThese pharmacogenomic variants differ from typical pathogenic mutations: many are common polymorphisms rather than rare deleterious alleles, but their clinical importance for prescribing decisions makes them a distinct category of actionable genetic variation. The CYP2D6 gene, for example, encodes an enzyme responsible for metabolizing approximately 25% of clinically used drugs, including codeine, tamoxifen, and many antidepressants (Whirl-Carrillo et al. 2012). Star-allele haplotypes (combinations of variants that travel together on a chromosome) determine metabolizer status, requiring phasing and structural variant detection that extend beyond simple SNV calling.\nThe CPIC guidelines provide evidence-based recommendations for adjusting drug selection or dosing based on pharmacogene diplotypes, and FDA drug labels document regulatory recognition of these associations. From a modeling perspective, pharmacogenomic resources offer a complementary type of label linking variants to molecular and clinical outcomes through different mechanisms than Mendelian disease pathogenicity.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch02-data.html#how-later-chapters-use-these-resources",
    "href": "p1-ch02-data.html#how-later-chapters-use-these-resources",
    "title": "2  The Genomic Data Landscape",
    "section": "2.8 How Later Chapters Use These Resources",
    "text": "2.8 How Later Chapters Use These Resources\nThe genomic deep learning models throughout this book inherit both the strengths and limitations of the data they are trained on. Chapter 3 draws on GWAS summary statistics and biobank-scale cohorts to construct polygenic scores, inheriting the ancestry biases embedded in cohort composition. Chapter 4 examines how annotation-based methods compress population frequencies, conservation, and functional signals into genome-wide deleteriousness scores, propagating the biases of each input. Chapters ?sec-reg and ?sec-splice use ENCODE, Roadmap, and Cistrome-style functional data as training labels for sequence-to-function models, learning representations limited by the cell types and assays in the training compendium. Chapters 10 through 19 revisit these resources as inputs, labels, and priors for genomic foundation models.\nBy surveying the data landscape in one place, this chapter establishes a common reference that later chapters build on rather than re-introducing each resource from scratch. The recurring theme is that biases, gaps, and circularity in these foundational datasets propagate through every model trained on them. A variant effect predictor trained on ClinVar labels inherits the ascertainment biases of clinical sequencing. A chromatin model trained on ENCODE cell lines may not generalize to primary tissues. A constraint model trained on human populations systematically misses gene-lethal variants that never appear in any catalog. Understanding these foundations is essential for interpreting what models learn and anticipating where they will fail.\n\n\n\n\nAll of Us Research Program Investigators, All of Us; 2019. “The ‘All of Us’ Research Program.” New England Journal of Medicine 381 (7): 668–76. https://doi.org/10.1056/NEJMsr1809937.\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F. Scott, and Ada Hamosh. 2015. “OMIM.org: Online Mendelian Inheritance in Man (OMIM®), an Online Catalog of Human Genes and Genetic Disorders.” Nucleic Acids Research 43 (D1): D789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAuton, Adam, Gonçalo R. Abecasis, David M. Altshuler, Richard M. Durbin, Gonçalo R. Abecasis, David R. Bentley, Aravinda Chakravarti, et al. 2015. “A Global Reference for Human Genetic Variation.” Nature 526 (7571): 68–74. https://doi.org/10.1038/nature15393.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025. “[TraitGym] Benchmarking DNA Sequence Models for Causal Regulatory Variant Prediction in Human Genetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nEdgar, Ron, Michael Domrachev, and Alex E. Lash. 2002. “Gene Expression Omnibus: NCBI Gene Expression and Hybridization Array Data Repository.” Nucleic Acids Research 30 (1): 207–10. https://doi.org/10.1093/nar/30.1.207.\n\n\nFrankish, Adam, Mark Diekhans, Anne-Maud Ferreira, Rory Johnson, Irwin Jungreis, Jane Loveland, Jonathan M Mudge, et al. 2019. “GENCODE Reference Annotation for the Human and Mouse Genomes.” Nucleic Acids Research 47 (D1): D766–73. https://doi.org/10.1093/nar/gky955.\n\n\nGamazon, Eric R., Heather E. Wheeler, Kaanan P. Shah, Sahar V. Mozaffari, Keston Aquino-Michaels, Robert J. Carroll, Anne E. Eyler, et al. 2015. “A Gene-Based Association Method for Mapping Traits Using Reference Transcriptome Data.” Nature Genetics 47 (9): 1091–98. https://doi.org/10.1038/ng.3367.\n\n\nGusev, Alexander, Arthur Ko, Huwenbo Shi, Gaurav Bhatia, Wonil Chung, Brenda W. J. H. Penninx, Rick Jansen, et al. 2016. “Integrative Approaches for Large-Scale Transcriptome-Wide Association Studies.” Nature Genetics 48 (3): 245–52. https://doi.org/10.1038/ng.3506.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data Navigation on the ENCODE Portal.” Nature Communications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. “Integrative Analysis of 111 Reference Human Epigenomes.” Nature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nKurki, Mitja I., Juha Karjalainen, Priit Palta, Timo P. Sipilä, Kati Kristiansson, Kati M. Donner, Mary P. Reeve, et al. 2023. “FinnGen Provides Genetic Insights from a Well-Phenotyped Isolated Population.” Nature 613 (7944): 508–18. https://doi.org/10.1038/s41586-022-05473-8.\n\n\nLambert, Samuel A., Laurent Gil, Simon Jupp, Scott C. Ritchie, Yu Xu, Annalisa Buniello, Aoife McMahon, et al. 2021. “The Polygenic Score Catalog as an Open Database for Reproducibility and Systematic Evaluation.” Nature Genetics 53 (4): 420–25. https://doi.org/10.1038/s41588-021-00783-5.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness, Glenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome Reference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nMarees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan, Emmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018. “[GWAS] A Tutorial on Conducting Genome-Wide Association Studies: Quality Control and Statistical Analysis.” International Journal of Methods in Psychiatric Research 27 (2): e1608. https://doi.org/10.1002/mpr.1608.\n\n\nMorales, Joannella, Shashikant Pujar, Jane E. Loveland, Alex Astashyn, Ruth Bennett, Andrew Berry, Eric Cox, et al. 2022. “A Joint NCBI and EMBL-EBI Transcript Set for Clinical Genomics and Research.” Nature 604 (7905): 310–15. https://doi.org/10.1038/s41586-022-04558-8.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy Schwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021. “An Open Approach to Systematically Prioritize Causal Variants and Genes at All Published Human GWAS Trait-Associated Loci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. “ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design.” Advances in Neural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V. Bzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The Complete Sequence of a Human Genome.” Science 376 (6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Leary, Nuala A., Mathew W. Wright, J. Rodney Brister, Stacy Ciufo, Diana Haddad, Rich McVeigh, Bhanu Rajput, et al. 2016. “Reference Sequence (RefSeq) Database at NCBI: Current Status, Taxonomic Expansion, and Functional Annotation.” Nucleic Acids Research 44 (D1): D733–45. https://doi.org/10.1093/nar/gkv1189.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel, Sean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022. “Calibration of Computational Tools for Missense Variant Pathogenicity Classification and ClinGen Recommendations for PP3/BP4 Criteria.” American Journal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nRegev, Aviv, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe Benoist, Ewan Birney, Bernd Bodenmiller, et al. 2017. “The Human Cell Atlas.” Edited by Thomas R Gingeras. eLife 6 (December): e27041. https://doi.org/10.7554/eLife.27041.\n\n\nRehm, Heidi L., Jonathan S. Berg, Lisa D. Brooks, Carlos D. Bustamante, James P. Evans, Melissa J. Landrum, David H. Ledbetter, et al. 2015. “ClinGen — The Clinical Genome Resource.” New England Journal of Medicine 372 (23): 2235–42. https://doi.org/10.1056/NEJMsr1406261.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nSherry, S. T., M.-H. Ward, M. Kholodov, J. Baker, L. Phan, E. M. Smigielski, and K. Sirotkin. 2001. “dbSNP: The NCBI Database of Genetic Variation.” Nucleic Acids Research 29 (1): 308–11. https://doi.org/10.1093/nar/29.1.308.\n\n\nSirugo, Giorgio, Scott M. Williams, and Sarah A. Tishkoff. 2019. “The Missing Diversity in Human Genetic Studies.” Cell 177 (1): 26–31. https://doi.org/10.1016/j.cell.2019.02.048.\n\n\nSollis, Elliot, Abayomi Mosaku, Ala Abid, Annalisa Buniello, Maria Cerezo, Laurent Gil, Tudor Groza, et al. 2023. “The NHGRI-EBI GWAS Catalog: Knowledgebase and Deposition Resource.” Nucleic Acids Research 51 (D1): D977–85. https://doi.org/10.1093/nar/gkac1010.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N. Phan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023. “Leveraging Base-Pair Mammalian Constraint to Understand Genetic Variation and Human Disease.” Science 380 (6643): eabn2937. https://doi.org/10.1126/science.abn2937.\n\n\nThe GTEx Consortium. 2020. “The GTEx Consortium Atlas of Genetic Regulatory Effects Across Human Tissues.” Science 369 (6509): 1318–30. https://doi.org/10.1126/science.aaz1776.\n\n\nThe Tabula Sapiens Consortium. 2022. “The Tabula Sapiens: A Multiple-Organ, Single-Cell Transcriptomic Atlas of Humans.” Science 376 (6594): eabl4896. https://doi.org/10.1126/science.abl4896.\n\n\nVõsa, Urmo, Annique Claringbould, Harm-Jan Westra, Marc Jan Bonder, Patrick Deelen, Biao Zeng, Holger Kirsten, et al. 2021. “Large-Scale Cis- and Trans-eQTL Analyses Identify Thousands of Genetic Loci and Polygenic Scores That Regulate Blood Gene Expression.” Nature Genetics 53 (9): 1300–1310. https://doi.org/10.1038/s41588-021-00913-z.\n\n\nWhirl-Carrillo, M, E M McDonagh, J M Hebert, L Gong, K Sangkuhl, C F Thorn, R B Altman, and T E Klein. 2012. “Pharmacogenomics Knowledge for Personalized Medicine.” Clinical Pharmacology & Therapeutics 92 (4): 414–17. https://doi.org/10.1038/clpt.2012.96.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll, and Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls Using DeepVariant and GLnexus.” Bioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei Sun, Chen-Hao Chen, et al. 2019. “Cistrome Data Browser: Expanded Datasets and New Tools for Gene Regulatory Analysis.” Nucleic Acids Research 47 (D1): D729–35. https://doi.org/10.1093/nar/gky1094.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Genomic Data Landscape</span>"
    ]
  },
  {
    "objectID": "p1-ch03-gwas.html",
    "href": "p1-ch03-gwas.html",
    "title": "3  GWAS and Polygenic Scores",
    "section": "",
    "text": "3.1 The GWAS Framework\nA human genome contains roughly five million common variants that differ from the reference sequence. Genome-wide association studies have now linked thousands of these variants to hundreds of diseases and traits, from height and blood pressure to schizophrenia and coronary artery disease. We can compute polygenic scores that aggregate these associations into a single number per person, rank individuals by genetic susceptibility, and identify those in the top percentiles of risk. Yet for most associated variants, we cannot explain how they influence biology. The statistical machinery is mature; the mechanistic understanding lags far behind.\nThis asymmetry between predictive power and explanatory depth defines the central tension in statistical genetics. GWAS excel at finding genomic regions that harbor trait-relevant variation, but association alone cannot distinguish causal variants from their correlated neighbors, cannot explain which genes or pathways are affected, and cannot predict whether a score trained in one population will transfer to another. Polygenic scores can stratify risk, but they are fundamentally correlational summaries rather than mechanistic models. When a patient asks why their score is high, or a clinician asks which intervention might help, GWAS and PGS provide only partial answers.\nThis chapter establishes the statistical foundations that underpin much of modern human genetics. We examine how GWAS identify associated regions through millions of parallel hypothesis tests, why linkage disequilibrium makes it difficult to pinpoint causal variants, how fine-mapping attempts to resolve this ambiguity, and how polygenic scores aggregate many small effects into clinically relevant predictions. Throughout, we confront the limitations that motivate the mechanistic approaches developed in later parts of this book: the failure of scores to transfer across ancestries, the difficulty of interpreting noncoding associations, and the fundamental gap between statistical association and biological mechanism.\nConsider a clinician counseling a patient about cardiovascular disease risk. Traditional risk factors (age, smoking, cholesterol, blood pressure) explain perhaps half the variation in who develops disease. Family history suggests that genetics contributes substantially to the remainder, but which genetic variants matter, and how much does each contribute? GWAS provide a systematic approach to answering these questions by testing each of millions of variants for association with the trait of interest.\nThe core logic is straightforward. For each variant in turn, we ask whether individuals carrying more copies of a particular allele tend to have higher or lower values of the phenotype (for quantitative traits) or higher or lower probability of disease (for binary outcomes). We estimate an effect size, compute a test statistic under the null hypothesis of no association, and record a p-value. After testing millions of variants, we identify those that exceed a stringent significance threshold, report the associated loci, and attempt to interpret which genes and pathways might be involved.\nThis apparently simple procedure requires careful attention to study design, quality control, and statistical modeling. The phenotype must be measured consistently across individuals. The genotypes must be accurate and the variants well-defined. Confounders that correlate with both genotype and phenotype (most notably population structure) must be controlled. Multiple testing across millions of variants demands stringent significance thresholds. Only after addressing these challenges can GWAS results be trusted and translated into downstream applications.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-gwas.html#the-gwas-framework",
    "href": "p1-ch03-gwas.html#the-gwas-framework",
    "title": "3  GWAS and Polygenic Scores",
    "section": "",
    "text": "3.1.1 Association Models for Quantitative Traits\nFor continuous phenotypes such as height, LDL cholesterol, or blood pressure, the standard approach is linear regression. Let \\(y_i\\) denote the phenotype for individual \\(i\\), and let \\(g_{ij}\\) denote the genotype dosage at variant \\(j\\), encoded as 0, 1, or 2 copies of the alternative allele (or as a fractional value for imputed genotypes). The model is:\n\\[\ny_i = \\alpha + \\beta_j g_{ij} + \\gamma^\\top c_i + \\varepsilon_i\n\\]\nThe coefficient \\(\\beta_j\\) represents the expected change in phenotype per additional copy of the alternative allele, holding covariates \\(c_i\\) fixed. When phenotypes are standardized to zero mean and unit variance, \\(\\beta_j\\) is expressed in standard deviation units per allele. The vector \\(c_i\\) typically includes age, sex, genotyping batch, and principal components capturing ancestry (discussed below). The residual \\(\\varepsilon_i\\) captures unexplained variation, assumed to be independent and identically distributed across individuals.\nFor each variant, we compute a test statistic for the null hypothesis \\(H_0: \\beta_j = 0\\). In large samples, the t-statistic follows approximately a standard normal distribution under the null, yielding a two-sided p-value. With \\(M\\) variants tested (typically \\(10^6\\) to \\(10^7\\) after imputation), we must correct for multiple comparisons. The conventional genome-wide significance threshold of \\(5 \\times 10^{-8}\\) approximates a Bonferroni correction for roughly one million effectively independent tests, accounting for correlation among variants due to linkage disequilibrium (Pe’er et al. 2008).\n\n\n3.1.2 Association Models for Disease Outcomes\nFor binary phenotypes (disease present or absent), logistic regression replaces linear regression. The model relates genotype to the log-odds of disease:\n\\[\n\\log \\frac{P(y_i = 1)}{P(y_i = 0)} = \\alpha + \\beta_j g_{ij} + \\gamma^\\top c_i\n\\]\nHere \\(\\beta_j\\) is the log-odds ratio per allele, and \\(\\exp(\\beta_j)\\) gives the odds ratio (OR). An odds ratio of 1.2 means that each additional copy of the alternative allele increases the odds of disease by 20%. For rare diseases, odds ratios approximate relative risks, but the distinction matters for common conditions and when communicating absolute risk.\nCase-control sampling, in which cases are enriched relative to their population frequency, distorts absolute risk estimates but preserves the validity of odds ratio estimation. This is why GWAS conducted in case-control designs can still produce effect sizes useful for polygenic scores, provided downstream applications account for baseline disease incidence.\n\n\n3.1.3 Controlling for Population Structure\nPopulation structure poses a fundamental challenge to GWAS interpretation. If allele frequencies differ systematically across subpopulations and the phenotype also varies across these groups for non-genetic reasons (differences in environment, diet, healthcare access, socioeconomic status), naive association testing will detect variants that mark ancestry rather than causal biology.\nPrincipal component analysis on the genotype matrix captures the major axes of genetic variation across individuals (Price et al. 2006; Patterson, Price, and Reich 2006). The leading principal components often correspond to continental ancestry gradients or finer-scale population structure within a study. Including these PCs as covariates in the regression model attenuates spurious associations driven by ancestry stratification.\nThis correction is imperfect. Subtle structure not captured by the included PCs, cryptic relatedness among individuals, and the interweaving of genetic ancestry with environmental exposures all complicate interpretation. The challenges extend far beyond technical statistical adjustment: ancestry is entangled with healthcare access, environmental exposures, and socioeconomic factors in ways that simple covariate correction cannot fully resolve. These issues become critical when translating GWAS results to clinical applications and when evaluating whether polygenic scores perform equitably across populations. We return to the full complexity of ancestry as a confounder in Chapter 22.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-gwas.html#linkage-disequilibrium-and-the-association-causation-gap",
    "href": "p1-ch03-gwas.html#linkage-disequilibrium-and-the-association-causation-gap",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.2 Linkage Disequilibrium and the Association-Causation Gap",
    "text": "3.2 Linkage Disequilibrium and the Association-Causation Gap\nGWAS test variants one at a time, but the genome is not inherited one variant at a time. Nearby variants travel together on haplotypes and are co-inherited across generations except when recombination separates them. This correlation structure, known as linkage disequilibrium (LD), is both essential to GWAS power and the source of their fundamental interpretive limitation.\nWhen a GWAS identifies a significant association at variant \\(j\\), three possibilities exist. The variant itself may be causal, directly influencing the phenotype through some molecular mechanism. Alternatively, variant \\(j\\) may simply be correlated with a nearby causal variant \\(k\\) due to LD, with the association signal reflecting this correlation rather than direct causation. In complex regions, multiple causal variants may exist, and the observed association pattern reflects their joint effects filtered through the local LD structure. Distinguishing these scenarios from GWAS summary statistics alone is often impossible.\n\n3.2.1 The Structure of Linkage Disequilibrium\nRecombination during meiosis shuffles genetic material between parental chromosomes. Over many generations, recombination breaks down long-range correlations between variants while preserving short-range structure. The result is a mosaic pattern: regions of high LD (haplotype blocks) where many variants are strongly correlated, interspersed with recombination hotspots where LD decays rapidly.\nThe squared correlation coefficient \\(r^2\\) quantifies LD between pairs of variants. When \\(r^2\\) approaches 1, the two variants are nearly always observed together on the same haplotypes; when \\(r^2\\) approaches 0, they segregate independently. From a GWAS perspective, if a causal variant \\(k\\) has strong association with the phenotype and variant \\(j\\) is in high LD with \\(k\\) (high \\(r^2\\)), then variant \\(j\\) will also show strong association even if it has no direct causal role.\nLD patterns vary across populations because demographic history (founder effects, bottlenecks, admixture, population expansion) shapes which haplotypes persist and at what frequencies. A variant that tags a causal allele effectively in one population may be a poor proxy in another where different recombination history has decoupled the correlation. This population-specificity of LD structure is one reason why polygenic scores fail to transfer across ancestries.\n\n\n3.2.2 Causal Variants, Tag Variants, and GWAS Catalogs\nA causal variant directly influences the phenotype, whether by altering protein sequence, disrupting transcription factor binding, affecting splicing, or modifying chromatin state. A tag variant is merely correlated with a causal variant through LD, serving as a statistical proxy without direct functional consequence.\nGWAS catalogs therefore report associated loci, not causal variants. The “lead SNP” at each locus (the variant with the smallest p-value) is often a tag rather than the causal variant, particularly when the causal variant is rare, poorly genotyped, or not present on the array. Even when a locus is robustly associated, dozens or hundreds of correlated variants may be statistically indistinguishable from the lead SNP.\nThis limitation has practical consequences. Drug development requires identifying causal genes and mechanisms, not just associated regions. Clinical variant interpretation needs to distinguish functional mutations from neutral passengers. Polygenic scores built on tag SNPs may lose power when applied to populations with different LD patterns. The gap between association and causation motivates the fine-mapping approaches we consider next.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-gwas.html#fine-mapping-from-loci-to-causal-variants",
    "href": "p1-ch03-gwas.html#fine-mapping-from-loci-to-causal-variants",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.3 Fine-Mapping: From Loci to Causal Variants",
    "text": "3.3 Fine-Mapping: From Loci to Causal Variants\nFine-mapping attempts to resolve the ambiguity created by LD, moving from “this region is associated” to “these specific variants are most likely causal.” The core insight is that while multiple variants may show similar marginal association statistics, their joint behavior under a model that accounts for LD can discriminate among them.\n\n3.3.1 The Statistical Framework\nBayesian fine-mapping methods approach the problem by specifying a prior distribution over which variants in a region might be causal, then computing posterior probabilities given the observed association statistics and local LD structure. The key outputs are posterior inclusion probabilities (PIPs), which estimate the probability that each variant is among the causal set, and credible sets, which are minimal sets of variants that contain the true causal variant(s) with specified probability (commonly 95%).\nThe procedure typically proceeds as follows. First, define a region around an index SNP, often all variants within 1 megabase. Second, specify a prior: perhaps at most \\(K\\) variants in the region are causal, and causal effect sizes follow some distribution. Third, use the observed marginal association statistics (effect sizes and standard errors) together with an LD matrix (correlations among variants) to compute the likelihood of the data under each possible configuration of causal variants. Fourth, sum over configurations to obtain the marginal PIP for each variant.\nVariants with high PIPs (say, above 0.5 or 0.9) are strong candidates for functional follow-up. Credible sets that contain few variants are more actionable than those containing dozens. The width of credible sets reflects both the strength of the association signal and the local LD structure: tight LD means many variants remain plausible even with strong statistical evidence.\n\n\n3.3.2 Leveraging Functional Annotations\nPure statistical fine-mapping uses only association statistics and LD. Annotation-informed approaches incorporate functional information to update priors. Variants in coding regions, promoters, enhancers, or regions of evolutionary constraint may be assigned higher prior probability of causality. Integration with chromatin accessibility, transcription factor binding, or expression quantitative trait loci (eQTL) data can further prioritize variants with plausible regulatory mechanisms.\nLarge-scale resources now link GWAS summary statistics, fine-mapping results, and functional genomic annotations across hundreds of traits and thousands of loci (Mountjoy et al. 2021). These datasets enable systematic identification of variants that are both statistically prioritized and functionally plausible.\n\n\n3.3.3 Multi-Ancestry Fine-Mapping\nBecause LD patterns differ across populations, fine-mapping in a single ancestry leaves substantial ambiguity. A variant in tight LD with many neighbors in Europeans may have fewer proxies in African-ancestry populations, where shorter LD blocks (reflecting larger historical effective population size) provide greater resolution.\nJoint fine-mapping across ancestries exploits these differences to narrow credible sets (Pasaniuc and Price 2016). When a variant remains strongly associated across populations despite different local LD structure, confidence in its causal role increases. Multi-ancestry approaches are increasingly important as large biobanks expand to include diverse populations, though they require careful attention to population-specific effect sizes and potential gene-environment interactions.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-gwas.html#polygenic-score-construction",
    "href": "p1-ch03-gwas.html#polygenic-score-construction",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.4 Polygenic Score Construction",
    "text": "3.4 Polygenic Score Construction\nPolygenic scores aggregate variant effects across the genome into a single number per individual:\n\\[\n\\text{PGS}_i = \\sum_{j} w_j g_{ij}\n\\]\nThe weight \\(w_j\\) reflects the estimated effect of variant \\(j\\), and \\(g_{ij}\\) is the genotype dosage for individual \\(i\\). The simplest approach uses GWAS effect size estimates directly as weights; more sophisticated methods adjust for LD, apply shrinkage, or incorporate fine-mapping information.\nThe clinical promise is substantial. For diseases with significant genetic components, PGS can identify individuals at elevated risk years or decades before disease onset, potentially enabling targeted screening or prevention. For quantitative traits, PGS explain a portion of individual variation that complements environmental factors. Yet the translation from statistical aggregation to clinical utility requires careful attention to how scores are constructed, validated, and interpreted.\n\n\n\n\n\n\nNoteTerminology: PGS versus PRS\n\n\n\nThe literature uses overlapping terminology. Polygenic risk score (PRS) is common in clinical contexts, emphasizing disease risk prediction. Polygenic score (PGS) is more general, encompassing both disease and quantitative trait prediction. Genomic risk score and related terms also appear, often interchangeably. This book uses PGS as the default, adding “risk” when specifically discussing disease outcomes. Methodological overviews provide detailed guidance on construction and evaluation (Choi, Mak, and O’Reilly 2020).\n\n\n\n3.4.1 Clumping and Thresholding\nThe simplest and historically most common approach is clumping and thresholding (C+T). The procedure involves three steps. First, clumping: rank variants by p-value, then iteratively select the most significant variant and remove all variants within a specified window (say, 250 kb) that are in LD above a threshold (say, \\(r^2 &gt; 0.1\\)). This yields a set of approximately independent index variants. Second, thresholding: apply a p-value cutoff and retain only variants below this threshold. Third, weighting: set \\(w_j\\) equal to the GWAS effect size estimate for retained variants, and zero otherwise.\nThe hyperparameters (LD window, \\(r^2\\) threshold, p-value threshold) are typically chosen by grid search to maximize predictive performance in a held-out validation set. This tuning introduces overfitting risk, particularly in small samples or when the validation population differs from the eventual deployment population.\nC+T is transparent and computationally simple, but it discards substantial information. Most variants are excluded, LD is handled only through coarse pruning, and variants with modest p-values that collectively explain meaningful variance may be entirely omitted. These limitations motivate more sophisticated approaches.\n\n\n3.4.2 LD-Aware Bayesian Methods\nA more principled framework models the joint distribution of effect sizes explicitly, treating the true effects \\(\\beta = (\\beta_1, \\ldots, \\beta_M)\\) as random variables drawn from a prior distribution. Given GWAS summary statistics and an LD reference panel, these methods infer posterior mean effect sizes that serve as PGS weights.\nLDpred assumes that a fraction \\(p\\) of variants have nonzero effects drawn from a Gaussian distribution, while the remainder have zero effect (Vilhjálmsson et al. 2015). The method uses GWAS summary statistics and LD from a reference panel to compute approximate posterior effect sizes. These posteriors shrink noisy estimates toward zero, borrow strength across correlated variants, and generally outperform C+T when properly tuned.\nPRS-CS extends this framework by placing a continuous shrinkage prior on effect sizes, which better accommodates the highly polygenic architecture of complex traits and reduces sensitivity to the sparsity hyperparameter (ge_prs-cs_2019?). The method has shown strong performance across a range of traits and ancestries, though like all methods it requires an LD reference that reasonably matches the target population.\nRelated approaches (lassosum, SBayesR, and others) use different priors or optimization strategies but share the core insight: jointly modeling effect sizes under LD yields better predictions than pruning LD away. Performance differences among methods are often modest when each is well-tuned, and the choice may depend on computational resources, availability of validation data, and specific trait architecture.\n\n\n3.4.3 Fine-Mapping-Informed Scores\nFine-mapping outputs, particularly posterior inclusion probabilities, provide another basis for PGS construction. Variants with high PIPs are more likely to be causal and may therefore be more robust to LD differences across populations. Two strategies are common: selection, where only variants above a PIP threshold contribute to the score, and weighting, where PIPs modulate the contribution of each variant.\nFine-mapping-informed approaches aim to concentrate weight on variants that are biologically meaningful rather than merely statistically associated. In principle, this should improve cross-ancestry transferability since causal variants remain causal regardless of population-specific LD patterns. In practice, gains depend on fine-mapping resolution, and the approaches remain an active area of methodological development.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-gwas.html#interpreting-polygenic-scores",
    "href": "p1-ch03-gwas.html#interpreting-polygenic-scores",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.5 Interpreting Polygenic Scores",
    "text": "3.5 Interpreting Polygenic Scores\nA polygenic score is a number. Converting that number into actionable information requires understanding what it represents, how it relates to risk or trait values, and where its interpretation breaks down.\n\n3.5.1 Relative Risk and Percentiles\nPGS are most naturally interpreted in relative terms. For a disease outcome, we might fit a logistic regression in a validation cohort:\n\\[\n\\log \\frac{P(y_i = 1)}{P(y_i = 0)} = \\alpha + \\theta \\cdot \\text{PGS}_i + \\eta^\\top z_i\n\\]\nwhere \\(z_i\\) contains covariates and \\(\\theta\\) captures the effect of the PGS. After standardizing the score to unit variance, \\(\\exp(\\theta)\\) gives the odds ratio per standard deviation of the PGS. This metric allows statements like “individuals one standard deviation above the mean have 1.5-fold higher odds of disease.”\nPercentile-based communication is common in clinical contexts. The risk for individuals in the top 1% or 5% of the PGS distribution can be compared to those near the median or in the bottom percentiles. For some conditions (coronary artery disease, breast cancer, type 2 diabetes), individuals in the top percentiles have risk comparable to or exceeding that conferred by single high-penetrance mutations, making PGS potentially relevant for clinical risk stratification.\n\n\n3.5.2 Absolute Risk\nRelative risk statements can mislead when baseline risk varies. A 1.5-fold increase in odds for a disease with 1% baseline risk differs substantially in absolute terms from the same relative increase for a disease with 20% baseline risk. Clinical decision-making typically requires absolute risk estimates: the probability that this individual will develop disease over a specified time horizon.\nConverting PGS to absolute risk requires combining the score with baseline incidence rates, which vary by age, sex, and other factors. The hazard ratio per SD of PGS, combined with age-specific incidence curves, can yield personalized risk trajectories. Such calculations demand careful attention to calibration: the model must produce well-calibrated probabilities in the population where it will be deployed, not just the population where it was trained. Clinical deployment of PGS is addressed in ?sec-clinical-risk.\n\n\n3.5.3 Explained Variance and Discrimination\nHow much of trait variation does a PGS explain? For quantitative traits, the squared correlation between PGS and phenotype (\\(R^2\\)) provides a direct answer. For binary traits, the \\(R^2\\) on the liability scale (the underlying continuous risk) is more interpretable than the observed-scale \\(R^2\\), which depends on disease prevalence.\nArea under the receiver operating characteristic curve (AUC) measures discrimination: the probability that a randomly selected case has a higher PGS than a randomly selected control. AUC values of 0.5 indicate no discrimination (random guessing); values approaching 1.0 indicate near-perfect separation. For most complex diseases, PGS achieve AUC values in the 0.55 to 0.70 range when used alone, with incremental gains when combined with traditional risk factors.\nThese metrics characterize population-level performance but say little about individual prediction. Even a PGS that explains 10% of trait variance leaves 90% unexplained by factors genetic and environmental. High-risk individuals by PGS may never develop disease; low-risk individuals may be affected. PGS provide probabilistic risk stratification, not deterministic prediction.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-gwas.html#ancestry-portability-and-fairness",
    "href": "p1-ch03-gwas.html#ancestry-portability-and-fairness",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.6 Ancestry, Portability, and Fairness",
    "text": "3.6 Ancestry, Portability, and Fairness\nThe vast majority of GWAS participants have been of European ancestry. This historical imbalance has profound consequences for who benefits from polygenic scores and who may be harmed by their limitations.\n\n3.6.1 The Portability Problem\nPolygenic scores derived from European-ancestry GWAS show markedly reduced performance in other populations. African-ancestry individuals typically experience 40% to 75% reductions in prediction accuracy compared to European-ancestry individuals, even for the same trait measured in the same study (Duncan et al. 2019). The pattern holds across traits and across methods, though the magnitude varies.\nSeveral factors contribute to this portability failure. LD structure differs across populations: tag SNPs that effectively proxy causal variants in Europeans may be poor proxies in populations with different recombination history. Allele frequencies differ: variants common in one population may be rare or absent in another. Effect sizes may genuinely differ across populations due to gene-environment interactions or genetic background effects. And GWAS in smaller non-European samples have less power to detect associations, yielding noisier effect estimates.\nMulti-ancestry GWAS and methods designed to leverage diverse training data offer partial solutions. Including multiple ancestries in discovery improves transferability, and methods that explicitly model ancestry-specific LD or effect sizes can enhance performance (Márquez-Luna et al. 2021). Yet even state-of-the-art approaches do not fully close the gap, and substantial research is needed before PGS perform equitably across populations.\n\n\n3.6.2 Fairness and Health Equity\nPortability failure is not merely a technical nuisance; it raises fundamental questions about fairness in precision medicine. If genomic models work primarily for individuals of European ancestry, deploying these models in diverse clinical populations risks exacerbating existing health disparities.\nConsider a scenario where PGS are used for risk-stratified screening. If the score identifies high-risk individuals more accurately in Europeans than in other groups, Europeans receive more targeted and efficient screening while others receive either under-screening (if falsely classified as low risk) or over-screening (if falsely classified as high risk). The benefits of precision medicine accrue disproportionately to those already overrepresented in research.\nThese challenges extend beyond PGS to every genomic model in this book. Foundation models can learn to exploit ancestry signals as shortcuts, achieving high benchmark performance while performing poorly on underrepresented groups. Aggregate performance metrics mask inequities across populations. Deployment in diverse clinical settings requires explicit evaluation of performance stratified by ancestry, along with transparent reporting of limitations. We examine these issues comprehensively in Chapter 22.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch03-gwas.html#from-association-to-mechanism",
    "href": "p1-ch03-gwas.html#from-association-to-mechanism",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.7 From Association to Mechanism",
    "text": "3.7 From Association to Mechanism\nGWAS and PGS have delivered remarkable achievements: thousands of robust trait associations, clinically useful risk scores for some conditions, and insights into the highly polygenic architecture of complex traits. Yet they have also exposed persistent gaps between statistical association and biological understanding.\nMost GWAS hits lie in noncoding regions, often within regulatory elements such as enhancers and promoters. The variant is associated; the mechanism is obscure. Fine-mapping can narrow the list of candidate variants but rarely identifies a single causal nucleotide. Even when a variant is confidently prioritized, the path from sequence change to molecular consequence to cellular phenotype to disease remains opaque.\nThis mechanistic gap limits translation. Drug development requires actionable targets, not just associated regions. Clinical variant interpretation needs to explain why a variant matters, not just that it correlates with disease. Polygenic scores provide population-level risk stratification but offer little guidance on individual intervention.\nThe models developed in subsequent parts of this book address this gap through different strategies. Regulatory sequence models (Chapter Chapter 13) predict how variants alter transcription factor binding, chromatin accessibility, and gene expression, providing mechanistic hypotheses for noncoding associations. Variant effect prediction methods (Chapter Chapter 14) combine foundation model representations with evolutionary information to assess likely functional impact. Multi-omics integration (Chapter Chapter 19) connects genetic variation to intermediate molecular phenotypes and ultimately to disease outcomes.\nThe goal is not to replace GWAS and PGS but to build on them. Statistical association provides a map of where trait-relevant variation resides; mechanistic models attempt to explain how that variation produces its effects. The combination of statistical genetics and mechanistic modeling offers the most promising path toward precision medicine that is both predictive and interpretable.\n\n\n\n\nChoi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O’Reilly. 2020. “[PRS] Tutorial: A Guide to Performing Polygenic Risk Score Analyses.” Nature Protocols 15 (9): 2759–72. https://doi.org/10.1038/s41596-020-0353-1.\n\n\nDuncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R. Peterson, and B. Domingue. 2019. “Analysis of Polygenic Risk Score Usage and Performance in Diverse Human Populations.” Nature Communications 10 (1): 3328. https://doi.org/10.1038/s41467-019-11112-0.\n\n\nMárquez-Luna, Carla, Steven Gazal, Po-Ru Loh, Samuel S. Kim, Nicholas Furlotte, Adam Auton, and Alkes L. Price. 2021. “Incorporating Functional Priors Improves Polygenic Prediction Accuracy in UK Biobank and 23andMe Data Sets.” Nature Communications 12 (1): 6052. https://doi.org/10.1038/s41467-021-25171-9.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy Schwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021. “An Open Approach to Systematically Prioritize Causal Variants and Genes at All Published Human GWAS Trait-Associated Loci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nPasaniuc, Bogdan, and Alkes L. Price. 2016. “Dissecting the Genetics of Complex Traits Using Summary Association Statistics.” Nature Reviews Genetics 18 (2): 117–27. https://doi.org/10.1038/nrg.2016.142.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006. “Population Structure and Eigenanalysis.” PLOS Genetics 2 (12): e190. https://doi.org/10.1371/journal.pgen.0020190.\n\n\nPe’er, Itsik, Roman Yelensky, David Altshuler, and Mark J. Daly. 2008. “Estimation of the Multiple Testing Burden for Genomewide Association Studies of Nearly All Common Variants.” Genetic Epidemiology 32 (4): 381–85. https://doi.org/10.1002/gepi.20303.\n\n\nPrice, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E. Weinblatt, Nancy A. Shadick, and David Reich. 2006. “Principal Components Analysis Corrects for Stratification in Genome-Wide Association Studies.” Nature Genetics 38 (8): 904–9. https://doi.org/10.1038/ng1847.\n\n\nVilhjálmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev, Sara Lindström, Stephan Ripke, Giulio Genovese, et al. 2015. “Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores.” American Journal of Human Genetics 97 (4): 576–92. https://doi.org/10.1016/j.ajhg.2015.09.001.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "p1-ch04-vep-classical.html",
    "href": "p1-ch04-vep-classical.html",
    "title": "4  Classical Variant Effect Prediction",
    "section": "",
    "text": "4.1 The Variant Prioritization Challenge\nA typical human genome contains approximately four to five million genetic variants relative to the reference assembly, encompassing single-nucleotide variants and short insertions and deletions. The vast majority of these are functionally neutral, representing the accumulated diversity of human evolution and population history. For any individual with a suspected genetic condition, the central interpretive challenge is to identify the handful of variants that plausibly contribute to disease from this enormous background of benign variation. Consider a child presenting with developmental delay and seizures whose exome sequencing reveals 25,000 variants from the reference genome: which ones, if any, explain the clinical presentation? This challenge of variant prioritization stands at the heart of clinical genomics.\nThe data resources surveyed in Chapter 2 provide multiple complementary views of variant function, each with distinct strengths and limitations. Population frequency databases such as gnomAD reveal which variants survive in large cohorts of ostensibly healthy individuals, offering a powerful filter for identifying rare, potentially deleterious alleles. Functional genomics consortia including ENCODE and the Roadmap Epigenomics Project indicate which genomic regions show evidence of biochemical activity across diverse cell types and developmental contexts. Clinical databases such as ClinVar and HGMD collect expert-curated variant classifications drawn from case reports and diagnostic laboratories, providing ground truth labels for known pathogenic and benign variants.\nEach of these sources is partial in important ways. Population databases are dominated by common variants, which are mostly tolerated by virtue of their high frequency. Rare and de novo variants, which are often most relevant for Mendelian disease, have sparse or no direct labels. Functional genomics data is inherently noisy and often context-specific: a region active in liver hepatocytes may be quiescent in neurons, and vice versa. Clinical databases are sparse and heavily biased toward well-studied genes and variant types, leaving vast swaths of the genome without reliable clinical annotations. The annotation density varies dramatically across the genome, with protein-coding exons densely labeled relative to deep intronic and intergenic sequences. Non-coding regions remain especially under-annotated despite harboring the majority of disease-associated variants identified by genome-wide association studies.\nThis asymmetry between variant abundance and interpretive capacity defines the variant effect prediction problem. Before deep learning transformed the field, computational approaches to this problem relied on feature engineering: human experts identified biologically meaningful signals, computed them for each variant, and combined them into predictive scores. Conservation across species, amino acid physicochemistry, proximity to splice sites, overlap with regulatory elements: each signal captures some aspect of variant function, but none is sufficient alone. The field’s response was to develop integrative methods that combine multiple weak signals into stronger predictions.\nThis chapter examines the pre-deep-learning approaches to variant effect prediction that established the conceptual foundations for modern methods. We trace the evolution from single-signal predictors (conservation scores, protein-level tools) through ensemble methods that integrate dozens of annotations into unified scores. Combined Annotation-Dependent Depletion (CADD) receives particular attention because it introduced design patterns that recur throughout genomic machine learning: proxy labels derived from evolutionary signals, large-scale training, integration of diverse features, and genome-wide precomputation for downstream reuse. Understanding these classical approaches illuminates both what they achieved and why the field ultimately moved toward learned representations.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p1-ch04-vep-classical.html#conservation-based-approaches",
    "href": "p1-ch04-vep-classical.html#conservation-based-approaches",
    "title": "4  Classical Variant Effect Prediction",
    "section": "4.2 Conservation-Based Approaches",
    "text": "4.2 Conservation-Based Approaches\nThe longest-preserved sequences in the genome are those where mutations have been consistently removed by natural selection across millions of years. This simple observation underlies conservation-based variant scoring: if a genomic position has remained unchanged across species separated by hundreds of millions of years of evolution, mutations at that position are likely to be deleterious. Conservation scores quantify this evolutionary constraint and provide some of the strongest signals for variant prioritization, particularly in non-coding regions where other functional annotations are sparse.\n\n4.2.1 Measuring Evolutionary Constraint\nConservation scores derive from multiple sequence alignments spanning diverse species. The human genome is aligned against dozens to hundreds of other vertebrate, mammalian, and more distantly related genomes, creating a matrix where each column represents homologous positions across species. At positions under purifying selection, the aligned bases show little variation; at neutral positions, substitutions accumulate at rates determined by mutational processes and genetic drift.\nPhyloP scores quantify the deviation of observed substitution rates from neutral expectation at individual positions (Siepel et al. 2005). The score is computed by comparing the observed pattern of bases at each alignment column against a neutral evolutionary model, typically fit to ancestral repeat sequences that are assumed to evolve without selective constraint. Positive phyloP scores indicate conservation (slower evolution than expected under neutrality), while negative scores indicate acceleration (faster evolution, potentially reflecting positive selection). A phyloP score of 2 indicates that the observed base is approximately 100-fold more conserved than expected under neutrality.\nGenomic Evolutionary Rate Profiling (GERP) takes a complementary approach by estimating “rejected substitutions” at each position: the number of substitutions that would have been expected under neutrality but are absent from the observed alignment (Davydov et al. 2010). Large positive GERP scores indicate strong constraint. For a position conserved across 30 mammalian species, a GERP score of 5 implies that approximately five substitutions were “rejected” by selection over mammalian evolution. This interpretation is intuitive but depends on accurate neutral rate estimation and alignment quality.\nPhastCons provides element-level rather than position-level conservation by identifying contiguous stretches of constrained sequence (Siepel et al. 2005). Using a hidden Markov model, phastCons classifies each position as belonging to a conserved or non-conserved state, then outputs the posterior probability of conservation. The resulting scores are smoother than position-level metrics, capturing functional elements that span multiple nucleotides even when individual positions show moderate conservation.\n\n\n4.2.2 Clinical Utility and Limitations\nConservation scores are particularly valuable for non-coding variant interpretation, where direct functional annotations are often incomplete or absent. A deeply conserved intronic position likely participates in splicing regulation, gene expression control, or other functional processes even if no explicit annotation overlaps it. In clinical variant interpretation, strong conservation provides computational evidence (PP3 criterion under ACMG-AMP guidelines) supporting pathogenicity.\nThe limitations of conservation-based approaches are equally important to recognize. First, conservation requires evolutionary time to accumulate signal. Recently evolved functional elements, including human-specific regulatory sequences and primate-specific genes, may show little conservation despite genuine function. Second, conservation reflects the aggregate of selective pressures across the species in the alignment, which may differ from the specific functional role in humans. A position conserved for its role in neural development across vertebrates may be less constrained for immune function, which evolves more rapidly. Third, lack of conservation does not prove neutrality; it may simply indicate rapid evolution under positive selection or lineage-specific function.\nConservation scores also face technical challenges from alignment quality. In repetitive regions, segmental duplications, and rapidly evolving gene families, reliable alignments may be impossible to construct, leaving conservation scores undefined or unreliable precisely where variant interpretation is most difficult. These technical limitations concentrate in genomically complex regions that are often clinically important, including the major histocompatibility complex and immunoglobulin loci.\nDespite these limitations, conservation remains among the most robust signals for variant effect prediction. It provides information that is largely orthogonal to population frequency (which reflects recent human history rather than deep evolutionary constraint) and to functional genomics annotations (which capture biochemical activity rather than selective importance). The integrative methods discussed later in this chapter combine conservation with these other signals to achieve better performance than any single source.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p1-ch04-vep-classical.html#protein-level-predictors",
    "href": "p1-ch04-vep-classical.html#protein-level-predictors",
    "title": "4  Classical Variant Effect Prediction",
    "section": "4.3 Protein-Level Predictors",
    "text": "4.3 Protein-Level Predictors\nFor variants that alter protein sequence, specialized tools assess the likely impact on protein function. These protein-level predictors emerged before genome-wide methods and remain widely used in clinical practice, either alone or as features within ensemble scores. The underlying premise is that amino acid substitutions disrupting protein structure or function are more likely to cause disease than those preserving biochemical properties.\n\n4.3.1 SIFT: Sequence Homology as Functional Constraint\nSorting Intolerant From Tolerant (SIFT) predicts whether an amino acid substitution affects protein function based on sequence homology (Ng and Henikoff 2003). The method collects homologous protein sequences from diverse species, constructs a multiple sequence alignment, and examines which amino acids appear at each position across the alignment. Positions that are highly conserved (showing the same or similar amino acids across species) are predicted to be functionally important; substitutions introducing amino acids not observed at that position are predicted to be deleterious.\nMechanistically, SIFT computes a normalized probability for each possible amino acid at each position based on the diversity observed in the alignment. The SIFT score for a substitution is the probability of observing the mutant amino acid, scaled by the position’s overall diversity. Scores range from 0 to 1, with low scores (typically below 0.05) indicating predicted damage. A SIFT score of 0.01 for a particular missense variant indicates that the mutant amino acid is rarely or never observed at that position across the sequence family, suggesting functional constraint.\nSIFT’s simplicity is both its strength and its limitation. The method requires only protein sequence information and a database of homologs; it makes no assumptions about protein structure, physicochemistry, or mechanism of damage. This generality allows application to any protein with sufficient homologs in sequence databases. However, SIFT captures only the evolutionary signal present in the alignment. For proteins with few homologs, young gene families, or positions with limited alignment depth, SIFT predictions may be unreliable.\n\n\n4.3.2 PolyPhen-2: Integrating Structure and Sequence\nPolymorphism Phenotyping (PolyPhen-2) extends sequence-based prediction by incorporating protein structure features and amino acid physicochemistry (Adzhubei et al. 2010). The method uses a naive Bayes classifier trained to distinguish disease-causing mutations from neutral polymorphisms based on a collection of sequence and structure-derived features.\nThe feature set includes sequence conservation (similar to SIFT), but adds several structural descriptors when three-dimensional structure data is available: solvent accessibility, secondary structure context, and proximity to known functional sites. Amino acid physicochemical properties (size, charge, hydrophobicity) inform predictions about whether substitutions are conservative or radical. The Grantham distance, a measure of biochemical dissimilarity between amino acid pairs, contributes to assessing substitution severity.\nPolyPhen-2 provides two models trained on different datasets: HumDiv, trained on disease-causing and neutral variants from protein sequence databases, and HumVar, trained on Mendelian disease mutations versus common human polymorphisms. The choice of training set affects score interpretation; HumVar produces more conservative predictions appropriate for clinical Mendelian disease variant classification, while HumDiv is more sensitive and may be preferable for research applications.\nPolyPhen-2 scores range from 0 to 1, with higher scores indicating greater predicted deleteriousness. The output includes qualitative classifications (benign, possibly damaging, probably damaging) based on score thresholds. A PolyPhen-2 score of 0.95 with a “probably damaging” classification indicates high confidence that the substitution disrupts protein function, though the clinical significance depends on additional evidence about the specific disease context.\n\n\n4.3.3 Limitations of Protein-Level Prediction\nSeveral fundamental limitations constrain all protein-level predictors. First, they are restricted to missense variants; nonsense, frameshift, splice-altering, and non-coding variants lie outside their scope. Second, they predict impact on protein function without specifying the mechanism or clinical consequence. A variant predicted to damage function might impair enzymatic activity, disrupt protein folding, eliminate a binding interface, or alter stability. The clinical relevance depends on which function is affected and whether the phenotype results from loss or gain of function. Third, protein-level predictors provide no information about inheritance mode, penetrance, or expressivity. A strongly predicted-damaging variant in a gene with high tolerance to heterozygous loss may be clinically benign in carriers.\nThese tools also inherit the training data biases present in their underlying databases. Disease mutations in training sets are enriched for severe, early-onset Mendelian conditions with clear inheritance patterns. Variants causing subtle effects, incomplete penetrance, or complex phenotypes may be systematically mispredicted. The well-studied genes that dominate training data may not generalize to poorly characterized genes where variants are most difficult to interpret.\nDespite these limitations, SIFT and PolyPhen-2 remain widely used in clinical practice and serve as features within more sophisticated ensemble methods. Their scores appear in diagnostic reports, contribute to ACMG-AMP classification criteria, and inform variant prioritization in research and clinical pipelines. Understanding their construction and limitations is essential for appropriate interpretation.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p1-ch04-vep-classical.html#the-cadd-framework",
    "href": "p1-ch04-vep-classical.html#the-cadd-framework",
    "title": "4  Classical Variant Effect Prediction",
    "section": "4.4 The CADD Framework",
    "text": "4.4 The CADD Framework\nCombined Annotation-Dependent Depletion (CADD) represented a fundamental shift in variant effect prediction (Kircher et al. 2014; Rentzsch et al. 2019). Rather than focusing on a single signal or variant type, CADD defined a general framework for genome-wide variant prioritization that integrates dozens of heterogeneous annotations and uses evolutionary depletion as a proxy training label. The key insight was to avoid training directly on small sets of known pathogenic versus benign variants, which are scarce and biased toward certain genes and variant types. Instead, CADD contrasts variants that have survived purifying selection in the human lineage with matched simulated variants that could have occurred but did not. This evolutionary proxy strategy yields an enormous training set, enables genome-wide coverage, and produces scores that generalize across coding and non-coding regions alike.\n\n4.4.1 Evolutionary Proxy Training\nThe most important conceptual contribution of CADD was reframing variant effect prediction as a large-scale machine learning problem with labels derived from evolutionary signal rather than clinical curation. Rather than relying solely on curated clinical labels, CADD uses evolution as a weak supervisory signal, constructing two proxy classes of variants.\nThe proxy-neutral class consists of variants that have been tolerated by purifying selection. CADD draws these from sequence differences that arose on the human lineage since the split from chimpanzees and became fixed or nearly fixed in modern humans. These derived human alleles are present at very high frequency in human populations yet absent from the inferred human-chimp ancestral sequence. Because they have persisted over millions of years of evolution, most are presumed to be neutral or only weakly deleterious. This is not a perfect proxy: some observed alleles are genuinely pathogenic, particularly those with incomplete penetrance, late onset, or context-dependent effects. However, the proxy-neutral class is, on average, substantially enriched for tolerated alleles relative to a random sample of possible mutations.\nThe proxy-deleterious class is constructed by simulating mutations across the genome according to realistic mutational processes. The simulation matches local sequence context, typically using trinucleotide frequencies to capture the strong dependence of mutation rates on flanking bases. CpG dinucleotides, for example, have elevated mutation rates due to spontaneous deamination of methylated cytosines, and the simulation accounts for this by generating more CpG transitions. Regional variation in mutation rates, driven by factors including replication timing and chromatin state, is similarly incorporated.\nThe logic underlying this construction is subtle but powerful. Simulated variants represent changes that could plausibly occur under human mutational processes but are generally not observed at high frequency in population databases. The proxy-deleterious class as a whole is enriched for alleles disfavored by selection, because the set of possible mutations includes many that disrupt conserved elements, alter protein function, or perturb regulatory sequences. By contrasting this set with the proxy-neutral class, CADD learns to recognize the annotation signatures that distinguish variants under purifying selection from those that have been tolerated.\n\n\n4.4.2 Feature Integration\nCADD’s second conceptual pillar is the integration of many weak, noisy annotations into a single composite score. Where earlier variant effect predictors typically relied on one or a few signals, CADD combines more than 60 features. This integrative approach recognizes that no single annotation captures the full complexity of variant function.\nGene model annotations describe the local transcript and coding context of each variant. The most fundamental is the predicted sequence consequence: whether a variant is synonymous, missense, nonsense, frameshift, splice-site disrupting, or located in untranslated or intronic regions. Distance to exon-intron boundaries and proximity to canonical splice sites provide additional context. Gene-level attributes including constraint metrics (pLI, LOEUF from gnomAD) quantify how tolerant each gene is to damaging variation.\nConservation features from phyloP, GERP, and phastCons provide the evolutionary signals described earlier in this chapter. By incorporating multiple conservation metrics computed from different alignments and using different methodologies, CADD captures complementary aspects of evolutionary constraint.\nProtein-level predictions from SIFT and PolyPhen-2 contribute assessments of amino acid substitution impact for coding variants. Amino acid physicochemical properties, Grantham distances, and domain annotations from databases like Pfam provide additional structural context.\nRegulatory annotations derived from ENCODE and Roadmap Epigenomics data capture chromatin accessibility, histone modifications, and transcription factor binding. These features help prioritize non-coding variants that disrupt active regulatory regions.\nAdditional features capture local sequence context (GC content, CpG density), genomic architecture (segmental duplications, repetitive elements), and chromosomal position. The power of CADD lies in learning how to weight and combine these heterogeneous signals, upweighting annotations that distinguish proxy-deleterious from proxy-neutral variants and downweighting those that do not.\n\n\n4.4.3 Model Architecture and Scoring\nCADD’s classifier operates on the high-dimensional feature vector assembled for each variant. The original CADD model uses a linear support vector machine trained to discriminate proxy-neutral and proxy-deleterious variants based on approximately 30 million training examples. The choice of a linear model was deliberate and pragmatic: with tens of millions of training examples and dozens of features, a linear SVM is computationally tractable while capturing the main structure of the classification problem.\nRaw CADD scores are not directly interpretable as probabilities or biological effect sizes. To provide a more intuitive scoring system, CADD defines PHRED-scaled scores based on the rank of each variant among all possible single-nucleotide substitutions in the reference genome. A scaled score of 10 indicates that a variant falls in the top 10% of predicted deleteriousness. A score of 20 indicates the top 1%, and a score of 30 indicates the top 0.1%. This rank-based transformation ensures comparability across CADD versions and provides immediate interpretability.\nIn rare disease pipelines, CADD scaled scores are commonly used as filters to enrich for potentially pathogenic variants. Typical thresholds range from 15 (top 3%) to 20 (top 1%) or higher. Variants with scores at or above 20 are considered moderately high deleteriousness candidates, while scores at or above 30 are frequently interpreted as strongly enriched for functional impact. These filters serve as prioritization tools that reduce the variant burden to a manageable number for expert review rather than as definitive pathogenicity calls.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p1-ch04-vep-classical.html#other-ensemble-methods",
    "href": "p1-ch04-vep-classical.html#other-ensemble-methods",
    "title": "4  Classical Variant Effect Prediction",
    "section": "4.5 Other Ensemble Methods",
    "text": "4.5 Other Ensemble Methods\nCADD established the paradigm of integrative scoring, but several other ensemble methods have achieved widespread use in clinical and research settings. These approaches differ in their training data, feature sets, and target variant types, offering complementary perspectives on variant deleteriousness.\n\n4.5.1 REVEL\nRare Exome Variant Ensemble Learner (REVEL) represents a missense-specific ensemble predictor widely used in clinical laboratories (Ioannidis et al. 2016). Rather than training on evolutionary proxy labels like CADD, REVEL directly discriminates pathogenic missense variants (curated from HGMD and other disease databases) from rare putatively neutral missense variants observed in population datasets.\nREVEL integrates predictions from a panel of individual tools: SIFT, PolyPhen-2, PROVEAN, MutationAssessor, FATHMM, GERP++, phyloP, and phastCons, among others. A random forest model learns to combine these scores, weighting each according to its discriminative power for the pathogenic versus neutral classification. The training set is carefully constructed to avoid label contamination, excluding variants present in both disease and population databases.\nREVEL scores range from 0 to 1, with higher values implying greater pathogenicity likelihood. Common interpretation thresholds treat scores above 0.5 as supporting evidence for pathogenicity, with scores above 0.75 providing stronger evidence. REVEL is restricted to missense single-nucleotide variants, making it more specialized than CADD but often more accurate within its scope.\n\n\n4.5.2 M-CAP\nMendelian Clinically Applicable Pathogenicity (M-CAP) addresses specifically the challenge of distinguishing pathogenic from benign rare missense variants in Mendelian disease contexts (jagadeesh_m-cap_2016?). The method uses gradient boosting on a feature set including conservation scores, protein structure features, and amino acid properties.\nM-CAP was explicitly designed to minimize false positives while maintaining reasonable sensitivity, recognizing that clinical variant interpretation requires high specificity. The authors tuned their classifier to achieve less than 5% false positive rate on known pathogenic variants, accepting some reduction in sensitivity as a tradeoff. This design philosophy differs from methods that balance sensitivity and specificity equally, reflecting M-CAP’s intended use in diagnostic settings where false positive pathogenicity calls have serious consequences.\n\n\n4.5.3 Comparison and Selection\nNo single ensemble method dominates across all variant types and clinical contexts. CADD provides the broadest coverage (genome-wide, all variant types) but may sacrifice accuracy within specific variant classes to achieve this generality. REVEL often outperforms CADD on missense-only benchmarks, reflecting its focused training objective. M-CAP prioritizes specificity over sensitivity, appropriate for clinical settings where avoiding false positives is paramount.\nClinical variant interpretation typically incorporates multiple computational scores rather than relying on any single predictor. Different scores may agree (providing stronger evidence) or disagree (flagging variants requiring careful manual review). Understanding the construction, training data, and intended use case of each method is essential for appropriate interpretation.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p1-ch04-vep-classical.html#circularity-and-ascertainment-bias",
    "href": "p1-ch04-vep-classical.html#circularity-and-ascertainment-bias",
    "title": "4  Classical Variant Effect Prediction",
    "section": "4.6 Circularity and Ascertainment Bias",
    "text": "4.6 Circularity and Ascertainment Bias\nTwo pervasive problems affect variant effect predictors and their evaluation: circularity between scores and clinical databases, and ascertainment bias in the variants available for training and testing. These issues do not invalidate classical scores but require careful attention in both development and application.\n\n4.6.1 The Circularity Problem\nClinVar and similar clinical databases increasingly incorporate computational predictions as evidence supporting variant classification. When a clinical laboratory classifies a variant as pathogenic, the CADD score, PolyPhen-2 prediction, or other computational evidence may have contributed to that determination. If CADD is subsequently evaluated on ClinVar pathogenic variants, its performance is artificially inflated: the benchmark contains variants that were labeled partly because CADD assigned them high scores.\nThis circularity operates through multiple pathways. Direct use occurs when clinical laboratories explicitly cite computational scores in their classifications. Indirect influence arises when computational predictions shape clinical suspicion, affecting which variants receive functional testing or expert review. Selection bias in benchmark construction can compound the problem: benchmark creators may preferentially include variants with strong computational evidence, excluding ambiguous cases that would provide a more stringent test.\nThe consequence is that benchmark performance may overestimate real-world utility. A method that has been widely adopted will appear to perform well on benchmarks populated by variants classified using that method, even if its true discriminative power is more limited. This concern applies to all established computational tools, including conservation scores, protein-level predictors, and ensemble methods.\nAddressing circularity requires careful benchmark construction. Temporal holdouts (using only classifications made before a method’s widespread adoption) can reduce but not eliminate the problem. Functional assays that directly measure variant effects provide ground truth independent of computational predictions but are available for only a small fraction of variants. Prospective evaluation on newly classified variants offers the cleanest test but requires patience and ongoing data collection.\n\n\n4.6.2 Ascertainment Bias\nThe variants available for training and evaluating predictors represent a biased sample of all possible disease-causing mutations. Clinical databases are dominated by variants in well-studied genes, particularly those causing severe Mendelian phenotypes with clear inheritance patterns. Protein-coding variants are overrepresented because they are easier to interpret and more often tested. Variants in genes associated with common diagnostic panels appear frequently; those in rarely tested genes are sparse.\nThis ascertainment bias shapes what models learn and how they perform. A predictor trained predominantly on variants in constrained genes may learn that gene-level constraint is the primary signal for pathogenicity. When applied to variants in less constrained genes (where pathogenic variants also occur, but less frequently), the model may systematically underestimate risk. Similarly, models trained on European-ancestry samples may encode population-specific patterns that transfer poorly to other populations.\nThe consequences extend to evaluation. Benchmark variants inherit the ascertainment biases of their source databases. Strong performance on benchmark sets may not translate to the rare genes, unusual variant types, or underrepresented populations encountered in real clinical practice. Variants that are “easy” to classify (stop-gains in highly constrained genes, for example) are overrepresented in benchmarks, while diagnostically challenging variants (missense variants in moderate-constraint genes, non-coding variants) are underrepresented.\n\n\n4.6.3 Implications for Clinical Use\nThese limitations do not render classical scores useless, but they counsel appropriate humility in interpretation. Computational predictions provide one line of evidence among several in variant interpretation. Strong scores in expected directions support clinical suspicion; unexpected scores prompt careful review. No computational score should override clear clinical or functional evidence, and borderline scores in complex cases may warrant agnosticism rather than confident prediction.\nThe ACMG-AMP framework for variant classification appropriately treats computational predictions as supporting evidence (PP3 for predictions supporting pathogenicity, BP4 for predictions supporting benign status) rather than standalone criteria (richards_standards_2015?). Multiple lines of computational evidence may be combined, but the weight assigned should reflect the limitations outlined here. Variants classified primarily on computational grounds should be flagged for potential reclassification as additional evidence emerges.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p1-ch04-vep-classical.html#limitations-of-the-feature-engineering-paradigm",
    "href": "p1-ch04-vep-classical.html#limitations-of-the-feature-engineering-paradigm",
    "title": "4  Classical Variant Effect Prediction",
    "section": "4.7 Limitations of the Feature Engineering Paradigm",
    "text": "4.7 Limitations of the Feature Engineering Paradigm\nClassical variant effect prediction achieved substantial success in prioritizing potentially pathogenic variants and established conceptual foundations that persist in modern methods. The integration of diverse annotations, use of evolutionary signals as proxy labels, and genome-wide precomputation all anticipate contemporary practices. Nevertheless, the feature engineering paradigm faces fundamental limitations that motivated the shift toward learned representations.\n\n4.7.1 The Feature Ceiling\nManually designed features encode human knowledge about which genomic properties matter for variant function. Conservation scores capture evolutionary constraint; protein-level predictors encode structural intuitions; regulatory annotations mark biochemically active regions. This encoded knowledge is valuable but necessarily incomplete. Biologists cannot specify all relevant patterns in advance, and the interactions between features may be too complex for simple combination rules to capture.\nThe performance of feature-engineered methods is therefore bounded by the quality and completeness of the features themselves. Adding more features provides diminishing returns as the most informative signals are exhausted. Interactions between features (a variant in a conserved enhancer within a constrained gene) may require explicit specification or rely on simple combination rules that miss nonlinear relationships.\n\n\n4.7.2 Limited Context\nClassical features typically describe variants in isolation or with minimal context. Conservation scores examine each position independently. Protein-level predictors consider amino acid substitutions without full protein context. Gene-level features apply uniformly across entire genes regardless of position-specific effects. This limited context prevents classical methods from learning the complex sequence patterns that determine variant effects.\nA variant disrupting a critical transcription factor binding motif may escape detection if the motif is not annotated, even though the underlying sequence pattern is learnable from data. A missense variant at a protein-protein interface may be more damaging than one in a loop region, but capturing this distinction requires understanding protein structure and interactions that feature engineering incompletely represents.\n\n\n4.7.3 The Path Forward\nThese limitations motivated the deep learning approaches covered in subsequent chapters. Convolutional neural networks learn local sequence patterns directly from data, discovering regulatory motifs without requiring pre-specification. Transformers extend this learning to long-range dependencies spanning thousands of bases. Foundation models pretrained on massive sequence databases learn representations that capture protein structure, evolutionary relationships, and regulatory grammar from sequence alone.\nThe transition from CADD to these modern approaches represents a shift from encoding knowledge as features to learning representations from data. Yet classical methods remain valuable as baselines, as interpretable components in hybrid systems, and as reminders that the variant effect prediction problem is fundamentally difficult regardless of methodology. The circularity and ascertainment issues that affect CADD equally affect deep learning methods; learned representations do not automatically overcome training data limitations.\nChapter 14 examines how foundation models have advanced variant effect prediction, comparing their performance to classical methods and addressing the persistent challenges that affect all approaches. Understanding the achievements and limitations of classical methods provides essential context for evaluating these newer approaches and for the ongoing challenge of variant interpretation in clinical genomics.\n\n\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky, Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev. 2010. “A Method and Server for Predicting Damaging Missense Mutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper, Arend Sidow, and Serafim Batzoglou. 2010. “Identifying a High Fraction of the Human Genome to Be Under Selective Constraint Using GERP++.” PLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nIoannidis, Nilah M., Joseph H. Rothstein, Vikas Pejaver, Sumit Middha, Shannon K. McDonnell, Saurabh Baheti, Anthony Musolf, et al. 2016. “REVEL: An Ensemble Method for Predicting the Pathogenicity of Rare Missense Variants.” The American Journal of Human Genetics 99 (4): 877–85. https://doi.org/10.1016/j.ajhg.2016.08.016.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory M. Cooper, and Jay Shendure. 2014. “A General Framework for Estimating the Relative Pathogenicity of Human Genetic Variants.” Nature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT: Predicting Amino Acid Changes That Affect Protein Function.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs, Minmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005. “[PhastCons] Evolutionarily Conserved Elements in Vertebrate, Insect, Worm, and Yeast Genomes.” Genome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "p2--principles.html",
    "href": "p2--principles.html",
    "title": "Part II: Core Principles",
    "section": "",
    "text": "The previous part surveyed the architectural landscape of deep learning for genomics, from convolutional sequence-to-function models through protein and DNA language models to hybrid architectures. Those chapters focused on how specific model families work and what capabilities they enable. This part steps back to examine the conceptual foundations that unite these approaches and distinguish foundation models from their task-specific predecessors.\nFive questions organize this part. First, how should genomic sequences be represented as input to neural networks? The choice of tokenization scheme profoundly shapes what patterns a model can discover and how efficiently it processes long sequences. Second, what mechanisms allow transformers to capture long-range dependencies in genomic data? The self-attention architecture that revolutionized natural language processing has become equally central to computational genomics, but its application to biological sequences requires careful adaptation. Third, what defines a foundation model in genomics, and how do we navigate the emerging ecosystem of pretrained models? Understanding this landscape is essential for practitioners who must choose among competing approaches. Fourth, how do models learn useful representations from unlabeled genomic data? The shift from supervised learning on narrow tasks to self-supervised pretraining on broad sequence corpora enables the reusability that defines foundation models. Fifth, how can pretrained models be adapted to specific downstream applications? The gap between pretraining objectives and real-world problems requires principled strategies for transfer learning and deployment.\nThese five chapters build a progression from fundamental choices about representation through architectural mechanisms to the training and adaptation strategies that make foundation models practical. 5  Sequence Representation and Tokenization examines tokenization from one-hot encoding through byte-pair encoding to biologically informed vocabularies, establishing how representation choices propagate through model design. 7  Attention and Transformers unpacks the self-attention mechanism and transformer architecture, showing how these components enable both local pattern recognition and long-range dependency modeling in genomic sequences. 10  The Foundation Model Paradigm develops a practical taxonomy of foundation models, distinguishing pretraining paradigms and establishing principles for model selection. 8  Pretraining Objectives and Strategies surveys the landscape of self-supervised objectives, from masked language modeling to next-token prediction to denoising approaches, examining how each shapes learned representations. 9  Transfer Learning and Adaptation closes the loop by addressing how pretrained models are adapted to downstream tasks through fine-tuning, few-shot learning, and deployment strategies.\nTogether, these chapters provide the conceptual foundation needed to understand both current genomic models and the emerging directions covered in later parts. The principles developed here apply whether working with DNA language models for regulatory prediction, protein models for structure and function, or hybrid architectures that combine multiple modalities.",
    "crumbs": [
      "Part II: Core Principles"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html",
    "href": "p2-ch05-representations.html",
    "title": "5  Sequence Representation and Tokenization",
    "section": "",
    "text": "5.1 From Sequence to Model: The Representation Problem\nEvery genomic deep learning model must answer a fundamental question before learning can begin: how should DNA sequence be represented as numerical input? This question might seem purely technical, a preprocessing detail to be settled and forgotten. Yet the choice of representation profoundly shapes what a model can learn, how efficiently it trains, and what biological phenomena it can capture. A patient’s genome contains roughly three billion nucleotides, but whether a clinical variant interpretation model sees that genome as three billion individual tokens, five hundred million overlapping hexamers, or some adaptive compression depends entirely on representation decisions made before a single parameter is learned.\nThe clinical stakes are immediate. A splice-disrupting variant in BRCA1 might span just two nucleotides at an exon-intron boundary. If the tokenization scheme merges those nucleotides with surrounding sequence into a single coarse token, the model loses the resolution needed to distinguish pathogenic from benign changes. Conversely, a regulatory variant’s effect might depend on enhancer sequences fifty kilobases away. If the representation limits context to a few hundred nucleotides, that distal information never reaches the model. Representation choices thus constrain not just computational efficiency but the biological questions a model can answer.\nThe challenge can be understood through an analogy to natural language processing. When training a language model on English text, researchers must decide how to segment the continuous stream of characters into discrete tokens. One could treat each character as a token, preserving maximum resolution but creating very long sequences. Alternatively, one could use words as tokens, compressing the sequence but potentially losing information about word structure. Or one could learn a vocabulary of subword units that balances these concerns. Each choice affects what patterns the model can discover and how efficiently it can process long documents.\nDNA presents similar choices but with important differences. The genome has only four letters rather than dozens, no natural word boundaries, and biological structure that operates at multiple scales simultaneously. A transcription factor binding site might span 6 to 12 nucleotides, but the regulatory grammar linking multiple binding sites can extend over hundreds of base pairs. Coding sequences follow a strict three-nucleotide codon structure, while noncoding regions have no such constraint. Any representation scheme must navigate these biological realities while remaining computationally tractable.\nThis chapter examines the evolution of sequence representation strategies in genomic deep learning. We begin with one-hot encoding, the foundation of CNN-based models, then trace the progression through k-mer tokenization and byte pair encoding to modern approaches including single-nucleotide tokens and biologically-informed schemes. We then address two aspects of representation that prove critical for transformer architectures: the transformation from discrete tokens to dense learned embeddings, and the encoding of positional information that genomic applications demand. Understanding these choices clarifies design decisions in models throughout Parts III and IV, and illuminates why seemingly minor representation choices can dramatically affect model capabilities.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#one-hot-encoding-the-cnn-foundation",
    "href": "p2-ch05-representations.html#one-hot-encoding-the-cnn-foundation",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.2 One-Hot Encoding: The CNN Foundation",
    "text": "5.2 One-Hot Encoding: The CNN Foundation\nOne-hot encoding represents the simplest possible approach to sequence representation: each nucleotide becomes a sparse binary vector with a single active element indicating its identity. Adenine is encoded as [1, 0, 0, 0], cytosine as [0, 1, 0, 0], guanine as [0, 0, 1, 0], and thymine as [0, 0, 0, 1]. A sequence of length \\(L\\) thus becomes a matrix of dimensions \\(4 \\times L\\), interpretable as four channels analogous to the RGB channels of an image plus one additional channel.\nThis representation dominated the CNN era of genomic deep learning for good reason. One-hot encoding is lossless, preserving every nucleotide explicitly without any information compression. It maintains single-nucleotide resolution, enabling detection of effects from individual SNPs, which is critical for variant interpretation. The representation exhibits translation equivariance, meaning that convolutional filters learn position-invariant motifs that can be recognized anywhere in the sequence. And it requires no preprocessing, vocabulary construction, or tokenizer training, making implementation straightforward.\nDeepSEA, ExPecto, and SpliceAI all employed one-hot encoding without modification (Chapter 6). The convolutional layers in these models learned to detect sequence patterns directly from the binary representation, with first-layer filters discovering motifs corresponding to transcription factor binding sites and deeper layers capturing combinations and spatial arrangements. The representation worked because CNNs process sequences through local operations, with each convolutional filter examining only a small window of positions at a time. The sparse, orthogonal nature of one-hot vectors posed no obstacle to this local processing.\nYet for transformer architectures, one-hot encoding presents significant challenges. Transformers compute attention between all pairs of positions in a sequence, with computational cost scaling as \\(O(L^2)\\) where \\(L\\) is the sequence length. A 10 kb sequence requires 10,000 tokens, meaning 100 million pairwise attention computations per layer. This quickly becomes prohibitive for the long sequences that genomic applications require. Furthermore, transformers typically learn dense embeddings for each token, but with only four possible nucleotides, there is little opportunity for the model to discover rich representations through the embedding layer. The sparse one-hot vectors provide minimal information for the embedding to transform. Most critically, practical transformer context windows of 512 to 4,096 tokens translate to only 512 to 4,096 base pairs when using one-hot encoding, a tiny fraction of genes or regulatory regions and far less than the context that proved valuable for models like Enformer and SpliceAI.\nThese limitations motivated the search for alternative representations that could compress genomic sequences into fewer tokens while preserving the information needed for biological prediction.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#k-mer-tokenization-the-dnabert-approach",
    "href": "p2-ch05-representations.html#k-mer-tokenization-the-dnabert-approach",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.3 K-mer Tokenization: The DNABERT Approach",
    "text": "5.3 K-mer Tokenization: The DNABERT Approach\nThe computational constraints of one-hot encoding for transformers motivated exploration of sequence compression through k-mer tokenization. This approach treats overlapping subsequences of length \\(k\\) as tokens, drawing an analogy between k-mers and words in natural language. Just as sentences are composed of words that carry meaning through their sequence and combination, genomic sequences might be understood as composed of k-mer “words” that encode biological function through their arrangement. DNABERT (2021) pioneered this approach for genomic transformers, using 6-mers as tokens and training a BERT-style masked language model on human reference sequences (Ji et al. 2021).\nThe k-mer vocabulary has a fixed size of \\(4^k\\) possible tokens. For 6-mers, this yields 4,096 distinct tokens, comparable to the vocabulary sizes used in some natural language models. Each token represents six consecutive nucleotides, creating a direct correspondence between subsequence and token identity. The tokenization proceeds by sliding a window across the sequence and recording each k-mer encountered.\nDNABERT used overlapping k-mers, meaning that for a sequence like ACGTACGT, the 6-mer tokens would share five nucleotides with their neighbors. The sequence position advances by one nucleotide at a time, generating one token per position (minus the k-1 positions at the end where a complete k-mer cannot be formed). This overlapping design preserves positional information and ensures that every nucleotide contributes to multiple tokens, potentially providing redundancy that helps the model learn robust representations.\nThe DNABERT approach provided valuable proof of concept. It demonstrated that self-supervised pretraining on raw DNA sequences could improve performance over training from scratch, that learned embeddings could capture biologically meaningful regularities even when trained only on the reference genome, and that BERT-style architectures could be reused across multiple downstream tasks. DNABERT achieved strong performance on prediction of promoters, splice sites, and transcription factor binding sites after fine-tuning with relatively small amounts of task-specific labeled data.\nSubsequent analysis, however, revealed fundamental limitations of k-mer tokenization that stemmed from the overlapping design. DNABERT-2 (2024) articulated these problems clearly (Zhou et al. 2024). Overlapping k-mers provide no sequence compression: the number of tokens equals the number of nucleotides (minus a small constant), so context window limitations persist unchanged. A 10 kb sequence still requires approximately 10,000 tokens, and the quadratic attention complexity remains prohibitive for long sequences.\nOverlapping tokenization also creates ambiguity in how sequence positions map to tokens. A single nucleotide contributes to \\(k\\) different tokens, complicating interpretation of which token is responsible for any given prediction. This ambiguity becomes particularly problematic for variant effect interpretation, where one wants to understand how changing a specific nucleotide alters model predictions. The effect of a single nucleotide substitution propagates through \\(k\\) different tokens in ways that can be difficult to disentangle.\nThe overlapping design introduces sample inefficiency as well. The model must learn that overlapping tokens share nucleotides, a relationship that is obvious from the tokenization scheme but must be discovered through training. This redundancy consumes model capacity that could otherwise be devoted to learning more complex biological patterns. Finally, the fixed \\(4^k\\) vocabulary does not adapt to corpus statistics. Frequent and rare k-mers receive equal representation capacity in the embedding table, even though their importance for prediction may differ substantially.\nThese limitations motivated exploration of alternative tokenization strategies that could achieve genuine sequence compression while preserving the information needed for biological prediction.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#byte-pair-encoding-learning-the-vocabulary",
    "href": "p2-ch05-representations.html#byte-pair-encoding-learning-the-vocabulary",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.4 Byte Pair Encoding: Learning the Vocabulary",
    "text": "5.4 Byte Pair Encoding: Learning the Vocabulary\nByte Pair Encoding offers a fundamentally different approach to tokenization. Rather than defining tokens through a fixed rule (every \\(k\\) consecutive nucleotides), BPE constructs a vocabulary by learning which subsequences appear frequently in the training corpus. The algorithm, originally developed for data compression, iteratively merges the most frequent adjacent token pairs until reaching a desired vocabulary size.\nThe BPE algorithm begins by initializing the vocabulary with single nucleotides: {A, C, G, T}. It then scans the training corpus to count all adjacent token pairs and identifies the most frequent pair. This pair is merged into a new token, added to the vocabulary, and all instances in the corpus are replaced with the new token. The process repeats, counting pairs again (now including the newly created token) and merging the next most frequent pair. Through many iterations, BPE builds a vocabulary of variable-length tokens that capture frequently occurring sequence patterns.\nThe critical insight is that BPE produces genuine sequence compression. Unlike overlapping k-mers where each nucleotide generates its own token, BPE creates non-overlapping tokens that can span multiple nucleotides. A 10 kb sequence might compress to 2,000 or 3,000 tokens depending on its repetitive structure, enabling transformers to process much longer sequences within the same context window.\nDNABERT-2 replaced 6-mer tokenization with BPE and demonstrated dramatic improvements (Zhou et al. 2024). The new model achieved comparable performance to state-of-the-art approaches while using 21 times fewer parameters and requiring approximately 92 times less GPU time in pretraining. The efficiency gains stem directly from non-overlapping tokenization: actual sequence compression enables processing longer sequences with the same computational budget, and eliminating the redundancy of overlapping tokens allows the model to focus capacity on learning biological patterns rather than token relationships.\nThe BPE vocabulary learns corpus statistics through its construction process. Repetitive elements that appear frequently throughout the genome, such as Alu sequences or common regulatory motifs, receive dedicated tokens that span many nucleotides. These long tokens enable efficient representation of repetitive regions while preserving single-nucleotide resolution for unique sequences. Rare sequences that BPE never encountered during vocabulary construction are represented as concatenations of shorter subunits, maintaining the ability to encode any sequence while allocating more representation capacity to common patterns.\nGROVER (Genome Rules Obtained Via Extracted Representations) extended this approach by training BPE specifically on the human genome and selecting vocabulary using a custom next-k-mer prediction task (Sanabria et al. 2024). Analysis of the resulting token embeddings revealed that the learned vocabulary encodes biologically meaningful structure. Common tokens cluster separately from rare ones in embedding space. GC-rich tokens segregate from AT-rich tokens, reflecting the different properties of these sequence compositions. Token length correlates with specific embedding dimensions, allowing the model to represent both the content and extent of each token. Some tokens appear primarily in repetitive regions while others distribute broadly across the genome, and this localization pattern is captured in the learned representations.\nBPE introduces its own complications. The variable-length tokens mean that variant positions fall at different locations relative to token boundaries depending on the local sequence context. A SNP might fall in the middle of a long token in one context but at a token boundary in another, potentially affecting how the model represents and processes the variant. This context-dependence can complicate variant effect interpretation, as the same nucleotide change may alter different numbers of tokens depending on surrounding sequence.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#single-nucleotide-tokenization-maximum-resolution",
    "href": "p2-ch05-representations.html#single-nucleotide-tokenization-maximum-resolution",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.5 Single-Nucleotide Tokenization: Maximum Resolution",
    "text": "5.5 Single-Nucleotide Tokenization: Maximum Resolution\nWhile k-mer and BPE tokenization compress sequences to enable longer context windows, they sacrifice single-nucleotide resolution in doing so. This trade-off becomes problematic for variant effect prediction, where the precise position and identity of mutations is paramount. A single nucleotide polymorphism can completely alter protein function through mechanisms ranging from amino acid substitution to splice site disruption to regulatory element ablation. Multi-nucleotide tokens obscure exactly where variants fall and how they relate to the boundaries of biological features.\nHyenaDNA (2023) took the opposite approach, using single-nucleotide tokens with no compression whatsoever (Nguyen et al. 2023). Each nucleotide (A, C, G, T) is a separate token, maintaining the maximum possible resolution. Every nucleotide is independently represented in the token sequence, SNP effects can be isolated to specific token positions without ambiguity, and there are no tokenization artifacts that depend on surrounding sequence context.\nThe challenge with single-nucleotide tokens is sequence length. A 1 Mb region requires 1 million tokens, far beyond the capacity of any standard transformer. The quadratic attention complexity would require a trillion pairwise computations per layer, rendering the approach computationally infeasible with conventional architectures.\nHyenaDNA addressed this challenge through a fundamental architectural innovation rather than a tokenization compromise. The Hyena architecture replaces the attention mechanism with implicit convolutions that scale sub-quadratically with sequence length. Where attention computes explicit pairwise interactions between all positions, Hyena uses long convolutions parameterized by a small neural network, achieving similar representational power with \\(O(L \\log L)\\) complexity rather than \\(O(L^2)\\). This enables processing of sequences hundreds of times longer than attention-based transformers within the same computational budget.\nThe result was a 500-fold increase in context length over dense attention models while maintaining single-nucleotide resolution. HyenaDNA could process 1 Mb sequences where DNABERT was limited to approximately 500 bp and the Nucleotide Transformer to approximately 6 kb. On the Nucleotide Transformer benchmarks, HyenaDNA reached state-of-the-art performance on 12 of 18 datasets with orders of magnitude fewer parameters and less pretraining data. On GenomicBenchmarks, it surpassed prior state-of-the-art on 7 of 8 datasets by an average of 10 accuracy points.\nPerhaps most notably, HyenaDNA demonstrated the first use of in-context learning in genomics. The model could perform tasks based on examples provided in the context window without any fine-tuning, simply by conditioning on demonstration sequences. This capability, familiar from large language models, had not previously been shown for genomic sequences and suggests that very long context combined with high resolution enables qualitatively new forms of biological reasoning.\nThe development of sub-quadratic architectures like Hyena, Mamba, and state space models has thus fundamentally changed the tokenization calculus. When computational constraints no longer force a choice between resolution and context length, single-nucleotide tokenization becomes increasingly attractive, particularly for applications requiring precise variant interpretation.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#biologically-informed-tokenization",
    "href": "p2-ch05-representations.html#biologically-informed-tokenization",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.6 Biologically-Informed Tokenization",
    "text": "5.6 Biologically-Informed Tokenization\nStandard tokenization schemes treat DNA as a homogeneous string of characters, ignoring the biological reality that different genomic regions serve fundamentally different functions and follow different structural rules. Coding sequences obey a strict codon structure where every three nucleotides encode an amino acid, while noncoding regions have no such constraint. Treating these regions identically wastes an opportunity to build biological knowledge directly into the representation.\nFor protein-coding regions, the natural unit of sequence is the codon, not the individual nucleotide. GenSLMs (2022) pioneered codon-level tokenization for genomic foundation models, treating each three-nucleotide codon as a single token and exploiting the fact that codons are the biologically meaningful units of protein-coding sequence (Zvyagin et al. 2022). The 64-codon vocabulary captures the complete space of possible genetic code words, with each token corresponding to either an amino acid or a stop signal. This alignment with translation semantics means that mutations affecting amino acid identity (nonsynonymous changes) alter the token sequence, while synonymous mutations within a codon alter the specific token used but maintain the broader codon-family structure.\nLife-Code (2025) extended codon-aware tokenization to broader genomic contexts, encoding coding and noncoding regions in a way that respects reading frame and local biological function (Liu et al. 2025). Coding regions are tokenized by codons, aligning token boundaries with the fundamental unit of protein translation. Noncoding regions, lacking codon structure, are tokenized by learned patterns that capture regulatory motifs and other functional elements. This biologically-informed design enables Life-Code to learn protein structure through knowledge distillation from protein language models, capture interactions between coding and noncoding regions within a unified framework, and achieve state-of-the-art results across tasks involving DNA, RNA, and protein.\nBioToken (2025) extends tokenization further to include explicit genomic structural annotations (Medvedev et al. 2025). Rather than treating variants as implicit changes in the sequence string, BioToken creates tokens that explicitly represent SNPs, insertions, and deletions. Known regulatory elements receive dedicated tokens encoding their presence and type. Gene structure, chromatin state, and other functional annotations are integrated directly into the token representation. This approach treats tokens as rich entities that bundle nucleotides with positional, functional, or experimental context.\nSuch variant-aware representations are especially attractive for variant effect prediction and clinical interpretation, where the input is often “reference plus variant” rather than a generic sequence. By incorporating biological inductive biases directly into tokenization, BioToken’s associated model achieves competitive or superior performance to specialized models like Enformer and SpliceAI with significantly fewer parameters. This efficiency suggests that appropriate representation can substitute for model scale, at least partially, by making the learning problem easier through informed structure.\nThe broader lesson is that tokenization can and perhaps should be informed by biological structure when that structure is known and relevant. BPE learns statistical patterns from the corpus, but those patterns need not correspond to biological units. Codon tokenization imposes biological semantics directly, at the cost of applicability to noncoding regions. Future approaches might combine these strategies, using codon-aware tokenization for coding regions and BPE or single-nucleotide tokens for noncoding sequence.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#from-tokens-to-embeddings-learning-representations",
    "href": "p2-ch05-representations.html#from-tokens-to-embeddings-learning-representations",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.7 From Tokens to Embeddings: Learning Representations",
    "text": "5.7 From Tokens to Embeddings: Learning Representations\nOnce a tokenization scheme divides sequence into discrete units, these tokens must be transformed into numerical representations that neural networks can process. The embedding layer performs this transformation, mapping each token from a finite vocabulary to a dense vector in continuous space. This seemingly simple operation proves fundamental to what models can learn.\nConsider the difference between one-hot encoding and learned embeddings. A one-hot representation treats each nucleotide as maximally distinct from every other: the dot product between any two different nucleotides is zero, providing no information about their relationships. Adenine and thymine are equally different from each other as adenine and guanine, despite the biological reality that purines (A, G) share structural properties distinct from pyrimidines (C, T), and that complementary base pairs (A-T, G-C) have special significance.\nLearned embeddings allow the model to discover such relationships from data. The embedding layer maintains a matrix \\(E\\) of dimensions \\(V \\times d\\), where \\(V\\) is vocabulary size and \\(d\\) is embedding dimension. Each token maps to a row of this matrix, and during training, backpropagation adjusts the embedding vectors to support downstream prediction. If distinguishing purines from pyrimidines helps the model predict regulatory function, the embedding space will organize to reflect this distinction. If complementary relationships matter, they will emerge in the geometry of the learned space.\nThe embedding dimension \\(d\\) controls representational capacity. Small embeddings (32 to 64 dimensions) suffice for simple tokenization schemes like single nucleotides, where only four vectors must be distinguished. Larger vocabularies require larger embeddings: DNABERT-2’s BPE tokens use 768-dimensional embeddings, comparable to natural language models. The choice involves a trade-off between expressiveness and efficiency, as larger embeddings increase both model capacity and computational cost.\nFor genomic sequences, the embedding layer can learn surprising structure. Analysis of trained DNA language models reveals that embedding spaces organize around biologically meaningful properties. GC content, often considered a nuisance variable in genomics, emerges as a major axis of variation in embedding space because it correlates with so many functional properties. Repetitive elements cluster together. Coding sequence embeddings differ systematically from noncoding embeddings, even when the tokenization scheme makes no explicit distinction.\nThis emergent organization has practical implications. The structure learned in the embedding layer propagates through all subsequent computations. If the embeddings fail to capture relevant distinctions, later layers must learn them from scratch. If embeddings encode spurious correlations, the model may exploit them inappropriately. Understanding what embeddings learn, and whether that learning aligns with biological reality, becomes an important diagnostic for model behavior.\nThe relationship between tokenization and embedding deserves emphasis. Coarse tokenization (large k-mers, aggressive BPE) creates more tokens, each with more room for rich embedding representations but requiring the model to learn more parameters. Fine tokenization (single nucleotides) creates fewer token types with simpler embeddings but forces the model to build complex representations through composition across layers. Neither approach is uniformly superior; the optimal choice depends on available training data, model scale, and task requirements.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#position-encodings-where-tokens-live",
    "href": "p2-ch05-representations.html#position-encodings-where-tokens-live",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.8 Position Encodings: Where Tokens Live",
    "text": "5.8 Position Encodings: Where Tokens Live\nTransformers process tokens as sets rather than sequences. The attention mechanism computes interactions between all pairs of tokens regardless of their positions, treating a sequence as a bag of elements with no inherent order. For language, this creates a problem: “dog bites man” and “man bites dog” contain identical tokens but mean very different things. For genomics, the problem is more severe: a transcription factor binding site has entirely different effects depending on whether it appears in a promoter, an enhancer, or a gene body. Position must somehow be encoded.\nThe standard solution adds positional information to token embeddings before attention computation. The combined representation carries both content (what nucleotide or k-mer) and position (where in the sequence). Several strategies exist, each with distinct properties that matter for genomic applications.\nAbsolute positional embeddings assign a learnable vector to each position in the sequence. Position 1 receives embedding \\(p_1\\), position 2 receives \\(p_2\\), and so forth. These embeddings are added to the token embeddings, creating combined representations that carry both identity and location. BERT and early genomic transformers like DNABERT used this approach. The limitation is that the model can only handle sequences up to the maximum position seen during training. A model trained with 512-position embeddings cannot process position 513; there is no embedding for it. This fixed maximum context proves particularly restrictive for genomics, where biological phenomena span scales from individual binding sites to megabase regulatory domains.\nSinusoidal positional encodings address the fixed-length limitation by computing position embeddings from mathematical functions rather than learning them. The original Transformer paper used sinusoids of different frequencies:\n\\[PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})\\] \\[PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})\\]\nEach position receives a unique pattern of sines and cosines, with lower-frequency components capturing coarse position and higher-frequency components capturing fine position. The mathematical structure means that any position, even one never seen during training, can receive a well-defined encoding. Sinusoidal encodings also have the property that relative positions are represented consistently: the relationship between positions 10 and 20 mirrors that between positions 110 and 120.\nRelative positional encodings directly represent the distance between tokens rather than their absolute locations. When computing attention between positions \\(i\\) and \\(j\\), the model incorporates information about \\((j - i)\\), the relative offset. This approach recognizes that for many biological phenomena, relative positioning matters more than absolute coordinates. A transcription factor binding site 50 bases upstream of a transcription start site has similar effects whether the TSS is at genomic position 1,000 or 1,000,000. Relative encodings also generalize naturally to sequences longer than those seen during training, since the relative distances remain bounded even as absolute positions grow.\nRotary position embeddings (RoPE) encode position by rotating token embeddings in the complex plane. Rather than adding a position vector, RoPE multiplies embeddings by a rotation matrix whose angle depends on position. This approach preserves the relative distance information in the dot product used for attention: the attention score between two positions depends on their relative separation regardless of absolute location. RoPE has become popular in recent large language models and has been adopted by several genomic foundation models including variants of the Nucleotide Transformer.\nALiBi (Attention with Linear Biases) takes a different approach entirely, adding position-dependent biases directly to attention scores rather than modifying embeddings. The bias penalizes attention between distant positions, with the penalty increasing linearly with distance. ALiBi requires no learned position parameters and generalizes straightforwardly to longer sequences. The linear distance penalty may not perfectly capture biological relationships, where some regulatory interactions span consistent long distances while others operate locally, but the simplicity and extrapolation properties have proven valuable.\nFor genomic applications, the choice of position encoding has implications beyond mere sequence length. Biological coordinates matter: a variant at chr17:41,276,045 has specific meaning that should be preserved. Cross-strand relationships exist: the reverse complement of a sequence carries related but distinct information. Circular genomes like mitochondrial DNA and bacterial chromosomes have no beginning or end, creating wraparound relationships that linear position encodings cannot represent naturally.\nSeveral recent models have explored genomic-specific position encoding strategies. Some incorporate absolute genomic coordinates, allowing models to learn position-specific patterns like centromeric sequences or telomeric regions. Others encode strand explicitly, representing Watson and Crick strands as distinct position modalities. Models for bacterial or viral genomes sometimes use circular position encodings that respect the topology of circular chromosomes. These adaptations illustrate that position encoding is not merely a technical detail but a design choice that shapes what biological patterns a model can capture.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#special-considerations-for-biological-sequences",
    "href": "p2-ch05-representations.html#special-considerations-for-biological-sequences",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.9 Special Considerations for Biological Sequences",
    "text": "5.9 Special Considerations for Biological Sequences\nGenomic sequences present several features that distinguish them from natural language text and require careful consideration in representation design.\nStrand awareness poses a fundamental challenge. DNA is double-stranded, with each strand carrying complementary information. A sequence ACGT on the forward strand corresponds to ACGT (read 5’ to 3’) but also implies ACGT on the reverse strand read in the opposite direction (which is the reverse complement ACGT → TGCA). Some biological features are strand-specific: a gene on the forward strand is transcribed from that strand only. Other features are strand-agnostic: many transcription factor binding sites function identically on either strand. Representation schemes must decide whether to treat strands as equivalent (by augmenting training data with reverse complements), as distinct (by encoding strand explicitly), or as related-but-different (through equivariant architectures that process both strands jointly).\nThe Nucleotide Transformer addressed strand by including both orientations during training, using data augmentation to ensure the model sees sequences from both directions. Caduceus (2024) introduced a more elegant solution: a bidirectional architecture that processes forward and reverse complement strands simultaneously through shared computation (Schiff et al. 2024). The model outputs are equivariant to reverse complementation, meaning that reversing and complementing the input produces correspondingly transformed outputs. This inductive bias ensures consistent treatment of strand without requiring augmentation or doubling computational cost.\nCircular genomes present another topological consideration. Bacterial chromosomes and plasmids, mitochondrial DNA, and many viral genomes are circular, with no natural start or end position. Linear position encodings impose arbitrary boundaries on these sequences. Some models address this through circular position encodings that wrap around at sequence boundaries, while others simply process circular genomes as linear sequences with the understanding that boundary effects may introduce artifacts.\nGenomic coordinates carry information absent from raw sequence. The position chr17:41,276,045 refers to a specific location in the BRCA1 gene, and variants at this position have been extensively studied. Knowing the genomic coordinate enables lookup of prior knowledge: population frequencies from gnomAD, clinical interpretations from ClinVar, functional annotations from ENCODE. Some representation schemes incorporate coordinate information explicitly, enabling models to learn position-specific patterns and to integrate with external databases. Others deliberately exclude coordinates to force models to learn purely from sequence, trading prior knowledge for generalization to novel sequences.\nMultiple sequence inputs arise frequently in genomic applications. Variant effect prediction requires comparing reference and alternate alleles. Comparative genomics involves aligned sequences from multiple species. Some regulatory predictions require input from multiple genomic regions (promoter plus enhancer, for instance). Representation schemes must accommodate these multi-sequence inputs, either through concatenation, paired encoding, or specialized architectures that process multiple sequences jointly.\nThese considerations illustrate that genomic sequence representation is not simply a matter of choosing tokens but of designing representations that respect the structure of biological data. The best choice depends on the specific application: single-nucleotide tokens for variant interpretation, strand-aware encoding for regulatory prediction, coordinate inclusion for clinical models that must integrate with knowledge bases.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#trade-offs-and-practical-guidance",
    "href": "p2-ch05-representations.html#trade-offs-and-practical-guidance",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.10 Trade-offs and Practical Guidance",
    "text": "5.10 Trade-offs and Practical Guidance\nThe choice between tokenization strategies involves multiple competing considerations that depend on the intended application.\nCompression versus resolution represents the fundamental tension. Higher compression enables longer context windows within fixed computational budgets but loses precision for identifying exactly where variants fall and how they relate to biological features. One-hot encoding and single-nucleotide tokenization provide no compression but maintain full resolution. Non-overlapping k-mers achieve approximately k-fold compression at the cost of k-nucleotide resolution. BPE provides variable compression depending on sequence repetitiveness, with corresponding variable resolution. For variant effect prediction, where single nucleotide changes can have dramatic phenotypic consequences, resolution is paramount and the computational costs of long single-nucleotide sequences are often justified.\nVocabulary size affects both model capacity and efficiency. Larger vocabularies require bigger embedding tables but may capture more complex patterns directly. Smaller vocabularies are parameter-efficient but require the model to learn compositional structure through multiple layers. The vocabulary size of one-hot encoding (4 tokens plus special tokens) minimizes embedding parameters but maximizes the compositional learning burden. K-mer vocabularies scale exponentially with \\(k\\), reaching 4,096 for 6-mers. BPE vocabularies are tunable, typically ranging from 4,096 to 32,000 tokens for genomic applications.\nComputational efficiency depends on both tokenization and architecture. For standard attention with \\(O(L^2)\\) complexity, any compression directly reduces cost: non-overlapping k-mers reduce attention cost by a factor of \\(k^2\\), and BPE with average compression \\(c\\) reduces cost by \\(c^2\\). Sub-quadratic architectures like Hyena change this calculus, making single-nucleotide tokenization computationally feasible at long contexts and eliminating the need to trade resolution for efficiency.\nVariant interpretation has specific implications. Single-nucleotide tokens enable clean comparison of reference and alternate alleles at the same token position with no ambiguity about effect localization. K-mer tokens complicate matters because a single SNP changes \\(k\\) overlapping tokens, requiring aggregation across affected tokens and introducing potential boundary effects. BPE tokens create context-dependent effects where the same variant may fall at different positions relative to token boundaries depending on surrounding sequence.\nFor practitioners, several heuristics have emerged. Use single-nucleotide tokens when variant-level reasoning or high-resolution interpretability is central. Use k-mers or BPE when context length is the primary bottleneck and tasks do not require base-level precision. Consider biologically-informed tokens when integrating multi-modal or annotation-rich data. Match position encoding to task requirements: relative encodings for tasks where absolute position is arbitrary, coordinate-aware encodings for clinical applications requiring integration with databases.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch05-representations.html#implications-for-subsequent-chapters",
    "href": "p2-ch05-representations.html#implications-for-subsequent-chapters",
    "title": "5  Sequence Representation and Tokenization",
    "section": "5.11 Implications for Subsequent Chapters",
    "text": "5.11 Implications for Subsequent Chapters\nThe representation choices examined in this chapter set the stage for design decisions discussed throughout the rest of the book. In Chapter 6, we see how convolutional filters operating on one-hot encoded sequence implicitly learn k-mer-like patterns, with first-layer filters resembling known transcription factor binding motifs. The CNN approach can be understood as learning the tokenization jointly with the prediction task rather than fixing it in advance.\nIn Chapter 7, transformer architectures bring position encoding to the foreground. The choice between absolute, relative, and rotary position encodings affects whether models can generalize to sequence lengths beyond training and whether they capture the relative spatial relationships central to regulatory grammar. The quadratic complexity of attention creates strong pressure for sequence compression, motivating the k-mer and BPE approaches discussed here.\nIn Chapter 11, we revisit why many DNA transformers still use k-mers and how models like the Nucleotide Transformer balance context length with biological resolution. The pretraining objectives for DNA language models interact with tokenization choices: masked language modeling over k-mers versus bases produces different learned representations. In Chapter 12, protein language models use amino acids as natural tokens, sidestepping many tokenization debates but raising new questions about how to represent insertions, deletions, and post-translational modifications.\nIn Chapter 13, long-range models like Enformer and Borzoi largely retained one-hot encoding or single-nucleotide input encodings for precision in variant effect prediction, leaning on architectural innovations (hybrid CNN-transformer designs) rather than token compression to scale context. The choice reflects the primacy of variant interpretation in regulatory genomics applications.\nThe representation problem remains an active area of research. As models grow larger and contexts extend further, new tokenization strategies may emerge that better balance compression, resolution, and biological structure. The field has moved from treating tokenization as a fixed preprocessing step to recognizing it as a fundamental design decision that shapes what models can learn and how they can be applied. Understanding sequence representation is therefore not a technical footnote but a core element of genomic foundation model design, with implications that ripple through every subsequent chapter of this book.\n\n\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.\n\n\nZvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. “GenSLMs: Genome-Scale Language Models Reveal SARS-CoV-2 Evolutionary Dynamics.” bioRxiv. https://doi.org/10.1101/2022.10.10.511571.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sequence Representation and Tokenization</span>"
    ]
  },
  {
    "objectID": "p2-ch06-cnn.html",
    "href": "p2-ch06-cnn.html",
    "title": "6  Convolutional Models for Genomic Sequence",
    "section": "",
    "text": "6.1 Convolutions as Sequence Pattern Detectors\nIn 2015, a convolutional neural network trained on ENCODE chromatin data learned to recognize transcription factor binding motifs that matched entries in the JASPAR database, despite never being shown those motifs during training (Zhou and Troyanskaya 2015). The network had discovered, through gradient descent on raw sequence, patterns that took decades of experimental work to catalog. This result announced a new era for computational genomics: deep learning could extract biologically meaningful representations directly from DNA sequence, bypassing the hand-crafted features that had defined the field since its inception.\nThe significance extended beyond mere accuracy improvements. Classical variant effect predictors like CADD (Chapter 4) achieved their performance by aggregating dozens of annotation features, each encoding human assumptions about what mattered for function: conservation scores, protein domain overlaps, known regulatory element positions. These features worked, but they imposed a ceiling. Models could only recognize patterns that humans had already identified and encoded. Convolutional networks promised something different: learning which sequence features mattered directly from functional data, potentially discovering patterns that had escaped prior notice.\nThis chapter examines the convolutional architectures that established the sequence-to-function paradigm between 2015 and 2019. DeepSEA demonstrated that CNNs could predict chromatin accessibility and transcription factor binding across hundreds of cell types. Basset and DanQ explored architectural variations that influenced subsequent work. ExPecto extended the paradigm to gene expression prediction by integrating chromatin signals across 40 kilobases of promoter-proximal sequence. SpliceAI achieved near-spliceosomal accuracy for splice site prediction, enabling clinical identification of cryptic splice variants invisible to annotation-based methods. Together, these models established core principles that persist in modern genomic AI: raw sequence as input, multi-task learning for shared representations, and variant effect prediction through in silico mutagenesis. They also revealed a fundamental limitation that would motivate the architectural innovations examined in subsequent chapters.\nBefore examining specific models, understanding how convolutional operations process biological sequence clarifies what these architectures can and cannot learn. A convolutional filter slides across an input sequence, computing similarity scores at each position. For genomic applications, the input is typically one-hot encoded DNA (a binary matrix with four rows for A, C, G, T and columns for each position), and filters learn weight patterns that respond to specific nucleotide arrangements.\nConsider a filter of width 8 nucleotides. At each position along the sequence, the filter computes a weighted sum of the underlying nucleotides, producing high activation when the sequence matches its learned pattern and low activation otherwise. This operation is mathematically equivalent to scanning a position weight matrix (PWM) across the sequence, but with a crucial difference: the filter weights are learned during training rather than derived from aligned binding site sequences.\nThe first layer of a genomic CNN typically contains hundreds of such filters, each learning to detect different local patterns. Analysis of trained filters consistently reveals that many correspond to known transcription factor binding motifs. The CTCF insulator motif, the ETS family consensus sequence, the AP-1 binding site: these patterns emerge from training on chromatin data without any explicit motif supervision. The network discovers them because they predict the training labels.\nDeeper layers operate on the output of earlier layers rather than raw sequence. A second-layer filter might learn to detect specific arrangements of first-layer motifs: two ETS sites within 20 base pairs, or a CTCF motif flanked by particular spacing patterns. This hierarchical feature learning enables CNNs to capture regulatory grammar beyond individual motifs, including spacing constraints, orientation preferences, and combinatorial requirements that govern transcription factor cooperativity.\nPooling operations between convolutional layers reduce spatial resolution while increasing the receptive field. Max pooling selects the strongest activation within a window, achieving a form of position-invariant detection: the network cares that a motif is present somewhere in a region, not its exact position. This property suits regulatory genomics, where binding site positions within an enhancer often matter less than their presence and combination.\nThe receptive field of a convolutional network defines how much input sequence can influence a single output prediction. For a network with kernel width \\(k\\), pooling factor \\(p\\), and \\(L\\) layers, the receptive field grows with depth but remains fundamentally limited by architecture. A three-layer network with typical parameters might integrate information from 200 to 1,000 base pairs. Reaching further requires either more layers (increasing computational cost and training difficulty) or dilated convolutions (spacing filter weights to sample larger regions). This receptive field limitation becomes critical when biological dependencies span tens of kilobases.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Models for Genomic Sequence</span>"
    ]
  },
  {
    "objectID": "p2-ch06-cnn.html#convolutions-as-sequence-pattern-detectors",
    "href": "p2-ch06-cnn.html#convolutions-as-sequence-pattern-detectors",
    "title": "6  Convolutional Models for Genomic Sequence",
    "section": "",
    "text": "Warning\n\n\n\nVISUAL SUGGESTION: Convolution mechanics\nThree-panel figure: (A) A single filter sliding across one-hot DNA, producing activation scores; (B) Comparison of learned filter weights to JASPAR motif logo; (C) Receptive field diagram showing how pooling + stacking expands the region influencing each output.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Models for Genomic Sequence</span>"
    ]
  },
  {
    "objectID": "p2-ch06-cnn.html#sec-deepsea",
    "href": "p2-ch06-cnn.html#sec-deepsea",
    "title": "6  Convolutional Models for Genomic Sequence",
    "section": "6.2 DeepSEA: Regulatory Prediction from Sequence",
    "text": "6.2 DeepSEA: Regulatory Prediction from Sequence\n\n6.2.1 The Noncoding Variant Problem\nGenome-wide association studies had, by 2015, mapped thousands of loci to complex diseases and traits, yet a persistent pattern complicated interpretation: the vast majority of associated variants fell in noncoding regions (Maurano et al. 2012). Only a small fraction directly altered protein sequences. The remainder landed in introns, intergenic regions, and putative regulatory elements where their functional consequences remained obscure.\nExisting approaches to noncoding variant interpretation relied on overlap with functional annotations. If a variant fell within a ChIP-seq peak or DNase hypersensitive site, it might be flagged as potentially regulatory. This strategy had obvious appeal since it grounded predictions in experimental observations. But overlap-based annotation could not predict whether a variant would strengthen or weaken regulatory activity, could not score variants in regions lacking experimental coverage, and provided no mechanism for quantifying effect magnitude. A variant might fall within an enhancer, but would it matter? The data said where regulatory elements were; they did not say how sequence changes would affect them.\nDeepSEA, introduced by Zhou and Troyanskaya, reframed the problem: rather than asking whether a variant overlaps known annotations, ask what regulatory activities a sequence encodes and how mutations would alter them (Zhou and Troyanskaya 2015). The shift from annotation lookup to sequence-based prediction enabled scoring any variant in any genomic context, including regions never assayed in any experiment.\n\n\n6.2.2 Architecture and Training\nThe DeepSEA architecture was deliberately simple by contemporary standards. Input sequences of 1,000 base pairs, one-hot encoded, passed through three convolutional layers with 320, 480, and 960 filters respectively. Max pooling after each convolution compressed spatial dimensions. A fully connected layer with 925 units integrated information across the compressed representation, and a final output layer with 919 sigmoid units produced independent probability predictions for each chromatin profile.\nTraining data came from ENCODE and Roadmap Epigenomics: 690 transcription factor binding profiles, 104 histone modification profiles, and 125 DNase I hypersensitivity profiles spanning diverse cell types (Kagda et al. 2025; Kundaje et al. 2015). For each 1,000-bp input, the model predicted whether the central 200-bp region exhibited each chromatin feature. Chromosome 8 was held out for evaluation.\nThe multi-task formulation proved essential. Predicting 919 features simultaneously forced the network to learn shared representations useful across many prediction problems. The first convolutional layer learns general sequence patterns (GC content, common dinucleotides, ubiquitous motifs); these representations then feed task-specific combinations in later layers. Joint training provides implicit regularization, preventing overfitting to any single task while amortizing the cost of learning basic sequence features across all outputs.\n\n\n6.2.3 What DeepSEA Learned\nAnalysis of first-layer filters revealed learned patterns matching known transcription factor motifs. The network had independently discovered sequence preferences cataloged in JASPAR and TRANSFAC, confirming that the training objective (predicting chromatin state) induced biologically meaningful feature extraction. This interpretability distinguished deep learning from prior black-box approaches and suggested that the models captured genuine regulatory logic rather than spurious correlations.\nBeyond individual motifs, DeepSEA implicitly learned aspects of regulatory grammar. Deeper layers combined first-layer patterns into more complex representations, potentially capturing motif spacing requirements, orientation preferences, and cooperative binding arrangements. The network encoded relationships between sequence features that position weight matrices, operating independently at each motif, could not represent.\nDeepSEA outperformed gkm-SVM (gapped k-mer support vector machines) on nearly all transcription factor binding prediction tasks. More revealing was the pattern with respect to context: gkm-SVM showed no improvement when given longer input sequences, while DeepSEA performance improved substantially with additional context. The difference reflects a fundamental limitation of k-mer counting methods: they cannot learn relationships between patterns at different positions.\n\n\n6.2.4 Variant Effect Prediction\nWith a trained sequence-to-chromatin model, variant scoring becomes straightforward: predict chromatin profiles for reference and alternative sequences, compute the difference. This produces a vector describing predicted changes across all 919 features. Critically, the model never sees variant data during training; effect prediction emerges from learned sequence-function relationships applied to mutations the model has never encountered.\nValidation used allelic imbalance data from digital genomic footprinting. For variants showing allele-specific DNase I sensitivity, DeepSEA predictions correlated with experimentally observed biases: variants predicted to increase accessibility tended to show higher accessibility on the corresponding allele. This correlation would not exist if the model merely learned coarse sequence features insensitive to point mutations.\nIn silico saturation mutagenesis extends single-variant scoring to systematic characterization. By predicting effects of all possible substitutions across a regulatory element, one identifies positions where mutations most strongly perturb function. These critical positions typically correspond to transcription factor binding motifs, providing a form of motif discovery that emerges from learned representations rather than explicit sequence alignment.\n\n\n\n\n\n\nWarning\n\n\n\nVISUAL SUGGESTION: DeepSEA architecture and validation\nThree-panel figure: (A) Architecture schematic showing convolutional stack, pooling, and 919-output vector; (B) First-layer filter aligned to JASPAR motif; (C) Scatter plot of predicted vs. observed allelic imbalance.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Models for Genomic Sequence</span>"
    ]
  },
  {
    "objectID": "p2-ch06-cnn.html#architectural-variations-basset-and-danq",
    "href": "p2-ch06-cnn.html#architectural-variations-basset-and-danq",
    "title": "6  Convolutional Models for Genomic Sequence",
    "section": "6.3 Architectural Variations: Basset and DanQ",
    "text": "6.3 Architectural Variations: Basset and DanQ\nDeepSEA established the paradigm; subsequent models explored architectural variations with different trade-offs. Basset, introduced by Kelley et al. in 2016, focused specifically on predicting chromatin accessibility from sequence (kelley_basset_2016?). Rather than the diverse chromatin features of DeepSEA, Basset predicted DNase-seq peaks across 164 cell types, enabling detailed analysis of regulatory element activity.\nBasset’s architecture introduced several refinements. Batch normalization after convolutional layers stabilized training and enabled deeper networks. The model used larger filters in early layers (19 nucleotides in the first layer) to capture longer motifs directly. A key contribution was demonstrating that in silico saturation mutagenesis profiles from trained models could identify causal variants underlying disease-associated haplotypes, moving beyond simple peak overlap to mechanistic variant prioritization.\nDanQ combined convolutional and recurrent components, reasoning that regulatory grammar involves sequential dependencies that convolutions alone might miss (quang_danq_2016?). After initial convolutional layers extracted local features, a bidirectional LSTM processed the resulting feature sequence, potentially learning ordering constraints and long-range patterns. The hybrid architecture achieved modest improvements on chromatin prediction benchmarks, though the recurrent components added computational cost and training complexity.\nThese variations illustrated a broader point: multiple architectures could learn useful regulatory representations from sequence. The specific choices (filter sizes, layer depths, recurrent components) mattered less than the fundamental framework of learning from one-hot encoded sequence to predict chromatin labels. This robustness suggested that the underlying signal (sequence determinants of regulatory activity) was strong enough to be captured by diverse architectural approaches.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Models for Genomic Sequence</span>"
    ]
  },
  {
    "objectID": "p2-ch06-cnn.html#sec-expecto",
    "href": "p2-ch06-cnn.html#sec-expecto",
    "title": "6  Convolutional Models for Genomic Sequence",
    "section": "6.4 ExPecto: From Chromatin to Expression",
    "text": "6.4 ExPecto: From Chromatin to Expression\n\n6.4.1 Beyond Intermediate Phenotypes\nChromatin accessibility and transcription factor binding are intermediate phenotypes, means to an end rather than the end itself. The ultimate functional readout for most regulatory variants is their effect on gene expression. A variant might disrupt a transcription factor binding site, but does that site actually regulate a nearby gene? In which tissues? By how much?\nExPecto, introduced by Zhou et al. in 2018, addressed these questions by extending sequence-to-chromatin prediction toward tissue-specific gene expression (Zhou et al. 2018). The framework predicts expression levels across 218 tissues and cell types by integrating predicted chromatin signals across a 40-kilobase promoter-proximal window. This context expansion, from DeepSEA’s 1 kb to ExPecto’s 40 kb, represented a significant architectural commitment: expression prediction requires integrating regulatory signals from distances far exceeding typical motif sizes.\n\n\n6.4.2 The Modular Architecture\nExPecto comprises three sequential components, each addressing a distinct computational challenge. The first component, an enhanced CNN called Beluga, predicts 2,002 chromatin profiles from 2,000-bp input sequences. Beluga incorporated architectural improvements over DeepSEA: six convolutional layers with residual connections, expanded chromatin targets, and broader cell-type coverage. This CNN scans the 40-kb region surrounding each transcription start site with a moving window, generating chromatin predictions at 200 spatial positions and producing over 400,000 features per gene.\nThe second component transforms these high-dimensional features through spatial aggregation. Ten exponential decay functions, applied separately to upstream and downstream regions, encode the prior belief that nearby elements contribute more than distant ones. This transformation reduces dimensionality while preserving spatial relationships, producing approximately 20,000 features per gene that capture both which chromatin features are predicted and where they occur relative to the TSS.\nThe final component comprises 218 L2-regularized linear regression models, one per tissue, predicting log expression from spatially-transformed features. Linear models were chosen deliberately: they provide interpretability, prevent overfitting given the high-dimensional feature space, and enable coefficient analysis to identify which chromatin features drive expression in each tissue. The combination of a shared sequence-to-chromatin CNN with separate tissue-specific linear heads cleanly separates sequence-level regulatory grammar from tissue-specific regulatory programs.\n\n\n6.4.3 Expression Prediction and Variant Effects\nExPecto achieved 0.819 median Spearman correlation between predicted and observed expression across tissues. Analysis of model coefficients revealed automatic learning of cell-type-relevant features: the liver expression model weighted HepG2-derived transcription factor features most heavily; breast tissue models emphasized estrogen receptor features from breast cancer cell lines. These tissue-specific patterns emerged purely from learning to predict expression, without tissue identity information provided to the chromatin model.\nVariant effect prediction follows the same logic as DeepSEA: compare expression predictions for reference and alternative sequences. Because the model never trains on variant data, predictions are unconfounded by linkage disequilibrium. ExPecto correctly predicted expression change direction for 92% of the strongest GTEx eQTL variants, and experimental validation confirmed that model-prioritized variants (not the GWAS lead SNPs) showed allele-specific regulatory activity in reporter assays.\nThe 40-kb window represents an empirically optimized trade-off. Smaller windows decreased performance; larger windows showed negligible improvement. This suggests that most promoter-proximal regulatory information lies within 40 kb of the TSS, at least within ExPecto’s linear modeling framework. Distal enhancers beyond this window, while biologically important, require more sophisticated integration. This limitation points toward the longer-context architectures examined in Chapter 13.\n\n\n\n\n\n\nWarning\n\n\n\nVISUAL SUGGESTION: ExPecto pipeline\nThree-component diagram: (1) Beluga CNN scanning 40 kb window; (2) Spatial feature transformation with decay functions; (3) Per-tissue linear models. Include example predicted expression values and delta scores.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Models for Genomic Sequence</span>"
    ]
  },
  {
    "objectID": "p2-ch06-cnn.html#sec-spliceai",
    "href": "p2-ch06-cnn.html#sec-spliceai",
    "title": "6  Convolutional Models for Genomic Sequence",
    "section": "6.5 SpliceAI: Clinical-Grade Splicing Prediction",
    "text": "6.5 SpliceAI: Clinical-Grade Splicing Prediction\n\n6.5.1 The Cryptic Splice Problem\nWhile DeepSEA and ExPecto addressed chromatin state and expression, a distinct class of functional variants operates through disruption of pre-mRNA splicing. The clinical stakes are substantial. Splice-disrupting mutations represent a major mechanism of Mendelian disease, yet variants affecting splicing outside canonical GT/AG dinucleotides are systematically underascertained. A patient’s exome may reveal no obvious coding variant explaining their phenotype, while a cryptic splice variant in an intron silently disrupts gene function.\nPrior splice prediction methods captured essential splice site motifs but could not model the long-range determinants contributing to splicing specificity. MaxEntScan operates on approximately 9 bp of context around donor/acceptor sites (Yeo and Burge 2004). These methods produced many false positives and missed variants acting through distal mechanisms. The limitations paralleled those of pre-deep-learning variant effect predictors: hand-crafted features impose ceilings that learned representations can transcend.\nSpliceAI, introduced by Jaganathan et al. in 2019, demonstrated that deep neural networks could learn splicing rules with near-spliceosomal precision (Jaganathan et al. 2019). The model predicts splice site locations directly from pre-mRNA sequence using 10,000 nucleotides of context, an order of magnitude beyond prior methods. This context expansion enabled recognition of distant determinants like branch points, exonic splicing enhancers, and intron length constraints that previous models could not see.\n\n\n6.5.2 Architecture: Depth and Dilation\nSpliceAI employs an ultra-deep residual network with 32 convolutional layers. Residual connections address the vanishing gradient problem:\n\\[\n\\text{output} = \\text{input} + F(\\text{input})\n\\]\nThis design enables training depths impossible with earlier architectures. Skip connections from every fourth residual block feed directly to the penultimate layer, further stabilizing gradients.\nDilated convolutions expand the receptive field efficiently. A dilated convolution with rate \\(d\\) samples input positions at intervals of \\(d\\) rather than consecutively. Stacking convolutions with increasing dilation rates allows the network to integrate information across the full 10-kb window while maintaining sensitivity to local patterns. Standard convolutions with small kernels would require impractical depth to achieve equivalent receptive fields.\nFor each position in the pre-mRNA sequence, SpliceAI outputs three probabilities: splice acceptor, splice donor, or neither. This per-position classification enables fine-grained predictions across entire transcripts. Training used GENCODE annotations, with odd and even chromosomes split for training and testing.\n\n\n6.5.3 Performance and Validation\nSpliceAI achieved 95% top-k accuracy for splice site identification (compared to 57% for MaxEntScan) and 0.98 precision-recall AUC. Even complex genes exceeding 100 kb are often reconstructed perfectly to nucleotide precision. Performance improved dramatically with context length:\n\n\n\nModel Variant\nContext (each side)\nPR-AUC\n\n\n\n\nSpliceAI-80nt\n40 bp\n0.87\n\n\nSpliceAI-400nt\n200 bp\n0.93\n\n\nSpliceAI-2k\n1,000 bp\n0.96\n\n\nSpliceAI-10k\n5,000 bp\n0.98\n\n\n\nThis progression confirms that distal sequence features contribute meaningfully to splicing decisions.\nThe delta score quantifies variant effects by comparing predictions for reference and alternative sequences:\n\\[\n\\Delta\\text{score} = \\max_{|p - v| \\leq 50} \\left| P_{\\text{alt}}(p) - P_{\\text{ref}}(p) \\right|\n\\]\nValidation against GTEx RNA-seq showed that mutations with higher delta scores showed higher validation rates at novel splice junctions: approximately 50% at Δ ≥ 0.2, 75% at Δ ≥ 0.5, and 85% at Δ ≥ 0.8. Population genetics provided orthogonal support: predicted cryptic splice variants showed 78% depletion at common allele frequencies, nearly matching the depletion of frameshift and stop-gain variants.\n\n\n6.5.4 Clinical Impact\nSpliceAI’s most significant contribution may be identifying cryptic splice mutations as a major, previously underappreciated cause of rare genetic disorders. Analysis of de novo mutations in over 4,000 individuals with intellectual disability found significant enrichment of predicted splice-disrupting variants compared to unaffected controls (1.51-fold, p = 4.2×10⁻⁴). Approximately 9% of pathogenic de novo mutations in intellectual disability act through cryptic splicing. Including these variants in gene discovery analyses identified additional candidate genes that would have fallen below discovery thresholds when considering only protein-coding mutations.\nThis clinical utility explains SpliceAI’s rapid adoption. Illumina integrated SpliceAI into their annotation pipelines. Clinical genetics laboratories worldwide use delta scores to flag potential splice-affecting variants for RNA-seq follow-up. The model exemplifies how task-specific deep learning can achieve clinical-grade accuracy on well-defined problems. We return to SpliceAI’s role in modern variant interpretation workflows in Chapter 14.\n\n\n\n\n\n\nWarning\n\n\n\nVISUAL SUGGESTION: SpliceAI clinical application\nCase-style figure: (A) Intronic variant location in a disease gene; (B) Delta score track showing predicted cryptic splice activation; (C) RNA-seq sashimi plot confirming the aberrant junction; (D) Comparison to MaxEntScan prediction (missed).",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Models for Genomic Sequence</span>"
    ]
  },
  {
    "objectID": "p2-ch06-cnn.html#sec-receptive-field",
    "href": "p2-ch06-cnn.html#sec-receptive-field",
    "title": "6  Convolutional Models for Genomic Sequence",
    "section": "6.6 The Receptive Field Ceiling",
    "text": "6.6 The Receptive Field Ceiling\nThe models examined in this chapter share a fundamental limitation rooted in their architecture: convolutional networks can only integrate information within their receptive fields. DeepSEA’s three-layer architecture effectively considers roughly 1 kb of context. ExPecto’s Beluga component operates on 2-kb windows, aggregated across a 40-kb region by the spatial transformation layer. SpliceAI pushes to 10 kb through dilated convolutions and 32 layers. Each expansion required significant architectural engineering, and each reached a practical ceiling.\nThe limitation matters because genomic regulation operates across scales these models cannot reach. Enhancers routinely regulate promoters 50 to 500 kilobases away. The beta-globin locus control region sits 40 to 60 kb from the genes it activates. Polycomb-mediated repression involves chromatin contacts spanning megabases. Topologically associating domains organize regulatory interactions across hundreds of kilobases. When regulatory elements and their targets lie beyond a model’s receptive field, the model cannot learn their relationship regardless of how much training data is available.\nThis creates a systematic blind spot. Variants within distal enhancers may have profound effects on gene expression, but a model with a 10-kb receptive field cannot connect the enhancer sequence to its target promoter. The model might correctly predict that the enhancer sequence contains regulatory features, but it cannot predict which gene those features regulate or how strongly. Regulatory effect prediction requires seeing both the regulatory element and its target simultaneously.\nThe architectural response to this challenge takes two forms. One approach, exemplified by Enformer (Chapter 13), combines convolutional feature extraction with attention mechanisms that enable direct interaction between distant positions. By computing pairwise relationships across a 200-kb context, Enformer can model enhancer-promoter communication that pure convolutions cannot reach. The other approach uses efficient attention variants or state space models that reduce the quadratic cost of full attention, enabling even longer contexts.\nThe progression from DeepSEA’s 1 kb to Enformer’s 200 kb represents a 200-fold expansion in effective context. This expansion did not come from better convolutions; it came from abandoning the assumption that local operations were sufficient and embracing architectures designed for long-range dependency modeling. Understanding why this architectural shift was necessary, and what CNNs could not provide, establishes the foundation for the attention mechanisms examined in Chapter 7.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Models for Genomic Sequence</span>"
    ]
  },
  {
    "objectID": "p2-ch06-cnn.html#architectural-themes-and-lasting-contributions",
    "href": "p2-ch06-cnn.html#architectural-themes-and-lasting-contributions",
    "title": "6  Convolutional Models for Genomic Sequence",
    "section": "6.7 Architectural Themes and Lasting Contributions",
    "text": "6.7 Architectural Themes and Lasting Contributions\nThe convolutional models examined here established paradigms that persist in modern genomic AI, even as transformer architectures have become dominant for many applications.\nRaw sequence as input: One-hot encoded nucleotides, with no hand-crafted features, are sufficient to learn complex regulatory signals. This end-to-end learning approach forces models to discover relevant patterns from data rather than encoding human assumptions about what matters.\nMulti-task training: Jointly predicting hundreds of related chromatin features improves both accuracy and generalization compared to training separate models. Shared representations in early layers benefit all tasks, and joint prediction provides implicit regularization against overfitting.\nVariant scoring through in silico mutagenesis: Comparing predictions for reference and alternative sequences enables effect prediction without training on variant data. This ab initio capability avoids linkage disequilibrium confounding and enables scoring of rare variants, de novo mutations, and hypothetical sequences never observed in any population.\nTask-specific architectures achieve remarkable accuracy: SpliceAI’s focus on splicing enabled near-spliceosomal precision that broad foundation models have not yet matched for this specific task. When the prediction target is well-defined and training data are abundant, specialized models remain competitive.\nThese models also revealed what CNNs cannot do. The receptive field ceiling limits modeling of long-range regulatory interactions. Expression prediction requires integrating signals across distances that pure convolutions cannot efficiently span. The fundamental operation of convolution, local pattern detection with limited spatial context, suits some problems better than others.\nThe tension between specialized and general-purpose models recurs throughout this book. For clinical applications requiring high accuracy on specific tasks (splice site prediction, chromatin accessibility), specialized CNNs may remain preferred. For discovery applications requiring broad coverage of molecular mechanisms across diverse prediction targets, the foundation model approach examined in Part III offers different trade-offs. The architectures examined in Chapter 7 provide mechanisms for long-range dependency modeling that extend the principles established here while transcending the receptive field limitations that constrained these pioneering models.\n\n\n\n\n\n\nWarning\n\n\n\nVISUAL SUGGESTION: Receptive field comparison\nDiagram comparing effective context windows: DeepSEA (1 kb), ExPecto/Beluga (2 kb windows, 40 kb aggregated), SpliceAI (10 kb), Enformer (200 kb). Overlay typical enhancer-promoter distances and TAD sizes to illustrate what each architecture can and cannot see.\n\n\n\n\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data Navigation on the ENCODE Portal.” Nature Communications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. “Integrative Analysis of 111 Reference Human Epigenomes.” Nature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nMaurano, Matthew T., Richard Humbert, Eric Rynes, Robert E. Thurman, Eric Haugen, Hao Wang, Alex P. Reynolds, et al. 2012. “Systematic Localization of Common Disease-Associated Variation in Regulatory DNA.” Science 337 (6099): 1190–95. https://doi.org/10.1126/science.1222794.\n\n\nYeo, Gene, and Christopher B. Burge. 2004. “Maximum Entropy Modeling of Short Sequence Motifs with Applications to RNA Splicing Signals.” Journal of Computational Biology 11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Models for Genomic Sequence</span>"
    ]
  },
  {
    "objectID": "p2-ch07-attention.html",
    "href": "p2-ch07-attention.html",
    "title": "7  Attention and Transformers",
    "section": "",
    "text": "7.1 The Self-Attention Mechanism\nConvolutional networks excel at local patterns but fail at long-range dependencies. When a regulatory element 50 kilobases upstream controls gene expression, no practical CNN architecture can span that distance. The receptive field would require hundreds of layers and billions of parameters to propagate information across such distances. This limitation extends beyond theoretical inconvenience: enhancer-promoter interactions routinely occur at these scales, topologically associating domains organize chromatin contacts across megabases, and GWAS variants often lie far from coding sequences, exerting effects through distal regulatory mechanisms. The convolutional models examined in Chapter 6 capture motifs, chromatin marks, and local regulatory grammar with remarkable fidelity, yet they remain fundamentally blind to the long-range interactions that govern much of gene regulation.\nThe attention mechanism, introduced for machine translation in 2017 (Vaswani et al. 2017), resolved this tension by abandoning sequential processing entirely. Rather than propagating information through layers of local operations, attention computes direct interactions between all positions in a single operation. A position near a gene’s promoter can attend directly to an enhancer 100 kilobases away without information passing through intermediate layers. The key insight was not incremental improvement to convolutional receptive fields but a fundamentally different approach to sequence modeling: let each position query all other positions and aggregate information based on learned relevance. This architectural shift enabled the foundation models that now dominate genomic AI, from protein language models like ESM that learn structure from sequence alone to regulatory models like Enformer that predict expression from 200-kilobase contexts.\nThis chapter examines the mechanisms that make transformers effective for genomic data. We unpack self-attention mathematically while maintaining intuition for what the computation accomplishes biologically. We address positional encodings (the solution to attention’s position-blindness), the transformer block architecture, and scaling considerations specific to genomic sequences that can span millions of bases. We survey transformer variants used across genomic applications and confront the quadratic complexity that ultimately motivates the state space models examined in Chapter 11.\nUnderstanding a variant’s regulatory impact often requires integrating information across thousands of base pairs. A transcription factor binding site gains meaning from the promoter it activates, the insulators that constrain its range, and the chromatin state that modulates accessibility. CNNs force this information to propagate through many layers, each aggregating only local context. Self-attention instead computes all pairwise interactions simultaneously, allowing the model to directly relate any position to any other regardless of distance.\nAt each position in the input sequence, self-attention computes three vectors: a query, a key, and a value. These vectors emerge from multiplying the input embedding at that position by three learned weight matrices \\(W^Q\\), \\(W^K\\), and \\(W^V\\). The query represents what information this position seeks from other positions. The key represents what information this position offers to queries from elsewhere. The value represents the actual information this position contributes when attended to. This query-key-value structure separates the question of “which positions should interact” (determined by query-key similarity) from “what information flows between them” (determined by values).\nThe attention mechanism computes similarity scores between each query and all keys. For position \\(i\\), we compute the dot product between its query \\(q_i\\) and every key \\(k_j\\) across all positions \\(j = 1, \\ldots, L\\), where \\(L\\) is sequence length. These scores are scaled by \\(\\sqrt{d_k}\\) (the square root of the key dimension) to prevent the dot products from growing large in high dimensions, which would push softmax outputs toward extreme values and create vanishing gradients:\n\\[\n\\text{score}(q_i, k_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\n\\]\nA softmax function converts these scores into attention weights \\(\\alpha_{ij}\\) that form a probability distribution over positions:\n\\[\n\\alpha_{ij} = \\frac{\\exp(\\text{score}(q_i, k_j))}{\\sum_{j'=1}^L \\exp(\\text{score}(q_i, k_{j'}))}\n\\]\nThese weights determine how strongly position \\(i\\) attends to each other position. High weight means position \\(i\\) aggregates substantial information from position \\(j\\); low weight means position \\(j\\) contributes little to the output at position \\(i\\). The final output at position \\(i\\) is a weighted sum of all value vectors:\n\\[\n\\text{output}_i = \\sum_{j=1}^L \\alpha_{ij} v_j\n\\]\nThis weighted aggregation is the core of self-attention. Each output position receives a mixture of information from across the entire sequence, with mixture proportions learned through backpropagation. For genomic sequences, this means a position near a splice site can attend to both the upstream exon and downstream intron, integrating context that determines whether splicing occurs. A position in a promoter can attend to distant enhancers, learning which distal elements influence expression at this gene.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Attention and Transformers</span>"
    ]
  },
  {
    "objectID": "p2-ch07-attention.html#the-self-attention-mechanism",
    "href": "p2-ch07-attention.html#the-self-attention-mechanism",
    "title": "7  Attention and Transformers",
    "section": "",
    "text": "7.1.1 Multi-Head Attention\nA single attention operation learns one pattern of position interactions. Multi-head attention extends this by running multiple attention operations in parallel, each with independent learned projections. If we use \\(H\\) heads, we split the model dimension \\(d\\) into \\(H\\) subspaces of dimension \\(d/H\\), compute separate queries, keys, and values for each head, run attention independently, concatenate outputs, and project back to dimension \\(d\\).\nDifferent heads can specialize in different interaction types. In genomic models, one head might attend to nearby positions (capturing local motif context) while another attends to positions at characteristic distances (capturing nucleosome spacing or enhancer-promoter loops). Empirically, attention heads in trained genomic models show diverse patterns: some attend locally regardless of content, others attend to specific sequence motifs, and still others show distance-dependent patterns suggestive of chromatin organization. This specialization emerges from training without explicit supervision, reflecting the model’s discovery that different types of interactions require different aggregation patterns.\nThe multi-head structure also provides redundancy that aids training. If one head fails to learn useful patterns, others can compensate. Gradient flow through multiple parallel paths stabilizes optimization. For genomic applications where training data may be limited compared to natural language, this redundancy helps prevent individual heads from overfitting to spurious correlations.\n\n\n7.1.2 Computational Cost\nThe computational cost of self-attention scales quadratically with sequence length. Computing all pairwise attention scores requires \\(L \\times L\\) operations. For a 10-kilobase sequence tokenized at single-nucleotide resolution, this means 100 million attention computations per layer. A 200-kilobase sequence requires 40 billion computations per layer. Memory requirements scale similarly because the attention matrix must be stored for backpropagation.\nThis quadratic scaling creates a fundamental tension between context length and computational tractability. Genomic applications demand long contexts (enhancer-promoter distances, gene bodies, regulatory domains), but transformer attention becomes prohibitively expensive precisely where long contexts matter most. A 1-megabase context (modest by genomic standards) would require \\(10^{12}\\) attention computations per layer with standard self-attention. This tension motivates both the efficient attention variants discussed later in this chapter and the state space models examined in Chapter 11.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Attention and Transformers</span>"
    ]
  },
  {
    "objectID": "p2-ch07-attention.html#positional-encoding",
    "href": "p2-ch07-attention.html#positional-encoding",
    "title": "7  Attention and Transformers",
    "section": "7.2 Positional Encoding",
    "text": "7.2 Positional Encoding\nSelf-attention has a critical limitation: it is permutation invariant. Shuffling input token order changes nothing about how attention weights are computed because the same queries, keys, and values exist regardless of position. The model has no inherent notion of sequence order. For genomic data where position matters fundamentally (5’ to 3’ directionality, distance from transcription start sites, strand orientation), this blindness to order would be catastrophic.\nPositional encodings inject information about token positions into the model. The approach varies across methods, but the goal is consistent: break permutation invariance by making the model aware of where each token sits in the sequence.\n\n7.2.1 Absolute Position Encodings\nThe original transformer used sinusoidal functions with different frequencies for each embedding dimension. For position \\(pos\\) and dimension \\(i\\):\n\\[\n\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\n\\]\n\\[\n\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n\\]\nThese fixed patterns have useful properties. They are deterministic (the same for all sequences), allow the model to learn to attend by relative positions (since \\(\\text{PE}(pos+k)\\) can be expressed as a linear function of \\(\\text{PE}(pos)\\)), and generalize to sequence lengths not seen during training. The different frequencies across dimensions create a unique “fingerprint” for each position that the model can learn to decode.\nMany genomic models use learned positional embeddings instead: lookup tables where each position has a learned vector added to the input embedding. Learned embeddings offer flexibility, allowing the model to discover position-dependent patterns specific to genomic data (such as characteristic distances for nucleosome positioning or regulatory element spacing). The trade-off is that learned embeddings require training and do not automatically extrapolate to longer sequences.\n\n\n7.2.2 Relative Position Encodings\nAbsolute encodings treat position 1,000 and position 1,001 as having different representations even though their relative relationship (adjacent positions) may matter more than their absolute locations. Relative positional encodings address this by encoding distances between positions rather than absolute coordinates.\nT5-style relative position bias adds a learnable scalar to attention scores based on the distance between query and key positions. This helps the model learn that nearby positions often interact more strongly than distant ones while remaining agnostic about absolute position in the sequence.\nAttention with Linear Biases (ALiBi) adds a fixed linear penalty to attention scores based on distance, without learned parameters. For a head with slope \\(m\\), attention between positions separated by distance \\(|i - j|\\) is penalized by \\(m|i - j|\\). Different heads use different slopes, encouraging some to focus locally and others globally. ALiBi generalizes well to longer contexts than seen during training because the linear penalty extrapolates naturally, making it attractive for genomic applications where sequence length varies dramatically.\nRotary Position Embeddings (RoPE) encode positions by rotating query and key vectors in a high-dimensional space, with rotation angle depending on position. The dot product between rotated query and key depends on their relative distance, combining benefits of relative encoding with efficient implementation. RoPE has become common in recent language models and appears increasingly in genomic transformers.\n\n\n7.2.3 Genomic Position Considerations\nGenomic sequences impose additional requirements on positional encoding. DNA has strand directionality: ACGT on the forward strand has different regulatory meaning than the same sequence on the reverse strand (which reads ACGT as TGCA from the other direction). Positional encodings should enable the model to learn strand-specific patterns. Some genomic transformers use separate embeddings for forward and reverse strands; others rely on the model learning strand orientation from sequence content.\nGenomic coordinates pose another design choice. Should position 1 correspond to a fixed genomic landmark (transcription start site, gene start) or simply indicate sequence order without biological reference? Models predicting regulatory activity often center sequences on promoters, using positions relative to the TSS. Foundation models trained on random genomic segments typically use positional encodings reflecting sequence order without genomic coordinate reference.\nThe choice of positional encoding interacts with tokenization (Chapter 5). K-mer tokenization reduces sequence length (and thus attention cost) but changes what “position” means: position 1 might represent nucleotides 1-6 rather than a single base. Positional encodings must be interpreted relative to the tokenization scheme, and different combinations may suit different applications.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Attention and Transformers</span>"
    ]
  },
  {
    "objectID": "p2-ch07-attention.html#the-transformer-block",
    "href": "p2-ch07-attention.html#the-transformer-block",
    "title": "7  Attention and Transformers",
    "section": "7.3 The Transformer Block",
    "text": "7.3 The Transformer Block\nA transformer model consists of stacked blocks, each refining the sequence representation through attention and feed-forward processing. Understanding this block structure illuminates how information flows through transformer models and why certain design choices matter.\n\n7.3.1 Block Components\nEach transformer block contains two main components: a multi-head self-attention layer and a position-wise feed-forward network. The attention layer enables global communication, allowing each position to gather information from the entire sequence. The feed-forward network processes each position independently, applying nonlinear transformations to the aggregated information.\nThe feed-forward network consists of two linear transformations with a nonlinearity between them. Typically, this expands the dimension by a factor of four (from model dimension \\(d\\) to \\(4d\\)), applies GELU or similar activation, then projects back to dimension \\(d\\). This expansion allows processing through a high-dimensional nonlinear transformation before producing output for the next layer. The position-wise nature means each position is transformed identically but independently; cross-position information flows only through attention.\nLayer normalization stabilizes training by normalizing activations across the feature dimension at each position. Two conventions exist for placement. Post-norm places normalization after each sublayer, applying it to output before the residual connection. Pre-norm places normalization before each sublayer, normalizing input to attention or feed-forward operations. Pre-norm has become more common because it improves training stability for deep networks, though post-norm can achieve slightly better final performance with careful tuning.\nResidual connections wrap around both attention and feed-forward sublayers, adding input directly to output. These connections serve two critical functions. First, they provide gradient highways during backpropagation, allowing gradients to flow directly through many layers without repeated transformation. This enables training of very deep networks. Second, they create an inductive bias toward incremental refinement: each layer makes small adjustments to the representation rather than constructing entirely new representations from scratch.\n\n\n7.3.2 Information Flow\nThe flow through a pre-norm transformer block proceeds as follows. Input \\(X\\) is normalized, processed by multi-head attention to produce \\(X'\\), and added back via residual connection, yielding \\(X + X'\\). This sum is normalized, passed through the feed-forward network to produce \\(X''\\), and added via another residual connection, yielding final output \\(X + X' + X''\\). Each layer thus adds refinements to the representation while preserving information from earlier processing.\nStacking depth determines how many times this refinement occurs. Shallow transformers (few layers) are parameter-efficient but may lack capacity for complex tasks. Deep transformers (many layers) can learn sophisticated representations but require more computation and careful optimization. Most genomic transformers use 6 to 24 layers, varying by application. Models for short sequences (small RNAs, peptides) might use fewer layers, while foundation models for long genomic contexts often use deeper stacks to build hierarchical representations.\nThe choice of depth balances capacity against trainability. Deeper networks learn more complex functions but are harder to optimize, prone to overfitting without sufficient data, and more expensive at training and inference. For genomics, depth often correlates with the complexity of patterns being modeled. Simple motif tasks might benefit more from wider layers (larger \\(d\\)) than deeper stacks, while tasks requiring hierarchical integration (promoter-enhancer-TAD relationships) may benefit from additional depth that builds increasingly abstract representations.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Attention and Transformers</span>"
    ]
  },
  {
    "objectID": "p2-ch07-attention.html#scaling-to-genomic-sequences",
    "href": "p2-ch07-attention.html#scaling-to-genomic-sequences",
    "title": "7  Attention and Transformers",
    "section": "7.4 Scaling to Genomic Sequences",
    "text": "7.4 Scaling to Genomic Sequences\nGenomic sequences present unique scaling challenges that distinguish them from the text sequences for which transformers were originally designed. DNA sequences span millions of bases, far exceeding typical context lengths in natural language processing. Tokenization choices interact with sequence length: single-nucleotide tokens create very long sequences, while k-mer tokens reduce length at the cost of vocabulary size. Effective application of transformers to genomics requires careful balance of model size, context length, and computational resources.\n\n7.4.1 Parameter Scaling\nTransformer parameters come primarily from two sources: width and depth. Width (model dimension \\(d\\)) determines embedding and hidden state sizes. Increasing width allows more complex pattern representation at each position but increases parameters quadratically because weight matrices scale as \\(d \\times d\\). Depth (number of layers) determines how many refinement steps occur. Increasing depth allows hierarchical abstractions through repeated processing but increases parameters linearly.\nScaling laws from natural language processing suggest performance improves smoothly with increased parameters, data, and compute. Similar principles apply to genomics, though optimal ratios differ. Genomic sequences are less compressible than natural language: each nucleotide carries less predictable information than words in structured sentences. This suggests genomic models might benefit relatively more from depth (more processing of limited information per position) than from width (more dimensions per position).\n\n\n7.4.2 Context Length Strategies\nStandard self-attention’s \\(O(L^2)\\) complexity becomes prohibitive for long genomic contexts. Several strategies address this bottleneck.\nSparse attention patterns restrict which positions attend to which others. Local windowing allows each position to attend only within a fixed window, reducing complexity to \\(O(Lw)\\) where \\(w\\) is window size. This works when most relevant interactions are local, as often holds for regulatory sequences where nearby elements interact more strongly than distant ones. The trade-off is missing important long-range interactions that fall outside windows.\nStrided attention creates hierarchy: lower layers use local windows while upper layers attend to every \\(k\\)-th position. This captures both local fine-grained patterns and global coarse-grained structure while maintaining sub-quadratic complexity. Hybrid models like Enformer (Chapter 13) apply CNNs to downsample sequences before transformer layers, reducing the effective sequence length that attention must handle.\nApproximations to full attention offer another approach. Linformer approximates the attention matrix through low-rank decomposition, reducing complexity to linear in sequence length. Performer uses random feature methods to approximate attention scores without explicitly computing the full \\(L \\times L\\) matrix. These approximations trade some expressiveness for efficiency and may miss certain long-range dependencies.\n\n\n7.4.3 Memory and Precision\nMemory requirements compound computational challenges. Training requires storing activations for backpropagation, and attention matrices are particularly memory-intensive. Gradient checkpointing trades compute for memory by recomputing activations during the backward pass rather than storing them. This enables training larger models or longer sequences on fixed hardware at the cost of additional computation time.\nMixed precision training uses 16-bit floating point for most computations while maintaining 32-bit precision for critical operations like loss computation and optimizer updates. Modern GPUs accelerate 16-bit arithmetic substantially, providing near 2× speedup with minimal precision loss. Genomic transformers routinely use mixed precision to train on longer sequences or with larger batch sizes than full precision would allow.\nFor genomic applications, the choice among scaling strategies depends on biological context. Regulatory prediction often benefits from local windowing because nearby elements dominate most enhancer-promoter interactions. Foundation models predicting masked tokens may require global attention because prediction requires integrating information from across entire sequences. Variant effect prediction sometimes requires attending to specific distal elements that sparse patterns might miss.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Attention and Transformers</span>"
    ]
  },
  {
    "objectID": "p2-ch07-attention.html#architectural-variants-for-genomics",
    "href": "p2-ch07-attention.html#architectural-variants-for-genomics",
    "title": "7  Attention and Transformers",
    "section": "7.5 Architectural Variants for Genomics",
    "text": "7.5 Architectural Variants for Genomics\nThe transformer architecture has been adapted in diverse ways for genomic modeling. Understanding these variants illuminates design trade-offs and guides architecture selection for specific applications.\n\n7.5.1 Encoder-Only Transformers\nEncoder-only transformers process sequences bidirectionally, allowing each position to attend to all other positions including those that follow in the sequence. DNABERT and Nucleotide Transformer exemplify this architecture, trained with masked language modeling objectives where random tokens are masked and predicted from bidirectional context.\nBidirectional attention suits tasks where both upstream and downstream context matters for understanding a position. Transcription factor binding sites are influenced by flanking sequence in both directions. Protein structures depend on residues throughout the chain, not just N-terminal sequence. Variant effects may depend on context from both sides of the mutation.\nEncoder-only models excel at representation learning: producing embeddings that capture biological properties useful for downstream tasks. These embeddings can feed into variant effect predictors, function classifiers, or other models. The bidirectional context produces richer representations than unidirectional processing, but encoder-only architectures cannot generate sequences autoregressively because they require seeing the full sequence to produce any output.\n\n\n7.5.2 Decoder-Only Transformers\nDecoder-only transformers use causal attention where each position attends only to itself and preceding positions. This enables autoregressive generation: the model produces sequences one token at a time, conditioning each new token on all previous tokens. GenSLM and other genomic models trained on next-token prediction use this architecture.\nCausal attention is essential for generation tasks but produces less rich representations for fixed sequences because each position sees only partial context. The choice between encoder and decoder architectures reflects a fundamental tension: representation learning benefits from bidirectional context, while generation requires causal structure. Some applications use both, with a bidirectional encoder producing initial representations that a causal decoder refines for generation or autoregressive prediction.\n\n\n7.5.3 Hybrid CNN-Transformer Models\nHybrid architectures combine convolutional layers with transformer blocks. Enformer and Borzoi (Chapter 13) apply convolutional stems to long sequences, downsampling through pooling, then pass compressed representations through transformer layers. This exploits CNNs’ efficiency for local pattern extraction while using transformers for long-range integration.\nThese hybrids achieve state-of-the-art performance on regulatory prediction tasks. They work because genomic regulation involves both local patterns (motifs, nucleosome positioning signals) and long-range interactions (enhancer-promoter loops, TAD boundaries). The CNN-transformer combination matches this multi-scale structure: CNNs handle the local motif grammar while transformers integrate across the broader regulatory landscape.\nHybrid approaches also address the quadratic attention bottleneck indirectly. By downsampling sequences before transformer layers (often by factors of 128 or more), they reduce effective sequence length and thus attention cost. A 200-kilobase genomic region might be compressed to roughly 1,500 positions after CNN processing, making full attention tractable. The cost is loss of single-nucleotide resolution in the transformer layers, though the CNN stem preserves local detail.\n\n\n7.5.4 Long-Context Modifications\nSeveral approaches adapt transformer attention specifically for ultra-long genomic contexts. Hierarchical windowed attention (as in Swin transformers adapted for 1D sequences) applies attention within local windows at lower layers, then merges windows at higher layers to enable cross-window communication. This maintains local resolution while building global context hierarchically.\nOther approaches use sparse attention patterns tailored to genomic structure. One might attend to fixed landmark positions (known promoters, insulators, or TAD boundaries) or use genomic distance-based sparsity that assumes nearby positions interact more strongly than distant ones. Success depends on whether imposed structure matches actual genomic interactions. If enhancer-promoter loops occur at predictable spacing, fixed sparse patterns work well. If interactions are highly variable and context-dependent, full attention or learned sparsity may be necessary.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Attention and Transformers</span>"
    ]
  },
  {
    "objectID": "p2-ch07-attention.html#training-dynamics",
    "href": "p2-ch07-attention.html#training-dynamics",
    "title": "7  Attention and Transformers",
    "section": "7.6 Training Dynamics",
    "text": "7.6 Training Dynamics\nTraining genomic transformers involves optimization algorithms, regularization strategies, and stability considerations that differ in important ways from text-domain models.\n\n7.6.1 Optimization\nTransformers typically train with Adam or AdamW, adaptive learning rate algorithms that maintain per-parameter learning rates adjusted based on gradient statistics. AdamW applies weight decay directly to parameter updates rather than to the loss function, improving generalization and training stability. Learning rate schedules typically use warmup (linearly increasing learning rate from near-zero to peak over the first several thousand steps) followed by decay (linear or cosine decrease over the remaining training).\nWarmup addresses a specific instability: transformers with random initialization can produce extreme gradients early in training, and adaptive optimizers need time to build accurate gradient statistics. Warmup allows the optimizer to stabilize before applying full learning rates. Skipping warmup often causes training collapse in the first few hundred steps.\nFor genomics, learning rate tuning may require adjustment from NLP defaults. Genomic sequences have different statistical properties than natural language, and optimal learning rates may differ substantially. Regulatory sequences with highly conserved motifs may require lower learning rates to avoid overfitting to these strong signals. Protein sequences with weaker positional conservation may benefit from higher rates that encourage exploration.\n\n\n7.6.2 Regularization\nRegularization prevents overfitting, particularly important when training data is limited relative to model size. Dropout randomly zeros activations during training, forcing the network to learn robust features independent of specific neurons. Attention dropout applies this to attention weights, randomly dropping connections between positions. This prevents over-reliance on specific position pairs and encourages distributed representations.\nWeight decay penalizes large parameter values, encouraging smaller, smoother weights. For transformers, weight decay is typically applied to all parameters except biases and layer normalization parameters. The coefficient requires careful tuning: too little provides insufficient regularization; too much constrains capacity and reduces model expressiveness.\n\n\n7.6.3 Gradient Stability\nGradient issues plague deep network training. Vanishing gradients occur when gradients become extremely small through many layers, preventing learning in early layers. Exploding gradients are the opposite, where gradients grow exponentially and destabilize training. Transformers mitigate vanishing gradients through residual connections that provide direct gradient paths. Exploding gradients are addressed through gradient clipping, which rescales gradients when their norm exceeds a threshold.\nFor genomic transformers, gradient issues manifest differently than in language models. Genomic sequences have less hierarchical structure than natural language (no grammatical sentence organization), which affects gradient flow through attention layers. Imbalanced token frequencies create gradient imbalances: common k-mers or amino acids receive large gradients while rare but biologically important tokens receive small gradients. Addressing this may require loss reweighting or adaptive sampling that ensures rare tokens appear frequently enough for effective learning.\n\n\n7.6.4 Distributed Training\nComputational infrastructure for large genomic transformers typically requires distributed approaches. Single-GPU training suffices only for small models on short sequences. Data parallelism replicates the model across GPUs, splitting batches across devices and aggregating gradients. This scales well up to batch sizes limited by convergence requirements. Model parallelism splits the model itself across devices, necessary when models exceed single-GPU memory. Pipeline parallelism divides layers across devices and pipelines forward and backward passes.\nBatch size selection involves competing considerations. Larger batches provide more stable gradient estimates and better GPU utilization but require more memory and may reduce generalization. Genomic transformers often use gradient accumulation to simulate large batches: small batches process sequentially, gradients accumulate, then a single parameter update occurs. This provides large-batch benefits without the memory cost, though it increases training time proportionally.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Attention and Transformers</span>"
    ]
  },
  {
    "objectID": "p2-ch07-attention.html#limitations-and-emerging-alternatives",
    "href": "p2-ch07-attention.html#limitations-and-emerging-alternatives",
    "title": "7  Attention and Transformers",
    "section": "7.7 Limitations and Emerging Alternatives",
    "text": "7.7 Limitations and Emerging Alternatives\nDespite their success, transformers have fundamental limitations that motivate continued architectural innovation. Understanding these limitations contextualizes recent developments and guides architecture selection for specific genomic applications.\n\n7.7.1 The Quadratic Barrier\nThe quadratic complexity of self-attention remains transformers’ most severe limitation for genomics. Computing all pairwise attention scores requires \\(O(L^2)\\) operations and memory. For genomic contexts exceeding 100 kilobases (roughly 100,000 single-nucleotide tokens), this becomes prohibitive. Even with sparse approximations, transformers struggle at megabase scales where some regulatory interactions occur.\nRecent models have pushed context lengths impressively. Enformer handles 200 kilobases; emerging models approach 1 megabase. But these achievements rely on hybrid architectures with significant downsampling or hierarchical windowing that may miss certain long-range patterns. Pure transformers without such modifications remain limited to shorter contexts.\n\n\n7.7.2 State Space Models\nState space models (SSMs) address the quadratic barrier directly by achieving linear complexity while maintaining long-range modeling capability. Rather than computing all pairwise interactions, SSMs represent sequences as continuous-time dynamical systems, maintaining memory through recurrent state updates.\nArchitectures like S4, Hyena, and Mamba have demonstrated competitive or superior performance to transformers on various sequence modeling tasks while scaling to much longer contexts. For genomics, this enables whole-chromosome or potentially whole-genome modeling that remains intractable for standard transformers. Chapter 11 examines these architectures in detail, including HyenaDNA and the Evo family of DNA foundation models built on SSM principles.\n\n\n7.7.3 Choosing Architectures\nWhen should one prefer transformers over alternatives? Transformers excel when global context matters but sequences are not extremely long (under 10-50 kilobases depending on computational resources). Attention maps provide interpretability, showing which positions the model considers relevant for predictions. Transformers benefit from extensive tooling and pretrained models from NLP that transfer readily to genomics.\nCNNs remain preferable when computational efficiency is paramount and local patterns dominate. For splice site prediction or promoter classification where relevant context spans at most a few hundred base pairs, a well-designed CNN may outperform transformers while using far fewer parameters. The inductive bias toward local patterns also regularizes against overfitting when training data is limited.\nHybrid approaches often achieve the best practical results. As Chapter 13 demonstrates, models combining CNNs for local feature extraction with transformers for long-range integration outperform pure architectures on regulatory prediction tasks. The optimal combination depends on the specific biological question and the scale of relevant interactions.\nThe transition toward sub-quadratic architectures continues. Early results suggest SSMs match or exceed transformers on some genomic benchmarks while scaling to longer contexts. However, transformers benefit from years of engineering optimization and extensive pretrained model ecosystems. The field is actively exploring whether SSMs’ theoretical advantages translate to practical improvements across diverse genomic tasks.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Attention and Transformers</span>"
    ]
  },
  {
    "objectID": "p2-ch07-attention.html#from-architecture-to-learning",
    "href": "p2-ch07-attention.html#from-architecture-to-learning",
    "title": "7  Attention and Transformers",
    "section": "7.8 From Architecture to Learning",
    "text": "7.8 From Architecture to Learning\nThe transformer architecture provides the computational substrate for modern genomic foundation models, but architecture alone does not determine what models learn. The attention mechanism enables long-range interaction modeling; position encodings break permutation invariance to preserve sequence order; stacked blocks build hierarchical representations through iterative refinement. Yet the patterns these components learn depend critically on training objectives and data.\nSelf-supervised pretraining, examined in Chapter 8, provides the learning signal that transforms architectural capacity into biological knowledge. Masked language modeling teaches models to predict held-out tokens from context, implicitly learning sequence patterns and constraints. Next-token prediction in autoregressive models captures sequential dependencies. These objectives, applied to massive genomic datasets, enable transformers to learn representations that transfer across diverse downstream tasks.\nThe foundation models in Chapter 11 and Chapter 12 represent the culmination of transformer architecture and self-supervised learning. DNABERT applies masked language modeling to k-mer tokenized DNA, learning regulatory grammar from genomic sequence alone. ESM-2 applies similar principles to protein sequences, discovering that structure and function emerge from evolutionary patterns without explicit structural supervision. Enformer (Chapter 13) combines transformer attention with convolutional processing to span the 200-kilobase contexts required for expression prediction. Each model demonstrates that transformers, properly trained on biological sequence, capture biologically meaningful patterns.\nThe quadratic attention bottleneck that limits standard transformers has spurred development of alternative architectures. State space models achieve linear complexity while maintaining long-range capability, enabling the ultra-long contexts required for genome-scale modeling. Whether these alternatives ultimately displace transformers or complement them remains an open question. What is clear is that the attention mechanism introduced a new paradigm for genomic modeling: one where global context is directly accessible, where positions communicate without mediation through local layers, and where the computational challenge shifts from “how to see far enough” to “how to attend efficiently.” This paradigm, regardless of its specific architectural instantiation, continues to shape genomic AI.\n\n\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” arXiv. https://doi.org/10.48550/arXiv.1706.03762.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Attention and Transformers</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html",
    "href": "p2-ch08-pretraining.html",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "",
    "text": "8.1 Masked Language Modeling\nGenomics generates sequence data far faster than we can annotate it. Reference genomes span billions of nucleotides across thousands of species. Population sequencing projects catalog genetic variation in millions of individuals. Functional genomics consortia measure chromatin accessibility, transcription factor binding, and gene expression across hundreds of cell types. Yet experimental labels remain sparse: for any given sequence, we typically lack direct measurements of its regulatory function, its effect on splicing, or its contribution to disease risk. The gap between available sequence and available annotation grows with each passing year.\nThis asymmetry between data abundance and label scarcity defines the central opportunity for self-supervised learning in genomics. Rather than training models from scratch on small labeled datasets, we can first learn general-purpose sequence representations from unlabeled genomes, then adapt these representations to specific tasks through fine-tuning or few-shot learning. The intuition is that patterns relevant to regulatory function, splice site recognition, and protein folding are embedded in sequence statistics themselves. A model that learns to predict missing nucleotides or adjacent sequence context must implicitly capture motifs, constraints, and compositional structure that generalize across tasks. Self-supervised pretraining transforms raw sequence abundance into learned representations that make downstream labeled data go further.\nThe choice of pretraining objective fundamentally shapes what a model learns. Masked language modeling encourages bidirectional context integration, teaching models to recognize patterns from both upstream and downstream sequence. Next-token prediction builds generative capabilities, enabling models to sample novel sequences that respect learned grammar. Contrastive learning teaches invariance, producing representations where functionally equivalent sequences map to nearby embeddings regardless of superficial differences. Multi-task objectives combine supervision signals from diverse functional assays, learning representations that capture chromatin state, gene expression, and evolutionary conservation simultaneously. Each objective encodes different assumptions about what matters in biological sequence, and these assumptions propagate through to downstream performance.\nMasked language modeling treats sequences as partially observed and trains models to reconstruct missing content from surrounding context. The objective is straightforward: randomly mask portions of an input sequence, feed the corrupted sequence to the model, and train the model to predict the original tokens at masked positions. A masking strategy replaces selected tokens with a special [MASK] token, leaving the surrounding context intact. The model processes the masked sequence through its layers and produces predictions for the masked positions, typically optimizing cross-entropy loss over the vocabulary at each masked location.\nThe key insight is that accurate prediction requires learning genuine sequence structure. To predict a masked position in a transcription factor binding site, the model must recognize the surrounding motif context. To predict masked splice donor sequences, the model must encode the consensus patterns characteristic of exon-intron boundaries. Over millions of training examples, models build distributed representations of motifs, compositional rules, and sequence constraints that transfer to tasks never seen during pretraining.\nMLM encourages bidirectional context integration. Unlike autoregressive models that condition only on past tokens, MLM models see both left and right context when predicting masked positions. For genomics, this is biologically appropriate: regulatory function depends on patterns both upstream and downstream of any given position. A transcription factor binding site is recognized through flanking sequences on both sides. Splicing signals require coordination between donor and acceptor sites separated by hundreds of bases. The bidirectional attention in MLM naturally captures these dependencies.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#masked-language-modeling",
    "href": "p2-ch08-pretraining.html#masked-language-modeling",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "",
    "text": "8.1.1 Masking Strategies and Their Implications\nThe choice of masking strategy significantly impacts what models learn. Random masking of individual tokens creates predictions that are relatively local, where each masked position can often be inferred from immediately adjacent nucleotides. Span masking, which masks contiguous blocks of tokens, forces models to infer longer-range dependencies and compositional patterns. If an entire transcription factor binding motif is masked, the model cannot rely on partial motif information and must instead recognize the motif’s role from surrounding regulatory context.\nMasking rates present a fundamental tradeoff. Higher masking rates (30-40% of tokens) provide more supervision per sequence but make prediction harder and may destabilize training. Lower masking rates (10-15%) produce more stable training but require more data to achieve equivalent coverage. The standard 15% rate from BERT represents a reasonable compromise, though genomic models have explored a range of values depending on context length and tokenization granularity. DNABERT used 15% masking on 6-mer tokens, while later models have experimented with adaptive masking rates that increase as training progresses.\nTokenization interacts with masking in important ways. DNABERT pioneered MLM for genomic sequences by applying it to overlapping k-mer tokens: rather than treating DNA as individual nucleotides, DNABERT tokenizes sequences into all possible 6-mers with overlapping windows. Masking then operates at the k-mer level, with entire 6-mers masked as units. This design encourages learning of k-mer level patterns corresponding to transcription factor binding motifs and other short functional elements. DNABERT-2 adopted byte-pair encoding tokenization, which learns a vocabulary of variable-length subword units from the training corpus. BPE tokens represent single nucleotides, common motifs, or repeated elements depending on their frequency. MLM with BPE balances flexibility with compositional structure, though the learned vocabulary may not align with biological functional units.\n\n\n8.1.2 What Masked Language Models Learn\nMLM objectives drive models to capture multiple levels of sequence organization. At the lowest level, models learn nucleotide statistics and local constraints: CpG dinucleotide frequencies, GC content biases, and simple repeat patterns. These basic properties are necessary but not sufficient for biological function.\nAt higher levels, MLM captures motif patterns and sequence grammar. Predicting masked positions in regulatory regions requires recognizing transcription factor binding sites, understanding how motifs combine, and learning context-dependent usage patterns. If certain transcription factor motifs co-occur at specific distances, masking one motif and predicting it from the other reinforces this grammatical relationship. This compositional learning is difficult to achieve with supervised learning alone, which typically provides coarse binary labels rather than fine-grained structural information about sequence organization.\nMLM also captures evolutionary conservation patterns implicitly. Conserved sequences are constrained because mutations would disrupt function. By learning to predict conserved patterns from surrounding context, models encode which sequence features are under selection. This knowledge transfers to variant effect prediction, where the model recognizes when a mutation disrupts a learned conserved pattern. The connection between pretraining on raw sequence and downstream variant interpretation illustrates how self-supervised objectives can capture biologically meaningful structure without explicit functional labels.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#next-token-prediction",
    "href": "p2-ch08-pretraining.html#next-token-prediction",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.2 Next-Token Prediction",
    "text": "8.2 Next-Token Prediction\nNext-token prediction represents an alternative paradigm where models learn to predict each token in a sequence given only the preceding tokens. This autoregressive approach, popularized by GPT-style language models, treats sequence generation as a core capability rather than a secondary feature. For a sequence of length \\(T\\), the model predicts token \\(t\\) from tokens \\(1\\) through \\(t-1\\), maximizing the likelihood of the observed sequence under the model’s learned distribution.\nAlgorithmically, next-token prediction requires causal masking in the attention mechanism. Each position attends only to earlier positions, ensuring predictions at position \\(t\\) depend exclusively on positions \\(1\\) through \\(t-1\\). This matches the conditional probability factorization inherent in the objective: the probability of a sequence factors as the product of conditional probabilities for each token given its predecessors. The loss function is cross-entropy over the vocabulary, computed at every position rather than only at masked locations.\nThe fundamental difference from MLM lies in what the model can see during prediction. Autoregressive models build representations from unidirectional context, learning to generate sequences that respect learned constraints. This makes autoregressive pretraining attractive for sequence design applications. Sampling new sequences proceeds naturally: predict the first token, condition on it to predict the second, and continue token by token. The generation process directly uses the learned conditional distributions without requiring additional architectural modifications.\n\n8.2.1 Genomic Applications\nDNA sequences present a complication: they have no inherent directionality. Both strands encode information, and regulatory function is often strand-agnostic. This contrasts with natural language, where left-to-right reading order is meaningful. Early autoregressive genomic models addressed this by training separate models for forward and reverse strands or by augmenting training data with reverse-complement sequences. More recent approaches treat strand symmetry as an architectural constraint, ensuring that forward and reverse complement sequences produce equivalent representations.\nEvo represents a large-scale autoregressive genomic model trained on whole genomes with long-context architectures. By predicting next tokens across chromosome-length sequences, Evo learns long-range dependencies and can generate coherent synthetic genomes. This capability enables designing regulatory circuits, generating training data through synthetic augmentation, and exploring sequence space beyond observed genomes. The autoregressive formulation makes conditional generation straightforward: to generate sequences with desired properties, incorporate conditioning information into the context at each prediction step.\nProtein sequence models also benefit from autoregressive pretraining. The N-terminus to C-terminus directionality of protein synthesis provides biological justification for left-to-right prediction. ESM models and their successors predict amino acid sequences autoregressively, learning protein grammar and evolutionary constraints that transfer to structure prediction and function annotation. The success of protein language models demonstrates that autoregressive objectives can capture deep biological structure when training data is sufficiently diverse.\n\n\n8.2.2 Comparing MLM and Autoregressive Objectives\nThe choice between MLM and next-token prediction involves several considerations that depend on intended downstream applications. For tasks requiring understanding of full sequence context, MLM’s bidirectional attention provides richer representations. Predicting transcription factor binding at a specific location benefits from seeing both upstream and downstream sequence, information that autoregressive models cannot access during inference.\nFor generative tasks, autoregressive models are more principled. Their sequential prediction structure matches the generation process exactly, whereas generating from MLM models requires iterative masking and filling procedures or auxiliary generative heads that were not part of pretraining. Autoregressive models also handle variable-length sequences naturally and can process streaming data where sequence length is not known in advance.\nTraining efficiency differs between objectives in subtle ways. MLM predicts only 15% of tokens per sequence but uses bidirectional context for each prediction. Autoregressive models predict all tokens but with unidirectional context. During training, teacher forcing allows efficient parallel computation for autoregressive models: the model predicts all positions simultaneously by feeding in the ground truth sequence shifted by one position. Generation at inference time is inherently sequential and slower, but pretraining speed is comparable to MLM.\nTask-specific performance depends on alignment between pretraining and downstream objectives. If the downstream task involves predicting missing information from context (variant effect prediction, binding site identification), MLM pretraining provides better transfer. If the downstream task involves generation or sequential decision-making (sequence design, sampling from conditional distributions), autoregressive pretraining aligns more naturally. For applications requiring both understanding and generation, hybrid approaches that combine bidirectional encoding with autoregressive decoding offer a middle ground, though these add architectural complexity.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#span-corruption-and-denoising",
    "href": "p2-ch08-pretraining.html#span-corruption-and-denoising",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.3 Span Corruption and Denoising",
    "text": "8.3 Span Corruption and Denoising\nSpan corruption generalizes masked language modeling by introducing more complex forms of input degradation. The T5 model popularized this approach for natural language, and the principles transfer to genomic sequences. Rather than masking individual tokens, span corruption masks contiguous spans of variable length and replaces each span with a single sentinel token. The model then generates the original content of all masked spans in sequence, learning to reconstruct substantial missing regions rather than isolated positions.\nThis objective teaches different aspects of sequence structure than standard MLM. Reconstructing entire spans requires understanding longer-range dependencies and compositional patterns. If a span encompasses an entire transcription factor binding motif, the model cannot infer the motif from partial information and must instead reason about the motif’s role from surrounding regulatory context. Span lengths are typically sampled from a distribution (geometric or uniform) with a mean around 3-5 tokens, creating a mix of short and long reconstruction challenges within each training example.\nDenoising objectives extend beyond masking to include other forms of corruption. Token substitution replaces input tokens with random tokens from the vocabulary, creating corrupted sequences that resemble sequencing errors or natural variation. The model learns to distinguish correct from incorrect tokens based on surrounding context, encouraging representations that capture local consistency and motif structure. Deletion and insertion corruptions remove or add tokens at random positions, teaching models about position-invariant features that remain identifiable despite surrounding changes. For genomics, insertions and deletions are biologically realistic mutation types, and models that handle them during pretraining may better predict their effects downstream.\n\n8.3.1 Biologically Motivated Corruption\nSimulating sequencing errors provides corruption strategies grounded in experimental reality. Base miscalls follow platform-specific patterns: Illumina sequencing shows characteristic substitution biases, while nanopore sequencing exhibits distinct error profiles concentrated in homopolymer regions. Training with corruptions that mimic these error patterns may improve generalization to real sequencing data with platform-specific artifacts.\nVariant augmentation introduces biologically realistic sequence changes based on population variation. Randomly substituting alleles at known polymorphic sites or injecting variants from databases like gnomAD creates corrupted sequences reflecting natural genetic diversity. The model learns that common polymorphisms represent normal variation rather than errors to be corrected, potentially improving robustness for variant effect prediction where distinguishing pathogenic variants from benign polymorphisms is the central challenge.\nStructural variation simulation models larger-scale genomic changes: tandem duplications, copy number variation, and segmental rearrangements. These corruptions are harder to implement but capture realistic sources of genomic diversity beyond single-nucleotide changes. Models trained with structural variation corruptions may better understand how gene dosage changes, enhancer duplications, or domain boundary disruptions affect function.\nThe benefit of denoising pretraining extends to robustness under distribution shift. If downstream applications involve sequences from different populations, experimental platforms, or tissue contexts than the pretraining corpus, models pretrained with appropriate corruptions can maintain performance despite distribution mismatch. This is valuable in clinical genomics, where validation cohorts often differ from discovery cohorts in ancestry, sequencing technology, or phenotyping protocols.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#contrastive-learning",
    "href": "p2-ch08-pretraining.html#contrastive-learning",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.4 Contrastive Learning",
    "text": "8.4 Contrastive Learning\nContrastive learning takes a fundamentally different approach to self-supervised pretraining. Rather than reconstructing corrupted inputs, contrastive objectives train models to produce similar representations for different views of the same sequence while distinguishing them from representations of unrelated sequences. The intuition is that augmented versions of a sequence (with minor corruptions or transformations) should map to nearby points in representation space, while unrelated sequences should map to distant points.\nThe algorithmic framework constructs positive pairs and negative samples. For a given anchor sequence, positive pairs are created through augmentation: reverse complementation, random cropping, variant injection, or other transformations that preserve functional identity. Negative samples are drawn from other sequences in the training batch. The model produces embeddings for all sequences, and the contrastive loss encourages anchor and positive embeddings to be similar (high cosine similarity) while pushing apart anchor and negative embeddings.\nInfoNCE loss is the most common contrastive objective. For an anchor embedding \\(z_i\\) and positive embedding \\(z_i^+\\), InfoNCE maximizes:\n\\[\\log \\frac{\\exp(z_i \\cdot z_i^+ / \\tau)}{\\sum_j \\exp(z_i \\cdot z_j / \\tau)}\\]\nwhere the sum runs over the positive and all negative samples, and \\(\\tau\\) is a temperature parameter controlling the sharpness of the distribution. Lower temperatures make the model more discriminative, requiring cleaner separation between positives and negatives.\n\n8.4.1 Augmentation Design for Genomic Sequences\nAugmentation design is critical for contrastive learning because augmentations must preserve functional identity while introducing variability. If augmentations change function, the contrastive objective will learn meaningless invariances. Several augmentation strategies are biologically grounded and preserve the functional relationships that matter for downstream tasks.\nReverse complementation is the simplest and most reliable augmentation. DNA is double-stranded, and many regulatory elements function identically on either strand. Training the model to treat forward and reverse complement sequences as equivalent captures strand symmetry inherent in molecular biology. This augmentation is universally applicable and introduces no risk of changing functional identity.\nRandom cropping extracts overlapping windows from longer sequences. If a transcription factor binding site appears in multiple cropped windows, the model learns that the binding site is the functionally relevant feature regardless of absolute position or surrounding context. This teaches position-invariant representations useful for tasks where genomic coordinates matter less than local sequence content.\nVariant injection introduces common polymorphisms or simulated mutations. If the variants are neutral or do not disrupt function, treating variant and reference sequences as positive pairs teaches robustness to genetic variation. This is valuable for cross-population generalization, where models must recognize functional elements despite surrounding sequence polymorphism that differs between ancestry groups.\nNegative sampling strategies also affect what models learn. Random genomic sequences provide straightforward negatives but may be too easy to distinguish: any functional regulatory sequence is readily separable from random intergenic sequence. Harder negatives, such as sequences from orthologous regions in related species or sequences with similar motif content but different functional annotations, provide more informative supervision that forces the model to learn subtle discriminative features.\n\n\n8.4.2 Cross-Species Contrastive Learning\nCross-species contrastive learning leverages evolutionary relationships for self-supervision. Orthologous sequences from different species share functional identity despite nucleotide divergence accumulated over millions of years of evolution. Treating orthologous pairs as positives and non-orthologous pairs as negatives teaches the model to extract species-invariant functional features. A human enhancer and its mouse ortholog should map to similar embeddings despite sequence differences, while unrelated sequences should map to distant embeddings.\nThis approach can improve cross-species transfer. A model pretrained with human-mouse contrastive pairs may generalize better to rat, primate, or other mammalian sequences by learning to ignore species-specific sequence differences while preserving functionally relevant patterns. The evolutionary record provides implicit labels about functional equivalence that would be expensive to obtain through experimental annotation.\nSequence embedding quality improves with contrastive pretraining in ways that benefit downstream applications. Models trained contrastively produce embedding spaces where functionally similar sequences cluster together, enabling nearest-neighbor search, sequence retrieval, and unsupervised clustering of regulatory elements. Variant effect prediction benefits through improved robustness: if the model learns that sequences differing only by neutral variants are functionally equivalent, it will better distinguish truly disruptive variants from benign polymorphisms.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#multi-task-pretraining",
    "href": "p2-ch08-pretraining.html#multi-task-pretraining",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.5 Multi-Task Pretraining",
    "text": "8.5 Multi-Task Pretraining\nMulti-task pretraining combines multiple related objectives during the same training run, jointly optimizing for several prediction tasks. Different tasks provide complementary supervision signals: masking captures local patterns, chromatin prediction captures regulatory function, and conservation scoring captures evolutionary constraint. Representations that satisfy all tasks simultaneously may develop richer and more general features than any single objective alone.\nTask selection is the first design decision. Ideally, tasks should be diverse enough to provide distinct supervision signals but related enough to benefit from shared representations. For genomic models, effective combinations include masked language modeling for general sequence structure, chromatin accessibility prediction for regulatory function, gene expression prediction for transcriptional output, evolutionary conservation scoring for functional constraint, and variant frequency prediction from population databases. Each task operates on the same input sequence but predicts different outputs using task-specific head layers. The shared backbone encoder processes the sequence into intermediate representations, and separate prediction heads map these representations to task-specific outputs.\nTask weighting determines how much each task contributes to the total loss. With \\(L_1, \\ldots, L_K\\) representing individual task losses, the multi-task loss combines them:\n\\[L_{\\text{total}} = \\sum_{k=1}^K w_k L_k\\]\nwhere \\(w_k\\) are task weights. Equal weighting is simple but may lead to imbalanced learning if tasks have different scales or difficulties. Dynamic weighting approaches adjust weights during training based on learning progress, using the magnitude of task losses or gradient norms as signals for rebalancing.\n\n8.5.1 Large-Scale Multi-Task Examples\nEnformer and Borzoi exemplify large-scale multi-task pretraining for genomics. Enformer predicts over 5,000 genomic assays simultaneously: ChIP-seq signals for hundreds of transcription factors and histone marks, DNase-seq and ATAC-seq accessibility, CAGE transcription initiation, and more. This massive multi-task objective forces the model to learn representations capturing diverse regulatory signals across cell types and experimental conditions.\nThe task diversity in Enformer provides supervision far richer than any single assay. A model trained only on DNase-seq learns general accessibility patterns but misses transcription factor specificity. A model trained only on H3K27ac ChIP-seq captures active enhancers but misses repressive marks. Training on all assays jointly allows the model to disentangle overlapping and complementary signals, learning representations that generalize across regulatory contexts. Borzoi extends this paradigm to full RNA-seq coverage prediction, jointly modeling transcription initiation, splicing, and transcript abundance.\nCombining MLM with functional prediction represents another multi-task configuration. The model predicts masked tokens through a language modeling head while simultaneously predicting chromatin accessibility or other functional readouts through regression heads. This hybrid objective balances sequence-level pretraining with functional supervision, potentially combining the benefits of both approaches. The MLM component ensures the model learns general sequence patterns even in regions without functional annotations, while the functional prediction component focuses learning on biologically relevant features.\n\n\n8.5.2 When Multi-Task Helps and When It Hurts\nTask interference is the primary concern with multi-task learning. If tasks require conflicting representations, jointly optimizing for both may compromise performance on each compared to single-task baselines. In genomics, this might occur if one task benefits from very local features while another requires long-range context, forcing the model to choose suboptimal representations for both.\nNegative transfer occurs when adding a task actually hurts downstream performance compared to training without it. This can happen if the additional task introduces noise, if task weights are poorly balanced, or if the auxiliary task shifts learned representations away from features useful for target applications. The risk of negative transfer increases with task diversity: distantly related tasks are more likely to require conflicting representations.\nThe benefits of multi-task pretraining are largest when tasks are complementary and data for individual tasks is limited. If chromatin data is sparse for a particular cell type but gene expression data is abundant, jointly training on both may improve performance on both compared to single-task models. The shared representations allow information to flow between tasks, compensating for data scarcity in any single modality. When functional labels exist at scale and tasks are genuinely related, multi-task pretraining consistently outperforms single-task alternatives.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#what-different-objectives-teach",
    "href": "p2-ch08-pretraining.html#what-different-objectives-teach",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.6 What Different Objectives Teach",
    "text": "8.6 What Different Objectives Teach\nThe choice of pretraining objective encodes assumptions about what matters in biological sequence, and these assumptions propagate through to downstream performance. Understanding what each objective teaches helps practitioners select appropriate strategies for their applications.\nMasked language modeling teaches bidirectional sequence understanding. Models learn to recognize patterns from both directions, capturing the full context that determines biological function at any position. This makes MLM well-suited for classification and interpretation tasks: predicting whether a sequence is a binding site, identifying regulatory elements, or scoring variant effects based on sequence context. The bidirectional representations naturally capture dependencies that span the prediction target.\nNext-token prediction teaches generative sequence modeling. Models learn the distribution over next tokens given preceding context, enabling coherent sequence generation that respects learned grammar. This makes autoregressive pretraining ideal for sequence design applications: generating novel promoters, designing therapeutic proteins, or creating synthetic regulatory circuits. The generative capability is inherent to the objective rather than an afterthought.\nContrastive learning teaches invariant representations. Models learn which sequence features matter for functional identity and which are incidental details that vary across examples. This makes contrastive pretraining valuable for cross-species transfer, robustness to genetic variation, and applications where functional equivalence matters more than sequence identity. The learned embedding spaces cluster functionally similar sequences regardless of superficial differences.\nMulti-task learning teaches shared structure across diverse functional readouts. Models learn representations that simultaneously predict chromatin state, gene expression, conservation, and other outputs, discovering latent structure that underlies multiple aspects of genome function. This makes multi-task pretraining effective when downstream tasks involve predicting functional properties that relate to the pretraining tasks, though the benefits depend on task complementarity.\nDenoising objectives teach robustness to corruption and noise. Models learn to recover original sequences from degraded inputs, building tolerance to sequencing errors, natural variation, and distribution shift. This makes denoising pretraining valuable when downstream applications involve data from different platforms, populations, or experimental conditions than the training corpus.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#data-strategies-for-pretraining",
    "href": "p2-ch08-pretraining.html#data-strategies-for-pretraining",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.7 Data Strategies for Pretraining",
    "text": "8.7 Data Strategies for Pretraining\nCorpus construction establishes the foundation for pretraining. The choice of training data determines what patterns the model can learn and how well it will generalize. For genomic models, this involves decisions about reference genomes, population variation, repeat handling, and chromosome segmentation.\nReference genomes are the standard starting point. Human genome assemblies like GRCh38 provide high-quality, contiguous sequence spanning all chromosomes. Training on the reference genome allows models to learn patterns characteristic of human DNA: base composition, repeat structure, gene organization, and regulatory architecture. The reference genome represents a single haploid consensus, missing variation present in human populations, but provides the foundation for most pretraining approaches.\nPopulation-scale variation can be incorporated through variant databases. Rather than training only on reference sequence, injecting variants at observed population frequencies creates synthetic diploid genomes reflecting real genetic diversity. This teaches models that common polymorphisms are normal variation, potentially improving robustness and variant effect prediction. Pan-genome approaches extend this by representing multiple high-quality assemblies from diverse individuals, capturing structural variation and population-specific haplotypes that a single reference cannot represent.\nRepeat handling impacts pretraining in ways that depend on downstream applications. Simple repeats, tandem repeats, and transposable elements occupy substantial genomic fractions but contribute less to regulatory function than unique sequences. Hard-masking repeats (replacing them with Ns) reduces training data but may discard information relevant to some tasks. Soft-masking retains sequence information while marking repetitive regions, allowing models to learn differential representations for repeats and unique sequences.\nData augmentation artificially increases training diversity. Reverse complementation exploits DNA strand symmetry, doubling effective training data. Random cropping extracts variable-length windows, teaching position-invariant features. Variant injection simulates genetic variation, building robustness to population diversity. These augmentations are typically applied on-the-fly during training rather than pre-computed, maintaining flexibility in the training pipeline.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#optimization-and-scaling",
    "href": "p2-ch08-pretraining.html#optimization-and-scaling",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.8 Optimization and Scaling",
    "text": "8.8 Optimization and Scaling\nEffective pretraining requires careful attention to optimization details that become critical at scale. Learning rate schedules, batch sizing, gradient handling, and numerical precision all affect training stability and final model quality.\nLearning rate warmup gradually increases the learning rate from near-zero over the first several thousand steps. This prevents early training instability when the model has random initializations and large gradient variance. After warmup, cosine decay schedules reduce the learning rate following a cosine curve from peak to near-zero over training, providing aggressive learning early when gradients are most informative and gentle refinement late as the model approaches convergence.\nGradient clipping prevents training instability from occasional large gradients. Clipping by global norm scales all gradients proportionally when the total norm exceeds a threshold, maintaining gradient direction while controlling magnitude. This is standard practice for transformer models where exploding gradients can occur, particularly with long sequences where attention matrices span many positions.\nMixed precision training uses lower-precision arithmetic (float16 or bfloat16 instead of float32) to reduce memory consumption and accelerate computation on modern GPUs. Loss scaling prevents numerical underflow in float16, and careful handling of gradient updates ensures stability. Mixed precision is now standard for large-scale pretraining, roughly doubling throughput with minimal impact on model quality.\nPretraining scales with model size, sequence length, and dataset size in predictable ways. Larger models with more parameters capture more complex patterns but require more data and compute to train. Longer sequence contexts enable learning of long-range dependencies but increase memory requirements quadratically for standard attention. More diverse training data improves generalization but requires proportionally more training time. The relationships between these factors follow scaling laws that predict optimal resource allocation given a compute budget, a topic developed further in Chapter 10 where we examine how these principles define the foundation model paradigm.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#monitoring-and-debugging",
    "href": "p2-ch08-pretraining.html#monitoring-and-debugging",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.9 Monitoring and Debugging",
    "text": "8.9 Monitoring and Debugging\nPretraining runs span days to weeks, making early detection of issues essential for avoiding wasted computation. Careful monitoring tracks training progress and identifies problems before they become catastrophic.\nTraining loss curves should decrease smoothly in early stages, eventually plateauing as the model approaches convergence. Sudden spikes suggest numerical instability, inappropriate learning rates, or corrupted data batches. Persistent plateaus may indicate insufficient model capacity, inappropriate objectives, or optimization hyperparameters that prevent further improvement. Tracking loss on held-out validation data monitors generalization: if training loss decreases while validation loss increases, the model is overfitting to the training corpus.\nGradient norms indicate whether optimization is proceeding normally. Very small gradients suggest the vanishing gradient problem, preventing effective learning in early layers. Very large gradients suggest instability that gradient clipping should catch. Tracking per-layer gradient norms helps diagnose where problems originate in deep networks.\nProbing tasks provide functional sanity checks during pretraining. Simple downstream evaluations can be run periodically on intermediate checkpoints to verify that learned representations capture biologically meaningful patterns. If probing performance plateaus or degrades while pretraining loss continues improving, the model may be learning patterns that do not transfer to downstream tasks.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#choosing-the-right-strategy",
    "href": "p2-ch08-pretraining.html#choosing-the-right-strategy",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.10 Choosing the Right Strategy",
    "text": "8.10 Choosing the Right Strategy\nSelecting a pretraining approach involves balancing computational budget, target downstream tasks, data availability, and model architecture constraints. No single strategy is universally optimal, so understanding when each approach excels guides practical decisions.\nFor most general-purpose DNA or protein models, MLM pretraining provides a strong default. It learns bidirectional context, scales efficiently, and transfers well to diverse downstream tasks. DNABERT and DNABERT-2 exemplify this approach for genomics, while ESM models demonstrate its effectiveness for proteins. Start with MLM unless there is a specific reason to prefer alternatives.\nNext-token prediction is preferred when generation is the primary goal. If designing sequences from scratch, sampling from autoregressive models produces coherent outputs respecting learned grammar. Evo and similar models demonstrate this for genomic sequence generation. The autoregressive structure makes conditional generation straightforward, enabling design applications that MLM does not naturally support.\nMulti-task pretraining makes sense when functional labels are available at scale and tasks are complementary. Enformer’s success with thousands of chromatin assays demonstrates the power of multi-task learning when data supports it. The infrastructure requirements are higher, including handling heterogeneous data and balancing losses across tasks, but the resulting representations capture functional information that pure sequence-based objectives miss.\nContrastive learning is valuable for cross-species applications or when robustness to variation is critical. If transferring models trained on model organisms to related species, or improving robustness to genetic polymorphism, contrastive pretraining on orthologous pairs or variant-augmented sequences provides targeted benefits that other objectives do not address directly.\nWhen deciding whether to pretrain from scratch or start from existing models, starting from pretrained checkpoints is almost always preferable if an appropriate model exists. Fine-tuning a DNABERT-2 checkpoint on a new task is faster and more data-efficient than training from scratch. Pretraining from scratch is necessary when using new tokenization schemes (incompatible vocabularies), targeting species without suitable existing models, or switching to fundamentally different architectures where pretrained weights cannot transfer.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#pretraining-in-practice-case-studies",
    "href": "p2-ch08-pretraining.html#pretraining-in-practice-case-studies",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.11 Pretraining in Practice: Case Studies",
    "text": "8.11 Pretraining in Practice: Case Studies\nExamining how successful models were pretrained provides concrete lessons and design patterns that inform new projects.\nDNABERT introduced MLM pretraining to genomics by adapting BERT’s architecture to DNA sequences with overlapping k-mer tokenization. The model was pretrained on the human genome with 6-mer tokens, masking 15% of tokens at random. Standard BERT hyperparameters proved effective: AdamW optimizer with warmup, dropout regularization, and layer normalization. The key lessons include the importance of tokenization choice (k-mers capture motif-level patterns better than single nucleotides), the value of reverse complement augmentation for strand symmetry, and the transferability of representations (pretrained DNABERT generalized to diverse regulatory tasks despite training only on raw genome sequence).\nHyenaDNA demonstrated that efficient long-range architectures enable pretraining on extremely long contexts. By using Hyena layers with subquadratic complexity, HyenaDNA scaled to contexts spanning one million bases, far beyond standard transformers. Pretraining used single-nucleotide next-token prediction with a curriculum that progressively increased context length. The lessons include the feasibility of million-base contexts with appropriate architectures, the benefits of curriculum learning for context scaling, and the emergence of long-range regulatory patterns when models have sufficient receptive field.\nEnformer pioneered multi-task chromatin prediction at scale. The model was pretrained jointly on over 5,000 assays from ENCODE, Roadmap Epigenomics, and related consortia, using a hybrid convolutional-transformer architecture with 200 kilobase context. Task weighting was balanced to prevent any single assay from dominating. The key insights include the power of large-scale multi-task learning for capturing diverse regulatory signals, the effectiveness of combining convolutions for local patterns with transformers for long-range interactions, and the interpretability benefits of attention patterns that reveal learned enhancer-promoter relationships.\nESM-2 represents the state of the art for protein language models, scaling to 15 billion parameters trained on evolutionary databases containing billions of protein sequences. Pretraining used standard MLM on amino acid sequences at unprecedented scale. The lessons include the continued benefit of scaling (larger models and more data improve even at billions of parameters), the value of evolutionary diversity (pretraining on hundreds of millions of protein families captures constraints invisible in individual genomes), and the emergence of structural understanding from sequence alone (ESM-2 representations encode 3D structure despite no explicit structural supervision).",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#open-questions",
    "href": "p2-ch08-pretraining.html#open-questions",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.12 Open Questions",
    "text": "8.12 Open Questions\nDespite rapid progress, fundamental questions about genomic pretraining remain open. Optimal objective combinations are unclear: should we jointly train with MLM and chromatin prediction, or train sequentially? How many auxiliary tasks help before diminishing returns? Do contrastive and generative objectives complement each other or interfere?\nIncorporating biological priors versus learning from scratch presents a design tension. Known motifs, pathway structure, and evolutionary constraints could be encoded in model architecture or initialization. Hand-engineered features risk encoding false assumptions, but pure data-driven learning may rediscover basic biology inefficiently. Hybrid approaches combining priors with learned representations are underexplored.\nContinual pretraining as new data arrives is increasingly relevant. As sequencing technologies improve and new assays emerge, updating pretrained models without catastrophic forgetting of prior knowledge presents challenges. Online learning and elastic weight consolidation are potential solutions largely untested in genomics.\nThe relationship between pretraining scale and downstream performance follows predictable patterns that are still being characterized for genomic models. Understanding these relationships more precisely would guide resource allocation and set realistic expectations for what different scales of pretraining can achieve. These scaling considerations connect to the broader foundation model paradigm examined in Chapter 10.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch08-pretraining.html#summary",
    "href": "p2-ch08-pretraining.html#summary",
    "title": "8  Pretraining Objectives and Strategies",
    "section": "8.13 Summary",
    "text": "8.13 Summary\nPretraining objectives transform abundant unlabeled sequence into learned representations that improve data efficiency and generalization for downstream tasks. Masked language modeling teaches bidirectional sequence understanding, making it the default choice for most genomic applications. Next-token prediction teaches generative capabilities essential for sequence design. Contrastive learning teaches invariance and robustness. Multi-task pretraining captures functional information when diverse labeled data is available at scale. Denoising objectives build tolerance to noise and distribution shift.\nThe choice of objective shapes what models learn in ways that propagate to downstream performance. Aligning pretraining objectives with intended applications improves transfer: bidirectional objectives for classification and interpretation, autoregressive objectives for generation, contrastive objectives for cross-species transfer and robustness. Data strategies, optimization details, and scaling considerations all affect final model quality.\nSelf-supervised pretraining has become the standard approach for building genomic foundation models. The next chapter (Chapter 9) examines how to adapt pretrained models to specific downstream tasks through fine-tuning, probing, and parameter-efficient methods that leverage pretraining investment for new applications.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Objectives and Strategies</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html",
    "href": "p2-ch09-transfer.html",
    "title": "9  Transfer Learning and Adaptation",
    "section": "",
    "text": "9.1 The Promise of Universal Representations\nTrain once, deploy everywhere. This vision drives the foundation model paradigm: invest heavily in pretraining on massive unlabeled data, then adapt the resulting model cheaply to any downstream task. The appeal is obvious. Rather than training separate models for splice site prediction, variant effect scoring, and regulatory element identification, a single pretrained model provides a universal starting point that can be fine-tuned with minimal labeled data. The representations learned during pretraining capture general sequence patterns (motifs, compositional regularities, long-range dependencies) that transfer across tasks, reducing both the data and compute required for each new application.\nThe reality proves more nuanced. Transfer learning succeeds spectacularly in some settings: protein language models predict variant effects with near-zero task-specific training, achieving competitive performance through log-likelihood ratios alone. DNA foundation models enable chromatin accessibility prediction from a few hundred labeled examples. Yet transfer also fails in subtle and frustrating ways. Models trained on human sequences struggle with other species. Representations optimized for coding regions miss noncoding regulatory logic. Tasks that seem conceptually similar turn out to require incompatible features. The central challenge of transfer learning is not whether to adapt pretrained models, but knowing when adaptation will help and choosing strategies that maximize benefit while minimizing risk.\nThe adaptation strategies examined in this chapter span a spectrum from minimal intervention (frozen features with a linear probe) to aggressive modification (full fine-tuning of all parameters). Each point on this spectrum trades off preservation of pretrained knowledge against flexibility for task-specific learning. Frozen approaches guarantee that pretrained representations remain intact but cannot escape their limitations. Fine-tuning enables task-specific optimization but risks overwriting useful general knowledge. Parameter-efficient methods attempt to navigate this tension, enabling targeted adaptation while protecting the bulk of pretrained weights. Understanding when each strategy is appropriate, and developing intuition for diagnosing transfer failures, separates effective practitioners from those who apply foundation models as black boxes.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#source-and-target-domains",
    "href": "p2-ch09-transfer.html#source-and-target-domains",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.2 Source and Target Domains",
    "text": "9.2 Source and Target Domains\nTransfer learning operates across two domains: the source domain where pretraining occurs and the target domain where the model will be deployed. Understanding what properties of each domain determine transfer success provides the conceptual foundation for choosing adaptation strategies.\nThe source domain in genomics typically consists of abundant unlabeled sequence data. For DNA models, this might span the human reference genome, pan-genomes from multiple species, or metagenomic collections from environmental samples. For protein models, UniRef and similar databases provide billions of sequences sampling the diversity of evolutionary history. Pretraining objectives like masked language modeling encourage models to learn regularities that help predict held-out tokens: local motifs, compositional patterns, long-range dependencies, and the statistical signatures that distinguish functional from random sequence. These learned regularities form the representations that transfer to downstream tasks.\nThe target domain differs fundamentally. Rather than abundant unlabeled data, the target domain offers sparse labeled examples of a specific task. A few thousand enhancer sequences with luciferase activity measurements. Several hundred variants with clinical pathogenicity classifications. Chromatin accessibility profiles across a handful of cell types. The target distribution may look nothing like the pretraining data. Pathogenic variants are rare outliers, not typical genomic sequence. Tissue-specific enhancers exhibit patterns that genome-wide pretraining may never emphasize. The gap between source and target distributions creates the core transfer learning challenge.\nSeveral factors determine whether this gap can be bridged. Task relatedness matters profoundly. If the target task requires sequence patterns similar to those the model encountered during pretraining, the pretrained representations will prove useful. If the target task depends on patterns the pretraining objective ignored or actively suppressed, transfer may provide little benefit. Target data quantity constrains which adaptation strategies are feasible. With thousands of labeled examples, aggressive fine-tuning can reshape representations toward task requirements. With dozens of examples, only the lightest-touch approaches avoid overfitting. Model architecture and scale influence adaptation flexibility: larger models with more expressive internal representations can potentially adapt to more diverse tasks, but also risk memorizing small target datasets.\nNot all transfer helps. Positive transfer accelerates learning or improves final performance beyond what training from scratch achieves. Negative transfer occurs when pretraining actively hurts performance, either because learned features conflict with task requirements or because the pretrained initialization creates optimization difficulties. Neutral transfer describes situations where pretraining neither helps nor hurts, often when target data is sufficient to learn effectively from scratch or when source and target domains share too little structure for meaningful knowledge sharing. Distinguishing these outcomes requires careful empirical validation, not assumptions about how transfer “should” work.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#feature-extraction-with-frozen-backbones",
    "href": "p2-ch09-transfer.html#feature-extraction-with-frozen-backbones",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.3 Feature Extraction with Frozen Backbones",
    "text": "9.3 Feature Extraction with Frozen Backbones\nThe simplest adaptation strategy treats the pretrained model as a fixed feature extractor. All backbone parameters remain frozen; only a lightweight classifier trained on top of the extracted representations learns from labeled data. This approach eliminates the risk of catastrophic forgetting (where fine-tuning overwrites useful pretrained knowledge) and requires minimal computation since gradients need not flow through the entire model.\nImplementation is straightforward. Pass input sequences through the frozen backbone to obtain embeddings, typically from the final layer or from a designated [CLS] token that aggregates sequence information. These embeddings serve as fixed feature vectors capturing the model’s learned understanding of sequence patterns. Train a supervised classifier (logistic regression, linear layer, or shallow neural network) to map embeddings to task labels. The backbone parameters never change.\nLinear probing represents the minimal variant. A single linear layer maps embeddings directly to predictions, introducing only \\(d \\times c\\) parameters where \\(d\\) is the embedding dimension and \\(c\\) is the number of output classes. This approach trains in minutes and resists overfitting even with very limited labeled data. DNABERT embeddings paired with linear probes achieve competitive chromatin accessibility prediction from a few hundred positive and negative examples, matching CNN baselines that require far more labeled data. The success reflects alignment between the pretraining objective (predicting masked k-mers from context) and the target task (distinguishing sequences based on local motif patterns).\nShallow multilayer perceptrons extend linear probing by adding one or two hidden layers between embeddings and predictions. The additional nonlinearity enables more complex decision boundaries while maintaining computational efficiency and overfitting resistance. With a few thousand labeled examples, shallow MLPs on HyenaDNA embeddings improve splice site prediction over linear probes, capturing interactions between features that linear models cannot represent.\nThe advantages of frozen feature extraction are clear. Computational cost is minimal. Catastrophic forgetting is impossible since pretrained parameters never change. Training completes quickly, enabling rapid experimentation across tasks. The approach works reliably even with very small labeled datasets. But these advantages come with a fundamental limitation: the backbone cannot adapt to task-specific patterns. Performance is capped by how well pretrained representations align with target task requirements. If the pretraining objective emphasized patterns irrelevant to the downstream task, or if the target task requires features the pretraining objective actively discouraged, frozen features will underperform models trained from scratch regardless of how sophisticated the classifier head becomes.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#parameter-efficient-fine-tuning",
    "href": "p2-ch09-transfer.html#parameter-efficient-fine-tuning",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.4 Parameter-Efficient Fine-Tuning",
    "text": "9.4 Parameter-Efficient Fine-Tuning\nParameter-efficient fine-tuning (PEFT) methods navigate the tension between frozen and full fine-tuning by updating a small subset of parameters while keeping the majority frozen. The model can adapt to task-specific patterns without the computational expense or overfitting risk of modifying all weights. Several PEFT techniques have emerged, with Low-Rank Adaptation (LoRA) proving particularly prominent in genomic applications.\nLoRA modifies weight matrices by adding low-rank decompositions. Rather than updating a large weight matrix \\(W\\) directly, LoRA introduces two smaller matrices \\(A\\) and \\(B\\) whose product approximates the desired weight change: \\(W' = W + BA\\). During fine-tuning, \\(W\\) remains frozen while only \\(A\\) and \\(B\\) receive gradient updates. The rank of these matrices (typically 8 to 64 for genomic models) controls the expressiveness of adaptation. Lower ranks introduce fewer parameters and stronger regularization; higher ranks enable more flexible task-specific modification at greater overfitting risk.\nThe memory and compute savings can be substantial. A transformer with hundreds of millions of parameters might require updating only a few million LoRA parameters, reducing memory requirements by 10 to 100 times compared to full fine-tuning. This efficiency enables training on consumer hardware for models that would otherwise require specialized infrastructure. LoRA adapters on Nucleotide Transformer have enabled tissue-specific expression prediction, where separate low-rank matrices capture tissue-specific regulatory patterns while the pretrained backbone encodes general sequence understanding.\nAdapter layers take a different architectural approach. Rather than modifying existing weight matrices, adapters insert small bottleneck modules between transformer layers. Each adapter consists of a down-projection reducing dimensionality, a nonlinear activation, and an up-projection restoring the original dimension. The original transformer parameters remain frozen; only adapter parameters are updated. Different adapters can be trained for different tasks, enabling multi-task deployment from a single shared backbone. Enformer with tissue-specific adapters predicts chromatin states across cell types, with each adapter learning tissue-specific transformations of shared regulatory representations.\nPrefix tuning prepends learnable embeddings to the input, effectively providing task-specific context that conditions model behavior. While less common in genomics due to the absence of natural “prompt” structure in biological sequence, prefix tuning has found limited application where task context can be encoded as additional input tokens. Other PEFT methods include BitFit (tuning only bias terms), prompt tuning (learning soft prompts), and compacter-style approaches combining low-rank decomposition with parameter sharing. These remain less explored in genomic contexts but may offer advantages for specific applications.\nPEFT methods suit intermediate data regimes (thousands to tens of thousands of examples) where frozen features underperform but full fine-tuning risks overfitting. They enable managing multiple related tasks efficiently, training separate lightweight adapters rather than separate full models. The computational savings make experimentation feasible on modest hardware, accelerating development cycles.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#full-fine-tuning",
    "href": "p2-ch09-transfer.html#full-fine-tuning",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.5 Full Fine-Tuning",
    "text": "9.5 Full Fine-Tuning\nFull fine-tuning updates all model parameters during adaptation, offering maximum flexibility to reshape representations for task-specific requirements. When target datasets are large enough and the target task diverges sufficiently from pretraining, full fine-tuning extracts more value from pretrained models than constrained approaches.\nImplementation requires careful attention to optimization details. Learning rates must be substantially lower than during pretraining (often 10 to 100 times smaller) to avoid catastrophically disrupting learned representations in early training steps. Gradual unfreezing, where top layers are updated first and deeper layers are progressively brought into training, helps preserve low-level features while allowing high-level task-specific adjustment. Regularization through weight decay, dropout, and early stopping on validation data prevents overfitting to the target dataset.\nFull fine-tuning is appropriate when labeled datasets are large (tens of thousands of examples or more), when the target task differs substantially from pretraining such that frozen or lightly adapted representations prove insufficient, or when performance requirements justify the computational investment. Enformer fine-tuned on new chromatin assays requires updating most parameters to capture assay-specific signal patterns distinct from original training conditions. The expense is justified when the adapted model will be deployed at scale or when the task is central to a research program.\nThe risks are real. Catastrophic forgetting occurs when fine-tuning overwrites general knowledge useful for related tasks or out-of-distribution inputs. A model fine-tuned aggressively on one tissue type may lose performance on others it previously handled well. Overfitting afflicts small target datasets, where the model memorizes training examples rather than learning generalizable patterns. Computational expense can be prohibitive for large models. These considerations make full fine-tuning a high-reward, high-risk strategy requiring careful validation that the investment yields genuine improvement over simpler approaches.\nBest practices emphasize starting conservatively. Begin with frozen features to establish whether transfer provides any benefit. If frozen features improve over training from scratch, try PEFT methods before committing to full fine-tuning. Always compare full fine-tuning against simpler baselines; if the performance gap is small, the additional cost may not be justified.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#probing-representations",
    "href": "p2-ch09-transfer.html#probing-representations",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.6 Probing Representations",
    "text": "9.6 Probing Representations\nProbing classifiers diagnose what information pretrained representations encode, guiding adaptation strategy by revealing alignment (or misalignment) between learned features and task requirements. Rather than immediately training for the target task, probing systematically interrogates representations to understand their structure.\nThe approach is simple: train lightweight classifiers to predict properties of interest from frozen embeddings, then examine how well different properties can be decoded. If chromatin accessibility can be predicted accurately from a linear probe, the representations already encode accessibility-relevant features and frozen feature extraction will likely succeed. If transcription factor binding requires a deep nonlinear classifier, the relevant information may be present but not linearly separable, suggesting that PEFT or fine-tuning might reorganize representations for easier extraction. If a property cannot be predicted even with flexible classifiers, the representations may lack the necessary information entirely.\nSystematic probing reveals what models learn during pretraining. DNA language models encode local motif information recoverable by simple linear probes but show weaker encoding of long-range dependencies that require multi-layer networks to extract. Protein language models learn secondary structure so thoroughly that linear probes achieve near state-of-the-art prediction accuracy, while tertiary contact information requires nonlinear decoding. These probing results explain why certain transfer tasks succeed readily (they rely on well-encoded features) while others prove difficult (they require information the model encodes weakly or not at all).\nProbing guides adaptation decisions. If the target task requires features that probing reveals as well-encoded, frozen feature extraction or minimal adaptation should suffice. If required features are present but deeply buried (requiring nonlinear probes to extract), PEFT methods may help by learning transformations that surface the relevant information. If probing suggests required features are absent or very weakly encoded, practitioners face a choice: either full fine-tuning to reshape representations (if target data is sufficient) or trying a different pretrained model whose pretraining objective better aligns with task requirements.\nLayer-wise probing reveals how information flows through the model. Early layers often encode local compositional features (k-mer frequencies, simple motifs) while later layers capture more abstract patterns (long-range dependencies, functional signatures). Tasks depending on local features may benefit from representations extracted from early or middle layers rather than the final layer, which may have abstracted away relevant details. Layer selection becomes another hyperparameter to optimize when adapting pretrained models.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#choosing-an-adaptation-strategy",
    "href": "p2-ch09-transfer.html#choosing-an-adaptation-strategy",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.7 Choosing an Adaptation Strategy",
    "text": "9.7 Choosing an Adaptation Strategy\nSelecting the appropriate adaptation strategy requires balancing three considerations: available labeled data, similarity between pretraining and target tasks, and computational budget. While no rigid rules cover all scenarios, several heuristics guide practical decisions.\nData quantity provides the primary decision axis. With fewer than 1,000 labeled examples, linear probing or shallow classifiers on frozen embeddings is often the only viable approach. More complex adaptation risks overfitting; the limited signal cannot support updating many parameters. With 1,000 to 10,000 examples, PEFT methods offer good tradeoffs between expressiveness and regularization. With more than 10,000 examples, full fine-tuning becomes feasible and may be necessary if the target task diverges substantially from pretraining.\nTask similarity to pretraining provides the second axis. When target tasks closely resemble pretraining patterns (predicting transcription factor binding after sequence pretraining involves similar local motif recognition), frozen feature extraction often suffices. The pretrained representations already capture relevant patterns; a simple classifier separates positive from negative examples. For moderately different tasks, PEFT methods enable selective adaptation. For tasks fundamentally different from pretraining (where required features were not emphasized or were actively suppressed during pretraining), full fine-tuning may be necessary to reshape representations.\nComputational budget imposes practical constraints. With minimal resources, only linear probing is feasible. With moderate resources, LoRA achieves much of fine-tuning’s benefit at a fraction of the cost. With generous resources, full fine-tuning becomes an option, though comparing against simpler baselines remains essential to verify the investment is justified.\nEmpirical validation trumps heuristics. No rule perfectly predicts which strategy will succeed. Always compare multiple approaches. Validate on held-out data matching the intended deployment distribution. Monitor for overfitting versus underfitting and adjust strategy accordingly. The goal is developing intuition for which strategies merit trying, not blindly following rules.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#domain-shift-and-cross-context-transfer",
    "href": "p2-ch09-transfer.html#domain-shift-and-cross-context-transfer",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.8 Domain Shift and Cross-Context Transfer",
    "text": "9.8 Domain Shift and Cross-Context Transfer\nTransfer learning assumes that knowledge from the source domain applies to the target domain. When this assumption breaks, performance degrades in ways that may not be obvious from standard validation metrics. Three types of domain shift commonly afflict genomic transfer: cross-species, cross-tissue, and cross-assay transfer.\nCross-species transfer applies models trained on one organism to another. The challenge is that evolutionary divergence introduces sequence differences affecting regulatory patterns, motif syntax, and functional constraints. Strategies for cross-species success include pretraining on multi-species data to learn conservation patterns, fine-tuning with species-specific adapters, and focusing on highly conserved features that transfer more readily. Human-to-mouse regulatory prediction faces both sequence divergence and lineage-specific regulatory innovations; core promoter elements transfer better than species-specific enhancers. Transfer across greater phylogenetic distances becomes progressively more difficult.\nCross-tissue transfer addresses tissue-specific regulatory programs. Gene expression and chromatin accessibility vary dramatically across tissues, with thousands of tissue-specific enhancers and repressors. Models trained predominantly on one tissue may miss regulatory logic specific to others. Effective strategies include shared backbones with tissue-specific prediction heads, tissue-conditional models taking tissue identity as input, and meta-learning approaches training across many tissues to extract general principles. Broadly expressed housekeeping genes transfer more readily than tissue-restricted genes.\nCross-assay transfer handles different molecular readouts of related biology. ChIP-seq and ATAC-seq both measure chromatin state but with different biochemistry and signal characteristics. Bulk and single-cell RNA-seq quantify expression at vastly different scales. Successful cross-assay transfer requires understanding the mechanistic relationships between assays and either multi-task pretraining across assays or explicit modeling of assay-specific biases.\nDetecting domain shift before deployment prevents silent failures. Statistical tests comparing source and target distributions flag potential problems. Embedding visualizations reveal whether target examples fall within the source distribution or in unfamiliar regions of representation space. Monitoring performance on “canary” examples (known easy cases that should always be predicted correctly) provides early warning of severe shift.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#few-shot-and-zero-shot-learning",
    "href": "p2-ch09-transfer.html#few-shot-and-zero-shot-learning",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.9 Few-Shot and Zero-Shot Learning",
    "text": "9.9 Few-Shot and Zero-Shot Learning\nFew-shot learning addresses scenarios with extremely scarce labels: 10 to 100 examples per class rather than thousands. This regime characterizes many genomic applications. Rare variant classes in ClinVar have few examples. Novel cell types in single-cell studies lack extensive annotation. Newly characterized functional elements have limited experimental validation. Standard adaptation approaches overfit catastrophically with so few examples.\nMeta-learning trains models explicitly for rapid adaptation. Model-Agnostic Meta-Learning (MAML) finds parameter initializations that can be fine-tuned effectively from minimal data. The model learns across many few-shot tasks during meta-training, optimizing for fast adaptation rather than performance on any single task. At deployment, a few gradient steps on new labeled examples produce a task-specific model. Prototypical networks learn to classify based on distance to class prototypes in embedding space, enabling classification from a handful of examples per class. These approaches remain underexplored in genomics but offer promise for the many settings where labels are genuinely scarce.\nZero-shot transfer makes predictions without any task-specific adaptation. The pretrained model produces outputs that can be interpreted as task predictions without further training. For protein variant effect prediction, ESM log-likelihood ratios score variants by how much they reduce the model’s probability of the observed sequence. Variants disrupting the model’s learned expectations for natural proteins are flagged as potentially deleterious. This approach proves competitive with supervised methods because evolutionary constraint (captured by pretraining on natural sequences) correlates with functional importance.\nZero-shot methods require strong alignment between pretraining objectives and target tasks. In genomics, most practical applications still require at least some labeled data; few-shot learning represents a more realistic minimal-data regime than true zero-shot transfer.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#when-transfer-fails",
    "href": "p2-ch09-transfer.html#when-transfer-fails",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.10 When Transfer Fails",
    "text": "9.10 When Transfer Fails\nTransfer learning can fail spectacularly, and understanding failure modes prevents wasted effort on adaptation strategies that cannot succeed. Negative transfer occurs when pretraining actively hurts performance, producing models worse than those trained from scratch on target data alone.\nCommon failure modes include misaligned pretraining objectives that emphasize features irrelevant or counterproductive for the target task. A model pretrained to predict masked nucleotides in coding sequence may learn features specific to protein-coding regions that mislead when applied to noncoding regulatory elements. Pretraining on data from one species may encode species-specific patterns that create false expectations when transferred to another organism.\nDiagnostic steps help identify whether transfer is helping or hurting. First, compare adapted model performance against a from-scratch baseline trained on the same target data. If the pretrained model does not outperform from-scratch training, transfer provides no benefit. Second, try simpler adaptation strategies before complex ones. If linear probing fails, full fine-tuning is unlikely to help unless the target dataset is large. Third, visualize embeddings from the pretrained model using dimensionality reduction. If target task examples are not well-separated in embedding space, the pretrained representations may not encode useful features for this task. Fourth, ablate pretraining entirely by comparing against randomly initialized models. This isolates whether pretrained weights provide value or whether architectural choices alone drive performance.\nWhen diagnostics reveal fundamental mismatches, several solutions may help. Task-specific pretraining on data more closely aligned with the target task can bridge the gap. Pretraining specifically on regulatory regions for regulatory prediction tasks, rather than genome-wide pretraining, may produce more suitable representations. Hybrid approaches combining pretrained and from-scratch components allow selective use of transfer where it helps. Trying different foundation models may reveal better alternatives. And accepting that transfer does not help, proceeding with from-scratch training, remains a valid option when pretrained representations genuinely misalign with task requirements.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#case-studies-in-transfer-learning",
    "href": "p2-ch09-transfer.html#case-studies-in-transfer-learning",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.11 Case Studies in Transfer Learning",
    "text": "9.11 Case Studies in Transfer Learning\nExamining concrete applications illustrates the principles developed throughout this chapter. Four case studies span different model architectures, adaptation strategies, and application domains.\nDNABERT applied to chromatin accessibility prediction demonstrates successful feature extraction. The model was pretrained using masked language modeling on k-mer tokenized human genomic sequence. For ATAC-seq peak classification, a linear probe on [CLS] token embeddings achieved competitive performance with CNNs trained from scratch while requiring 10 times less labeled data. This success reflects strong alignment between pretraining (learning local sequence patterns) and the target task (identifying accessibility signals depending on motif composition). The lightweight adaptation was appropriate given limited labeled ATAC-seq data.\nESM for variant effect prediction illustrates zero-shot and minimal-supervision transfer. ESM was pretrained on UniRef protein sequences using masked language modeling. For ClinVar pathogenicity classification, zero-shot scoring based on how variants reduce sequence likelihood proved competitive with supervised methods. Adding a linear probe on ESM embeddings improved performance further. Pretraining captures the target objective implicitly: evolutionary constraint, which masked language modeling learns to predict, correlates with functional importance.\nEnformer for cross-tissue gene expression shows benefits of full fine-tuning. Pretrained on thousands of chromatin and expression tracks, Enformer learned sequence-to-function mappings over long genomic contexts. Fine-tuning with tissue-specific prediction heads captured tissue-specific regulatory logic unavailable from frozen features, outperforming from-scratch models on individual tissues. The large scale of both pretraining and fine-tuning data justified computational expense.\nHyenaDNA for regulatory element classification leverages long-range context. Pretrained on human genomic sequence with contexts up to one million base pairs, HyenaDNA embeddings capture distal regulatory relationships. LoRA adapters enabled efficient fine-tuning for enhancer and promoter classification, with long-range context improving accuracy on elements depending on distal interactions. This case demonstrates architecture-specific pretraining benefits: the long context that distinguishes HyenaDNA enables transfer to tasks where long-range dependencies matter.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#validation-and-common-pitfalls",
    "href": "p2-ch09-transfer.html#validation-and-common-pitfalls",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.12 Validation and Common Pitfalls",
    "text": "9.12 Validation and Common Pitfalls\nProper validation distinguishes genuine transfer success from artifacts of evaluation design. Several failure modes pervade genomic model evaluation, many arising from subtle data leakage or inappropriate evaluation protocols.\nTest set overlap with pretraining data creates artificial inflation of transfer learning performance. Foundation models trained on massive corpora may inadvertently include sequences later used for evaluation. When evaluating models on “held-out” data that actually appeared during pretraining, the model has seen the answers. Verifying that test sequences were excluded from pretraining corpora requires careful provenance tracking.\nTemporal leakage uses future information unavailable at prediction time. Evaluating a variant pathogenicity model on variants annotated after the training data was collected creates an unrealistically favorable setting. Temporal splits, where the model sees only variants discovered before a cutoff and is evaluated on variants discovered afterward, provide more realistic assessment.\nInappropriate baselines inflate apparent transfer learning benefits. Comparing adapted foundation models against weak or poorly-tuned from-scratch baselines makes transfer look more valuable than it is. Strong baselines, including properly hyperparameter-tuned models trained on the same target data, provide fair comparisons.\nReporting confidence intervals from multiple training runs reveals performance variability. A single training run may produce misleadingly good or bad results. Testing on multiple datasets rather than a single benchmark reveals whether gains generalize. Failure case analysis, examining where and why the model errs, often reveals more about model behavior than aggregate metrics.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#emerging-directions",
    "href": "p2-ch09-transfer.html#emerging-directions",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.13 Emerging Directions",
    "text": "9.13 Emerging Directions\nTransfer learning continues evolving, with several directions particularly relevant to genomic applications.\nIn-context learning, where models make predictions by conditioning on examples provided in the input context, has emerged as a powerful capability in very large language models. Early evidence suggests that sufficiently large genomic models may exhibit similar behavior, performing tasks by observing a few examples without explicit fine-tuning. This could transform deployment: rather than training adapters or fine-tuning, users would simply provide examples of desired behavior.\nTest-time adaptation updates models during inference based on characteristics of test examples. Rather than freezing models after training, test-time adaptation allows limited parameter updates to match deployment conditions. This approach handles distribution shift without requiring labeled target data.\nFederated learning enables collaborative training across institutions without sharing raw data, addressing privacy constraints that limit data pooling in clinical genomics. Institutions train local models on private data and share only aggregated updates. This could enable foundation models trained on far more diverse data than any single institution can access.\nBetter theory for predicting when transfer will succeed based on measurable source and target properties would reduce trial-and-error. Currently, practitioners must empirically test whether transfer helps; theoretical guidance could focus effort on promising combinations.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p2-ch09-transfer.html#practical-guidelines",
    "href": "p2-ch09-transfer.html#practical-guidelines",
    "title": "9  Transfer Learning and Adaptation",
    "section": "9.14 Practical Guidelines",
    "text": "9.14 Practical Guidelines\nTransfer learning bridges pretrained models and specific applications, balancing adaptation flexibility against computational cost and overfitting risk. Several key principles emerge from this chapter.\nMatch adaptation strategy to available data. With fewer than 1,000 examples, feature extraction is safest. With 1,000 to 10,000 examples, PEFT methods balance expressiveness and regularization. With more than 10,000 examples, consider full fine-tuning but compare against simpler baselines.\nValidate that transfer helps. Compare adapted models against from-scratch baselines trained on the same target data. Without this comparison, the value of pretrained models cannot be established. Sometimes training from scratch performs equivalently or better.\nConsider domain shift. Models trained in one context may fail silently in another. Explicit domain adaptation and careful out-of-distribution evaluation help identify and mitigate these risks.\nStart simple and increase complexity as needed. Linear probes train quickly and often prove surprisingly effective. Invest in more complex adaptation only when simpler approaches demonstrably fail.\nUse probing to diagnose representations before committing to adaptation strategies. Understanding what features the pretrained model encodes guides choices about whether frozen features will suffice or whether more aggressive adaptation is needed.\nThese guidelines connect to later chapters examining specific foundation model families. DNA language models (Chapter 11), protein language models (Chapter 12), and regulatory models (Chapter 13) each present characteristic transfer learning patterns. Variant effect prediction (Chapter 14) synthesizes transfer across modalities. The evaluation principles in Chapter 21 and confounding considerations in Chapter 22 further inform validation of transfer learning claims. Throughout, the core message persists: transfer learning is powerful but not automatic. Understanding when and how to adapt pretrained models separates effective practitioners from those who treat foundation models as black boxes.",
    "crumbs": [
      "Part II: Core Principles",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning and Adaptation</span>"
    ]
  },
  {
    "objectID": "p3--architectures.html",
    "href": "p3--architectures.html",
    "title": "Part III: Deep Learning Architectures",
    "section": "",
    "text": "Warning\n\n\n\n“While protein language models like ESM preceded DNA transformers chronologically, we present these architectures following the central dogma to maintain conceptual coherence.”\n\n\nPart I established the data resources, statistical foundations, and pre-deep learning variant scoring methods that preceded the current era of genomic modeling. This part turns to the architectural innovations that transformed what is computationally possible. The chapters that follow trace an arc from early convolutional neural networks through transformer-based approaches to hybrid designs that combine the strengths of both paradigms.\nThe progression is not merely chronological. Each architecture embodies a different set of assumptions about biological sequence. Convolutional models assume that local motifs and their short-range combinations are the primary carriers of regulatory information. Transformer-based language models treat sequences as structured compositions of tokens whose meaning emerges from context, leveraging self-attention to capture dependencies across arbitrary distances. Hybrid architectures attempt to reconcile these perspectives, using convolutions to extract local features efficiently while deploying attention mechanisms to model long-range interactions that span tens or hundreds of kilobases.\nChapter 6  Convolutional Models for Genomic Sequence begins with the CNN-based models that first demonstrated deep learning could outperform handcrafted features for regulatory genomics. DeepSEA, ExPecto, and SpliceAI established the paradigm of training deep networks on functional genomics data to predict chromatin accessibility, transcription factor binding, gene expression, and splicing from sequence alone. These models remain widely used and provide the conceptual foundation for everything that follows.\nChapter 11  DNA Language Models then examines DNA language models that apply self-supervised pretraining strategies to genomic sequence. Models like DNABERT, Nucleotide Transformer, and HyenaDNA treat DNA as text to be understood through masked token prediction or autoregressive modeling, learning representations that transfer across diverse downstream tasks without task-specific architectures.\nChapter 15  RNA Models extends this treatment to RNA, covering models that predict secondary structure, capture splicing regulation beyond what CNN-based methods achieve, and represent the emerging frontier of RNA foundation models. RNA sits in the middle of the central dogma as both an information carrier and a structured biochemical object, requiring models that account for base pairing, epitranscriptomic modifications, and the relationship between sequence and structure.\nChapter 12  Protein Language Models examines protein language models, where the success of masked language modeling on natural language translated most directly to amino acid sequences. Models like ESM and ProtTrans learn rich representations of protein structure and function without explicit supervision. AlphaFold demonstrated that these representations could revolutionize structure prediction, while AlphaMissense applied them to proteome-wide variant pathogenicity scoring. These models established the foundation model paradigm that inspired subsequent work in DNA and RNA.\nFinally, Chapter 13  Long-Context Regulatory Models examines hybrid architectures like Enformer, Borzoi, and AlphaGenome that combine convolutional processing with transformer blocks to achieve context windows spanning hundreds of kilobases. These models enable direct prediction of gene expression and chromatin state from sequence, capturing enhancer-promoter interactions and long-range regulatory effects that shorter-context models cannot represent.\nBy the end of this part, readers will have a working understanding of the major architectural paradigms in genomic deep learning: what each assumes, what each can and cannot capture, and how these design choices translate to practical performance on regulatory prediction tasks.",
    "crumbs": [
      "Part III: Deep Learning Architectures"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html",
    "href": "p3-ch10-fm-principles.html",
    "title": "10  The Foundation Model Paradigm",
    "section": "",
    "text": "10.1 From Task-Specific Models to Foundation Models\nA curious asymmetry defines modern computational genomics. We can train models to predict chromatin accessibility with remarkable accuracy, yet these same models fail entirely when asked about splicing. We can predict splice site recognition with near-perfect precision, yet learn nothing transferable about transcription factor binding. Each model excels within its narrow domain while remaining blind to the broader genomic landscape. The result is an ecosystem of specialized tools, each requiring its own training data, computational infrastructure, and domain expertise, collectively covering only fragments of the questions we need to answer.\nFoundation models offer a different approach: train once on vast genomic data, then adapt to many downstream tasks. This paradigm, which transformed natural language processing and protein structure prediction, promises to unify fragmented genomic modeling efforts under shared representations. A single foundation model might provide embeddings useful for regulatory prediction, variant interpretation, and sequence design simultaneously. The efficiency gains would be substantial: rather than curating labeled datasets for each new question, researchers could fine-tune existing models on modest task-specific data.\nYet foundation models carry their own costs and limitations. Pretraining at scale requires computational resources beyond most academic budgets. Adaptation to specific tasks demands expertise in transfer learning techniques. Predictions from general-purpose models may lack the precision of specialized alternatives. The decision to use, adapt, or build foundation models involves trade-offs that depend on available resources, target applications, and acceptable uncertainty. This chapter provides the conceptual framework for navigating these decisions, establishing what foundation models are, how they scale, what emerges at scale, and when they represent the right tool for a given genomic question.\nThe history of computational genomics reveals a consistent pattern: models become more general while maintaining or improving task-specific performance. Hand-crafted scores such as CADD and SIFT established that integration of diverse genomic annotations could improve variant pathogenicity prediction (Rentzsch et al. 2019; Schubach et al. 2024). These approaches relied on expert feature engineering, combining conservation scores, functional annotations, and population frequency data through ensemble methods or logistic regression.\nTask-specific deep learning models demonstrated that neural networks could learn relevant features directly from sequence. DeepSEA predicted chromatin accessibility and transcription factor binding from 1 kb sequences using convolutional architectures (J. Zhou and Troyanskaya 2015). ExPecto extended this approach to gene expression prediction by modeling regulatory elements across multiple cell types (J. Zhou et al. 2018). Sei organized regulatory predictions into interpretable sequence classes through unsupervised clustering (Chen et al. 2022). SpliceAI achieved near-perfect splice site prediction through dilated convolutions over 10 kb contexts (Chapter 6). Enformer scaled sequence-to-function modeling to 200 kb windows and thousands of chromatin tracks through transformer architectures (Avsec et al. 2021).\nThese models succeeded within their specific domains but remained difficult to repurpose. Training a DeepSEA model required chromatin accessibility data. Using SpliceAI for regulatory prediction would require complete retraining on different labels. Each application domain needed its own model, trained from scratch on task-specific data. The fundamental limitation was not model capacity but training paradigm: supervised learning on narrow tasks produced narrow capabilities.\nSequence language models introduced self-supervised learning to genomics. DNABERT applied masked language modeling to DNA sequences, demonstrating that general representations could be learned without task-specific labels (Ji et al. 2021). ESM and ESM-2 showed that protein language models pretrained on sequence alone could transfer effectively to structure prediction, variant effect prediction, and protein design (Rives et al. 2021; Lin et al. 2022). The Nucleotide Transformer family scaled DNA language modeling to cross-species training corpora (Dalla-Torre et al. 2023). HyenaDNA used implicit convolutions to reach million-token contexts at single-nucleotide resolution (Nguyen et al. 2023).\nThe transition from task-specific to foundation models changes the relationship between model developers and users. Task-specific models deliver predictions as their primary product. Foundation models deliver representations that users adapt to their own tasks. This distinction affects everything from model architecture design to evaluation strategies to deployment infrastructure.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html#defining-genomic-foundation-models",
    "href": "p3-ch10-fm-principles.html#defining-genomic-foundation-models",
    "title": "10  The Foundation Model Paradigm",
    "section": "10.2 Defining Genomic Foundation Models",
    "text": "10.2 Defining Genomic Foundation Models\nThe term “foundation model” appears frequently in genomics literature, sometimes applied to any large neural network trained on biological sequences. Establishing clear criteria helps separate true genomic foundation models from ordinary deep learning approaches that happen to operate on DNA or protein sequences.\n\n10.2.1 Essential Properties\nA genomic foundation model satisfies several key properties that distinguish it from task-specific architectures.\nLarge-scale pretraining with minimal supervision. Foundation models train on entire genomes, pan-genomic sequence collections, or large assay compendia. The pretraining objectives include masked language modeling, next-token prediction, denoising, or multi-task sequence-to-function prediction. Critically, these objectives do not require dense task-specific labels for every training example. A model that requires annotated enhancers or curated pathogenic variants for every training instance does not qualify as a foundation model under this criterion.\nGeneral-purpose representations. Foundation models produce embeddings that prove useful across many downstream tasks. These representations can be extracted through forward passes and reused with simple linear probes or lightweight adapter modules rather than requiring full model retraining. The representations should encode biological information at multiple scales, from local sequence motifs to long-range regulatory grammar.\nBroad transfer capability. Foundation models support diverse downstream applications without architectural modifications or complete retraining. Transfer occurs across multiple dimensions: different assays (from chromatin accessibility to gene expression), different tissues and cell types, different species, and different variant types (from SNVs to structural variants). Evidence of broad transfer requires evaluation across multiple benchmarks rather than demonstration of performance on a single task.\nScale along at least one dimension. Foundation models operate at a scale that would be impractical for task-specific training. Some scale context length, as HyenaDNA scales to million-token windows at single-nucleotide resolution. Others scale parameter count, as the ESM and Nucleotide Transformer families reach billions of parameters. Still others scale data diversity through pan-genomic pretraining across hundreds of species or integration of many assays and cell types. The scaling dimension chosen reflects the model’s intended applications and architectural constraints.\nStandardized interfaces. Foundation models typically expose consistent APIs for common operations. These include embedding extraction for sequences or variants, sequence probability scoring, and mask-based in-silico mutagenesis for variant effect prediction. Models distributed through repositories such as Hugging Face often include documented recipes for downstream fine-tuning and example notebooks demonstrating common use cases.\n\n\n10.2.2 What Doesn’t Count\nMany excellent genomic models fail one or more of these criteria and should not be classified as foundation models. Early versions of DeepSEA trained specifically on chromatin accessibility data from a limited set of cell types lack the generality and standardized interface of foundation models, though later iterations that integrate many assays begin to approach foundation model territory (J. Zhou and Troyanskaya 2015). SpliceAI predicts splicing outcomes exceptionally well but was designed for that specific task and provides neither general-purpose embeddings nor easy transfer to other genomic prediction problems (Jaganathan et al. 2019). Even a very large Enformer-like model trained solely on human chromatin tracks remains bound to its specific prediction interface despite its scale and sophistication (Avsec et al. 2021).\nThe distinction matters for several reasons. It affects evaluation strategy, since foundation models must be assessed across families of tasks rather than single benchmarks. It affects integration into existing pipelines, since foundation models serve as feature extractors while task-specific models typically provide end-to-end predictions. It affects how we think about model development, since foundation model training requires different infrastructure and data curation than task-specific supervised learning.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html#scaling-laws-and-compute-optimal-training",
    "href": "p3-ch10-fm-principles.html#scaling-laws-and-compute-optimal-training",
    "title": "10  The Foundation Model Paradigm",
    "section": "10.3 Scaling Laws and Compute-Optimal Training",
    "text": "10.3 Scaling Laws and Compute-Optimal Training\nThe success of foundation models in natural language processing rests partly on empirical scaling laws: predictable relationships between model size, training data, computational budget, and performance. Understanding these relationships guides resource allocation and model development decisions.\n\n10.3.1 The Scaling Law Framework\nScaling laws describe how model performance (typically measured as loss on held-out data) varies with three key quantities: the number of model parameters (N), the amount of training data (D), and the total compute budget (C). The Chinchilla scaling analysis demonstrated that for a fixed compute budget, there exists an optimal balance between model size and training data (hoffmann_chinchilla_2022?). Training a model that is too large on too little data, or too small on too much data, yields worse performance than compute-optimal allocation.\nThe practical implications are significant. Many early large language models were undertrained relative to their parameter count. GPT-3’s 175 billion parameters were trained on roughly 300 billion tokens, while Chinchilla-optimal training would suggest either fewer parameters or more data. The Chinchilla model itself matched GPT-3 performance with only 70 billion parameters trained on 1.4 trillion tokens.\nFor genomic foundation models, scaling relationships are less well characterized but increasingly important. Several key questions arise: Do genomic sequences exhibit the same scaling behavior as natural language? How does the finite size of reference genomes constrain data scaling? What role does sequence diversity (cross-species, population variation) play in data scaling? Can synthetic data augmentation extend effective training data beyond natural sequences?\n\n\n10.3.2 Empirical Scaling in Genomic Models\nSeveral genomic foundation model families have reported scaling experiments, though systematic scaling laws comparable to NLP remain elusive.\nThe Nucleotide Transformer family provides perhaps the clearest genomic scaling data (Dalla-Torre et al. 2023). Performance on downstream benchmarks improves consistently with parameter count across models from 50 million to 2.5 billion parameters. The largest models (trained on multi-species data) outperform smaller models trained on human sequences alone, suggesting that cross-species data provides effective scaling even when human-specific performance is the target. Training compute scaled from approximately 10^19 to 10^21 FLOPs across the model family.\nESM-2 demonstrated similar scaling for protein language models, with performance on structure prediction and variant effect tasks improving smoothly from 8 million to 15 billion parameters (Lin et al. 2022). The largest ESM-2 models approach the structure prediction accuracy of AlphaFold2 using only single-sequence input, a capability entirely absent in smaller models. This represents a qualitative capability threshold crossed through scale.\nHyenaDNA focused on context length scaling rather than parameter scaling, demonstrating that million-token contexts at single-nucleotide resolution could be achieved through sub-quadratic architectures (Nguyen et al. 2023). The relationship between context length and downstream performance is task-dependent: some tasks (local motif recognition) show saturation at kilobase scales, while others (long-range regulatory prediction) continue improving with longer contexts.\n\n\n10.3.3 Compute-Optimal Decisions for Genomics\nThe scaling law framework has direct implications for model development decisions in genomics.\nData-constrained regimes. Unlike natural language, where text data is effectively unlimited, genomic sequence data faces hard constraints. Reference genomes for well-studied species total perhaps 10^11 to 10^12 nucleotides. Population-level variant data can expand this somewhat, but the effective diversity may be lower than raw counts suggest. In data-constrained regimes, smaller models trained to convergence may outperform larger models that are undertrained.\nCompute-constrained regimes. Academic groups typically face stricter compute constraints than industry labs. Given fixed compute budgets, the Chinchilla framework suggests allocating resources toward longer training of smaller models rather than abbreviated training of larger models. A 500 million parameter model trained for 10 epochs on diverse genomic data may outperform a 5 billion parameter model trained for 1 epoch on the same data.\nTask-specific considerations. Not all downstream tasks benefit equally from foundation model scale. Tasks that depend primarily on local sequence patterns (transcription factor motif recognition, splice site identification) may show diminishing returns beyond modest model sizes. Tasks requiring integration of long-range context or rare variant interpretation may continue benefiting from larger models and longer contexts.\nData curation versus model scaling. For many practical applications, investment in data curation and quality may yield better returns than model scaling. A foundation model trained on carefully curated, diverse, high-quality sequences may outperform a larger model trained on noisier data. The relative value of these investments is task-dependent and not well characterized by current scaling law frameworks.\n\n\n10.3.4 Emergent Capabilities\nPerhaps the most intriguing aspect of foundation model scaling is the emergence of qualitatively new capabilities at sufficient scale. Emergence refers to abilities that are absent or negligible in smaller models but appear discontinuously as models grow.\nIn large language models, emergent capabilities include multi-step reasoning, code generation, and in-context learning. These capabilities appear at model scales of roughly 10^10 parameters and above, with no clear precursor in smaller models.\nGenomic foundation models exhibit analogous emergence, though the capability thresholds are less well characterized.\nStructural understanding from sequence. ESM-2 at sufficient scale produces contact maps and secondary structure predictions from single sequences with accuracy approaching multiple sequence alignment methods (Lin et al. 2022). Smaller ESM models show no meaningful structural understanding. This capability emerges at approximately 650 million parameters and continues improving with scale.\nCross-species transfer. Larger Nucleotide Transformer models transfer more effectively to novel species not seen during training (Dalla-Torre et al. 2023). The ability to generalize beyond training species appears to require sufficient model capacity to learn abstract regulatory principles rather than memorizing species-specific patterns.\nZero-shot variant effect prediction. Foundation models at sufficient scale can predict variant effects without task-specific fine-tuning, using only the difference in likelihood between reference and alternative sequences. This zero-shot capability requires models large enough to capture subtle sequence dependencies.\nIn-context learning. The most recent genomic foundation models show preliminary evidence of in-context learning: the ability to adapt to new tasks based on examples provided in the input context without parameter updates. This capability, central to the utility of large language models, remains nascent in genomic models but may emerge with further scaling.\nThe practical implication is that capability thresholds exist: models below certain scales may be fundamentally incapable of certain tasks regardless of fine-tuning. Identifying these thresholds helps guide model selection and prevents wasted effort fine-tuning models that lack necessary capacity.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html#a-taxonomy-of-genomic-foundation-models",
    "href": "p3-ch10-fm-principles.html#a-taxonomy-of-genomic-foundation-models",
    "title": "10  The Foundation Model Paradigm",
    "section": "10.4 A Taxonomy of Genomic Foundation Models",
    "text": "10.4 A Taxonomy of Genomic Foundation Models\nThe landscape of genomic foundation models can be organized into four broad families. Each family exhibits distinct characteristics, strengths, limitations, and typical application domains.\n\n10.4.1 DNA Language Models\nDNA language models learn sequence representations from raw nucleotide strings through self-supervised objectives. These models treat DNA as a language to be modeled without explicit functional labels, discovering patterns through statistical regularities in genomic sequence.\nCore characteristics. DNA language models typically use masked language modeling or autoregressive next-token prediction as their pretraining objective. They train on reference genomes or pan-genomic sequence collections spanning multiple species. The resulting models produce per-position or pooled sequence embeddings that can be extracted and used for downstream tasks. Critically, these embeddings are not tied to specific assays or cell types, making them applicable to any task that benefits from general sequence context.\nRepresentative models. DNABERT and DNABERT-2 apply BERT-style masked language modeling to DNA sequences, using overlapping k-mers as tokens (Ji et al. 2021; Z. Zhou et al. 2024). The Nucleotide Transformer family scales this approach to larger parameter counts and cross-species training (Dalla-Torre et al. 2023). HyenaDNA achieves subquadratic complexity through implicit convolutions, enabling context lengths up to one million nucleotides (Nguyen et al. 2023). Caduceus incorporates bidirectional processing and reverse-complement equivariance as architectural inductive biases. Evo-2 combines long-range attention with biological tokenization strategies. GROVER integrates learned BPE-style tokenization with training on regulatory tracks in addition to raw sequence (Sanabria et al. 2024).\nStrengths and limitations. DNA language models provide truly general representations not bound to specific assays, cell types, or experimental conditions. They can process novel sequences not present in reference genomes. Their self-supervised training requires only genome sequences, making them scalable. Without explicit functional grounding, however, they may not capture subtle regulatory patterns that manifest only under specific cellular conditions. Performance on tasks requiring fine-grained functional discrimination may lag models trained with functional supervision.\nTypical applications. Sequence classification (promoters, enhancers, transposons), motif discovery, variant effect prediction through embedding perturbation, sequence generation for synthetic biology, and transfer learning to new species with limited labeled data.\n\n\n10.4.2 Sequence-to-Function Foundation Models\nSequence-to-function models predict molecular readouts directly from sequence through supervised or semi-supervised training on assay compendia. These models blur into foundation model territory when their output space is sufficiently broad and their internal representations prove useful for tasks beyond the original assay set.\nCore characteristics. These models map DNA sequences to high-dimensional vectors of molecular measurements, including chromatin accessibility, histone modifications, transcription factor binding, and gene expression levels. Training uses large collections of functional genomics assays spanning many cell types. The models learn regulatory grammar through supervised prediction of molecular phenotypes.\nRepresentative models. Enformer predicts thousands of chromatin and expression tracks from 200 kb sequence windows through transformer attention (Avsec et al. 2021). Borzoi extends this with refined architectures and expanded coverage. Sei organizes predictions into interpretable sequence classes through unsupervised clustering (Chen et al. 2022). Earlier models including DeepSEA and Basset established the paradigm at smaller scales.\nStrengths and limitations. Explicit functional supervision provides mechanistic grounding. Predictions can be interpreted through comparison to experiments. The models naturally support variant effect prediction by computing reference-alternative differences. Models remain tied to training assays and cell types. Extension to new contexts typically requires retraining or new data collection.\nTypical applications. Regulatory variant interpretation in well-studied cell types, eQTL fine-mapping, enhancer identification, transcription factor binding prediction, and regulatory mechanism discovery.\n\n\n10.4.3 Variant Effect Prediction Models\nModels optimized specifically for predicting functional or clinical consequences of genetic variants. These take a variant and predict its effect on molecular phenotypes, organismal fitness, or disease risk.\nCore characteristics. Variant effect prediction models integrate sequence context with evolutionary information, population genetics signals, and sometimes structural or functional annotations. They output pathogenicity scores, effect size estimates, or functional consequence predictions. Training combines multiple data sources: clinical labels from ClinVar, population frequency from gnomAD, functional assays such as deep mutational scanning, and evolutionary constraint metrics.\nRepresentative examples. AlphaMissense applies protein language models to predict pathogenicity of missense variants (Cheng et al. 2023). ESM-1v uses evolutionary context for protein variant effect prediction. EVE combines evolutionary and structural information. Genomic foundation models like DNABERT and Enformer provide variant effect predictions through in silico mutagenesis. The architecture, training, evaluation, and clinical deployment of variant effect predictors are covered comprehensively in Chapter 14.\n\n\n10.4.4 Multi-Omic Foundation Models\nModels that natively integrate multiple molecular modalities, jointly processing DNA sequence, chromatin state, gene expression, protein abundance, 3D genome structure, or phenotypic descriptions.\nCore characteristics. Multi-omic models employ architectures handling heterogeneous input types: transformer variants with cross-attention, graph neural networks, or modality-specific encoders with fusion layers. Training objectives encourage cross-modal alignment through contrastive learning, joint prediction, or generative modeling of multiple data types.\nRepresentative models. Omni-DNA uses transformer-based autoregressive models with vocabulary expansion and multi-task finetuning, unifying diverse genomic tasks under an instruction-response paradigm (Li et al. 2025). Models integrating Hi-C data capture 3D genome organization. Cross-modal architectures align DNA embeddings with chromatin or expression predictions.\nStrengths and limitations. Unified representations enable cross-modal queries. Joint training can improve performance through multi-task effects. Data engineering becomes substantially more complex, with different modalities requiring different measurement technologies and quality control. The field is early, with few models reaching production maturity.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html#design-dimensions",
    "href": "p3-ch10-fm-principles.html#design-dimensions",
    "title": "10  The Foundation Model Paradigm",
    "section": "10.5 Design Dimensions",
    "text": "10.5 Design Dimensions\nWithin and across families, individual models differ along orthogonal design dimensions that affect suitability for specific tasks.\n\n10.5.1 Data Composition\nSpecies coverage. Human-only training focuses on clinically relevant patterns. Cross-species training encourages learning of conserved elements and evolutionary constraints, potentially improving generalization but risking dilution of human-specific signals.\nSequence diversity. Training on reference genomes alone provides clean sequences but limited exposure to population variation. Incorporating variant data improves robustness but requires careful design to avoid learning spurious associations.\nAnnotation integration. Models may train on raw sequence alone or incorporate functional annotations. The degree of integration trades generality against functional grounding.\n\n\n10.5.2 Architecture Choices\nTransformer variants. Encoder-only models (DNABERT, Nucleotide Transformer) excel at classification and embedding tasks. Decoder-only models (GROVER) support generative applications. Full and sparse attention patterns, linear approximations, and Flash attention implementations affect computational efficiency.\nSub-quadratic architectures. Hyena-based models and state space models achieve subquadratic scaling, enabling longer contexts than standard transformers with comparable parameters.\nHybrid architectures. CNN-transformer combinations use local convolutions followed by global attention, as in Enformer. Multi-scale approaches process sequences at multiple resolutions.\n\n\n10.5.3 Context Length\nShort context (under 1 kb) captures local patterns: motifs, splice sites, promoter elements. Medium context (1-10 kb) spans complete genes with proximal regulatory regions. Long context (10-200 kb) represents enhancer-promoter interactions and TAD-scale organization. Ultra-long context (over 200 kb) enables chromosomal domain modeling and complex structural variant interpretation. The effective use of long context requires appropriate tokenization and positional encoding.\n\n\n10.5.4 Tokenization\nCharacter-level maintains single-base resolution but imposes longest sequence lengths. K-mer tokenization reduces length by a factor approaching k, with vocabulary reaching 4,096 for 6-mers. Learned tokenization (BPE-style) discovers schemes from data, potentially allocating vocabulary more efficiently (Medvedev et al. 2025). The choice should align with both computational constraints and biological resolution requirements.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html#build-versus-use-decisions",
    "href": "p3-ch10-fm-principles.html#build-versus-use-decisions",
    "title": "10  The Foundation Model Paradigm",
    "section": "10.6 Build Versus Use Decisions",
    "text": "10.6 Build Versus Use Decisions\nThe availability of pretrained foundation models creates strategic choices about when to use existing models, when to adapt them, and when to train from scratch.\n\n10.6.1 When to Use Existing Models\nExisting foundation models provide immediate utility when the target application aligns with model capabilities, labeled data is limited, and computational resources are constrained.\nEmbedding extraction. For tasks where general sequence representations suffice, frozen foundation model embeddings with simple downstream classifiers often perform competitively with fine-tuned alternatives. This approach requires minimal compute (single forward passes), no gradient computation through large models, and modest labeled data (hundreds to thousands of examples). Applications include sequence classification, clustering, and similarity search.\nZero-shot inference. Some foundation models support zero-shot variant effect prediction through likelihood ratio scoring. This requires no task-specific training and produces calibrated scores for novel variants immediately. Zero-shot approaches work well when the pretraining objective aligns with the target task and when fine-tuning data is unavailable or unreliable.\nRapid prototyping. Foundation model APIs enable quick assessment of whether a modeling approach is viable before committing resources to custom development. Testing variant effect prediction with ESM-1v takes hours rather than the weeks required to train a custom model.\n\n\n10.6.2 When to Adapt Existing Models\nAdaptation through fine-tuning or lightweight methods (LoRA, adapters, prefix tuning) makes sense when downstream tasks require specialized behavior beyond what frozen embeddings provide, sufficient labeled data exists (typically thousands to tens of thousands of examples), and the target domain falls within the pretraining distribution.\nParameter-efficient fine-tuning. Methods like LoRA update a small fraction of model parameters (often under 1%) while keeping the foundation model frozen (hu_lora_2021?). This preserves general knowledge while allowing task-specific adaptation. Compute requirements are modest: a few GPU-hours for most genomic tasks. The approach works well when the foundation model’s representations are largely appropriate but need refinement for specific applications.\nFull fine-tuning. Updating all parameters typically achieves the best single-task performance but requires more data (tens of thousands of examples), more compute (GPU-days to weeks), and careful regularization to prevent overfitting. Full fine-tuning makes sense for high-stakes applications where maximum accuracy justifies the investment.\n\n\n10.6.3 When to Train from Scratch\nBuilding custom foundation models requires substantial justification given the resources involved.\nNovel domains. When target sequences differ fundamentally from existing model pretraining data (novel species, synthetic sequences, non-standard nucleotides), existing models may provide poor transfer. Custom pretraining on domain-specific data may be necessary.\nSpecialized architectures. If the application requires architectural features absent from existing models (specific attention patterns, custom tokenization, multi-modal inputs), building from scratch may be unavoidable.\nScale requirements. For applications requiring larger models or longer contexts than available options, custom training is necessary. This applies primarily to well-resourced groups with specific performance requirements.\nProprietary data advantages. Organizations with unique large-scale datasets (clinical biobanks, pharmaceutical screening data) may achieve better performance through custom pretraining than public models allow. The data advantage must be substantial to justify training costs.\n\n\n10.6.4 Cost-Benefit Analysis\nThe decision framework involves comparing expected performance against resource requirements.\nCompute costs. Training a foundation model from scratch requires 10^20 to 10^22 FLOPs, translating to thousands of GPU-hours and tens of thousands of dollars at current cloud prices. Fine-tuning requires 10^16 to 10^18 FLOPs, often achievable in hours on single GPUs. Inference with frozen embeddings requires only forward passes.\nData costs. Foundation model pretraining requires billions of tokens. Fine-tuning requires thousands to tens of thousands of labeled examples. Zero-shot and embedding approaches require only evaluation data.\nPerformance expectations. For well-studied tasks with abundant labeled data, fine-tuned models typically outperform frozen embeddings by 5-15% on standard metrics. Zero-shot approaches often achieve 70-90% of fine-tuned performance. Custom foundation models rarely outperform existing options by large margins unless the application involves genuinely novel domains.\nTime costs. Using existing models takes hours to days. Fine-tuning takes days to weeks. Training from scratch takes weeks to months. For time-sensitive applications, using existing models often dominates even if custom training would eventually yield better results.\nThe practical recommendation for most applications: start with frozen embeddings from the most appropriate existing foundation model. If performance is insufficient, try parameter-efficient fine-tuning. Train from scratch only if adaptation fails and the application justifies the investment.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html#evaluation-principles",
    "href": "p3-ch10-fm-principles.html#evaluation-principles",
    "title": "10  The Foundation Model Paradigm",
    "section": "10.7 Evaluation Principles",
    "text": "10.7 Evaluation Principles\nFoundation models resist evaluation on single tasks. Their value lies in transfer across many applications, making comprehensive evaluation substantially more complex than benchmarking task-specific models.\n\n10.7.1 Multi-Task Assessment\nA genomic foundation model should be evaluated across families of related tasks rather than isolated benchmarks. For DNA language models, this includes sequence classification tasks, variant effect prediction across multiple variant types, motif discovery, and cross-species transfer. For sequence-to-function models, evaluation should span prediction of held-out assays, transfer to novel cell types, and consistency with experimental measurements.\nThe diversity of evaluation tasks complicates comparison across models. A model excelling at promoter classification may underperform on eQTL fine-mapping. Direct comparisons require controlling for differences in training data, model scale, and evaluation protocols.\n\n\n10.7.2 Transfer Versus Pretraining Performance\nFoundation models are intended for transfer, making pretraining loss only moderately predictive of downstream utility. A model with slightly worse masked language modeling loss may produce better embeddings if its training objective better aligns with useful representations. Evaluation should explicitly test transfer through zero-shot performance, few-shot learning, cross-domain transfer, and robustness to distribution shift.\nDetailed discussion of benchmark suites, evaluation protocols, and methodological best practices appears in Chapter 20 and Chapter 21.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html#the-foundation-model-ecosystem",
    "href": "p3-ch10-fm-principles.html#the-foundation-model-ecosystem",
    "title": "10  The Foundation Model Paradigm",
    "section": "10.8 The Foundation Model Ecosystem",
    "text": "10.8 The Foundation Model Ecosystem\nGenomic foundation models exist within a broader ecosystem of infrastructure, community resources, and shared practices.\n\n10.8.1 Model Distribution\nMost models are distributed through centralized repositories. Hugging Face hosts many DNA and protein language models with documented APIs. GitHub repositories accompany publications with weights, code, and examples. Standardized formats reduce friction in adoption, enabling rapid benchmarking and experimentation.\n\n\n10.8.2 Documentation Requirements\nResponsible distribution requires comprehensive documentation: training data provenance, preprocessing procedures, architecture details, hyperparameters, evaluation protocols, and known limitations. Data provenance is particularly important given population-specific biases and use restrictions in genomic datasets.\n\n\n10.8.3 Industry and Academic Contributions\nBoth academic and industry groups develop genomic foundation models. Academic models emphasize reproducibility and open access. Industry models may offer superior performance through proprietary data or compute but with limited transparency. Notable industry contributions include NVIDIA’s BioNeMo platform and Microsoft’s Azure genomics integration. Users should review license terms before clinical or commercial deployment.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html#open-questions",
    "href": "p3-ch10-fm-principles.html#open-questions",
    "title": "10  The Foundation Model Paradigm",
    "section": "10.9 Open Questions",
    "text": "10.9 Open Questions\nDespite rapid progress, fundamental challenges remain.\nConvergence or divergence. Whether the field converges toward unified architectures or maintains specialized families remains unclear. The diversity of genomic scales, resolution requirements, and functional contexts may preclude the convergence seen in NLP.\nCausal reasoning. Existing models learn correlations without distinguishing causal from spurious relationships. Integrating causal structure could improve robustness and enable counterfactual reasoning.\nRare variant interpretation. Models trained on reference genomes and common variants may not calibrate well for ultra-rare or de novo variants. Improved integration of structural and evolutionary constraints could strengthen rare variant interpretation.\nClinical deployment. Translation to clinical use requires robust cross-population performance, calibrated uncertainty, interpretability for clinicians, prospective validation, and regulatory approval. These requirements extend well beyond benchmark performance.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch10-fm-principles.html#summary",
    "href": "p3-ch10-fm-principles.html#summary",
    "title": "10  The Foundation Model Paradigm",
    "section": "10.10 Summary",
    "text": "10.10 Summary\nThis chapter established a conceptual framework for genomic foundation models. We defined foundation models through five essential properties: large-scale pretraining with minimal supervision, general-purpose representations, broad transfer capability, scale along at least one dimension, and standardized interfaces.\nScaling laws describe predictable relationships between parameters, data, compute, and performance. Compute-optimal training allocates resources based on these relationships, while emergent capabilities represent qualitative thresholds that appear only at sufficient scale. The practical implication: some capabilities require minimum model scales regardless of fine-tuning effort.\nFour model families serve different needs: DNA language models for general sequence representations, sequence-to-function models for regulatory prediction with mechanistic grounding, variant effect models for clinical interpretation, and multi-omic models for systems-level integration. The build-versus-use framework guides resource allocation: use existing models when possible, adapt when needed, train from scratch only when justified by unique requirements.\nSubsequent chapters apply this framework to specific domains. Chapter 11 and Chapter 12 examine DNA and protein language models in detail. Chapter 13 covers long-context regulatory models. Chapter 14 addresses variant effect prediction as a capstone application integrating multiple model families. Throughout, the principles established here guide model selection, evaluation, and deployment decisions.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLi, Zehui, Vallijah Subasri, Yifei Shen, Dongsheng Li, Yiren Zhao, Guy-Bart Stan, and Caihua Shan. 2025. “Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning.” arXiv. https://doi.org/10.48550/arXiv.2502.03499.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html",
    "href": "p3-ch11-dna-lm.html",
    "title": "11  DNA Language Models",
    "section": "",
    "text": "11.1 From Task-Specific CNNs to General-Purpose Language Models\nThe human genome contains roughly three billion base pairs of DNA, yet we lack the ability to read most of it. Protein-coding sequences constitute less than two percent of the total, and decades of research have characterized only a fraction of the regulatory elements that orchestrate when and where genes are expressed. The remaining sequence was once dismissed as “junk,” but we now recognize it contains a sophisticated regulatory grammar whose rules we can barely articulate. Every genome-wide association study identifies variants in noncoding regions whose mechanisms remain opaque; every clinical exome returns dozens of variants of uncertain significance that resist interpretation. The fundamental challenge is not sequencing the genome but understanding the language in which it is written.\nThe success of language models in natural language processing suggested a potential path forward. BERT, GPT, and their successors demonstrated that statistical patterns in unlabeled text could yield representations encoding grammar, semantics, and even world knowledge. Proteins, too, proved amenable to this approach: models trained to predict masked amino acids learned representations that captured evolutionary constraints, structural properties, and functional relationships without explicit supervision (Chapter 12). DNA presents an analogous opportunity. If genomes encode a regulatory language, perhaps self-supervised learning on raw sequence could discover its grammar.\nDNA language models import the self-supervised paradigm to nucleotide sequences. Rather than training separate models for each genomic prediction task (as the CNN era required; see Chapter 6), these approaches learn general-purpose representations from unlabeled genomes that transfer across applications. A single pretrained backbone can support regulatory element classification, variant effect prediction, cross-species analysis, and sequence generation through different downstream heads or scoring strategies. The transition mirrors the broader shift in machine learning from task-specific architectures to foundation models (Chapter 10), with implications for how genomic AI systems are developed and deployed.\nThis chapter traces the development of DNA language models from early proof-of-concept systems through current state-of-the-art architectures. We examine the design choices that distinguish different approaches (tokenization strategies, context lengths, training objectives, and architectural innovations), analyze what these models actually learn through probing studies, and assess their performance across standardized benchmarks. The goal is to equip readers with the conceptual framework and practical knowledge needed to apply these models effectively while understanding their limitations.\nThe convolutional neural networks examined in Chapter 6 achieved remarkable performance on specific genomic prediction tasks. DeepSEA predicted chromatin marks from sequence; SpliceAI identified splice junctions with clinical utility; ExPecto estimated expression effects of variants. Each model was engineered for its particular application, with architectural choices (filter sizes, dilation patterns, pooling strategies) optimized for the task at hand.\nThis paradigm succeeded but imposed three constraints that limited scalability. Label dependence meant that every new assay, cell type, or phenotype required fresh labeled data. A model trained on ENCODE chromatin data could not predict histone modifications in a new cell type without additional labeled examples. Task coupling bound model architecture to specific prediction problems. SpliceAI’s dilated convolutions were tailored for splice junction detection; ExPecto’s spatial transformation was designed for the distance-dependent relationship between regulatory elements and transcription start sites. These choices, while effective, did not transfer naturally to other problems. Limited reuse meant features learned for one task could not easily support others. A model that learned to recognize transcription factor binding sites during chromatin accessibility training could not directly apply those representations to variant effect prediction without substantial re-engineering.\nProtein language models demonstrated an alternative. ESM and related models trained on massive corpora of protein sequences using masked language modeling (predicting held-out amino acids from context) or autoregressive objectives (predicting the next amino acid). The resulting representations transferred to structure prediction, function annotation, and variant effect scoring without architecture changes. DNA language models import this recipe: pretrain on large collections of genomic sequences using self-supervised objectives, then adapt the learned representations to downstream tasks through probing, fine-tuning, or zero-shot scoring.\nThe practical workflow involves several steps. First, train a language model on unlabeled genomic sequences, where the model learns to predict masked or subsequent nucleotides from context. Second, extract embeddings from the trained model for sequences of interest (windows around variants, regulatory elements, or entire genes). Third, apply these embeddings to downstream tasks through one of several strategies: train lightweight classifiers on frozen embeddings (probing), update model parameters for specific applications (fine-tuning), or score sequence variants by comparing model probabilities (zero-shot evaluation). The promise is that once a sufficiently powerful backbone exists, it becomes the default starting point for nearly any DNA-level prediction problem.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#dnabert-the-first-dna-language-model",
    "href": "p3-ch11-dna-lm.html#dnabert-the-first-dna-language-model",
    "title": "11  DNA Language Models",
    "section": "11.2 DNABERT: The First DNA Language Model",
    "text": "11.2 DNABERT: The First DNA Language Model\nDNABERT applied the BERT masked language modeling framework to genomic sequences, establishing proof of concept for DNA self-supervision (Ji et al. 2021). The model used overlapping k-mers (typically 6-mers) as tokens, creating a vocabulary of 4,096 tokens from the 4^6 possible hexamers. Training on the human reference genome, DNABERT learned to predict masked tokens from surrounding context using the standard BERT architecture.\nThe design choices reflected computational constraints of the time. The k-mer tokenization provided some sequence compression compared to single-nucleotide representations, but the overlapping nature (each nucleotide participates in multiple adjacent k-mers) meant the compression was modest and created ambiguity about precise variant positions. Context windows were limited to 512 tokens, corresponding to a few hundred base pairs of genomic sequence. The standard transformer architecture with quadratic attention complexity made longer contexts computationally prohibitive.\nDespite these limitations, DNABERT demonstrated several important principles. Fine-tuning on downstream tasks (promoter classification, splice site prediction, transcription factor binding site identification) achieved competitive performance with task-specific models trained from scratch. Learned embeddings captured biologically meaningful patterns, with similar sequences clustering together in embedding space even when trained only on the reference genome. The BERT-style architecture could be reused across multiple tasks with modest adaptation.\nDNABERT-2 addressed the tokenization limitations through improved approaches including BPE-style token merging that better compressed repetitive sequences (Zhou et al. 2024). The resulting model could represent longer genomic contexts within the same number of tokens, improving computational efficiency. On standardized benchmarks spanning sequence classification, regulatory element prediction, and variant effect scoring, DNABERT-2 achieved consistent gains over both the original DNABERT and non-pretrained baselines. These improvements validated the importance of thoughtful tokenization design for genomic applications (see Chapter 5 for detailed discussion of tokenization strategies).\nThe DNABERT family collectively established that self-supervision on DNA works, that tokenization choices substantially affect performance, and that masked language model training produces reusable representations for diverse sequence tasks. The foundation model paradigm transfers effectively from natural language to genomic sequence.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#nucleotide-transformer-scaling-data-and-model-diversity",
    "href": "p3-ch11-dna-lm.html#nucleotide-transformer-scaling-data-and-model-diversity",
    "title": "11  DNA Language Models",
    "section": "11.3 Nucleotide Transformer: Scaling Data and Model Diversity",
    "text": "11.3 Nucleotide Transformer: Scaling Data and Model Diversity\nDNABERT demonstrated feasibility but operated at modest scale relative to the size of genomes. The Nucleotide Transformer family pushed substantially further, emphasizing diversity in both training data and model architecture (Dalla-Torre et al. 2023).\nThe training corpus spanned genomic data from multiple species and human populations, exposing models to diverse sequence patterns, different regulatory architectures, and evolutionary constraints recurring across lineages. This cross-species pretraining mirrors the use of large multi-species alignments in protein language models but operates directly on raw DNA without explicit alignment. Context length expanded to approximately 6 kb per input sequence, representing an order-of-magnitude increase over DNABERT while still using dense transformer attention. The training objective remained masked language modeling on subsequences sampled from genomes.\nThe Nucleotide Transformer project introduced a benchmark panel that has become a standard yardstick for evaluating DNA language models. Tasks include promoter and enhancer classification, histone mark and chromatin accessibility prediction, splice site identification, and regulatory element type classification. Models are evaluated through linear probes or light fine-tuning on standardized train/validation/test splits. This benchmark infrastructure enabled systematic comparison across models and established the evaluation protocols now used throughout the field (see Chapter 20 for comprehensive discussion of genomic benchmarks).\nScaling experiments revealed predictable relationships between model size, training data, and performance. Larger models with more pretraining data and longer context windows achieved better downstream performance, following patterns observed in natural language and protein modeling. These scaling trends suggest that continued investment in larger genomic language models will yield further improvements, though the optimal allocation between parameters, data, and compute remains an active research question (Chapter 10).",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#gpn-cross-species-pretraining-for-variant-effect-prediction",
    "href": "p3-ch11-dna-lm.html#gpn-cross-species-pretraining-for-variant-effect-prediction",
    "title": "11  DNA Language Models",
    "section": "11.4 GPN: Cross-Species Pretraining for Variant Effect Prediction",
    "text": "11.4 GPN: Cross-Species Pretraining for Variant Effect Prediction\nWhile the Nucleotide Transformer demonstrated the value of scaling, the Genomic Pre-trained Network (GPN) explored a complementary direction: what can be learned from cross-species pretraining on relatively small, well-annotated genomes (Benegas, Batra, and Song 2023). Rather than scaling to maximum size, GPN asked whether self-supervision could yield useful variant effect predictors even in constrained settings.\nGPN was trained on unaligned reference genomes from Arabidopsis thaliana and seven related species within the Brassicales order using masked language modeling. Despite this modest training corpus, analysis revealed emergent encoding of gene structure (exon-intron boundaries, splice sites) and DNA sequence motifs (transcription factor binding patterns) without explicit supervision. The model discovered these patterns purely from statistical regularities of genomic sequence across related species.\nFor variant effect prediction, GPN used a likelihood ratio approach. Given reference and alternate alleles at a position, the model computes the log-likelihood of each under the learned sequence distribution. Variants that substantially reduce sequence likelihood (relative to the reference) are inferred to be more disruptive. This scoring strategy exploits the fact that constrained positions should have confident predictions for the reference allele, while unconstrained positions allow more flexibility.\nEvaluated on A. thaliana variants using allele frequencies from the 1001 Genomes Project, GPN outperformed traditional conservation scores including phyloP and phastCons. This was notable because phyloP and phastCons require explicit multiple sequence alignments and evolutionary models, while GPN learned its representations from unaligned sequences through self-supervision alone. The later GPN-MSA extended this approach to mammalian genomes by incorporating multi-species alignments, achieving strong performance on human variant benchmarks as discussed in Chapter 14.\nGPN established that cross-species pretraining captures evolutionary constraints transferable to variant effect prediction, that relatively small models trained on focused phylogenetic groups can outperform larger generic conservation measures within that group, and that the masked language modeling objective naturally produces representations suitable for variant scoring via likelihood comparisons.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#the-long-context-revolution",
    "href": "p3-ch11-dna-lm.html#the-long-context-revolution",
    "title": "11  DNA Language Models",
    "section": "11.5 The Long-Context Revolution",
    "text": "11.5 The Long-Context Revolution\nQuadratic attention complexity limits transformer context to tens of kilobases at best. Processing a 100 kb sequence with dense attention requires on the order of 10^10 computations per layer. Yet regulatory phenomena routinely span larger distances: enhancer-promoter interactions extend 50-200 kb, topologically associating domains organize chromatin at the megabase scale, and some gene regulation involves even longer-range dependencies. The mismatch between biological context and computational context represented a fundamental architectural limitation.\n\n11.5.1 HyenaDNA: Megabase Context via Implicit Convolutions\nHyenaDNA addressed this limitation by replacing attention with implicit convolutions that scale sub-quadratically (Nguyen et al. 2023). The Hyena architecture parameterizes long convolutional filters through neural networks rather than storing explicit filter weights, achieving O(L log L) complexity through efficient FFT-based convolution compared to O(L^2) for standard attention. The result was a 500-fold increase in context length: HyenaDNA processes sequences up to 1 Mb while maintaining single-nucleotide resolution.\nProcessing megabase-scale windows allows the model to capture entire gene bodies plus flanking regulatory regions, long-range enhancer-promoter interactions, and topologically associating domain structure. Despite the long context, single-nucleotide tokens preserve maximum resolution for variant effect prediction. Each nucleotide is independently represented without the ambiguity introduced by k-mer tokenization.\nOn Nucleotide Transformer benchmarks, HyenaDNA achieved state-of-the-art results on the majority of tasks with orders of magnitude fewer parameters. On GenomicBenchmarks, it surpassed prior state-of-the-art on seven of eight datasets. Perhaps most notably, HyenaDNA demonstrated in-context learning in genomics: performance improved when examples were included in the input context without updating model weights. This capability, familiar from large language models, had not previously been observed for genomic sequences and suggests that sufficient context length combined with appropriate architecture enables qualitatively new forms of biological reasoning.\n\n\n11.5.2 Caduceus: Bidirectional Processing with Reverse-Complement Equivariance\nDNA is double-stranded, and any sequence can be read from either strand. The reverse complement of a sequence encodes the same information from the opposite strand’s perspective. For many biological processes, predictions should be identical or related consistently regardless of which strand is presented. Standard neural networks can produce divergent predictions for a sequence and its reverse complement, even with data augmentation during training.\nCaduceus addressed this challenge by building reverse-complement equivariance directly into the architecture (Schiff et al. 2024). The model extends the Mamba state space architecture (which achieves O(L) complexity) to support both bidirectional processing and strand equivariance. The BiMamba component enables information flow in both directions along the sequence, while the MambaDNA block ensures mathematically related predictions for sequences and their reverse complements.\nOn downstream benchmarks, Caduceus outperformed previous long-range models. On challenging long-range variant effect prediction tasks, it exceeded models with ten times as many parameters that lacked bidirectionality or equivariance. The key insight was that incorporating appropriate biological inductive biases can substitute for raw scale. Strand symmetry is a known property of DNA; building it into the architecture avoids wasting model capacity learning what could be specified directly.\n\n\n11.5.3 Evo 2: Genome-Scale Modeling Across the Tree of Life\nEvo 2 represents the current frontier: training at genome scale across all domains of life (Brixi et al. 2025). While previous models focused on specific organisms (DNABERT on human, GPN on plants) or trained on multi-species corpora at limited scale (Nucleotide Transformer), Evo 2 aims to learn universal genomic patterns spanning bacteria, archaea, eukaryotes, and phages.\nThe training corpus draws from the OpenGenome2 dataset comprising 9.3 trillion DNA tokens across all domains of life. This massive scale exposes the model to the full spectrum of genomic organization: compact prokaryotic gene arrangements, sprawling eukaryotic regulatory landscapes with extensive noncoding sequence, viral genomes with overlapping reading frames, and the diversity of regulatory architectures across evolution. The model comes in 7 billion and 40 billion parameter variants.\nThe architecture builds on StripedHyena 2, a hybrid design combining convolutional operations with selective attention mechanisms. This enables processing of sequences up to 1 million nucleotides while maintaining computational tractability. The autoregressive training objective (predicting the next base given all previous bases) differs from the masked language modeling used in DNABERT and related models. Autoregressive training may provide complementary strengths for sequence generation and likelihood-based scoring, since the model learns to generate plausible sequences in addition to discriminating between them.\nEvo 2 exhibits several forms of emergent biological knowledge despite training only on raw sequence. The model learns to identify exon-intron boundaries without explicit annotation, discovers transcription factor binding site patterns matching known motifs, captures aspects of protein secondary and tertiary structure when processing coding sequences, and identifies prophage insertion regions in bacterial genomes. These capabilities emerge from pure sequence statistics, demonstrating that genome-scale pretraining captures fundamental biological organization.\nFor variant effect prediction, Evo 2 enables zero-shot scoring through likelihood ratios. Variants can be scored for consistency with learned genomic patterns by comparing model probabilities for reference versus alternate sequences. On benchmarks of pathogenic versus benign variants, zero-shot scores achieve competitive performance with specialized supervised methods, though calibration remains necessary before clinical application. The model also supports classification of variants of uncertain significance through simple classifiers trained on its embeddings.\nThe pan-species training enables cross-species applications. Variant interpretation extends naturally to non-model organisms, supporting conservation genomics and agricultural breeding where labeled training data is scarce. Model representations cluster sequences by phylogenetic relationships even without explicit evolutionary modeling. Beyond discriminative tasks, Evo 2 demonstrates generative capabilities: synthesizing plausible mitochondrial genomes, prokaryotic operons, and eukaryotic regulatory regions with coherence across kilobase to megabase scales.\n\n\n\n\n\n\nNote\n\n\n\nFigure suggestion: Multi-panel figure showing (A) OpenGenome2 training corpus composition across the tree of life with taxonomic breakdown, (B) StripedHyena 2 architecture schematic highlighting hybrid attention-convolution blocks, (C) t-SNE projection of sequence embeddings colored by taxonomic group showing phylogenetic clustering, and (D) zero-shot variant effect scores compared to experimental pathogenicity labels demonstrating calibration.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#training-data-and-what-models-learn",
    "href": "p3-ch11-dna-lm.html#training-data-and-what-models-learn",
    "title": "11  DNA Language Models",
    "section": "11.6 Training Data and What Models Learn",
    "text": "11.6 Training Data and What Models Learn\nDNA language models are trained on diverse corpora ranging from single reference genomes to pan-genomic collections spanning the tree of life. Understanding what training data is used and what models learn from it is essential for anticipating model capabilities and limitations.\n\n11.6.1 Training Corpus Composition\nEarly models like DNABERT trained primarily on the human reference genome (GRCh38), providing exposure to approximately 3 billion nucleotides from a single individual. The Nucleotide Transformer expanded to include multiple species and human population variation from resources like the 1000 Genomes Project. Evo 2 scaled to 9.3 trillion tokens spanning all domains of life, including complete bacterial chromosomes, eukaryotic genomes, viral sequences, and metagenomic assemblies.\nThe composition of training data shapes what models learn. Reference-only training captures the genome’s architecture but not population variation. Multi-individual training exposes models to common polymorphisms but may underrepresent rare variants. Cross-species training provides evolutionary context (constrained regions are conserved, variable regions diverge) but may not capture species-specific regulatory patterns. Training on functional genomics data (GROVER-style approaches) teaches regulatory activity patterns but ties models to specific assays and cell types.\nA tension exists between generality and specificity. Models trained on broader corpora learn more general representations that transfer across species and contexts, but may underperform narrower models on specific applications. Models trained on focused datasets may capture task-relevant patterns more effectively but transfer less well. The optimal training strategy depends on intended applications.\n\n\n11.6.2 Probing What Models Learn\nLinear probing experiments reveal what information is encoded in model representations without task-specific fine-tuning. By training simple classifiers (logistic regression, single-layer perceptrons) on frozen embeddings to predict known annotations, researchers can assess whether models have learned biologically meaningful patterns.\nDNA language models consistently learn to recognize several categories of genomic features. Motif recognition emerges naturally: models learn patterns corresponding to known transcription factor binding sites, splice signals, and other sequence motifs without explicit supervision. Probing for specific motif presence shows that model embeddings can distinguish sequences containing binding sites from those lacking them. Gene structure is encoded in representations: models distinguish coding from noncoding regions, identify exon-intron boundaries, and recognize splice donor and acceptor sites. This knowledge emerges from sequence statistics alone, suggesting that the compositional and structural differences between genomic region types are learnable from DNA sequence.\nEvolutionary constraints are implicitly captured, particularly in models trained on multi-species data. Positions under purifying selection (constrained across evolution) show different embedding patterns than neutral positions. This provides a self-supervised analog to traditional conservation scoring, though the relationship between model-learned and alignment-based conservation measures varies across genomic contexts.\nMore complex patterns like regulatory grammar (the syntax governing how transcription factors combine to specify expression) show mixed evidence. Models capture some aspects of regulatory logic, such as the spacing preferences between binding sites, but may not fully represent the combinatorial complexity of enhancer function. Similarly, long-range dependencies (enhancer-promoter interactions across tens of kilobases) are accessible to long-context models but require extensive probing to assess whether they are actually leveraged.\n\n\n11.6.3 What Models Do Not Learn\nEqually important is recognizing what current DNA language models struggle to represent. Epigenetic context is not captured by sequence-only models: DNA methylation, histone modifications, and chromatin accessibility all affect gene regulation but are not encoded in primary sequence. Some models (like GROVER) address this by incorporating functional genomics data, but this ties them to specific cell types and experimental conditions.\nThree-dimensional structure of chromatin affects which regulatory elements can physically interact, but linear sequence models cannot represent folding (see Chapter 17). Cell-type specificity of gene regulation depends on transcription factor expression levels and chromatin state, not just sequence; models trained on sequence alone can predict potential regulatory activity but not its realization in specific contexts.\nComplex variant patterns beyond single nucleotide changes remain challenging. Indels, structural variants, repeat expansions, and epistatic interactions between distant loci are either not representable (depending on tokenization) or poorly predicted. Most benchmark tasks focus on SNVs, leaving multi-nucleotide effects underexplored.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#benchmark-performance-and-evaluation",
    "href": "p3-ch11-dna-lm.html#benchmark-performance-and-evaluation",
    "title": "11  DNA Language Models",
    "section": "11.7 Benchmark Performance and Evaluation",
    "text": "11.7 Benchmark Performance and Evaluation\nStandardized benchmarks enable systematic comparison across DNA language models, though each benchmark captures only part of what we care about. Understanding benchmark construction and limitations is essential for interpreting performance claims.\n\n11.7.1 Major Benchmark Suites\nBEND (Benchmark for Nucleotide Deep learning) provides a unified framework with tasks including gene finding, enhancer annotation, chromatin state prediction, and variant effect scoring (Marin et al. 2024). Standardized splits and metrics enable fair comparison. BEND specifically evaluates whether models capture biologically meaningful features at different resolution scales.\nGenomic Benchmarks focus on regulatory element classification tasks: distinguishing promoters from nonpromoters, identifying active enhancers, predicting histone mark presence. These tasks test whether model representations encode basic genomic annotations. Most current DNA language models achieve high accuracy on these tasks, suggesting benchmark saturation for simpler classification problems.\nLong Range Benchmark (LRB) and DNALongBench evaluate long-context modeling capabilities (Cheng et al. 2024). Tasks include predicting distal enhancer-promoter interactions, modeling chromatin structure across hundreds of kilobases, and integrating information over extended genomic windows. These benchmarks specifically test whether long-context architectures provide meaningful advantages over shorter-context models.\nComparative evaluations across model families reveal that no single architecture dominates all tasks (Manzo, Borkowski, and Ovcharenko 2025). Performance varies substantially depending on task characteristics (local motif recognition versus long-range integration), training data composition, and architectural choices. HyenaDNA and Caduceus excel on long-range tasks where their architectural innovations matter; DNABERT-2 and Nucleotide Transformer perform well on shorter-range regulatory classification; Evo 2 shows advantages on cross-species tasks and variant effect prediction.\n\n\n11.7.2 Benchmark Limitations\nSeveral systematic issues affect benchmark interpretation. Saturation occurs when multiple models achieve near-perfect performance, eliminating discriminative power. This has happened for simpler classification tasks in Genomic Benchmarks. Leakage arises when training and test sequences share homology, allowing models to succeed through memorization rather than generalization. Careful sequence clustering (using tools like MMseqs2 or CD-HIT) is required to prevent this, but many older benchmarks lack rigorous split design.\nDistribution shift between benchmark data and real-world applications means strong benchmark performance may not predict deployment success. Most benchmarks derive from well-studied regions of well-characterized genomes; performance on understudied regions, rare variants, or non-European populations may differ substantially (see Chapter 22 for discussion of ancestry bias).\nMetric selection affects what gets optimized. AUROC favors discrimination regardless of calibration; Spearman correlation measures rank ordering but not absolute effect size prediction. Clinical applications may require well-calibrated probability estimates or accurate quantitative predictions, neither of which standard metrics directly assess. The gap between benchmark performance and deployment utility remains substantial for most genomic applications.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#annotation-aware-extensions",
    "href": "p3-ch11-dna-lm.html#annotation-aware-extensions",
    "title": "11  DNA Language Models",
    "section": "11.8 Annotation-Aware Extensions",
    "text": "11.8 Annotation-Aware Extensions\nRecent work explores enriching DNA language models with explicit biological structure beyond raw sequence. These approaches represent early steps toward multi-modal genomic foundation models.\nLife-Code proposes central-dogma-informed tokenization, treating coding and noncoding regions differently (Liu et al. 2025). Coding regions use codon tokens (three-nucleotide units specifying amino acids), respecting the genetic code’s fundamental structure. Noncoding regions use learned subword units optimized during training. Knowledge distillation from protein language models imports protein-level structural knowledge into DNA representations. Life-Code achieves competitive results across DNA, RNA, and protein tasks, suggesting that encoding biological structure into tokenization provides useful inductive bias.\nBioToken extends tokenization to include explicit genomic annotations (Medvedev et al. 2025). Rather than representing regions purely as nucleotide strings, BioToken creates composite tokens encoding sequence content, variant presence, structural annotations (exon, intron, UTR), and functional context. The associated BioFM model achieves state-of-the-art performance across genomic benchmarks with substantially fewer parameters (265M), suggesting that annotation-aware representations improve parameter efficiency.\nThese approaches foreshadow the multi-modal foundation models discussed in Part IV, where sequence is only one of many integrated information streams.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#using-dna-language-models-in-practice",
    "href": "p3-ch11-dna-lm.html#using-dna-language-models-in-practice",
    "title": "11  DNA Language Models",
    "section": "11.9 Using DNA Language Models in Practice",
    "text": "11.9 Using DNA Language Models in Practice\nDNA language models support multiple usage patterns for different applications.\n\n11.9.1 Embeddings as Universal Features\nThe simplest approach extracts embeddings from a pretrained model and uses them as features for downstream classifiers. The workflow involves extracting embeddings for windows around loci of interest, pooling or selecting positions relevant to the task, and training lightweight downstream models (linear layers, shallow MLPs, gradient boosting) on the extracted features.\nThis approach supports diverse applications. Regulatory element classification distinguishes promoters, enhancers, silencers, and insulators based on learned representations. Chromatin state prediction uses sequence embeddings to predict ATAC-seq or histone mark presence. Variant effect scoring replaces or augments hand-crafted features in frameworks like CADD with language model features (analogous to CADD v1.7’s incorporation of protein language model features). Splicing analysis combines embeddings with specialized architectures.\nBecause the language model remains frozen, this approach is computationally efficient and avoids catastrophic forgetting when new tasks are added. The pretrained model serves as a general-purpose feature extractor supporting many downstream applications.\n\n\n11.9.2 Fine-Tuning and Adaptation\nWhen sufficient labeled data exists, fine-tuning typically outperforms frozen embedding approaches. Full fine-tuning updates all language model parameters for a specific task, allowing representations to specialize. This achieves highest performance but requires more compute and risks catastrophic forgetting of general knowledge.\nParameter-efficient fine-tuning methods like LoRA (Low-Rank Adaptation) insert small trainable modules into each layer while keeping the backbone mostly frozen. These approaches achieve most of the performance gains of full fine-tuning while maintaining computational efficiency and preserving general capabilities. Adapter-based methods similarly add small bottleneck modules tuned for specific tasks.\n\n\n11.9.3 Zero-Shot and Few-Shot Scoring\nFor variant interpretation, language models enable zero-shot scoring based on sequence likelihood. Compute the model’s probability for a sequence containing the reference allele, compare to probability for the sequence with the alternative allele, and interpret variants reducing probability as more disruptive. This approach requires no variant-specific training and can score any single-nucleotide variant the model can represent.\nZero-shot scoring quality depends on how well the model’s learned distribution captures biological constraints. Performance tends to improve with model scale and training data diversity. Few-shot approaches include task examples in the input context, allowing in-context learning without parameter updates. HyenaDNA demonstrated this capability for genomic tasks, suggesting that sufficiently large models with long context can adapt through prompts rather than training.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#limitations-and-open-challenges",
    "href": "p3-ch11-dna-lm.html#limitations-and-open-challenges",
    "title": "11  DNA Language Models",
    "section": "11.10 Limitations and Open Challenges",
    "text": "11.10 Limitations and Open Challenges\nDespite substantial progress, DNA language models face several fundamental limitations.\nContext length versus resolution tradeoffs persist. Long-context models like HyenaDNA and Evo 2 can process megabase sequences but require efficient architectures that may not capture all the relationships dense attention would learn. Whether these architectural tradeoffs matter for specific applications remains task-dependent.\nComplex variant patterns beyond SNVs are poorly handled. Most tokenization schemes represent insertions and deletions awkwardly or not at all. Structural variants spanning kilobases, repeat expansions, and complex rearrangements fall outside what current models can process. Epistatic interactions between variants at distant loci are not captured even by long-context models.\nTraining data bias shapes model capabilities in underexplored ways. Models trained primarily on European-ancestry genomes may perform poorly on variants common in other populations. Ascertainment bias in training databases (enrichment for coding regions, well-studied genes, specific diseases) propagates to learned representations. The field lacks systematic evaluation of performance disparities across populations.\nInterpretability remains limited. While probing studies reveal what models encode, explaining why a specific variant receives a particular score in terms connecting to biological mechanism is difficult. Attention patterns and gradient-based attribution provide some insight but often fail to identify the specific sequence features driving predictions.\nIntegration with other modalities is nascent. DNA sequence provides necessary but insufficient information for predicting gene regulation. Epigenomic state, three-dimensional chromatin structure, transcription factor concentrations, and cellular context all matter. Current DNA language models cannot represent these factors; multi-modal approaches (discussed in Part IV) aim to address this limitation.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch11-dna-lm.html#summary",
    "href": "p3-ch11-dna-lm.html#summary",
    "title": "11  DNA Language Models",
    "section": "11.11 Summary",
    "text": "11.11 Summary\nThis chapter traced the development of DNA language models from early proof-of-concept systems like DNABERT through scaled approaches like Nucleotide Transformer to architectural innovations enabling megabase context in HyenaDNA, Caduceus, and Evo 2. We examined how cross-species training captures evolutionary constraints (GPN), how biological inductive biases like reverse-complement equivariance improve efficiency (Caduceus), and how annotation-aware tokenization enriches sequence representations (Life-Code, BioToken).\nSeveral principles emerged. Self-supervised pretraining on DNA works and produces representations competitive with task-specific models across diverse applications. Tokenization and architectural choices substantially affect performance, with single-nucleotide resolution and long context enabling applications inaccessible to earlier models. Biological inductive biases (strand symmetry, codon structure) can substitute for raw scale on appropriate tasks. Training data composition shapes what models learn; cross-species pretraining captures evolutionary constraints while multi-population training improves variant interpretation.\nCurrent DNA language models capture sequence patterns, regulatory motifs, and evolutionary constraints but cannot predict quantitative molecular readouts, represent epigenomic or three-dimensional context, or provide mechanistic explanations of their predictions. In Chapter 13, we examine how long-range sequence-to-function models like Enformer address some of these limitations by explicitly predicting molecular phenotypes from sequence. These models complement pure language models by providing quantitative predictions of regulatory activity, expression levels, and variant effects that DNA language models alone cannot provide.\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “[GPN] DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nCheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu Yang, Lei Li, and Jian Ma. 2024. “DNALONGBENCH: A Benchmark Suite For Long-Range DNA Prediction Tasks,” October. https://openreview.net/forum?id=opv67PpqLS.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nManzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025. “Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory Variants.” bioRxiv: The Preprint Server for Biology, June, 2025.05.19.654920. https://doi.org/10.1101/2025.05.19.654920.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. “BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks.” arXiv. https://doi.org/10.48550/arXiv.2311.12570.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html",
    "href": "p3-ch12-protein-lm.html",
    "title": "12  Protein Language Models",
    "section": "",
    "text": "12.1 The ESM Model Family\nEvolution is the most thorough experiment ever conducted on protein sequences. Over billions of years, natural selection has tested trillions of amino acid combinations, ruthlessly eliminating those that fail to fold or function while preserving those that work. The sequences that survive to populate modern databases are not random strings but successful solutions to biological problems, each one implicitly encoding information about structure, stability, and function. The central insight of protein language models is that this evolutionary record, comprising hundreds of millions of sequences in databases like UniRef, contains sufficient information to learn the fundamental principles of protein biology without ever being shown a crystal structure or a functional assay.\nThis insight transformed computational biology. Traditional approaches to understanding proteins required either expensive experimental characterization or physics-based simulations that struggled to capture the full complexity of protein behavior. Multiple sequence alignments (MSAs) could extract evolutionary conservation patterns, but required finding homologous sequences for each protein of interest and could not generalize beyond the specific alignment. Protein language models changed the equation by compressing evolutionary knowledge into neural network parameters that transfer across the entire protein universe. A model trained to predict masked amino acids learns, as a byproduct, which residues contact each other in three-dimensional space, which positions tolerate variation, and which substitutions disrupt function. The physics of protein folding, selected for across evolutionary time, emerges from the statistics of surviving sequences.\nThis chapter examines how protein language models pioneered biological foundation modeling, establishing principles that would later guide genomic language models. The ESM family demonstrated that transformers can learn protein structure and function from sequence alone, achieving results that rival methods requiring explicit structural supervision. Understanding these successes, and their limitations, provides essential context for the DNA and RNA language models covered in subsequent chapters.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#the-esm-model-family",
    "href": "p3-ch12-protein-lm.html#the-esm-model-family",
    "title": "12  Protein Language Models",
    "section": "",
    "text": "12.1.1 ESM-1b: Establishing the Paradigm\nThe Evolutionary Scale Modeling project, developed at Meta AI Research, demonstrated that transformer language models trained on protein sequences learn biologically meaningful representations without explicit supervision (Rives et al. 2021). The approach was strikingly simple: take the BERT architecture that had revolutionized natural language processing, replace words with amino acids, and train on protein sequence databases. The resulting models learned far more than anyone expected.\nESM-1b was trained on UniRef50, a clustered database of approximately 33 million protein sequences covering the known diversity of protein families. UniRef50 clusters sequences at 50% identity, providing broad coverage while reducing redundancy that would otherwise bias the model toward overrepresented families (Suzek et al. 2007). This curation strategy ensures the model encounters diverse evolutionary solutions to protein function rather than memorizing common motifs.\nThe architecture follows the BERT-style bidirectional transformer design with 650 million parameters distributed across 33 layers, a hidden dimension of 1,280, and 20 attention heads. The maximum sequence length of 1,024 amino acids accommodates most individual protein domains and many complete proteins. The training objective is masked language modeling: randomly mask 15% of amino acids in each sequence, and train the model to predict the masked positions given surrounding context. This objective contains no information about structure, function, or evolution beyond what is implicit in the sequences themselves.\n\n\n12.1.2 Emergent Biological Knowledge\nThe surprise was not that ESM-1b learned to predict masked amino acids accurately, but what else it learned in the process. Despite never seeing structural or functional labels during training, ESM-1b’s internal representations encode information about protein biology at multiple levels of organization.\nSecondary structure emerges in the attention patterns. When researchers analyzed which sequence positions the model attends to when making predictions, they found that attention concentrates along patterns corresponding to alpha helices and beta sheets. The model implicitly learns that certain amino acid sequences form specific structural elements, encoding this knowledge without ever being told what secondary structure is.\nMore remarkably, ESM-1b captures residue-residue contacts. Amino acids that are distant in the linear sequence but close in three-dimensional space attend to each other in the model’s attention matrices. This emergent capability suggests the model learns aspects of protein folding purely from sequence statistics. When attention weights were converted to contact predictions, they achieved accuracy approaching dedicated contact prediction methods that were explicitly trained for that task.\nThe model’s masked token predictions correlate strongly with position-specific conservation scores derived from multiple sequence alignments. ESM effectively learns which positions tolerate variation and which are evolutionarily constrained, extracting this information from the statistical patterns across 33 million sequences rather than from explicit conservation annotations. Positions where the model confidently predicts specific amino acids correspond to positions that are conserved across protein families.\nPerhaps most striking, attention concentrates on functionally important positions. Catalytic residues, binding sites, and other sites of biological importance receive elevated attention even without explicit functional annotation in the training data. The model discovers that certain sequence positions are more informative about surrounding context, and these positions frequently correspond to sites where nature has constrained variation because they perform essential functions.\n\n\n12.1.3 ESM-2: Scaling Up\nESM-2 extended the ESM approach across a range of model scales, from 8 million to 15 billion parameters, enabling systematic study of how biological knowledge scales with model capacity (Lin et al. 2022). The results confirmed a pattern familiar from natural language processing: bigger models learn more.\n\nESM-2 model family spanning four orders of magnitude in parameter count, with architecture details and relative performance on structure-related tasks.\n\n\nModel\nParameters\nLayers\nHidden Dim\nPerformance Gain\n\n\n\n\nESM-2 (8M)\n8M\n6\n320\nBaseline\n\n\nESM-2 (35M)\n35M\n12\n480\nModest\n\n\nESM-2 (150M)\n150M\n30\n640\nSubstantial\n\n\nESM-2 (650M)\n650M\n33\n1280\nLarge\n\n\nESM-2 (3B)\n3B\n36\n2560\nNear-optimal\n\n\nESM-2 (15B)\n15B\n48\n5120\nState-of-the-art\n\n\n\nPerformance scales smoothly with model size across structure prediction, contact prediction, and variant effect tasks. The scaling relationship is not linear: doubling parameters does not double accuracy. But gains remain consistent through even the largest models, suggesting that the 15-billion parameter ceiling reflects computational constraints rather than fundamental limits on what sequence statistics can teach.\nThe scaling behavior mirrors observations in natural language processing, where larger models consistently capture more nuanced patterns. This predictable relationship between scale and capability provides a roadmap for model development: if more biological knowledge is needed, train a larger model on more data. The practical implications shaped how the field approached subsequent genomic foundation models.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#alternative-architectures",
    "href": "p3-ch12-protein-lm.html#alternative-architectures",
    "title": "12  Protein Language Models",
    "section": "12.2 Alternative Architectures",
    "text": "12.2 Alternative Architectures\nThe success of ESM raised a natural question: how much depends on the specific BERT architecture versus the general approach of self-supervised learning on protein sequences? The ProtTrans family explored this question by applying multiple transformer architectures to protein modeling (Elnaggar et al. 2021).\nProtBERT applies the bidirectional encoder to protein sequences, trained on the Big Fantastic Database (BFD) comprising approximately 2.1 billion protein sequences. This training corpus, substantially larger than UniRef50, provides broader coverage at the cost of including more redundant and potentially lower-quality sequences. The architectural choices match ESM closely, enabling direct comparison of training data effects.\nProtT5 adapts the encoder-decoder architecture from T5, enabling both understanding and generation tasks (Raffel et al. 2019). The encoder processes input sequences to produce contextual representations, while the decoder can generate output sequences conditioned on those representations. This architecture proved valuable for tasks requiring sequence generation, such as structure-conditioned design or sequence completion, though the encoder-only architecture remains dominant for embedding and classification tasks.\nProtXLNet explores permutation language modeling, capturing bidirectional context without the artificial [MASK] token that BERT-style models require during training (Yang et al. 2020). By training on all possible token orderings, XLNet-style models learn to predict each token from any subset of context tokens, potentially capturing richer dependencies at the cost of more complex training.\nThese architectural variants demonstrate that the protein language modeling paradigm generalizes beyond specific design choices. All architectures learn meaningful representations when trained on sufficient data, though performance differences emerge for specific downstream tasks. Encoder-only models excel at classification and embedding tasks where the entire sequence is available. Encoder-decoder models enable generation tasks where outputs must be produced token by token.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#attention-and-evolutionary-coupling",
    "href": "p3-ch12-protein-lm.html#attention-and-evolutionary-coupling",
    "title": "12  Protein Language Models",
    "section": "12.3 Attention and Evolutionary Coupling",
    "text": "12.3 Attention and Evolutionary Coupling\nThe emergence of contact information in ESM’s attention patterns connects to a deeper principle: evolutionary coupling. When two residues must maintain physical contact for a protein to function, mutations at one position create selective pressure for compensatory mutations at the other. Over evolutionary time, these correlated mutations leave statistical signatures in protein families that can be detected through covariance analysis of multiple sequence alignments.\nDirect Coupling Analysis (DCA) and related methods extract these coevolutionary signals to predict residue-residue contacts (morcos_dca_2011?). The approach requires constructing multiple sequence alignments, computing covariance matrices, and applying statistical corrections to distinguish direct from indirect correlations. The resulting contact predictions enabled the first accurate structure predictions for proteins lacking homologs in structural databases.\nProtein language models learn to extract similar information through a different route. Rather than computing covariance explicitly, transformers learn attention patterns that capture which positions inform predictions at other positions. When position \\(i\\) strongly attends to position \\(j\\) during masked prediction, the model has learned that knowing the amino acid at \\(j\\) helps predict the amino acid at \\(i\\). This is precisely the signature of evolutionary coupling: positions that covary because they must maintain physical contact.\nThe attention-based approach offers several advantages over traditional covariance analysis. Language models generalize across protein families, learning shared principles that transfer to proteins with sparse evolutionary sampling. They handle the statistical challenge of distinguishing direct from indirect correlations implicitly through deep architecture rather than requiring explicit correction. And they provide rich representations beyond binary contact predictions, encoding information about the strength and nature of residue relationships.\nRao and colleagues demonstrated this connection directly by extracting attention weights from ESM and converting them to contact predictions (rao_transformer_2021?). The resulting predictions approached the accuracy of dedicated contact prediction methods, despite the model never being trained to predict contacts. The attention mechanism, optimized purely for masked token prediction, discovers the coevolutionary structure of protein sequences as a byproduct.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#esmfold-structure-from-sequence",
    "href": "p3-ch12-protein-lm.html#esmfold-structure-from-sequence",
    "title": "12  Protein Language Models",
    "section": "12.4 ESMFold: Structure from Sequence",
    "text": "12.4 ESMFold: Structure from Sequence\n\n12.4.1 Eliminating the Alignment Bottleneck\nThe most dramatic demonstration of protein language model capabilities came with ESMFold, which predicts protein 3D structure directly from ESM-2 embeddings without requiring multiple sequence alignments (Lin et al. 2022). Traditional structure prediction, including AlphaFold2, relies heavily on MSAs constructed through computationally expensive searches against sequence databases. These searches can take hours per protein, and prediction quality depends critically on finding informative homologs.\nESMFold eliminates this requirement entirely. The architecture couples ESM-2 (using the 15-billion parameter variant) with a structure module adapted from AlphaFold2’s Evoformer and structure module. The language model embeddings replace MSA-derived features, providing the evolutionary context that the structure module needs to predict atomic coordinates. The model takes a single sequence as input and outputs predicted 3D coordinates for all atoms.\nThe computational speedup is substantial: approximately 60-fold faster than AlphaFold2 for typical proteins. This speed advantage makes it feasible to predict structures for the millions of protein sequences emerging from environmental sequencing projects, where computing MSAs would be prohibitively expensive. Metagenomic proteins, often lacking close homologs in existing databases, represent exactly the cases where MSA-based methods struggle and where single-sequence predictions become essential.\nESMFold achieves atomic-level accuracy for many proteins, though slightly below AlphaFold2 for proteins that benefit strongly from MSA information. The accuracy gap is largest for proteins with sparse evolutionary sampling, where explicit alignments provide information that single-sequence analysis cannot fully recover. For well-represented protein families, ESMFold approaches AlphaFold2 accuracy at a fraction of the computational cost.\n\n\n12.4.2 What ESMFold Reveals About PLMs\nESMFold’s success demonstrates that ESM-2’s internal representations encode sufficient information to determine 3D structure. The language model has learned not merely local sequence patterns but global folding principles, capturing what makes a sequence fold into a particular three-dimensional shape.\nThis has profound implications for understanding what protein language models learn. The attention patterns that emerge from masked prediction are, in some sense, learning the physics of protein folding. Residues that need to be close in 3D space to maintain stability attend to each other in the transformer’s attention matrices. The statistical patterns in protein sequences, shaped by billions of years of evolution under physical constraints, encode structural information that sufficiently powerful language models can decode.\nThe fundamental insight is that evolution has already solved the structure prediction problem, millions of times over, and recorded the solutions in sequence databases. Language models learn to read those solutions, extracting the implicit structural knowledge that selection has embedded in surviving sequences.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#function-prediction",
    "href": "p3-ch12-protein-lm.html#function-prediction",
    "title": "12  Protein Language Models",
    "section": "12.5 Function Prediction",
    "text": "12.5 Function Prediction\nBeyond structure, protein language models enable prediction of protein function directly from sequence. Function prediction encompasses multiple tasks: predicting Gene Ontology terms that describe molecular function, biological process, and cellular component; classifying enzyme activity; identifying binding sites and interaction partners; and predicting subcellular localization.\nTraditional function prediction relied on homology: proteins similar in sequence are assumed to share function. This approach fails for orphan proteins lacking characterized homologs and cannot distinguish functional differences between closely related sequences. PLM-based approaches address both limitations by learning representations that capture functional signatures beyond simple sequence similarity.\nFor Gene Ontology term prediction, PLM embeddings serve as input features to classification models that predict which GO terms apply to each protein. The embeddings capture evolutionary and structural information relevant to function, enabling accurate predictions even for proteins with limited homology to characterized sequences. Performance improves with embedding quality, suggesting that larger language models capture more functionally relevant information.\nEnzyme classification benefits similarly from PLM representations. The Enzyme Commission hierarchy categorizes enzymes by the reactions they catalyze, from broad classes (oxidoreductases, transferases) to specific substrate preferences. PLM embeddings distinguish these categories effectively, capturing the sequence features that determine catalytic activity without requiring explicit structural analysis.\nBinding site prediction applies attention analysis to identify which residues participate in ligand binding, protein-protein interactions, or nucleic acid recognition. Positions that the model identifies as important for contextual prediction often correspond to functionally important sites, including binding pockets and catalytic residues. This capability enables rapid identification of functional sites in newly sequenced proteins.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#variant-effect-prediction",
    "href": "p3-ch12-protein-lm.html#variant-effect-prediction",
    "title": "12  Protein Language Models",
    "section": "12.6 Variant Effect Prediction",
    "text": "12.6 Variant Effect Prediction\nA critical clinical application of protein language models is predicting the effects of amino acid substitutions. Missense variants are the most common type of protein-coding mutation, and clinical genetics pipelines must routinely assess whether specific substitutions are likely pathogenic or benign. The traditional approach required either direct experimental characterization or computational methods trained on labeled pathogenicity data, both of which scale poorly to the millions of possible variants in each human genome.\nESM-1v demonstrated that PLMs can predict variant effects without any training on variant labels (Meier et al. 2021). The approach exploits the masked language modeling objective directly: for a variant at position \\(i\\) changing amino acid \\(a\\) to amino acid \\(b\\), compute the log-likelihood ratio:\n\\[\\Delta \\text{score} = \\log P(b \\mid \\text{context}) - \\log P(a \\mid \\text{context})\\]\nIf the model assigns higher probability to the mutant amino acid than the wild-type, the variant is predicted benign; if lower, deleterious. This zero-shot prediction requires no labeled training data. The model’s evolutionary knowledge, learned from sequence databases, directly informs variant interpretation.\nThe intuition is straightforward. Evolution has shaped protein sequences such that certain positions strongly prefer certain amino acids. Substitutions that violate these preferences are more likely to disrupt function. The language model captures these preferences through training on millions of evolutionarily successful sequences. Variants that the model finds surprising are more likely to be functionally disruptive.\nBrandes and colleagues applied ESM-1b to predict effects for all approximately 450 million possible missense variants in the human genome, providing a precomputed resource for clinical variant interpretation (Brandes et al. 2023). On ClinVar benchmarks, ESM-1b outperformed existing methods in classifying variants as pathogenic or benign.\nAlphaMissense extended this approach by combining PLM representations with structural context from predicted protein structures (Cheng et al. 2023). The integration of sequence-based and structure-based signals improves accuracy, particularly for variants affecting protein stability or buried residues. AlphaMissense provides predictions for all approximately 71 million possible single amino acid substitutions in the human proteome.\nThe detailed comparison of variant effect prediction methods, including how PLM-based scores integrate with clinical classification frameworks, is covered in Chapter 14. Here, the key point is that protein language models provide the foundational representations that make accurate zero-shot variant prediction possible.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#integration-with-structure-prediction",
    "href": "p3-ch12-protein-lm.html#integration-with-structure-prediction",
    "title": "12  Protein Language Models",
    "section": "12.7 Integration with Structure Prediction",
    "text": "12.7 Integration with Structure Prediction\nProtein language models exist within a broader ecosystem of computational methods for protein analysis. Understanding how PLMs relate to structure prediction systems clarifies their role and capabilities.\nAlphaFold2 achieved breakthrough accuracy in structure prediction by combining learned representations with explicit geometric modeling (Jumper et al. 2021). The architecture processes both sequence information through embeddings and evolutionary information through multiple sequence alignments, using an attention-based module (Evoformer) to integrate these signals before predicting atomic coordinates. AlphaFold2’s success depended critically on MSA quality: proteins with many homologs could be predicted accurately, while orphan proteins remained challenging.\nESMFold demonstrated that PLM embeddings can replace MSA-derived features, achieving competitive accuracy without the alignment bottleneck. This finding clarified the relationship between language models and structure prediction: PLMs learn to compress evolutionary information into representations that are functionally equivalent to explicit alignments, at least for proteins with sufficient representation in training databases.\nAlphaFold3 extended structure prediction to protein complexes, nucleic acids, and small molecules (Abramson et al. 2024). The architecture incorporates diffusion-based generation, enabling prediction of binding poses and complex assemblies. These capabilities complement PLM-based function prediction by providing structural context for interpreting functional predictions.\nGenerative protein design methods including RFDiffusion and ProteinMPNN leverage both structural and sequence information (Watson et al. 2023; Dauparas et al. 2022). RFDiffusion generates novel protein backbones through diffusion processes conditioned on design objectives. ProteinMPNN designs sequences likely to fold into specified structures. Both methods benefit from PLM representations when designing sequences with desired functional properties, demonstrating how language models integrate into the broader protein engineering pipeline.\nThe trajectory from ESM to ESMFold to integration with design tools illustrates how PLMs serve as a foundation for diverse downstream applications. The representations learned through self-supervised training transfer across tasks, providing a common language for structure prediction, function annotation, variant interpretation, and protein engineering. This pattern of foundation models enabling diverse applications recurs throughout genomic AI, as discussed in Chapter 10.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#limitations",
    "href": "p3-ch12-protein-lm.html#limitations",
    "title": "12  Protein Language Models",
    "section": "12.8 Limitations",
    "text": "12.8 Limitations\nDespite their success, protein language models face several limitations that inform the development of genomic models and guide appropriate application.\n\n12.8.1 Orphan and Dark Proteins\nPLMs learn from evolutionary statistics, performing best for proteins with rich representation in training databases. Orphan proteins, those unique to specific lineages without detectable homologs, lack the evolutionary context that PLMs exploit. For these proteins, the model has no basis for distinguishing likely from unlikely amino acids at each position, and predictions degrade accordingly.\nThe problem extends to “dark” proteins that are poorly characterized despite having homologs. If an entire protein family has escaped experimental characterization, PLMs may learn statistical patterns without capturing functional relevance. The model cannot distinguish constraint imposed by function from constraint imposed by historical accident.\n\n\n12.8.2 Novel Folds\nTraining data shapes what models can predict. PLMs trained on natural protein databases learn the statistical patterns of naturally occurring folds, potentially struggling with designed proteins or hypothetical folds outside the training distribution. When researchers design proteins with novel topologies not found in nature, PLM predictions become less reliable because the relevant sequence patterns were never encountered during training.\n\n\n12.8.3 Conformational Flexibility\nMost PLM representations assume a single static structure, but many proteins adopt multiple conformations relevant to function. Allosteric proteins, intrinsically disordered regions, and proteins that undergo conformational changes upon binding present challenges for methods that embed each sequence into a single representation. The language model learns the average properties of sequences but may not capture the dynamic range that determines biological behavior.\n\n\n12.8.4 Epistasis\nMost variant effect predictions assume independence: the effect of mutation A does not depend on whether mutation B is present. Real proteins exhibit epistasis, where variant effects depend on sequence context. Two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. Current PLM-based predictors model marginal effects at each position but do not explicitly capture higher-order interactions, though the contextual embeddings may represent some epistatic relationships implicitly.\n\n\n12.8.5 Interpretability\nWhile attention patterns correlate with biological features, understanding exactly what PLMs learn remains challenging. The field is developing interpretation methods (Chapter 24), but PLMs remain partially opaque. For clinical applications where explanations matter, this interpretability gap limits adoption. A prediction that a variant is pathogenic is more useful when accompanied by mechanistic insight into why the variant disrupts function.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#lessons-for-genomic-foundation-models",
    "href": "p3-ch12-protein-lm.html#lessons-for-genomic-foundation-models",
    "title": "12  Protein Language Models",
    "section": "12.9 Lessons for Genomic Foundation Models",
    "text": "12.9 Lessons for Genomic Foundation Models\nThe success of protein language models established principles that guided subsequent development of genomic foundation models. These lessons transfer with appropriate modifications to DNA and RNA modeling.\n\n12.9.1 Self-Supervision Captures Biological Knowledge\nPLMs demonstrated that massive amounts of biological knowledge can be learned from unlabeled sequences. The same evolutionary pressures that shape proteins also shape DNA. Purifying selection removes deleterious variants, leaving statistical signatures in sequence databases that self-supervised models can exploit. This principle underlies the entire foundation model paradigm: sufficiently large models trained on sufficiently large datasets with appropriate objectives will learn representations that capture biological function.\n\n\n12.9.2 Scale Yields Consistent Improvements\nPerformance improves predictably with model size through the range currently explored. The progression from 8 million to 15 billion parameters in ESM-2 showed consistent gains across structure prediction, contact prediction, and variant effect tasks. While scaling cannot continue indefinitely, current models remain in a regime where additional capacity yields reliable improvements. This relationship justified the computational investment in large genomic foundation models.\n\n\n12.9.3 Transfer Learning is Effective\nRepresentations learned for one task (masked token prediction) transfer to other tasks (structure prediction, variant effects, function annotation). This suggests that self-supervised pretraining captures fundamental biological knowledge rather than task-specific shortcuts. A model trained to predict masked amino acids simultaneously learns about protein structure, function, evolutionary constraint, and disease relevance. The same principle motivates genomic language models: models trained to predict masked nucleotides may simultaneously learn about regulatory elements, evolutionary conservation, and variant effects.\n\n\n12.9.4 Architecture Choices Must Match Sequence Properties\nThe BERT-style bidirectional encoder proved effective for proteins, where entire sequences are typically available and lengths rarely exceed a thousand residues. Genomic sequences present different challenges: much longer lengths spanning kilobases to megabases, different information density with coding regions being dense while intergenic regions are sparser, and structural features including reverse-complement relationships absent in proteins. These differences motivate architectural adaptations in genomic language models, as explored in Chapter 11.\n\n\n12.9.5 Integration Multiplies Capability\nAlphaMissense demonstrated that PLM embeddings combine effectively with structural and population genetics information, achieving accuracy beyond what any single information source provides. The most powerful methods integrate multiple signals, using PLMs as one component of larger systems. This principle extends to genomic foundation models, where sequence-based representations complement rather than replace functional annotations, chromatin data, and clinical information.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch12-protein-lm.html#significance",
    "href": "p3-ch12-protein-lm.html#significance",
    "title": "12  Protein Language Models",
    "section": "12.10 Significance",
    "text": "12.10 Significance\nProtein language models established that transformer architectures can learn deep biological knowledge from sequence data alone. ESM’s ability to predict structure, function, and variant effects without explicit labels demonstrated the power of self-supervised learning on evolutionary data. The framework validated a paradigm: treat biological sequences as a language, train large models to predict masked tokens, and extract functional knowledge from the learned representations.\nThis success directly motivated the development of genomic language models. If proteins constitute a language that transformers can learn, perhaps DNA does too. The DNA language models covered in Chapter 11 adapt PLM architectures and training strategies to the distinct challenges of genomic sequences: longer contexts, different alphabets, and the full complexity of gene regulation. RNA language models (Chapter 15) occupy an intermediate position, sharing features with both protein and DNA modeling.\nThe integration path continues beyond sequence modeling. Just as PLM representations feed into structure prediction (ESMFold) and variant effect prediction (AlphaMissense), genomic language model embeddings integrate into regulatory models (Chapter 13) and clinical risk prediction (?sec-clinical-risk). Protein design methods (Chapter 28) demonstrate how generative modeling builds on the representations that language models provide. Throughout this progression, the principle established by ESM remains: self-supervised learning on biological sequences captures knowledge that transfers across diverse applications, providing a foundation for genomic AI.\n\n\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. “[AlphaFold3] Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. “Robust Deep Learning–Based Protein Sequence Design Using ProteinMPNN.” Science 378 (6615): 49–56. https://doi.org/10.1126/science.add2187.\n\n\nElnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, et al. 2021. “ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Deep Learning and High Performance Computing.” arXiv. https://doi.org/10.48550/arXiv.2007.06225.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “[AlphaFold2] Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” arXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. “UniRef: Comprehensive and Non-Redundant UniProt Reference Clusters.” Bioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. “De Novo Design of Protein Structure and Function with RFdiffusion.” Nature 620 (7976): 1089–1100. https://doi.org/10.1038/s41586-023-06415-8.\n\n\nYang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. “XLNet: Generalized Autoregressive Pretraining for Language Understanding.” arXiv. https://doi.org/10.48550/arXiv.1906.08237.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "p3-ch13-regulatory.html",
    "href": "p3-ch13-regulatory.html",
    "title": "13  Long-Context Regulatory Models",
    "section": "",
    "text": "13.1 The Long-Range Regulation Problem\nGene regulation in higher eukaryotes poses a fundamental modeling challenge: the elements that control a gene’s expression often lie tens or hundreds of kilobases away from the gene itself. An enhancer 80 kilobases upstream of a promoter can drive tissue-specific expression; an insulator 50 kilobases downstream can block inappropriate activation. Disease-associated variants discovered through genome-wide association studies frequently map to these distal regulatory regions, yet short-context models treat such elements as noise. The convolutional architectures examined in Chapter 6 excel at detecting local motifs but cannot span the distances over which mammalian gene regulation operates. Pure transformer architectures (Chapter 7) could theoretically attend across arbitrary distances, but quadratic attention costs make naive application to hundred-kilobase windows computationally prohibitive.\nHybrid architectures resolve this tension by combining the strengths of both paradigms. A convolutional front-end efficiently extracts local sequence features and compresses the input to a manageable length, while a transformer backbone propagates information across the full window through attention. This design enables models like Enformer to process 200 kilobase windows and predict chromatin state, transcription initiation, and gene expression from sequence alone. The result is a new class of regulatory models that can capture enhancer-promoter interactions, predict the effects of distal variants on gene expression, and provide mechanistic hypotheses about long-range regulation. These models do not merely extend receptive fields; they reformulate what sequence-based regulatory prediction can accomplish.\nThis chapter examines the major long-context regulatory architectures, their training regimes, and their applications to variant effect prediction. We begin with the biological motivation for long-range context, then trace the development from Enformer through Borzoi to AlphaGenome and related approaches like Sei. Each model makes distinct architectural choices that shape what it can and cannot capture about regulatory biology.\nConsider a canonical mammalian gene with complex tissue-specific expression. The promoter sits at the transcription start site, but the sequences that determine when and where the gene is expressed may be scattered across a 200 kilobase neighborhood. Multiple enhancers drive expression in different tissues; silencers suppress expression in inappropriate contexts; insulators demarcate regulatory domains. Chromatin looping brings these distal elements into physical proximity with the promoter, but the loops themselves are dynamic and cell-type-specific.\nShort-context models face an information-theoretic barrier in this setting. A model with a 2 kilobase receptive field cannot distinguish a variant in an enhancer 50 kilobases upstream from a variant in neutral sequence at the same distance. Both fall outside the model’s effective context. Stacking more convolutional layers or using dilated convolutions can expand the receptive field, but the computational path between distant positions grows long, and gradients attenuate over many layers. Models like Basenji2 (Kelley et al. 2018) pushed convolutional receptive fields to tens of kilobases through aggressive pooling, but purely convolutional architectures struggle to propagate information across hundreds of kilobases without impractical depth.\nThe scale of the problem becomes concrete when examining enhancer-promoter distances in the human genome. Median enhancer-promoter distances in many tissues span 20 to 50 kilobases, with substantial fractions exceeding 100 kilobases (gasperini_genome-wide_2019?). Topologically associating domains (TADs), which define the neighborhoods within which regulatory elements typically interact, range from hundreds of kilobases to several megabases. A model that cannot span these distances cannot fully capture the regulatory grammar of the genome.\nAttention mechanisms offer a direct solution: by computing pairwise interactions between all positions, attention can model dependencies across arbitrary distances in a single layer. The cost is quadratic scaling with sequence length. A naive transformer operating on 200,000 base pairs at single-nucleotide resolution would require attention matrices with 40 billion entries, far exceeding practical memory limits. Hybrid architectures sidestep this constraint by using convolutions to compress the sequence before attention, reducing the effective sequence length to a few thousand tokens while preserving the information needed for long-range modeling.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Long-Context Regulatory Models</span>"
    ]
  },
  {
    "objectID": "p3-ch13-regulatory.html#enformer-attention-meets-regulatory-genomics",
    "href": "p3-ch13-regulatory.html#enformer-attention-meets-regulatory-genomics",
    "title": "13  Long-Context Regulatory Models",
    "section": "13.2 Enformer: Attention Meets Regulatory Genomics",
    "text": "13.2 Enformer: Attention Meets Regulatory Genomics\nEnformer (Ž. Avsec et al. 2021) demonstrated that combining convolutional compression with transformer attention could dramatically improve expression prediction from sequence. The model processes 200 kilobase windows of DNA and predicts thousands of chromatin and transcription tracks across cell types and species, establishing a template that subsequent models have extended and refined.\n\n13.2.1 Architecture\nThe Enformer architecture consists of three stages that progressively transform raw sequence into multi-task predictions.\nThe convolutional stem takes one-hot encoded DNA (four channels for A, C, G, T) and applies a series of convolutional blocks with residual connections. Each block includes convolutions that detect local patterns, batch normalization and nonlinearities, and pooling operations that reduce sequence length while increasing channel depth. By the end of the stem, a 200 kilobase input has been compressed to roughly 1,500 tokens, each representing approximately 128 base pairs of underlying sequence. This compression is crucial: it reduces the attention computation from quadratic in 200,000 to quadratic in 1,500, a reduction of roughly 17,000-fold in memory requirements.\nThe transformer trunk operates on the compressed sequence through a stack of self-attention layers. Each layer computes attention scores between all pairs of positions, allowing information to flow directly between any two locations in the 200 kilobase window. Relative positional encodings preserve information about the distances between elements, which matters for regulatory biology where the spacing between motifs often carries functional significance. The combination of multi-head attention and feed-forward layers enables the model to learn complex, position-dependent relationships across the full window.\nTask-specific output heads branch from the shared transformer backbone. Separate heads predict different types of outputs: DNase accessibility and ATAC-seq signal (chromatin openness), histone modifications including H3K4me3, H3K27ac, and other marks, CAGE signal reflecting transcription initiation, and additional functional genomics readouts where training data is available. Each head consists of convolutional and linear layers that transform the shared representation into track-specific predictions.\nThe multi-task design serves multiple purposes. Different assays provide complementary supervision: chromatin accessibility reflects regulatory potential, histone marks indicate active enhancers and promoters, and CAGE captures transcriptional output. Training on all assays jointly encourages the backbone to learn representations that capture the full regulatory cascade from accessible chromatin through enhancer activation to transcription initiation.\n\n\n13.2.2 Training Data and Cross-Species Learning\nEnformer trains on functional genomics data from both human and mouse, spanning hundreds of assays and cell types. Human training data derives largely from ENCODE and Roadmap Epigenomics consortia, supplemented by CAGE data from FANTOM and additional chromatin profiling studies. Mouse data from analogous consortia provides complementary supervision.\nCross-species training confers several advantages. Regulatory sequences that are functionally constrained evolve more slowly than neutral sequence, so mouse and human share many regulatory motifs and principles despite 80 million years of divergence. Training on both species helps the model distinguish conserved regulatory logic from species-specific noise, reduces overfitting to idiosyncrasies of human data, expands the effective training set without requiring additional human samples, and implicitly emphasizes evolutionarily conserved patterns that are more likely to be functionally important.\nThe training objective combines losses across all tracks, positions, and species. Count-based likelihoods (Poisson or negative binomial) handle sequencing-derived signals, while correlation-based objectives ensure the model captures the overall shape of coverage profiles. Per-track weighting prevents abundant assays from dominating gradients.\n\n\n13.2.3 Variant Effect Prediction\nThe clinical and scientific value of Enformer lies substantially in its ability to predict how sequence variants alter regulatory activity. The procedure follows a straightforward logic: extract a 200 kilobase window containing the variant, compute predictions for the reference allele, compute predictions for the alternative allele, and compare the outputs across all tracks and positions.\nThe resulting variant effect scores span thousands of dimensions, one for each assay and cell type. A variant might increase predicted DNase accessibility in one cell type while decreasing predicted CAGE signal in another, suggesting context-dependent regulatory effects. By aggregating predictions around gene promoters, researchers can estimate variant effects on gene expression in specific tissues.\nValidation against GTEx expression quantitative trait loci (eQTLs) demonstrated that Enformer’s predictions correlate with observed genetic effects on expression (Ž. Avsec et al. 2021). Variants with large predicted effects on promoter-proximal CAGE signal were enriched among significant eQTLs. Notably, this correlation extended to distal variants: sequence changes 50 kilobases or more from a gene’s transcription start site still showed predictive power when they fell in regions of predicted regulatory activity. This long-range predictive capacity distinguishes Enformer from short-context models and validates the architectural investment in extended context windows.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Long-Context Regulatory Models</span>"
    ]
  },
  {
    "objectID": "p3-ch13-regulatory.html#borzoi-from-chromatin-to-transcriptome",
    "href": "p3-ch13-regulatory.html#borzoi-from-chromatin-to-transcriptome",
    "title": "13  Long-Context Regulatory Models",
    "section": "13.3 Borzoi: From Chromatin to Transcriptome",
    "text": "13.3 Borzoi: From Chromatin to Transcriptome\nWhile Enformer predicts transcription initiation through CAGE, RNA-seq captures a richer picture of gene expression: not just where transcription begins, but how the transcript is spliced, which isoforms dominate, where transcription terminates, and how stable the resulting mRNA is. Borzoi (Linder et al. 2025) extends the hybrid architecture paradigm to predict full RNA-seq coverage profiles, enabling a unified view of how sequence variation affects the entire transcriptional program.\n\n13.3.1 Motivation\nA single gene can produce multiple transcript isoforms through alternative promoter usage, alternative splicing, and alternative polyadenylation. These isoforms may have different stabilities, different translation efficiencies, and different functions. A variant that shifts isoform ratios without changing total expression could have substantial phenotypic consequences: a switch from a cytoplasmic to a nuclear isoform, for instance, or inclusion of a premature stop codon in the predominant transcript.\nCAGE and chromatin assays cannot capture these complexities. They measure where transcription might begin and what the chromatin environment looks like, but they do not reveal how RNA polymerase traverses the gene body, where splicing occurs, or which 3’ end is selected. RNA-seq coverage profiles encode all of this information: exon boundaries appear as coverage drops at intron junctions, alternative splicing manifests as variable junction usage, and polyadenylation site choice appears in the coverage pattern near gene 3’ ends.\n\n\n13.3.2 Architecture and Training\nBorzoi builds on an Enformer-style backbone with modifications tailored to RNA-seq prediction. The convolutional stem and transformer trunk follow similar principles, compressing long input windows and propagating information through attention. Output heads predict stranded RNA-seq coverage across the window, with additional heads for complementary signals like PRO-seq (nascent transcription), CAGE, and other assays when available.\nTraining on RNA-seq coverage imposes different demands than training on chromatin marks. Coverage varies over orders of magnitude between introns and exons; the model must capture both the overall expression level and the fine structure of the coverage profile. Junction reads that span splice sites provide particularly informative supervision, as they directly constrain the model to learn splicing patterns. The loss function balances accurate prediction of coverage levels with faithful reproduction of the coverage shape, including sharp transitions at exon boundaries.\n\n\n13.3.3 Applications Beyond Expression Level\nBy predicting full RNA-seq coverage, Borzoi enables analyses that go beyond simple expression quantification. Splicing variant effects can be assessed by comparing predicted coverage at exons and junctions under reference and alternative alleles. A variant that reduces predicted junction reads for a particular exon suggests exon skipping; increased junction reads to a cryptic splice site suggests aberrant splicing. These predictions complement specialized splicing models like SpliceAI (Chapter 6), providing additional context about how splicing changes fit within the broader transcriptional program.\nAlternative promoter usage becomes visible through coverage patterns near transcription start sites. A variant that increases coverage downstream of one TSS while decreasing it downstream of another suggests a shift in promoter preference. Such shifts can alter the 5’ UTR of the resulting transcript, affecting translation efficiency and regulatory motif content.\nPolyadenylation site choice affects 3’ UTR length and content. Shorter 3’ UTRs may escape microRNA-mediated repression; longer ones may include additional regulatory elements. Borzoi’s coverage predictions around annotated polyadenylation sites can reveal variants that shift site usage, potentially explaining effects on mRNA stability and translation that would be invisible to chromatin-based models.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Long-Context Regulatory Models</span>"
    ]
  },
  {
    "objectID": "p3-ch13-regulatory.html#sei-a-regulatory-vocabulary-from-sequence",
    "href": "p3-ch13-regulatory.html#sei-a-regulatory-vocabulary-from-sequence",
    "title": "13  Long-Context Regulatory Models",
    "section": "13.4 Sei: A Regulatory Vocabulary from Sequence",
    "text": "13.4 Sei: A Regulatory Vocabulary from Sequence\nWhile Enformer and Borzoi predict continuous coverage tracks, Sei (chen_sequence_2022?) takes a complementary approach: learning a discrete vocabulary of sequence classes that capture distinct regulatory activities. Rather than predicting thousands of individual assays, Sei maps sequences to a reduced set of regulatory states, each associated with characteristic chromatin and transcription patterns.\n\n13.4.1 Approach\nSei builds on observations that chromatin states cluster into interpretable categories: active promoters, strong enhancers, poised enhancers, heterochromatin, and so forth. Previous methods like ChromHMM defined such states from observed chromatin marks in specific cell types. Sei learns to predict sequence class membership directly from DNA, asking what regulatory identity a sequence carries based on its intrinsic properties.\nThe model predicts 40 sequence classes derived from clustering patterns across chromatin accessibility, histone modifications, and transcription factor binding. Each class corresponds to a recognizable regulatory state: promoter-like sequences, enhancer-like sequences, CTCF binding sites, repressed regions, and various intermediate states. The output is not a single class assignment but a probability distribution over classes, reflecting the observation that many sequences have context-dependent regulatory potential.\n\n\n13.4.2 Complementary to Track Prediction\nSei and Enformer-style models serve complementary purposes. Enformer provides detailed, quantitative predictions across specific assays and cell types; Sei provides a compressed, interpretable summary of regulatory identity. For variant interpretation, both perspectives can be valuable. Enformer might reveal that a variant reduces predicted H3K27ac signal in liver but not heart; Sei might reveal that the same variant shifts sequence class membership from “strong enhancer” toward “weak enhancer,” a more immediately interpretable characterization.\nThe regulatory vocabulary approach also facilitates systematic analysis across many variants. Rather than tracking changes in thousands of individual tracks, researchers can ask how a set of variants affects the distribution of regulatory classes, identifying patterns that might be obscured in high-dimensional track space.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Long-Context Regulatory Models</span>"
    ]
  },
  {
    "objectID": "p3-ch13-regulatory.html#alphagenome-unifying-modalities-at-megabase-scale",
    "href": "p3-ch13-regulatory.html#alphagenome-unifying-modalities-at-megabase-scale",
    "title": "13  Long-Context Regulatory Models",
    "section": "13.5 AlphaGenome: Unifying Modalities at Megabase Scale",
    "text": "13.5 AlphaGenome: Unifying Modalities at Megabase Scale\nAlphaGenome (Z. Avsec, Latysheva, and Cheng 2025) extends the hybrid modeling paradigm in two directions: longer context windows (approximately one megabase) and broader output modalities spanning chromatin, expression, splicing, and three-dimensional contacts. The goal is a single model that provides a comprehensive view of how sequence determines regulatory state.\n\n13.5.1 Architectural Extensions\nThe megabase context window pushes against computational limits even with hybrid architectures. AlphaGenome addresses this through efficient attention mechanisms that reduce the quadratic cost, hierarchical processing that handles different output modalities at appropriate resolutions, and architectural refinements accumulated from Enformer and Borzoi development.\nThe output repertoire spans chromatin accessibility and histone modifications (following Enformer), gene expression and RNA coverage (following Borzoi), splicing predictions including exon inclusion and junction usage, and contact predictions reflecting three-dimensional chromatin organization.\nUnifying these modalities in a single model offers several advantages. The backbone representation must capture information relevant to all outputs, encouraging learning of features that connect chromatin state to transcription to RNA processing. Variant effect predictions become coherent across modalities: a single forward pass reveals how a variant affects chromatin, expression, splicing, and contacts, rather than requiring separate runs through independent models.\n\n\n13.5.2 Access and Practical Considerations\nAlphaGenome is primarily available through an API interface rather than as a downloadable model. This arrangement simplifies use for many applications: researchers can score variants without managing large model weights or specialized hardware. It also introduces constraints around data privacy, customization, and integration with local pipelines. Clinical applications that cannot send patient sequence data to external services may be unable to use API-only models directly, motivating interest in openly available alternatives.\nFrom the perspective of variant interpretation workflows, AlphaGenome provides a comprehensive set of predictions from a single query. A variant can be assessed for effects on local chromatin state, expression of nearby genes, splicing of overlapping transcripts, and potential disruption of chromatin contacts, all from the same underlying model. The challenge lies in synthesizing these multiple outputs into actionable conclusions, a topic addressed further in Chapter 14.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Long-Context Regulatory Models</span>"
    ]
  },
  {
    "objectID": "p3-ch13-regulatory.html#what-hybrid-architectures-accomplish",
    "href": "p3-ch13-regulatory.html#what-hybrid-architectures-accomplish",
    "title": "13  Long-Context Regulatory Models",
    "section": "13.6 What Hybrid Architectures Accomplish",
    "text": "13.6 What Hybrid Architectures Accomplish\nThe progression from DeepSEA through Enformer, Borzoi, and AlphaGenome reflects accumulating solutions to specific limitations. Each model addresses constraints that bounded its predecessor’s utility.\n\n13.6.1 Spanning Enhancer-Promoter Distances\nThe most direct contribution is enabling long-range interaction modeling. A 200 kilobase context window encompasses the distances over which most cis-regulatory interactions occur. Attention mechanisms allow the model to learn direct relationships between enhancers and promoters without requiring information to propagate through many intermediate layers. Empirically, this translates to improved prediction of expression and better correlation with eQTLs, particularly for variants in distal regulatory elements.\n\n\n13.6.2 Multi-Task Regularization\nTraining on hundreds of assays jointly constrains the model to learn representations that generalize across regulatory modalities. A feature useful only for predicting H3K4me3 in one cell type provides less gradient signal than a feature useful across chromatin, transcription, and accessibility. This multi-task pressure steers the model toward learning fundamental regulatory logic rather than assay-specific artifacts.\n\n\n13.6.3 Cross-Species Constraints\nTraining on human and mouse together further regularizes the model. Species-specific binding site variants, repetitive elements, and technical artifacts in training data affect one species but not the other. Features that generalize across species are more likely to reflect conserved regulatory mechanisms. This provides a form of evolutionary validation built into the training process.\n\n\n13.6.4 Unified Variant Effect Prediction\nPerhaps most practically valuable, hybrid models provide a unified framework for variant effect prediction on expression and related phenotypes. Rather than assembling scores from multiple specialized models, researchers can query a single model for comprehensive predictions. The outputs span cell types and assays, enabling tissue-specific interpretation of regulatory variants.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Long-Context Regulatory Models</span>"
    ]
  },
  {
    "objectID": "p3-ch13-regulatory.html#limitations-and-open-challenges",
    "href": "p3-ch13-regulatory.html#limitations-and-open-challenges",
    "title": "13  Long-Context Regulatory Models",
    "section": "13.7 Limitations and Open Challenges",
    "text": "13.7 Limitations and Open Challenges\nDespite their power, long-context regulatory models face fundamental limitations that bound their current utility and define directions for future development.\n\n13.7.1 Training Data Constraints\nFunctional genomics data is biased in coverage, overrepresenting well-studied cell types (embryonic stem cells, K562, HepG2, lymphoblastoid cell lines) while leaving many tissue types and disease-relevant cell states poorly covered. Models trained on available data will perform better in represented contexts and may fail silently in underrepresented ones. Ancestry bias compounds the problem: most functional genomics studies derive from individuals of European descent, limiting the diversity of haplotypes and regulatory variants represented in training data.\nThese biases propagate to variant effect predictions. A variant in a regulatory element active primarily in pancreatic beta cells may receive poor predictions if beta cell data is sparse in training. A variant on a haplotype common in African populations but rare in Europeans may fall outside the model’s effective training distribution. Users must recognize that prediction confidence varies with representation in training data, a consideration that current models do not explicitly communicate.\n\n\n13.7.2 Finite Context\nEven megabase-scale windows capture only local regulation. Trans-acting factors, three-dimensional contacts spanning multiple megabases, and whole-chromosome organization fall outside model context. Structural variants that rearrange large genomic segments, duplicate enhancers, or create novel fusion genes cannot be modeled within fixed-window architectures. The reference genome assumption underlying these models further limits their applicability to complex haplotypes and populations with substantial structural variation relative to the reference.\n\n\n13.7.3 Missing Three-Dimensional Context\nLinear sequence models treat DNA as a one-dimensional string, but gene regulation occurs in three-dimensional nuclear space. Chromatin loops bring distal elements into proximity; nuclear compartmentalization segregates active and repressed regions; phase-separated condensates concentrate regulatory factors. While AlphaGenome predicts some contact features, current hybrid models do not fully integrate three-dimensional chromatin organization. The relationship between linear sequence, three-dimensional structure, and regulatory output remains incompletely captured. We return to this gap in Chapter 17, which examines models that explicitly address chromatin architecture.\n\n\n13.7.4 Correlation Versus Causation\nHybrid models learn correlations between sequence and functional readouts, not causal mechanisms. A variant might receive a high predicted effect score because it disrupts a motif correlated with expression in training data, not because the motif causally drives expression. Attribution methods can identify which sequence features contribute to predictions, but attribution is not validation. High-confidence predictions require experimental confirmation through approaches like massively parallel reporter assays, CRISPR perturbation, or allelic series analysis.\n\n\n13.7.5 Interpretability Challenges\nThe scale of these models (hundreds of millions of parameters) makes mechanistic interpretation difficult. Attention patterns provide some insight into which positions the model considers related, but attention weights are not guaranteed to reflect the model’s actual computational strategy. Attribution methods (saliency maps, integrated gradients) can highlight important input positions, but the features the model constructs from those positions remain opaque. Chapter Chapter 24 examines these interpretability methods and their limitations in detail.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Long-Context Regulatory Models</span>"
    ]
  },
  {
    "objectID": "p3-ch13-regulatory.html#relationship-to-foundation-models",
    "href": "p3-ch13-regulatory.html#relationship-to-foundation-models",
    "title": "13  Long-Context Regulatory Models",
    "section": "13.8 Relationship to Foundation Models",
    "text": "13.8 Relationship to Foundation Models\nLong-context regulatory models occupy an interesting position in the genomic foundation model landscape. They share key characteristics with foundation models: large scale, broad training data, strong performance across tasks, and utility as feature extractors for downstream applications. Yet they differ from self-supervised DNA language models in their heavy reliance on supervised, task-specific training signals.\nEnformer and its descendants can be viewed as highly specialized foundation models, pre-trained on the specific task of regulatory prediction and adaptable to related applications. Their representations encode regulatory logic learned from functional genomics supervision, complementing the sequence patterns learned by self-supervised models from raw DNA. In practice, the two approaches may prove most powerful in combination: self-supervised models provide sequence representations from evolutionary context, while supervised regulatory models provide representations from functional genomics context. Integrating these representations for tasks like variant effect prediction is an active area of development, explored further in Chapter 14.\nFrom a practical standpoint, hybrid regulatory models remain among the most directly useful genomic deep learning systems for variant interpretation. They provide quantitative, tissue-specific predictions for regulatory variants, outperform short-context alternatives on distal regulatory elements, and integrate naturally into variant prioritization workflows. Their limitations are real but understood; their strengths are substantial and empirically validated.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Long-Context Regulatory Models</span>"
    ]
  },
  {
    "objectID": "p3-ch13-regulatory.html#toward-next-generation-architectures",
    "href": "p3-ch13-regulatory.html#toward-next-generation-architectures",
    "title": "13  Long-Context Regulatory Models",
    "section": "13.9 Toward Next-Generation Architectures",
    "text": "13.9 Toward Next-Generation Architectures\nThe hybrid CNN-transformer design that defines Enformer and its descendants represents one solution to the long-context modeling problem, but not necessarily the final one. Alternative architectures offer different trade-offs that may prove advantageous as the field evolves.\nHierarchical attention approaches like the 1D-Swin architecture used in Genomic Interpreter (Li et al. 2023) partition sequences into local windows, applying attention within windows and using shifted-window patterns to propagate information across boundaries. This reduces attention complexity while preserving long-range modeling capacity, potentially enabling even longer context windows or higher resolution within existing compute budgets.\nState-space models and related architectures (Hyena, Mamba) replace attention entirely with mechanisms that have sub-quadratic or linear scaling in sequence length. These approaches can process very long sequences efficiently, making them attractive for genome-scale modeling. Early applications to DNA sequence show promise, though integration with multi-task regulatory prediction remains less developed than for hybrid transformers.\nThe computational landscape continues to evolve. Hardware advances (larger memory, faster attention implementations), algorithmic improvements (more efficient attention variants), and architectural innovations may shift the optimal design point. What remains constant is the underlying biological problem: gene regulation spans long distances, and models that capture this reach provide better predictions than those that do not.\nThe models examined in this chapter demonstrate that long-range regulatory prediction from sequence is tractable. Enformer established that hybrid architectures could span 200 kilobases and predict expression-related features; Borzoi extended this to full transcriptome coverage; AlphaGenome unified multiple modalities at megabase scale. These models do not explain regulatory mechanism, but they predict regulatory outcomes with sufficient accuracy to guide variant interpretation and hypothesis generation. The integration of these predictions into clinical variant assessment, alongside protein-based and splicing-focused models, is the subject of Chapter 14.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nKelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y. McLean, and Jasper Snoek. 2018. “[Basenji2] Sequential Regulatory Activity Prediction Across Chromosomes with Convolutional Neural Networks.” Genome Research 28 (5): 739–50. https://doi.org/10.1101/gr.227819.117.\n\n\nLi, Zehui, Akashaditya Das, William A. V. Beardall, Yiren Zhao, and Guy-Bart Stan. 2023. “Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer.” arXiv. https://doi.org/10.48550/arXiv.2306.05143.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Long-Context Regulatory Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html",
    "href": "p3-ch14-vep-fm.html",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "",
    "text": "14.1 The Foundation Model Paradigm for Variant Interpretation\nA clinician reviewing a genome sequence faces thousands of variants, each a potential explanation for a patient’s condition, yet only a handful will prove relevant. For the vast majority of these variants, no direct experimental evidence exists: they have never been observed in other patients, never tested in functional assays, never catalogued in clinical databases. The clinician must nonetheless decide which variants warrant further investigation and which can be safely set aside. This decision, repeated millions of times across clinical laboratories worldwide, depends on computational tools that predict whether a given sequence change disrupts biological function.\nThe challenge is not simply classification but calibrated uncertainty. A pathogenic variant missed means a diagnosis delayed; a benign variant flagged as concerning triggers unnecessary anxiety and expensive follow-up testing. Classical approaches to variant effect prediction (VEP) relied on conservation scores and hand-engineered features, achieving useful but limited discrimination (Chapter 4). The foundation models introduced in preceding chapters offer a different paradigm: representations learned from massive sequence databases that capture evolutionary and biophysical constraints far more nuanced than any feature set a human could design. This chapter examines how these foundation models transform variant effect prediction, what they add beyond classical methods, and where significant gaps remain.\nThe models surveyed here span the protein and DNA modalities covered in Chapter 12, Chapter 13, and Chapter 11. Protein language models score missense variants by measuring how unexpected an amino acid substitution appears in learned evolutionary context. DNA language models and regulatory models assess how sequence changes disrupt motifs, splicing signals, or long-range regulatory interactions. The most sophisticated systems combine multiple modalities to provide mechanistically interpretable predictions across variant types. Yet even state-of-the-art models face fundamental challenges in calibration, uncertainty quantification, and generalization across populations and variant classes. Understanding both their capabilities and limitations is essential for responsible clinical deployment.\nClassical variant effect predictors operate by aggregating hand-crafted features: conservation scores computed from multiple sequence alignments, amino acid property changes, protein domain annotations, and regulatory marks at genomic loci (Chapter 4). Methods like CADD train machine learning models to distinguish pathogenic from benign variants using these features, achieving useful discrimination but ultimately limited by what features the developers chose to include. When a variant falls in a region poorly covered by existing annotations, classical methods have little to offer.\nFoundation models invert this relationship. Rather than engineering features, they learn representations from raw sequence data during pretraining, then apply those representations to variant interpretation. A protein language model trained to predict masked amino acids implicitly learns which substitutions violate evolutionary constraints. A DNA language model trained to predict nucleotides in genomic context learns which changes disrupt sequence grammar. The representations encode information about structure, function, and constraint that was never explicitly labeled during training.\nThis paradigm shift has practical consequences. Coverage extends to any variant in any gene, not just those with extensive prior annotation. Representations capture subtle patterns (co-evolution between distant residues, context-dependent motif strength) that resist manual feature engineering. Transfer learning enables rapid adaptation to new tasks and variant classes. The cost is interpretability: understanding why a foundation model assigns a particular score requires specialized analysis techniques rather than simple inspection of feature weights.\nThree architectural families dominate current VEP applications. Protein language models (Chapter 12) encode amino acid sequences and score missense variants by measuring likelihood changes. DNA language models (Chapter 11) operate on nucleotide sequences and can score variants of any type. Regulatory models (Chapter 13) predict molecular phenotypes (chromatin accessibility, gene expression, splicing) and score variants by their predicted impact on these phenotypes. The most powerful systems combine elements from multiple families.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html#protein-based-variant-effect-prediction",
    "href": "p3-ch14-vep-fm.html#protein-based-variant-effect-prediction",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "14.2 Protein-Based Variant Effect Prediction",
    "text": "14.2 Protein-Based Variant Effect Prediction\nMissense variants (single amino acid substitutions) account for approximately half of known pathogenic variants in ClinVar, making protein-level prediction a central challenge. Foundation model approaches exploit a simple insight: evolution has already tested billions of amino acid substitutions across millions of years; variants that repeatedly survive natural selection are likely tolerable, while those never observed in homologous proteins likely disrupt function.\n\n14.2.1 Zero-Shot Scoring with Protein Language Models\nThe simplest foundation model approach to missense VEP requires no task-specific training. A protein language model trained on masked token prediction assigns probabilities to each amino acid at each position given surrounding context. Variant effect scores emerge from comparing the probability of the reference amino acid to the probability of the variant amino acid.\nESM-1v operationalizes this approach using the ESM-2 architecture fine-tuned for single-sequence variant effect prediction (Meier et al. 2021). For a variant substituting amino acid \\(a_\\text{ref}\\) with \\(a_\\text{var}\\) at position \\(i\\), the score is computed as:\n\\[\\Delta \\text{LLR} = \\log P(a_\\text{var} | \\text{context}) - \\log P(a_\\text{ref} | \\text{context})\\]\nNegative scores indicate that the variant amino acid is less probable than reference in learned evolutionary context, suggesting potential deleteriousness. The model sees only the single query sequence, not multiple sequence alignments, yet achieves discrimination competitive with alignment-based methods on deep mutational scanning benchmarks.\nThis zero-shot capability reflects what protein language models learn during pretraining: structural constraints (buried positions are hydrophobic), functional constraints (active sites are conserved), and co-evolutionary patterns (compensating mutations at contacting residues). The model has never seen pathogenicity labels, yet its predictions correlate with disease association because evolution and disease share underlying biology.\n\n\n14.2.2 Alignment-Based Models: EVE and popEVE\nAn alternative approach explicitly models multiple sequence alignments rather than relying on implicit evolutionary information in single-sequence representations. EVE (Evolutionary Model of Variant Effect) fits a variational autoencoder to the MSA for each protein, learning a generative model that captures position-specific and pairwise constraints (Frazer et al. 2021). Variant scores derive from the change in sequence probability under this model.\nThe EVE architecture consists of an encoder that maps sequences to a latent space and a decoder that reconstructs sequences from latent representations. Training maximizes a lower bound on sequence likelihood across the MSA. For variant scoring, EVE computes the log-likelihood ratio between mutant and wild-type sequences, capturing how surprising the substitution appears given the evolutionary record for that specific protein.\npopEVE extends this framework with improved training procedures and explicit modeling of population allele frequencies (Orenbuch et al. 2025). By incorporating frequency information, popEVE better separates rare deleterious variants from common benign polymorphisms. The model achieves strong performance on ClinVar classification while providing uncertainty estimates through ensemble disagreement.\nThe tradeoff between single-sequence and MSA-based approaches involves coverage versus depth. ESM-1v scores any protein sequence without requiring alignment construction. EVE provides stronger performance when high-quality MSAs are available but cannot score proteins lacking sufficient homologs. For well-studied protein families with deep evolutionary sampling, MSA-based methods remain competitive; for orphan proteins or rapidly evolving sequences, single-sequence models offer the only foundation model option.\n\n\n14.2.3 AlphaMissense: Structure-Informed Pathogenicity Prediction\nAlphaMissense represents the current state of the art for proteome-wide missense pathogenicity prediction, combining protein language model representations with structural information from AlphaFold2 (Cheng et al. 2023). The system provides precomputed scores for 71 million possible missense variants across the human proteome, enabling instant lookup for any variant in any protein-coding gene.\nThe architecture integrates multiple information sources. Sequence representations come from a protein language model encoding the wild-type sequence and mutation position. Structural representations derive from AlphaFold2 predictions, capturing local geometry (secondary structure, solvent accessibility, packing density) and longer-range contacts. A neural network combines these representations to produce a pathogenicity probability between 0 and 1.\nTraining uses a carefully constructed dataset that avoids the circularity plaguing earlier predictors. Rather than training on ClinVar labels (which themselves derive from computational predictions), AlphaMissense uses population frequency as a proxy for pathogenicity: variants common in gnomAD are likely benign, while variants absent from large population samples and observed in disease contexts are likely pathogenic. This approach reduces the risk of learning features that simply recapitulate existing predictor scores.\nCalibration receives explicit attention. Raw model outputs undergo isotonic regression calibration against held-out ClinVar variants, ensuring that predicted probabilities correspond to observed pathogenic proportions. A score of 0.8 should mean that 80% of variants with similar scores are pathogenic, enabling meaningful clinical interpretation. AlphaMissense reports calibrated scores along with discrete classifications (likely pathogenic, likely benign, uncertain) at thresholds chosen to achieve specific precision targets.\nPerformance on independent benchmarks substantially exceeds classical predictors. On deep mutational scanning datasets (where experimental fitness measurements provide ground truth independent of clinical labels), AlphaMissense achieves correlations of 0.5 to 0.7 depending on the assay, compared to 0.3 to 0.5 for CADD or PolyPhen-2. On ClinVar expert-reviewed variants held out from training, AlphaMissense achieves AUROC values above 0.9, representing a meaningful improvement over the 0.85 to 0.88 typical of classical methods.\nThe structural component proves essential for this performance. Ablation experiments removing AlphaFold2 features degrade performance substantially, particularly for variants at protein-protein interfaces and buried core positions where local geometry determines functional impact. The protein language model captures evolutionary constraint; structural information explains why that constraint exists.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html#dna-based-variant-effect-prediction",
    "href": "p3-ch14-vep-fm.html#dna-based-variant-effect-prediction",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "14.3 DNA-Based Variant Effect Prediction",
    "text": "14.3 DNA-Based Variant Effect Prediction\nApproximately 98% of the human genome lies outside protein-coding regions, yet noncoding variants contribute substantially to disease risk through effects on gene regulation, splicing, and genome stability. Predicting the impact of these variants requires models that operate directly on DNA sequence rather than translated protein.\n\n14.3.1 Splice Variant Prediction with SpliceAI\nSplicing variants illustrate both the promise and current limitations of deep learning for noncoding VEP. Approximately 10% of pathogenic variants in ClinVar act through splicing mechanisms, disrupting the precise excision of introns from pre-mRNA. Classical approaches relied on position weight matrices matching consensus splice site sequences, achieving limited sensitivity for variants outside the core GT-AG dinucleotides.\nSpliceAI applies deep residual convolutional networks to predict splice site usage from raw DNA sequence (jaganathan_predicting_2019?). The architecture processes 10,000 nucleotides of context through 32 residual blocks with dilated convolutions (dilation rates increasing from 1 to 128), enabling the receptive field to span several kilobases while maintaining nucleotide resolution. Output heads predict splice donor probability, splice acceptor probability, and junction usage at each position.\nFor variant effect prediction, SpliceAI compares predictions between reference and alternate sequences. The delta score quantifies the change in splice site probability, with positive values indicating gained splice sites and negative values indicating lost sites. Scores exceeding 0.2 correlate with experimentally validated splicing changes; scores above 0.5 have high specificity for pathogenic splicing variants.\nClinical deployment has validated SpliceAI’s utility. Illumina integrated the model into their clinical interpretation pipeline, and multiple diagnostic laboratories use SpliceAI scores as supporting evidence for ACMG classification. The model identifies pathogenic splicing variants missed by classical methods, particularly deep intronic variants that create novel splice sites through cryptic activation.\nLimitations reflect the model’s training data. SpliceAI learned from annotated transcripts representing major isoforms in common tissues. Tissue-specific alternative splicing, rare isoforms, and developmental stage-specific patterns fall outside the training distribution. The model also does not capture downstream consequences: whether a predicted splicing change produces a functional protein, triggers nonsense-mediated decay, or has no phenotypic effect requires additional analysis.\n\n\n14.3.2 Regulatory Variant Prediction with Enformer\nWhile SpliceAI addresses one specific noncoding mechanism, regulatory variants that alter enhancer activity, promoter function, or chromatin organization require different approaches. Enformer (Chapter 13) predicts multiple molecular phenotypes (histone modifications, transcription factor binding, chromatin accessibility, gene expression) from 196,608 base pairs of DNA sequence, providing a substrate for regulatory VEP (Ž. Avsec et al. 2021).\nVariant effect prediction with Enformer compares predicted tracks between reference and alternate sequences. For a variant in an enhancer, the model might predict reduced H3K27ac signal and decreased CAGE expression at the target promoter. These molecular predictions can be aggregated into variant effect scores, with larger predicted changes indicating greater functional impact.\nSeveral challenges complicate Enformer-based VEP. The model predicts relative effects (fold changes in predicted signal) rather than absolute deleteriousness. Calibrating these predictions against pathogenicity labels requires additional supervised training. Cell-type specificity adds complexity: a variant might strongly affect predictions in cardiac tissue while showing no effect in liver, requiring prior knowledge of relevant tissues for clinical interpretation.\nSei extends this approach by learning a regulatory vocabulary: clusters of predicted effects that correspond to interpretable categories like “active promoter,” “strong enhancer,” or “CTCF binding site” (chen_sei_2022?). Variant scores reflect shifts between these categories, providing more interpretable outputs than raw track changes. A variant that converts an enhancer prediction to a quiescent state has clearer implications than one that reduces H3K27ac by 0.3 log-fold.\n\n\n14.3.3 DNA Language Models: GPN-MSA and Evo 2\nDNA language models provide an alternative to phenotype prediction: scoring variants by how unexpected they appear in learned sequence context, analogous to protein language model approaches for missense variants.\nGPN-MSA combines DNA language modeling with multi-species sequence alignments (Benegas et al. 2024). The model processes aligned sequences from dozens of vertebrate species, learning which positions are conserved and which tolerate variation. Variant scores derive from likelihood ratios: how much less probable is the variant allele compared to reference given the alignment context? This approach captures deep evolutionary constraint missed by simple conservation scores while providing genome-wide coverage including noncoding regions.\nEvo 2 pushes context length to approximately one megabase, enabling single models to capture local motifs and long-range dependencies simultaneously (Brixi et al. 2025). The StripedHyena architecture provides computational efficiency at this scale through state-space-based sequence modeling rather than quadratic attention. Training on diverse genomes across the tree of life teaches general principles of sequence organization that transfer to human variant interpretation.\nZero-shot variant scoring with Evo 2 follows the standard likelihood ratio approach. Initial benchmarks show performance competitive with conservation-based scores for coding variants and potentially superior performance for noncoding variants where local sequence context matters more than position-specific conservation. The extremely long context enables modeling of effects mediated by distal elements, though whether this theoretical capability translates to improved VEP remains under investigation.\n\n\n14.3.4 AlphaGenome: Unified Multi-Omic Variant Effect Prediction\nAlphaGenome represents the most ambitious current attempt at comprehensive VEP, predicting multiple molecular phenotypes from megabase-scale DNA sequence and using those predictions to assess variant effects across modalities (Z. Avsec, Latysheva, and Cheng 2025).\nThe architecture combines convolutional feature extraction at local scales with attention-based integration across the full million-base-pair context. Output heads predict one-dimensional tracks (chromatin accessibility, histone modifications, transcription factor binding), two-dimensional contact maps (Hi-C interactions), and splicing features (junction usage, splice site strength). Training uses functional genomics data from hundreds of cell types and assays, learning to map sequence to molecular phenotype across diverse contexts.\nVariant effect prediction with AlphaGenome provides mechanistically interpretable outputs. A promoter variant might show reduced accessibility and decreased expression prediction. An enhancer variant might show weakened contact with its target promoter in addition to reduced local histone acetylation. A splicing variant triggers SpliceAI-like splice site changes while also affecting regulatory track predictions near the affected exon.\nThe multi-omic approach enables variant prioritization that considers multiple mechanisms. A variant in a regulatory element that affects accessibility, expression, and chromatin contacts represents stronger evidence than one affecting only a single predicted phenotype. Conversely, variants with no predicted effect across modalities can be deprioritized despite proximity to disease genes.\nLimitations include substantial computational cost (evaluating a single variant requires forward passes through the full model), potential overconfidence when extrapolating beyond training cell types, and the challenge of calibrating multi-dimensional predictions into single pathogenicity scores. AlphaGenome represents a direction rather than a destination: future systems will likely extend its multi-omic integration while addressing these practical constraints.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html#combining-evidence-across-modalities",
    "href": "p3-ch14-vep-fm.html#combining-evidence-across-modalities",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "14.4 Combining Evidence Across Modalities",
    "text": "14.4 Combining Evidence Across Modalities\nNo single model addresses all variant types and mechanisms. Missense variants in protein-coding regions call for protein-level predictors; splicing variants require splice-specific models; regulatory variants benefit from long-context DNA models. Practical VEP workflows combine multiple predictors to achieve comprehensive coverage.\n\n14.4.1 Integration Strategies\nThe simplest integration approach applies different models to different variant classes. Missense variants receive AlphaMissense scores; synonymous and intronic variants near splice sites receive SpliceAI scores; promoter and enhancer variants receive Enformer or AlphaGenome predictions. This modular strategy ensures that each variant type receives predictions from an appropriate model.\nMore sophisticated integration aggregates scores across models for the same variant. A missense variant might receive both AlphaMissense (protein impact) and Enformer (regulatory impact, relevant if the codon overlaps a regulatory element) predictions. Combining these requires decisions about weighting and potential double-counting of shared information.\nBayesian approaches offer principled integration. Priors encode beliefs about variant mechanism proportions; likelihoods incorporate model predictions given mechanism; posteriors combine evidence across models while respecting uncertainty. REVEL demonstrated this approach for classical predictors; extending it to foundation model outputs requires careful calibration of each component score.\n\n\n14.4.2 Avoiding Double-Counting\nFoundation models trained on overlapping data risk capturing correlated rather than independent information. AlphaMissense and ESM-1v both encode evolutionary constraint; combining their scores as independent evidence overweights evolutionary signal. Similarly, conservation-based DNA models like GPN-MSA share information with phyloP scores already incorporated in classical predictors.\nCorrelation analysis helps quantify redundancy. If two model scores correlate above 0.8 across a benchmark dataset, they likely provide similar information and should not be counted as independent evidence. Residual analysis can identify what unique signal each model contributes beyond shared components.\nFor ACMG classification, guidelines specifically address computational evidence weighting. The PP3 (computational evidence supporting pathogenicity) and BP4 (computational evidence supporting benign) criteria apply when multiple tools agree. Using five correlated predictors that all derive from evolutionary conservation should not count as five independent pieces of evidence. Clinical laboratories develop local policies for which tools to consult and how to weight their outputs, ideally based on validation against known variants in their patient population.\n\n\n14.4.3 Practical Workflow Design\nAn effective VEP workflow balances comprehensiveness against efficiency. Genome-wide screening might use fast, zero-shot models (DNA language model likelihood scores) to identify variants deviating from expected sequence patterns. Prioritized variants then receive detailed evaluation with computationally expensive models (AlphaGenome multi-omic predictions). Final interpretation combines computational scores with population frequency, gene-level constraint metrics, segregation data, and clinical phenotype.\nThe ordering matters for efficiency. Filtering the majority of variants with fast models before applying expensive models reduces computational cost by orders of magnitude. The choice of filtering threshold trades sensitivity against specificity: strict thresholds miss true pathogenic variants; lenient thresholds burden downstream analysis with false positives. Threshold selection should match intended use: diagnostic applications prioritize sensitivity while research screening may prioritize specificity.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html#calibration-and-clinical-categories",
    "href": "p3-ch14-vep-fm.html#calibration-and-clinical-categories",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "14.5 Calibration and Clinical Categories",
    "text": "14.5 Calibration and Clinical Categories\nModel scores become clinically useful only when they map to actionable categories. A score of 0.73 means nothing without context; knowing that 73% of variants with similar scores are pathogenic enables interpretation. Calibration ensures this correspondence between scores and outcomes.\n\n14.5.1 Assessing Calibration\nCalibration plots (reliability diagrams) visualize the relationship between predicted probabilities and observed frequencies. Variants are binned by predicted score; the proportion of pathogenic variants in each bin is plotted against the bin’s mean predicted probability. Perfect calibration falls on the diagonal: predicted 0.8 pathogenicity corresponds to 80% observed pathogenic rate.\nMost raw model outputs are poorly calibrated. Neural networks trained with cross-entropy loss tend toward overconfidence, predicting probabilities near 0 or 1 more often than warranted. Protein language model likelihood ratios produce unbounded scores requiring transformation before probability interpretation. Calibration procedures address these systematic biases.\nExpected calibration error (ECE) quantifies miscalibration as the weighted average absolute difference between predicted and observed frequencies across bins. Lower ECE indicates better calibration. Comparing ECE across models identifies which provide more reliable probability estimates independent of discrimination performance (AUROC).\n\n\n14.5.2 Calibration Methods\nTemperature scaling applies a learned scalar divisor to logits before softmax, effectively “softening” overconfident predictions (guo_calibration_2017?). The temperature parameter is optimized on a held-out calibration set to minimize negative log-likelihood. This simple approach often substantially improves ECE with no change to discrimination.\nIsotonic regression learns a monotonic mapping from raw scores to calibrated probabilities. The method fits a step function that preserves ranking while adjusting probability estimates to match observed frequencies. Isotonic regression handles more complex miscalibration patterns than temperature scaling but requires larger calibration sets to avoid overfitting.\nPlatt scaling fits a logistic regression from raw scores to binary outcomes, learning slope and intercept parameters. This produces well-calibrated probabilities when the relationship between scores and log-odds is approximately linear. For foundation model outputs with more complex score distributions, isotonic regression typically outperforms Platt scaling.\nCalibration should use data representative of deployment conditions. Calibrating on ClinVar expert-reviewed variants produces good performance on similar variants but may not transfer to novel genes, rare populations, or variant classes underrepresented in ClinVar. Stratified calibration by gene function, variant class, or population improves reliability at the cost of increased data requirements.\n\n\n14.5.3 Mapping to ACMG Categories\nThe ACMG-AMP variant classification framework defines five categories: pathogenic, likely pathogenic, uncertain significance, likely benign, and benign (richards_standards_2015?). Computational evidence contributes to classification through specific criteria: PP3 (computational evidence for pathogenicity) and BP4 (computational evidence for benignity).\nMapping continuous foundation model scores to these discrete criteria requires threshold selection. Conservative thresholds ensure high precision at the cost of low recall: only variants with very high (or very low) scores receive computational evidence designation. Lenient thresholds increase recall but admit more false positives, potentially inflating pathogenicity classifications.\nClinGen sequence variant interpretation working groups have developed model-specific recommendations for several classical predictors. Similar guidance for foundation models remains under development. In the interim, laboratories should validate thresholds against local truth sets and document threshold choices in variant reports.\nThe uncertain significance category deserves special attention. Variants with intermediate foundation model scores genuinely reflect uncertainty: the models cannot confidently distinguish pathogenic from benign. Forcing these variants into discrete categories by applying arbitrary cutoffs misrepresents the actual evidence. Reporting calibrated probabilities alongside discrete classifications preserves information for downstream decision-making.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html#uncertainty-quantification",
    "href": "p3-ch14-vep-fm.html#uncertainty-quantification",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "14.6 Uncertainty Quantification",
    "text": "14.6 Uncertainty Quantification\nCalibration addresses systematic bias in probability estimates; uncertainty quantification addresses the confidence of individual predictions. A well-calibrated model might correctly estimate that 70% of variants in some category are pathogenic, but for any individual variant, we want to know whether the model’s prediction is reliable or whether the variant falls outside the model’s competence.\n\n14.6.1 Sources of Uncertainty\nEpistemic uncertainty reflects gaps in the model’s knowledge: regions of input space with sparse training data, variant types rarely observed during training, or proteins from understudied families. This uncertainty is reducible in principle by collecting more data and can be estimated by measuring model disagreement across training variations.\nAleatoric uncertainty reflects inherent noise in the prediction target: variants whose pathogenicity genuinely varies across individuals or contexts, or cases where the same score corresponds to both pathogenic and benign variants for biological rather than modeling reasons. This uncertainty is irreducible by additional training and represents fundamental limits on predictability.\nDistinguishing these uncertainty types matters for interpretation. High epistemic uncertainty suggests caution: the model has not seen similar variants and may be extrapolating unreliably. High aleatoric uncertainty suggests that the variant’s effect genuinely depends on factors not captured by sequence alone.\n\n\n14.6.2 Uncertainty Estimation Methods\nEnsemble methods train multiple models on different data subsets or with different random initializations. Prediction variance across ensemble members estimates epistemic uncertainty. Large disagreement indicates that the prediction depends strongly on training specifics rather than robust learned patterns. Deep ensembles provide well-calibrated uncertainty estimates but multiply computational cost linearly with ensemble size.\nMonte Carlo dropout approximates Bayesian inference by applying dropout at test time and averaging predictions across multiple stochastic forward passes. Variance across passes estimates uncertainty without training multiple models. This approach adds modest computational overhead and can be applied to any dropout-containing architecture.\nConformal prediction provides distribution-free uncertainty quantification with coverage guarantees (angelopoulos_conformal_2023?). Given a calibration set, conformal methods construct prediction sets guaranteed to contain the true label with specified probability (e.g., 90%). For variant classification, this might produce sets like {pathogenic, uncertain} or {benign} depending on the variant and desired coverage. Larger prediction sets indicate greater uncertainty; single-element sets indicate confident predictions.\n\n\n14.6.3 Out-of-Distribution Detection\nBeyond quantifying uncertainty for in-distribution predictions, responsible deployment requires detecting when inputs fall outside the model’s training distribution. A protein language model trained on natural proteins may produce confident but unreliable predictions for synthetic sequences or fragments. A regulatory model trained on common cell types may fail on rare developmental stages.\nLikelihood-based detection uses the model’s own representations to identify unfamiliar inputs. Sequences with low embedding density or anomalous attention patterns may fall outside the training distribution regardless of predicted scores. Flagging these inputs for manual review prevents automated classification of cases the model cannot reliably assess.\nDistance-based methods compare new inputs to training examples in representation space. Variants far from any training example in embedding space warrant skepticism even if the model produces confident predictions. Maintaining summary statistics of training representations enables efficient distance computation at deployment.\nChapter Chapter 23 develops uncertainty quantification methods in detail, including practical implementation guidance and evaluation metrics. For VEP applications, the key insight is that uncertainty estimates complement point predictions: high-confidence predictions can inform clinical decisions; low-confidence predictions should prompt additional evidence gathering rather than blind acceptance of model outputs.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html#what-foundation-models-add",
    "href": "p3-ch14-vep-fm.html#what-foundation-models-add",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "14.7 What Foundation Models Add",
    "text": "14.7 What Foundation Models Add\nHaving surveyed current foundation model approaches, we can now directly address what they contribute beyond classical methods (Chapter 4). The answer is nuanced: substantial improvements in some domains, modest gains in others, and persistent blind spots that new architectures have not yet resolved.\n\n14.7.1 Improved Discrimination\nOn standard benchmarks, foundation model VEP methods consistently outperform classical predictors. AlphaMissense achieves AUROC of 0.91 on held-out ClinVar missense variants compared to 0.85 for CADD. SpliceAI detects pathogenic splicing variants with sensitivity of 0.90 compared to 0.60 for MaxEntScan. GPN-MSA scores correlate more strongly with deep mutational scanning measurements than phyloP or GERP.\nThese improvements reflect richer representations. Classical methods aggregate independent features (conservation, amino acid properties, domain annotations); foundation models learn nonlinear interactions among positions and capture patterns too subtle for manual feature engineering. The gap is largest for variants where context matters: buried core missense variants where structural environment determines impact, splice variants where cryptic site activation depends on flanking sequence, regulatory variants where motif disruption interacts with chromatin context.\n\n\n14.7.2 Extended Coverage\nClassical methods often fail silently on understudied genes, rare variant classes, or poorly annotated regions. SIFT and PolyPhen require protein alignments; variants in singleton genes without homologs receive no prediction. CADD depends on annotation features; variants in regions lacking regulatory marks receive uninformative scores.\nFoundation models degrade more gracefully. Protein language models score any amino acid sequence regardless of available homologs. DNA language models score any genomic position regardless of existing annotation. This extended coverage matters for clinical sequencing of rare diseases, where pathogenic variants often reside in less-studied genes precisely because their severe effects are incompatible with population frequency.\n\n\n14.7.3 Mechanistic Interpretability\nAlphaGenome and similar multi-output models provide predictions about mechanism rather than bare pathogenicity scores. A variant flagged as deleterious might also show predicted effects on chromatin accessibility, contact frequency, and downstream gene expression. These mechanistic predictions enable hypothesis generation and targeted experimental validation (Chapter 24).\nClassical methods offer limited mechanistic insight. CADD provides a single score without indicating whether it derives from conservation, protein impact, regulatory disruption, or other features. Decomposing the score into component contributions requires separate analysis. Foundation models that predict molecular phenotypes naturally provide this decomposition.\n\n\n14.7.4 Persistent Limitations\nFoundation models have not solved several fundamental challenges. Ancestry bias persists because training data remain skewed toward European populations; performance degrades for variants common in African or Asian populations but rare in training sets (Chapter 22). Calibration requires substantial labeled data that inherit existing biases. Rare variant classes (structural variants, complex indels, repeat expansions) lack sufficient training examples for reliable prediction.\nThe comparison to classical methods reveals diminishing returns on certain axes. For well-conserved active site variants in thoroughly studied proteins, PolyPhen-2 already achieves near-optimal performance; AlphaMissense improves marginally. The largest foundation model gains appear for difficult cases where classical features are uninformative or misleading.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html#clinical-integration-considerations",
    "href": "p3-ch14-vep-fm.html#clinical-integration-considerations",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "14.8 Clinical Integration Considerations",
    "text": "14.8 Clinical Integration Considerations\nFoundation model VEP tools require thoughtful integration into clinical workflows. Their impressive benchmark performance does not automatically translate to improved patient outcomes without attention to deployment context, validation requirements, and human factors.\n\n14.8.1 Laboratory Validation\nBefore clinical use, laboratories should validate foundation model tools against local truth sets representing their patient population. Published benchmark performance on ClinVar may not generalize to a laboratory’s specific case mix. Validation should assess discrimination (can the tool distinguish pathogenic from benign?), calibration (do probability estimates match observed frequencies?), and utility (does incorporating the tool improve variant classification compared to existing workflows?).\nValidation requires variants with known pathogenicity independent of the computational predictions being tested. Using ClinVar variants whose classifications already incorporated CADD scores to validate CADD creates circular reasoning. Gold-standard variants from functional studies, segregation data, or expert review provide cleaner validation targets.\n\n\n14.8.2 Workflow Integration\nFoundation model predictions represent one evidence type among many. ACMG guidelines specify how computational evidence combines with population frequency, functional data, segregation, and clinical phenotype. Computational evidence alone rarely suffices for pathogenic or benign classification; it supports or weakens classifications established by other evidence types.\nLaboratory information systems require modification to display and store foundation model outputs alongside existing annotations. Analyst training ensures appropriate interpretation: understanding that high scores indicate deleteriousness without establishing causation, recognizing when scores fall outside validated ranges, and knowing when to request additional evidence for uncertain cases.\n\n\n14.8.3 Communication to Clinicians\nVariant reports communicated to ordering clinicians should present foundation model evidence appropriately. Reporting raw scores without context confuses non-specialist clinicians. Reporting discrete classifications without uncertainty may convey false confidence. Effective reporting might state: “Computational tools (AlphaMissense, SpliceAI) concordantly predict this variant is likely to affect protein function, supporting the PP3 criterion for pathogenicity classification.”\nWhen foundation model predictions conflict with other evidence, reports should acknowledge the discrepancy rather than suppressing inconvenient results. A variant segregating with disease in a family but receiving a benign computational prediction warrants explicit discussion, not quiet exclusion of the computational evidence.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html#open-challenges",
    "href": "p3-ch14-vep-fm.html#open-challenges",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "14.9 Open Challenges",
    "text": "14.9 Open Challenges\nCurrent foundation model approaches leave substantial problems unsolved. These open challenges define directions for future research and areas where clinical caution remains warranted.\n\n14.9.1 Complex Variant Types\nMost current models address single nucleotide variants and small indels. Structural variants (deletions, duplications, inversions spanning kilobases to megabases) remain largely outside foundation model capabilities. Copy number variation, repeat expansions, and complex rearrangements alter genome architecture in ways current sequence models cannot represent. Extending foundation model paradigms to these variant classes requires architectural innovations beyond current approaches.\n\n\n14.9.2 Combinatorial Effects\nGenomes contain multiple variants that may interact. Compound heterozygosity (two variants affecting both copies of a gene) creates pathogenic states from individually tolerable variants. Modifier variants in other genes modulate penetrance. Haplotype effects mean variants on the same chromosome have different consequences than variants on opposite chromosomes. Current models score variants independently, ignoring these interactions that determine clinical presentation.\n\n\n14.9.3 Phenotype Specificity\nA variant pathogenic for one phenotype may be benign for another. SCN5A variants cause distinct cardiac arrhythmia syndromes depending on their specific functional effects. Foundation models trained on pathogenic/benign labels average across phenotypes, potentially obscuring clinically relevant specificity. Phenotype-specific training requires much larger datasets than currently available.\n\n\n14.9.4 Temporal and Environmental Context\nVariant effects often depend on age, environmental exposures, or physiological state. A variant pathogenic under metabolic stress may be tolerable at baseline. Foundation models capture sequence context but not the dynamic biological context determining phenotypic expression. Integrating longitudinal clinical data with sequence-level predictions remains an unsolved challenge.\n\n\n14.9.5 Equity and Access\nState-of-the-art foundation models require substantial computational resources for training and sometimes for inference. Laboratories in resource-limited settings may lack access to cutting-edge tools. Precomputed scores (like AlphaMissense’s proteome-wide release) partially address this, but equity concerns extend beyond compute access to representation in training data and validation cohorts. Ensuring that foundation model VEP benefits all populations requires deliberate effort beyond technical development.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p3-ch14-vep-fm.html#summary",
    "href": "p3-ch14-vep-fm.html#summary",
    "title": "14  Variant Effect Prediction with Foundation Models",
    "section": "14.10 Summary",
    "text": "14.10 Summary\nFoundation models have transformed variant effect prediction from feature engineering to representation learning. Protein language models capture evolutionary constraint at unprecedented resolution. DNA language models and regulatory models extend coverage to noncoding variants across the genome. Multi-omic systems like AlphaGenome provide mechanistic predictions enabling hypothesis generation beyond bare deleteriousness scores.\nYet these advances come with caveats. Calibration requires careful attention; benchmark performance does not automatically yield clinical utility. Uncertainty quantification remains immature; models often fail to recognize when inputs fall outside their competence. Population bias persists; improvements over classical methods are smallest for underrepresented groups. Complex variant types and combinatorial effects lie beyond current capabilities.\nClinical deployment demands humility alongside enthusiasm. Foundation model VEP tools are powerful aids to human interpretation, not autonomous classifiers. Their predictions inform rather than determine variant classification, complementing population data, functional evidence, and clinical judgment. Used appropriately, they accelerate diagnosis and reduce missed findings. Used inappropriately, they create false confidence and perpetuate existing inequities.\nThe chapters that follow examine how variant effect predictions integrate into clinical workflows for rare disease diagnosis (Chapter 26) and risk prediction (?sec-clinical-risk), the broader challenges of uncertainty quantification that apply across genomic applications (Chapter 23), and the interpretability methods that help us understand what foundation models have learned (Chapter 24). Variant effect prediction sits at the center of genomic medicine; foundation models have raised its ceiling while leaving substantial work to achieve its potential.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nFrazer, Jonathan, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin Gal, and Debora S. Marks. 2021. “[EVE] Disease Variant Prediction with Deep Generative Models of Evolutionary Data.” Nature 599 (7883): 91–95. https://doi.org/10.1038/s41586-021-04043-8.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nOrenbuch, Rose, Courtney A. Shearer, Aaron W. Kollasch, Aviv D. Spinner, Thomas Hopf, Lood van Niekerk, Dinko Franceschi, Mafalda Dias, Jonathan Frazer, and Debora S. Marks. 2025. “[popEVE] Proteome-Wide Model for Human Disease Genetics.” Nature Genetics, November, 1–10. https://doi.org/10.1038/s41588-025-02400-1.",
    "crumbs": [
      "Part III: Deep Learning Architectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction with Foundation Models</span>"
    ]
  },
  {
    "objectID": "p4--multi-scale.html",
    "href": "p4--multi-scale.html",
    "title": "Part IV: Multi-Scale Modeling",
    "section": "",
    "text": "The preceding parts traced genomic deep learning from its origins in sequence-to-function CNNs through transformer-based language models and into the foundation model paradigm. Yet biology operates across scales that sequence alone cannot capture. Cells of different types read the same genome differently. Genes function not in isolation but within networks of regulation and interaction. The three-dimensional folding of chromatin brings distal elements into contact, creating regulatory logic invisible to one-dimensional sequence models. This part examines how foundation model principles extend beyond sequence to embrace these multi-scale aspects of genome biology.\nThe transition from sequence-centric to multi-scale modeling reflects a deeper shift in how we conceptualize genomic computation. Early deep learning models asked what a sequence encodes; the models in this part ask what that sequence becomes in particular cellular contexts, interaction networks, and organizational architectures. This shift demands new data modalities, from single-cell transcriptomes that reveal cellular heterogeneity to Hi-C contact maps that expose spatial genome organization, and new computational approaches that can learn meaningful representations across these modalities.\nThree chapters develop this theme. Chapter 16  Single-Cell and Epigenomic Models examines foundation models for single-cell transcriptomics, epigenomics, and three-dimensional genome structure, showing how transformer architectures adapt to the unique characteristics of these data types. Chapter Revised Chapter: Networks and Graph-Based Reasoning turns to graph neural networks and network-based approaches that represent genes, proteins, and their interactions as structured graphs rather than flat sequences. Chapter 19  Multi-Omics Integration broadens the view further to multi-omics integration and systems-level reasoning, exploring how models can jointly represent genomic, transcriptomic, proteomic, and clinical information.\nTogether, these chapters address a central challenge: connecting sequence variation to phenotype requires traversing multiple layers of biological organization, from DNA through RNA and protein to cellular state and tissue function. The models surveyed here represent early attempts to bridge these layers computationally, learning representations that capture not just what the genome says but what it does in living systems.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html",
    "href": "p4-ch15-rna.html",
    "title": "15  RNA Models",
    "section": "",
    "text": "15.1 RNA as Molecule Versus Transcriptome Readout\nProteins have structure prediction solved to atomic resolution. DNA language models train on billions of base pairs and learn regulatory grammar without supervision. RNA occupies a curious middle ground: essential to every step from transcription to translation, yet lacking the computational attention lavished on its neighbors in the central dogma. The disparity is not merely historical accident. Protein sequences accumulate over billions of years of evolution, providing the massive training corpora that enabled ESM to learn structure from sequence alone. DNA benefits from reference genomes, population-scale sequencing, and functional genomics consortia that have generated petabytes of chromatin and expression data. RNA databases remain comparatively sparse, structural annotations cover only well-characterized families, and the field lacks the equivalent of AlphaFold’s crystallographic training set. The result is a modeling landscape where RNA foundation models exist but remain immature relative to their protein and DNA counterparts, where secondary structure prediction has advanced but tertiary modeling lags far behind, and where the most successful applications target specific RNA classes rather than offering general-purpose representations.\nThis gap matters because RNA performs functions that neither DNA nor protein can capture. The same genomic sequence produces different transcripts through alternative splicing, different protein levels through translation regulation, and different cellular fates through RNA localization and decay. Synonymous mutations that leave protein sequence unchanged can dramatically alter expression by changing codon optimality or mRNA structure. The epitranscriptome of chemical modifications (m6A, pseudouridine, and dozens of others) adds another layer of regulation invisible to DNA-centric models. Understanding these phenomena requires models that take RNA sequence as input and reason about RNA-specific constraints.\nThis chapter opens Part IV by establishing RNA as a bridge between the sequence-level foundation models of Part III and the cellular and systems contexts that follow. The foundation models examined in previous chapters (DNA language models, protein language models, long-context regulatory models) all ultimately manifest their predictions through RNA intermediates. Enformer predicts RNA-seq coverage. Protein models predict the products of translation. SpliceAI models the recognition of RNA by the spliceosome. RNA-specific models add a distinct layer: they treat RNA not merely as a readout of DNA or a precursor to protein, but as a structured molecule with its own sequence constraints, folding landscapes, and functional roles. The chapter examines secondary structure prediction, RNA foundation models, codon-level mRNA models, and noncoding RNA classification before confronting the limitations that prevent RNA modeling from matching the maturity of protein or DNA approaches.\nTwo complementary perspectives frame computational approaches to RNA. The molecular view treats RNA as a physical object with primary sequence, secondary structure through base pairing, tertiary organization in three-dimensional space, and chemical modifications that alter its properties. In this view, modeling goals include predicting which bases pair with which, how the molecule folds, which proteins bind to it, and how synthetic RNAs might be designed with desired properties. The transcriptomic view treats RNA as a cellular readout: coverage profiles along the genome, splice junction usage, isoform abundances, expression levels that vary across cell types and conditions. Here the goal is explaining how genomic sequence and chromatin state give rise to these measurements.\nModels that predict transcriptomic signals from DNA sequence (Enformer, Borzoi, and related architectures covered in Chapter 13) operate in the second paradigm. They take genomic sequence as input and output RNA-seq or CAGE profiles as predictions. These models never see RNA sequence directly; they learn the mapping from DNA context to transcriptional output. This chapter focuses instead on the molecular perspective: models whose input is RNA sequence itself and whose outputs concern RNA structure, function, or design.\nThe distinction parallels the difference between protein language models and proteomics prediction models. ESM takes amino acid sequences and learns structural representations (Chapter 12). A model predicting protein abundance from genomic features would be solving a different problem. Both perspectives are valuable, and both ultimately concern RNA, but they operate at different levels of the biological hierarchy and require different architectures and training strategies.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#why-secondary-structure-creates-a-distinct-modeling-challenge",
    "href": "p4-ch15-rna.html#why-secondary-structure-creates-a-distinct-modeling-challenge",
    "title": "15  RNA Models",
    "section": "15.2 Why Secondary Structure Creates a Distinct Modeling Challenge",
    "text": "15.2 Why Secondary Structure Creates a Distinct Modeling Challenge\n\n15.2.1 The Flat Energy Landscape Problem\nRNA’s defining computational challenge emerges from thermodynamics. Proteins fold into stable three-dimensional structures because their energy landscapes contain deep minima: the native state sits in a pronounced funnel that guides the folding process. RNA energy landscapes are remarkably flatter. Multiple conformations compete for occupancy, with free energy differences often smaller than thermal fluctuations at cellular temperatures. A given RNA sequence may adopt several alternative structures with similar stabilities, and the dominant conformation can shift in response to ion concentrations, temperature, protein binding, or chemical modifications.\nThis conformational plasticity has biological functions (riboswitches that change structure in response to ligand binding, RNA thermometers that regulate translation at different temperatures) but creates modeling difficulties. Minimum free energy predictions, which identify the single lowest-energy structure, may miss functionally relevant alternative conformations. Partition function calculations that consider the full ensemble are more complete but computationally expensive and harder to interpret. Deep learning models that predict structure from sequence must somehow capture this many-to-many relationship between sequence and conformation, a challenge that protein structure prediction largely avoided because the sequence-to-structure mapping for most proteins is effectively one-to-one.\n\n\n15.2.2 Base Pairing and Long-Range Dependencies\nSecondary structure arises from Watson-Crick base pairing (A-U, G-C) and wobble pairs (G-U) that create stems, loops, bulges, and internal loops. Unlike protein secondary structure, where alpha helices and beta sheets are local motifs determined by nearby residues, RNA secondary structure involves long-range contacts. A base at position \\(i\\) may pair with a base at position \\(j\\) hundreds of nucleotides away. The intervening sequence must accommodate this pairing without introducing steric clashes or thermodynamically unfavorable arrangements.\nThis long-range dependency structure differs fundamentally from protein contact prediction, where most important contacts occur between residues close in primary sequence. RNA structure prediction must consider all possible pairings across the entire sequence, evaluate their compatibility, and identify the globally optimal (or near-optimal) arrangement. The number of possible secondary structures grows exponentially with sequence length, making exhaustive enumeration intractable for long RNAs.\n\n\n15.2.3 Pseudoknots and Tertiary Complexity\nPseudoknots occur when bases in a loop pair with bases outside that loop, creating interleaved base-pairing patterns that violate the nested structure assumed by standard secondary structure algorithms. A typical pseudoknot involves two stem regions whose base pairs cross each other when drawn in standard notation. These structures are functionally important (the telomerase RNA catalytic core contains a pseudoknot essential for activity) but algorithmically challenging. Standard dynamic programming approaches for secondary structure prediction exclude pseudoknots because their inclusion increases computational complexity from \\(O(n^3)\\) to \\(O(n^6)\\) or worse.\nTertiary structure involves the three-dimensional arrangement of secondary structure elements in space, including long-range interactions mediated by non-Watson-Crick base pairs, metal ion coordination, and RNA-RNA kissing loops. Predicting RNA tertiary structure remains far less developed than protein tertiary structure prediction. No RNA equivalent of AlphaFold exists, and the training data situation is dire: the Protein Data Bank contains over 200,000 protein structures but fewer than 2,000 RNA structures, many of which are ribosomal RNA fragments or tRNA variants from the same structural families.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#classical-approaches-to-structure-prediction",
    "href": "p4-ch15-rna.html#classical-approaches-to-structure-prediction",
    "title": "15  RNA Models",
    "section": "15.3 Classical Approaches to Structure Prediction",
    "text": "15.3 Classical Approaches to Structure Prediction\n\n15.3.1 Thermodynamic Folding Models\nThe dominant classical paradigm for RNA secondary structure prediction relies on nearest-neighbor thermodynamic models. These approaches assign free energy contributions to each base pair and structural element (loops, bulges, internal loops, multiloops) based on experimentally calibrated parameters. Given these parameters, dynamic programming algorithms identify the minimum free energy structure or compute the partition function over all possible structures.\nMfold and the ViennaRNA package represent the most widely used implementations. They achieve reasonable accuracy for short, well-behaved RNAs where the thermodynamic parameters are most reliable. Limitations emerge for longer RNAs where the flat energy landscape means many structures have similar energies, for RNAs in complex cellular environments where proteins and other factors alter folding, and for RNAs with modifications or non-canonical interactions not captured by standard parameter sets. These methods also assume equilibrium conditions that may not hold for co-transcriptional folding or kinetically trapped states.\n\n\n15.3.2 Comparative and Covariation Methods\nFor RNAs with sufficient homologous sequences, comparative approaches provide an orthogonal route to structure inference. If two positions exhibit compensatory mutations (G-C changing to A-U while maintaining complementarity), those positions likely base-pair. Databases like Rfam curate consensus secondary structures for RNA families based on these covariation signals.\nComparative methods are powerful but require multiple sequence alignments of homologous RNAs. Novel RNAs, rapidly evolving regulatory elements, or species-specific transcripts may lack sufficient homologs for reliable inference. The approach also assumes that structure is conserved across the aligned sequences, which breaks down for RNAs that have diverged in function or that adopt condition-specific alternative structures.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#deep-learning-for-secondary-structure-prediction",
    "href": "p4-ch15-rna.html#deep-learning-for-secondary-structure-prediction",
    "title": "15  RNA Models",
    "section": "15.4 Deep Learning for Secondary Structure Prediction",
    "text": "15.4 Deep Learning for Secondary Structure Prediction\n\n15.4.1 From Thermodynamics to Learned Patterns\nDeep learning models for RNA structure prediction frame the task as sequence-to-structure mapping, analogous to protein contact prediction. Given an RNA sequence, the model predicts base-pairing probabilities for all position pairs, contact maps indicating which bases interact, or per-nucleotide structural states (paired, unpaired, in loop, in stem).\nModels like SPOT-RNA use convolutional or attention-based architectures to capture long-range dependencies in sequence. Some approaches directly predict pairing matrices as dense outputs; others output per-position classifications that are post-processed into structures. Training typically uses experimentally determined structures from databases like RNAstralign or bpRNA, supplemented by computationally predicted structures from thermodynamic models.\nPerformance on benchmark datasets often exceeds classical thermodynamic methods, particularly for RNAs with complex structures or pseudoknots where dynamic programming approaches struggle. The learned models can capture patterns beyond nearest-neighbor rules, potentially encoding longer-range sequence dependencies that contribute to folding but were not parameterized in classical approaches.\n\n\n15.4.2 Structure Probing as Supervision\nHigh-throughput structure probing experiments provide an alternative source of supervision. SHAPE (selective 2’-hydroxyl acylation analyzed by primer extension), DMS-seq, and icSHAPE measure nucleotide accessibility or flexibility across entire transcriptomes. Positions that are base-paired or buried in tertiary structure show lower reactivity than exposed positions.\nThese data offer several advantages for model training. They cover far more RNAs than crystal structures, extending beyond well-characterized families to regulatory elements and novel transcripts. They capture structure in cellular context, reflecting the influence of proteins, modifications, and physiological conditions. And they provide soft constraints rather than binary pairing assignments, potentially better matching the conformational heterogeneity of real RNA populations.\nModels trained on structure probing data learn to predict accessibility profiles from sequence. These predictions can be integrated with thermodynamic models (using predicted accessibility as constraints) or used directly for downstream tasks like predicting RNA-protein binding or designing stable constructs.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#rna-foundation-models",
    "href": "p4-ch15-rna.html#rna-foundation-models",
    "title": "15  RNA Models",
    "section": "15.5 RNA Foundation Models",
    "text": "15.5 RNA Foundation Models\n\n15.5.1 The Scale Gap with Protein Language Models\nRNA foundation models attempt to replicate the protein language model paradigm: train large transformers on massive sequence corpora using self-supervised objectives, then transfer learned representations to downstream tasks. The approach has produced working models, but the results lag substantially behind protein LMs in both scale and demonstrated capabilities.\nThe comparison with ESM illustrates the gap. ESM-2 trained on over 65 million protein sequences from UniRef, spanning the known diversity of protein families. RNA-FM, one of the more successful RNA foundation models, trained on approximately 23 million noncoding RNA sequences from RNAcentral (Chen et al. (2022)). While not a trivial corpus, this represents an order of magnitude fewer sequences, and the RNA sequences span a narrower range of structural and functional diversity than proteins. The consequences appear in downstream performance: RNA-FM improves over baselines on secondary structure prediction and family classification, but shows nothing like the emergent structure prediction that made ESM-2’s attention patterns predict contact maps without supervision.\nSeveral factors explain the disparity. Protein sequences have accumulated over 4 billion years of evolution across all domains of life, with each functional protein family represented by thousands of homologs. RNA databases are biased toward well-characterized structural families (tRNAs, rRNAs, ribozymes) with sparser coverage of regulatory ncRNAs and lineage-specific transcripts. The epitranscriptomic modifications that alter RNA function are invisible in sequence databases, unlike protein post-translational modifications that at least occur at predictable sequence motifs.\n\n\n15.5.2 Architectures and Objectives\nMost RNA foundation models follow the masked language modeling (MLM) paradigm established by BERT. RNA-FM uses a transformer encoder with nucleotide-level tokenization, predicting masked bases from surrounding context. The learned embeddings show some correspondence to secondary structure when probed with downstream tasks, though the correspondence is weaker than the structure-function relationship learned by protein LMs.\nAlternative architectures explore different design choices. Some models incorporate explicit structure tokens or operate on sequence-structure graphs, learning joint representations over both modalities. Others use codon-level tokenization for coding RNAs (discussed in the next section) or explore state-space models and other efficient attention variants to handle longer sequences. RNAErnie and related models experiment with multi-task objectives that combine MLM with auxiliary predictions for structural features or family classification.\nThe field remains in active development, with no clear consensus on optimal architecture, tokenization, or training strategy. Unlike protein modeling, where ESM established a dominant paradigm that subsequent work has refined, RNA modeling still explores fundamental design choices.\n\n\n15.5.3 Downstream Applications\nRNA foundation model embeddings support various downstream tasks. Secondary structure prediction fine-tunes the model to output pairing probabilities or SHAPE reactivity profiles. RNA-protein binding prediction uses CLIP-seq data to predict interactions with RNA-binding proteins. Family classification assigns sequences to Rfam families or functional categories (tRNA, rRNA, miRNA, lncRNA). Expression and stability tasks predict transcript half-life or steady-state levels from UTR sequences.\nPerformance varies substantially across tasks. For structurally constrained RNAs like tRNAs and rRNAs, where sequence motifs strongly determine structure and function, foundation model embeddings provide useful features. For regulatory lncRNAs that often lack stable secondary structures and conserved motifs, improvement over baseline methods is more modest. The diversity of RNA types and tasks complicates benchmarking, and models that excel on one task may struggle on others.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#codon-level-models-for-coding-rna",
    "href": "p4-ch15-rna.html#codon-level-models-for-coding-rna",
    "title": "15  RNA Models",
    "section": "15.6 Codon-Level Models for Coding RNA",
    "text": "15.6 Codon-Level Models for Coding RNA\n\n15.6.1 Beyond Nucleotide Tokenization\nCoding sequences occupy a special niche where protein and nucleic acid constraints intersect. The genetic code assigns 61 sense codons to 20 amino acids, creating synonymous redundancy where multiple codons encode the same amino acid. This redundancy is not functionally neutral: synonymous codons differ in tRNA availability, translation speed, co-translational folding effects, and mRNA stability. Protein language models, which operate on amino acid sequences, cannot capture these codon-level signals.\nCodon-level foundation models address this gap by tokenizing mRNA into codons rather than nucleotides. Models like cdsFM, EnCodon, and DeCodon treat each three-nucleotide codon as a single token, training on masked codon prediction and related objectives (Naghipourfar et al. (2024)). This tokenization encodes a biological prior: codons are the fundamental units of translation, and mutations at the codon level determine amino acid changes while mutations within synonymous codons affect expression without changing protein sequence.\nThe codon vocabulary contains 61 tokens (excluding stop codons) plus special tokens for noncoding regions and boundaries. This intermediate vocabulary size (between character-level nucleotide tokenization and typical BPE vocabularies of thousands of tokens) balances resolution with context length. A 300-amino-acid protein corresponds to 900 nucleotides or 300 codons, making whole-gene modeling tractable within standard transformer context windows.\n\n\n15.6.2 What Codon Models Add\nCompared to protein language models, codon-level models enable direct modeling of mRNA design problems where amino acid sequence is fixed but codon choice is variable. They capture codon usage bias and its relationship to expression, model translation elongation dynamics that affect co-translational folding, and distinguish synonymous variants that are neutral at the protein level but affect mRNA properties.\nLife-Code extends this approach into a central-dogma-wide framework, linking DNA, RNA, and protein representations through shared or aligned embedding spaces (Liu et al. (2025)). CodonBERT specifically targets mRNA design for vaccines and therapeutics, training on over 10 million mRNA sequences to learn representations that predict expression, stability, and immunogenicity (Li et al. (2023)).\nHowever, codon models typically ignore mRNA secondary structure and modifications. Local structure affects ribosome access and translation rate; modifications like m6A influence stability and localization. Combining codon-aware tokenization with structure-aware representations remains an open direction, less mature than the parallel integration of sequence and structure in protein modeling.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#mrna-design-and-optimization",
    "href": "p4-ch15-rna.html#mrna-design-and-optimization",
    "title": "15  RNA Models",
    "section": "15.7 mRNA Design and Optimization",
    "text": "15.7 mRNA Design and Optimization\n\n15.7.1 Design Objectives and Trade-offs\nmRNA sequence design selects nucleotide sequences that encode a desired protein while optimizing expression, stability, safety, and manufacturability. For a 300-amino-acid protein, there are approximately \\(3^{300}\\) possible synonymous mRNA sequences (roughly the number of synonymous codons raised to the protein length). This astronomical space cannot be exhaustively searched, motivating both classical heuristics and modern machine learning approaches.\nKey objectives include high protein expression in target tissues, mRNA stability during manufacturing and in vivo, controlled translation kinetics that influence co-translational folding, and low immunogenicity for therapeutic applications. These objectives often conflict: increasing GC content may improve stability but introduce unwanted secondary structure, while avoiding rare codons may reduce expression if tRNA pools are limiting.\n\n\n15.7.2 Lessons from COVID-19 Vaccines\nThe COVID-19 mRNA vaccines provided a high-profile demonstration of mRNA design principles at unprecedented scale. The Pfizer-BioNTech and Moderna vaccines incorporated several design elements: N1-methylpseudouridine modification throughout the sequence to reduce innate immune activation, codon optimization to enhance expression in human cells, optimized 5’ and 3’ UTRs from highly expressed genes, and sequence modifications to stabilize the prefusion spike conformation. These choices drew on decades of basic research but were refined through empirical optimization rather than systematic model-based design.\nThe vaccines’ success demonstrated that rationally designed mRNAs can achieve therapeutic efficacy at scale. It also revealed limitations in current understanding: the optimal combination of modifications, codons, and UTRs for a given protein target remains partly empirical, and transferring designs across proteins or therapeutic applications requires substantial optimization.\n\n\n15.7.3 Model-Based Design Strategies\nRNA and codon foundation models enable several approaches to systematic design. Scoring and screening use pretrained models to evaluate large candidate sets for predicted expression or stability, selecting top designs for experimental validation. When models are differentiable with respect to input embeddings, gradient-based methods can guide sequence optimization toward desired objectives. Generative approaches sample diverse high-scoring sequences subject to constraints like fixed amino acid sequence or avoided motifs.\nEmpirical results suggest that deep models trained on high-throughput reporter assays or ribosome profiling can outperform classical codon adaptation indices like CAI or tAI, particularly for context-specific expression prediction. Classical indices rely on genome-wide codon frequencies that may not reflect the relevant cellular context, while deep models can learn local effects of codon pairs, mRNA structure, and regulatory elements. However, these models require substantial training data and may not generalize across organisms or synthetic constructs far from natural sequences.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#noncoding-rna-classification-and-function",
    "href": "p4-ch15-rna.html#noncoding-rna-classification-and-function",
    "title": "15  RNA Models",
    "section": "15.8 Noncoding RNA Classification and Function",
    "text": "15.8 Noncoding RNA Classification and Function\n\n15.8.1 The Diversity of Noncoding RNA\nRNA that does not encode protein spans an enormous functional and structural range. Housekeeping RNAs (tRNAs, rRNAs, snRNAs, snoRNAs) perform essential cellular functions with well-characterized structures. Regulatory RNAs (miRNAs, siRNAs, piRNAs, lncRNAs) control gene expression through diverse mechanisms. Structural and catalytic RNAs (ribozymes, riboswitches) adopt complex folds that enable enzymatic activity or ligand sensing. Circular RNAs (circRNAs) and other noncanonical species continue to expand the catalog of RNA diversity.\nEach class has characteristic lengths, structural motifs, genomic contexts, and functional mechanisms. tRNAs are approximately 76 nucleotides with a conserved cloverleaf structure. miRNAs are approximately 22 nucleotides processed from longer hairpin precursors. lncRNAs span thousands of nucleotides with poorly conserved sequence and often no stable secondary structure. Unifying these classes under a single modeling framework is challenging, and models that excel on one class may fail on others.\n\n\n15.8.2 From Handcrafted Features to Learned Representations\nClassical ncRNA classification relied on engineered features: k-mer frequencies, GC content, minimum free energy of predicted secondary structure, structural motif counts, and genomic context features like proximity to coding genes or chromatin marks. These features fed conventional classifiers (SVMs, random forests, shallow neural networks) that achieved reasonable performance for well-studied classes with strong sequence and structure signatures.\nThe limits of handcrafted features emerge most clearly for lncRNAs. These transcripts are defined partly by what they lack (no long open reading frame) rather than what they possess. Many lncRNAs show poor conservation, lack stable secondary structures, and have diverse, poorly characterized functions. Distinguishing functional lncRNAs from transcriptional noise remains difficult, and classical feature sets often collapse to generic statistics like length and GC content.\nFoundation model embeddings offer a more flexible approach. Per-nucleotide representations can be pooled into fixed-dimensional vectors that support classification with simple downstream heads. For ncRNAs without strong sequence motifs, the pretrained embeddings may capture subtle distributional patterns learned during self-supervised training. Few-shot learning becomes possible: given a handful of newly characterized RNAs, their embeddings can seed new clusters in representation space, guiding annotation of related sequences.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#splicing-and-transcript-processing-models",
    "href": "p4-ch15-rna.html#splicing-and-transcript-processing-models",
    "title": "15  RNA Models",
    "section": "15.9 Splicing and Transcript Processing Models",
    "text": "15.9 Splicing and Transcript Processing Models\n\n15.9.1 Beyond SpliceAI\nSpliceAI demonstrated that deep convolutional networks could predict splice sites with near-spliceosomal precision (Chapter 6). The model’s success in identifying cryptic splice variants has made it a standard tool in clinical variant interpretation. However, splicing involves more than splice site recognition, and several extensions address aspects that SpliceAI does not fully capture.\nTissue-specific splicing patterns vary substantially across cell types and developmental stages. A splice site may be used in brain but skipped in liver due to differential expression of splicing factors. Models like Pangolin extend splice prediction by training on tissue-specific RNA-seq data, learning to predict not just whether a site is splice-competent but whether it is used in specific cellular contexts. These models enable variant interpretation that accounts for tissue-relevant splicing patterns rather than generic predictions.\nBranchpoint prediction identifies the adenosine residue where the lariat intermediate forms during splicing. While SpliceAI focuses on donor and acceptor sites, branchpoint recognition involves distinct sequence features (typically a degenerate YURAY motif 18-40 nucleotides upstream of the acceptor) that specialized models can capture. Combined analysis of donor, acceptor, and branchpoint predictions provides more complete characterization of splice-altering variants.\nAlternative splicing prediction moves beyond binary splice site identification to model exon inclusion rates and isoform usage. Models in this space attempt to predict not just whether an exon can be included but quantitative measures of inclusion across conditions, enabling analysis of splicing quantitative trait loci (sQTLs) and their effects on transcript diversity.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#limitations-and-open-challenges",
    "href": "p4-ch15-rna.html#limitations-and-open-challenges",
    "title": "15  RNA Models",
    "section": "15.10 Limitations and Open Challenges",
    "text": "15.10 Limitations and Open Challenges\n\n15.10.1 Sparse Structural Data\nThe fundamental limitation of RNA modeling is data scarcity. Protein structure prediction benefits from over 200,000 experimentally determined structures; RNA has fewer than 2,000, heavily biased toward ribosomal RNA and tRNA. This scarcity limits supervised learning for tertiary structure prediction and constrains the emergence of structural knowledge from self-supervised pretraining. Until high-throughput methods generate RNA structures at scale comparable to protein crystallography and cryo-EM, RNA tertiary structure prediction will remain a frontier problem rather than a solved one.\nSecondary structure data is more abundant but still limited. Experimentally validated structures cover mainly well-characterized families, while computational predictions for novel sequences rely on thermodynamic models whose accuracy degrades for long RNAs and complex folds. Structure probing experiments provide genome-wide coverage but measure accessibility rather than pairing directly, requiring inference to convert reactivity profiles into structural models.\n\n\n15.10.2 Functional Annotation Gaps\nFor many ncRNA classes, function remains poorly characterized. LncRNA annotations often specify only genomic location and expression pattern without mechanistic understanding. Circular RNA functions are emerging but incompletely cataloged. Even for better-characterized classes like miRNAs, target prediction remains noisy and context-dependent.\nThis annotation gap limits supervised learning for function prediction and complicates evaluation. When ground truth is uncertain, it becomes difficult to assess whether a model’s predictions reflect genuine biological insight or artifacts of incomplete training data. The field needs both experimental advances to characterize ncRNA function and computational approaches that can learn from weak or partial supervision.\n\n\n15.10.3 The Maturity Gap\nRNA foundation models exist but have not achieved the transformative impact of protein language models. ESM-2 enabled ESMFold, providing structure prediction from single sequences that nearly matches AlphaFold. No comparable RNA breakthrough has occurred. The reasons include data scarcity, the conformational complexity of RNA, and the diversity of RNA classes that makes unified modeling difficult.\nThis maturity gap represents both a limitation and an opportunity. The techniques that succeeded for proteins (large-scale self-supervised learning, attention mechanisms, scaling laws) provide a roadmap. Applying that roadmap to RNA requires addressing the data challenge through structure probing, synthetic data generation, or more efficient use of limited experimental structures. It requires architectural innovations that handle RNA’s long-range base pairing and conformational flexibility. And it requires benchmarks and evaluation frameworks that cover the full diversity of RNA types and tasks.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#rna-models-in-context",
    "href": "p4-ch15-rna.html#rna-models-in-context",
    "title": "15  RNA Models",
    "section": "15.11 RNA Models in Context",
    "text": "15.11 RNA Models in Context\nThis chapter has established RNA as a distinct modeling domain with its own architectures, training paradigms, and applications. RNA foundation models learn sequence representations through masked token prediction on noncoding RNA corpora. Codon-level models capture translation-relevant signals invisible to protein language models. Structure prediction has advanced through deep learning but lacks the breakthrough that transformed protein modeling. Design applications, particularly for mRNA therapeutics, demonstrate practical value while revealing the limits of current approaches.\nThe relationship between RNA models and other chapters reflects RNA’s position in the central dogma. Splicing models like SpliceAI (Chapter 6) operate on pre-mRNA and predict transcript processing outcomes. Protein language models (Chapter 12) provide the maturity benchmark that RNA models have not yet reached. Regulatory models like Enformer (Chapter 13) predict transcriptomic readouts from DNA but do not model RNA sequence directly. Single-cell models (Chapter 16) use RNA expression as a primary modality for cell state representation. Sequence design applications (Chapter 28) include mRNA optimization as a central task.\nThe following chapters extend from sequence-level modeling to cellular and systems context. Single-cell and epigenomic models treat RNA expression as a readout of cellular state. 3D genome models add spatial context that influences transcription. Network models integrate gene relationships that transcend individual sequences. RNA models provide the sequence-level representations that feed into these higher-level frameworks, bridging the foundation models of Part III with the multi-scale perspectives of Part IV.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch15-rna.html#figure-recommendations",
    "href": "p4-ch15-rna.html#figure-recommendations",
    "title": "15  RNA Models",
    "section": "15.12 Figure Recommendations",
    "text": "15.12 Figure Recommendations\nFigure 15.1: RNA energy landscape versus protein. Schematic contrasting the funneled energy landscape of protein folding with the flatter landscape of RNA, illustrating multiple competing conformations for a typical RNA sequence.\nFigure 15.2: Secondary structure elements. Diagram showing stems, loops, bulges, internal loops, and a pseudoknot with standard notation, highlighting the long-range base pairing that distinguishes RNA from protein secondary structure.\nFigure 15.3: RNA foundation model comparison. Table or chart comparing RNA-FM, protein ESM-2, and DNA foundation models on training corpus size, model parameters, and demonstrated emergent capabilities.\nFigure 15.4: Codon-level tokenization. Illustration showing the same protein encoded by different synonymous codons, highlighting how codon choice affects tRNA availability, translation speed, and mRNA secondary structure.\nFigure 15.5: mRNA design pipeline. Workflow showing the progression from protein sequence target through codon optimization, UTR design, structure optimization, and modification selection for therapeutic mRNA development.\n\n\n\n\nChen, Jiayang, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze Yu, Licheng Zong, et al. 2022. “[RNA-FM] Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions.” arXiv. https://doi.org/10.48550/arXiv.2204.00300.\n\n\nLi, Sizhen, Saeed Moayedpour, Ruijiang Li, Michael Bailey, Saleh Riahi, Milad Miladi, Jacob Miner, et al. 2023. “CodonBERT: Large Language Models for mRNA Design and Optimization.” bioRxiv. https://doi.org/10.1101/2023.09.09.556981.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nNaghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B. Macdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow Coyote-Maestas, and Hani Goodarzi. 2024. “[cdsFM - EnCodon/DeCodon] A Suite of Foundation Models Captures the Contextual Interplay Between Codons.” bioRxiv. https://doi.org/10.1101/2024.10.10.617568.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Models</span>"
    ]
  },
  {
    "objectID": "p4-ch16-single-cell.html",
    "href": "p4-ch16-single-cell.html",
    "title": "16  Single-Cell and Epigenomic Models",
    "section": "",
    "text": "16.1 The Single-Cell Data Landscape\nA human body contains approximately 37 trillion cells, yet every one of them carries an essentially identical genome. The same three billion base pairs encode the neurons that fire in the brain, the cardiomyocytes that contract in the heart, and the hepatocytes that metabolize drugs in the liver. How does one genome produce hundreds of distinct cell types, each with its own morphology, function, and disease susceptibility? The answer lies not in sequence but in state: which genes are active, which regulatory elements are accessible, which epigenetic marks are present. Capturing this state at single-cell resolution has become possible only in the past decade, and the resulting data deluge now rivals the scale that enabled large language models.\nThe regulatory models examined in Chapter 13 predict how sequence encodes molecular phenotypes, but they operate on bulk tissue averages that obscure cellular heterogeneity. A tumor biopsy might contain malignant cells, immune infiltrates, stromal fibroblasts, and endothelial cells in varying proportions. Bulk RNA-seq reports the average expression across this mixture, potentially masking the drug-resistant subpopulation that will cause relapse. Single-cell technologies decompose this mixture, revealing which cells express which genes and how cellular composition shifts during disease progression or treatment response. The challenge is that single-cell data are sparse, noisy, and high-dimensional, with tens of thousands of features measured across millions of cells. Foundation models offer a path through this complexity: learn general representations of cellular state from massive pretraining corpora, then transfer to specific tasks with limited labeled data.\nThis chapter examines foundation models for single-cell and epigenomic data across three interconnected scales. Cellular language models treat gene expression profiles as documents and learn the grammar of which genes co-occur in different cellular contexts. Epigenomic models capture the regulatory state encoded in DNA methylation and chromatin accessibility. Integration methods align cells across modalities when different assays are performed on different cells from the same tissue. Throughout, the central question remains: can models trained on cellular state representations learn regulatory logic that generalizes across tissues, conditions, and even species?",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell and Epigenomic Models</span>"
    ]
  },
  {
    "objectID": "p4-ch16-single-cell.html#the-single-cell-data-landscape",
    "href": "p4-ch16-single-cell.html#the-single-cell-data-landscape",
    "title": "16  Single-Cell and Epigenomic Models",
    "section": "",
    "text": "16.1.1 From Bulk to Single-Cell Resolution\nTraditional transcriptomic studies measure gene expression in bulk tissue, producing a single measurement per gene that represents the average across thousands to millions of cells. This averaging is both a strength and a limitation. It provides robust, reproducible measurements that have powered decades of biological discovery. It also fundamentally limits what questions can be asked. If a gene appears moderately expressed in bulk, is it uniformly expressed across all cells, or highly expressed in a rare subpopulation while silent elsewhere? Bulk data cannot distinguish these scenarios.\nSingle-cell RNA sequencing (scRNA-seq) resolves this ambiguity by measuring gene expression in individual cells. The technology has evolved rapidly since its introduction in 2009. Early methods captured hundreds of cells per experiment; current platforms routinely profile hundreds of thousands of cells, with some studies exceeding a million. Public repositories now contain tens of millions of single-cell transcriptomes spanning diverse tissues, developmental stages, disease states, and species. This scale approaches the data volumes that enabled large language models in natural language processing.\nThe analogy between cells and documents runs deeper than dataset size. In language, words combine according to grammatical rules to form sentences that convey meaning. In cells, genes combine according to regulatory programs to form expression profiles that define cellular identity. A hepatocyte expresses genes for drug metabolism, albumin synthesis, and bile production; a neuron expresses genes for synaptic transmission, ion channels, and neurotransmitter receptors. These expression programs are not random: transcription factors activate coherent sets of target genes, signaling pathways coordinate cellular responses, and developmental programs establish cell type identities through cascades of regulatory events. Just as language models learn syntax and semantics by predicting masked words, single-cell foundation models might learn regulatory logic by predicting masked genes.\n\n\n16.1.2 Technical Challenges and Data Characteristics\nSingle-cell data present distinctive challenges that shape how foundation models must be designed. Dropout is pervasive: due to inefficiencies in RNA capture and amplification, many genes that are actually expressed in a cell register as zero in the measurement. A gene with true expression may appear as zero in 50% to 90% of cells where it is actually transcribed. This zero-inflation means that absence of signal is not absence of expression.\nSparsity compounds the interpretation challenge. A typical single-cell transcriptome measures 20,000 genes, but any individual cell might have detectable expression for only 1,000 to 5,000 of them. The resulting data matrices are more than 90% zeros, requiring specialized computational approaches.\nBatch effects arise because technical variation between experiments often exceeds biological variation within them. Cells processed on different days, by different operators, or with different reagent lots may cluster by batch rather than by biological type. A model that learns batch-specific patterns rather than biological ones will fail to generalize.\nDynamic range spans orders of magnitude, from highly expressed housekeeping genes to rare transcription factors present at a few copies per cell. Normalizing across this range while preserving biologically meaningful variation requires careful preprocessing choices that can affect downstream results.\nDespite these challenges, the scale of available data creates opportunities. Tens of millions of cells, spanning hundreds of cell types across dozens of tissues and multiple species, provide training corpora large enough to learn general representations. The question is whether foundation model architectures can extract biological signal from noisy, sparse, high-dimensional measurements.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell and Epigenomic Models</span>"
    ]
  },
  {
    "objectID": "p4-ch16-single-cell.html#cellular-language-models",
    "href": "p4-ch16-single-cell.html#cellular-language-models",
    "title": "16  Single-Cell and Epigenomic Models",
    "section": "16.2 Cellular Language Models",
    "text": "16.2 Cellular Language Models\n\n16.2.1 Geneformer: Learning Network Biology\nGeneformer exemplifies the cellular language model approach, treating each cell as a sentence where genes serve as tokens (Theodoris et al. 2023). The model was pretrained on approximately 30 million single-cell transcriptomes to learn context-aware representations that capture how genes function within cellular regulatory networks. The key insight was that during pretraining, the model gained understanding of network dynamics in a completely self-supervised manner, encoding network hierarchy in its attention weights without explicit supervision on network structure.\nRather than using raw expression counts, Geneformer employs rank-based encoding that emphasizes relative expression. For each cell, genes are ranked by their expression level compared to their typical expression across the training corpus. This transformation highlights which genes are unusually active or silent in each cellular context. A gene ranked highly in a given cell is one whose expression deviates from its baseline, potentially indicating context-specific regulatory activation. The representation discards absolute counts, which vary with sequencing depth and capture efficiency, while preserving the relative ordering that reflects cellular state.\nPretraining uses a masked gene prediction objective analogous to BERT-style language modeling. A fraction of genes are masked in each cell, and the model learns to predict which genes were masked based on the remaining expression context. This forces the model to learn co-expression patterns: which genes tend to appear together at high ranks in the same cells, and which genes predict each other’s presence. The objective implicitly captures regulatory modules, signaling pathways, and cell-type-specific programs.\nAfter pretraining, Geneformer supports diverse downstream applications through fine-tuning or feature extraction. Cell type annotation achieves high accuracy even with limited labeled examples, leveraging general biological knowledge acquired during pretraining. The model identified candidate therapeutic targets for cardiomyopathy by analyzing how disease-associated genes fit within learned network structure, demonstrating potential for accelerating discovery in rare diseases where large disease-specific datasets are unavailable (Theodoris et al. 2023).\n\n\n16.2.2 scGPT: Generative Pretraining for Single-Cell Analysis\nscGPT extends the foundation model paradigm with a generative architecture trained on over 33 million cells (Cui et al. 2024). The model functions as a generalist backbone for single-cell analysis pipelines, supporting applications from cell type annotation to perturbation response prediction within a unified framework.\nThe architecture incorporates several innovations tailored to single-cell data characteristics. Gene tokens combine learnable embeddings with position encodings that can capture genomic location when relevant. Expression values are discretized into bins to handle the wide dynamic range and zero-inflation characteristic of single-cell measurements; rather than predicting continuous values, the model predicts which expression bin a gene falls into. Special tokens mark cell boundaries and indicate modality when multi-omic data are available.\nscGPT uses multiple pretraining objectives simultaneously. Masked gene prediction encourages learning of co-expression patterns, similar to Geneformer. Autoregressive generation predicts expression of one set of genes conditioned on others, enabling the model to generate synthetic expression profiles or impute missing values. Contrastive objectives push cells from the same type to cluster in embedding space while separating different types, providing discriminative signal that complements the generative objectives.\nThe combination of objectives enables scGPT to excel across multiple applications. Cell type annotation benefits from rich pretrained representations, including identification of fine-grained subtypes that might be missed by simpler methods. Multi-batch integration aligns cells from different experiments while preserving genuine biological variation, addressing the pervasive batch effect problem. Perturbation response prediction anticipates how cells will respond to genetic knockouts or drug treatments, providing a foundation for in silico experimentation.\n\n\n16.2.3 scFoundation and Scaling Single-Cell Models\nscFoundation pushes the scale of single-cell foundation models further, training on over 50 million cells with an architecture designed for both representation learning and generation (hao_scfoundation_2024?). The model explores how scaling laws observed in language models translate to cellular data, finding that larger models trained on more diverse data produce embeddings that transfer better across tasks and contexts.\nThe pretraining corpus spans diverse tissues, developmental stages, and disease states, including both human and mouse data. This diversity proves essential: models trained on narrow datasets (a single tissue or condition) learn representations that capture that specific context but fail to generalize. Models trained on diverse corpora learn more abstract representations of cellular state that transfer across biological contexts.\nscFoundation emphasizes the importance of tokenization and normalization choices for downstream performance. The model systematically compared different approaches to handling zero-inflation, normalization across sequencing depth, and gene vocabulary selection. These preprocessing decisions, often treated as implementation details, significantly affect what biological signals the model can capture.\n\n\n16.2.4 TranscriptFormer: Cross-Species Cellular Models\nTranscriptFormer extends single-cell foundation models across evolutionary time, training on over 112 million cells spanning 1.5 billion years of evolution across 12 species (Pearce et al. 2025). This cross-species approach tests whether regulatory principles learned from one organism generalize to others.\nThe model uses a novel autoregressive architecture that jointly predicts genes and their expression levels. Rather than treating gene identity and expression as separate prediction problems, TranscriptFormer generates them together, enabling it to produce synthetic cells conditioned on prompts specifying species, tissue, or cell type. Because the vocabulary spans multiple species with ortholog mappings, the model can transfer cell type annotations across evolutionary distances.\nIn zero-shot settings, TranscriptFormer demonstrates strong performance on both in-distribution and out-of-distribution cell type classification. Remarkably, models trained predominantly on mouse and human data can annotate cell types in zebrafish and other species separated by hundreds of millions of years of evolution. This cross-species transfer reveals that core principles of cellular regulation are deeply conserved, and that foundation models can capture these conserved principles when trained on evolutionarily diverse data.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell and Epigenomic Models</span>"
    ]
  },
  {
    "objectID": "p4-ch16-single-cell.html#perturbation-response-prediction",
    "href": "p4-ch16-single-cell.html#perturbation-response-prediction",
    "title": "16  Single-Cell and Epigenomic Models",
    "section": "16.3 Perturbation Response Prediction",
    "text": "16.3 Perturbation Response Prediction\n\n16.3.1 The In Silico Experiment Promise\nOne of the most compelling applications of cellular foundation models is predicting how cells will respond to perturbations. If a model truly understands regulatory logic, it should be able to anticipate the transcriptional consequences of knocking out a gene, activating a pathway, or treating with a drug. Such predictions could accelerate drug discovery by prioritizing candidates before expensive wet-lab validation, identify synthetic lethal interactions for cancer therapy, and suggest targets for diseases without known interventions.\nThe perturbation prediction task requires more than memorizing co-expression patterns. The model must understand directional relationships: if gene A activates gene B, then knocking out A should reduce B’s expression. It must capture network effects: perturbations propagate through regulatory cascades, producing secondary and tertiary effects beyond direct targets. It must recognize context dependence: the same perturbation may have different effects in different cell types or states.\n\n\n16.3.2 Perturb-seq and Foundation Model Training\nPerturb-seq combines CRISPR-based genetic perturbations with single-cell RNA sequencing, measuring the transcriptional consequences of gene knockouts across thousands of cells (dixit_perturb-seq_2016?). These datasets provide supervised signal for perturbation prediction: given the pre-perturbation state and the identity of the perturbed gene, predict the post-perturbation expression profile.\nFoundation models approach this task through transfer learning. A model pretrained on tens of millions of unperturbed cells learns general representations of cellular state and gene-gene relationships. Fine-tuning on Perturb-seq data teaches the model to map these representations to perturbation outcomes. The hope is that general biological knowledge from pretraining will enable accurate predictions for perturbations not seen during fine-tuning, including knockouts of genes never directly perturbed in training data.\nscGPT and Geneformer both demonstrate perturbation prediction capabilities, though performance varies across perturbation types and cellular contexts. Predictions are most accurate for well-characterized genes with many training examples and clear regulatory relationships. Performance degrades for poorly characterized genes, complex combinatorial perturbations, and cell types underrepresented in training data.\n\n\n16.3.3 Limitations of Current Approaches\nDespite promising results, current perturbation prediction models face fundamental limitations. Most training data come from immortalized cell lines that may not reflect primary tissue biology. Perturbations are typically single-gene knockouts; combinatorial perturbations involving multiple genes remain challenging. The models predict average responses across perturbed cells rather than the heterogeneity of individual responses.\nMore fundamentally, correlation-based learning from expression data cannot reliably distinguish correlation from causation. A gene that is always co-expressed with another may be co-regulated rather than directly regulating. Training on observational data (unperturbed cells) and interventional data (perturbed cells) provides complementary signals, but even Perturb-seq data have limited coverage of the regulatory network. Foundation models capture patterns in data; whether those patterns reflect causal regulatory relationships remains an empirical question that requires experimental validation.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell and Epigenomic Models</span>"
    ]
  },
  {
    "objectID": "p4-ch16-single-cell.html#epigenomic-foundation-models",
    "href": "p4-ch16-single-cell.html#epigenomic-foundation-models",
    "title": "16  Single-Cell and Epigenomic Models",
    "section": "16.4 Epigenomic Foundation Models",
    "text": "16.4 Epigenomic Foundation Models\n\n16.4.1 DNA Methylation and CpGPT\nDNA methylation occupies a privileged position in the regulatory hierarchy, sitting at a junction between genotype, environment, and phenotype. Methylation patterns integrate genetic influences, since sequence context affects which CpG sites can be methylated and polymorphisms can create or destroy CpG dinucleotides. They also integrate developmental programs, since methylation landscapes are extensively remodeled during differentiation and establish cell-type-specific regulatory states. Environmental exposures including diet, smoking, and stress leave lasting methylation signatures that persist long after the exposure ends.\nBeyond serving as an integrative readout, methylation encodes rich information about cellular identity and state. Epigenetic clocks built from methylation data predict chronological age with striking accuracy, and deviations from predicted age (epigenetic age acceleration) correlate with mortality risk and disease burden. Cell types can be distinguished by their methylation profiles, and disease states often manifest as characteristic methylation changes.\nCpGPT (Cytosine-phosphate-Guanine Pretrained Transformer) treats methylation as a sequence-like object amenable to transformer-based pretraining (Camillo et al. 2024). The model was pretrained on over 1,500 DNA methylation datasets encompassing more than 100,000 samples from diverse tissues and conditions. Each sample is tokenized as a sequence of CpG sites with their methylation values (beta values ranging from 0 to 1) and genomic positions. The model learns to predict masked methylation values from surrounding context, capturing both local correlations between neighboring CpG sites and global patterns that distinguish different tissues or conditions.\nAfter pretraining, CpGPT supports several capabilities with minimal additional supervision. The model can impute methylation levels at CpG sites not directly measured on a given array platform, effectively enabling conversion between different array technologies such as EPIC and 450K. For biological age prediction, fine-tuned CpGPT models match or exceed purpose-built epigenetic clocks while using a more general architecture. The learned embeddings cluster by tissue type without explicit supervision during pretraining, suggesting that the model captures biologically meaningful variation. For disease-associated methylation patterns, CpGPT can be adapted to distinguish cases from controls across multiple disease contexts through transfer learning.\n\n\n16.4.2 Chromatin Accessibility Models\nChromatin accessibility, measured by ATAC-seq and related assays, provides a complementary view of regulatory state. Accessible chromatin regions mark active regulatory elements: promoters, enhancers, and insulators where transcription factors can bind. The accessibility landscape varies across cell types and conditions, reflecting the regulatory programs that define cellular identity.\nFoundation models for chromatin accessibility face the challenge of representing accessibility peaks, which are genomic intervals of variable width rather than single values at fixed positions. Different approaches tokenize this data differently: some treat peaks as binary features (accessible or not), others use continuous accessibility scores, and some operate directly on the underlying sequence to predict accessibility.\nModels that predict chromatin accessibility from DNA sequence, such as those built on Enformer-style architectures (see Chapter 13), learn how sequence motifs and their arrangements determine accessibility. These models complement single-cell accessibility measurements by providing a mechanistic link between genotype and epigenetic state. Variants that alter predicted accessibility become candidates for regulatory function even when they fall outside coding regions.\nSingle-cell ATAC-seq (scATAC-seq) provides cell-type-resolved accessibility profiles, revealing which regulatory elements are active in which cells. Foundation models for scATAC-seq face similar challenges to scRNA-seq models (sparsity, dropout, batch effects) with the additional complexity that the feature space (accessibility peaks) varies across datasets depending on peak calling procedures. Models that operate on fixed genomic coordinates can integrate across datasets more readily than those that rely on dataset-specific peak sets.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell and Epigenomic Models</span>"
    ]
  },
  {
    "objectID": "p4-ch16-single-cell.html#cross-modality-integration",
    "href": "p4-ch16-single-cell.html#cross-modality-integration",
    "title": "16  Single-Cell and Epigenomic Models",
    "section": "16.5 Cross-Modality Integration",
    "text": "16.5 Cross-Modality Integration\n\n16.5.1 The Unpaired Integration Challenge\nSingle-cell experiments often profile different modalities in different cells. A study might include scRNA-seq data from one set of cells, scATAC-seq data from another set, and perhaps a small subset with both modalities measured simultaneously through multiome protocols. Integrating these data into a unified atlas requires aligning cells across modalities when the feature spaces are entirely different.\nThis problem is harder than standard batch correction because there is no direct correspondence between features. RNA-seq measures expression across roughly 20,000 genes. ATAC-seq measures accessibility across hundreds of thousands of peaks. A gene is not the same object as a peak. Simple approaches assign peaks to nearby genes and use gene-level summaries for alignment, but this conversion loses information about the detailed structure of accessibility within regulatory regions and introduces arbitrary choices about assignment rules.\n\n\n16.5.2 GLUE: Graph-Linked Unified Embedding\nGLUE (Graph-Linked Unified Embedding) addresses unpaired integration by combining modality-specific encoders with a graph of biological prior knowledge linking features across omics (Cao and Gao 2022). Rather than converting features between modalities, GLUE explicitly encodes regulatory relationships into a guidance graph and learns cell embeddings that are consistent with this graph.\nThe architecture has three key components. Modality-specific variational autoencoders provide encoders that map cells to a shared low-dimensional latent space and decoders that reconstruct modality-specific features. Generative distributions are tailored to each modality: negative binomial for count data, appropriate alternatives for accessibility.\nThe feature graph encodes biological prior knowledge about relationships between features across modalities. Nodes represent genes, peaks, and other genomic features. Edges connect ATAC peaks to genes they might regulate based on genomic proximity or chromatin conformation data. Edges connect genes to transcription factors that bind their promoters. This graph is provided as input rather than learned, allowing incorporation of external knowledge from databases and literature.\nA graph variational autoencoder learns feature embeddings from the guidance graph. These embeddings are used in the decoders, tying different modalities to a common regulatory backbone. Biologically related features (a gene and its putative enhancer) have similar representations, helping align the latent spaces.\nAdversarial alignment ensures that cell embeddings from different modalities are truly integrated. A discriminator tries to distinguish which modality produced each embedding, and encoders are trained to fool the discriminator. This forces the encoders to produce modality-invariant embeddings where cells from different assays occupy a shared manifold reflecting biological rather than technical variation.\n\n\n16.5.3 Applications of Cross-Modal Integration\nGLUE enables several applications beyond basic integration. Triple-omics integration combines gene expression, chromatin accessibility, and DNA methylation measured in different cells, producing unified cell type annotations that leverage all data types. Regulatory inference uses learned feature embeddings to identify candidate enhancer-gene links, providing a principled alternative to simple distance-based assignment.\nCross-modal prediction becomes possible once cells are aligned. The model can predict chromatin accessibility from expression or vice versa, enabling imputation of missing modalities. If a new dataset contains only scRNA-seq, the integrated model can predict which accessibility peaks would likely be active in each cell type based on expression patterns.\nSCGLUE extends the framework with optimizations for single-cell scale and sparsity (Cao and Gao 2022). The adversarial alignment handles batch effects common in single-cell experiments, and the graph structure incorporates tissue-specific regulatory relationships. The model scales to millions of cells while maintaining biological grounding from the guidance graph.\nThe success of graph-guided integration demonstrates that biological prior knowledge can regularize learning and improve alignment. The feature graph constrains what the model learns, ensuring consistency with known regulatory relationships while allowing discovery of new patterns. This combination of learned representations with structured biological knowledge provides a template for integrating foundation model embeddings with domain expertise (see Chapter 19 for further discussion of graph-based approaches).",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell and Epigenomic Models</span>"
    ]
  },
  {
    "objectID": "p4-ch16-single-cell.html#practical-challenges-and-limitations",
    "href": "p4-ch16-single-cell.html#practical-challenges-and-limitations",
    "title": "16  Single-Cell and Epigenomic Models",
    "section": "16.6 Practical Challenges and Limitations",
    "text": "16.6 Practical Challenges and Limitations\n\n16.6.1 Batch Effects and Technical Artifacts\nBatch effects remain the dominant challenge in single-cell analysis. Technical variation between experiments, protocols, and platforms can exceed biological variation, causing cells to cluster by batch rather than by type. Foundation models pretrained on diverse data may be more robust to batch effects than models trained on narrow datasets, but robustness is not guaranteed.\nThe problem is particularly acute when applying pretrained models to new data from platforms or protocols not represented in pretraining. A model trained predominantly on 10x Genomics data may perform poorly on Smart-seq2 data, not because of biological differences but because of systematic technical differences in capture efficiency, amplification bias, and gene detection. Evaluation must carefully distinguish genuine biological generalization from memorization of technical signatures.\n\n\n16.6.2 Cell Type Imbalance\nTraining corpora overrepresent common cell types while rare populations are poorly captured. Immune cells, particularly from blood, dominate many datasets. Rare cell types that may be disease-relevant, such as specific neuronal subtypes or tissue-resident stem cells, appear infrequently. Models may excel at distinguishing well-represented types while struggling with rare or novel populations.\nThis imbalance has equity implications when certain tissues or conditions are systematically undersampled. Neurological and psychiatric diseases involve cell types less represented in current atlases than blood or epithelial cells. Diseases affecting underrepresented populations may be modeled less accurately if training data come predominantly from European ancestry cohorts.\n\n\n16.6.3 Evaluation Complexity\nEvaluating single-cell foundation models is complicated by uncertain ground truth. Cell type labels in training data reflect current annotations that may be incomplete or inconsistent. Different studies use different annotation schemes, different levels of granularity, and different evidence standards. Performance metrics conflate model quality with annotation quality.\nPerturbation predictions face similar challenges. The “correct” transcriptional response to a perturbation depends on cell type, context, and measurement technology. Even well-characterized perturbations produce variable responses across replicates. Evaluation protocols must acknowledge these uncertainties rather than treating benchmarks as definitive ground truth.\n\n\n16.6.4 Causality and Mechanism\nThe most fundamental limitation is that correlation-based learning cannot establish causation. Foundation models learn patterns of co-occurrence: which genes appear together, which accessibility peaks associate with which expression changes. These patterns may reflect regulatory relationships, but they may also reflect confounding factors, indirect associations, or artifacts of data processing.\nThe perturbation prediction task illustrates this limitation. A model that accurately predicts perturbation outcomes for well-characterized genes may be learning genuine regulatory logic, or it may be exploiting superficial correlations that happen to work for genes with abundant training data. Distinguishing these possibilities requires experimental validation and careful analysis of model behavior on held-out perturbations.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell and Epigenomic Models</span>"
    ]
  },
  {
    "objectID": "p4-ch16-single-cell.html#connections-and-future-directions",
    "href": "p4-ch16-single-cell.html#connections-and-future-directions",
    "title": "16  Single-Cell and Epigenomic Models",
    "section": "16.7 Connections and Future Directions",
    "text": "16.7 Connections and Future Directions\nSingle-cell and epigenomic foundation models provide representations of cellular state that complement the sequence-based models examined in earlier chapters. While DNA and protein language models learn what sequences encode, cellular models learn what states cells occupy. These perspectives are complementary: sequence determines the possible states a cell can achieve, while cellular state reflects which possibilities are realized in a given context.\nThe three-dimensional organization of the genome, examined in Chapter 17, provides crucial context for interpreting cellular state. Chromatin accessibility and gene expression are not independent of genome folding; enhancer-promoter contacts determine which regulatory elements can influence which genes. Models that integrate 3D structure with cellular state measurements may achieve more accurate predictions than either approach alone.\nCell embeddings from foundation models can serve as node features in graph-based reasoning systems (see Chapter 19). Protein-protein interactions, regulatory networks, and pathway databases provide relational structure that pure sequence models cannot capture. Using foundation model embeddings as inputs to graph neural networks combines the representational power of large-scale pretraining with the relational inductive biases of graph architectures.\nMulti-omics integration (see Chapter 19) extends beyond the two-modality cases examined here to encompass proteomics, metabolomics, spatial transcriptomics, and clinical data. The GLUE framework demonstrates how biological prior knowledge can guide integration across modalities with different feature spaces. Scaling these approaches to many modalities while maintaining computational tractability remains an active research direction.\nLooking forward, several directions promise to enhance cellular foundation models. Training on more diverse data, spanning underrepresented tissues, conditions, and populations, should improve generalization. Incorporating temporal dynamics through developmental trajectories or disease progression could capture how states transition rather than just which states exist. Connecting cellular models to sequence-based models through mechanistic links would ground cellular state in the genetic programs that establish it.\nThe ultimate goal is models that explain rather than just predict: models that identify the regulatory mechanisms underlying cellular state, the variants that perturb those mechanisms, and the interventions that might restore normal function. Current foundation models capture patterns in cellular data with remarkable fidelity. Whether those patterns reflect the causal structure of biological regulation, or merely correlations useful for prediction, remains an open question that will require continued integration of computational modeling with experimental validation.\n\n\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T. Higgins-Chen, Steve Horvath, and Bo Wang. 2024. “CpGPT: A Foundation Model for DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. 2024. “scGPT: Toward Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI.” Nature Methods 21 (8): 1470–80. https://doi.org/10.1038/s41592-024-02201-0.\n\n\nPearce, James D., Sara E. Simmonds, Gita Mahmoudabadi, Lakshmi Krishnan, Giovanni Palla, Ana-Maria Istrate, Alexander Tarashansky, et al. 2025. “[TranscriptFormer] Cross-Species Generative Cell Atlas Across 1.5 Billion Years of Evolution: The TranscriptFormer Single-Cell Model.” bioRxiv. https://doi.org/10.1101/2025.04.25.650731.\n\n\nTheodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023. “[Geneformer] Transfer Learning Enables Predictions in Network Biology.” Nature 618 (7965): 616–24. https://doi.org/10.1038/s41586-023-06139-9.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell and Epigenomic Models</span>"
    ]
  },
  {
    "objectID": "p4-ch17-3d-genome.html",
    "href": "p4-ch17-3d-genome.html",
    "title": "17  3D Genome and Spatial Models",
    "section": "",
    "text": "17.1 Chromatin Organization Hierarchy\nThe human genome spans approximately two meters of linear DNA, yet it must fit within a nucleus roughly ten micrometers in diameter: a compaction ratio of nearly 200,000 to one. This folding is not random. Specific sequences contact each other across vast genomic distances while others remain isolated, and these contact patterns determine which enhancers can activate which genes. An enhancer 500 kilobases from its target gene can activate transcription only because the intervening chromatin folds to bring them into physical proximity. The regulatory models examined in Chapter 13 predict expression from sequence within a fixed window, but they treat the genome as a one-dimensional string. They cannot explain why an enhancer activates one gene and not another when multiple promoters lie within range.\nDisruptions to 3D genome architecture cause disease. When structural variants delete a boundary between chromatin domains, enhancers can contact genes they normally never reach, a phenomenon called enhancer hijacking that underlies developmental disorders and cancer. The clinical consequences depend entirely on which 3D contacts are disrupted, yet sequence alone cannot predict these outcomes without models of chromatin folding. A deletion that removes a domain boundary may be pathogenic; an identical-sized deletion that preserves boundaries may be benign. Current variant effect prediction tools (Chapter 14) largely ignore this mechanism, creating a systematic blind spot for structural variant interpretation.\nThis chapter examines how the genome folds, what determines that folding, and how computational models predict 3D structure from sequence. We begin with the hierarchy of chromatin organization from megabase-scale compartments to kilobase-scale loops, then introduce the experimental methods that measure these structures. The core technical sections cover models that predict chromatin contacts from sequence, including Akita, Orca, and C.Origami. We extend to spatial transcriptomics, where tissue architecture provides cellular context for gene expression. Throughout, we connect 3D structure to the regulatory models from Chapter 13, addressing what spatial context adds that sequence alone cannot provide.\nThe genome folds through multiple organizational levels, each with distinct functional consequences and arising from different molecular mechanisms. Understanding this hierarchy is essential for interpreting both normal gene regulation and how structural variants cause disease. The levels are not independent; they interact in complex ways that computational models must capture to predict 3D structure accurately.\nAt the largest scale, chromosomes occupy distinct nuclear volumes called chromosome territories. Gene-rich chromosomes tend toward the nuclear interior while gene-poor chromosomes associate with the nuclear periphery. This territorial organization limits which chromosomes can exchange material during translocations: recurrent cancer-associated translocations occur preferentially between chromosomes that occupy neighboring territories. While chromosome territory organization has clear functional implications, most computational models focus on finer-scale structures where sequence determinants are more tractable.\nWithin chromosome territories, chromatin partitions into two major compartment types distinguished by their transcriptional activity and chromatin state. A compartments contain gene-rich, transcriptionally active chromatin with open, accessible structure. B compartments contain gene-poor, transcriptionally silent regions often associated with the nuclear lamina at the nuclear periphery. This compartmentalization is visible in Hi-C contact maps as a characteristic checkerboard pattern: A compartment regions preferentially contact other A regions even when separated by megabases, while B regions contact other B regions. Compartment identity correlates strongly with histone modifications (H3K27ac marks active A compartments; H3K9me3 marks repressive B compartments) and changes during cellular differentiation as lineage-specific genes shift between active and inactive states. The molecular mechanism underlying compartmentalization appears to involve phase separation: regions with similar chromatin states aggregate through weak multivalent interactions, creating nuclear microenvironments with distinct biochemical properties.\nBelow the megabase scale of compartments, the genome organizes into topologically associating domains, or TADs: sub-megabase regions (median approximately 800 kilobases in mammals) within which sequences contact each other more frequently than with sequences outside the domain. TAD boundaries appear as sharp transitions in contact frequency, visible in Hi-C maps as triangular domains along the matrix diagonal. These boundaries show remarkable conservation across mammalian species and across cell types within a species, suggesting strong selective pressure to maintain domain organization. The prevailing model holds that TADs constrain enhancer-promoter interactions: regulatory elements within a TAD can contact genes in the same domain, but boundaries prevent crosstalk with genes in adjacent domains. This insulation function has clear clinical relevance. Deletions that remove TAD boundaries allow enhancers to contact genes they normally cannot reach. In a well-characterized example, deletions removing the boundary between the EPHA4 locus and the WNT6/PAX3 region allow limb enhancers to ectopically activate WNT6, causing brachydactyly and other limb malformations (lupiañez_disruptions_2015?).\nThe molecular basis of TAD formation is now well understood through the loop extrusion model. The cohesin protein complex loads onto chromatin and extrudes DNA bidirectionally, progressively enlarging the extruded loop until it encounters an obstacle. The key obstacle is CTCF protein bound to DNA in a specific orientation. When cohesin encounters CTCF sites oriented toward each other (convergent orientation), extrusion halts and a stable loop forms with the convergent CTCF sites at the loop anchors. This model explains several key observations: TAD boundaries are enriched for CTCF binding sites; CTCF motif orientation predicts which sites will anchor loops (convergent pairs form loops while divergent pairs do not); and acute degradation of cohesin eliminates TADs within hours while leaving compartments intact. The distinction between compartment and TAD formation mechanisms has important implications for prediction. Models that capture CTCF binding and orientation can predict TAD boundaries; predicting compartments requires learning different sequence features associated with chromatin state.\nAt the finest scale, chromatin forms specific loops between defined loci. Enhancer-promoter loops bring distal regulatory elements into physical proximity with their target genes, while structural loops between convergent CTCF sites establish the TAD framework. Most enhancer-promoter contacts span less than 200 kilobases, but some extend over a megabase. Detecting these fine-scale contacts requires high-resolution data; the Micro-C method uses micrococcal nuclease digestion to achieve nucleosome-level resolution, revealing contact patterns invisible in standard Hi-C. The functional significance of individual loops remains debated. Some loops appear essential for gene activation; others may be structural features without direct regulatory consequences.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome and Spatial Models</span>"
    ]
  },
  {
    "objectID": "p4-ch17-3d-genome.html#sec-3d-measurement",
    "href": "p4-ch17-3d-genome.html#sec-3d-measurement",
    "title": "17  3D Genome and Spatial Models",
    "section": "17.2 Measuring the 3D Genome",
    "text": "17.2 Measuring the 3D Genome\nPredicting 3D genome structure requires training data: measurements of which sequences contact which other sequences in real cells. Chromosome conformation capture methods provide these measurements through a common biochemical principle. Cells are crosslinked with formaldehyde to freeze chromatin contacts in place; DNA is digested with restriction enzymes; free DNA ends are ligated, preferentially joining fragments that were spatially proximate; and the ligated junctions are identified through sequencing. The frequency of junction reads between two genomic regions reflects how often those regions were in physical contact across the cell population.\nHi-C extends this principle genome-wide by incorporating biotinylated nucleotides at ligation junctions, enabling purification of chimeric fragments from the entire genome. The output is a contact matrix where rows and columns represent genomic bins (typically 1 to 50 kilobases depending on sequencing depth) and values represent contact frequencies between bin pairs. Resolution depends directly on sequencing depth: achieving 1 kilobase resolution requires billions of reads, while 10 kilobase resolution requires hundreds of millions. Raw contact frequencies require extensive normalization to correct for biases from GC content, restriction site density, and mappability. The ICE (iterative correction and eigenvector decomposition) method and related approaches remove these technical artifacts while preserving biological signal.\nThe contact matrix encodes all levels of chromatin organization. Compartments appear as the checkerboard pattern when viewing megabase-scale interactions; TADs appear as triangular domains of enriched contacts along the diagonal; and loops appear as focal enrichments at specific off-diagonal positions. The matrix is dominated by the polymer effect: sequences that are close in linear distance contact each other frequently regardless of specific 3D structure, creating strong signal along the diagonal that can obscure biologically meaningful contacts at greater distances.\nBeyond standard Hi-C, several technologies address specific limitations. Micro-C achieves nucleosome-level resolution by using micrococcal nuclease instead of restriction enzymes, revealing fine-scale contact patterns invisible at standard Hi-C resolution. Single-cell Hi-C measures contacts in individual cells, revealing that any two loci contact each other in only 5 to 15 percent of cells, but the resulting matrices are extremely sparse (most possible contacts are unmeasured in any single cell). Imaging methods such as DNA FISH directly visualize genomic loci in the nucleus, providing ground truth for computational predictions but at much lower throughput than sequencing-based approaches.\nTraining data for 3D prediction models comes primarily from a small number of well-characterized cell lines. The lymphoblastoid cell line GM12878 and the leukemia cell line K562 have deep Hi-C coverage across multiple laboratories, making them the default training sets for most models. Primary tissues and rare cell types have sparse coverage, creating a significant gap between where models are trained and where clinical applications require predictions. The 4D Nucleome Data Portal and ENCODE provide the most comprehensive repositories of 3D genome data, though coverage remains heavily biased toward common cell lines and human samples.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome and Spatial Models</span>"
    ]
  },
  {
    "objectID": "p4-ch17-3d-genome.html#sec-3d-prediction",
    "href": "p4-ch17-3d-genome.html#sec-3d-prediction",
    "title": "17  3D Genome and Spatial Models",
    "section": "17.3 Predicting 3D Structure from Sequence",
    "text": "17.3 Predicting 3D Structure from Sequence\nIf 3D genome structure determines which enhancers contact which genes, can we predict that structure directly from DNA sequence? Success here would enable predicting how mutations affect chromatin folding even when no Hi-C data exists for the relevant cell type or genetic variant. The core prediction task takes DNA sequence surrounding a genomic region (typically 1 to 2 megabases) as input and produces a predicted contact matrix at some resolution (typically 2 to 10 kilobase bins) as output. The training signal comes from experimentally measured Hi-C contact frequencies, with models learning to map sequence features to the patterns of contacts observed in real data.\nAkita established the foundational approach for sequence-based 3D prediction (fudenberg_predicting_2020?). The architecture uses dilated convolutions similar to SpliceAI (Chapter 6), processing the one-dimensional input sequence through convolutional layers that progressively expand the receptive field to approximately one megabase. A symmetric output layer transforms the processed sequence into a two-dimensional contact matrix. Training uses mean squared error loss on log-transformed, distance-normalized contact frequencies, where distance normalization removes the strong diagonal signal from the polymer effect. Akita demonstrated that sequence alone contains sufficient information to predict Hi-C contact maps with meaningful accuracy. The model correctly predicts TAD boundaries, capturing the enrichment of CTCF binding sites and transcription start sites at domain edges. In silico mutagenesis (systematically introducing mutations and observing predicted changes) reveals which sequence features most strongly influence predicted contacts, providing interpretable links between sequence and structure. The primary limitations are fixed resolution (2 kilobase bins), training on a single cell type (GM12878), and lack of explicit CTCF binding modeling.\nOrca extended this approach through multi-scale architecture that predicts at multiple resolutions simultaneously (zhou_orca_2022?). Rather than predicting directly at fine resolution, Orca first generates coarse predictions at megabase scale, then progressively refines to kilobase scale. This cascaded approach better captures the hierarchical nature of chromatin organization, improving compartment predictions while maintaining TAD and loop accuracy. Orca also introduced virtual 4C analysis: extracting the predicted contact profile from any genomic viewpoint, enabling focused analysis of specific loci without rerunning the model. The multi-scale training objective combines losses at different resolutions, allowing the model to learn both compartment-scale and loop-scale features from the same training data.\nC.Origami addressed the cell-type specificity problem by incorporating CTCF ChIP-seq as auxiliary input (tan_corigami_2023?). While Akita and Orca predict from sequence alone, C.Origami takes both sequence and CTCF binding profiles, enabling cell-type-specific predictions based on cell-type-specific CTCF patterns. This design reflects the loop extrusion model: TAD formation depends on where CTCF binds and in what orientation, and CTCF binding varies across cell types. The practical advantage is transfer learning: the model can be trained on cell types with expensive Hi-C data, then applied to cell types where only CTCF ChIP-seq is available (a much cheaper assay). C.Origami substantially outperforms sequence-only models for cross-cell-type prediction, though it requires CTCF data as input rather than predicting purely from sequence.\nOther approaches explore different architectural and methodological choices. DeepC uses transfer learning to adapt models trained on one cell type to another, partially addressing the limited training data problem. Epiphany applies diffusion models to 3D structure prediction, treating contact matrix generation as a denoising process. Higashi addresses single-cell Hi-C data specifically, using variational autoencoders to impute missing contacts in the sparse single-cell matrices. Emerging transformer-based models may better capture the very long-range dependencies inherent in chromatin organization, though the computational cost of attention over megabase sequences presents challenges.\nInterpretability analysis reveals what these models learn about sequence determinants of 3D structure. Attribution methods consistently identify CTCF motifs as the strongest predictors of contact patterns, with convergent CTCF pairs (motifs oriented toward each other) most strongly associated with loop anchors. Transcription start sites contribute to boundary predictions, consistent with the observation that active promoters often coincide with domain edges. GC content correlates with compartment identity (GC-rich regions tend toward A compartment), and repetitive element composition shows systematic associations (LINE elements with B compartment; Alu elements with A compartment). The orientation rule for CTCF emerges naturally from training: models learn that CTCF motif orientation, not just presence, predicts which sites will anchor loops. This learned relationship matches the mechanistic understanding from the loop extrusion model, providing validation that models capture biologically meaningful features.\nDespite these advances, significant limitations remain. Resolution is constrained by training data; predicting nucleosome-level contacts requires Micro-C training data that exists for few cell types. The single-cell variation problem is fundamental: models trained on bulk Hi-C predict population averages, but gene regulation may depend on the stochastic 3D configurations in individual cells. Causality cannot be established from prediction alone; a model may correctly predict that two regions contact each other without revealing whether that contact causes any functional consequence. Generalization to cell types distant from training data remains uncertain, and the computational cost of processing megabase sequences limits practical applications for genome-wide analysis.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome and Spatial Models</span>"
    ]
  },
  {
    "objectID": "p4-ch17-3d-genome.html#sec-3d-regulation",
    "href": "p4-ch17-3d-genome.html#sec-3d-regulation",
    "title": "17  3D Genome and Spatial Models",
    "section": "17.4 3D Structure and Gene Regulation",
    "text": "17.4 3D Structure and Gene Regulation\nThe regulatory models from Chapter 13 predict gene expression from sequence within a 200 kilobase window, capturing many enhancer-promoter relationships but treating the genome as a one-dimensional string. This representation cannot distinguish an enhancer that loops to a distant gene from one blocked by a TAD boundary, nor can it explain cell-type-specific contacts that activate different genes from the same enhancer in different contexts. The 3D genome provides this missing context: physical proximity through chromatin loops determines which regulatory elements can communicate.\nConsider an enhancer located 300 kilobases from two genes, one upstream and one downstream. Linear models would predict similar regulatory influence on both genes based on comparable distances. But if a TAD boundary lies between the enhancer and the upstream gene, 3D structure predicts that only the downstream gene receives regulatory input. The boundary insulates the upstream gene from enhancer activity regardless of linear proximity. This insulation function explains why TAD boundaries show such strong evolutionary conservation: disrupting boundaries allows regulatory crosstalk that can dysregulate gene expression with pathogenic consequences.\nThe clinical significance is clearest in structural variant interpretation. Deletions that remove TAD boundaries cause enhancer hijacking, where regulatory elements gain access to genes in adjacent domains. The EPHA4 locus provides the canonical example: limb enhancers normally activate EPHA4 expression in developing limbs. When deletions remove the TAD boundary separating EPHA4 from the adjacent WNT6/PAX3 domain, these enhancers ectopically activate WNT6, causing limb malformations including brachydactyly and polydactyly. Different deletion sizes produce different phenotypes depending on which boundaries are removed and which new enhancer-gene contacts form. Similar mechanisms operate in cancer, where structural variants create novel enhancer-oncogene contacts that drive tumor growth. The diagnostic challenge is substantial: predicting pathogenicity of structural variants requires understanding which 3D contacts will be disrupted and what new contacts will form, predictions that sequence-only models cannot provide.\nIntegrating 3D predictions with expression models remains technically challenging. Hybrid approaches use predicted contacts to weight enhancer contributions: rather than treating all enhancers within a window equally, weights reflect predicted contact frequency with the target promoter. This activity-by-contact framework (expression proportional to the sum of enhancer activities weighted by contact frequencies) captures some of the regulatory logic that 1D models miss. Graph-based representations (Chapter 19) can encode genes and enhancers as nodes with contacts as edges, enabling graph neural networks to reason about regulatory relationships in 3D space. End-to-end training of combined 3D and expression models remains difficult; most current approaches train the components separately and combine predictions post hoc.\nThe causality question complicates interpretation. Do enhancer-promoter contacts cause gene activation, or does gene activation cause contacts? Transcription itself can influence chromatin organization: active transcription may stabilize enhancer-promoter contacts that would otherwise be transient. Perturbation experiments provide cleaner causal tests than correlational analysis. Acute degradation of cohesin eliminates TADs within hours, yet most genes show minimal expression changes, suggesting that many TAD structures are permissive rather than deterministic for gene regulation. CRISPR-based deletion of specific TAD boundaries similarly produces more modest effects than the structural disruption would suggest. The emerging view is nuanced: 3D structure constrains which enhancer-promoter interactions are possible, but whether those interactions occur depends on additional factors including transcription factor availability and chromatin state.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome and Spatial Models</span>"
    ]
  },
  {
    "objectID": "p4-ch17-3d-genome.html#sec-spatial-transcriptomics",
    "href": "p4-ch17-3d-genome.html#sec-spatial-transcriptomics",
    "title": "17  3D Genome and Spatial Models",
    "section": "17.5 Spatial Transcriptomics",
    "text": "17.5 Spatial Transcriptomics\nSingle-cell RNA sequencing (Chapter 16) reveals cellular heterogeneity but discards spatial information: we learn which genes each cell expresses but not where that cell sits within the tissue. For understanding tumor microenvironments, developmental gradients, or tissue architecture, spatial context is essential. A T cell adjacent to a tumor cell experiences a different microenvironment than one in the surrounding stroma, and this spatial context shapes gene expression programs in ways that dissociated single-cell data cannot capture.\nSpatial transcriptomics technologies fall into two broad categories with complementary strengths. Spot-based methods like Visium (10x Genomics) capture polyadenylated RNA at arrayed positions on a slide, providing transcriptome-wide measurement at approximately 55 micrometer resolution (typically 1 to 10 cells per spot). These methods offer comprehensive gene coverage but limited spatial resolution. Imaging-based methods like MERFISH use sequential rounds of fluorescent hybridization to identify RNA molecules in situ, achieving subcellular resolution but limited to pre-selected gene panels (hundreds to thousands of genes rather than transcriptome-wide). Newer technologies like Stereo-seq achieve near-cellular resolution with transcriptome-wide coverage through spatial barcoding, though they remain less validated than established methods.\nComputational challenges in spatial transcriptomics mirror and extend those in single-cell analysis. Spot deconvolution addresses the multiple-cells-per-spot problem in Visium data: inferring the cell type composition within each spot by comparing spot expression profiles to reference single-cell atlases. Imputation methods predict expression of genes not measured in imaging-based assays, leveraging correlations learned from reference datasets. Integration aligns spatial data with single-cell references, mapping reference cell types onto spatial coordinates. Domain correction handles batch effects that manifest in spatial patterns as well as expression levels. The sparsity problem is even more severe than in standard scRNA-seq; gene detection rates in spatial methods often fall below 10 percent.\nSpatial foundation models remain much less mature than sequence-based models. The fundamental challenge is the lack of an equivalent to evolutionary pretraining: DNA and protein models learn from billions of years of evolutionary experiments encoded in sequence databases, but no comparable natural augmentation exists for spatial organization. Current approaches include graph neural networks that encode spatial relationships as edges between neighboring cells or spots, transformer architectures that treat spatial positions as tokens with positional encodings derived from coordinates, and generative models that learn spatial patterns from atlases of reference tissues. Models like Nicheformer apply transformer architectures to spatial niches (local cellular neighborhoods), learning representations that capture cell-cell communication patterns and tissue microenvironment signatures. SpaGCN uses graph convolutional networks with spatial graphs, propagating information between spatially adjacent regions to identify spatial domains with coherent expression patterns.\nThe clinical applications motivating spatial foundation model development center on tumor microenvironment characterization. The spatial organization of immune cells relative to tumor cells predicts treatment response: tumors with immune cells infiltrating the tumor core respond better to immunotherapy than those with immune exclusion at the tumor periphery. Spatial models aim to learn these prognostic patterns from training data, enabling prediction of treatment response from spatial organization alone. Similar applications exist in developmental biology (understanding morphogen gradients and cell fate decisions), neuroscience (mapping brain region organization), and pathology (characterizing disease architecture in tissue sections).",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome and Spatial Models</span>"
    ]
  },
  {
    "objectID": "p4-ch17-3d-genome.html#sec-3d-limitations",
    "href": "p4-ch17-3d-genome.html#sec-3d-limitations",
    "title": "17  3D Genome and Spatial Models",
    "section": "17.6 Limitations and Open Questions",
    "text": "17.6 Limitations and Open Questions\nCurrent 3D genome and spatial models face limitations that constrain their utility for clinical and research applications. Resolution remains a fundamental constraint: most Hi-C prediction models operate at 2 to 10 kilobase resolution, while functionally relevant enhancer-promoter contacts involve specific sequences within those bins. Predicting which specific kilobases within a TAD contact each other requires resolution that exceeds current training data in most cell types. The resolution needed for accurate prediction may exceed the resolution achievable from bulk Hi-C, creating a data ceiling that computational methods cannot overcome.\nThe population averaging problem is more fundamental than a mere technical limitation. Bulk Hi-C measurements average over millions of cells, each with a different 3D configuration. Any two loci contact each other in only a minority of cells at any given time, yet the averaged contact frequency appears as a single value in the training data. Single-cell Hi-C reveals this heterogeneity but produces extremely sparse data (most possible contacts unmeasured in each cell). Models trained on population averages cannot predict single-cell behavior, yet gene regulation may depend on the stochastic dynamics of contact formation in individual cells. Whether the population average or the single-cell distribution matters more for predicting gene expression remains unclear.\nCausality represents the deepest conceptual challenge. Predicting that two regions contact each other does not establish that the contact causes any biological consequence. Many TAD disruptions produce minimal expression changes; many enhancer-promoter contacts may be bystanders rather than drivers of transcription. The loop extrusion machinery that creates TADs operates continuously, but the transcriptional machinery that reads out enhancer-promoter communication operates on different timescales and with different requirements. Computational predictions of 3D structure are correlational; establishing which predicted contacts matter functionally requires experimental validation that computational methods cannot replace.\nFor clinical applications, the sparse training data creates systematic blind spots. Models trained on GM12878 and K562 may not transfer to the primary cells, developmental stages, or disease states where predictions matter most. A structural variant affecting 3D organization in neural progenitor cells cannot be reliably interpreted using models trained only on lymphoblastoid cells. The cell types most relevant for clinical interpretation are often those with the least 3D characterization data available.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome and Spatial Models</span>"
    ]
  },
  {
    "objectID": "p4-ch17-3d-genome.html#sec-3d-integration",
    "href": "p4-ch17-3d-genome.html#sec-3d-integration",
    "title": "17  3D Genome and Spatial Models",
    "section": "17.7 Toward Integration",
    "text": "17.7 Toward Integration\nThe genome’s 3D organization provides context that one-dimensional sequence models cannot capture. Enhancer-promoter contacts explain regulatory relationships spanning hundreds of kilobases; TAD boundaries constrain which elements can interact; and tissue architecture determines the cellular neighborhoods where gene expression programs execute. Models like Akita, Orca, and C.Origami demonstrate that sequence contains substantial information about chromatin folding, while spatial transcriptomics foundation models begin to learn the organizing principles of tissue structure.\nThe chromatin contacts examined here become edges in the gene regulatory networks of Chapter 19, where graph-based methods reason about regulatory relationships using FM embeddings as node features and Hi-C contacts as structural priors. The spatial expression patterns integrate with the multi-omics approaches of Chapter 19, adding tissue architecture as another modality alongside genomics, transcriptomics, and epigenomics. For interpretability (Chapter 24), 3D structure offers mechanistic hypotheses: a predicted regulatory effect may operate through chromatin contacts that bring enhancer and promoter into proximity.\nWhether 3D structure is truly the missing link between sequence and regulation, or merely another correlate that reflects underlying causation without providing it, remains an open question. The experimental perturbation studies suggest a more modest role than early enthusiasm implied: TAD disruption often has limited expression consequences, and many contacts appear permissive rather than instructive. The 3D genome may establish the possibility of regulatory communication without determining whether that communication occurs. Resolving this question requires the kind of careful experimental validation that computational models can motivate but not replace.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome and Spatial Models</span>"
    ]
  },
  {
    "objectID": "p4-ch17-3d-genome.html#figures",
    "href": "p4-ch17-3d-genome.html#figures",
    "title": "17  3D Genome and Spatial Models",
    "section": "17.8 Figures",
    "text": "17.8 Figures\nFigure 17.1: Chromatin Organization Hierarchy. Four-panel schematic showing nested levels of genome organization. Panel A shows chromosome territories within the nucleus, with gene-rich chromosomes positioned toward the interior. Panel B displays a Hi-C contact matrix with the characteristic checkerboard pattern of A and B compartments visible at megabase scale. Panel C zooms to show TADs as triangular domains of enriched contact along the matrix diagonal, with boundaries marked by sharp transitions. Panel D highlights chromatin loops as focal enrichments at off-diagonal positions, marking specific enhancer-promoter or CTCF-CTCF contacts. Scale bars indicate approximate genomic distances at each level.\nFigure 17.2: The Loop Extrusion Model. Schematic illustrating how cohesin and CTCF establish TAD boundaries and chromatin loops. Top panel shows cohesin loading onto chromatin and beginning bidirectional extrusion. Middle panel shows extrusion proceeding until cohesin encounters CTCF bound in convergent orientation (motifs facing each other). Bottom panel shows the resulting stable loop with convergent CTCF sites at the anchors. Inset emphasizes that CTCF motif orientation determines boundary function: convergent sites anchor loops while divergent sites do not.\nFigure 17.3: Sequence-to-Structure Prediction Architecture. Diagram of Akita-style model architecture. Input is one-dimensional DNA sequence (approximately 1 megabase). Dilated convolutional layers process the sequence with progressively expanding receptive fields. A symmetric output transformation produces the two-dimensional contact matrix prediction. Comparison of predicted and experimental Hi-C for a representative region illustrates the correspondence between model output and ground truth, with TAD boundaries and loop anchors correctly positioned.\nFigure 17.4: TAD Boundary Disruption and Enhancer Hijacking. Three-panel illustration of the EPHA4 locus example. Left panel shows normal configuration with limb enhancers (marked) contacting EPHA4 within their shared TAD, separated by a boundary from the adjacent WNT6/PAX3 domain. Middle panel shows the boundary deletion that removes insulation between domains. Right panel shows the pathogenic outcome: enhancers now contact WNT6, causing ectopic expression and limb malformations. Clinical phenotypes (brachydactyly, polydactyly) indicated for different deletion configurations.\nFigure 17.5: Spatial Transcriptomics Data Structure. Three-panel figure showing spatial data types. Panel A shows a tissue section with Visium spots overlaid, colored by dominant cell type from deconvolution analysis. Panel B shows expression of a marker gene mapped to spatial coordinates, revealing tissue architecture (example: zonation pattern in liver). Panel C shows cell-cell communication inference, with arrows indicating ligand-receptor pairs between neighboring cell types in the spatial context.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome and Spatial Models</span>"
    ]
  },
  {
    "objectID": "p4-ch18-networks.html",
    "href": "p4-ch18-networks.html",
    "title": "18  Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning",
    "section": "",
    "text": "18.1 Critical Issues Requiring Revision",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span>"
    ]
  },
  {
    "objectID": "p4-ch18-networks.html#critical-issues-requiring-revision",
    "href": "p4-ch18-networks.html#critical-issues-requiring-revision",
    "title": "18  Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning",
    "section": "",
    "text": "18.1.1 1. Core Reframing Gap (CRITICAL)\nOutline Requirement: “The key reframing is that GNNs and networks are not alternatives to foundation models but rather operate at a higher level of abstraction using FM embeddings as inputs.”\nCurrent State: The chapter mentions this integration but does not position it as THE central thesis. The “Integration with Sequence Foundation Models” section appears late (after applications) rather than serving as the organizing principle.\nFix: Restructure to make FM-embeddings-as-node-features the central architectural insight, introduced early and threaded throughout all subsequent sections.\n\n\n18.1.2 2. Opening/Motivation Issues\nOutline Requirement: “Open with the abstraction hierarchy: sequence → molecule → network → phenotype”\nCurrent State: Opening discusses sequences and relations but doesn’t explicitly establish the abstraction hierarchy as a framing device.\nFix: Rewrite opening paragraphs to explicitly establish this hierarchy and position networks as the level where FM representations gain relational context.\n\n\n18.1.3 3. Em-Dash Violations (CRITICAL)\nCurrent Issues Found: - “nodes (or vertices)” has em-dash variant in some places - Several compound clauses using em-dashes throughout - Need systematic replacement with colons, semicolons, parentheses, or separate sentences\n\n\n18.1.4 4. Cross-Reference Updates\nRequired Changes: - Chapter 12 → verify (Ch 12) - Chapter 11 → verify (Ch 11)\n- Chapter 16 → verify (Ch 16) - Chapter 19 → Ch 19 (forward reference) - Chapter 27 → Ch 27 (forward reference) - Chapter 20 → Ch 20 - Chapter 21 → Ch 21 - Chapter 22 → Ch 22 - Chapter 14 → Ch 14 - Chapter 3 → Ch 3 - ?sec-clinical-risk → Ch 25\n\n\n18.1.5 5. Missing/Underdeveloped Content\nFrom Outline Key Concepts: - Gene prioritization for GWAS: partially covered, needs strengthening - Drug-target interaction: mentioned but thin - Knowledge graphs: underdeveloped - Network data sources (STRING, BioGRID, Reactome): mentioned but could be more systematic\n\n\n18.1.6 6. Section Organization Issues\nCurrent Order: 1. Opening 2. GNN Fundamentals 3. Biological Graph Types 4. Key Applications 5. Architecture Patterns 6. Practical Considerations 7. Summary\nRecommended Reorder (to emphasize FM integration): 1. Opening (abstraction hierarchy, FM integration thesis) 2. Biological Networks and Data Resources 3. GNN Fundamentals (message passing, architectures) 4. Foundation Model Embeddings as Node Features (CENTRAL section) 5. Applications (all framed as FM+GNN combinations) 6. Practical Considerations 7. Summary\n\n\n18.1.7 7. Clinical Stakes Weakness\nIssue: Clinical motivation is present but could be stronger, particularly for disease gene prioritization.\nFix: Add concrete clinical scenarios at section openings per guidelines.\n\n\n18.1.8 8. Typography Issues\nRequired: - Bold for glossary terms on first mention (message passing, heterogeneous graphs, etc.) - Italics for gene/protein names - Monospace for formats (VCF, etc.) and tools\n\n\n18.1.9 9. “Lead with Why” Violations\nSeveral subsections start with mechanisms before motivation. Examples: - “Graph Construction and Quality” section - “Scalability and Efficiency” section",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span>"
    ]
  },
  {
    "objectID": "p4-ch18-networks.html#biological-networks-and-data-resources",
    "href": "p4-ch18-networks.html#biological-networks-and-data-resources",
    "title": "18  Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning",
    "section": "19.1 Biological Networks and Data Resources",
    "text": "19.1 Biological Networks and Data Resources\n\n19.1.1 The Landscape of Biological Graphs\nBefore examining graph neural network architectures, it is essential to understand what biological networks exist and where they come from. The choice of network fundamentally shapes what a model can learn, and the biases inherent in network construction propagate through all downstream analyses.\nProtein-protein interaction networks represent physical associations between proteins. Major databases include STRING, which integrates experimental data with computational predictions and text mining to assign confidence scores to interactions; BioGRID, which focuses on curated experimental interactions; and IntAct, which provides detailed interaction metadata from direct molecular experiments. These networks are incomplete (current estimates suggest only 20-30% of human PPIs are catalogued) and biased toward well-studied proteins in well-characterized pathways. A gene involved in cancer or a common disease may have hundreds of documented interactions, while an uncharacterized protein in a specialized tissue may have none, not because it lacks interactions but because no one has looked.\nGene regulatory networks encode transcriptional control relationships. Unlike PPIs, regulatory networks are inherently directed: a transcription factor activates or represses its targets, not vice versa. Sources include ChIP-seq experiments that identify transcription factor binding sites, chromatin accessibility data (ATAC-seq, DNase-seq) that reveals active regulatory regions, and chromosome conformation capture (Hi-C) that maps enhancer-promoter contacts. Databases like ENCODE and the Roadmap Epigenomics Project provide regulatory annotations across cell types, though coverage varies dramatically by tissue. Computational methods infer regulatory edges from expression correlations or sequence motifs, but such predictions contain substantial false positives and miss context-specific interactions.\nPathway and metabolic networks organize biochemical knowledge into structured representations. KEGG, Reactome, and WikiPathways curate reactions, enzymatic steps, and signaling cascades into hierarchical graphs where nodes can represent genes, proteins, metabolites, or abstract pathway concepts. These networks encode decades of molecular biology knowledge but reflect historical research priorities: metabolism and signal transduction are well-characterized, while more recently discovered processes like autophagy or RNA modification have sparser coverage.\nKnowledge graphs extend beyond molecular interactions to encode relationships among genes, diseases, drugs, phenotypes, and other biomedical entities. Resources like Hetionet, the Unified Medical Language System (UMLS), and disease-gene association databases (DisGeNET, OMIM) provide heterogeneous graphs linking diverse entity types. A knowledge graph might encode that gene X is associated with disease Y, drug Z targets gene X, and disease Y presents with symptom W. Such graphs enable multi-hop reasoning: if a new drug binds protein A, and protein A interacts with protein B, and protein B is implicated in disease C, then the drug becomes a candidate for disease C.\nSpatial and cell-cell interaction graphs arise from spatially resolved transcriptomics and imaging data. Nodes represent cells or spatial locations, edges encode physical proximity or inferred ligand-receptor communication. These graphs capture tissue organization invisible to bulk or even single-cell measurements, enabling questions about how spatial context influences cell behavior.\n\n\n19.1.2 Biases and Limitations\nAll biological networks share systematic biases that affect downstream modeling. Well-studied genes appear as highly connected hubs not necessarily because they have more interactions but because researchers have investigated them more thoroughly. This ascertainment bias means that GNNs trained on network structure may primarily learn to propagate signals toward well-characterized genes, potentially missing novel biology in peripheral network regions.\nNetwork incompleteness creates particular challenges for message passing algorithms. If a critical interaction is missing, information cannot flow across that gap. If a spurious interaction is present, noise propagates where it should not. These issues are especially acute for less-studied organisms, tissues, or disease contexts where network coverage is sparse.\nThe distinction between physical and functional associations matters for interpretation. A protein-protein interaction might represent stable complex membership, transient signaling, or indirect association through shared binding partners. Different edge types may warrant different treatment by graph models, but many databases conflate these categories or provide insufficient metadata to distinguish them.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span>"
    ]
  },
  {
    "objectID": "p4-ch18-networks.html#graph-neural-network-fundamentals",
    "href": "p4-ch18-networks.html#graph-neural-network-fundamentals",
    "title": "18  Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning",
    "section": "19.2 Graph Neural Network Fundamentals",
    "text": "19.2 Graph Neural Network Fundamentals\n\n19.2.1 Why Message Passing?\nThe challenge of learning from graph-structured data lies in the irregular topology: unlike images (regular grids) or sequences (linear chains), graphs have variable-degree nodes, no inherent ordering, and complex connectivity patterns. Classical approaches computed hand-crafted features such as degree centrality, clustering coefficients, or shortest path statistics, then fed these to standard machine learning models. Such features capture useful properties but cannot adapt to task-specific patterns.\nMessage passing provides a learnable alternative. The core intuition is local information exchange: each node should update its representation based on what its neighbors know. By iterating this process across multiple layers, information propagates across the graph, allowing nodes to incorporate signals from increasingly distant parts of the network.\nFormally, at layer \\(\\ell\\), each node \\(i\\) maintains a hidden state \\(\\mathbf{h}_i^{(\\ell)}\\). A message passing layer computes, for each edge from neighbor \\(j\\) to node \\(i\\), a message:\n\\[\n\\mathbf{m}_{ij}^{(\\ell)} = \\phi_m\\left(\\mathbf{h}_i^{(\\ell)}, \\mathbf{h}_j^{(\\ell)}, \\mathbf{e}_{ij}\\right)\n\\]\nwhere \\(\\phi_m\\) is a learned function and \\(\\mathbf{e}_{ij}\\) represents edge features. The node then aggregates messages from all neighbors and updates its state:\n\\[\n\\mathbf{h}_i^{(\\ell+1)} = \\phi_h\\left(\\mathbf{h}_i^{(\\ell)}, \\bigoplus_{j \\in \\mathcal{N}(i)} \\mathbf{m}_{ij}^{(\\ell)}\\right)\n\\]\nwhere \\(\\mathcal{N}(i)\\) denotes neighbors of node \\(i\\) and \\(\\bigoplus\\) is a permutation-invariant aggregation (sum, mean, max, or attention-weighted combination). The aggregation must be permutation-invariant because neighbors have no inherent ordering.\nAfter \\(L\\) layers, a node’s representation incorporates information from all nodes within \\(L\\) hops. For biological networks, this means a gene’s learned embedding can reflect not only its own features but signals from interaction partners, their partners, and so on, capturing pathway-level and module-level context.\n\n\n19.2.2 Canonical Architectures\nSeveral GNN architectures have become standard tools for biological applications, each with distinct design choices.\nGraph Convolutional Networks (GCN) perform normalized neighborhood averaging followed by linear transformation and nonlinearity. GCNs are computationally efficient and conceptually straightforward but suffer from over-smoothing when stacked deeply: repeated averaging causes node representations to converge, losing the discriminative signal that distinguishes different network positions.\nGraphSAGE addresses scalability by learning aggregation functions that operate on sampled neighborhoods rather than the full neighbor set. This enables mini-batch training on large graphs and provides inductive capability: the model can generate embeddings for nodes not seen during training by applying learned aggregators to their neighborhoods. For biological networks that grow as new genes are characterized, this generalization is valuable.\nGraph Attention Networks (GAT) introduce attention mechanisms to weight neighbors differentially based on relevance. Rather than treating all interactions equally, GAT learns compatibility scores between node pairs, allowing the model to focus on the most informative relationships. In biological contexts, attention weights are often interpreted as highlighting key regulatory connections or critical binding partners, though such interpretations require careful validation.\nGraph Transformers extend transformer architectures to graphs, replacing local message passing with structured or global attention. Some variants attend over all node pairs with positional encodings derived from graph structure (shortest paths, Laplacian eigenvectors); others restrict attention to k-hop neighborhoods. These architectures blur the boundary between sequence and graph models, potentially capturing long-range dependencies that multi-layer message passing struggles to propagate.\nThe expressiveness of GNNs is bounded by their ability to distinguish different graph structures. Theoretical analysis connects standard message passing to the Weisfeiler-Lehman graph isomorphism test, revealing that certain graph structures remain indistinguishable regardless of the number of layers. For most biological applications, this theoretical limitation is less constraining than practical issues of data quality, training efficiency, and interpretability.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span>"
    ]
  },
  {
    "objectID": "p4-ch18-networks.html#foundation-model-embeddings-as-node-features",
    "href": "p4-ch18-networks.html#foundation-model-embeddings-as-node-features",
    "title": "18  Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning",
    "section": "19.3 Foundation Model Embeddings as Node Features",
    "text": "19.3 Foundation Model Embeddings as Node Features\n\n19.3.1 The Integration Principle\nThe central architectural insight for genomic graph learning is that foundation models and graph neural networks operate at complementary levels of abstraction. Sequence-based foundation models excel at extracting biological information from linear sequences: ESM-2 learns evolutionary constraints and structural propensities from protein sequences; DNABERT and its successors capture regulatory motifs and sequence grammar; single-cell foundation models like scGPT learn cell state representations from expression profiles. These representations encode rich biological knowledge but operate on individual entities without explicit relational information.\nGraph neural networks excel at learning from relational structure but require informative node features to propagate. When node features are uninformative (simple one-hot encodings or scalar expression values), message passing can only learn from network topology. When node features carry substantial biological signal, message passing can refine and contextualize that information based on network position.\nCombining these approaches follows a natural two-stage pattern. First, apply a foundation model to each entity in the graph to generate initial node embeddings. For a protein-protein interaction network, run ESM-2 on each protein sequence; for a gene regulatory network, use DNA embeddings for regulatory elements and protein embeddings for transcription factors; for a cell graph, apply scGPT to generate cell state representations. Second, train a GNN on these embeddings using the biological graph structure, allowing message passing to integrate entity-level representations with relational context.\nThis combination yields capabilities that neither component achieves alone. The foundation model provides rich, transferable features that would require massive labeled datasets to learn from scratch. The GNN provides relational reasoning that sequence models cannot perform. A protein’s druggability depends both on intrinsic properties (binding pocket geometry, expression pattern) that ESM captures and on network context (pathway position, interaction partners) that the GNN integrates.\n\n\n19.3.2 Practical Integration Patterns\nSeveral integration patterns have emerged in practice. The simplest approach freezes foundation model weights and treats embeddings as fixed features, training only the GNN layers. This is computationally efficient and prevents catastrophic forgetting of pretrained knowledge but limits the model’s ability to adapt representations to the specific task.\nJoint fine-tuning allows gradients to flow through both the GNN and (parts of) the foundation model, enabling end-to-end optimization. This typically requires careful learning rate scheduling, with smaller updates to foundation model parameters and larger updates to GNN layers. The approach can improve performance when sufficient task-specific data is available but risks overfitting and requires substantially more computation.\nAdapter-based integration inserts small trainable modules between foundation model layers or at the interface between foundation model outputs and GNN inputs. This provides task adaptation with modest parameter overhead, avoiding full fine-tuning costs while retaining flexibility.\nMulti-scale integration uses foundation model representations at multiple granularities. For proteins, one might extract both per-residue embeddings (capturing local structure) and sequence-level embeddings (capturing global properties), concatenating these as node features. For regulatory networks, one might combine nucleotide-level DNA embeddings with region-level chromatin accessibility predictions.\nThe choice of integration pattern depends on data availability, computational resources, and the degree of distribution shift between foundation model pretraining and the target application. For well-characterized systems with substantial labeled data, joint fine-tuning may be warranted. For novel organisms or rare diseases with limited labels, frozen embeddings with simple GNN layers often generalize better.\n\n\n19.3.3 Evidence for the Integration Benefit\nEmpirical studies consistently demonstrate that foundation model embeddings improve GNN performance on biological tasks. In protein function prediction, ESM embeddings combined with PPI network GNNs substantially outperform either sequence-only or network-only baselines (gligorijevic_structure_2021?). The improvement is particularly pronounced for proteins with few characterized interaction partners, where network structure alone provides limited signal but sequence features carry evolutionary information.\nFor disease gene prioritization, combining DNA and protein foundation model embeddings with multi-relational GNNs over heterogeneous biological networks improves ranking of causal genes from GWAS loci (schulte_schrepping_analysis_2020?). The foundation model features help distinguish genes with similar network positions based on sequence-level functional signals.\nIn single-cell analysis, scGPT embeddings combined with cell-cell communication graphs enable more accurate prediction of perturbation effects than either component alone (Cui et al. 2024). The cell embeddings capture transcriptional state, while the graph structure encodes spatial and molecular interaction context.\nThese results suggest that the integration principle generalizes across biological domains. The specific foundation models and graph types vary, but the architectural pattern (rich entity embeddings + relational structure + message passing) consistently outperforms simpler alternatives.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span>"
    ]
  },
  {
    "objectID": "p4-ch18-networks.html#applications",
    "href": "p4-ch18-networks.html#applications",
    "title": "18  Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning",
    "section": "19.4 Applications",
    "text": "19.4 Applications\n\n19.4.1 Disease Gene Prioritization\nGenome-wide association studies identify genomic loci associated with disease risk but rarely pinpoint causal genes (Chapter 3). A typical GWAS locus contains dozens of genes, most of which are passengers linked to the true causal variant through linkage disequilibrium. Identifying which gene(s) mediate the association requires integrating functional evidence with genetic signal.\nNetwork-based prioritization leverages the observation that disease genes cluster in biological networks. If a GWAS locus contains genes A, B, and C, and gene B interacts with five known disease genes while A and C interact with none, gene B becomes a stronger causal candidate. Graph neural networks formalize and extend this intuition, learning to propagate disease labels through networks and score candidate genes based on their network context.\nThe integration with foundation models strengthens this approach. Rather than relying solely on network topology, which favors well-studied hub genes, the model can assess each candidate’s intrinsic functional properties through sequence embeddings. A gene with protein features characteristic of disease-relevant functions (membrane localization, DNA binding, signaling domains) receives higher scores even if its network position is peripheral. This helps mitigate the ascertainment bias toward well-characterized genes that plagues purely topological methods.\nClinical applications include rare disease diagnosis, where patient exome sequencing identifies hundreds of candidate variants and network-based scoring helps prioritize which genes to investigate further (Chapter 26). The approach also supports drug target identification by highlighting genes whose network position and functional properties make them amenable to therapeutic modulation (Chapter 27).\n\n\n19.4.2 Drug-Target Interaction Prediction\nIdentifying which proteins a drug binds is fundamental to understanding mechanism and predicting side effects. Experimental screening of drug-target pairs is expensive and incomplete; computational prediction can prioritize candidates for validation.\nDrug-target interaction prediction naturally fits a graph framework. Construct a heterogeneous graph with drug nodes, protein nodes, and edges representing known interactions. Node features for proteins come from sequence foundation models; node features for drugs come from molecular encodings (fingerprints, learned representations from molecular graphs). Train a GNN to predict missing edges, learning which drug and protein features, combined with network context, indicate likely binding.\nThe foundation model integration is critical here. Protein embeddings from ESM capture binding pocket characteristics, domain structure, and evolutionary constraint that influence druggability. The graph structure provides context: if a drug binds protein A, and protein A participates in complex with protein B, then the drug may also affect protein B’s function. Multi-relational GNNs can learn different propagation patterns for different edge types (physical binding versus pathway membership versus sequence similarity), improving prediction accuracy.\nThis application connects to broader drug discovery workflows (Chapter 27), where target identification is one component of a multi-stage pipeline. GNN-based predictions provide hypotheses for experimental validation, accelerating the search for novel therapeutic targets.\n\n\n19.4.3 Pathway and Module Analysis\nUnderstanding complex diseases often requires moving beyond individual genes to identify dysregulated pathways or functional modules. Graph neural networks provide natural tools for learning such modular structure.\nGiven a pathway graph with nodes annotated by patient-specific multi-omic measurements, a GNN can learn to predict clinical outcomes (disease subtype, treatment response, survival). After training, attention weights or gradient-based attribution highlight which edges and nodes most influence predictions. These highlighted subgraphs often correspond to known disease-relevant pathways but can also reveal novel modules whose coherence was not previously appreciated.\nHierarchical GNNs extend this approach by explicitly learning multi-scale structure. Pooling operations coarsen the graph by grouping nodes into super-nodes, creating a hierarchy from individual genes to modules to pathways to biological processes. Each level of this hierarchy provides a different granularity of analysis, aligning with biological intuition that disease perturbations can occur at multiple scales.\nFor rare diseases with patient-specific perturbation patterns, module-level analysis is particularly valuable. Rather than asking whether a patient’s variants affect canonical pathways, a GNN can score pathway activity based on the patient’s specific genetic background, enabling more personalized interpretation (Chapter 26).\n\n\n19.4.4 Cell Type and State Annotation\nSingle-cell foundation models generate rich representations of individual cells (Chapter 16), but many biological questions involve relationships between cells: which cells communicate, how spatial neighborhoods influence behavior, which cell types co-occur in disease states.\nGraph neural networks over cell-cell interaction graphs enable several applications. Cell type annotation propagates labels from well-characterized cells to ambiguous ones based on expression similarity and spatial proximity. Perturbation response prediction models how signals from perturbed cells propagate to neighbors. Tissue region classification identifies coherent spatial domains (tumor, stroma, immune infiltrate) based on local cell compositions.\nThe foundation model integration follows the standard pattern: scGPT or similar models generate cell embeddings, spatial proximity or inferred ligand-receptor interactions define edges, and GNN message passing refines cell representations based on neighborhood context. The resulting embeddings capture both intrinsic cell state and extrinsic spatial/communicative context, enabling predictions that purely expression-based or purely spatial models cannot make.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span>"
    ]
  },
  {
    "objectID": "p4-ch18-networks.html#practical-considerations",
    "href": "p4-ch18-networks.html#practical-considerations",
    "title": "18  Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning",
    "section": "19.5 Practical Considerations",
    "text": "19.5 Practical Considerations\n\n19.5.1 Graph Construction Quality\nThe impact of graph construction choices cannot be overstated. A GNN can only learn from relationships encoded in its input graph; missing edges prevent information flow, spurious edges introduce noise, and biased edge sets propagate ascertainment artifacts.\nSource selection involves tradeoffs between precision and completeness. Curated databases like BioGRID provide high-confidence interactions but miss most true relationships. Computational predictions from STRING or co-expression analysis are more comprehensive but noisier. The appropriate choice depends on the downstream task: high-precision networks may be preferable when false positives are costly, while high-recall networks enable discovery of novel biology at the risk of chasing artifacts.\nThresholding decisions determine network density. Confidence scores or distance metrics allow continuous edge weights, but many GNN implementations require discrete edges or work better with relatively sparse graphs. Cross-validation over threshold values or principled selection criteria (target edge density, ensure graph connectivity) help navigate this choice.\nFor heterogeneous graphs, schema design (which node types exist, which edge types connect them) encodes strong assumptions about relevant biology. A knowledge graph that separates genes, transcripts, and proteins as distinct node types enables fine-grained reasoning but requires more training data than a simpler gene-only representation.\n\n\n19.5.2 Scalability and Mini-Batching\nBiological graphs range from thousands of nodes (a single-patient cell graph) to millions (a comprehensive knowledge graph or large spatial transcriptomics dataset). Full-batch training, where the entire graph is processed simultaneously, becomes infeasible at scale due to memory constraints.\nMini-batching strategies partition computation into manageable pieces. Neighborhood sampling (GraphSAGE-style) restricts message passing to a fixed sample of neighbors per node, enabling node-level mini-batches. Subgraph sampling trains on induced subgraphs corresponding to meaningful units (individual pathways, tissue regions, patient subsets). Cluster-based training partitions the graph into communities, processes each independently, and handles cross-cluster edges in a second pass.\nFor foundation model integration, computational cost compounds: generating embeddings for millions of proteins or cells may itself be expensive. Pre-computing and caching embeddings is often practical, decoupling the foundation model forward pass from GNN training. When embeddings must be computed on-the-fly (for dynamic features or joint fine-tuning), careful batching and gradient checkpointing become essential.\n\n\n19.5.3 Robustness to Noise and Missingness\nAll biological networks contain errors. Experimental methods for detecting interactions have false positive and false negative rates; computational predictions rely on imperfect proxies; even curated databases contain mistakes. GNNs must tolerate this noise to be practically useful.\nEdge dropout during training randomly masks edges, forcing the model to not rely on any single interaction. This improves robustness to missing or incorrect edges and serves as a form of regularization. Node dropout similarly masks node features or entire nodes, preventing overfitting to well-connected hubs.\nEnsemble methods train multiple GNNs on different network subsamples or with different random initializations, aggregating predictions to reduce variance from network noise. Bayesian GNNs provide uncertainty estimates that flag low-confidence predictions for manual review.\nEvaluation should explicitly assess robustness by testing on held-out edges, nodes from poorly characterized network regions, or networks constructed from different data sources than training. A model that performs well only on hub genes or well-characterized interactions may fail in precisely the scenarios where computational prediction is most needed.\n\n\n19.5.4 Interpretation and Validation\nA key advantage of graph models is interpretability: the graph structure itself provides a scaffold for understanding predictions. Several techniques extract biological insight from trained GNNs.\nAttention weight analysis in GAT and graph transformer models indicates which neighbors most influenced each node’s prediction. Aggregating attention across predictions can highlight critical edges or subgraphs, suggesting which interactions drive model behavior.\nGradient-based attribution computes how predictions change with respect to node or edge features, identifying which parts of the input most affect outputs. Integrated gradients and similar methods provide smoother, more reliable attributions than raw gradients.\nCounterfactual analysis systematically removes edges, masks nodes, or perturbs features and observes prediction changes. This reveals which graph elements are necessary for specific predictions and can identify model vulnerabilities.\nEmbedding visualization projects learned node representations into two dimensions using UMAP or t-SNE, revealing clusters that may correspond to functional categories, cell types, or disease subtypes. Comparing embeddings across conditions identifies network regions that show context-specific changes.\nInterpretation is not an afterthought but a central goal. The most impactful applications are those where GNN predictions generate testable hypotheses about biological mechanism, ultimately validated by experiment. Attention weights highlighting a regulatory edge or gradient attribution implicating a signaling pathway should prompt follow-up experiments, not immediate clinical action.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span>"
    ]
  },
  {
    "objectID": "p4-ch18-networks.html#limitations-and-open-challenges",
    "href": "p4-ch18-networks.html#limitations-and-open-challenges",
    "title": "18  Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning",
    "section": "19.6 Limitations and Open Challenges",
    "text": "19.6 Limitations and Open Challenges\n\n19.6.1 The Study Bias Problem\nNetwork-based methods inherit the biases of their input networks. Well-studied genes appear as hubs; poorly characterized genes are peripheral or disconnected. GNNs trained on such networks learn to propagate signals toward well-characterized genes, effectively recapitulating rather than extending existing knowledge.\nThis creates particular problems for disease gene discovery, where the goal is often to identify previously unrecognized genes. A model that consistently ranks known disease genes highly may simply be exploiting their network prominence rather than learning generalizable disease biology. Careful evaluation on temporal holdouts (genes characterized after training data was assembled) or stratified by network degree can reveal whether models truly generalize.\nMitigation strategies include degree-corrected training objectives, explicit modeling of ascertainment bias, or alternative network constructions that reduce dependence on historical research focus. None fully solves the problem, which reflects fundamental data limitations rather than algorithmic shortcomings.\n\n\n19.6.2 Causality Versus Association\nNetwork edges typically represent associations (two proteins bind, two genes correlate) rather than causal relationships (perturbing gene A changes gene B). GNNs learn to exploit correlational patterns, which may not correspond to causal mechanisms.\nFor applications like drug target identification, this distinction matters enormously. A gene that correlates with disease through confounding may be a poor target despite high network-based prioritization scores. Integrating causal inference methods with graph learning is an active research area, but current GNN applications should be interpreted as identifying associations worthy of experimental follow-up rather than establishing causal relationships.\n\n\n19.6.3 Negative Data and Class Imbalance\nMost biological network datasets encode only positive relationships: known interactions, confirmed regulatory edges, documented associations. The absence of an edge may indicate true non-interaction or simply lack of evidence. This creates severe class imbalance for edge prediction tasks and makes negative sampling strategies critical.\nRandom negative sampling (assuming absent edges represent non-interactions) is common but biologically unrealistic. More sophisticated approaches sample negatives with matched properties (same degree distribution, similar node features) to create harder and more meaningful contrasts. Evaluation should report performance separately on different negative sampling schemes to assess whether models generalize beyond easily discriminated negatives.\n\n\n19.6.4 Distribution Shift\nA GNN trained on one biological network (human PPI from STRING) may not transfer to another (mouse regulatory network, patient-specific spatial graph). Foundation model embeddings help by providing transferable features, but network structure differences can still break performance.\nCross-species transfer is particularly challenging: network topology, edge type distributions, and gene function may all differ. Cross-tissue or cross-disease transfer poses similar challenges. Explicit domain adaptation methods, multi-task training across related networks, or foundation model fine-tuning on target domains can help but add complexity.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span>"
    ]
  },
  {
    "objectID": "p4-ch18-networks.html#summary",
    "href": "p4-ch18-networks.html#summary",
    "title": "18  Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning",
    "section": "19.7 Summary",
    "text": "19.7 Summary\nGraphs and graph neural networks provide essential tools for reasoning about biological relationships that sequence models cannot directly capture. By representing entities as nodes and interactions as edges, graphs enable questions about network propagation, pathway enrichment, and spatial organization. Message passing architectures learn task-specific transformations of graph structure, integrating node features with relational context through iterative neighborhood aggregation.\nThe central insight of this chapter is that GNNs operate at a complementary level of abstraction to sequence-based foundation models. Foundation models provide rich, learned representations of biological entities; GNNs provide relational reasoning over those representations. Combining them follows a natural architectural pattern: generate embeddings with foundation models, then refine them with graph neural networks. This integration yields capabilities that neither component achieves alone, improving performance across applications from disease gene prioritization to drug target prediction to spatial tissue analysis.\nPractical deployment requires attention to graph construction (source selection, thresholding, schema design), scalability (sampling strategies, efficient implementations), robustness (handling noise and missingness), and interpretation (attention analysis, attribution, visualization). The well-documented biases of biological networks (study bias toward well-characterized genes, correlation versus causation, class imbalance) propagate through GNN predictions and require explicit consideration in evaluation and application.\nNetworks and graphs form one modality in the broader multi-omics integration strategies discussed in Chapter 19. The combination of foundation model embeddings, graph structure, and multi-modal data integration provides increasingly comprehensive representations of biological systems, enabling the clinical applications explored in ?sec-clinical-risk and Chapter 26. As both foundation models and biological networks continue to expand, the integration patterns established here will remain central to genomic AI.\n\n\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. 2024. “scGPT: Toward Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI.” Nature Methods 21 (8): 1470–80. https://doi.org/10.1038/s41592-024-02201-0.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Gap Analysis: Chapter 18 Networks and Graph-Based Reasoning</span>"
    ]
  },
  {
    "objectID": "p4-ch19-multi-omics.html",
    "href": "p4-ch19-multi-omics.html",
    "title": "19  Multi-Omics Integration",
    "section": "",
    "text": "19.1 The Limits of Single-Modality Models\nA patient presents with symptoms suggesting an autoimmune condition, but standard genetic testing reveals no pathogenic variants in known disease genes. Transcriptomic profiling shows aberrant expression of inflammatory pathways, methylation analysis indicates hypomethylation at immune regulatory loci, and proteomic data reveals elevated cytokine levels. No single data type provides a diagnosis, yet the pattern across modalities points unmistakably toward systemic lupus erythematosus. This scenario, increasingly common in research hospitals with multi-omics capabilities, illustrates both the promise and the challenge: each molecular layer captures part of the biological story, but the complete narrative emerges only through integration. The central tension of multi-omics modeling is that combining data types should improve prediction, yet naive combination often degrades performance through noise amplification, batch effects, and the curse of dimensionality.\nThis chapter examines strategies for integrating multiple data types into unified representations. We compare early, intermediate, and late fusion approaches and their tradeoffs for different applications. We survey multi-omics foundation models that learn joint representations across genomics, transcriptomics, proteomics, and other modalities. We extend to clinical integration, where electronic health records, imaging, and molecular data must be combined for patient-level prediction. We develop a systems biology perspective that traces the information cascade from genetic variants through molecular intermediates to clinical phenotypes. Throughout, we address the practical challenges of missing modalities, batch effects, and the gap between multi-omics potential and deployment reality.\nEach molecular layer tells an incomplete story. DNA sequence is static; it encodes potential but not state. A variant’s presence says nothing about whether the gene is expressed, whether the protein is active, or whether the pathway is perturbed. Transcriptomic data captures expression state but misses post-transcriptional regulation, protein modifications, and metabolic flux. Proteomic measurements reveal protein abundance but not necessarily activity or localization. Methylation profiles indicate epigenetic state but require expression data to understand functional consequences.\nThe incompleteness becomes concrete when modeling complex traits. Genome-wide association studies explain perhaps 10-20% of heritability for most common diseases through identified variants. Adding expression quantitative trait loci (eQTLs) improves fine-mapping by suggesting which variants affect gene expression, but many causal mechanisms operate through splicing, translation, or post-translational modification rather than expression level. Single-cell RNA sequencing reveals cellular heterogeneity invisible to bulk measurements, but the same cell cannot simultaneously undergo RNA-seq and ATAC-seq, forcing computational integration across modalities measured in different cells.\nConsider the challenge of predicting drug response. Germline variants in drug-metabolizing enzymes explain some inter-individual variation, but tumor-specific somatic mutations, expression programs, and microenvironment all influence therapeutic efficacy. A genomics-only model sees the inherited component; a transcriptomics-only model sees the current expression state; neither captures the full picture. Multi-omics integration promises to bridge these gaps by learning representations that span molecular layers.\nThe promise comes with caveats. Adding modalities increases the number of parameters that must be estimated, potentially worsening overfitting when sample sizes are limited. Different modalities have different noise characteristics, batch structures, and missingness patterns. The same patient’s measurements across platforms may not align perfectly due to sample handling, timing, or technical variation. Naive concatenation of features often performs worse than single-modality models because the signal-to-noise ratio degrades when noisy features outnumber informative ones.\nThese challenges motivate careful consideration of integration strategy. The question is not whether to integrate, but how.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "p4-ch19-multi-omics.html#integration-strategies-and-their-tradeoffs",
    "href": "p4-ch19-multi-omics.html#integration-strategies-and-their-tradeoffs",
    "title": "19  Multi-Omics Integration",
    "section": "19.2 Integration Strategies and Their Tradeoffs",
    "text": "19.2 Integration Strategies and Their Tradeoffs\nThree broad strategies have emerged for combining multi-omics data, each with distinct strengths and limitations.\n\n19.2.1 Early Fusion\nEarly fusion concatenates features from multiple modalities before any modeling, creating a single high-dimensional input vector that contains genomic variants, expression values, methylation levels, and any other available measurements. A classifier or regressor then learns directly from this concatenated representation.\nThe appeal of early fusion lies in its simplicity and flexibility. Any downstream model architecture can operate on concatenated features, from linear regression to deep neural networks. The model can learn arbitrary interactions between features from different modalities, since all information is present in the input. Implementation requires only normalization and alignment of features across samples.\nThe limitations become apparent at scale. Dimensionality explodes when combining genome-wide variants (millions of features), gene expression (tens of thousands of genes), methylation (hundreds of thousands of CpG sites), and protein abundance (thousands of proteins). Most samples have far fewer observations than features, creating severe overfitting risk. Regularization helps but cannot fully compensate when the ratio of features to samples exceeds practical bounds.\nMissing data creates additional complications. If any modality is missing for a sample, early fusion requires either excluding that sample (reducing effective sample size) or imputing the missing modality (introducing noise and potential bias). Since multi-omics studies often have incomplete overlap between modalities, with some patients having genomics and transcriptomics but not proteomics, early fusion frequently operates on substantially reduced cohorts.\nScale differences between modalities pose another challenge. Expression values span orders of magnitude; methylation beta values range from zero to one; variant encodings are typically binary. Without careful normalization, modalities with larger variance can dominate the learned representation regardless of biological relevance. Batch effects within each modality add further complexity, since batch correction must precede concatenation but may interact with cross-modal relationships.\nDespite these limitations, early fusion remains appropriate when sample sizes are large relative to feature counts, when all modalities are available for all samples, and when the downstream task is well-defined enough to guide feature selection. Biobank-scale studies with thousands of participants and focused feature sets can succeed with early fusion approaches.\n\n\n19.2.2 Late Fusion\nLate fusion trains separate models for each modality and combines their predictions at the output level. A genomics model produces a risk score; a transcriptomics model produces another risk score; an ensemble method or meta-learner combines these modality-specific predictions into a final output.\nThis approach handles missing modalities gracefully. If a patient lacks proteomic data, the proteomics model simply does not contribute to the ensemble. Sample sizes for each modality-specific model can differ, since training requires only samples with that modality rather than complete multi-omics profiles. Each modality can use whatever architecture works best for its data type: deep networks for imaging, gradient boosting for tabular omics, convolutional architectures for sequence.\nLate fusion cannot capture cross-modal interactions at the feature level. If a variant’s effect on disease depends on expression level of a regulatory gene, neither the genomics model nor the transcriptomics model alone can detect this interaction. The ensemble sees only the modality-specific predictions, not the underlying features. This limitation is fundamental: late fusion assumes that each modality provides independent signal that can be additively combined.\nThe assumption of independence often fails in biological systems. Gene expression depends on genetic variants through eQTLs. Protein levels depend on both transcription and post-transcriptional regulation. Methylation states influence and are influenced by transcription. The molecular layers are not independent information sources but coupled components of a dynamic system. Late fusion ignores this coupling.\nCalibration presents a practical challenge. For ensemble predictions to be meaningful, the modality-specific models must produce well-calibrated probability estimates. If the genomics model is overconfident and the transcriptomics model is underconfident, naive averaging produces biased predictions. Calibration techniques help but add complexity to the modeling pipeline.\nLate fusion works well when modalities genuinely provide independent signals, when sample sizes for each modality differ substantially, or when interpretability requires understanding each modality’s contribution separately. Clinical deployment often favors late fusion because it gracefully handles the reality that not all patients will have all measurements.\n\n\n19.2.3 Intermediate Fusion\nIntermediate fusion learns modality-specific encoders that map each data type into a shared latent space, then operates on the aligned representations for downstream tasks. This approach combines the flexibility of early fusion with the robustness of late fusion.\nEach modality has its own encoder architecture tailored to its characteristics. A variational autoencoder might encode single-cell expression data, handling sparsity and dropout noise. A convolutional network might process methylation profiles along chromosomal coordinates. A graph neural network might encode protein interaction data. These diverse architectures share nothing except their output dimensionality: all encoders produce embeddings in a common latent space.\nAlignment between modalities is encouraged through multiple mechanisms. Reconstruction losses require each encoder’s latent representation to support decoding back to the original features, ensuring that the embeddings retain modality-specific information. Contrastive terms pull together representations of the same biological entity across modalities: the expression embedding for a cell should be similar to the ATAC-seq embedding for the same cell. Graph constraints enforce consistency with known biological relationships: genes connected in interaction networks should have similar embeddings.\nThe shared latent space enables cross-modal reasoning. A classifier operating on the shared space can learn interactions between genomic and transcriptomic features, since both are present in the same representation. Transfer becomes possible: a model trained on expression data can be applied to samples with only ATAC-seq by encoding through the ATAC-seq encoder into the shared space.\nMissing modalities no longer require imputation or exclusion. If a sample lacks proteomics, only the available encoders fire, producing a partial representation in the shared space. The downstream model operates on whatever representation is available, degrading gracefully as modalities are missing rather than failing entirely.\nGLUE, introduced in Chapter 16 for single-cell multi-omics integration, exemplifies this approach. Separate variational autoencoders encode RNA-seq and ATAC-seq data into a shared cell embedding space. A feature graph links ATAC-seq peaks to genes based on genomic proximity and transcription factor binding, providing biological constraints on the alignment. The result enables integration of measurements from different cells, not just different modalities in the same cell.\nIntermediate fusion dominates modern multi-omics deep learning because it balances flexibility with robustness. The modality-specific encoders can be pretrained on large single-modality datasets, then fine-tuned for alignment. New modalities can be added by training new encoders without retraining existing components. The shared space provides a natural target for interpretation and visualization.\nThe approach is not without limitations. The quality of alignment depends heavily on the training objective and the availability of paired samples where multiple modalities are measured in the same biological entity. Without sufficient anchoring, the shared space may fail to capture true biological correspondence. Hyperparameter choices for balancing reconstruction against alignment losses require careful tuning.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "p4-ch19-multi-omics.html#multi-omics-foundation-models",
    "href": "p4-ch19-multi-omics.html#multi-omics-foundation-models",
    "title": "19  Multi-Omics Integration",
    "section": "19.3 Multi-Omics Foundation Models",
    "text": "19.3 Multi-Omics Foundation Models\nThe foundation model paradigm, introduced in Chapter 10, extends naturally to multi-omics settings. Rather than training task-specific models that integrate modalities for a single downstream application, multi-omics foundation models learn general-purpose representations that transfer across tasks.\n\n19.3.1 Factor-Based Integration\nMulti-Omics Factor Analysis (MOFA and its successor MOFA+) provides a probabilistic framework for learning shared and modality-specific factors from multi-omics data. The approach assumes that observed measurements across modalities can be explained by a small number of latent factors, some shared across modalities and others specific to individual data types.\nMOFA+ extends this framework to handle multiple sample groups (such as different tissues or conditions), non-Gaussian likelihoods appropriate for count data, and scalable inference for large datasets. The factors learned by MOFA+ capture sources of variation that span modalities, enabling biological interpretation: a factor that loads heavily on inflammatory genes in expression data and on hypomethylation at immune loci in methylation data suggests coordinated epigenetic-transcriptional regulation of inflammation.\nWhile MOFA+ is not a deep learning method in the strict sense, its factor-based decomposition provides a foundation for understanding what multi-omics integration should capture. The shared factors correspond to biological processes that manifest across molecular layers; the modality-specific factors capture technical variation or layer-specific biology.\n\n\n19.3.2 Deep Generative Multi-Omics Models\ntotalVI (Total Variational Inference) integrates protein abundance from CITE-seq with gene expression in single-cell data through a hierarchical Bayesian model. The approach learns a joint latent space that captures cell state while properly modeling the distinct noise characteristics of RNA and protein measurements. Protein abundance follows a negative binomial distribution with technical factors including background binding; RNA counts follow a zero-inflated negative binomial accounting for dropout.\nThe generative model structure enables imputation of missing modalities. Given RNA expression alone, totalVI can predict expected protein abundance by sampling from the learned joint distribution. This imputation is not mere correlation-based prediction but reflects the full posterior distribution over protein levels given expression.\nMultiVI extends this framework to integrate gene expression with chromatin accessibility. The model learns to align measurements from different cells, enabling construction of unified cell atlases from studies that measured different modalities. The alignment relies on the biological assumption that gene expression and chromatin state reflect the same underlying cell state, even when measured in different cells.\nThese Bayesian deep generative models exemplify intermediate fusion with principled uncertainty quantification. The posterior distributions over latent variables capture not just point estimates but confidence in the learned representations. This property becomes important for clinical applications where prediction uncertainty must inform decision-making.\n\n\n19.3.3 Contrastive Multi-Modal Learning\nContrastive learning provides another path to multi-omics integration. The CLIP model for vision-language demonstrated that contrastive objectives can align embeddings from fundamentally different data types (images and text) into a shared space. Similar approaches apply to biological modalities.\nThe contrastive objective is straightforward: embeddings of the same biological entity across modalities should be similar, while embeddings of different entities should be dissimilar. A cell’s expression embedding should be close to its methylation embedding and far from other cells’ methylation embeddings. A patient’s genomic embedding should be close to their transcriptomic embedding across the cohort.\nThis objective requires paired samples for training: the same cells or patients measured across modalities. Anchor pairs define the positive examples; negative examples come from non-matching pairs within a batch. The encoders learn to produce embeddings where cross-modal correspondence emerges from training dynamics rather than explicit feature engineering.\nContrastive approaches scale well and can incorporate foundation model encoders pretrained on single modalities. An expression encoder pretrained on millions of cells via masked gene prediction can be fine-tuned with contrastive objectives to align with an ATAC-seq encoder. The pretraining provides rich initial representations; the contrastive fine-tuning establishes cross-modal correspondence.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "p4-ch19-multi-omics.html#clinical-integration-ehr-imaging-and-molecular-data",
    "href": "p4-ch19-multi-omics.html#clinical-integration-ehr-imaging-and-molecular-data",
    "title": "19  Multi-Omics Integration",
    "section": "19.4 Clinical Integration: EHR, Imaging, and Molecular Data",
    "text": "19.4 Clinical Integration: EHR, Imaging, and Molecular Data\nThe ultimate goal of multi-omics modeling for many applications is patient-level prediction: disease risk, treatment response, prognosis. Achieving this goal requires integrating molecular measurements with clinical data that directly captures patient state and outcomes.\n\n19.4.1 Electronic Health Records as a Modality\nElectronic health records contain decades of longitudinal observations for millions of patients: diagnoses, procedures, medications, laboratory values, vital signs, clinical notes. This wealth of phenotypic information complements molecular data by capturing disease manifestation rather than molecular mechanism.\nIntegrating EHR with genomics poses distinct challenges. The data types differ fundamentally: structured codes, continuous lab values, free-text notes, and time-stamped events versus static or slowly-changing molecular measurements. Temporal structure matters: the sequence of diagnoses and treatments contains prognostic information that static snapshots miss. Missingness is informative: the absence of a laboratory test may indicate that a clinician deemed it unnecessary, which itself conveys information about patient state.\nFoundation models for EHR data learn representations from the longitudinal event sequences. These models, often based on transformer architectures that process sequences of medical codes, capture temporal dependencies and co-occurrence patterns in clinical trajectories. The resulting patient embeddings encode disease state and prognosis in a form amenable to integration with molecular data.\nCombining EHR embeddings with genomic features requires handling the different temporal scales. Genetic variants are constant throughout life; EHR observations accumulate over years. The integration must determine which clinical observations are relevant to a given molecular measurement, accounting for the time between sample collection and clinical events.\n\n\n19.4.2 Imaging Integration\nMedical imaging provides spatial information that molecular assays lack. A CT scan reveals tumor location, size, and heterogeneity; histopathology slides show cellular morphology and tissue architecture; MRI captures organ structure and function. These spatial data complement molecular measurements that aggregate over dissected tissue regions.\nRadiogenomics links imaging features to genetic and molecular characteristics. Glioblastoma tumors with specific imaging signatures have distinct methylation patterns and expression programs. Radiomic features extracted from CT scans correlate with mutational burden and immune infiltration in lung cancer. These associations enable prediction of molecular state from non-invasive imaging, potentially guiding treatment decisions when biopsy is impractical.\nFoundation models for medical imaging, pretrained on millions of scans through self-supervised objectives, provide rich representations for downstream tasks. Integrating these imaging embeddings with molecular data follows the intermediate fusion paradigm: modality-specific encoders produce representations in a shared latent space where multi-modal classifiers operate.\nThe integration must account for correspondence between imaging regions and molecular samples. A tumor may be molecularly heterogeneous, with different subclones in different spatial locations. A biopsy samples one location; imaging captures the entire lesion. Alignment requires either spatial registration of biopsy location to imaging coordinates or acceptance that the correspondence is imperfect.\n\n\n19.4.3 Multi-Modal Clinical Prediction Models\nCombining EHR, imaging, and molecular data for clinical prediction follows the intermediate fusion pattern. Each data type has a specialized encoder: a transformer for longitudinal EHR events, a vision encoder for imaging, domain-specific encoders for expression, methylation, and other molecular modalities. All encoders produce embeddings in a common patient representation space.\nThe training objective typically combines modality-specific reconstruction losses with alignment terms that encourage consistency across data types. A patient’s EHR embedding should be predictive of their molecular state; their imaging embedding should be consistent with their clinical trajectory. Downstream classifiers for outcomes like survival, treatment response, or disease progression operate on the combined representation.\nMissing modalities are common in clinical settings. Not all patients have genomic data; imaging may be unavailable for some conditions; the depth of EHR history varies by healthcare system and patient engagement. Multi-modal clinical models must handle this missingness gracefully, producing useful predictions from whatever data are available while leveraging cross-modal information when present.\nThe clinical deployment path for such models requires validation on external cohorts, prospective evaluation, and regulatory clearance. These practical considerations, addressed in ?sec-clinical-risk, shape model development from the outset. A model that performs well on a research cohort but requires modalities unavailable in clinical workflows provides little value.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "p4-ch19-multi-omics.html#the-systems-view-from-variant-to-phenotype",
    "href": "p4-ch19-multi-omics.html#the-systems-view-from-variant-to-phenotype",
    "title": "19  Multi-Omics Integration",
    "section": "19.5 The Systems View: From Variant to Phenotype",
    "text": "19.5 The Systems View: From Variant to Phenotype\nMulti-omics integration gains conceptual clarity from a systems biology perspective that traces information flow from genetic variation through molecular intermediates to clinical phenotypes. This cascade view organizes the molecular layers into a causal hierarchy and identifies where integration should occur.\n\n19.5.1 The Information Cascade\nGenetic variants are the starting point: heritable differences in DNA sequence that perturb downstream molecular processes. Some variants directly alter protein structure through missense or nonsense mutations. Others affect regulation: promoter variants change expression level, splice site variants alter transcript isoforms, enhancer variants modulate tissue-specific expression.\nThese primary effects propagate through molecular layers. Expression changes alter the cellular protein complement. Protein level changes affect enzyme activity, signaling cascades, and transcriptional feedback. Metabolic flux shifts in response to enzyme availability. Cell behavior changes as the integrated molecular state crosses thresholds for proliferation, differentiation, or death.\nTissue-level phenotypes emerge from cellular behavior aggregated across the organ. Tumor growth reflects altered cell proliferation; fibrosis reflects aberrant extracellular matrix deposition; inflammation reflects immune cell recruitment and activation. These tissue phenotypes manifest as clinical symptoms, laboratory abnormalities, and imaging findings.\nThe cascade view suggests where different modalities provide information. Genomics captures the inherited potential and somatic alterations. Transcriptomics and epigenomics capture the current regulatory state. Proteomics and metabolomics capture the functional molecular complement. Clinical data captures the phenotypic consequences.\n\n\n19.5.2 Bottleneck Modalities\nNot all modalities are equally informative for all questions. The concept of bottleneck modalities identifies which molecular layers most directly mediate the relationship between genetic variation and phenotype.\nFor many coding variants, protein structure is the bottleneck. A missense variant’s effect on disease depends primarily on how it alters protein function, which depends on how the amino acid substitution affects folding, stability, and activity. Expression level matters less than structural consequence. Protein language models that predict structural effects from sequence directly address this bottleneck.\nFor regulatory variants, expression is closer to the bottleneck. An enhancer variant affects disease through its effect on target gene expression, which affects downstream processes. Chromatin accessibility and transcription factor binding are intermediate steps; expression level is the more proximal readout. Models that predict expression effects from sequence address this bottleneck.\nFor some phenotypes, the bottleneck may lie downstream of molecular measurements entirely. Behavioral traits depend on neural circuit function that emerges from complex cellular and network dynamics. Metabolic traits depend on flux through interconnected pathways that may not be apparent from enzyme abundance alone. These cases suggest that molecular measurements provide incomplete information regardless of integration sophistication.\n\n\n19.5.3 Causal vs. Correlational Integration\nMulti-omics data are pervasively correlated. Genes in the same pathway have correlated expression. Methylation and expression are anti-correlated at many promoters. Clinical variables cluster by disease category. These correlations can improve prediction even without causal understanding.\nCausal integration seeks to identify the mechanistic relationships between molecular layers. If a variant causes reduced expression, which causes protein deficiency, which causes metabolic dysfunction, this causal chain suggests intervention targets: expression restoration or enzyme supplementation might address the downstream effects. Correlational integration might achieve the same predictive performance without identifying this chain, since all layers correlate with the phenotype.\nDistinguishing causal from correlational relationships requires experimental perturbation or careful causal inference from observational data. Mendelian randomization uses genetic variants as instruments to infer causal effects of expression on outcomes. CRISPR screens directly perturb gene function and measure consequences. Multi-omics integration methods increasingly incorporate causal assumptions or validation against perturbation data.\nThe distinction matters for interpretation and intervention. A predictive model based on correlations may fail when the data distribution shifts or when interventions alter the causal structure. A causally informed model captures mechanism that persists across contexts.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "p4-ch19-multi-omics.html#handling-missing-modalities",
    "href": "p4-ch19-multi-omics.html#handling-missing-modalities",
    "title": "19  Multi-Omics Integration",
    "section": "19.6 Handling Missing Modalities",
    "text": "19.6 Handling Missing Modalities\nReal-world multi-omics data are incomplete. Different studies measure different modalities. Within studies, technical failures, sample limitations, and cost constraints create missing data. Clinical deployment must handle patients with incomplete molecular profiles. Robust multi-omics methods must address missingness directly.\n\n19.6.1 Training with Incomplete Data\nIntermediate fusion architectures handle missing modalities naturally during inference: only the available encoders contribute to the shared representation. Training is more complex because alignment terms require paired measurements across modalities.\nOne approach trains on the subset of samples with complete data, then applies the trained encoders to samples with partial data during inference. This wastes information from the samples with incomplete profiles and may learn representations that fail to generalize to the missing-modality setting.\nA better approach incorporates missingness into training. Modality dropout randomly masks modalities during training, forcing the model to learn representations robust to missing inputs. The reconstruction and alignment losses are computed only for available modalities, so samples with partial data can still contribute to training.\nCurriculum learning strategies may first train with complete data to establish alignment, then gradually increase modality dropout to improve robustness. The balance between alignment quality (which benefits from complete data) and robustness (which requires training on partial data) requires empirical tuning.\n\n\n19.6.2 Cross-Modal Imputation\nIntermediate fusion enables principled imputation of missing modalities. Given a sample’s available modalities encoded into the shared latent space, decoders for missing modalities can predict expected values. If a patient has expression data but not methylation, the expression encoder produces a latent embedding, and the methylation decoder generates predicted methylation values from that embedding.\nThe imputation quality depends on how well the shared space captures the biological factors underlying both modalities. If expression and methylation reflect the same cell state, the imputation may be accurate. If they capture distinct aspects of biology, imputation will smooth over true variation.\nUncertainty in imputation matters for downstream use. Point estimates of missing values provide no indication of confidence. Generative models that produce distributions over missing values enable propagation of uncertainty through downstream analyses. A risk prediction that depends heavily on imputed values should have wider confidence intervals than one based entirely on measured data.\n\n\n19.6.3 Zero-Shot Cross-Modal Transfer\nThe most ambitious application of multi-omics integration is zero-shot prediction across modalities: using a model trained on one set of modalities to make predictions for samples measured with entirely different modalities.\nThis transfer relies on the shared latent space capturing biological state independently of measurement modality. If the space truly represents cell state, then a classifier trained on expression-derived embeddings should work on ATAC-seq-derived embeddings, since both encoders map to the same biological meaning. The alignment training enables this transfer by ensuring that the same biological entity maps to the same latent location regardless of which modality was measured.\nZero-shot transfer is rarely perfect. The modalities may capture somewhat different aspects of biology, and the alignment may be imprecise. But partial transfer can still be valuable: a model achieving 80% of supervised performance without any labeled examples in the new modality saves substantial annotation effort.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "p4-ch19-multi-omics.html#practical-challenges",
    "href": "p4-ch19-multi-omics.html#practical-challenges",
    "title": "19  Multi-Omics Integration",
    "section": "19.7 Practical Challenges",
    "text": "19.7 Practical Challenges\n\n19.7.1 Batch Effects Across Modalities\nBatch effects, systematic technical variation between experimental batches, are endemic in high-throughput biology. Multi-omics integration faces compounded batch effects: each modality may have its own batch structure, batches may be correlated or anti-correlated across modalities, and batch correction methods designed for single modalities may not extend to multi-modal settings.\nConsider a study where expression data were generated at three sequencing centers and proteomics data were generated at two mass spectrometry facilities. The batch effects in each modality are independent. Samples from expression batch 1 are spread across proteomics batches. Correcting expression batch effects does not address proteomics batch effects, and vice versa.\nIntegration must either correct batch effects within each modality before combining (risking removal of real biology that correlates with batch) or incorporate batch as a covariate in the integrated model (requiring that batch structure be known and modeled correctly). Domain adaptation techniques treat batches as domains and learn representations invariant to domain while retaining biological signal.\n\n\n19.7.2 Sample Size and Power\nMulti-omics studies typically have smaller sample sizes than single-modality studies due to cost constraints. Each additional modality increases per-sample cost, trading breadth for depth. This tradeoff has implications for statistical power and model complexity.\nThe effective sample size for multi-omics integration may be smaller than for any single modality. If 1000 patients have expression data and 800 have methylation data but only 600 have both, intermediate fusion sees 600 fully informative samples. Late fusion can use all 1000 expression samples and all 800 methylation samples, avoiding the intersection penalty.\nPower analyses for multi-omics studies must account for the specific integration strategy and the expected missingness pattern. A study designed for early fusion needs larger sample sizes (relative to feature count) than one designed for late fusion. Grant applications and study planning should explicitly consider how integration choices affect required sample sizes.\n\n\n19.7.3 Evaluation Complexity\nEvaluating multi-omics models is more complex than evaluating single-modality models. Multiple dimensions of performance matter: prediction accuracy, calibration, cross-modality transfer, robustness to missing modalities, biological plausibility of learned representations, and clinical utility.\nA model might achieve high prediction accuracy by memorizing batch effects or leveraging shortcuts in the data. Evaluation should include cross-batch and cross-cohort validation to assess generalization. Ablation studies that remove each modality quantify the contribution of each data type and identify whether the model genuinely integrates information or relies predominantly on one modality.\nBiological validation through comparison to known biology provides another evaluation axis. Do the learned factors correspond to known pathways? Are attention patterns consistent with regulatory relationships? Do imputed values match held-out measurements? These checks assess whether the model captures biological signal rather than technical artifacts.\nClinical evaluation, addressed in ?sec-clinical-risk, requires prospective validation in real deployment settings. A model that improves prediction in research cohorts may not improve clinical decisions if the predictions do not change management or if the required modalities are unavailable in clinical workflows.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "p4-ch19-multi-omics.html#from-integration-to-prediction",
    "href": "p4-ch19-multi-omics.html#from-integration-to-prediction",
    "title": "19  Multi-Omics Integration",
    "section": "19.8 From Integration to Prediction",
    "text": "19.8 From Integration to Prediction\nMulti-omics integration is not an end in itself but a means to improved prediction, understanding, and intervention. The integration strategies and foundation models surveyed here provide representations; downstream models convert representations to actionable outputs.\nRisk prediction combines multi-omic embeddings with clinical variables for individualized prognosis. Treatment response models predict which patients will benefit from specific therapies based on their integrated molecular profiles. Biomarker discovery identifies features that discriminate conditions or predict outcomes, potentially revealing new biology or therapeutic targets.\nThe systems view shapes how these predictions should be interpreted. A risk prediction based on multi-omics features inherits explanatory power from the causal structure relating molecular layers to phenotypes. Understanding which modalities drive predictions, and how they relate to underlying biology, supports clinical reasoning and identifies potential interventions.\nThe path from research models to clinical deployment, examined in ?sec-clinical-risk and Chapter 26, requires addressing the practical challenges discussed here: batch effects, missing modalities, sample size limitations, and evaluation complexity. Multi-omics integration offers richer patient characterization than any single modality, but realizing this promise requires careful attention to the realities of clinical data and deployment constraints.\nAs the field advances toward whole-patient foundation models that jointly encode genomics, transcriptomics, proteomics, imaging, and clinical data, the integration principles established here provide the foundation. The tradeoffs between fusion strategies, the importance of shared latent spaces, the challenge of missing modalities, and the systems biology perspective on information flow will remain relevant as the scale and scope of integration expand.",
    "crumbs": [
      "Part IV: Multi-Scale Modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "p5--eval-interp.html",
    "href": "p5--eval-interp.html",
    "title": "Part V: Evaluation and Reliability",
    "section": "",
    "text": "The preceding sections have surveyed an impressive landscape of genomic foundation models, from convolutional architectures that learn regulatory grammar to transformer-based systems that capture long-range chromatin interactions. Yet the enthusiasm surrounding these advances must be tempered by rigorous assessment. A model’s utility in genomics depends not merely on its architectural sophistication but on whether it genuinely captures biological signal, generalizes beyond its training distribution, and provides insights that translate to clinical or experimental settings. Part V addresses these critical questions, examining the frameworks, methodologies, and potential pitfalls that determine whether genomic AI models deliver on their promises.\nEvaluating genomic models presents unique challenges that distinguish this domain from natural language processing or computer vision. Biological sequences contain nested hierarchies of functional elements, population-stratified variation, and evolutionary constraints that can masquerade as predictive signal. Standard machine learning metrics may obscure fundamental problems: a variant effect predictor might achieve impressive aggregate performance while systematically failing on clinically actionable mutations, or a regulatory model might exploit sequence artifacts rather than genuine enhancer logic. The chapters that follow develop a comprehensive framework for understanding what benchmarks actually measure, how evaluation methodology shapes conclusions, what confounders threaten validity, and how interpretability methods can distinguish genuine biological insight from spurious pattern matching.\nWe begin in 20  Benchmarks for Genomic AI with a survey of established benchmarks in genomic deep learning, examining their construction, scope, and limitations. 21  Evaluation Methodology then develops principles for rigorous evaluation methodology, addressing issues of train-test contamination, metric selection, and clinical relevance that frequently compromise published comparisons. 22  Confounders in Model Training confronts the systematic biases and data leakage pathways that pervade genomic datasets, with particular attention to population stratification and linkage disequilibrium. 24  Interpretability and Mechanism explores interpretability methods that move beyond black-box prediction toward mechanistic understanding, examining both the promise and limitations of attribution approaches in the genomic context. Finally, 14  Variant Effect Prediction with Foundation Models demonstrates how these evaluation principles converge in variant effect prediction, surveying landmark systems including AlphaMissense, GPN-MSA, Evo 2, and AlphaGenome that translate foundation model representations into pathogenicity scores. This final chapter serves as both a comprehensive case study in model evaluation and a bridge to the clinical applications covered in Part VI, showing how rigorous assessment frameworks determine whether genomic AI delivers actionable insights for patient care.",
    "crumbs": [
      "Part V: Evaluation and Reliability"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html",
    "href": "p5-ch20-benchmarks.html",
    "title": "20  Benchmarks for Genomic AI",
    "section": "",
    "text": "20.1 Protein Language Model Benchmarks\nA foundation model that achieves state-of-the-art performance on published benchmarks may fail catastrophically when deployed on patient data from underrepresented populations. Conversely, a model dismissed as mediocre on standard leaderboards may prove invaluable for a specialized clinical application the benchmarks never measured. The genomic AI field has accumulated dozens of benchmark suites, hundreds of individual tasks, and thousands of leaderboard entries, yet fundamental questions remain unanswered: Do these benchmarks measure what matters? When does benchmark success predict deployment value? How do we know when a benchmark has outlived its usefulness?\nThese questions are not merely academic. Clinical laboratories considering whether to adopt a new variant classifier, pharmaceutical companies evaluating foundation models for target discovery, and research groups selecting architectures for regulatory prediction all face the same challenge: translating benchmark metrics into deployment decisions. The gap between benchmark performance and real-world utility represents one of the most consequential yet underexplored problems in genomic AI. A model optimized for AUROC on ClinVar may systematically miscalibrate predictions for the rare variants that matter most clinically. A DNA language model that excels at enhancer classification may learn superficial sequence features rather than regulatory grammar. Understanding what benchmarks actually measure, and what they miss, is prerequisite to responsible model development and deployment.\nThis chapter surveys the benchmark landscape for genomic foundation models, organized by the biological modality each benchmark family targets. We examine protein benchmarks first, as the most mature evaluation ecosystem, then DNA and regulatory benchmarks, variant effect prediction benchmarks, and finally trait-level benchmarks. Throughout, we attend not only to what these benchmarks measure but to their construction, their biases, and their limitations. The methodological principles for using benchmarks properly (designing experiments, choosing metrics, avoiding pitfalls) are covered in Chapter 21. Here we focus on cataloging what exists and developing the critical perspective necessary to interpret benchmark claims.\nProtein language models (Chapter 12) benefit from the longest-established and most systematic evaluation ecosystem in genomic AI. The maturity of protein benchmarks reflects both the longer history of computational protein science and the relative tractability of protein structure and function prediction compared to regulatory genomics.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html#protein-language-model-benchmarks",
    "href": "p5-ch20-benchmarks.html#protein-language-model-benchmarks",
    "title": "20  Benchmarks for Genomic AI",
    "section": "",
    "text": "20.1.1 TAPE: Tasks Assessing Protein Embeddings\nThe Tasks Assessing Protein Embeddings (TAPE) benchmark, introduced in 2019, established the template for systematic protein representation evaluation (rao_evaluating_2019?). TAPE frames protein language model assessment as transfer learning evaluation: pretrained models generate embeddings, which are then used as features for supervised prediction on downstream tasks. This framework decouples representation quality from task-specific modeling, enabling comparison across architectures that may have very different inductive biases.\nTAPE comprises five tasks spanning different aspects of protein biology. Secondary structure prediction requires classifying each residue as helix, sheet, or coil, testing whether embeddings capture local structural preferences. Contact prediction asks whether residue pairs are spatially proximate in the folded structure, probing the representation’s ability to encode tertiary structure information from sequence alone. Remote homology detection requires classifying proteins into structural superfamilies, testing whether embeddings capture evolutionary relationships that transcend sequence similarity. Fluorescence prediction and stability prediction use data from deep mutational scanning experiments to assess whether embeddings encode fitness landscapes.\nThe benchmark’s design reflects deliberate methodological choices. Train, validation, and test splits enforce sequence identity thresholds to prevent homology-based leakage. Evaluation uses simple linear or shallow neural network heads rather than complex task-specific architectures, isolating representation quality from modeling capacity. Standardized preprocessing and data loading eliminate confounds from inconsistent implementation.\nTAPE’s influence extended beyond its specific tasks. The benchmark established norms for protein representation evaluation: systematic coverage of diverse prediction targets, controlled transfer learning protocols, and explicit attention to data splitting. Subsequent benchmarks adopted and extended this framework.\n\n\n20.1.2 FLIP: Function-Linked Protein Benchmark\nThe FLIP (Function-Linked Integrated Protein) benchmark addresses gaps in TAPE’s coverage by focusing on experimentally measured functional properties (dallago_flip_2021?). Where TAPE includes structurally derived labels and computational annotations, FLIP emphasizes high-throughput experimental assays that directly measure protein fitness.\nFLIP aggregates deep mutational scanning datasets across diverse proteins and functional readouts. The benchmark includes assays measuring enzymatic activity, binding affinity, thermostability, and expression level. Each dataset provides quantitative measurements for thousands of single-point mutations, enabling evaluation of fine-grained variant effect prediction.\nThe benchmark’s value lies in its experimental grounding. Computational structure predictions and evolutionary conservation scores, while useful, are indirect proxies for function. Deep mutational scanning provides direct measurements of how sequence changes affect the property of interest. Models that perform well on FLIP demonstrate the ability to predict experimentally validated functional consequences rather than computationally inferred annotations.\nFLIP also introduced systematic evaluation of different splitting strategies. Random splits, where training and test variants are sampled uniformly from the same protein, represent the easiest setting. Contiguous splits, where training and test variants occupy different sequence regions, test spatial generalization. Modulo splits, which interleave training and test positions along the sequence, provide intermediate difficulty. Performance typically degrades from random to contiguous splits, revealing how much models rely on local sequence context versus genuine functional understanding.\n\n\n20.1.3 ProteinGym: Comprehensive Variant Effect Evaluation\nProteinGym has emerged as the most comprehensive benchmark for protein variant effect prediction, compiling 217 deep mutational scanning assays across diverse protein families (notin_proteingym_2024?). The benchmark’s scale enables statistically robust comparison across modeling approaches while its diversity reveals where different methods excel or struggle.\nThe primary evaluation metric is Spearman correlation between predicted and experimentally measured fitness effects. This rank-based metric is appropriate for deep mutational scanning data, where absolute fitness values depend on assay-specific calibration but relative rankings are more comparable across experiments. ProteinGym reports correlations for each assay individually and aggregated across the full benchmark, enabling both global comparison and identification of task-specific strengths.\nProteinGym distinguishes between zero-shot and supervised evaluation regimes. In zero-shot evaluation, models predict variant effects without any task-specific training, relying entirely on representations learned during pretraining. Models like ESM-1v compute effects as log-likelihood ratios under the pretrained language model, while structure-based methods like AlphaMissense incorporate predicted structural consequences. In supervised evaluation, models are fine-tuned on a subset of measured variants before predicting held-out effects. The gap between zero-shot and supervised performance indicates how much task-specific information improves over general-purpose representations.\nThe benchmark reveals systematic patterns in model performance. Protein language models generally outperform conservation-based methods, particularly for variants in regions with sparse evolutionary sampling. Structure-aware models show advantages for variants affecting protein stability or buried residues. Ensemble methods that combine multiple predictors often achieve the highest correlations, suggesting that different approaches capture complementary information.\nProteinGym’s limitations mirror those of its constituent datasets. Deep mutational scanning experiments are biased toward well-studied proteins amenable to high-throughput screening. Assay-specific selection pressures affect which variants appear deleterious: a variant may strongly affect enzymatic activity while leaving thermostability unchanged, or vice versa. The benchmark measures correlation with specific experimental readouts rather than clinical pathogenicity, which integrates multiple functional consequences in complex ways.\n\n\n20.1.4 Structure Prediction Benchmarks\nProtein structure prediction benchmarks derive from the Critical Assessment of protein Structure Prediction (CASP) tradition, which has evaluated computational methods against experimentally determined structures since 1994. The dramatic success of AlphaFold2 at CASP14 in 2020 transformed the field, but structure prediction benchmarks remain relevant for evaluating single-sequence methods and assessing whether language model pretraining improves structural accuracy.\nStructure prediction quality is typically assessed using the Global Distance Test (GDT-TS) and Template Modeling score (TM-score). GDT-TS measures the percentage of residues that can be superimposed within various distance thresholds, providing a single number between 0 and 100 that correlates well with visual assessment of structural similarity. TM-score normalizes by protein length, enabling comparison across proteins of different sizes.\nFor protein language models, the relevant evaluation setting is single-sequence structure prediction, where the model receives only the target sequence without multiple sequence alignments. This tests whether pretraining on evolutionary sequence databases enables structure prediction without explicit evolutionary analysis at inference time. ESMFold demonstrated that single-sequence prediction can approach MSA-based methods for many proteins, though performance gaps remain for sequences with sparse evolutionary coverage.\nStructure prediction benchmarks complement sequence-based evaluations by testing whether learned representations encode biophysical constraints. A model that achieves high accuracy on contact prediction or secondary structure classification may still fail to integrate these local predictions into globally consistent structures. The emergence of accurate single-sequence structure prediction from language model embeddings suggests that pretraining captures substantial structural information, even without explicit structural supervision.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html#dna-and-regulatory-benchmarks",
    "href": "p5-ch20-benchmarks.html#dna-and-regulatory-benchmarks",
    "title": "20  Benchmarks for Genomic AI",
    "section": "20.2 DNA and Regulatory Benchmarks",
    "text": "20.2 DNA and Regulatory Benchmarks\nDNA foundation models (Chapter 11) and regulatory models (Chapter 13) face a less mature but rapidly developing benchmark landscape. Early deep learning work in genomics focused on individual tasks derived from ENCODE-style assays. Recent efforts have introduced benchmark suites that attempt to standardize evaluation across multiple tasks, tissues, and species.\n\n20.2.1 Classical Regulatory Prediction Tasks\nThe earliest deep learning benchmarks for genomics framed regulatory prediction as classification over short sequence windows. Transcription factor binding prediction asks whether a specific TF ChIP-seq peak overlaps a given sequence window, typically around 1 kilobase centered on the binding site. Open chromatin prediction requires classifying regions as accessible or inaccessible based on DNase-seq or ATAC-seq signal. Histone mark prediction asks whether a chromatin modification peak (H3K27ac, H3K4me3, etc.) is present at each position.\nThese tasks derive from consortia like ENCODE and Roadmap Epigenomics, which systematically profiled chromatin states across cell types. Benchmark construction typically involves defining positive regions from called peaks and sampling negative regions from elsewhere in the genome, extracting fixed-length sequences centered on each region, and evaluating binary classification using AUROC or average precision.\nModels such as DeepSEA, Basset, and DanQ established baseline performance on these tasks. Their success demonstrated that convolutional networks could learn sequence features predictive of regulatory state without hand-crafted motifs. Modern foundation models still report performance on similar tasks as sanity checks, though these classical benchmarks have significant limitations.\nThe primary limitation is that binary classification over short windows fails to capture the quantitative, cell-type-specific, and long-range nature of transcriptional regulation. A region may be weakly accessible in some cell types and strongly accessible in others; binary labels collapse this continuous variation. Short windows cannot assess whether models capture distal regulatory interactions that span tens to hundreds of kilobases. Evaluation on curated peak regions may overestimate performance relative to genome-wide prediction, where the vast majority of positions are regulatory “background.”\n\n\n20.2.2 Quantitative Regulatory Prediction\nBeyond binary classification, benchmarks increasingly require prediction of quantitative regulatory readouts. Signal regression asks models to predict per-base or per-bin signal intensity from ChIP-seq, ATAC-seq, or related assays. Gene expression prediction requires predicting transcript abundance (TPM, counts) from promoter sequences or larger genomic contexts. Massively parallel reporter assays (MPRAs) provide systematic measurements of enhancer or promoter activity for thousands of sequences, enabling evaluation of quantitative activity prediction.\nHybrid architectures like Enformer (Chapter 13) popularized benchmarks combining large receptive fields with dense quantitative targets across many assays and cell types. Evaluation metrics shift from AUROC to Pearson or Spearman correlation between predicted and observed profiles. Some benchmarks report correlation relative to replicate concordance, establishing an upper bound set by experimental reproducibility.\nQuantitative benchmarks better reflect the continuous nature of regulatory activity but introduce new challenges. Heterogeneous noise across assays and laboratories complicates aggregation: should a model be penalized equally for poor performance on a low-quality assay versus a high-quality one? Cell-type diversity raises questions about how to weight performance across tissues: is accurate prediction in a rare cell type more or less important than in a common one? The relationship between predicted and observed signal depends on assay-specific calibration that may not transfer across experimental batches.\n\n\n20.2.3 Genomic Benchmarks\nThe Genomic Benchmarks resource provides standardized classification datasets for DNA sequence models (Grešová et al. 2023). The benchmark compiles tasks including enhancer identification, promoter recognition, splice site detection, and coding sequence classification across multiple species. Standardized train, validation, and test splits enable direct comparison of different architectures without confounds from inconsistent data processing.\nGenomic Benchmarks emphasizes accessibility and reproducibility. Datasets are available in a unified format with documented preprocessing. Baseline results for multiple architectures provide reference points for new models. The benchmark includes tasks of varying difficulty, from relatively easy (distinguishing coding from non-coding sequence) to challenging (identifying tissue-specific enhancers).\nThe benchmark’s limitations reflect its design priorities. Focus on classification rather than regression excludes quantitative prediction tasks. Task difficulty varies substantially, with some tasks approaching saturation where gains become difficult to measure. Species coverage, while broader than many benchmarks, remains biased toward well-studied model organisms.\n\n\n20.2.4 BEND: Benchmark for DNA Language Models\nBEND (Benchmark for Evaluating DNA Models) provides a unified framework for evaluating genomic foundation models across diverse tasks (de_almeida_bend_2024?). The benchmark includes regulatory element classification, chromatin accessibility prediction, variant effect scoring, and gene expression prediction. Standardized splits and evaluation protocols enable fair comparison across model families.\nBEND’s design reflects lessons learned from earlier benchmarks. Tasks span multiple biological scales, from nucleotide-level variant effects to kilobase-scale regulatory elements. Evaluation includes both zero-shot settings (using pretrained representations directly) and fine-tuned settings (adapting models to specific tasks). Performance is reported separately for each task rather than aggregated into a single score, acknowledging that different models may excel at different aspects of genomic prediction.\nComparative evaluations using BEND reveal that no single model dominates across all tasks. Architecture choices (CNN versus transformer versus state space model), tokenization schemes (single nucleotide versus k-mer versus BPE), and pretraining corpora all influence task-specific performance. These patterns inform model selection for specific applications while highlighting the limitations of aggregate benchmarks that obscure such variation.\n\n\n20.2.5 Long-Range Benchmarks\nLong-range regulatory interactions, where enhancers tens to hundreds of kilobases from their target genes influence expression, require benchmarks that specifically test extended context modeling. The Long Range Benchmark (LRB) evaluates models’ ability to integrate information across large genomic distances, with tasks including predicting distal enhancer-promoter interactions, modeling TAD boundary effects, and identifying long-range regulatory dependencies.\nDNALongBench extends evaluation to ultra-long contexts spanning up to millions of base pairs. Tasks at this scale test whether models can leverage chromosome-level context for regulatory prediction, potentially capturing effects from 3D chromatin organization and large-scale chromatin domains.\nThese benchmarks are particularly relevant for evaluating efficient attention mechanisms, state space models, and other architectures designed to extend effective context length. Performance on long-range benchmarks does not necessarily correlate with short-range task performance, indicating that different architectural choices optimize for different aspects of sequence modeling.\n\n\n20.2.6 Cross-Species Evaluation\nGenBench and related resources test whether models trained on one organism generalize to related species. Cross-species evaluation is important for several reasons. Many applications require predictions in non-human organisms (agricultural genomics, model organism research, comparative genomics). Multi-species training may improve within-species performance by providing additional evolutionary signal. The ability to transfer across species indicates that models have learned general principles of genome organization rather than species-specific artifacts.\nCross-species benchmarks typically evaluate models on held-out species not seen during training. Performance degradation from training to held-out species indicates the degree to which learned representations depend on species-specific features. Some architectures show better cross-species transfer than others, suggesting differences in how well they capture conserved regulatory principles.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html#variant-effect-prediction-benchmarks",
    "href": "p5-ch20-benchmarks.html#variant-effect-prediction-benchmarks",
    "title": "20  Benchmarks for Genomic AI",
    "section": "20.3 Variant Effect Prediction Benchmarks",
    "text": "20.3 Variant Effect Prediction Benchmarks\nVariant effect prediction (VEP) benchmarks connect sequence changes to molecular or phenotypic consequences, addressing the clinically central question of which variants matter. These benchmarks span multiple biological levels, from molecular function to clinical pathogenicity.\n\n20.3.1 Clinical Variant Databases\nClinVar provides the most widely used labels for clinical variant effect prediction, aggregating pathogenicity assertions from clinical laboratories and researchers worldwide. Benchmarks derived from ClinVar frame variant interpretation as classification: given a variant, predict whether it is pathogenic, likely pathogenic, benign, or likely benign.\nClinVar’s value as a benchmark stems from its clinical relevance. Variants classified in ClinVar represent the actual population of variants encountered in clinical testing. Performance on ClinVar directly addresses whether a model can assist variant interpretation workflows. The database’s scale (over 2 million variant submissions as of 2024) enables statistically robust evaluation.\nClinVar’s limitations as a benchmark are equally important. Submission heterogeneity means that label quality varies dramatically: expert-curated panels provide high-confidence classifications while single-laboratory submissions may reflect limited evidence. Version sensitivity means that benchmark composition changes over time as new submissions arrive and old classifications are updated. Most consequentially, circularity with computational predictors creates feedback loops: variants may have been classified using the very tools being evaluated, inflating apparent performance.\nAncestry and gene coverage biases profoundly shape what ClinVar benchmarks measure. Variants from European ancestry individuals and well-studied disease genes are heavily overrepresented. High performance on ClinVar demonstrates accuracy for this specific population rather than robust generalization across human genetic diversity. Benchmarks stratified by ancestry reveal substantial performance gaps, with models typically performing worse on variants from underrepresented populations.\nBest practices for using ClinVar as a benchmark include specifying the exact database version and download date, excluding variants with conflicting assertions, stratifying performance by evidence level and ancestry, and comparing to baselines using only allele frequency to detect circularity. These practices are detailed in Chapter 21.\n\n\n20.3.2 CAGI: Critical Assessment of Genome Interpretation\nThe Critical Assessment of Genome Interpretation (CAGI) challenges provide prospective evaluation of variant effect predictors on unpublished datasets. Unlike retrospective benchmarks that evaluate models on historical data, CAGI distributes prediction targets before ground truth is available, preventing any possibility of overfitting to known labels.\nCAGI challenges cover diverse prediction targets. Some challenges focus on molecular phenotypes: predicting the effect of variants on protein stability, binding affinity, or enzymatic activity. Others target clinical phenotypes: predicting disease risk, drug response, or clinical severity from individual genomes. The diversity of challenges tests whether models generalize across different types of variant effects.\nThe prospective design provides several advantages over retrospective benchmarks. Predictions must be made before labels are known, eliminating leakage from any source. The timeline forces models to commit to predictions rather than post-hoc optimization. Community participation enables comparison across many approaches under identical conditions.\nCAGI’s limitation is scale: challenges include hundreds to thousands of variants rather than the millions available in databases like ClinVar. Statistical power to detect small performance differences is correspondingly limited. The challenges also depend on experimental collaborators willing to withhold data until after the prediction deadline, limiting the range of phenotypes that can be assessed.\n\n\n20.3.3 Deep Mutational Scanning Benchmarks\nDeep mutational scanning (DMS) provides systematic experimental measurement of variant effects across entire proteins or regulatory elements. DMS benchmarks test whether models can predict these experimentally determined effects, providing direct validation against measured functional consequences rather than inferred clinical classifications.\nMaveDB aggregates DMS datasets in a standardized format, enabling systematic benchmarking across diverse proteins and assays. ProteinGym’s DMS component (discussed above) represents the most comprehensive benchmark in this space. For non-coding variants, MPRA datasets provide analogous systematic measurements of regulatory activity.\nDMS benchmarks have distinct strengths and limitations compared to clinical databases. The experimental grounding means that labels reflect actual measured effects rather than clinical inference that may involve multiple assumptions. However, the relationship between DMS fitness and clinical pathogenicity is complex: a variant may substantially affect enzymatic activity without causing disease if the residual activity suffices for normal physiology. DMS benchmarks measure one component of the variant interpretation puzzle rather than the full clinical picture.\n\n\n20.3.4 Regulatory and Non-Coding Variant Benchmarks\nNon-coding variants require specialized benchmarks because their effects operate through different mechanisms than coding variants. MPRA-based benchmarks test whether models can predict the quantitative effect of variants on enhancer or promoter activity measured in reporter assays. eQTL-based benchmarks use naturally occurring variants associated with expression changes, treating the statistical evidence for eQTL status as a proxy for regulatory impact.\nThe challenge for non-coding benchmarks is connecting molecular effects to phenotypic consequences. A variant may alter chromatin accessibility without affecting any gene’s expression. A variant may affect expression without influencing disease risk. This gap between molecular and clinical effects complicates interpretation: high performance on MPRA prediction does not necessarily translate to accurate regulatory disease variant interpretation.\nFine-mapped GWAS variants provide another benchmark source for non-coding VEP. Statistical fine-mapping identifies putatively causal variants within associated loci, and models can be evaluated on their ability to prioritize these variants over nearby non-causal variants. Performance on fine-mapping tasks more directly assesses clinical relevance than molecular phenotype prediction, though fine-mapping itself has substantial uncertainty.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html#trait-and-population-level-benchmarks",
    "href": "p5-ch20-benchmarks.html#trait-and-population-level-benchmarks",
    "title": "20  Benchmarks for Genomic AI",
    "section": "20.4 Trait and Population-Level Benchmarks",
    "text": "20.4 Trait and Population-Level Benchmarks\nAt the individual and population level, benchmarks assess whether models improve prediction of complex traits and disease risk.\n\n20.4.1 Polygenic Score Evaluation\nPolygenic score (PGS) benchmarks evaluate how well genotype-derived scores predict disease risk or quantitative traits. Common evaluation settings include within-biobank evaluation, where a single large cohort is partitioned into training and test sets, and cross-biobank evaluation, where models trained in one population are tested in another.\nMetrics depend on the phenotype. For quantitative traits, benchmarks report the coefficient of determination (R²) or incremental R² over non-genetic covariates. For binary disease outcomes, AUROC and AUPRC quantify discrimination. Calibration metrics assess whether predicted risks match observed event rates. The clinical utility of PGS, discussed in ?sec-clinical-risk, depends on all these properties: a score may discriminate well (high AUROC) while being poorly calibrated (predicted risks don’t match actual event rates).\nCross-population evaluation is particularly important because PGS portability is a major limitation of current methods (Chapter 3). Benchmarks stratified by ancestry typically reveal substantial performance degradation from European ancestry (where most GWAS have been conducted) to other populations. This degradation stems from multiple sources: different linkage disequilibrium patterns mean that tag SNPs identify different causal variants, population-specific variants are absent from training data, and effect sizes may differ across populations due to gene-environment interactions.\n\n\n20.4.2 TraitGym\nTraitGym provides a framework specifically designed to assess complex trait prediction using genomic foundation models. The benchmark evaluates whether foundation model embeddings or variant scores improve prediction beyond traditional polygenic score methods.\nTraitGym’s design addresses several limitations of standard PGS benchmarks. Ancestry stratification is built into the evaluation protocol, requiring models to report performance separately for different population groups. Multiple phenotypes spanning different genetic architectures (highly polygenic versus more oligogenic) test generalization across trait types. Comparison to appropriate baselines (standard PGS methods, clinical covariates alone) isolates the contribution of foundation model features.\nThe benchmark is particularly relevant for assessing claims that genomic foundation models add predictive value beyond classical statistical genetics. Foundation models incur substantial computational costs compared to linear PGS models; TraitGym helps determine whether these costs are justified by improved prediction.\n\n\n20.4.3 EmbedGEM Framework\nThe EmbedGEM framework evaluates whether foundation model embeddings capture biologically meaningful genetic signal, as opposed to technical artifacts or confounders (Mukherjee et al. 2024). The framework assesses embeddings along two axes: heritability and disease relevance.\nThe heritability axis measures how much genetic signal an embedding captures. EmbedGEM counts the number of genome-wide significant loci associated with embedding components and quantifies the strength of association through mean chi-squared statistics. Higher values indicate that the embedding reflects heritable biology rather than noise.\nThe disease relevance axis measures whether embedding-associated variants predict clinically meaningful outcomes. Polygenic scores constructed from embedding GWAS hits are evaluated for their ability to predict disease in independent cohorts. Incremental predictive value over standard clinical models indicates that the embedding captures disease-relevant genetic information.\nThis two-axis evaluation addresses a critical question for foundation model deployment: do learned representations discover novel biology or merely recapitulate known associations with additional computational overhead? Embeddings that show high heritability but low disease relevance may capture biological signal that is not clinically actionable. Embeddings that show disease relevance without novel genetic discoveries may not add value beyond existing PGS methods.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html#benchmark-construction-and-hidden-assumptions",
    "href": "p5-ch20-benchmarks.html#benchmark-construction-and-hidden-assumptions",
    "title": "20  Benchmarks for Genomic AI",
    "section": "20.5 Benchmark Construction and Hidden Assumptions",
    "text": "20.5 Benchmark Construction and Hidden Assumptions\nBeyond cataloging benchmark suites, understanding how benchmarks are constructed reveals assumptions that shape what they measure and what they miss.\n\n20.5.1 Data Sources and Label Provenance\nBenchmark labels derive from diverse sources with different properties. Experimental assays (ChIP-seq, DMS, MPRA) provide direct measurements but are limited by assay-specific artifacts and selection pressures. Computational annotations (gene calls, functional predictions, conservation scores) provide broader coverage but introduce circular dependencies if models are trained and evaluated on overlapping sources. Clinical classifications aggregate expert judgment but reflect the evidence available at classification time, which may include the very predictors being benchmarked.\nThe provenance of benchmark labels determines what success on that benchmark actually means. High performance on experimentally derived labels suggests the model captures the specific molecular process assayed. High performance on clinical labels may indicate genuine clinical utility or may reflect circularity with existing prediction tools. Understanding label provenance is prerequisite to interpreting benchmark results.\n\n\n20.5.2 Splitting Strategies and Leakage\nHow benchmarks partition data into training and test sets determines whether evaluation measures generalization or memorization. Random splitting, where examples are assigned to splits uniformly at random, represents the weakest form of evaluation. In genomics, random splits often permit homology-based leakage: training and test sequences may share sufficient similarity that memorization suffices for good performance.\nHomology-aware splitting clusters sequences by similarity before assigning clusters to splits, ensuring that test sequences are evolutionarily distant from training sequences. This approach is standard for protein benchmarks (using tools like CD-HIT or MMseqs2) but less consistently applied for DNA benchmarks.\nChromosome-based splitting holds out entire chromosomes for testing, preventing any position-based leakage within chromosomes. This approach is common for regulatory benchmarks but does not account for homologous sequences on different chromosomes. Temporal splitting reserves recent data for testing, appropriate when benchmarks derive from databases with submission timestamps. Each splitting strategy tests different aspects of generalization; the choice should match the intended deployment scenario.\n\n\n20.5.3 Metric Selection and Aggregation\nBenchmark metrics determine what aspects of model performance are measured. Discrimination metrics (AUROC, AUPRC, correlation) assess whether models rank predictions correctly. Calibration metrics (ECE, reliability diagrams) assess whether predicted probabilities match observed frequencies. Clinical utility metrics (net benefit, decision curves) assess whether predictions improve decisions compared to treating all patients the same.\nDifferent metrics can yield different rankings of models. A model with superior discrimination may have poor calibration, predicting the right relative order but wrong absolute probabilities. Choosing which metric to optimize, and how to aggregate across multiple tasks or datasets, involves implicit decisions about what matters for downstream use.\nAggregation across tasks raises additional issues. Mean performance across many tasks weights each task equally, regardless of clinical importance or dataset quality. Median performance is robust to outliers but obscures variation. Reporting full distributions of task-level performance provides more information but complicates comparison. The choice of aggregation method can substantially affect which model appears best.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html#benchmark-saturation-and-staleness",
    "href": "p5-ch20-benchmarks.html#benchmark-saturation-and-staleness",
    "title": "20  Benchmarks for Genomic AI",
    "section": "20.6 Benchmark Saturation and Staleness",
    "text": "20.6 Benchmark Saturation and Staleness\nBenchmarks have finite useful lifetimes. As models improve, benchmarks saturate; as data and methods evolve, benchmarks become stale.\n\n20.6.1 Saturation: When Benchmarks Stop Discriminating\nA benchmark saturates when the best models achieve performance that cannot be meaningfully improved. Saturation may reflect fundamental limits (the benchmark approaches the Bayes error rate), measurement noise (the benchmark’s labels are too noisy to support finer discrimination), or ceiling effects (the metric itself cannot distinguish between excellent and perfect performance).\nSaturation is problematic because it removes the benchmark’s value for model selection. When all reasonable models achieve 0.97 AUROC, differences between 0.970 and 0.975 are unlikely to reflect meaningful capability differences. Yet benchmark reporting conventions often emphasize such decimal places, creating an illusion of progress.\nDetecting saturation requires estimating the irreducible error. For benchmarks with replicate measurements, comparing model performance to replicate concordance provides an upper bound: models cannot systematically outperform the reproducibility of the underlying assay. For benchmarks without replicates, saturation is harder to diagnose. One heuristic is tracking the rate of improvement: when new methods provide diminishing gains despite substantial architectural innovations, saturation is likely.\nThe response to saturation should be moving to harder benchmarks that still discriminate between methods, developing new benchmarks that capture aspects of performance that existing benchmarks miss, and retiring saturated benchmarks from active leaderboard competition while retaining them as sanity checks.\n\n\n20.6.2 Staleness: When Benchmarks Diverge from Practice\nBenchmarks become stale when they no longer reflect current data, methods, or clinical practice. Assays evolve: a benchmark constructed from early ENCODE data may not represent current experimental protocols. Annotations improve: gene models, variant classifications, and functional element maps are continuously updated. Clinical practice shifts: treatment guidelines and diagnostic criteria change the meaning of historical labels.\nStaleness is insidious because it erodes benchmark validity gradually rather than abruptly. A benchmark that accurately represented regulatory prediction in 2015 may systematically misrepresent it in 2025, yet the benchmark’s continued use perpetuates optimization for an outdated target.\nAddressing staleness requires periodic benchmark refresh with updated data and annotations, version control that documents exactly what each benchmark version contains, and awareness that performance on historical benchmarks may not predict performance on current data.\n\n\n20.6.3 Leakage from Scale\nModern foundation models are pretrained on corpora that may include most publicly available genomic data. This creates novel leakage risks distinct from classical train-test overlap. A model pretrained on all ENCODE data may effectively have seen the exact experiments used in many regulatory benchmarks. A model pretrained on all UniRef may have seen sequences highly similar to protein benchmark test sets. This pretraining-benchmark overlap inflates performance in ways that are difficult to detect and even more difficult to correct.\nLeakage from scale is particularly problematic because it is often undocumented. Model papers rarely enumerate exactly which datasets were included in pretraining corpora, and benchmark papers rarely specify which datasets should be excluded. The result is ambiguity about whether benchmark success reflects genuine generalization or memorization from pretraining.\nMitigating leakage from scale requires explicit documentation of pretraining corpora, tools or hashes that help identify overlap between pretraining data and benchmark test sets, and held-out evaluation consortia that reserve data specifically for assessment without any use in pretraining.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html#the-benchmark-deployment-gap",
    "href": "p5-ch20-benchmarks.html#the-benchmark-deployment-gap",
    "title": "20  Benchmarks for Genomic AI",
    "section": "20.7 The Benchmark-Deployment Gap",
    "text": "20.7 The Benchmark-Deployment Gap\nHigh benchmark performance does not guarantee deployment success. Understanding why requires examining the systematic differences between benchmark settings and real-world applications.\n\n20.7.1 Distribution Shift\nBenchmark test sets sample from the same distribution as training sets. Deployment populations may differ systematically. For variant effect prediction, benchmark variants are typically common enough to appear in multiple databases, while deployment often targets rare variants seen in single individuals. For regulatory prediction, benchmarks derive from well-studied cell types and tissues, while deployment may require prediction in understudied contexts.\nDistribution shift manifests as degraded performance, but the pattern of degradation varies. Some models degrade gracefully, maintaining reasonable accuracy across the distribution shift. Others degrade catastrophically, with confident predictions that prove systematically wrong. Benchmarks that include held-out subpopulations or out-of-distribution test sets provide some information about robustness, but cannot anticipate every deployment scenario.\n\n\n20.7.2 Calibration Requirements\nClinical deployment requires not just accurate rankings but accurate probability estimates. A variant classifier that achieves 0.95 AUROC by assigning probability 0.9 to all pathogenic variants and 0.3 to all benign variants discriminates well but provides miscalibrated uncertainty. Clinical decisions that depend on thresholded predictions (reporting variants above a certain probability) will perform poorly if those probabilities don’t reflect actual pathogenicity rates.\nMost benchmark metrics emphasize discrimination over calibration. AUROC is invariant to monotonic transformations of predicted probabilities. Correlation measures rank preservation. As a result, models may be optimized for benchmark success through strategies that damage calibration. The benchmark-deployment gap for calibration can be large even when discrimination metrics are excellent.\n\n\n20.7.3 Metric Mismatch\nBenchmarks optimize specific metrics that may not align with deployment objectives. AUROC weights errors equally regardless of where they occur on the score distribution, but clinical utility may depend primarily on performance at specific operating points. Correlation rewards getting the overall pattern right but may not penalize systematic errors in clinically important regions.\nThe gap between optimized metrics and deployment objectives creates misaligned incentives. Model developers optimize for benchmark success, which rewards specific metric improvements. Deployment success may require different tradeoffs: prioritizing calibration over discrimination, minimizing false negatives over false positives, or performing well on specific subpopulations rather than overall.\n\n\n20.7.4 Practical Constraints\nDeployment environments impose constraints that benchmarks typically ignore. Inference speed matters when predictions must be returned in clinical timescales. Model size matters when deployment hardware has limited memory. Interpretability matters when predictions must be explained to clinicians or patients. Benchmarks that evaluate only accuracy miss these dimensions of deployment fitness.\nThe benchmark-deployment gap is not merely a technical inconvenience. It represents a fundamental tension between evaluation tractability and deployment validity. Benchmarks are valuable precisely because they are standardized, reproducible, and comparable across methods. Deployment is valuable precisely because it addresses the specific needs of real-world applications. Bridging this gap requires benchmark designs that better approximate deployment conditions and deployment evaluations that provide feedback to benchmark development.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html#systematic-gaps-in-current-benchmarks",
    "href": "p5-ch20-benchmarks.html#systematic-gaps-in-current-benchmarks",
    "title": "20  Benchmarks for Genomic AI",
    "section": "20.8 Systematic Gaps in Current Benchmarks",
    "text": "20.8 Systematic Gaps in Current Benchmarks\nDespite the proliferation of benchmark suites, systematic gaps remain in the genomic evaluation landscape.\nVariant types: Structural variants, inversions, copy number variants, and complex rearrangements are rarely evaluated despite accounting for substantial genomic variation and disease burden. Repeat regions are often excluded or masked. Multi-variant effects and haplotype-specific phenomena receive minimal attention.\nPopulations: Non-European ancestry groups remain severely underrepresented. Performance stratified by ancestry reveals gaps that aggregate metrics conceal. Environmental diversity (lifestyle, exposures, treatments) that shapes phenotypic expression is rarely incorporated.\nModalities: Long-read sequencing data is scarce in benchmarks despite its advantages for structural variants and phasing. Single-cell benchmarks are emerging but remain limited compared to bulk assay benchmarks. Spatial transcriptomics and other emerging modalities have minimal coverage.\nClinical endpoints: Most benchmarks use molecular surrogates rather than hard clinical endpoints. Disease incidence, progression, treatment response, and patient-reported outcomes are rarely the direct prediction target. The gap between molecular proxy accuracy and clinical utility remains poorly characterized.\nThese gaps mean that strong benchmark performance may not predict utility for underserved populations, understudied variant classes, or clinical applications that depend on endpoints the benchmarks don’t measure.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch20-benchmarks.html#summary",
    "href": "p5-ch20-benchmarks.html#summary",
    "title": "20  Benchmarks for Genomic AI",
    "section": "20.9 Summary",
    "text": "20.9 Summary\nBenchmarks structure the incentives of genomic AI development. The specific tasks, metrics, and leaderboards that the community adopts determine what models are optimized for and what claims of progress are evaluated against. This chapter surveyed the benchmark landscape across biological modalities:\nProtein benchmarks, including TAPE, FLIP, and ProteinGym, provide the most mature evaluation ecosystem. These benchmarks test whether language model representations capture structural, functional, and evolutionary constraints on protein sequences. DNA and regulatory benchmarks span classical classification tasks, quantitative signal prediction, and recent standardized suites like Genomic Benchmarks, BEND, and long-range evaluation frameworks. Variant effect benchmarks connect sequence changes to functional and clinical consequences, using both clinical databases (ClinVar, CAGI) and high-throughput experimental measurements (DMS, MPRA). Trait-level benchmarks evaluate complex phenotype prediction and genetic discovery, with frameworks like TraitGym and EmbedGEM specifically designed for foundation model assessment.\nAcross all categories, we identified persistent challenges: benchmark saturation that reduces discriminative power, staleness that erodes validity over time, leakage risks that inflate apparent performance, and systematic gaps in population, variant type, and clinical endpoint coverage. The benchmark-deployment gap, where strong benchmark performance fails to predict deployment success, represents perhaps the most consequential limitation.\nThe methodological principles for using benchmarks properly, including experiment design, metric selection, and avoiding common pitfalls, are the subject of Chapter 21. The confounding issues that plague both benchmark construction and model training are examined in Chapter 22. Together with this catalog of the benchmark landscape, these chapters provide the foundation for rigorous evaluation of genomic foundation models.\n\n\n\n\nGrešová, Katarína, Vlastimil Martinek, David Čechák, Petr Šimeček, and Panagiotis Alexiou. 2023. “Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification.” BMC Genomic Data 24 (1): 25. https://doi.org/10.1186/s12863-023-01123-8.\n\n\nMukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom Soare, Raghav Tandon, David Amar, et al. 2024. “EmbedGEM: A Framework to Evaluate the Utility of Embeddings for Genetic Discovery.” Bioinformatics Advances 4 (1). https://doi.org/10.1093/bioadv/vbae135.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks for Genomic AI</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html",
    "href": "p5-ch21-eval.html",
    "title": "21  Evaluation Methodology",
    "section": "",
    "text": "21.1 Why Random Splits Fail\nA model that achieves state-of-the-art performance on a chromatin benchmark may fail catastrophically on real clinical variants. A foundation model that improves AUROC from 0.89 to 0.90 may have learned nothing about biology and everything about data leakage. A polygenic score that predicts disease with apparent precision in British biobank participants may perform no better than random chance in African populations. The history of genomic machine learning is littered with models that worked brilliantly on paper and failed quietly in practice.\nThe fundamental problem is that genomic data makes it exceptionally easy to fool yourself. Sequences share evolutionary history. Variants cluster in families and populations. Experimental measurements carry batch effects invisible to the untrained eye. Training labels often derive from the very databases that will be used for evaluation, creating circular validations that inflate performance without testing genuine predictive power. Random data splits that work perfectly well for natural images become actively misleading when applied to genomes, proteins, or variants. Every shortcut that simplifies evaluation in other domains creates an opportunity for false confidence in genomics.\nChapter 20 catalogs the benchmark landscape in detail, describing what tasks exist, how they are constructed, and what capabilities they probe. This chapter addresses the complementary question: how to use benchmarks appropriately to draw valid conclusions about model performance. The difference between a trustworthy evaluation and a misleading one often lies not in the choice of benchmark but in the details of how that benchmark is applied. Data splitting strategies, metric selection, baseline comparisons, ablation designs, and statistical testing collectively determine whether reported results mean anything at all.\nThe principles developed here apply across all the benchmark categories described in Chapter 20, from chromatin state prediction to clinical variant classification. By this point in the book, we have encountered genomic models deployed at almost every scale: variant calling from NGS reads (Chapter 1), polygenic scores and GWAS (Chapter 3), classical deleteriousness predictors (Chapter 4), foundation model approaches to variant effects (Chapter 14), CNN-based regulatory models (Chapter 6), and genomic language models (Chapter 11, Chapter 10). Clinical risk prediction and rare disease diagnosis (?sec-clinical-risk, Chapter 26) add still more evaluation considerations. What has been missing is a unified treatment of the question: what does it mean for a genomic model to work, and how do we know when it does?\nThe standard machine learning recipe calls for randomly partitioning data into training, validation, and test sets. For image classification or sentiment analysis, this approach works well because individual examples are approximately independent. A photograph of a cat shares no special relationship with another photograph of a different cat beyond their common label. Random assignment ensures that training and test distributions match, and performance on the test set provides an unbiased estimate of performance on new examples from the same distribution.\nGenomic data violates these assumptions at every level. Consider a protein dataset where the goal is to predict stability from sequence. Proteins in the same family share evolutionary history and often similar structures. If a training set includes beta-lactamase variants from E. coli and the test set includes beta-lactamase variants from Klebsiella, the model may appear to generalize to “new” proteins while actually recognizing sequence patterns it saw during training. The test performance reflects memorization of family-specific features rather than general principles of protein stability.\nThe problem compounds when sequence identity is high. Two proteins sharing 80% sequence identity will typically have similar structures and functions. A model trained on one and tested on the other is not really being tested on a novel example; it is being asked to interpolate within a region of sequence space it has already explored. Even at 30% sequence identity, the so-called “twilight zone” of homology detection, proteins often share structural and functional similarities that can be exploited by sufficiently powerful models.\nVariant-level data presents analogous challenges. Variants within the same gene share genomic context, and variants affecting the same protein domain share structural environment. Variants from the same individual share haplotype background. Variants from the same population share allele frequency distributions shaped by demographic history. Each of these relationships creates opportunities for models to learn shortcuts that generalize within the training distribution but fail on genuinely novel examples.\nThe consequence is that random splits systematically overestimate generalization. A model that achieves 0.90 AUROC with random splitting might achieve only 0.75 AUROC when evaluated on truly held-out examples, with the gap reflecting how much the model learned about biology versus how much it learned about the structure of the training data. Recognizing this problem is the first step toward solving it.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#homology-aware-splitting",
    "href": "p5-ch21-eval.html#homology-aware-splitting",
    "title": "21  Evaluation Methodology",
    "section": "21.2 Homology-Aware Splitting",
    "text": "21.2 Homology-Aware Splitting\nThe solution to homology-driven leakage is to explicitly account for sequence similarity when constructing data splits. Rather than random assignment, examples are clustered by sequence identity, and entire clusters are assigned to training, validation, or test sets. This ensures that no test example is “too similar” to any training example, forcing the model to demonstrate genuine generalization.\n\n21.2.1 Clustering Tools and Workflows\nTwo tools dominate homology-aware splitting in practice. CD-HIT clusters sequences by greedy incremental clustering, assigning each sequence to an existing cluster if it exceeds a similarity threshold to the cluster representative, or creating a new cluster otherwise. The algorithm is fast and scales to millions of sequences. For proteins, a typical workflow clusters at 40% sequence identity for stringent splitting or 70% for moderate splitting. For nucleotide sequences, thresholds are typically higher (80-95%) due to different evolutionary rates.\n# Cluster proteins at 40% identity\ncd-hit -i proteins.fasta -o proteins_clustered -c 0.4 -n 2\n\n# Cluster nucleotides at 90% identity  \ncd-hit-est -i sequences.fasta -o sequences_clustered -c 0.9 -n 8\nMMseqs2 offers faster clustering with similar sensitivity, becoming essential for large-scale analyses. The tool supports multiple clustering modes and can handle databases with hundreds of millions of sequences. For foundation model pretraining where deduplication affects billions of sequences, MMseqs2 is often the only practical option.\n# Create database and cluster at 30% identity\nmmseqs createdb proteins.fasta DB\nmmseqs cluster DB DB_clu tmp --min-seq-id 0.3\nmmseqs createtsv DB DB DB_clu clusters.tsv\nThe choice of identity threshold involves trade-offs. Stricter thresholds (lower identity for proteins, higher for nucleotides) create harder generalization tests but may leave insufficient data for training if clusters are small. Permissive thresholds retain more training data but allow more leakage through homologous sequences. For protein function prediction, 30-40% identity thresholds are common; for variant effect prediction within genes, even stricter gene-family-level splits may be necessary.\n\n\n21.2.2 Practical Considerations\nSeveral subtleties affect the quality of homology-aware splits. Cluster size distribution matters: if one cluster contains half the data and is assigned to training, the remaining clusters may be too small or too biased to serve as representative test sets. Stratified sampling within clusters or careful balancing across splits can mitigate this issue.\nTransitive homology creates hidden relationships that pairwise clustering can miss. Protein A may share 35% identity with protein B, and protein B may share 35% identity with protein C, yet A and C share only 20% identity. If A is in training and C is in testing, B serves as an indirect bridge. Connected component analysis or multi-step clustering can address these transitive relationships, though at increased computational cost.\nDomain-level homology complicates whole-protein clustering. A multi-domain protein may share one domain with training proteins and another domain with test proteins. Whether this represents leakage depends on the prediction task: if predicting whole-protein function, shared domains matter; if predicting domain-specific properties, they matter more acutely. Domain-aware splitting assigns domains rather than whole proteins to clusters, though this requires domain annotation that may not always be available.\nFor genomic (non-protein) sequences, repeat elements and transposable elements create analogous challenges. A model trained to predict chromatin state may learn features of LINE elements that recur throughout the genome. Excluding repetitive regions from evaluation or explicitly accounting for repeat content can clarify what the model has actually learned about regulatory sequences versus repetitive element patterns.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#splitting-by-biological-axis",
    "href": "p5-ch21-eval.html#splitting-by-biological-axis",
    "title": "21  Evaluation Methodology",
    "section": "21.3 Splitting by Biological Axis",
    "text": "21.3 Splitting by Biological Axis\nBeyond sequence homology, genomic data admits multiple axes along which splits can be constructed. The choice of axis determines what kind of generalization is being tested.\n\n21.3.1 Splitting by Individual\nFor tasks involving human genetic variation, ensuring that data from the same individual (or related individuals) does not appear in both training and test sets is essential. A variant effect predictor trained on variants from person A and tested on other variants from person A may learn individual-specific patterns, such as haplotype structure or ancestry-correlated allele frequencies, that do not generalize to new individuals.\nFamily structure creates subtler leakage. First-degree relatives share approximately 50% of their genomes identical by descent. Even distant relatives share genomic segments that can be exploited by sufficiently powerful models. Best practice involves computing kinship coefficients across all individuals and either excluding one member of each related pair or assigning entire family clusters to the same split. The UK Biobank provides pre-computed relatedness estimates; other cohorts may require explicit calculation using tools like KING or PLINK.\n\n\n21.3.2 Splitting by Genomic Region\nChromosome-based splits assign entire chromosomes to training or testing. This approach is common in regulatory genomics, where models trained on chromosomes 1-16 are tested on chromosomes 17-22 (or similar partitions). The advantage is simplicity and reproducibility; the disadvantage is that chromosomes are not independent. Chromosome 6 contains the HLA region with its unusual patterns of variation and selection; chromosome 21 is small and gene-poor; sex chromosomes have distinct biology. Results may vary substantially depending on which chromosomes are held out.\nRegion-based splits hold out contiguous segments (e.g., 1 Mb windows) distributed across the genome. This provides more uniform coverage than chromosome splits but requires careful attention to boundary effects. If a regulatory element spans the boundary between training and test regions, parts of its context may leak into training.\n\n\n21.3.3 Splitting by Gene or Protein Family\nFor variant effect prediction, holding out entire genes or protein families tests whether models learn general principles versus gene-specific patterns. A model that achieves high accuracy by memorizing that TP53 variants are often pathogenic has not demonstrated understanding of mutational mechanisms. Gene-level splits force models to generalize to genes they have never seen, providing stronger evidence of biological insight.\nFamily-level splits extend this logic to groups of related genes. Holding out all kinases or all GPCRs tests whether models can generalize across evolutionary families. This is particularly stringent for protein structure and function prediction, where family membership strongly predicts properties.\n\n\n21.3.4 Splitting by Experimental Context\nMulti-task models that predict chromatin marks across cell types can be split by cell type rather than genomic position. Training on liver, lung, and brain while testing on heart and kidney assesses whether learned regulatory logic transfers across tissues. This matters because cell-type-specific factors drive much of regulatory variation; a model that has simply learned which regions are accessible in the training cell types may fail on novel cell types even when sequence features should transfer.\nSimilarly, models can be split by assay type (e.g., training on ATAC-seq, testing on DNase-seq), laboratory (to assess batch effects), or time point (for longitudinal data). Each split tests a different axis of generalization.\n\n\n21.3.5 Splitting by Ancestry\nFor human genomic applications, ancestry-stratified evaluation has become essential. Models trained predominantly on European ancestry cohorts often show degraded performance in African, East Asian, South Asian, and admixed populations. This degradation reflects both differences in allele frequency spectra and differences in linkage disequilibrium patterns that affect which variants are informative.\nBest practice reports performance separately for each major ancestry group represented in the data. When held-out ancestry groups are available (e.g., training on Europeans and testing on Africans), this provides the strongest test of cross-population generalization. When only European data are available, this limitation should be explicitly acknowledged, and claims about generalization should be appropriately modest. The confounding effects of ancestry on genomic predictions are detailed in Chapter 22.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#leakage-taxonomy-and-detection",
    "href": "p5-ch21-eval.html#leakage-taxonomy-and-detection",
    "title": "21  Evaluation Methodology",
    "section": "21.4 Leakage Taxonomy and Detection",
    "text": "21.4 Leakage Taxonomy and Detection\nEven with careful splitting, leakage can enter evaluations through multiple pathways. Understanding common leakage patterns helps practitioners design cleaner evaluations and critically assess published results.\n\n21.4.1 Feature Leakage\nFeature leakage occurs when input features encode information about the target that would not be available at prediction time. In genomics, conservation scores are a common source. If a model uses PhyloP scores as features and the target is pathogenicity, the model may learn that “conserved positions are more likely pathogenic” without learning anything about variant biology. This would be fine if conservation scores are intended to be part of the prediction pipeline, but problematic if the goal is to develop a model that can predict pathogenicity from sequence alone.\nSimilarly, population allele frequency encodes selection pressure. A model that learns “rare variants are more likely pathogenic” has discovered a useful heuristic but not necessarily a mechanistic understanding. Whether this counts as leakage depends on the intended use case. For clinical variant interpretation where allele frequency is always available, exploiting this feature is appropriate. For understanding variant biology, it may mask whether the model has learned anything beyond allele frequency.\n\n\n21.4.2 Label Leakage\nLabel leakage occurs when target labels are derived from information that the model can access through its features. The classic example is training pathogenicity predictors on ClinVar annotations while using sequence features that were used to construct ClinVar annotations. If ClinVar curators used SIFT and PolyPhen scores when classifying variants, and the model uses similar sequence features, high performance may reflect recapitulation of curation criteria rather than independent predictive power.\nTemporal label leakage is subtler. A model trained on ClinVar annotations from 2020 and tested on annotations from 2023 may perform well because new annotations were informed by model-like predictions. The apparent validation is circular: the model predicts labels that were partially derived from model-like reasoning.\n\n\n21.4.3 Benchmark Leakage\nBenchmark leakage occurs when test set construction was influenced by methods similar to those being evaluated. If a protein function benchmark was created by selecting proteins with high-confidence annotations, and those annotations were partly derived from sequence similarity searches, sequence-based models may perform well by exploiting the same similarity that guided benchmark construction.\nFoundation models face particular challenges with benchmark leakage. If a DNA language model is pretrained on all publicly available genomic sequence including ENCODE data, and then evaluated on ENCODE-derived benchmarks, the pretraining has exposed the model to information about the test distribution even if specific test examples were held out. The model may have learned statistical patterns in ENCODE data that transfer to ENCODE benchmarks without reflecting genuine biological understanding.\n\n\n21.4.4 Detection Strategies\nSeveral strategies help detect leakage. Baseline analysis asks whether simple models that could not plausibly have learned biology achieve suspiciously high performance. If a linear model using only allele frequency achieves 0.80 AUROC on a pathogenicity benchmark, and a sophisticated deep model achieves 0.82, the marginal improvement may not justify claims of biological insight.\nFeature ablation systematically removes potentially leaky features and measures performance degradation. If removing conservation scores causes a 20-point drop in AUROC, the model was heavily dependent on conservation rather than learning independent predictors.\nConfounder analysis explicitly models potential confounders and tests whether model predictions remain informative after conditioning. If a variant effect predictor’s scores become non-predictive after controlling for gene length and expression level, the model may have learned gene-level confounders rather than variant-level effects.\nTemporal validation evaluates models on data collected after the training data was frozen. If performance degrades substantially on newer data, the model may have been fitted to temporal artifacts in the original dataset.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#metrics-for-genomic-tasks",
    "href": "p5-ch21-eval.html#metrics-for-genomic-tasks",
    "title": "21  Evaluation Methodology",
    "section": "21.5 Metrics for Genomic Tasks",
    "text": "21.5 Metrics for Genomic Tasks\nMetrics quantify model performance but different metrics answer different questions. Choosing appropriate metrics requires clarity about what aspect of performance matters for the intended application.\n\n21.5.1 Discrimination Metrics\nFor binary outcomes (pathogenic versus benign, bound versus unbound, accessible versus closed), discrimination metrics assess how well the model separates classes. The area under the receiver operating characteristic curve (AUROC) measures the probability that a randomly selected positive example is ranked above a randomly selected negative example. AUROC is threshold-independent and widely reported but can be misleading when classes are highly imbalanced.\nThe area under the precision-recall curve (AUPRC) better reflects performance when positives are rare. For variant pathogenicity prediction, where perhaps 1% of variants are truly pathogenic, a model achieving 0.95 AUROC might still have poor precision at useful recall levels. AUPRC directly captures the precision-recall trade-off that matters for applications requiring both high sensitivity and manageable false positive rates.\nSensitivity, specificity, positive predictive value, and negative predictive value require specifying a decision threshold. These metrics are more interpretable for specific use cases (e.g., “the model identifies 90% of pathogenic variants while flagging only 5% of benign variants as false positives”) but require choosing thresholds that may not generalize across settings.\n\n\n21.5.2 Regression and Correlation Metrics\nFor continuous predictions (expression levels, effect sizes, binding affinities), correlation metrics assess agreement between predicted and observed values. Pearson correlation measures linear association; Spearman correlation measures rank association and is robust to nonlinear relationships. The coefficient of determination (R²) measures variance explained, though interpretation requires care when baseline performance is near zero.\nFor predictions at genomic scale (e.g., predicted versus observed expression across thousands of genes), these metrics may obscure important patterns. A model might achieve high genome-wide correlation by correctly predicting which genes are highly expressed while failing on the genes where predictions matter most. Task-specific stratification, such as correlation within expression quantiles or among disease-relevant genes, provides more actionable information.\n\n\n21.5.3 Ranking and Prioritization Metrics\nMany genomic workflows care about ranking rather than absolute prediction. Variant prioritization pipelines rank candidates for follow-up; gene prioritization ranks targets for experimental validation. Top-k recall measures the fraction of true positives captured in the top k predictions. Enrichment at k compares the true positive rate in the top k to the background rate. Normalized discounted cumulative gain (NDCG) weights ranking quality by position, penalizing relevant items placed lower in the list more than those placed near the top.\nThese metrics align with how predictions are actually used. If experimental capacity permits validating only 20 variants per locus, top-20 recall matters more than global AUROC. Reporting both global metrics and rank-aware metrics at relevant cutoffs provides a complete picture.\n\n\n21.5.4 Calibration Metrics\nCalibration assesses whether predicted probabilities match observed frequencies. A well-calibrated model that predicts 0.8 probability of pathogenicity should be correct about 80% of the time across all variants receiving that score. Reliability diagrams plot predicted probabilities against observed frequencies within binned intervals, with deviations from the diagonal indicating miscalibration. Expected calibration error (ECE) summarizes miscalibration as the weighted average absolute deviation across bins.\nCalibration matters whenever predictions inform decisions. A miscalibrated model that reports 0.95 probability when the true probability is 0.60 will lead to inappropriate confidence in uncertain predictions. Even models with excellent discrimination can be poorly calibrated, requiring post-hoc calibration methods such as Platt scaling or isotonic regression. Chapter 23 addresses calibration and uncertainty quantification in greater depth.\n\n\n21.5.5 Clinical Utility Metrics\nFor clinical applications, discrimination and calibration are necessary but not sufficient. Decision curves plot net benefit across decision thresholds, where net benefit weighs the value of true positives against the cost of false positives at each threshold. A model may achieve high AUROC but offer no net benefit at clinically relevant thresholds if it fails to discriminate in the region where decisions are actually made.\nNet reclassification improvement (NRI) measures how often adding genomic features to a clinical model changes risk classifications in the correct direction. This directly addresses whether genomics adds clinical value beyond existing predictors. ?sec-clinical-risk provides detailed treatment of clinical evaluation frameworks.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#baseline-selection",
    "href": "p5-ch21-eval.html#baseline-selection",
    "title": "21  Evaluation Methodology",
    "section": "21.6 Baseline Selection",
    "text": "21.6 Baseline Selection\nBaseline comparisons determine the meaning of reported performance. A model achieving 0.85 AUROC might represent a major advance if the best prior method achieved 0.70, or a trivial improvement if simple heuristics achieve 0.83. Choosing appropriate baselines is as important as choosing appropriate metrics.\n\n21.6.1 Strong Baselines, Not Straw Men\nThe temptation to compare against weak baselines inflates apparent contributions. A deep learning model compared against a naive prior or a deliberately crippled baseline will appear impressive regardless of whether it offers genuine value. Strong baselines force honest assessment of improvement.\nFor sequence-based predictions, position weight matrices (PWMs) and k-mer logistic regression provide classical baselines that capture sequence composition without deep learning. If a convolutional model barely outperforms logistic regression on k-mer counts, the convolutional architecture may not be contributing as much as claimed.\nFor variant effect prediction, simple features like allele frequency, conservation scores, and amino acid properties provide baselines that any sophisticated model should substantially exceed. CADD (Chapter 4) serves as a well-calibrated baseline that combines many hand-crafted features; outperforming CADD demonstrates that learning provides value beyond feature engineering.\nFor foundation models, comparisons should include both randomly initialized models of similar architecture (to isolate the value of pretraining) and simpler pretrained models (to isolate the value of scale or architectural innovations). Claiming that pretraining helps requires demonstrating improvement over training from scratch on the same downstream data.\n\n\n21.6.2 Historical Baselines and Progress Tracking\nComparing to methods from five years ago may demonstrate progress but overstates the contribution of any single method. Comparisons should include the best currently available alternatives, not just historically important ones. When prior work is not directly comparable (different data, different splits, different metrics), reimplementing baselines on common benchmarks provides fairer comparison.\nField-wide progress tracking benefits from persistent benchmarks with frozen test sets. Once test set results for a benchmark are published, that benchmark becomes less useful for future model development because the test set is no longer truly held out. Periodic benchmark refresh with new held-out data helps maintain evaluation integrity.\n\n\n21.6.3 Non-Deep-Learning Baselines\nDeep learning models should be compared against strong non-deep alternatives. Gradient-boosted trees, random forests, and regularized linear models often achieve competitive performance with far less computation. If a 100-million-parameter transformer barely outperforms XGBoost on tabular features, the complexity may not be justified.\nThis comparison is especially important for clinical deployment, where simpler models may be preferred for interpretability, computational efficiency, or regulatory approval. Demonstrating that deep learning provides substantial gains over strong non-deep baselines strengthens the case for adoption.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#ablation-studies",
    "href": "p5-ch21-eval.html#ablation-studies",
    "title": "21  Evaluation Methodology",
    "section": "21.7 Ablation Studies",
    "text": "21.7 Ablation Studies\nAblation studies systematically remove or modify model components to understand their contributions. Where baselines compare across methods, ablations investigate within a method, revealing which design choices actually matter.\n\n21.7.1 Component Isolation\nStandard ablations remove individual components: attention layers, skip connections, normalization schemes, specific input features. If removing attention heads causes minimal performance degradation, the model may not be exploiting long-range dependencies as claimed. If removing a particular input modality has no effect, that modality may not be contributing useful information.\nAblations should be designed to test specific hypotheses. If the claim is that a foundation model learns biologically meaningful representations, ablating pretraining (comparing to random initialization) directly tests this claim. If the claim is that cross-attention between modalities enables integration, ablating cross-attention while retaining separate encoders tests whether integration provides value.\n\n\n21.7.2 Hyperparameter Sensitivity\nReporting performance across hyperparameter ranges reveals robustness. A model that achieves state-of-the-art performance only at a narrow learning rate range with specific regularization may be overfit to the evaluation setup. Consistent performance across reasonable hyperparameter variations provides stronger evidence of genuine capability.\n\n\n21.7.3 Architecture Search Confounds\nWhen model development involves extensive architecture search, reported performance conflates the value of the final architecture with the value of search on the validation set. The validation set is no longer truly held out; it has been used to select among architectures. Final evaluation on a completely untouched test set, with the architecture fixed before test set examination, provides cleaner assessment.\n\n\n21.7.4 Reporting Standards\nAblation tables should clearly indicate what was changed in each condition, the number of random seeds or runs, and measures of variance. Single-run ablations can produce misleading results due to training stochasticity. Reporting means and standard deviations across multiple runs reveals whether observed differences exceed random variation.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#statistical-rigor",
    "href": "p5-ch21-eval.html#statistical-rigor",
    "title": "21  Evaluation Methodology",
    "section": "21.8 Statistical Rigor",
    "text": "21.8 Statistical Rigor\nPerformance differences between models may reflect genuine capability differences or random variation in training and evaluation. Statistical analysis distinguishes signal from noise.\n\n21.8.1 Significance Testing\nFor classification metrics, significance tests ask whether observed differences exceed what would be expected from sampling variation. Bootstrap confidence intervals resample the test set with replacement, recompute metrics on each resample, and report the distribution of metric values. Non-overlapping 95% confidence intervals suggest significant differences. Permutation tests shuffle predictions between models and measure how often shuffled differences exceed observed differences.\nFor comparing multiple models across multiple benchmarks, correction for multiple testing becomes important. Without correction, 20 pairwise comparisons will produce an expected one false positive at the 0.05 level even when all models perform equally. The Bonferroni correction divides the significance threshold by the number of tests; the Benjamini-Hochberg procedure controls false discovery rate with more power than Bonferroni.\n\n\n21.8.2 Effect Sizes\nStatistical significance does not imply practical significance. A difference of 0.001 AUROC might be statistically significant with millions of test examples while being practically meaningless. Effect sizes quantify the magnitude of differences independent of sample size. Cohen’s d for continuous outcomes and odds ratios for binary outcomes provide standardized measures of effect magnitude.\nReporting both significance tests and effect sizes provides complete information. A result that is statistically significant with a tiny effect size warrants different interpretation than one that is significant with a large effect size.\n\n\n21.8.3 Confidence Intervals on Metrics\nPoint estimates of AUROC or correlation should be accompanied by confidence intervals. DeLong’s method provides analytical confidence intervals for AUROC; bootstrap methods provide distribution-free intervals for any metric. Reporting “AUROC = 0.85 (95% CI: 0.82-0.88)” is more informative than “AUROC = 0.85” alone.\n\n\n21.8.4 Variance Across Random Seeds\nDeep learning models are sensitive to initialization and optimization stochasticity. Training the same architecture with different random seeds can produce substantially different results. Best practice trains multiple runs and reports means and standard deviations. If the standard deviation across runs exceeds the difference between methods, claimed improvements may not be reproducible.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#evaluating-foundation-models",
    "href": "p5-ch21-eval.html#evaluating-foundation-models",
    "title": "21  Evaluation Methodology",
    "section": "21.9 Evaluating Foundation Models",
    "text": "21.9 Evaluating Foundation Models\nGenomic foundation models (Chapter 10) admit multiple evaluation paradigms, each testing different aspects of learned representations.\n\n21.9.1 Zero-Shot Evaluation\nIn zero-shot evaluation, the pretrained model is applied without any task-specific training. For masked language models, this typically means using token probabilities to score variants or classify sequences. A variant that disrupts a position the model predicts with high confidence may indicate functional importance.\nZero-shot performance tests whether pretraining captures task-relevant structure without explicit supervision. Strong zero-shot performance suggests the pretraining objective aligned with the evaluation task; weak zero-shot performance suggests misalignment. Comparing zero-shot performance to simple baselines (e.g., conservation scores for variant effects) calibrates whether the foundation model provides value beyond what simpler approaches achieve.\n\n\n21.9.2 Linear Probing\nLinear probing freezes the foundation model and trains only a linear classifier on extracted embeddings. This isolates representation quality from fine-tuning capacity. If a linear probe on foundation model embeddings substantially outperforms a linear probe on random embeddings, the foundation model has learned useful features.\nLayer-wise probing reveals where information is encoded. Early layers may capture local sequence features while later layers capture more abstract patterns. If the information needed for a task is extractable from early layers, the model may not require the full depth of the architecture for that application.\n\n\n21.9.3 Fine-Tuning Evaluation\nFull fine-tuning adapts all model parameters to the downstream task. This provides the best performance but conflates representation quality with adaptation capacity. A foundation model might achieve high fine-tuned performance through the capacity of its architecture rather than the quality of its pretrained representations.\nComparing fine-tuned foundation models to equivalently architected models trained from scratch isolates the value of pretraining. If both approaches converge to similar performance given sufficient downstream data, pretraining provides label efficiency (less data needed to reach a given performance level) rather than improved final performance. Data efficiency curves, plotting performance against downstream training set size, reveal this trade-off.\n\n\n21.9.4 Transfer Across Tasks\nFoundation models justify their “foundation” designation by transferring to diverse downstream tasks. Evaluating on a single task, however well-designed, cannot assess breadth of transfer. Multi-task evaluation across regulatory prediction, variant effects, protein properties, and other applications reveals whether foundation models provide general-purpose representations or excel only on tasks similar to their pretraining objective.\nTransfer across species, tissues, and experimental modalities provides additional evidence of generalization. A DNA language model that transfers from human to mouse, or from blood cells to neurons, demonstrates that its representations capture biological principles rather than species-specific or tissue-specific patterns.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#calibration-essentials",
    "href": "p5-ch21-eval.html#calibration-essentials",
    "title": "21  Evaluation Methodology",
    "section": "21.10 Calibration Essentials",
    "text": "21.10 Calibration Essentials\nEven models with strong discrimination may produce probability estimates that mislead decisions. Calibration assesses whether predicted probabilities match observed frequencies, a property essential for rational decision-making.\n\n21.10.1 Assessing Calibration\nReliability diagrams bin predictions by predicted probability and plot the mean predicted probability against the observed frequency within each bin. Perfect calibration produces points along the diagonal; deviations reveal systematic over-confidence (points below the diagonal) or under-confidence (points above).\nExpected calibration error summarizes miscalibration as a single number: the weighted average absolute difference between predicted and observed probabilities across bins. Lower ECE indicates better calibration. However, ECE is sensitive to binning choices and should be reported alongside reliability diagrams for interpretability.\nCalibration by subgroup reveals whether miscalibration varies across populations or conditions. A model might be well-calibrated overall but systematically overconfident for rare variant classes or underrepresented ancestries. Stratified calibration analysis identifies these disparities.\n\n\n21.10.2 Recalibration Methods\nPost-hoc recalibration adjusts predicted probabilities to improve calibration without retraining the model. Temperature scaling divides logits by a learned temperature parameter before applying softmax, compressing or expanding the probability distribution. Platt scaling fits a logistic regression from model outputs to true labels. Isotonic regression fits a monotonic function that maps model outputs to calibrated probabilities.\nThese methods require held-out calibration data distinct from training and test sets. Calibrating on test data and then evaluating calibration on the same test data produces overoptimistic estimates.\nFor detailed treatment of calibration and uncertainty quantification, including epistemic versus aleatoric uncertainty and selective prediction, see Chapter 23.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#putting-it-all-together",
    "href": "p5-ch21-eval.html#putting-it-all-together",
    "title": "21  Evaluation Methodology",
    "section": "21.11 Putting It All Together",
    "text": "21.11 Putting It All Together\nWhen designing or evaluating a genomic model assessment, working through a systematic checklist helps identify gaps and potential problems.\nLevel of decision: Is the model intended for molecular prediction, variant prioritization, patient risk stratification, or clinical action? Metrics should align with the actual decision context. Enrichment metrics suit variant ranking; net benefit matters for clinical decisions.\nData splits: Are individuals, genomic regions, gene families, and ancestries appropriately separated? Has homology-aware clustering been applied with appropriate identity thresholds? Is there any plausible pathway for leakage or circularity?\nBaselines: Are comparisons made against the best available alternatives, not just historical or deliberately weak baselines? Do non-deep-learning baselines establish floors? Does the improvement over baselines justify the complexity?\nMetrics: Are multiple metrics reported to capture discrimination, calibration, and ranking quality? Are metrics computed with confidence intervals? Are subgroup-stratified metrics reported for fairness assessment?\nAblations: Have component contributions been isolated through systematic ablation? Is performance robust across hyperparameter ranges and random seeds?\nStatistical rigor: Are significance tests applied with multiple testing correction? Are effect sizes reported alongside p-values? Are confidence intervals provided for key metrics?\nFoundation model specifics: For foundation models, is performance reported across zero-shot, probing, and fine-tuning regimes? Do data efficiency curves reveal where pretraining value lies? Has transfer been tested across diverse tasks?\nRobustness: How does performance vary across cohorts, platforms, and ancestries? How does the model behave under distribution shift, missing data, or label noise?\nThis checklist is not exhaustive but covers the most common evaluation pitfalls. Working through it systematically at the design stage can prevent problems that are difficult to fix retrospectively. Reviewers and readers can use the same checklist to critically assess published work.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch21-eval.html#looking-forward",
    "href": "p5-ch21-eval.html#looking-forward",
    "title": "21  Evaluation Methodology",
    "section": "21.12 Looking Forward",
    "text": "21.12 Looking Forward\nRigorous evaluation requires sustained effort at every stage, from data splitting through statistical analysis. The shortcuts that accelerate research in other domains, random splits, single-metric comparisons, significance tests without effect sizes, produce misleading conclusions when applied to genomic data. Homology, population structure, batch effects, and label circularity create countless opportunities for self-deception.\nThe chapters that follow address complementary aspects of reliability. Chapter 22 examines how confounders, biases, and data artifacts can produce evaluation results that evaporate under deployment. Population stratification, batch effects, and benchmark leakage can all create illusions of performance that careful evaluation might fail to detect. Chapter 23 develops the theory and practice of uncertainty quantification, extending the calibration discussion here to cover epistemic versus aleatoric uncertainty, selective prediction, and communication of model confidence. Chapter 24 addresses the complementary question of not just whether models work but why they work, developing tools to distinguish genuine biological insight from pattern matching on confounded data.\nTogether with the benchmark survey in Chapter 20, these chapters equip readers with the critical perspective needed to engage with the genomic foundation model literature. The question is never simply “what is the AUROC?” but rather “what has been demonstrated, and how much should we trust it?” The answer depends on the details.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Methodology</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html",
    "href": "p5-ch22-confounding.html",
    "title": "22  Confounders in Model Training",
    "section": "",
    "text": "22.1 Confounding, Bias, and Leakage\nA variant effect predictor trained on ClinVar achieves 0.92 AUC on held-out variants from the same database, yet performance drops to 0.71 when evaluated on a prospectively collected clinical cohort. A polygenic risk score for coronary artery disease stratifies European ancestry individuals with impressive discrimination, then fails almost completely when applied to individuals of African ancestry. A gene expression model trained on GTEx data predicts tissue-specific patterns with apparent precision, until deployment reveals it has learned to distinguish sequencing centers rather than biological states.\nThese failures share a common cause: the models learned shortcuts rather than biology. Genomic datasets encode hidden structure including ancestry, family relatedness, sequencing center, capture kit, hospital system, recruitment year, and label curation protocol. These factors correlate with both features (genotypes, expression levels, epigenomic marks) and labels (disease status, variant pathogenicity, molecular phenotypes). When such confounders remain uncontrolled, models exploit them. The central challenge is that confounded models can appear to work, sometimes spectacularly well, until they encounter data where the shortcuts no longer apply.\nThis problem is not unique to deep learning. Linear regression and logistic models suffer from the same biases. What makes confounding particularly dangerous in the foundation model era is scale: larger datasets and more expressive architectures make it easier to discover subtle shortcuts that remain invisible in standard diagnostics but cause dramatic failures when distribution shifts at deployment. A shallow model might miss the correlation between sequencing center and disease status; a transformer with hundreds of millions of parameters will find it if that correlation helps optimize the training objective.\nThe discussion here builds on the benchmark construction principles in Chapter 20 and the methodological framework in Chapter 21. Where those chapters address how to measure performance, this chapter addresses why performance measurements can be misleading and what to do about it. The emphasis is on pitfalls: ways models can appear impressive while failing to capture the biology we care about.\nThe terminology of confounding, bias, and leakage describes distinct phenomena that often co-occur and reinforce each other. Precision in language helps clarify what has gone wrong when a model fails.\nA confounder is a variable that influences both the input features and the label. Ancestry provides a canonical example: it affects allele frequencies across the genome (the features) and disease risk through environmental, socioeconomic, and healthcare pathways (the labels). If ancestry is not explicitly modeled or controlled, a model trained to predict disease may learn to identify ancestry rather than disease biology. The prediction appears accurate because ancestry correlates with outcome, but the model has captured correlation rather than mechanism.\nBias refers to systematic deviation from the quantity we intend to estimate or predict. Bias can result from confounding, but also arises from measurement error, label definitions, sampling procedures, or deployment differences. A case-control study with 50% disease prevalence will train models that systematically over-predict risk when deployed in populations where true prevalence is 5%. The model may be perfectly calibrated for the training distribution yet dangerously miscalibrated for clinical use.\nLeakage occurs when information about the test set inadvertently influences model training or selection. Leakage pathways include overlapping individuals or variants between training and evaluation, shared family members across splits, duplicated samples under different identifiers, and indirect channels such as pretraining on resources that later serve as benchmarks. The circularity between computational predictors and ClinVar annotations discussed in Chapter 4 exemplifies this last category: CADD-like scores influence which variants receive pathogenic annotations, and those annotations then become training labels for the next generation of predictors.\nDistribution shift describes mismatch between training and deployment data distributions. Shift can be driven by changes in ancestry composition, sequencing technology, clinical coding practices, or temporal trends in care. A model that learns hospital-specific coding patterns will fail when deployed at a different institution, not because the biology differs but because the label generation process does.\nThese phenomena interact. Confounders create biases in estimated effects. Leakage hides those biases by making held-out performance appear better than warranted. Distribution shift then reveals the problem when deployment performance collapses. For foundation models, three features magnify these risks. First, genomes encode ancestry, relatedness, and assay conditions in thousands of subtle features, even when those labels are never explicitly provided. Second, large transformers find shortcuts that smaller models would miss if those shortcuts improve the training objective. Third, complex training regimes involving pretraining on biobank-scale data, fine-tuning on curated labels, and evaluation on community benchmarks create many opportunities for direct and indirect leakage.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#sources-of-confounding-in-genomic-data",
    "href": "p5-ch22-confounding.html#sources-of-confounding-in-genomic-data",
    "title": "22  Confounders in Model Training",
    "section": "22.2 Sources of Confounding in Genomic Data",
    "text": "22.2 Sources of Confounding in Genomic Data\nConfounders in genomic modeling cluster into several categories, though the same underlying variable (such as recruitment site) may simultaneously induce ancestry differences, batch effects, and label bias.\nPopulation structure and relatedness encompasses continental and sub-continental ancestry, family relationships (siblings, parent-offspring pairs, cryptic relatedness detectable only through genotype similarity), and founder effects that create local haplotype structure. Ancestry affects both features and many phenotypes of interest, creating classic confounding. Relatedness creates a more subtle problem: when close relatives appear in both training and test sets, models can memorize shared haplotype segments rather than learning generalizable patterns, producing inflated performance estimates that collapse for unrelated individuals.\nTechnical batch and platform effects arise throughout the sequencing and analysis pipeline. Different instruments produce distinct error profiles. Library preparation protocols vary in GC bias, coverage uniformity, and adapter content. Capture kits determine which genomic regions receive adequate coverage. Alignment algorithms and variant callers make different decisions at ambiguous positions. When samples from a particular batch disproportionately represent a specific label class (cases sequenced at one center, controls at another), models learn to distinguish batches rather than biology.\nCohort and institutional effects reflect differences in patient populations, clinical practices, and data collection procedures. Hospital systems use distinct coding practices, diagnostic thresholds, and follow-up schedules. Population-based biobanks differ from referral-center cohorts in disease severity, comorbidity patterns, and demographic composition. Individuals who receive genomic testing may be more severely affected, more affluent, or preferentially drawn from particular ancestry groups, introducing selection bias that distorts apparent variant-phenotype relationships.\nLabel and curation bias stems from how ground truth annotations are generated. Clinical labels derived from billing codes or problem lists reflect documentation practices as much as underlying disease. Variant pathogenicity databases exhibit the systematic biases detailed in Chapter 2: ClinVar annotations over-represent European ancestry, well-studied genes, and variants submitted by high-volume clinical laboratories (Landrum et al. 2018). Expression, regulatory, or splicing labels derived from specific tissues or cell lines may not generalize to other biological contexts. The circularity problem identified in Chapter 4 persists into the foundation model era: when model predictions influence which variants receive expert review, and expert classifications become training labels, feedback loops amplify historical biases.\nTemporal drift encompasses changes in clinical practice, diagnostic criteria, and coding conventions over time, evolving sequencing technologies and quality control pipelines, and shifts in the patient population served by a healthcare system. A model trained on 2015 data may fail on 2024 data not because biology changed but because documentation practices, coding standards, and available treatments all evolved.\nKnowledge-base and benchmark leakage occurs when resources like gnomAD or UK Biobank appear in both model training and evaluation. A foundation model pretrained on gnomAD allele frequencies, then evaluated on a benchmark that uses gnomAD for population filtering, faces indirect leakage even if specific variants do not overlap. Community benchmarks that reuse widely available variant sets across multiple publications create additional leakage pathways that accumulate over time as the field iterates.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#population-structure-as-a-shortcut",
    "href": "p5-ch22-confounding.html#population-structure-as-a-shortcut",
    "title": "22  Confounders in Model Training",
    "section": "22.3 Population Structure as a Shortcut",
    "text": "22.3 Population Structure as a Shortcut\nPopulation structure represents one of the most pervasive confounders in genomic modeling. The core issue is that ancestry simultaneously affects genomic features and many phenotypes through pathways that have nothing to do with direct genetic causation.\nHuman genetic variation is structured by ancestry: allele frequencies, haplotype blocks, and linkage disequilibrium patterns differ across populations in ways that reflect demographic history. Principal components computed from genome-wide genotypes provide a low-dimensional summary of this structure and have become standard in GWAS to correct for stratification (Patterson, Price, and Reich 2006; Price et al. 2006). Yet ancestry is not merely a statistical nuisance. It is intertwined with geography, environment, socioeconomic status, and access to healthcare, factors that directly impact disease risk, likelihood of receiving genetic testing, and the quality of phenotyping when testing occurs.\nConsider a rare disease clinic serving primarily individuals of European ancestry. This clinic contributes most pathogenic variant submissions to ClinVar, while variants observed predominantly in other ancestries remain classified as variants of uncertain significance (Landrum et al. 2018). A model trained on ClinVar may learn that European-enriched variants tend to have pathogenic labels and non-European-enriched variants tend to have uncertain or benign labels, not because of any biological difference in pathogenicity but because of differential clinical characterization. The model appears to predict pathogenicity while actually predicting ancestry-correlated ascertainment.\nFoundation models trained on nucleotide sequences see ancestry information directly: the distribution of k-mers and haplotypes differs by population. When such models are fine-tuned to predict disease risk or variant effects, they may leverage ancestry as a shortcut. Increasing model capacity does not solve this problem; it often makes it worse by enabling detection of increasingly subtle ancestry-linked features. The polygenic score portability literature provides stark evidence: risk scores derived from European ancestry cohorts show 40-75% reductions in prediction accuracy when applied to African ancestry individuals (Duncan et al. 2019). Similar patterns emerge for variant effect predictors and regulatory models, though they are often less thoroughly documented due to limited cross-ancestry evaluation.\nThis mismatch between the populations used for model development and the populations that would benefit from genomic medicine creates a fundamental tension between current practice and equitable healthcare. Models that work primarily for European ancestry individuals perpetuate existing health disparities, regardless of their benchmark performance.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#technical-artifacts-as-biological-signal",
    "href": "p5-ch22-confounding.html#technical-artifacts-as-biological-signal",
    "title": "22  Confounders in Model Training",
    "section": "22.4 Technical Artifacts as Biological Signal",
    "text": "22.4 Technical Artifacts as Biological Signal\nTechnical pipelines are complex, and each step from sample collection through final variant calls can introduce systematic differences that models may learn.\nSequencing centers differ in instruments, reagents, and quality control thresholds. Library preparation protocols produce distinct coverage profiles and GC bias patterns. Capture kits determine which genomic regions are well-covered and which have systematic dropout. Read length affects the ability to span repetitive regions and call structural variants. Alignment and variant calling algorithms make different decisions at ambiguous genomic positions.\nWhen samples from a particular batch or platform are disproportionately drawn from a specific phenotype class, models learn to distinguish batches. In high-dimensional feature spaces, even subtle batch-specific artifacts (coverage dips at particular loci, variant density patterns reflecting caller behavior, residual adapter sequences) can become predictive. Foundation models that process raw reads, coverage tracks, or variant streams are particularly vulnerable because batch signatures may be encoded in features that preprocessing would typically remove.\nCommon patterns suggesting batch confounding include embedding spaces where samples cluster by sequencing center rather than phenotype, strong predictive performance that collapses when evaluated on data from a new platform, and models that can accurately predict batch identity (sequencing center, capture kit, processing date) from inputs that should be batch-independent. When a model designed to predict disease can also predict which laboratory processed the sample, something has gone wrong.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#label-bias-and-circularity",
    "href": "p5-ch22-confounding.html#label-bias-and-circularity",
    "title": "22  Confounders in Model Training",
    "section": "22.5 Label Bias and Circularity",
    "text": "22.5 Label Bias and Circularity\nLabels in genomic applications rarely represent ground truth in any absolute sense. They represent the outputs of complex processes involving clinical documentation, expert review, computational prediction, and database curation. These processes introduce biases that models absorb and may amplify.\nClinical phenotypes derived from electronic health records inherit the limitations of medical documentation. Billing codes capture what was reimbursable, not necessarily what was present. Problem lists reflect what clinicians chose to document, which varies by specialty, institution, and individual practice patterns. Diagnostic criteria change over time, creating apparent temporal trends in disease prevalence that reflect evolving definitions rather than changing biology.\nVariant pathogenicity labels illustrate the problem of circularity. ClinVar aggregates submissions from clinical laboratories, research groups, and expert panels (Landrum et al. 2018). The evidence underlying these submissions often includes computational predictions: a laboratory may cite CADD, REVEL, or other predictors as supporting evidence for a pathogenic classification. When the next generation of predictors trains on ClinVar, it learns to replicate the computational predictions that contributed to those labels. Performance on ClinVar-derived benchmarks thus reflects, in part, agreement with previous predictors rather than independent biological insight.\nThis circularity extends across the ecosystem of genomic resources. gnomAD allele frequencies inform variant filtering in clinical pipelines. UK Biobank genotype-phenotype associations shape which variants receive functional follow-up. Structural annotations from ENCODE and Roadmap Epigenomics influence which regulatory regions are considered biologically important. Foundation models pretrained on these resources, then evaluated against benchmarks derived from the same resources, may achieve impressive scores while learning to reproduce the assumptions and biases of existing annotations rather than discovering new biology.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#data-splitting-and-benchmark-leakage",
    "href": "p5-ch22-confounding.html#data-splitting-and-benchmark-leakage",
    "title": "22  Confounders in Model Training",
    "section": "22.6 Data Splitting and Benchmark Leakage",
    "text": "22.6 Data Splitting and Benchmark Leakage\nData splitting is among the primary tools for assessing generalization, yet naive splits can silently permit leakage that inflates apparent performance.\nRandom individual-level splits assign samples randomly to training, validation, and test sets. This approach fails when samples are not independent: family members may appear on both sides of a split, allowing models to memorize shared haplotypes. Rare variant analysis is particularly vulnerable because disease-causing variants may be private to specific families, and memorizing which families have which variants is far easier than learning generalizable sequence-function relationships.\nFamily-aware splits address relatedness by ensuring that all members of a family appear in the same split. This prevents direct memorization of family-specific variants but does not address population structure (ancestry groups may remain imbalanced across splits) or other confounders.\nLocus-level splits hold out entire genomic positions, ensuring that no variant at a test position appears during training. This stringent approach prevents models from memorizing site-specific patterns and is essential for variant effect prediction where the goal is to score novel variants at positions the model has never seen. Many published benchmarks fail to implement locus-level splitting, allowing models to achieve high scores by recognizing familiar positions rather than learning generalizable effects.\nRegion or chromosome splits hold out entire genomic regions, testing whether models learn biology that transfers across the genome rather than region-specific patterns. This is particularly relevant for regulatory prediction, where local chromatin context may differ between regions.\nCohort or site splits hold out entire institutions, sequencing centers, or biobanks, directly testing robustness to the batch and cohort effects discussed above. Models that perform well only within their training cohort but fail on held-out cohorts have learned institution-specific patterns.\nTime-based splits use temporal ordering, training on earlier data and evaluating on later data. This approach simulates prospective deployment and tests robustness to temporal drift. A model trained on 2018 data and evaluated on 2023 data faces realistic distribution shift that random splits would obscure.\nBeyond explicit split design, indirect leakage remains a concern. A variant that appears in ClinVar may also appear in gnomAD (with population frequency information), in functional assay datasets (with splicing or expression effects), and in literature-derived databases (with disease associations). Pretraining on any of these resources while evaluating on another creates indirect information flow that standard deduplication would miss.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#detecting-confounding",
    "href": "p5-ch22-confounding.html#detecting-confounding",
    "title": "22  Confounders in Model Training",
    "section": "22.7 Detecting Confounding",
    "text": "22.7 Detecting Confounding\nConfounding is often subtle, requiring systematic diagnostics rather than reliance on aggregate performance metrics.\nConfounder-only baselines provide the most direct test. Train simple models (logistic regression, gradient boosting) using only potential confounders: ancestry principal components, batch indicators, sequencing center identifiers, recruitment site. If these baselines approach the performance of complex genomic models, confounding likely drives a substantial portion of the signal. Reporting confounder-only baselines alongside genomic model results makes hidden shortcuts visible.\nSubgroup stratification reveals whether aggregate performance masks heterogeneity. Report metrics stratified by ancestry group, sequencing platform, institution, and time period. Include both discrimination (AUROC, AUPRC) and calibration diagnostics for each subgroup. Models may achieve high overall AUC while being poorly calibrated or nearly useless for specific subpopulations. Performance that varies dramatically across subgroups suggests confounding or distribution shift even when overall metrics appear strong.\nPrediction-confounder association tests whether model outputs encode confounders beyond what the label requires. Plot predictions against ancestry principal components, adjusting for true label status. Compare mean predicted risk across batches or time periods within the same true label class. Conduct formal association tests (regression, mutual information) between predictions and confounders. Strong residual associations indicate the model has learned confounder-related features that go beyond predicting the label itself.\nSplit sensitivity analysis varies the splitting strategy to probe for leakage. Re-evaluate performance under locus-level splits, cohort holdouts, or temporal splits. Large drops in performance under stricter splitting indicate that initial results were inflated. A model that achieves 0.90 AUC with random splits but only 0.75 AUC with locus-level splits has likely memorized site-specific patterns.\nNegative controls use outcomes known to be unrelated to genomics. If a model trained to predict disease from genotypes can also predict administrative outcomes (insurance type, documentation completeness) with similar accuracy, it has learned confounders. Shuffling labels within batch or ancestry strata should eliminate predictive signal; if it does not, the model exploits structure that transcends any specific outcome.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#mitigation-strategies",
    "href": "p5-ch22-confounding.html#mitigation-strategies",
    "title": "22  Confounders in Model Training",
    "section": "22.8 Mitigation Strategies",
    "text": "22.8 Mitigation Strategies\nNo mitigation strategy eliminates confounding entirely, and each involves trade-offs between bias, variance, and coverage. Nonetheless, several practical approaches substantially reduce reliance on confounders.\nStudy design and cohort construction provide the most robust protection. Matching cases and controls on age, sex, ancestry, and recruitment site removes these variables as potential confounders. Balanced sampling (down-sampling majority groups or up-sampling minority groups within mini-batches) prevents models from optimizing primarily for the majority pattern. Prospective collection with diversity targets ensures that training data represent the populations where models will be deployed. These design-based approaches constrain confounding before modeling begins, avoiding the need for post-hoc correction.\nCovariate adjustment explicitly models confounders rather than ignoring them. Including ancestry principal components, batch indicators, and site variables as covariates in regression or classification models allows estimation of outcome effects that are adjusted for these factors. Residualizing features or phenotypes with respect to known confounders before training genomic models removes confounder-associated variance, though this risks removing genuine biological signal when confounders correlate with causal variants. Mixed models or hierarchical structures model institution or batch as random effects, allowing estimation of genomic effects while accounting for clustering.\nDomain adaptation and invariance learning aim to learn representations that do not encode confounders. Adversarial training adds a discriminator that attempts to predict batch or ancestry from learned representations; the feature extractor is trained to maximize prediction accuracy while minimizing the discriminator’s ability to recover confounder labels, promoting invariance. Domain adversarial networks and importance weighting align distributions across batches or cohorts. Group-robust optimization targets worst-group performance (minimizing maximum error across subgroups) rather than average performance, encouraging models that work for all groups rather than only the majority.\nData curation and benchmark design determine what signals are available to learn. Deduplicating individuals, families, and variants across training and evaluation prevents direct memorization. Locus-level or region-level splits prevent site-specific pattern learning. Benchmarks that explicitly include diverse ancestries, institutions, and platforms test generalization rather than fitting to a single distribution. Documentation of overlaps between training resources and benchmarks enables readers to assess potential leakage.\nThese approaches complement each other. Good design reduces the need for modeling corrections. Adjustment handles residual confounding that design did not eliminate. Invariance learning provides additional protection when explicit confounder measurement is incomplete. Rigorous benchmark construction ensures that evaluation reflects generalization rather than shortcut learning.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#fairness-and-external-validity",
    "href": "p5-ch22-confounding.html#fairness-and-external-validity",
    "title": "22  Confounders in Model Training",
    "section": "22.9 Fairness and External Validity",
    "text": "22.9 Fairness and External Validity\nConfounding connects directly to fairness and health equity. Models that achieve high average performance while failing for specific populations may appear successful while exacerbating existing disparities.\nPolygenic risk scores illustrate this tension. European ancestry-derived scores predict cardiovascular disease, diabetes, and breast cancer risk reasonably well within European ancestry populations. Applied to African or Asian ancestry individuals, the same scores show substantially worse discrimination and calibration (Duncan et al. 2019). Healthcare systems that deploy these scores without ancestry-specific validation risk providing inferior risk stratification to already underserved populations.\nVariant interpretation exhibits similar patterns. ClinVar contains many more pathogenic variant classifications for European ancestry individuals than for other populations (Landrum et al. 2018). Predictors trained on ClinVar inherit this imbalance, performing better for variants common in European populations and worse for variants enriched in other ancestries. Clinical deployment of such predictors may reduce diagnostic yield for non-European patients.\nThe uncertainty quantification approaches discussed in Chapter 23 provide partial mitigation: models that report high uncertainty for under-represented populations at least flag predictions that should not be trusted. The interpretability methods in Chapter 24 can reveal when models rely on ancestry-correlated features. Yet technical solutions alone are insufficient. Addressing fairness requires intentional data collection that prioritizes under-represented populations, evaluation protocols that mandate subgroup analysis, and deployment decisions that consider equity alongside aggregate accuracy.\nExternal validity asks whether a model’s performance in one setting predicts its performance in another. Confounding and distribution shift often cause dramatic external validity failures. A model that achieves excellent metrics in the development cohort may fail when deployed at a different institution, in a different healthcare system, or in a different country. The clinical risk prediction frameworks in ?sec-clinical-risk emphasize multi-site validation precisely because single-site performance frequently fails to generalize.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#a-practical-checklist",
    "href": "p5-ch22-confounding.html#a-practical-checklist",
    "title": "22  Confounders in Model Training",
    "section": "22.10 A Practical Checklist",
    "text": "22.10 A Practical Checklist\nThe following checklist synthesizes the diagnostics and mitigations discussed above. Systematic application during model development and evaluation surfaces confounding that would otherwise remain hidden.\nPopulation structure and relatedness: Quantify ancestry via principal components and relatedness via kinship coefficients. Decide explicitly whether to match, stratify, or adjust for these factors, and document the justification. Report performance stratified by ancestry group. When family structure exists in the data, verify that relatives do not appear across train-test boundaries.\nData splits and leakage: Ensure individuals, families, and genomic loci do not cross the train-validation-test boundaries for target tasks. Implement stricter splits (locus-level, chromosome-level, cohort-based, time-based) and report the performance differences. Check for overlap with external databases or benchmarks used in evaluation and document any shared resources.\nBatch, platform, and cohort effects: Catalog technical variables (sequencing center, instrument, protocol, assay) and cohort identifiers. Test whether these variables predict labels or align with subgroups of interest. Use embedding visualizations, principal components, or simple classifiers to detect batch signatures. Apply mitigation (design matching, covariate adjustment, domain adaptation) when batch effects are detected.\nLabel quality and curation bias: Document how labels were defined and what processes (billing codes, expert review, computational prediction, registry inclusion) produced them. Quantify label noise where possible. Consider robust training strategies when labels are noisy. Assess how curated resources like ClinVar reflect historical biases and whether those biases affect evaluation validity.\nCross-group performance and fairness: Report metrics for each major subgroup (ancestry, sex, age, cohort, platform) rather than only aggregate performance. Examine calibration across groups, not just discrimination. Discuss clinical implications of residual performance gaps and whether deployment might worsen existing disparities.\nReproducibility and transparency: Document dataset construction, inclusion criteria, and splitting strategies completely. Release preprocessing, training, and evaluation code when feasible. Describe which confounders were measured, how they were handled, and what limitations remain.\nModels that pass this checklist provide more reliable evidence of genuine biological learning. Models that fail at multiple points may achieve benchmark success while learning shortcuts that will not transfer to new settings.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch22-confounding.html#toward-robust-genomic-models",
    "href": "p5-ch22-confounding.html#toward-robust-genomic-models",
    "title": "22  Confounders in Model Training",
    "section": "22.11 Toward Robust Genomic Models",
    "text": "22.11 Toward Robust Genomic Models\nThe confounding and bias problems described in this chapter are not reasons for despair. They are reasons for rigor. The same expressive capacity that enables foundation models to discover subtle shortcuts also enables them to learn complex biological patterns when training data and evaluation protocols are designed appropriately.\nSeveral trends support progress. Multi-ancestry biobanks and international collaborations are expanding the diversity of available training data. Benchmark developers are implementing stricter splitting protocols and requiring subgroup analyses. Pretraining strategies that explicitly promote invariance to technical factors are emerging. Uncertainty quantification methods (discussed in Chapter 23) provide mechanisms for models to express appropriate caution when inputs fall outside their training distribution.\nYet vigilance remains essential. New datasets bring new confounders. Novel architectures create new opportunities for shortcut learning. Community benchmarks accumulate indirect leakage as resources are reused across studies. The checklist provided above is not a one-time audit but an ongoing practice.\nThe goal is not models that perform well on convenient benchmarks but models that reveal genuine biology and behave reliably in diverse clinical and scientific settings. Achieving that goal requires treating confounding as a first-order concern throughout model development, not an afterthought addressed only when reviewers complain. The effort invested in rigorous evaluation pays dividends in models that actually work when deployed.\n\n\n\n\nDuncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R. Peterson, and B. Domingue. 2019. “Analysis of Polygenic Risk Score Usage and Performance in Diverse Human Populations.” Nature Communications 10 (1): 3328. https://doi.org/10.1038/s41467-019-11112-0.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006. “Population Structure and Eigenanalysis.” PLOS Genetics 2 (12): e190. https://doi.org/10.1371/journal.pgen.0020190.\n\n\nPrice, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E. Weinblatt, Nancy A. Shadick, and David Reich. 2006. “Principal Components Analysis Corrects for Stratification in Genome-Wide Association Studies.” Nature Genetics 38 (8): 904–9. https://doi.org/10.1038/ng1847.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders in Model Training</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html",
    "href": "p5-ch23-uncertainty.html",
    "title": "23  Uncertainty Quantification",
    "section": "",
    "text": "23.1 Types of Uncertainty in Genomic Prediction\nA clinician receives a pathogenicity prediction of 0.73 for a missense variant in a child with developmental delay. Should she act on this prediction? The answer depends entirely on whether that 0.73 means what it claims to mean. If the model is well-calibrated, approximately 73% of variants receiving this score are truly pathogenic, and the clinician can weigh this probability against the costs and benefits of further testing. If the model is miscalibrated, the true pathogenicity rate among variants scored at 0.73 could be 40% or 95%, and the nominal probability provides no reliable basis for decision-making. The distinction between these scenarios determines whether the family receives a timely diagnosis or continues their diagnostic odyssey for years.\nFoundation models produce continuous scores, but clinical decisions require categorical actions: test or do not test, treat or do not treat, report to the family or flag for expert review. This translation from probability to action only works when probabilities are trustworthy. A model that reports high confidence on inputs it has never seen before, or that systematically overstates certainty for variants in under-represented populations, fails at a fundamental level regardless of its average accuracy. Uncertainty quantification provides the tools to assess, calibrate, and communicate how much trust model predictions deserve, with particular attention to the unique challenges that arise when models encounter sequences, variants, and populations absent from their training data.\nThe concepts developed in this chapter directly enable the clinical risk prediction workflows examined in ?sec-clinical-risk and the variant interpretation pipelines detailed in Chapter 26, where uncertainty estimates determine which predictions are actionable and which require human review. Before predictions can inform medical decisions, we must understand when models know what they claim to know.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#sec-uncertainty-types",
    "href": "p5-ch23-uncertainty.html#sec-uncertainty-types",
    "title": "23  Uncertainty Quantification",
    "section": "",
    "text": "23.1.1 Why Uncertainty Matters\nClinical genetics operates under fundamental uncertainty. When a laboratory reports a variant of uncertain significance (VUS), they acknowledge that current evidence cannot confidently classify the variant as pathogenic or benign. ClinVar contains approximately two million VUS compared to roughly 250,000 variants classified as pathogenic, reflecting the reality that most genetic variation remains incompletely understood. Foundation models inherit and sometimes amplify this uncertainty: they may produce confident-seeming scores for variants where the underlying biology remains genuinely unknown.\nThe consequences of ignoring uncertainty extend beyond statistical abstraction. An overconfident pathogenic prediction may trigger unnecessary interventions, from prophylactic surgeries to reproductive decisions that alter family planning. An overconfident benign prediction may provide false reassurance, delaying diagnosis while a treatable condition progresses. In both cases, the harm stems not from prediction error per se but from the mismatch between stated confidence and actual reliability. A model that accurately conveys its uncertainty enables appropriate clinical reasoning even when the prediction itself is imperfect.\nDecision theory formalizes this intuition. The expected value of a clinical action depends on the probability of each outcome weighted by its utility. When a model reports 0.73 probability of pathogenicity, downstream decision-making implicitly assumes this probability is accurate. If the true probability is 0.50, actions optimized for 0.73 will systematically err. Uncertainty quantification ensures that the probabilities entering clinical decisions reflect genuine knowledge rather than artifacts of model architecture or training procedure.\n\n\n23.1.2 Epistemic Uncertainty\nA model trained exclusively on European-ancestry data encounters its first genome from an individual of African ancestry. The model’s predictions may be statistically valid within the distribution it has seen, yet unreliable for this new input due to limited exposure to ancestry-specific patterns of variation, linkage disequilibrium, and regulatory architecture. This uncertainty about what the model has learned, as distinct from noise inherent in the prediction task itself, constitutes epistemic uncertainty.\nEpistemic uncertainty arises from limitations in training data that could, in principle, be reduced by gathering more examples. In genomic foundation models, epistemic uncertainty concentrates in predictable regions of biological space. Proteins from poorly characterized families, where training data contained few homologs, exhibit high epistemic uncertainty because the model has limited basis for inference. Genes with few characterized variants in ClinVar or gnomAD provide sparse supervision, leaving the model uncertain about which sequence features distinguish pathogenic from benign variation. Rare variant classes, such as in-frame deletions in specific protein domains, appear infrequently in training data and consequently generate uncertain predictions. Populations under-represented in biobanks contribute fewer training examples, creating systematic epistemic uncertainty for individuals from these backgrounds.\nMathematically, epistemic uncertainty reflects uncertainty over model parameters or learned representations. A Bayesian perspective treats the trained model as one sample from a posterior distribution over possible models consistent with the training data. Different plausible models may disagree on predictions for inputs far from training examples while agreeing on well-represented inputs. This disagreement manifests as high variance in predictions across model variants, sensitivity to random initialization, or instability under small perturbations to training data.\nFoundation models exhibit epistemic uncertainty through several observable signatures. Embeddings for unfamiliar sequences cluster in sparse regions of representation space, distant from the dense clusters formed by well-represented sequence families. Ensemble members trained with different random seeds produce divergent predictions for novel inputs while converging for familiar ones. Fine-tuning on the same downstream task with different random seeds yields inconsistent results for edge cases. These signatures provide practical diagnostics for identifying when epistemic uncertainty is high.\n\n\n23.1.3 Aleatoric Uncertainty\nSome variants are genuinely ambiguous regardless of how much data we collect. The same pathogenic variant in BRCA1 causes breast cancer in one carrier but not another due to modifier genes, hormonal exposures, or stochastic developmental processes. Incomplete penetrance, the phenomenon where disease-associated variants do not always produce disease, creates irreducible uncertainty that no amount of training data can eliminate. This inherent randomness in the mapping from genotype to phenotype constitutes aleatoric uncertainty.\nAleatoric uncertainty reflects noise or stochasticity intrinsic to the prediction problem rather than limitations of the model. Variable expressivity means that even when a variant causes disease, the severity and specific manifestations vary across individuals. Measurement noise in functional assays introduces uncertainty into the labels used for training: deep mutational scanning experiments typically exhibit 10 to 20 percent technical variation between replicates, creating a floor below which prediction error cannot decrease regardless of model sophistication. Stochastic gene expression means that two genetically identical cells may express a gene at different levels due to random fluctuations in transcription and translation. These sources of randomness set fundamental limits on predictive accuracy.\nAleatoric uncertainty often varies with the input, a property termed heteroscedasticity. Coding variants in essential genes may have relatively low aleatoric uncertainty because strong selection pressure produces consistent phenotypic effects. Regulatory variants exhibit higher aleatoric uncertainty because their effects depend on cellular context, developmental timing, and interactions with other genetic and environmental factors. A model that captures this heteroscedasticity can provide more informative uncertainty estimates by conveying that some predictions are inherently more reliable than others.\n\n\n23.1.4 Decomposing Total Uncertainty\nTotal predictive uncertainty combines epistemic and aleatoric components, and distinguishing between them has practical implications for decision-making. High epistemic uncertainty suggests that gathering more data, either through additional training examples or further investigation of the specific case, could reduce uncertainty and improve the prediction. High aleatoric uncertainty indicates that the prediction is as good as it can get given inherent noise in the problem; additional data will not help because the underlying biology is stochastic.\nThe law of total variance provides a mathematical framework for decomposition. Total variance in predictions equals the sum of variance due to model uncertainty (epistemic) and variance inherent in the data-generating process (aleatoric). In practice, ensemble methods approximate epistemic uncertainty through disagreement between members: if five independently trained models produce predictions of 0.65, 0.68, 0.70, 0.72, and 0.75, the spread reflects epistemic uncertainty, while the residual variance within each model’s predictions reflects aleatoric uncertainty. Heteroscedastic neural networks, which output both a predicted mean and a predicted variance, can estimate aleatoric uncertainty by learning input-dependent noise levels.\nThese decompositions depend on modeling assumptions and provide approximations rather than exact separations. Ensemble disagreement may underestimate epistemic uncertainty if all members share similar biases from common training data. Heteroscedastic models may confound aleatoric and epistemic uncertainty if the training data is too sparse to reliably estimate noise levels. Despite these limitations, approximate decomposition provides actionable information: variants flagged for high epistemic uncertainty warrant additional data collection or expert review, while variants with high aleatoric uncertainty may require acceptance of irreducible limits on predictive confidence.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#sec-calibration",
    "href": "p5-ch23-uncertainty.html#sec-calibration",
    "title": "23  Uncertainty Quantification",
    "section": "23.2 Calibration: Do Confidence Scores Mean What They Say?",
    "text": "23.2 Calibration: Do Confidence Scores Mean What They Say?\n\n23.2.1 The Calibration Problem\nAlphaMissense outputs a continuous score between 0 and 1 for each possible missense variant in the human proteome. When it reports 0.85 for a particular variant, what does this number mean? If the model is calibrated, collecting all variants scored near 0.85 and checking their true clinical status should reveal that approximately 85% are pathogenic. Perfect calibration means that predicted probabilities match observed frequencies across the entire range of model outputs: among variants scored at 0.30, roughly 30% should be pathogenic; among variants scored at 0.95, roughly 95% should be pathogenic. This alignment between stated confidence and empirical accuracy is calibration, and most foundation models fail to achieve it.\nFormally, a model \\(f\\) mapping inputs \\(X\\) to probability estimates \\(p = f(X)\\) is calibrated if \\(P(Y=1 \\mid f(X)=p) = p\\) for all \\(p\\) in the interval from 0 to 1. The calibration condition requires that the model’s stated confidence equals the true probability of the positive class conditional on that stated confidence. Miscalibration occurs when this equality fails: overconfident models produce predicted probabilities that exceed true frequencies (a variant scored at 0.85 is pathogenic only 60% of the time), while underconfident models produce predicted probabilities below true frequencies.\nModern deep neural networks are systematically miscalibrated despite achieving high accuracy. Guo and colleagues demonstrated that contemporary architectures exhibit worse calibration than older, less accurate models (guo_calibration_2017?). The phenomenon arises because standard training objectives like cross-entropy loss optimize for discrimination (separating positive from negative examples) rather than calibration (matching predicted probabilities to frequencies). Over-parameterized models with capacity exceeding what the data requires can achieve near-perfect training loss while producing overconfident predictions on held-out data. The softmax temperature in transformer architectures affects the sharpness of probability distributions, and default settings often produce excessively peaked outputs.\nCalibration and discrimination are distinct properties. A model can achieve perfect AUROC, correctly ranking all positive examples above all negative examples, while being arbitrarily miscalibrated. If a classifier assigns probability 0.99 to all positive examples and 0.98 to all negative examples, it ranks perfectly but provides useless probability estimates. Conversely, a calibrated model that assigns 0.51 to positives and 0.49 to negatives would be calibrated but nearly useless for discrimination. Clinical applications typically require both: accurate ranking to identify high-risk variants and accurate probabilities to inform decision-making.\n\n\n23.2.2 Measuring Calibration\nReliability diagrams provide visual assessment of calibration by plotting predicted probabilities against observed frequencies. Construction involves binning predictions into intervals (commonly ten bins spanning 0 to 0.1, 0.1 to 0.2, and so forth), computing the mean predicted probability within each bin, computing the fraction of positive examples within each bin, and plotting these two quantities against each other. A perfectly calibrated model produces points along the diagonal where predicted probability equals observed frequency. Systematic deviations reveal calibration patterns: points below the diagonal indicate overconfidence (predictions exceed reality), points above indicate underconfidence, and S-shaped curves suggest nonlinear miscalibration requiring more flexible correction.\nExpected Calibration Error (ECE) provides a scalar summary of calibration quality. ECE computes the weighted average absolute difference between predicted probabilities and observed frequencies across bins:\n\\[\\text{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{n} \\left| \\text{acc}(B_m) - \\text{conf}(B_m) \\right|\\]\nwhere \\(B_m\\) denotes the set of examples in bin \\(m\\), \\(|B_m|\\) is the number of examples in that bin, \\(n\\) is the total number of examples, \\(\\text{acc}(B_m)\\) is the accuracy (fraction of positives) in bin \\(m\\), and \\(\\text{conf}(B_m)\\) is the mean predicted probability in bin \\(m\\). Lower ECE indicates better calibration, with zero representing perfect calibration. ECE depends on binning strategy; equal-width bins may place most examples in a few bins for models with concentrated predictions, while equal-mass bins ensure each bin contains the same number of examples but may span wide probability ranges.\nMaximum Calibration Error (MCE) captures worst-case miscalibration by reporting the largest absolute gap between predicted and observed frequencies across all bins. MCE is appropriate when any severe miscalibration is unacceptable, as in high-stakes clinical applications where even rare catastrophic errors carry significant consequences.\nBrier score decomposes into components measuring calibration and discrimination (refinement), providing a single proper scoring rule that rewards both properties. The Brier score equals the mean squared difference between predicted probabilities and binary outcomes, and its decomposition reveals whether poor scores stem from miscalibration, poor discrimination, or both.\n\n\n23.2.3 Why Foundation Models Are Often Miscalibrated\nFoundation models face calibration challenges beyond those affecting standard neural networks. Pretraining objectives like masked language modeling optimize for predicting held-out tokens, not for producing calibrated probability distributions over downstream tasks. The representations learned during pretraining may encode useful information about sequence biology while providing no guarantee that fine-tuned classifiers will be well-calibrated.\nDistribution shift between pretraining and evaluation compounds miscalibration. A protein language model pretrained on UniRef sequences encounters a fine-tuning task using ClinVar variants. The pretraining distribution emphasizes common proteins with many homologs, while clinical variants concentrate in disease-associated genes with different sequence characteristics. Models may be well-calibrated on held-out pretraining data while miscalibrated on clinically relevant evaluation sets.\nLabel noise in training data propagates to calibration errors. ClinVar annotations reflect the state of knowledge at submission time and may contain errors, particularly for older entries or variants from less-studied genes. Deep mutational scanning experiments provide functional labels but with measurement noise that varies across assays. Models trained on noisy labels may learn the noise distribution, producing predictions that match training labels but not underlying truth.\nZero-shot approaches present particular calibration challenges. ESM-1v log-likelihood ratios measure how surprising a mutation is to the language model, but these ratios are not probabilities and have no inherent calibration. Converting log-likelihood ratios to pathogenicity probabilities requires explicit calibration against external labels, and the resulting calibration depends on the reference dataset used for this conversion.\n\n\n23.2.4 Calibration Across Subgroups\nAggregate calibration metrics can mask severe miscalibration in clinically important subgroups. A model might achieve low ECE overall while being dramatically overconfident for variants in African-ancestry individuals and underconfident for European-ancestry individuals, with opposite errors canceling in aggregate statistics. Subgroup-stratified calibration assessment is essential for any model intended for diverse populations.\nAncestry-stratified calibration reveals systematic patterns in current foundation models. Training data for protein language models and variant effect predictors derive predominantly from European-ancestry cohorts, creating differential epistemic uncertainty across populations. Calibration curves stratified by ancestry often show that models are better calibrated for populations well-represented in training data and overconfident or underconfident for under-represented populations. This differential calibration has direct fairness implications: clinical decisions based on miscalibrated predictions will be systematically worse for patients from under-represented backgrounds.\nCalibration may also vary by variant class, gene constraint level, protein family, or disease category. Missense variants in highly constrained genes may show different calibration patterns than those in tolerant genes. Variants in well-studied protein families with abundant training examples may be better calibrated than variants in orphan proteins. Stratified reliability diagrams across these categories reveal whether a single calibration correction suffices or whether subgroup-specific approaches are necessary.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#sec-post-hoc-calibration",
    "href": "p5-ch23-uncertainty.html#sec-post-hoc-calibration",
    "title": "23  Uncertainty Quantification",
    "section": "23.3 Post-Hoc Calibration Methods",
    "text": "23.3 Post-Hoc Calibration Methods\n\n23.3.1 Temperature Scaling\nThe simplest calibration fix is often the most effective. Temperature scaling applies a single learned parameter to adjust model confidence, dramatically improving calibration with minimal computational overhead and no change to model predictions’ ranking.\nThe method modifies the softmax function by dividing logits by a temperature parameter \\(T\\) before applying softmax:\n\\[\\hat{p}_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\\]\nwhere \\(z_i\\) are the logits (pre-softmax outputs) and \\(\\hat{p}_i\\) are the calibrated probabilities. When \\(T &gt; 1\\), the distribution becomes softer (more uniform), reducing overconfidence. When \\(T &lt; 1\\), the distribution becomes sharper, increasing confidence. The optimal temperature is learned by minimizing negative log-likelihood on a held-out calibration set, typically yielding \\(T\\) between 1.5 and 3 for overconfident deep networks.\nTemperature scaling preserves the model’s ranking because dividing all logits by the same constant does not change their relative ordering. A variant ranked as more likely pathogenic than another remains more likely after temperature scaling; only the magnitudes of probability estimates change. This preservation of discrimination while improving calibration makes temperature scaling particularly attractive: calibration improves without sacrificing the model’s hard-won ability to distinguish pathogenic from benign variants.\nThe method’s simplicity (one parameter) is both strength and limitation. A single global temperature cannot fix heterogeneous miscalibration where the model is overconfident in some regions of input space and underconfident in others. When reliability diagrams show complex nonlinear patterns, more flexible calibration methods are necessary.\n\n\n23.3.2 Platt Scaling\nPlatt scaling fits a logistic regression model on the original model’s outputs, learning both a slope and intercept to transform scores into calibrated probabilities. For binary classification:\n\\[\\hat{p} = \\sigma(a \\cdot f(x) + b)\\]\nwhere \\(f(x)\\) is the original model’s output, \\(\\sigma\\) is the sigmoid function, and parameters \\(a\\) and \\(b\\) are learned on calibration data. The two parameters provide more flexibility than temperature scaling’s single parameter, allowing correction of both the sharpness and the location of the probability distribution.\nPlatt scaling is appropriate when miscalibration involves systematic bias (predictions consistently too high or too low) in addition to over- or underconfidence. The method assumes that a monotonic logistic transformation suffices to correct miscalibration, which may not hold for models with complex, non-monotonic calibration curves.\n\n\n23.3.3 Isotonic Regression\nIsotonic regression provides a non-parametric approach that fits a monotonically increasing function mapping raw scores to calibrated probabilities. Unlike temperature or Platt scaling, isotonic regression makes no assumptions about the functional form of miscalibration, allowing it to correct arbitrary monotonic patterns.\nThe method works by pooling adjacent bins whose empirical frequencies violate monotonicity, then assigning each bin its pooled frequency. The resulting calibration function is a step function that increases with the original score. This flexibility comes at a cost: with limited calibration data, isotonic regression may overfit to noise in the calibration set, and the step-function output can appear discontinuous. Additionally, isotonic regression provides no uncertainty estimate on the calibration itself; we learn a point estimate of the calibration function without knowing how reliable that estimate is.\n\n\n23.3.4 Calibrating Foundation Model Outputs\nGenomic foundation models present specific calibration considerations beyond standard classification settings. The choice of calibration approach depends on whether the model produces logits, log-likelihood ratios, or continuous regression outputs, and on whether calibration targets are available for the deployment distribution.\nFor zero-shot variant effect scores like ESM-1v log-likelihood ratios, raw outputs have no inherent probabilistic interpretation. Calibration requires mapping these continuous scores to pathogenicity probabilities using external labels, typically from ClinVar or population frequency data. This mapping should occur on held-out genes or variants not used for any model development, and the resulting calibration reflects the specific label set used; calibration against ClinVar pathogenic/benign labels may not transfer to other clinical contexts.\nMulti-output models that predict across many tasks (multiple cell types, tissues, or assays) may require separate calibration for each output. A regulatory model predicting expression across 200 cell types is unlikely to be uniformly calibrated across all outputs; cell types with more training data may show better calibration than rare cell types.\nTemporal stability of calibration deserves consideration. As ClinVar annotations evolve with new evidence, the ground truth against which models were calibrated changes. A model calibrated against 2020 ClinVar labels may become miscalibrated relative to 2025 labels as variant classifications are updated. Periodic recalibration against current labels helps maintain clinical relevance.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#sec-uq-methods",
    "href": "p5-ch23-uncertainty.html#sec-uq-methods",
    "title": "23  Uncertainty Quantification",
    "section": "23.4 Uncertainty Quantification Methods for Foundation Models",
    "text": "23.4 Uncertainty Quantification Methods for Foundation Models\n\n23.4.1 Deep Ensembles\nIf one model expresses uncertainty about a prediction, querying multiple models reveals whether that uncertainty reflects genuine ambiguity in the data or an artifact of a particular training run. When five independently trained models agree on a prediction, confidence is warranted; when they disagree, the disagreement itself signals uncertainty. Ensemble disagreement provides one of the most reliable uncertainty estimates available in deep learning, at the cost of training and maintaining multiple models.\nDeep ensembles train \\(M\\) models (typically 5 to 10) with different random initializations, data orderings, or minor architectural variations. At inference time, all members produce predictions, and uncertainty is estimated from the variance or entropy of the ensemble distribution. For classification, epistemic uncertainty appears as disagreement in predicted class probabilities across members. For regression, epistemic uncertainty appears as variance in predicted values.\nThe theoretical basis for ensemble uncertainty estimation rests on the observation that disagreement between models reflects regions of input space where the training data provides insufficient constraint. Where training examples are dense, gradient descent from different initializations converges to similar solutions, producing agreement. Where training examples are sparse or conflicting, different initializations find different local optima, producing disagreement. This interpretation connects ensembles to Bayesian model averaging, where predictions are averaged over the posterior distribution of model parameters.\nFor foundation models with billions of parameters, training full ensembles becomes prohibitively expensive. Training five copies of ESM-2 requires approximately five times the compute of a single model, potentially millions of dollars in cloud computing costs. Several practical alternatives reduce this burden. Last-layer ensembles freeze the pretrained backbone and train only an ensemble of prediction heads, reducing cost by orders of magnitude while still capturing uncertainty from the fine-tuning process. Snapshot ensembles save model checkpoints at various points during optimization and use these snapshots as ensemble members, requiring only single-model training time. Multi-seed fine-tuning trains the same architecture from multiple random seeds on the fine-tuning task, which is far cheaper than multi-seed pretraining.\n\n\n23.4.2 Monte Carlo Dropout\nMonte Carlo (MC) dropout provides uncertainty estimates from a single trained model by treating dropout regularization as approximate Bayesian inference. During standard training with dropout, random subsets of neurons are zeroed at each forward pass. MC dropout keeps dropout active at test time and performs multiple stochastic forward passes, treating the variation across passes as a measure of model uncertainty.\nGal and Ghahramani showed that this procedure approximates variational inference over the model’s weights (gal_dropout_2016?). Each forward pass with dropout samples a different subnetwork, and the distribution of predictions across samples approximates the predictive distribution under a particular prior over weights. High variance across MC samples indicates epistemic uncertainty about the model’s parameters for that input.\nMC dropout offers the significant advantage of requiring only a single trained model, avoiding the computational overhead of ensembles. Implementation is straightforward: enable dropout during inference and average predictions over 10 to 50 stochastic forward passes. The variance or entropy of these predictions serves as the uncertainty estimate.\nLimitations temper the method’s appeal. Modern transformer architectures often do not use dropout in their standard configurations, or use dropout only in specific locations (attention dropout, residual dropout) where the approximation may be less accurate. The quality of uncertainty estimates depends on the dropout rate and architecture, with higher dropout rates providing better uncertainty estimates but potentially degrading mean predictions. Empirical comparisons often find that MC dropout underestimates uncertainty relative to deep ensembles, particularly in low-data regimes where epistemic uncertainty should be high.\n\n\n23.4.3 Heteroscedastic Models\nStandard regression models predict a single output value, implicitly assuming constant noise variance across all inputs. Heteroscedastic models instead predict both a mean and a variance for each input, capturing the intuition that prediction uncertainty varies depending on the input. For genomic applications, this approach naturally handles the observation that some prediction tasks are inherently noisier than others: coding variant effects may be more predictable than regulatory variant effects, constrained genes more predictable than tolerant genes.\nArchitecture modifications are minimal. Instead of outputting a single value, the model outputs two values interpreted as the mean \\(\\mu(x)\\) and variance \\(\\sigma^2(x)\\) of a Gaussian distribution over outputs. Training uses negative log-likelihood loss under this Gaussian, which penalizes both prediction errors and miscalibrated variance estimates:\n\\[\\mathcal{L} = \\frac{1}{2\\sigma^2(x)}(y - \\mu(x))^2 + \\frac{1}{2}\\log \\sigma^2(x)\\]\nThe first term penalizes prediction errors, weighted by inverse variance so that high-variance predictions are penalized less for the same absolute error. The second term prevents the model from simply predicting infinite variance to avoid all penalties. The result is a model that learns to predict larger variance for inputs where training labels are noisy or inconsistent, capturing aleatoric uncertainty in an input-dependent manner.\n\n\n23.4.4 Evidential Deep Learning\nEvidential deep learning places a prior distribution over the class probabilities themselves rather than directly predicting probabilities. For classification, the model outputs parameters of a Dirichlet distribution, which serves as a prior over the simplex of class probabilities. The concentration parameters of this Dirichlet encode both the predicted class probabilities (via their relative magnitudes) and the model’s uncertainty (via their absolute magnitudes).\nLow total concentration indicates high uncertainty: the model is unsure which class is correct. High total concentration with one dominant class indicates confident prediction. This framework provides a principled way to separate epistemic uncertainty (low concentration) from confident predictions (high concentration), all from a single forward pass without ensembling or MC sampling.\nCritics have noted that evidential deep learning can produce unreliable uncertainty estimates when the distributional assumptions are violated or when training data is limited. Practical experience suggests that ensembles and MC dropout often provide more robust uncertainty estimates, though evidential methods continue to be refined.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#sec-conformal",
    "href": "p5-ch23-uncertainty.html#sec-conformal",
    "title": "23  Uncertainty Quantification",
    "section": "23.5 Conformal Prediction: Distribution-Free Guarantees",
    "text": "23.5 Conformal Prediction: Distribution-Free Guarantees\n\n23.5.1 The Conformal Prediction Framework\nMost uncertainty quantification methods make assumptions about model behavior or data distributions that may not hold in practice. Temperature scaling assumes miscalibration follows a particular functional form. Ensembles assume that disagreement reflects epistemic uncertainty rather than artifacts of training. Bayesian methods assume specific priors over model parameters. When these assumptions fail, uncertainty estimates may be unreliable precisely when reliability matters most.\nConformal prediction offers something stronger: finite-sample coverage guarantees that hold under minimal assumptions. Instead of outputting a point prediction, conformal methods produce a prediction set guaranteed to contain the true label with probability at least \\(1 - \\alpha\\), where \\(\\alpha\\) is a user-specified error rate. If we request 90% coverage (\\(\\alpha = 0.10\\)), the prediction set will contain the true label at least 90% of the time, regardless of the model’s accuracy or calibration. This guarantee requires only that calibration and test examples are exchangeable (a condition weaker than independent and identically distributed), making conformal prediction robust to model misspecification.\nThe coverage guarantee is finite-sample: it holds exactly for any sample size, not just asymptotically. For clinical genomics applications where individual predictions carry significant consequences, this finite-sample property provides assurance that cannot be obtained from asymptotic calibration arguments.\n\n\n23.5.2 Split Conformal Prediction\nThe most practical conformal method, split conformal prediction, proceeds in five steps. First, split available labeled data into a proper training set and a calibration set. Second, train the model on the training set only. Third, compute non-conformity scores on the calibration set, where higher scores indicate poorer fit between the model’s prediction and the true label. Fourth, find the threshold \\(q\\) at the \\((1-\\alpha)(1+1/n)\\) quantile of calibration scores, where \\(n\\) is the calibration set size. Fifth, at test time, include in the prediction set all labels whose non-conformity score falls below \\(q\\).\nNon-conformity scores measure how “strange” a candidate label is given the model’s output. For classification, a common choice is \\(1 - \\hat{p}_y\\), where \\(\\hat{p}_y\\) is the predicted probability of the true class. High predicted probability means low non-conformity (the label conforms to the model’s expectations); low predicted probability means high non-conformity. For regression, absolute residuals \\(|y - \\hat{y}|\\) serve as non-conformity scores.\nThe construction ensures coverage because calibration scores are exchangeable with test scores under the exchangeability assumption. The quantile threshold is set so that a random calibration score exceeds the threshold with probability at most \\(\\alpha\\); by exchangeability, the same holds for test scores. This elegant argument yields exact coverage guarantees without requiring the model to be accurate or well-calibrated.\n\n\n23.5.3 Conformal Prediction for Variant Effect Prediction\nVariant effect prediction provides a natural application for conformal methods. Instead of reporting a single pathogenicity score, a conformalized variant classifier outputs a prediction set from the possibilities: {pathogenic}, {benign}, {pathogenic, benign}, or the empty set. The set is guaranteed to contain the true label at the specified coverage rate.\nAdaptive set sizes convey uncertainty naturally. Confident predictions yield small sets ({pathogenic} alone), while uncertain predictions yield larger sets ({pathogenic, benign}). The set size itself communicates the model’s confidence without requiring users to interpret numerical probabilities. A clinician seeing {pathogenic, benign} knows immediately that the model cannot distinguish between these possibilities, whereas a score of 0.55 might be misinterpreted as mild confidence in pathogenicity.\nCalibration set construction requires careful thought. Holding out variants at random may not prevent information leakage if related variants (same gene, same protein domain) appear in both calibration and test sets. Holding out entire genes or protein families provides more stringent evaluation but may reduce calibration set size for rare gene families. For applications intended to work across populations, calibration sets should include diverse ancestries to ensure coverage guarantees hold across patient populations.\nConformal prediction intervals for regression tasks (expression prediction, quantitative trait prediction) provide bounds rather than sets. Conformalized quantile regression produces intervals guaranteed to contain the true value with specified probability, directly applicable to predicting gene expression changes or polygenic score uncertainty.\n\n\n23.5.4 Limitations and Practical Considerations\nConformal guarantees are marginal rather than conditional. The coverage guarantee holds on average across all test examples, not for each individual example. A model might achieve exact 90% coverage overall while dramatically undercovering some subgroups and overcovering others. Subgroup-conditional coverage requires additional assumptions or methods like stratified conformal prediction.\nThe exchangeability assumption can fail in practice. If the calibration set derives from one population and the test set from another, coverage guarantees may not hold. Temporal shifts (calibration on historical data, testing on future data) similarly violate exchangeability. Methods for conformal prediction under distribution shift exist but require additional assumptions about the nature of the shift.\nPrediction set size trades off against informativeness. Larger sets provide more reliable coverage but less useful predictions. A model that produces {pathogenic, benign} for every variant achieves perfect coverage but provides no discrimination. Careful model development to improve underlying accuracy reduces average set size while maintaining coverage guarantees.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#sec-ood-detection",
    "href": "p5-ch23-uncertainty.html#sec-ood-detection",
    "title": "23  Uncertainty Quantification",
    "section": "23.6 Out-of-Distribution Detection",
    "text": "23.6 Out-of-Distribution Detection\n\n23.6.1 The Out-of-Distribution Problem\nA DNA language model trained on mammalian genomes encounters a novel archaeal sequence. The model’s embedding places this sequence in an unfamiliar region of representation space, far from the clusters formed by training examples. Yet the model still produces a prediction, potentially with high confidence, because standard neural networks are not designed to recognize when inputs lie outside their training distribution. Detecting out-of-distribution (OOD) inputs is essential for safe deployment of foundation models in settings where novel sequences are inevitable.\nOOD detection identifies inputs that differ meaningfully from training data, allowing systems to flag uncertain predictions before they cause harm. Novel pathogens may share little sequence similarity with characterized viruses in training data. Synthetic proteins designed for therapeutic purposes may occupy regions of sequence space unsampled by evolution. Variants in poorly characterized genes may lack the contextual information that models rely on for accurate prediction. In each case, recognizing that the input is unusual enables appropriate caution.\nThe confidence problem compounds OOD challenges. Neural networks often produce high-confidence predictions on OOD inputs because nothing in standard training penalizes confidence on unfamiliar examples. A classifier trained to distinguish pathogenic from benign variants may confidently predict “pathogenic” for a completely random sequence, not because it has evidence for pathogenicity but because it lacks the capacity to say “I don’t know.” This failure mode makes OOD detection essential rather than optional.\n\n\n23.6.2 Likelihood-Based Detection and Its Failures\nThe intuitive approach to OOD detection uses model likelihood: inputs the model finds improbable should be flagged as OOD. Language models assign likelihoods to sequences; surely OOD sequences should receive low likelihood?\nThis intuition fails for deep generative models. Complex models can assign high likelihood to OOD data for reasons unrelated to semantic similarity to training examples. In high-dimensional spaces, typical sets (regions where most probability mass concentrates) do not coincide with high-density regions. A sequence might land in a high-density region of the model’s distribution while being semantically distant from any training example.\nEmpirically, language models assign high likelihood to repetitive sequences, sequences with unusual but consistent patterns, and sequences from different domains that happen to share statistical properties with training data. For genomic models, this means likelihood alone cannot reliably distinguish novel biological sequences from sequences within the training distribution.\n\n\n23.6.3 Embedding-Based Detection\nLearned representations provide more reliable OOD detection than raw likelihood. The key insight is that embeddings encode semantic structure: similar sequences cluster together in embedding space, and OOD sequences land in sparse regions distant from training clusters.\nMahalanobis distance measures how far a test embedding lies from training data, accounting for the covariance structure of the embedding space. For each class, compute the mean embedding and covariance matrix from training examples. For a test input, compute its distance to each class centroid in units of standard deviations, accounting for correlations between embedding dimensions. Large Mahalanobis distance indicates OOD inputs.\nNearest-neighbor methods provide a non-parametric alternative. For a test embedding, find the \\(k\\) nearest neighbors among training embeddings and compute the average distance. Large average distance to neighbors indicates the test input lies in a sparse region of embedding space, suggesting it is OOD. This approach makes no distributional assumptions and scales well with modern approximate nearest-neighbor algorithms.\nFor genomic foundation models, embedding-based OOD detection enables practical deployment safeguards. ESM embeddings place novel protein folds in regions distant from characterized folds, allowing detection of sequences outside the model’s training experience. DNABERT embeddings reveal unusual sequence composition or repeat structures that may confound predictions. Flagging these cases for expert review prevents confident but unreliable predictions from reaching clinical decisions.\n\n\n23.6.4 Practical OOD Detection for Genomic Applications\nDefining what counts as OOD requires domain knowledge. Novel species or clades may share evolutionary history with training examples yet differ enough to warrant caution. Extreme GC content can indicate contamination, unusual biology, or simply under-represented genomic regions. Engineered sequences (designed proteins, synthetic regulatory elements) intentionally explore regions of sequence space not represented in natural sequences.\nCombining multiple OOD signals improves reliability. Embedding distance, likelihood, and prediction confidence each capture different aspects of distributional difference. An input flagged by multiple methods is more reliably OOD than one flagged by a single method. Threshold selection involves trade-offs between false positives (flagging in-distribution examples unnecessarily) and false negatives (missing true OOD examples).\nThe operational response to OOD detection depends on the application. For variant interpretation, OOD inputs might trigger automatic flagging for expert review rather than automated classification. For high-throughput screening, OOD inputs might receive tentative predictions with explicit uncertainty warnings. For safety-critical applications, OOD inputs might trigger rejection with a request for additional information.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#sec-selective-prediction",
    "href": "p5-ch23-uncertainty.html#sec-selective-prediction",
    "title": "23  Uncertainty Quantification",
    "section": "23.7 Selective Prediction and Abstention",
    "text": "23.7 Selective Prediction and Abstention\n\n23.7.1 When to Abstain\nA variant effect predictor achieving 95% accuracy overall provides more clinical value if it can identify which predictions are reliable. Selective prediction allows models to abstain on difficult cases, concentrating predictions on inputs where confidence is warranted. The trade-off between coverage (fraction of inputs receiving predictions) and accuracy (correctness among predictions made) defines the selective prediction problem.\nThe coverage-accuracy trade-off reflects a fundamental tension. At 100% coverage, the model predicts on all inputs and achieves its baseline accuracy. As coverage decreases (more abstention), accuracy among predictions made typically increases because the model abstains on its most uncertain cases. The shape of this trade-off curve characterizes the model’s ability to identify reliable predictions.\nAbstention is appropriate when the cost of errors exceeds the cost of deferral. In clinical variant interpretation, a confident but incorrect pathogenic prediction may trigger unnecessary medical intervention, while abstention merely defers the decision to expert review. If expert review is available and affordable relative to error costs, abstaining on uncertain cases improves overall decision quality. Conversely, in high-throughput screening where expert review is infeasible, abstention may provide little benefit because all predictions eventually require automated handling.\n\n\n23.7.2 Selective Prediction Methods\nConfidence-based selection abstains when the model’s maximum predicted probability falls below a threshold. For a classifier producing probabilities over classes, if \\(\\max_c \\hat{p}_c &lt; \\tau\\), the model abstains. This simple approach works well when model confidence correlates with correctness, but fails when models are confidently wrong.\nEnsemble-based selection abstains when ensemble members disagree beyond a threshold. High disagreement indicates epistemic uncertainty about the correct prediction, warranting abstention even if individual members express confidence. This approach captures uncertainty that confidence-based selection misses when models are overconfident.\nConformal selection abstains when prediction sets exceed a size threshold. If the conformal prediction set contains more than one class, the model lacks confidence to make a unique prediction. This approach connects selective prediction to the coverage guarantees of conformal methods: the model makes predictions with guaranteed coverage on the non-abstained cases.\nLearned selection trains a separate model to predict whether the primary model will be correct on each input. This “rejection model” learns to identify failure modes that simple confidence thresholds miss, potentially achieving better coverage-accuracy trade-offs than heuristic methods.\n\n\n23.7.3 Evaluating Selective Prediction\nRisk-coverage curves plot accuracy (or its complement, risk) as a function of coverage, revealing how performance improves as the model becomes more selective. The area under the risk-coverage curve summarizes overall selective prediction quality. Models with better uncertainty estimates produce steeper curves, achieving high accuracy at lower coverage.\nSelective accuracy at fixed coverage specifies a coverage level (e.g., 80%) and reports accuracy among predictions made at that coverage. This metric directly answers practical questions: “If we let the model predict on its 80% most confident cases, how accurate will it be?”\nComparison across methods requires matched coverage levels. A method that achieves 99% accuracy at 50% coverage and 95% accuracy at 90% coverage may be preferable to a method achieving 97% accuracy at both levels, depending on operational requirements. Reporting full risk-coverage curves enables stakeholders to select operating points appropriate to their cost structures.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#sec-genomic-uq",
    "href": "p5-ch23-uncertainty.html#sec-genomic-uq",
    "title": "23  Uncertainty Quantification",
    "section": "23.8 Uncertainty for Specific Genomic Tasks",
    "text": "23.8 Uncertainty for Specific Genomic Tasks\n\n23.8.1 Variant Effect Prediction Uncertainty\nVariant effect prediction concentrates the challenges of uncertainty quantification. Epistemic uncertainty arises from poorly characterized genes, novel protein folds, and under-represented populations in training data. Aleatoric uncertainty stems from incomplete penetrance, variable expressivity, and noise in functional assay labels. Both types of uncertainty must be estimated and communicated for variant predictions to inform clinical decisions appropriately.\nCalibration challenges for VEP include the evolving nature of ground truth labels. ClinVar annotations change as new evidence emerges; variants classified as VUS may be reclassified as pathogenic or benign, and even confident classifications occasionally reverse. A model calibrated against a historical version of ClinVar may appear miscalibrated against current annotations, not because the model changed but because the labels did. Periodic recalibration against current databases maintains alignment between model outputs and contemporary clinical understanding.\nPopulation-specific calibration addresses the reality that training data predominantly derive from European-ancestry cohorts. For patients from other ancestral backgrounds, both epistemic uncertainty (fewer training examples) and calibration (different baseline pathogenicity rates, different patterns of variation) may differ from the aggregate. Stratified reliability diagrams by ancestry reveal these differences; ancestry-conditional calibration may be necessary for equitable performance across populations.\n\n\n23.8.2 Regulatory Variant Uncertainty\nRegulatory variants present distinct uncertainty challenges. Unlike coding variants where effects can be localized to specific amino acid changes, regulatory variants act through complex mechanisms involving transcription factor binding, chromatin accessibility, and three-dimensional genome organization. This mechanistic complexity translates to higher aleatoric uncertainty: even perfectly characterized regulatory variants may have context-dependent effects that vary across cell types, developmental stages, and genetic backgrounds.\nExpression prediction models like Enformer and Borzoi provide uncertainty estimates for predicted expression changes. Ensemble approaches quantify disagreement across model variants. Heteroscedastic architectures predict tissue-specific confidence alongside tissue-specific expression. These uncertainties propagate to downstream interpretations: a variant predicted to alter expression with high uncertainty warrants different treatment than one with narrow confidence bounds.\n\n\n23.8.3 Uncertainty Across Populations\nDifferential uncertainty across populations has direct implications for health equity. Models trained predominantly on European-ancestry data exhibit higher epistemic uncertainty for other populations, leading to larger prediction sets from conformal methods, higher abstention rates from selective prediction, and less reliable confidence estimates from calibration.\nTransparent reporting of population-stratified uncertainty metrics enables informed decisions about model deployment. If a model abstains on 30% of variants in one population but only 10% in another, users can make informed choices about supplementary analyses for the higher-abstention population. Ignoring these differences risks providing lower-quality predictions to already under-served populations while presenting a false appearance of uniform reliability.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#sec-uncertainty-communication",
    "href": "p5-ch23-uncertainty.html#sec-uncertainty-communication",
    "title": "23  Uncertainty Quantification",
    "section": "23.9 Communicating Uncertainty to End Users",
    "text": "23.9 Communicating Uncertainty to End Users\n\n23.9.1 The Communication Challenge\nA pathogenicity score of \\(0.73 \\pm 0.15\\) may be statistically accurate but nearly useless to a clinician deciding whether to order confirmatory testing. The gap between statistical uncertainty and decision-relevant communication presents a persistent challenge for genomic AI deployment. Different users reason differently about probability and risk; effective communication requires understanding these differences.\nCognitive biases complicate probability interpretation. Humans tend toward overconfidence in point estimates, treating 0.73 as more certain than warranted. Prediction intervals are frequently misunderstood: a 90% confidence interval does not mean the true value has a 90% chance of being in that specific interval (a Bayesian interpretation) but rather that 90% of such intervals would contain the true value (a frequentist interpretation). Base rate neglect leads users to interpret variant-level pathogenicity predictions without accounting for prior probability based on clinical presentation, family history, and phenotypic specificity.\nDifferent stakeholders have different needs. Clinicians require actionable categories that map to clinical decision points, not continuous scores requiring interpretation. Researchers may prefer full probability distributions enabling flexible downstream analysis. Patients and families need understandable risk communication that supports informed decision-making without inducing inappropriate anxiety or false reassurance.\n\n\n23.9.2 Categorical Reporting\nClinical genetics has established categorical frameworks for variant interpretation. The ACMG-AMP guidelines define five categories: pathogenic, likely pathogenic, variant of uncertain significance, likely benign, and benign. Mapping continuous model outputs to these categories requires threshold selection that balances sensitivity and specificity at clinically meaningful operating points.\nUncertainty within categories can be conveyed through confidence qualifiers or numerical confidence scores attached to categorical calls. A “likely pathogenic” call with 95% confidence differs meaningfully from one with 60% confidence, even though both receive the same categorical label. Two-dimensional reporting combining category and confidence enables more nuanced interpretation without abandoning the categorical framework that clinicians expect.\nThreshold selection involves value judgments beyond pure statistics. The consequences of false positive and false negative pathogenic calls differ by clinical context. For a severe, treatable condition, false negatives carry higher cost, warranting lower thresholds for pathogenic classification. For untreatable conditions where pathogenic classification affects reproductive decisions, the calculus differs. Uncertainty quantification enables informed threshold selection by revealing the trade-offs at different operating points.\n\n\n23.9.3 Visual Communication\nProbability bars and confidence intervals provide visual representation of uncertainty, though their interpretation depends on user familiarity with statistical graphics. Icon arrays, which represent probabilities as proportions of colored icons in a grid (e.g., 73 red icons and 27 blue icons out of 100), improve comprehension for users without statistical training. The visual representation of proportion is more intuitive than numerical probability for many audiences.\nRisk ladders place the prediction in context by showing where it falls relative to other risks of varying magnitude. A variant with 0.73 probability of pathogenicity can be placed alongside risks from other genetic conditions, environmental exposures, or common medical procedures, enabling intuitive comparison.\nInteractive visualizations allow users to explore uncertainty in detail, examining how predictions change under different assumptions or how uncertainty varies across related variants. These approaches suit sophisticated users engaged in research or detailed clinical analysis but may overwhelm users seeking simple answers.\n\n\n23.9.4 Decision-Theoretic Framing\nRather than communicating probability alone, decision-theoretic framing presents expected outcomes under different actions. Instead of “this variant has 73% probability of being pathogenic,” the report might state “if we assume this variant is pathogenic and proceed with surveillance, the expected outcomes are X; if we assume it is benign and decline surveillance, the expected outcomes are Y.”\nThis framing integrates uncertainty with action, helping users understand how uncertainty affects what they should do rather than treating probability as an end in itself. The approach requires modeling clinical outcomes, which introduces additional assumptions, but makes explicit the decision-relevant implications of uncertainty rather than leaving users to integrate probability with consequences on their own.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch23-uncertainty.html#toward-trustworthy-genomic-ai",
    "href": "p5-ch23-uncertainty.html#toward-trustworthy-genomic-ai",
    "title": "23  Uncertainty Quantification",
    "section": "23.10 Toward Trustworthy Genomic AI",
    "text": "23.10 Toward Trustworthy Genomic AI\nUncertainty quantification transforms foundation model outputs from opaque scores into components of a rational decision process. A well-calibrated pathogenicity prediction that honestly communicates its limitations enables appropriate clinical reasoning; an overconfident score that claims false precision can cause harm through both false positives and false negatives. The methods developed in this chapter, from temperature scaling to conformal prediction to OOD detection, provide the technical foundation for trustworthy genomic AI.\nThe path from uncertainty quantification to clinical impact requires integrating these methods into operational workflows. Selective prediction enables triage between automated handling and expert review. Conformal prediction sets provide coverage guarantees that support informed decision-making. OOD detection prevents confident predictions on unfamiliar inputs. Calibration ensures that numerical probabilities mean what they claim to mean. Together, these tools enable foundation models to participate in clinical decisions without overstating their reliability.\nYet uncertainty quantification alone is insufficient. A perfectly calibrated black box remains a black box. The clinician who receives an uncertain prediction wants to understand why the model is uncertain: is it because the variant falls in a poorly characterized gene, because the model has never encountered this protein fold, or because the underlying biology is genuinely ambiguous? Interpretability, the subject of the next chapter, complements uncertainty by revealing the mechanistic basis for predictions and their associated confidence. The conjunction of calibrated uncertainty and mechanistic understanding approaches what trustworthy clinical AI requires.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html",
    "href": "p5-ch24-interpretability.html",
    "title": "24  Interpretability and Mechanism",
    "section": "",
    "text": "24.1 Attribution Methods and Input Importance\nGenomic foundation models achieve remarkable predictive accuracy while offering essentially no explanation for their predictions. A model can predict that a particular variant will disrupt gene regulation with 95% confidence, yet a clinician examining that prediction cannot determine whether the model detected a disrupted transcription factor binding site, learned a spurious correlation with GC content, or exploited some pattern in the training data that has nothing to do with biology. This opacity creates a fundamental tension: the models most capable of capturing complex regulatory logic are precisely those whose internal representations resist human understanding.\nThe stakes of this tension extend beyond scientific curiosity. Variant interpretation guidelines from the American College of Medical Genetics require that computational evidence be weighed alongside functional assays, segregation data, and population frequency. A pathogenicity score alone satisfies only weak evidence criteria; knowing that a variant disrupts a specific CTCF binding site in a cardiac enhancer provides interpretable mechanistic evidence that can be combined with clinical presentation and family history. When models cannot explain their predictions, clinicians cannot integrate computational evidence with biological reasoning. The same limitation affects research applications: a model that predicts enhancer activity cannot generate testable hypotheses about regulatory grammar unless its internal computations can be translated into statements about motifs, spacing constraints, and combinatorial logic.\nThis chapter examines the toolkit for extracting meaning from genomic foundation models, from classical attribution methods that identify important input positions through motif discovery algorithms that translate attributions into regulatory vocabularies, to emerging techniques from mechanistic interpretability that probe the internal circuits of transformer architectures. Throughout, a critical distinction guides the analysis: plausible explanations that match biological intuition are not the same as faithful explanations that accurately reflect model computation. Understanding when these diverge, and how to validate interpretability claims experimentally, determines whether model explanations accelerate discovery or provide false comfort.\nWhen a model predicts that a 200-kilobase genomic region will show high chromatin accessibility in hepatocytes, a natural question arises: which bases within that region drive the prediction? Attribution methods answer this question by assigning importance scores to input positions, identifying where the model focuses its computational attention. These scores can reveal candidate regulatory elements, highlight the sequence features underlying variant effects, and provide the raw material for downstream motif discovery.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#attribution-methods-and-input-importance",
    "href": "p5-ch24-interpretability.html#attribution-methods-and-input-importance",
    "title": "24  Interpretability and Mechanism",
    "section": "",
    "text": "24.1.1 In Silico Mutagenesis\nThe most direct approach to measuring input importance is simply to change each base and observe what happens to the prediction. In silico mutagenesis (ISM) systematically introduces mutations at every position, computing the difference between mutant and reference predictions. For a sequence of length \\(L\\), ISM creates three mutant sequences at each position (substituting each non-reference nucleotide), yielding \\(3L\\) forward passes through the model. The resulting mutation effect matrix captures how sensitive the prediction is to changes at each position and to each alternative base.\nISM provides true counterfactual information rather than approximations. When ISM shows that mutating position 47 from A to G reduces the predicted accessibility by 0.3 log-fold, that is a direct observation about model behavior, not an estimate derived from gradients or attention weights. This directness makes ISM the gold standard for faithfulness: if ISM identifies a position as important, perturbing that position genuinely changes the output.\nThe limitation is computational cost. Scoring all single-nucleotide substitutions in a 200-kilobase input requires 600,000 forward passes, which becomes prohibitive for large models or genome-wide analysis. Practical applications often restrict ISM to targeted windows around variants of interest, using faster methods to identify candidate regions for detailed analysis. For variant effect prediction specifically, ISM reduces to comparing reference and alternative allele predictions, requiring only two forward passes per variant.\n\n\n24.1.2 Gradient-Based Attribution\nGradient-based methods approximate the counterfactual information from ISM using backpropagation. The gradient of the output with respect to each input position measures how much an infinitesimal change at that position would affect the prediction. With one-hot encoded sequence, the gradient at each base indicates the sensitivity to substituting that nucleotide.\nThe simplest approach, often called saliency mapping, computes raw gradients and visualizes their magnitudes across the sequence. A common variant multiplies gradients by inputs (gradient × input), focusing on positions where the current nucleotide is both important and present. These methods require only a single backward pass, making them orders of magnitude faster than ISM.\nGradient-based methods suffer from saturation in regions where the model is already confident. If a strong motif drives the prediction into a saturated region of the output nonlinearity, small perturbations produce near-zero gradients even though the motif is functionally critical. DeepLIFT addresses this limitation by comparing activations between an input sequence and a reference, propagating differences through the network using custom rules that avoid gradient saturation. The resulting attributions satisfy a completeness property: contributions sum to the difference between input and reference predictions.\nIntegrated gradients provide theoretical grounding through the path integral of gradients along a linear interpolation from reference to input:\n\\[\\text{IG}_i(x) = (x_i - x'_i) \\int_{\\alpha=0}^{1} \\frac{\\partial f(x' + \\alpha(x - x'))}{\\partial x_i} \\, d\\alpha\\]\nThis integral, approximated by summing gradients at discrete interpolation steps, satisfies sensitivity (any input that affects the output receives nonzero attribution) and implementation invariance (functionally equivalent networks produce identical attributions). Integrated gradients have become a standard choice for genomic models, balancing computational efficiency with theoretical guarantees.\nAll gradient-based methods require choosing a reference sequence, which substantially affects the resulting attributions. Common choices include dinucleotide-shuffled versions of the input (preserving local composition while disrupting motifs), average non-functional sequence, or simply zeros. The reference defines what counts as informative: attributions highlight features that differ from the reference and contribute to the prediction difference. A shuffled reference emphasizes motif content; a zero reference treats any sequence information as potentially important.\nFigure recommendation: A multi-panel comparison showing the same genomic region analyzed by ISM, gradient × input, DeepLIFT, and integrated gradients. Highlight a known transcription factor binding site and show how different methods assign importance to the motif core versus flanking regions. Include a small table comparing computational cost (forward passes required) and typical failure modes for each method.\n\n\n24.1.3 Reconciling Attribution Methods\nDifferent attribution methods can produce strikingly different importance maps for the same sequence and prediction. A position might show high importance under ISM but near-zero gradients due to saturation, or high gradient magnitude but minimal effect when actually mutated due to redundancy with nearby positions. This disagreement reflects genuine differences in what each method measures: gradients capture local sensitivity, ISM captures counterfactual effects, and DeepLIFT captures contribution relative to a reference.\nPractical workflows often combine multiple methods. Gradient-based approaches efficiently scan long sequences to identify candidate regions, ISM validates importance in targeted windows, and agreement across methods increases confidence that identified features genuinely drive predictions. Disagreement flags positions for closer investigation, potentially revealing saturation effects, redundancy, or artifacts in individual methods.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#interpreting-convolutional-filters",
    "href": "p5-ch24-interpretability.html#interpreting-convolutional-filters",
    "title": "24  Interpretability and Mechanism",
    "section": "24.2 Interpreting Convolutional Filters",
    "text": "24.2 Interpreting Convolutional Filters\nConvolutional neural networks remain central to genomic sequence modeling, as discussed in Chapter 6, and their first-layer filters offer a particularly tractable interpretability target. Each filter slides along the sequence computing dot products with local windows, and high activation indicates that the local sequence matches the filter’s learned pattern. This architecture creates a natural correspondence between filters and sequence motifs.\n\n24.2.1 From Filters to Position Weight Matrices\nConverting learned filters to interpretable motifs follows a standard workflow. The trained model processes a large sequence set, typically training data or genome-wide tiles, recording positions where each filter’s activation exceeds a threshold. The fixed-length windows around high-activation positions are extracted and aligned, and nucleotide frequencies at each position are computed to build a position weight matrix (PWM). This PWM can be visualized as a sequence logo and compared to databases like JASPAR or HOCOMOCO.\nWhen this procedure is applied to models trained on chromatin accessibility or transcription factor binding, first-layer filters frequently match known transcription factor motifs. DeepSEA filters include recognizable matches to CTCF, AP-1, and cell-type-specific factors. This correspondence validates that models discover biologically meaningful patterns rather than arbitrary correlations, and it provides a direct link between model weights and decades of experimental characterization of transcription factor binding preferences.\nSeveral complications affect filter interpretation. DNA is double-stranded, and models may learn forward and reverse-complement versions of the same motif as separate filters. Some filters capture general sequence composition (GC-rich regions, homopolymer runs) rather than specific binding sites. These patterns can be biologically meaningful in contexts like nucleosome positioning or purely artifactual depending on the training task. Distinguishing informative filters from compositional shortcuts requires cross-referencing with known biology and testing whether filter-derived motifs predict binding in held-out data.\n\n\n24.2.2 Deeper Layers and Combinatorial Patterns\nBeyond the first layer, convolutional filters combine lower-level patterns into complex representations. Deeper layers can encode motif pairs that co-occur at characteristic spacing, orientation preferences between binding sites, and contextual dependencies where a motif’s importance varies with surrounding sequence. These combinatorial patterns capture aspects of regulatory grammar that individual motifs cannot represent.\nDirect interpretation of deeper filters becomes increasingly difficult as receptive fields expand and nonlinearities accumulate. The activation of a layer-5 filter depends on intricate combinations of earlier patterns, resisting simple biological annotation. Indirect approaches prove more tractable: analyzing which input regions drive high activation at deeper layers, clustering high-activation sequences to find common themes, or probing whether deeper representations encode specific biological properties.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#motif-discovery-from-attributions",
    "href": "p5-ch24-interpretability.html#motif-discovery-from-attributions",
    "title": "24  Interpretability and Mechanism",
    "section": "24.3 Motif Discovery from Attributions",
    "text": "24.3 Motif Discovery from Attributions\nAttribution maps highlight important positions but do not directly reveal motifs. A DeepLIFT track might show scattered high-importance bases throughout a sequence without indicating that those bases collectively form instances of the same transcription factor binding site. TF-MoDISco (Transcription Factor Motif Discovery from Importance Scores) bridges this gap by discovering motifs from attribution scores rather than raw sequences.\nThe insight underlying TF-MoDISco is that importance-weighted sequences focus motif discovery on positions the model actually uses. Traditional motif finders must contend with the fact that most positions in regulatory sequences do not participate in functional motifs. By extracting seqlets (short windows where total importance exceeds a threshold) and clustering them based on both sequence content and importance profiles, TF-MoDISco identifies patterns that drive model predictions.\nThe workflow proceeds through several stages. Base-level importance scores are computed for many sequences using DeepLIFT, ISM, or integrated gradients. Windows where total importance exceeds a threshold are extracted as seqlets, each representing a candidate motif instance. These seqlets are compared using metrics that consider both sequence content and importance profiles, then clustered into groups corresponding to putative motifs. Within each cluster, seqlets are aligned and consolidated into PWMs and importance-weighted logos. The resulting motifs can be matched to known transcription factors or flagged as novel patterns.\nBeyond individual motifs, TF-MoDISco enables grammar inference by analyzing motif co-occurrence. Mapping discovered motif instances back to genomic coordinates reveals characteristic spacing between motif pairs, orientation preferences, and cell-type-specific usage patterns. These grammatical rules can be validated through in silico experiments: inserting or removing motifs in synthetic sequences and checking whether predictions change as expected.\nApplications to models like BPNet trained on ChIP-seq data have recovered known transcription factor motifs, discovered novel sequence variants, and revealed spacing constraints validated through synthetic reporter assays. The same workflow applies to foundation model analysis: use the model to produce base-level attributions for a downstream task, run TF-MoDISco to extract a task-specific motif vocabulary, and analyze how motif usage varies across conditions.\nFigure recommendation: A pipeline diagram showing the complete TF-MoDISco workflow, from input sequences through attribution computation, seqlet extraction, clustering, and motif logo generation. Include an example showing discovered motifs aligned to JASPAR matches and a small grammar diagram illustrating inferred spacing constraints between co-occurring motifs.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#probing-learned-representations",
    "href": "p5-ch24-interpretability.html#probing-learned-representations",
    "title": "24  Interpretability and Mechanism",
    "section": "24.4 Probing Learned Representations",
    "text": "24.4 Probing Learned Representations\nAttribution methods ask which input positions matter; probing asks what information the model’s internal representations encode. A probing classifier is a simple supervised model (typically linear) trained to predict some property of interest from the hidden representations of a pretrained model. If a linear probe can accurately predict a property, that property is encoded in an accessible form within the representation.\n\n24.4.1 Probing Methodology\nThe standard probing workflow extracts hidden states from a pretrained model for a set of inputs where the property of interest is known. These hidden states, without further transformation, serve as features for training a simple classifier to predict the property. The classifier’s accuracy indicates how well the representation encodes the probed property, while its simplicity (linearity, minimal parameters) ensures that the probe identifies information present in the representation rather than information the probe itself computes.\nFor protein language models like ESM-2, probing has revealed that representations encode secondary structure, solvent accessibility, contact maps, and even 3D coordinates to a surprising degree, as discussed in Chapter 12. These properties emerge despite training on sequence alone, demonstrating that masked language modeling on evolutionary sequences induces representations that capture structural information. For DNA language models, probing can assess whether representations encode chromatin state, gene boundaries, promoter versus enhancer identity, or species-specific regulatory signatures.\nProbing provides diagnostic information distinct from downstream task performance. A model might achieve high accuracy on a regulatory prediction task by learning shortcuts (correlations with GC content, distance to annotated genes) rather than encoding genuine regulatory grammar. Probing can detect such shortcuts: if representations strongly encode GC content but weakly encode transcription factor binding site presence, the model may be exploiting composition rather than sequence logic. This diagnostic function complements the confounder analysis discussed in Chapter 22.\n\n\n24.4.2 Limitations of Probing\nProbing results require careful interpretation. A probe’s failure to predict some property might indicate that the representation does not encode it, or might reflect limitations of the probe architecture, insufficient training data, or mismatch between the probe’s capacity and the complexity of the encoding. Linear probes may miss nonlinearly encoded information; more complex probes risk learning the property themselves rather than reading it from the representation.\nThe selectivity-accessibility tradeoff complicates interpretation. A representation might encode a property accessibly (recoverable by a linear probe) or selectively (encoded but requiring nonlinear decoding). Properties encoded selectively might be present but not easily extracted, while properties encoded accessibly might be incidentally correlated with the training objective rather than causally important. Combining probing with causal interventions (ablating representation components and measuring effects on downstream predictions) provides stronger evidence about which encoded properties actually matter.\nFigure recommendation: A schematic showing the probing setup, with a pretrained model producing hidden representations, a linear probe trained on those representations, and accuracy metrics for different probed properties. Include a bar chart comparing probe accuracy for different properties (secondary structure, binding site presence, GC content) across model layers, illustrating how different information localizes to different depths.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#attention-patterns-in-transformer-models",
    "href": "p5-ch24-interpretability.html#attention-patterns-in-transformer-models",
    "title": "24  Interpretability and Mechanism",
    "section": "24.5 Attention Patterns in Transformer Models",
    "text": "24.5 Attention Patterns in Transformer Models\nTransformer-based genomic models use self-attention to aggregate information across long sequence contexts, potentially capturing distal regulatory interactions invisible to models with narrow receptive fields. Attention weights indicate which positions each position attends to, creating natural candidates for interpretability: perhaps high attention weights identify functionally related sequence elements.\n\n24.5.1 What Attention Patterns Reveal\nWhen attention weights are analyzed in genomic language models, certain heads exhibit strikingly structured patterns. Some heads preferentially connect positions within the same predicted gene or operon, suggesting the model has learned gene boundaries from sequence alone. Other heads show long-range connections that align with known enhancer-promoter relationships or chromatin loop anchors. Still others cluster positions by functional annotation, connecting genes with similar Gene Ontology terms despite lacking explicit functional labels during training.\nIn models like Enformer that predict regulatory outputs from long genomic windows, attention can reveal which distal regions influence predictions at a target gene. Contribution scores aggregated across attention heads often peak at known enhancers, insulators, and chromatin domain boundaries. These patterns suggest that the model has learned aspects of regulatory architecture from the correlation between sequence and chromatin output labels.\n\n\n24.5.2 Why Attention Weights Mislead\nRaw attention weights require skeptical interpretation. High attention between two positions indicates information flow in the model’s computation but does not necessarily indicate causal influence on predictions. Attention serves multiple computational roles beyond identifying important features: routing information for intermediate computations, implementing positional reasoning, and satisfying architectural constraints. A position receiving high attention might be used for bookkeeping rather than contributing to the final output.\nSeveral specific issues undermine naive attention interpretation. Attention weights describe information movement before value vectors are applied; positions with high attention but small value vector magnitudes contribute little to the output. Multi-head attention averages across heads with different functions; examining average attention obscures specialized head behavior. Cross-layer effects mean that the importance of early-layer attention depends on what later layers do with the routed information.\nMore robust approaches combine attention analysis with perturbation experiments. If deleting a position that receives high attention changes the prediction substantially, the attention is functionally meaningful. If deletion has minimal effect, the attention may serve computational purposes unrelated to the target output. Attention rollout and attention flow methods propagate attention through layers to better capture information movement across the full network, though these too provide correlational rather than causal evidence.\nFigure recommendation: An attention heatmap for a genomic region containing a gene and distal enhancer, showing different attention head patterns: one head connecting enhancer to promoter, another attending uniformly, a third showing local patterns. Annotate with known regulatory elements and indicate which attention patterns survive deletion testing.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#regulatory-vocabularies-and-global-interpretability",
    "href": "p5-ch24-interpretability.html#regulatory-vocabularies-and-global-interpretability",
    "title": "24  Interpretability and Mechanism",
    "section": "24.6 Regulatory Vocabularies and Global Interpretability",
    "text": "24.6 Regulatory Vocabularies and Global Interpretability\nLocal interpretability methods explain individual predictions; global interpretability characterizes what a model has learned across its entire training distribution. For genomic models trained to predict thousands of chromatin features, global interpretability asks whether the model has learned a coherent vocabulary of regulatory sequence classes and how those classes map to biological programs.\n\n24.6.1 Sequence Classes from Sei\nSei exemplifies the global interpretability approach by learning a vocabulary of regulatory sequence classes that summarize chromatin profile diversity across the genome. The model predicts tens of thousands of chromatin outputs (transcription factor binding, histone modifications, accessibility across cell types), then compresses this high-dimensional prediction space into approximately 40 sequence classes through dimensionality reduction and clustering.\nEach sequence class corresponds to a characteristic regulatory activity pattern. Some classes show promoter-like signatures (H3K4me3, TSS proximity, broad expression). Others exhibit enhancer patterns (H3K27ac, H3K4me1, cell-type-restricted activity). Repressive classes display H3K27me3 or H3K9me3 enrichment. Cell-type-specific classes capture lineage-restricted regulatory programs (neuronal, immune, hepatic). This vocabulary transforms thousands of raw chromatin predictions into a compact, interpretable representation.\nVariants can be characterized by their effects on sequence class scores, yielding functional descriptions more informative than raw pathogenicity predictions. A variant that shifts a region from enhancer-like to promoter-like class, or from active to repressive, provides mechanistic hypotheses about its functional consequences. GWAS enrichment analysis can identify which sequence classes are overrepresented among disease-associated variants, revealing the regulatory programs most relevant to specific phenotypes.\n\n\n24.6.2 Embedding Geometry and Regulatory Programs\nBeyond discrete sequence classes, the continuous geometry of learned representations encodes regulatory relationships. Sequences with similar regulatory functions cluster in embedding space; directions in this space correspond to biological axes of variation. Dimensionality reduction techniques (UMAP, t-SNE, PCA) visualize these relationships, revealing how the model organizes regulatory diversity.\nFor foundation models trained on diverse genomic tasks, embedding geometry can capture cross-task relationships. Sequences that function as enhancers in one cell type might cluster near sequences with enhancer function in related cell types, even if trained independently. Variants that disrupt shared regulatory logic should produce similar embedding perturbations. These geometric properties enable transfer of interpretability insights across tasks and provide compact summaries of model knowledge.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#mechanistic-interpretability",
    "href": "p5-ch24-interpretability.html#mechanistic-interpretability",
    "title": "24  Interpretability and Mechanism",
    "section": "24.7 Mechanistic Interpretability",
    "text": "24.7 Mechanistic Interpretability\nClassical interpretability methods treat models as input-output functions, probing what they compute without examining how they compute it. Mechanistic interpretability takes a different approach, attempting to reverse-engineer the algorithms implemented by neural network weights. This emerging field, most developed for language models, offers tools increasingly applicable to genomic foundation models.\n\n24.7.1 Circuits and Features\nThe central hypothesis of mechanistic interpretability is that neural networks implement interpretable computations through identifiable circuits: connected subnetworks that perform specific functions. A circuit might detect whether a motif is present, compute the distance between two motifs, or integrate evidence across regulatory elements. Identifying circuits requires tracing information flow through the network and characterizing what each component contributes.\nFeatures are the atomic units of this analysis: directions in activation space that correspond to interpretable concepts. In language models, features have been found that activate for specific topics, syntactic structures, or semantic properties. Analogous features in genomic models might activate for transcription factor binding sites, coding versus non-coding sequence, or regulatory element types. Sparse autoencoders trained on model activations can extract interpretable features by encouraging representations where most features are inactive for any given input.\nSuperposition complicates feature identification. Neural networks can represent more features than they have dimensions by using overlapping, nearly orthogonal directions. Features active for different inputs can share parameters, enabling high-capacity representations but complicating interpretation. Techniques from compressed sensing and dictionary learning help decompose superposed representations into constituent features.\n\n\n24.7.2 Applications to Genomic Models\nMechanistic interpretability remains nascent for genomic foundation models, but initial applications show promise. Attention head analysis in DNA language models has identified heads specialized for different genomic functions: some attend within genes, others across regulatory regions, still others implement positional computations. Probing activations at different layers reveals hierarchical feature construction, from local sequence patterns in early layers to long-range regulatory relationships in later layers.\nCircuit analysis can explain specific model behaviors. If a model predicts that a variant disrupts regulation, mechanistic analysis can trace which features activate differently for reference versus variant sequence, which attention heads route information about the variant to the prediction, and which intermediate computations change. This mechanistic trace provides far richer explanation than attribution scores alone, potentially identifying the regulatory logic the model has learned.\nThe challenge is scalability. Current mechanistic interpretability techniques require substantial manual analysis and work best for small models or specific behaviors. Foundation models with billions of parameters resist exhaustive circuit enumeration. Developing automated tools for circuit discovery and scaling mechanistic analysis to large genomic models represents an active research frontier.\nFigure recommendation: A simplified circuit diagram showing how a hypothetical genomic model might process a sequence containing a transcription factor binding site. Show information flow from input embeddings through attention heads that detect the motif, through intermediate layers that contextualize it, to the final prediction. Annotate with interpretable feature descriptions at each stage.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#validation-from-explanations-to-experiments",
    "href": "p5-ch24-interpretability.html#validation-from-explanations-to-experiments",
    "title": "24  Interpretability and Mechanism",
    "section": "24.8 Validation: From Explanations to Experiments",
    "text": "24.8 Validation: From Explanations to Experiments\nInterpretability methods produce explanations, but explanations are only valuable if they accurately reflect model behavior and connect to biological reality. Validation closes the loop by testing whether interpretability-derived hypotheses hold when subjected to experimental scrutiny.\n\n24.8.1 Faithfulness Testing\nAn interpretation is faithful if it accurately describes what the model does. Testing faithfulness requires interventions: changing the features identified as important and verifying that predictions change accordingly. If an attribution method highlights certain positions as driving a prediction, deleting or scrambling those positions should reduce the prediction. If discovered motifs are claimed to be necessary for regulatory activity, removing them from sequences should impair predicted and measured function.\nSanity checks provide baseline validation. When model weights are randomized, attributions should degrade to uninformative noise. When training labels are scrambled, discovered motifs should disappear or lose predictive power. These checks identify methods that produce plausible-looking outputs regardless of model content, revealing explanations that reflect method biases rather than genuine model features.\nCounterfactual experiments go further by testing whether identified features are sufficient as well as necessary. Inserting discovered motifs into neutral sequences should increase predicted regulatory activity if the motifs genuinely encode functional elements. Constructing synthetic sequences that combine motifs according to discovered grammatical rules should produce predictions consistent with those rules. Discrepancies between expected and observed effects indicate gaps in the interpretation.\n\n\n24.8.2 Experimental Validation\nThe ultimate test of interpretability connects model-derived hypotheses to biological experiments. Motifs discovered through TF-MoDISco can be tested through electrophoretic mobility shift assays, ChIP-qPCR, or reporter constructs. Predicted spacing constraints can be validated by varying distances between motifs in synthetic constructs and measuring activity. Hypothesized enhancer-promoter connections can be tested through CRISPR deletion of predicted enhancers and measurement of target gene expression.\nThis experimental validation distinguishes genuine mechanistic discovery from pattern matching that happens to produce plausible-looking results. A model might learn that certain k-mers correlate with regulatory activity for confounded reasons (batch effects, mappability artifacts) yet produce motif logos resembling real transcription factors. Only experimental testing can determine whether model-derived hypotheses reflect causal regulatory logic.\nHigh-throughput functional assays enable systematic validation at scale. Massively parallel reporter assays (MPRAs) can test thousands of model-predicted regulatory elements simultaneously. Perturb-seq combines CRISPR perturbations with single-cell RNA-seq to measure effects of knocking out predicted regulatory factors. These technologies create opportunities for iterative model improvement: interpretability generates hypotheses, experiments test them, and results refine both model architecture and training.\nFigure recommendation: A validation pipeline showing the loop from interpretability analysis through hypothesis generation, experimental testing, and model refinement. Include specific examples: TF-MoDISco motif → EMSA validation, attention-derived enhancer prediction → CRISPR deletion, probing-derived feature → reporter assay.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#interpretability-in-clinical-variant-assessment",
    "href": "p5-ch24-interpretability.html#interpretability-in-clinical-variant-assessment",
    "title": "24  Interpretability and Mechanism",
    "section": "24.9 Interpretability in Clinical Variant Assessment",
    "text": "24.9 Interpretability in Clinical Variant Assessment\nVariant interpretation guidelines require that computational predictions be weighed alongside experimental and clinical evidence, as discussed further in Chapter 26. Interpretability determines whether model predictions can contribute meaningful evidence beyond raw pathogenicity scores.\nCurrent ACMG-AMP criteria allow computational evidence as supporting (PP3) or opposing (BP4) pathogenicity, but the evidence strength depends on understanding what the prediction reflects. A splice site disruption score from SpliceAI provides interpretable mechanistic evidence: the variant is predicted to alter splicing because it changes the consensus splice site sequence. This prediction can be evaluated against splice site models, tested with minigene assays, and combined with observations of aberrant transcripts in patient samples. The interpretation enables evidence integration.\nFoundation model predictions are less immediately interpretable but potentially more informative. A pathogenicity score from ESM-1v reflects evolutionary constraint inferred from protein language modeling, but the specific sequence features driving the prediction require attribution analysis to identify. An expression effect predicted by Enformer might result from disrupted transcription factor binding, altered chromatin accessibility, or changed 3D regulatory contacts; interpretability analysis distinguishes these mechanisms and guides experimental validation.\nFor clinical utility, interpretability must be communicated effectively. Genome browsers displaying attribution tracks alongside variant calls help clinicians identify mechanistic hypotheses. Reports that accompany pathogenicity scores with regulatory vocabulary classifications (this variant shifts an enhancer toward a repressive state) provide actionable context. These communication challenges extend interpretability beyond algorithm development to user interface design and clinical workflow integration.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#practical-approaches-for-foundation-model-analysis",
    "href": "p5-ch24-interpretability.html#practical-approaches-for-foundation-model-analysis",
    "title": "24  Interpretability and Mechanism",
    "section": "24.10 Practical Approaches for Foundation Model Analysis",
    "text": "24.10 Practical Approaches for Foundation Model Analysis\nWorking with genomic foundation models requires matching interpretability methods to specific questions. Several complementary strategies address different aspects of model behavior.\nFor understanding variant effects, the primary goal is explaining why a specific variant receives a particular prediction. Attribution methods (ISM for validation, integrated gradients for efficiency) identify which input positions drive the difference between reference and alternative predictions. If the variant falls within a discovered motif, the interpretation is straightforward. If attributions spread across the sequence, the effect may operate through long-range regulatory changes requiring attention analysis or contribution scores from models like Enformer.\nFor characterizing model representations, probing classifiers diagnose what information is encoded and at which layers. Probing for known regulatory features (promoter versus enhancer, tissue specificity, evolutionary conservation) establishes which biological properties the model captures. Probing for potential confounders (GC content, distance to annotated genes, technical artifacts) identifies shortcuts that might inflate benchmark performance without reflecting genuine regulatory understanding.\nFor discovering regulatory logic, TF-MoDISco applied to high-confidence predictions extracts motif vocabularies specific to prediction tasks or cell types. Grammar analysis of motif co-occurrence reveals combinatorial rules. Sei-style sequence class analysis situates local motifs within global regulatory programs. Comparing discovered vocabularies across models or training conditions reveals shared versus idiosyncratic features.\nFor debugging and auditing, interpretability methods identify what features drive predictions in held-out distributions. If a model fails on a new cell type, attribution analysis can reveal whether it relies on cell-type-specific versus generalizable features. If performance degrades on specific genomic regions, local interpretability can identify confounding patterns or training data gaps.\nFor generating experimental hypotheses, interpretability produces testable predictions. Discovered motifs can be synthesized and tested. Predicted regulatory elements can be perturbed. Hypothesized TF binding can be validated by ChIP. Model-derived predictions that survive experimental testing represent genuine mechanistic insights; predictions that fail point toward model limitations or confounding.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p5-ch24-interpretability.html#from-explanations-to-understanding",
    "href": "p5-ch24-interpretability.html#from-explanations-to-understanding",
    "title": "24  Interpretability and Mechanism",
    "section": "24.11 From Explanations to Understanding",
    "text": "24.11 From Explanations to Understanding\nInterpretability for genomic foundation models is evolving from post hoc explanation toward model-assisted mechanistic discovery. Attribution methods identify important positions, motif discovery extracts regulatory vocabularies, probing characterizes learned representations, and mechanistic interpretability traces computational circuits. Combined with experimental validation, these tools transform black-box predictors into sources of testable biological hypotheses.\nThe distinction between plausibility and faithfulness remains central. Models can produce compelling motifs, structured attention patterns, and interpretable probing results while operating through mechanisms that do not correspond to biological reality. Only interventional experiments, both computational (deletion tests, counterfactual sequence generation) and biological (reporter assays, CRISPR perturbations), can distinguish genuine regulatory insight from sophisticated pattern matching.\nAs foundation models grow in scale and capability, interpretability becomes simultaneously more important and more challenging. Larger models implement more complex computations, potentially capturing subtler regulatory logic but resisting simple interpretation. Mechanistic interpretability offers a path forward by characterizing model internals directly, though scaling these techniques to billion-parameter genomic models remains an open problem.\nThe integration of interpretability with model development points toward a future where understanding and prediction advance together. Motifs and grammars discovered through interpretation can inform architecture design and training objectives. Experimentally validated hypotheses can become supervision signals for model refinement. Interpretability failures that reveal confounding can drive improvements in training data and evaluation protocols. In this vision, interpretability is not merely a tool for explaining existing models but a methodology for building models that embody genuine biological understanding, creating systems whose predictions we trust because we understand the mechanisms they have learned.",
    "crumbs": [
      "Part V: Evaluation and Reliability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability and Mechanism</span>"
    ]
  },
  {
    "objectID": "p6--translation.html",
    "href": "p6--translation.html",
    "title": "Part VI — Translation and Application",
    "section": "",
    "text": "The preceding parts of this book have traced the development of genomic foundation models from their architectural foundations through the challenges of reliable evaluation and interpretation. We have examined how convolutional networks learn sequence-to-function mappings, how transformers capture long-range dependencies and emergent representations, and how multi-omic integration extends these capabilities to systems-level biology. We have also confronted the reliability concerns that shadow this progress: confounders that inflate apparent performance, calibration failures that undermine clinical utility, and interpretability gaps that complicate mechanistic claims.\nPart VI turns from methods to practice. The question shifts from how these models work to how they are used, and from what they can predict to what they enable us to do. This transition is not merely practical but conceptual: deploying a model in a clinical or industrial setting exposes assumptions that benchmarks leave implicit and reveals failure modes that curated evaluations obscure.\nThe five chapters in this part span the major application domains where genomic foundation models are reshaping practice. ?sec-clinical-risk examines clinical risk prediction, where foundation model features combine with electronic health records and traditional risk factors to stratify patients for disease, progression, and treatment response. The discussion emphasizes calibration, uncertainty quantification, and fairness considerations that are essential when predictions inform medical decisions and resource allocation. 26  Rare Disease and Variant Interpretation focuses on variant interpretation in rare disease and cancer, where models enter diagnostic pipelines alongside clinical geneticists and laboratory scientists, and where the stakes of misclassification are measured in missed diagnoses and inappropriate interventions. 27  Drug Discovery and Target Identification explores drug discovery and biotechnology, where genomic foundation models contribute to target identification, genetic validation, biomarker development, and the broader industrial ecosystem that translates genetic insights into therapeutics. 28  Sequence Design and Engineering reverses the direction of inference, moving from prediction to generation: how foundation models guide protein engineering, regulatory element design, and the emerging field of programmable biology. Finally, 29  Regulatory, Ethical, and Future Considerations steps back to consider open problems and responsible development, from technical challenges in generalization and robustness to ethical questions about equity, consent, and the governance of increasingly powerful genomic AI.\nA thread running through these chapters is the gap between benchmark performance and real-world utility. Models that achieve impressive metrics on held-out test sets may falter when deployed on populations underrepresented in training data, when integrated into workflows designed around different assumptions, or when their outputs must be communicated to clinicians, patients, and regulators who lack the technical background to interpret confidence intervals and attribution scores. Closing this gap requires not only better models but also better infrastructure for validation, monitoring, and human oversight. It requires attending to the social and institutional contexts in which genomic predictions are produced and consumed.\nThe goal of Part VI is not to provide definitive protocols for each application domain, since such protocols would be obsolete before publication in a field moving this rapidly. Instead, the aim is to develop a framework for reasoning about deployment: what questions to ask when evaluating a model for clinical use, what pitfalls to anticipate when integrating foundation model outputs into existing pipelines, and what principles should guide responsible development as these tools become more powerful. Readers who have worked through the earlier parts of this book should be well positioned to engage critically with new applications as they emerge, recognizing both the genuine capabilities and the persistent limitations of genomic foundation models in practice.",
    "crumbs": [
      "Part VI — Translation and Application"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html",
    "href": "p6-ch25-clinical-risk.html",
    "title": "25  Clinical Risk Prediction",
    "section": "",
    "text": "25.1 From Polygenic Scores to Foundation Model Features\nWe can sequence a human genome for a few hundred dollars and compute polygenic scores for dozens of diseases in minutes, yet these genetic insights rarely change clinical decisions. A patient with a high polygenic risk score for coronary artery disease receives the same treatment recommendations as one without genetic testing, because the score alone does not tell the cardiologist what to do differently. The fundamental challenge is not generating genomic predictions but translating them into actions that improve outcomes.\nThis translation gap persists because clinical risk prediction demands more than statistical association. A useful risk model must discriminate between patients who will and will not experience an outcome, produce well-calibrated probabilities that clinicians can trust at face value, perform equitably across the diverse populations who present for care, and integrate seamlessly into workflows where decisions are made in minutes, not hours. Traditional polygenic scores, despite their scientific validity, often fail one or more of these requirements. They reduce entire genomes to single numbers that provide little mechanistic insight, transfer poorly across ancestries due to European-dominated training data, and exist outside the electronic health records where clinical decisions actually happen.\nGenomic foundation models offer a different approach. Rather than collapsing genetic information into scalar risk scores derived from genome-wide association studies, foundation models produce rich embeddings that capture sequence context, regulatory grammar, and functional consequences. These representations can integrate with clinical data through the fusion architectures discussed in Chapter 19, adapt to diverse prediction tasks through transfer learning (Chapter 9), and provide interpretable feature attributions that connect predictions to biological mechanisms (Chapter 24). The question is whether these capabilities translate into clinical tools that actually change practice.\nThis chapter examines that translation challenge. The discussion covers how foundation model features combine with electronic health records in risk architectures, the evidence standards required for clinical deployment, the fairness considerations that determine whether genomic AI reduces or amplifies health disparities, and the practical realities of integrating predictions into care delivery. Three case studies illustrate these principles in cardiometabolic risk stratification, oncology prognosis, and pharmacogenomic adverse event prediction.\nThe limitations of classical polygenic risk scores define the opportunity for foundation model approaches. As discussed in Chapter 3, polygenic scores aggregate the effects of common variants into weighted sums, with weights derived from genome-wide association study effect sizes. This framework has demonstrated that common genetic variation contributes substantially to risk for conditions including coronary artery disease, type 2 diabetes, and breast cancer. A patient in the top percentile of polygenic risk for coronary disease faces roughly threefold higher lifetime risk than one in the bottom percentile, a gradient comparable to traditional risk factors like smoking or hyperlipidemia.\nThree limitations constrain the clinical impact of this approach. First, the linear additive model cannot capture epistatic interactions where the effect of one variant depends on the presence of others, nor can it represent the complex nonlinear relationships between genetic variation and disease that emerge from regulatory networks and cellular pathways. Second, polygenic scores derived from European-ancestry genome-wide association studies substantially underperform in other populations, with effect sizes often attenuating by half or more in African or East Asian ancestries due to differences in linkage disequilibrium structure and allele frequencies (Chapter 22). Third, a single scalar provides no mechanistic insight: a high polygenic score for diabetes does not indicate whether risk stems from impaired insulin secretion, insulin resistance, or altered satiety signaling, information that might guide intervention selection.\nFoundation models address these limitations through richer representations. Instead of treating variants as independent weighted features, models like Delphi and G2PT learn genome-wide embeddings that encode sequence context, regulatory annotations, and cross-variant interactions (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025). These approaches can capture nonlinear structure in genetic risk, leverage functional priors that transfer across ancestries, and provide attention-based attributions that highlight which genomic regions contribute most to predictions. Fine-mapping models like MIFM estimate posterior probabilities for causal variants within association loci, allowing risk models to weight variants by evidence for causality rather than treating all correlated variants equally (Rakowski and Lippert 2025).\nThe practical architecture of a foundation model-enabled risk system typically involves three components: pretrained encoders that transform genomic data into embeddings, aggregation modules that summarize variant-level or region-level representations into patient-level features, and prediction heads that map these features (combined with clinical covariates) to risk estimates. This modular design separates the computationally expensive foundation model inference from the task-specific prediction layer, enabling updates to either component while maintaining clear interfaces for validation.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html#defining-clinical-risk-prediction",
    "href": "p6-ch25-clinical-risk.html#defining-clinical-risk-prediction",
    "title": "25  Clinical Risk Prediction",
    "section": "25.2 Defining Clinical Risk Prediction",
    "text": "25.2 Defining Clinical Risk Prediction\nA risk prediction model is only as useful as the decision it informs. Effective clinical risk prediction requires precise specification of four elements: the outcome being predicted, the time horizon over which prediction applies, the target population for whom the model is intended, and the clinical action the prediction will trigger.\nConsider a 55-year-old woman with moderately elevated cholesterol and a family history of early coronary disease. Her cardiologist must decide whether to initiate statin therapy, a decision traditionally guided by 10-year cardiovascular risk estimates from tools like the Pooled Cohort Equations. A genomic foundation model could augment this decision in several ways. It might refine her absolute risk estimate by incorporating polygenic information that the traditional calculator ignores. It might identify whether her genetic risk concentrates in pathways amenable to specific interventions (LDL metabolism favoring statins versus inflammatory pathways suggesting alternative approaches). It might flag pharmacogenomic variants affecting statin metabolism that influence dose selection or drug choice.\nEach of these applications represents a different prediction task with distinct requirements. The 10-year risk estimate for major adverse cardiovascular events is an individual-level incident risk problem where discrimination and calibration matter most. The pathway-level attribution is an interpretability challenge requiring mechanistic grounding. The pharmacogenomic prediction is a treatment selection problem where the relevant outcome is adverse drug reaction risk conditional on therapy initiation.\nClinical risk prediction tasks cluster into several archetypes. Incident risk concerns whether a currently disease-free individual will develop disease within a specified window, such as 10-year diabetes risk for prediabetic patients. Progression risk asks which patients with existing disease will develop complications, for instance nephropathy in diabetes or heart failure after myocardial infarction. Survival and prognosis involve time-from-diagnosis to events like death, recurrence, or transplant, often requiring survival models that handle censoring and competing risks. Treatment response and toxicity concerns whether a patient will benefit from one therapy versus another and their probability of experiencing serious adverse effects.\nFoundation models enter these problems as feature generators. They transform raw sequence data into structured representations that downstream prediction models combine with clinical covariates. The architectural choices for this combination, and the evidence required to trust the resulting predictions, constitute the core methodological challenges of clinical translation.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html#feature-integration-architectures",
    "href": "p6-ch25-clinical-risk.html#feature-integration-architectures",
    "title": "25  Clinical Risk Prediction",
    "section": "25.3 Feature Integration Architectures",
    "text": "25.3 Feature Integration Architectures\nThe features available for clinical risk models draw on multiple foundation model families, each capturing different aspects of genetic and molecular risk.\nDNA-level foundation models provide variant effect predictions without requiring trait-specific training. Systems like Nucleotide Transformer, HyenaDNA, and GPN compute sequence-based deleteriousness scores that reflect how mutations disrupt regulatory grammar, splice sites, or protein-coding sequences (Dalla-Torre et al. 2023; Nguyen et al. 2023; Benegas, Batra, and Song 2023). These zero-shot predictions transfer across traits and ancestries because they derive from sequence properties rather than population-specific association statistics. Fine-mapping models like MIFM integrate such functional priors with association evidence to estimate which variants within a locus are likely causal, providing principled weights for aggregation (Rakowski and Lippert 2025).\nProtein language models add coding variant interpretation. AlphaMissense and related systems predict pathogenicity for missense mutations based on evolutionary conservation patterns learned from millions of protein sequences, as discussed in Chapter 12. For conditions with strong coding variant contributions (Mendelian cardiomyopathies, cancer predisposition syndromes), these predictions provide crucial signal beyond what noncoding regulatory models capture.\nMulti-omics foundation models extend beyond germline sequence. Cell-type-resolved representations from GLUE, scGLUE, and CpGPT capture regulatory state across chromatin accessibility, methylation, and expression (Chapter 16) (Cao and Gao 2022; Camillo et al. 2024). Rare variant burden scores from DeepRVAT aggregate predicted effects across genes into pathway-level impairment measures (Clarke et al. 2024). For oncology applications, tumor embedding models like SetQuence and graph neural network-based subtypers encode complex somatic mutation landscapes into patient-level representations (Jurenaite et al. 2024; Li et al. 2022).\nElectronic health record features provide the clinical context without which genomic predictions lack meaning. Demographics, vital signs, laboratory values, medication lists, problem codes, and procedure histories characterize the patient’s current state and trajectory. Time-varying biomarker trajectories (estimated glomerular filtration rate trends, hemoglobin A1c patterns, tumor marker dynamics) capture disease evolution that static snapshots miss.\nThe architectural question is how to combine these heterogeneous inputs. Three fusion strategies offer different tradeoffs.\nEarly fusion concatenates all features into a single input vector and trains a unified model (neural network, gradient boosting, survival regression) on the combined representation. This approach allows the model to learn arbitrary interactions between genomic and clinical features but requires all inputs to be present for every patient, handles scale differences between modalities poorly, and can be dominated by whichever input provides the most features or strongest signal.\nIntermediate fusion trains separate encoders for each modality, producing genomic embeddings, clinical embeddings, and multi-omic embeddings that a fusion module then combines. The fusion module might use attention mechanisms to weight modality contributions dynamically, cross-modal transformers that allow features from one modality to attend to features from another, or simpler concatenation with learned combination weights. This approach offers modularity (foundation model encoders can be swapped as new versions become available) while still enabling learned cross-modal interactions.\nLate fusion trains independent models for each modality and combines their predictions through ensemble methods or meta-learning. A polygenic score model, an electronic health record model, and a multi-omic model each produce risk estimates that a final layer integrates. This approach handles missing modalities gracefully (each submodel operates independently) and allows modality-specific architectures but may underutilize cross-modal structure since interactions can only be captured at the final combination stage.\nFor clinical deployment, intermediate fusion often provides the best balance. It enables modular updates as foundation models improve, allows graceful degradation when modalities are missing, and captures cross-modal interactions that late fusion misses. The specific fusion mechanism (attention, concatenation, cross-modal transformer) matters less than ensuring the architecture supports the operational requirements of clinical deployment: batch computation, uncertainty quantification, and interpretable feature attribution.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html#evaluation-for-clinical-deployment",
    "href": "p6-ch25-clinical-risk.html#evaluation-for-clinical-deployment",
    "title": "25  Clinical Risk Prediction",
    "section": "25.4 Evaluation for Clinical Deployment",
    "text": "25.4 Evaluation for Clinical Deployment\nHigh performance on held-out test sets is necessary but far from sufficient for clinical deployment. Risk models must satisfy multiple evidence standards that typical machine learning papers do not address, and teams planning translation must understand these requirements from the outset rather than discovering them after development is complete.\n\n25.4.1 Discrimination\nDiscrimination measures how well a model ranks patients by risk, distinguishing those who will experience outcomes from those who will not. For binary endpoints like disease occurrence within a fixed time window, the area under the receiver operating characteristic curve (AUROC) summarizes discrimination across all classification thresholds. When outcomes are rare (severe adverse drug reactions, specific disease subtypes), the area under the precision-recall curve (AUPRC) better reflects how well the model identifies true positives among many negatives. For survival tasks with censoring, the concordance index and time-dependent AUC generalize these metrics to the time-to-event setting.\nStrong discrimination is necessary but not sufficient. A model that correctly ranks patients but systematically overestimates or underestimates absolute risk magnitudes will lead to inappropriate clinical decisions. If a model predicts 5% risk for patients who actually experience 15% event rates, physicians using those predictions will undertreat. Conversely, systematically inflated predictions lead to overtreatment with attendant harms and costs.\n\n\n25.4.2 Calibration\nCalibration asks whether predicted probabilities match observed frequencies. If a model assigns 20% risk to a group of patients, approximately 20% should experience the outcome. Well-calibrated predictions can be interpreted at face value and used directly for clinical decision-making; miscalibrated predictions mislead regardless of discrimination quality.\nAssessment involves calibration plots comparing predicted risk deciles to observed event rates, statistical tests like the Hosmer-Lemeshow test, and proper scoring rules like the Brier score that combine calibration and discrimination. These assessments must be stratified by clinically relevant subgroups (ancestry, sex, age, comorbidity burden) because a model well-calibrated overall may be systematically miscalibrated for specific populations.\nFor polygenic score-informed models, calibration requires particular attention. Raw polygenic scores are typically centered and scaled rather than calibrated to absolute risk. Mapping a score to an absolute event probability requires post-hoc models incorporating baseline incidence and clinical covariates. Foundation models can shift score distributions as architectures evolve, meaning recalibration may be necessary when updating encoders. The connection to Chapter 23 is direct: calibration is one form of uncertainty quantification, assessing whether model confidence aligns with actual outcome frequencies.\n\n\n25.4.3 Clinical Utility\nBeyond discrimination and calibration, clinical utility asks whether using the model will change decisions beneficially. Net reclassification improvement quantifies how many patients are appropriately moved across risk thresholds compared to a baseline model. Decision curve analysis estimates net benefit across threshold probabilities, accounting for the relative costs of false positives and false negatives in specific clinical contexts.\nFor foundation model-based tools, these analyses must demonstrate incremental value over existing alternatives. If a complex genomic foundation model provides only marginal improvement over a traditional polygenic score plus standard clinical calculator, the additional complexity, cost, and implementation burden may not be justified. The relevant comparison is not “better than nothing” but “better than what clinicians can already access.”\n\n\n25.4.4 The Validation Hierarchy\nEvidence strength depends critically on validation design. Internal validation through cross-validation or temporal splits within development data is useful but insufficient due to potential overfitting and subtle data leakage issues discussed in Chapter 22. External validation across institutions and ancestries tests the same locked model in independent health systems and diverse populations. This step is essential for assessing whether performance reflects genuine biological signal versus idiosyncratic features of the development dataset.\nProspective observational validation runs the model silently alongside clinical care without influencing decisions, measuring real-time performance and drift in deployment conditions. Prospective interventional trials use randomized or quasi-experimental designs to assess whether model-guided care actually improves outcomes, equity, and cost-effectiveness compared to usual care.\nFor most foundation model-based tools, regulators and health systems expect robust external validation at minimum. High-stakes applications (cancer prognosis affecting treatment intensity, pharmacogenomic predictions affecting drug choice) may require prospective interventional evidence. The investment required increases at each level of the hierarchy, but so does the confidence that deployment will produce benefit rather than harm.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html#uncertainty-quantification",
    "href": "p6-ch25-clinical-risk.html#uncertainty-quantification",
    "title": "25  Clinical Risk Prediction",
    "section": "25.5 Uncertainty Quantification",
    "text": "25.5 Uncertainty Quantification\nIn clinical settings, models must know when they do not know. A risk prediction offered with false confidence is more dangerous than one accompanied by appropriate uncertainty bounds, because the former invites unwarranted action while the latter prompts appropriate caution or additional evaluation.\nTwo sources of uncertainty require distinction. Aleatoric uncertainty reflects irreducible noise in the outcome: even with perfect input features, some patients with identical measured characteristics will experience different outcomes due to unmeasured variables, stochastic biology, or measurement error. Epistemic uncertainty reflects model limitations: insufficient training data, architectural constraints, or distributional shift between training and deployment conditions. Aleatoric uncertainty cannot be reduced by collecting more data or improving models; epistemic uncertainty can.\nPractical uncertainty quantification methods include ensemble approaches, where multiple models trained with different random seeds provide prediction intervals based on their disagreement. Monte Carlo dropout approximates Bayesian uncertainty by averaging predictions across stochastic forward passes. Conformal prediction provides principled prediction intervals with guaranteed coverage under exchangeability assumptions, avoiding the distributional assumptions required by parametric methods. Temperature scaling post-hoc adjusts model outputs to improve calibration without retraining.\nFor foundation model-based systems, uncertainty decomposes into genomic and clinical components. Genomic uncertainty reflects confidence in variant effect predictions, fine-mapping probabilities, or embedding reliability; it increases for variants from underrepresented populations, rare variants with limited training examples, or sequences falling outside the distribution seen during pretraining. Clinical uncertainty reflects extrapolation to new care settings, practice patterns, or patient populations not represented in development data.\nSelective prediction allows models to abstain when uncertainty exceeds thresholds, flagging cases for human review rather than providing potentially misleading predictions. This is particularly important for patients from rare ancestries underrepresented in training data or with unusual clinical presentations. The tension between coverage (providing predictions for all patients) and reliability (ensuring predictions are trustworthy) must be navigated thoughtfully, ideally with input from the clinicians who will use the system.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html#fairness-and-health-equity",
    "href": "p6-ch25-clinical-risk.html#fairness-and-health-equity",
    "title": "25  Clinical Risk Prediction",
    "section": "25.6 Fairness and Health Equity",
    "text": "25.6 Fairness and Health Equity\nMany genomic and electronic health record datasets encode historical inequities in who gets genotyped, which populations are recruited into biobanks, and how healthcare is documented and delivered. Risk models trained on such data can amplify disparities if not carefully evaluated and designed.\nThe ancestry bias in genome-wide association studies persists in foundation model applications. As discussed in Chapter 3, polygenic scores derived from European-ancestry data substantially underperform in other populations. Foundation models have the opportunity but not the guarantee to improve portability by leveraging functional priors that transfer across ancestries (sequence-based deleteriousness does not depend on population-specific linkage disequilibrium) and by incorporating multi-ancestry training data. Whether they succeed depends on training data composition, evaluation practices, and explicit attention to cross-ancestry performance throughout development.\nElectronic health record features introduce additional bias sources. Which patients receive genetic testing, which laboratory tests are ordered, how diagnoses are coded, and how thoroughly clinical notes are documented all differ systematically across patient populations, care settings, and health systems. A model trained on one institution’s data may encode those institutional patterns rather than underlying biology.\nHealth equity evaluation requires disparity metrics measuring performance differences in discrimination, calibration, and clinical utility across subgroups defined by ancestry, sex, socioeconomic proxies, and care site. Access metrics assess whether financial, geographic, or systemic barriers limit which patients can benefit from genomic risk tools. Outcome metrics evaluate whether clinical actions triggered by predictions differ across groups and whether benefits accrue equitably or concentrate among already-advantaged populations.\nTechnical mitigation strategies include reweighting training data to reduce representation disparities, group-wise calibration ensuring equitable performance across subgroups, and localized fine-tuning using deployment-site data. However, technical interventions alone cannot overcome structural inequities. Non-technical approaches including expanding sequencing access, subsidizing testing for underserved populations, and designing workflows that accommodate diverse care settings are equally essential.\nThe core principle is that equity cannot be an afterthought addressed during final evaluation. It must inform pretraining data selection, benchmark choice, validation study design, and deployment planning from the outset. A model that appears well-calibrated overall but is miscalibrated for specific populations will exacerbate rather than reduce health disparities.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html#clinical-integration",
    "href": "p6-ch25-clinical-risk.html#clinical-integration",
    "title": "25  Clinical Risk Prediction",
    "section": "25.7 Clinical Integration",
    "text": "25.7 Clinical Integration\nEven a comprehensively validated model can fail in practice if it does not integrate into clinical workflows. Genomic risk predictions must reach clinicians at decision points, in formats that support rather than disrupt care delivery, with appropriate interpretability and uncertainty communication.\n\n25.7.1 Workflow Integration Patterns\nClinical genomics has established pathways for returning results through CLIA-certified laboratories, structured reports, and genetic counseling. Foundation model-based risk tools can augment these pathways in two primary ways. Laboratory interpretation augmentation uses foundation model predictions to prioritize variants for manual review, provide richer functional annotations, and suggest likely disease mechanisms supporting differential diagnosis. Direct risk embedding in electronic health records precomputes risk scores for patients with genomic data, surfaces them in structured fields or clinical dashboards, and triggers alerts when thresholds are crossed.\nDesign choices include batch versus on-demand computation (batch overnight processing is often preferable given foundation model computational costs and the relative stability of genomic data), synchronous alerts at order entry versus asynchronous reports in clinical inboxes, and whether high-impact predictions require human-in-the-loop review before reaching front-line clinicians.\nThe specifics vary by clinical context. Pharmacogenomic alerts might appear synchronously at prescription order entry, providing immediate guidance on drug selection or dosing. Cardiometabolic risk scores might appear in primary care dashboards updated weekly, informing prevention discussions at annual visits. Oncology prognosis estimates might be generated at diagnosis and reviewed in tumor board settings where multidisciplinary teams make treatment decisions.\n\n\n25.7.2 System Architecture\nFrom an engineering perspective, foundation model-based clinical tools typically require a secure model-serving endpoint handling inference requests, input adapters transforming laboratory and electronic health record data into model-ready formats, output adapters mapping predictions to structured clinical concepts or user-facing text, and logging infrastructure providing audit trails and enabling drift detection.\nRegulated settings impose additional requirements: versioning of models, data pipelines, and reference genomes with complete reproducibility; access controls and network segmentation protecting genomic data; and validation environments separated from production for safe testing of updates. Practical guidance on hardware requirements, deployment patterns, and cost estimation appears in Appendix B.\n\n\n25.7.3 Post-Deployment Monitoring\nClinical deployment begins rather than ends the model lifecycle. Practice patterns evolve as new treatments and guidelines emerge. Patient populations shift as screening programs expand or contract. Laboratory assays and sequencing pipelines change, introducing distributional shifts in input features.\nMonitoring systems should track input distributions (genotype frequencies, electronic health record feature patterns) to detect when current patients differ from training populations. Output distributions (risk score histograms, threshold-crossing rates) reveal whether model behavior is changing. Performance metrics computed via rolling windows or periodic audits detect calibration or discrimination degradation before clinical consequences accumulate.\nWhen drift is detected, responses range from recalibration (adjusting the score-to-probability mapping while preserving ranking behavior) through partial retraining (updating prediction heads while keeping foundation model weights fixed) to full model updates (retraining encoders, requiring renewed validation). The modular separation between foundation model backbones and clinical prediction heads facilitates this maintenance: encoders can be versioned and swapped with compatibility testing while prediction heads adapt to local deployment conditions.\nIncident response processes allow clinicians to report surprising or harmful predictions, triggering root-cause analysis and potential remediation. Governance structures including AI oversight committees review models periodically and establish clear criteria for deprecation when performance degrades below acceptable thresholds.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html#regulatory-and-quality-frameworks",
    "href": "p6-ch25-clinical-risk.html#regulatory-and-quality-frameworks",
    "title": "25  Clinical Risk Prediction",
    "section": "25.8 Regulatory and Quality Frameworks",
    "text": "25.8 Regulatory and Quality Frameworks\nFoundation model-based clinical tools exist on a spectrum from research-only applications supporting hypothesis generation through clinical decision support tools informing diagnosis or management to regulated medical devices subject to formal oversight. The regulatory classification depends on intended use, risk level, and the claims made for the tool.\nJurisdictions differ in specifics, but common expectations include transparent descriptions of training data and known limitations, quantitative performance evidence across relevant subgroups, plans for post-market surveillance and incident reporting, and change management procedures for model updates. Beyond formal regulation, health systems typically require standard operating procedures for deployment and decommissioning, model cards describing training data and limitations, validation reports documenting evaluation evidence, and governance structures reviewing and approving new tools.\nFoundation models introduce additional documentation requirements. Descriptions of pretraining corpora must specify which genomes, assays, and populations were included. Fine-tuning datasets and label definitions require detailed documentation. Procedures for updating to new genome builds, reference panels, or assay types must be established and tested. The modular separation between pretrained encoders and clinical prediction heads can ease regulatory management by allowing independent updates to each component, but this requires careful version control and compatibility testing to ensure that updating one component does not degrade performance of the combined system.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html#case-studies",
    "href": "p6-ch25-clinical-risk.html#case-studies",
    "title": "25  Clinical Risk Prediction",
    "section": "25.9 Case Studies",
    "text": "25.9 Case Studies\nThree stylized case studies illustrate how foundation model features integrate into clinical risk prediction across different disease contexts, time horizons, and decision types.\n\n25.9.1 Cardiometabolic Risk Stratification\nA 52-year-old man presents to his primary care physician for an annual wellness visit. His LDL cholesterol is 145 mg/dL, blood pressure is 138/88 mmHg, and hemoglobin A1c is 5.9%, placing him in the prediabetic range. His father had a myocardial infarction at age 58. The standard Pooled Cohort Equations estimate his 10-year atherosclerotic cardiovascular disease risk at 8.2%, just below the threshold where guidelines recommend statin therapy.\nA foundation model-augmented risk system could refine this assessment. Variant effect scores from DNA foundation models annotate variants in cardiometabolic risk loci with predicted regulatory and coding impacts. A polygenic embedding model like Delphi or G2PT produces a genome-wide representation capturing nonlinear risk structure beyond simple effect size sums (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025). This genomic embedding combines with electronic health record features through an intermediate fusion architecture, producing an updated 10-year risk estimate of 11.4%, above the treatment threshold.\nThe clinical value depends on what this refined estimate enables. If genomic foundation model features merely replicate traditional polygenic score information with higher computational cost, the benefit is marginal. But if the embedding captures pathway-level structure that identifies this patient’s risk as concentrating in LDL metabolism pathways rather than inflammatory or thrombotic mechanisms, that information might strengthen the indication for statin therapy specifically. Attention-based attributions highlighting which genomic regions contribute most to the elevated risk could inform counseling about heritability and family screening.\nExternal validation across multiple health systems and ancestries would need to demonstrate that the foundation model approach provides calibrated predictions and meaningful reclassification improvement over traditional tools. Equity analysis would verify that performance holds across the diverse populations the health system serves rather than degrading for non-European ancestries underrepresented in training data.\n\n\n25.9.2 Oncology Prognosis\nA 64-year-old woman has undergone surgical resection for stage II colorectal cancer with microsatellite stable tumor characteristics. Her oncology team must decide whether adjuvant chemotherapy is warranted given the balance between recurrence risk reduction and treatment toxicity. Traditional staging provides prognostic information, but substantial heterogeneity exists within stage categories.\nFoundation models can enrich prognostic assessment through multiple channels. Tumor mutation profiles encoded through models like SetQuence or SetOmic produce embeddings capturing the specific constellation of somatic alterations beyond simple mutation counts (Jurenaite et al. 2024). Transcriptomic profiling integrated through GLUE-style latent spaces adds expression context reflecting tumor microenvironment and pathway activity (Cao and Gao 2022). Graph neural network-based subtyping assigns the tumor to a molecular subtype with characteristic prognosis and treatment response patterns (Li et al. 2022).\nThese tumor-level representations combine with germline pharmacogenomic features (variants affecting fluoropyrimidine metabolism that influence toxicity risk) and clinical features (performance status, comorbidities, patient preferences) in a survival model predicting two-year recurrence hazard. A high-risk prediction might favor more intensive adjuvant therapy, while low-risk predictions might support observation with close surveillance.\nThe validation requirements are stringent. Retrospective analysis of institutional cohorts establishes proof of concept, but prospective validation in cohorts receiving contemporary treatment regimens is necessary given the rapid evolution of oncology care. Interpretability connecting predictions to specific mutations, pathways, or molecular subtypes supports clinical adoption by providing rationale beyond a black-box hazard estimate.\n\n\n25.9.3 Pharmacogenomic Adverse Event Prediction\nA 45-year-old man with newly diagnosed epilepsy requires anticonvulsant therapy. Carbamazepine is a common first-line choice, but it carries risk of severe cutaneous adverse reactions including Stevens-Johnson syndrome and toxic epidermal necrolysis. The HLA-B*15:02 allele is strongly associated with carbamazepine hypersensitivity in patients of Asian ancestry, and FDA labeling recommends genetic testing before initiating therapy in at-risk populations.\nThis established pharmacogenomic association illustrates both the potential and limitations of current approaches. Single-variant associations with high effect sizes enable straightforward clinical implementation, but they cover a small fraction of drug-gene interactions. Many patients who do not carry HLA-B*15:02 still experience adverse reactions, suggesting additional genetic (and non-genetic) risk factors that single-variant testing misses.\nFoundation models could extend pharmacogenomic prediction beyond established single-gene associations. Variant effect scores across HLA genes, drug metabolism enzymes, and immune-related loci provide features reflecting the patient’s overall pharmacogenetic landscape. These features aggregate into a polygenic adverse event risk score that captures contributions from many variants rather than relying on individual high-effect alleles. Combined with clinical features (renal function affecting drug clearance, concomitant medications with interaction potential, prior adverse reaction history), the model predicts adverse event probability specific to the proposed drug.\nThe validation challenge is severe. Serious adverse drug reactions are rare, making endpoint ascertainment difficult and underpowered. Case-control designs enriched for adverse events may overestimate model performance compared to prospective deployment. Multi-site validation across healthcare systems with different prescribing patterns and population ancestry compositions is essential.\nClinical implementation requires integration at the point of prescribing, providing actionable information when drug selection decisions are being made. This argues for pre-computed pharmacogenomic profiles that alert at order entry rather than reactive testing after a prescription is written. The interpretability requirement is particularly acute: clinicians must understand why a model flags a patient as high-risk for a specific drug to make informed risk-benefit decisions.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch25-clinical-risk.html#translation-checklist",
    "href": "p6-ch25-clinical-risk.html#translation-checklist",
    "title": "25  Clinical Risk Prediction",
    "section": "25.10 Translation Checklist",
    "text": "25.10 Translation Checklist\nTeams translating foundation model-based risk tools into clinical practice navigate a complex landscape of technical, clinical, regulatory, and operational requirements. The following considerations distill this chapter’s themes into actionable guidance.\nProblem definition comes first. The outcome, time horizon, target population, and intended clinical action must be precisely specified before model development begins. A model designed to screen broadly differs fundamentally from one designed to guide treatment selection in high-risk patients. The clinical action the prediction will trigger should be concrete: initiate therapy, intensify surveillance, refer for specialist evaluation, or adjust dosing.\nEvidence generation spans the validation hierarchy. Internal validation establishes proof of concept but cannot support clinical deployment. External validation across institutions and ancestries assesses transportability and robustness. Prospective validation in deployment conditions (ideally with interventional designs measuring outcome impact) provides the strongest evidence for clinical benefit.\nEquity evaluation is not optional. Performance and calibration must be assessed across subgroups defined by ancestry, sex, age, and socioeconomic factors. Disparities identified during development should be addressed before deployment rather than documented and ignored. Access to the tool and downstream benefits should be monitored for equitable distribution.\nRegulatory and governance requirements depend on intended use. Clinical decision support tools may require FDA clearance or institutional governance review. Documentation including model cards, validation reports, and standard operating procedures should anticipate these requirements from project inception.\nClinical workflow integration determines real-world impact. Co-design with clinicians ensures the tool fits decision points and information needs. Alert fatigue from excessive notifications undermines adoption. Interpretability and uncertainty communication support appropriate trust calibration.\nPost-deployment monitoring detects drift and maintains performance. Input and output distributions, performance metrics, and incident reports should be tracked continuously. Clear triggers for investigation and potential retraining should be established. Governance structures should review models periodically and authorize updates or deprecation as needed.\nIf genomic foundation models are to realize their promise in clinical medicine, success will depend less on model scale and more on rigorous translation: careful problem selection, comprehensive evidence generation, stakeholder engagement, and vigilant post-deployment stewardship. The representational advances that foundation models provide become valuable only when they flow through validated, equitable, well-integrated clinical tools into decisions that improve patient outcomes.\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “[GPN] DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T. Higgins-Chen, Steve Horvath, and Bo Wang. 2024. “CpGPT: A Foundation Model for DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024. “[DeepRVAT] Integration of Variant Annotations Using Deep Set Networks Boosts Rare Variant Association Testing.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. “Delphi: A Deep-Learning Method for Polygenic Risk Prediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nJurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge, and René Jäkel. 2024. “SetQuence & SetOmic: Deep Set Transformers for Whole Genome and Exome Tumour Analysis.” BioSystems 235 (January): 105095. https://doi.org/10.1016/j.biosystems.2023.105095.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam, Amit R. Majithia, and Trey Ideker. 2025. “[G2PT] A Genotype-Phenotype Transformer to Assess and Explain Polygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and Yunping Zhu. 2022. “MoGCN: A Multi-Omics Integration Method Based on Graph Convolutional Network for Cancer Subtype Analysis.” Frontiers in Genetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "p6-ch26-rare-disease.html",
    "href": "p6-ch26-rare-disease.html",
    "title": "26  Rare Disease and Variant Interpretation",
    "section": "",
    "text": "26.1 The Variant Prioritization Funnel\nA four-year-old presents with developmental delay, hypotonia, and seizures that began at eighteen months. Standard metabolic testing reveals nothing. A gene panel for epilepsy returns negative. The neurologist orders whole-exome sequencing, which identifies 23,847 single nucleotide variants and 1,203 small insertions or deletions compared to the reference genome. Somewhere in this list of approximately 25,000 variants may lie the molecular explanation for this child’s condition. The clinical team must somehow reduce this number to a handful of candidates for expert review, ideally to a single variant or gene that explains the phenotype and guides management. This is the diagnostic odyssey: the gap between sequencing a genome and understanding what it means for a patient.\nRare diseases collectively affect approximately 300 million people worldwide, yet individually each condition may have only a handful of known cases. Over 7,000 rare diseases have been characterized, the majority following Mendelian inheritance patterns where single genes exert large effects. For these patients, identifying the causal variant can end years of uncertainty, enable accurate genetic counseling for families, and increasingly guide targeted therapies. The technical capacity to sequence genomes has advanced enormously; the interpretive bottleneck has not kept pace.\nFoundation models offer new tools for this interpretive challenge. As detailed in Chapter 14, models like AlphaMissense provide proteome-wide estimates of missense pathogenicity, while regulatory models like Enformer predict variant effects on gene expression across tissues. These computational predictions become one line of evidence within structured interpretation frameworks. This chapter examines how foundation model outputs integrate into clinical variant interpretation workflows, from initial prioritization through ACMG-AMP evidence classification, family-based analysis, and laboratory validation. The goal is not prediction for its own sake but actionable clinical insight: which variant explains this patient’s disease, and what should we do about it?\nClinical variant interpretation operates through progressive filtering, narrowing tens of thousands of candidates to a manageable set for expert review. Each filtering step applies different types of evidence, and foundation models contribute at multiple stages.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease and Variant Interpretation</span>"
    ]
  },
  {
    "objectID": "p6-ch26-rare-disease.html#the-variant-prioritization-funnel",
    "href": "p6-ch26-rare-disease.html#the-variant-prioritization-funnel",
    "title": "26  Rare Disease and Variant Interpretation",
    "section": "",
    "text": "26.1.1 Quality and Technical Filters\nThe first filter removes variants that are likely technical artifacts rather than true biological variation. Sequencing depth below 20x, strand bias exceeding established thresholds, and clustering of variants in repetitive regions all raise suspicion of false positives. Variant calling pipelines like GATK and DeepVariant (Chapter 1) produce quality scores that guide this initial triage. Variants failing quality thresholds are removed before any biological interpretation begins.\nFor trio analysis (proband plus both parents), Mendelian inheritance consistency provides an additional quality check. A variant called heterozygous in the child should appear in at least one parent unless it arose de novo. Widespread Mendelian inconsistencies indicate sample swaps, contamination, or systematic calling errors that must be resolved before interpretation proceeds.\n\n\n26.1.2 Population Frequency Filters\nVariants common in the general population are unlikely to cause rare, severe disease. If a variant appears in 1% of gnomAD individuals, it cannot plausibly explain a condition affecting one in 100,000 people under a dominant model. Frequency thresholds depend on inheritance mode and disease prevalence: dominant conditions with complete penetrance require extremely rare variants (often absent from population databases), while recessive conditions can tolerate higher carrier frequencies.\nThe gnomAD database provides allele frequencies across over 800,000 individuals from diverse ancestries. Applying a frequency threshold of 0.01% for dominant conditions and 1% for recessive carriers typically removes 95% or more of variants from consideration. Ancestry-matched frequencies matter: a variant rare in European populations may be common in African or East Asian populations, and global frequency alone can be misleading.\n\n\n26.1.3 Consequence and Gene Filters\nPredicted functional consequence shapes prioritization. Loss-of-function variants (frameshift, nonsense, canonical splice site) in genes intolerant to haploinsufficiency receive immediate attention. Missense variants require additional assessment, as most are benign. Intronic and intergenic variants have historically been deprioritized, though foundation models are beginning to identify functional noncoding variants with greater precision.\nGene-level filters incorporate prior knowledge. Curated gene panels for specific phenotypes (such as the PanelApp epilepsy panel or cardiomyopathy panel) restrict analysis to genes with established disease associations. For undiagnosed cases without clear phenotype match, broader approaches may include all OMIM disease genes or genes with high constraint (low observed/expected loss-of-function ratios in gnomAD).\n\n\n26.1.4 Foundation Model Scoring\nAfter quality, frequency, and consequence filters, foundation model predictions provide quantitative effect estimates for remaining candidates. For missense variants, AlphaMissense scores offer genome-wide pathogenicity estimates derived from protein structure and evolutionary conservation. For splice-region variants, SpliceAI predictions quantify the probability and magnitude of splicing disruption. For regulatory variants, Enformer and related models estimate effects on chromatin accessibility and gene expression in relevant tissues.\nThese scores do not directly translate to pathogenicity classifications. A high AlphaMissense score indicates that the protein change is likely functionally disruptive, not that it causes a specific disease. The clinical relevance of any functional disruption depends on the gene’s role in the patient’s phenotype, the inheritance pattern, and whether disruption of that gene produces the observed clinical features. Foundation model scores become one input to a structured evidence framework, not a standalone answer.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease and Variant Interpretation</span>"
    ]
  },
  {
    "objectID": "p6-ch26-rare-disease.html#acmg-amp-criteria-and-computational-evidence",
    "href": "p6-ch26-rare-disease.html#acmg-amp-criteria-and-computational-evidence",
    "title": "26  Rare Disease and Variant Interpretation",
    "section": "26.2 ACMG-AMP Criteria and Computational Evidence",
    "text": "26.2 ACMG-AMP Criteria and Computational Evidence\nThe American College of Medical Genetics and Genomics and Association for Molecular Pathology (ACMG-AMP) framework provides the dominant structure for clinical variant classification. Published in 2015 and subsequently refined through ClinGen expert panels, this framework assigns variants to five categories: pathogenic, likely pathogenic, variant of uncertain significance (VUS), likely benign, and benign. Classification emerges from combining multiple evidence types, each assigned a strength level (very strong, strong, moderate, supporting) and direction (pathogenic or benign).\n\n26.2.1 Evidence Categories\nACMG-AMP evidence spans several domains. Population data includes allele frequency in controls (BA1, BS1, BS2 for benign; PM2 for pathogenic support when absent). Computational predictions include in silico tools predicting deleterious effects (PP3 for pathogenic support) or benign effects (BP4 for benign support). Functional data includes well-established functional assays demonstrating deleterious (PS3) or no (BS3) effect. Segregation data addresses co-segregation with disease in multiple affected family members (PP1) or lack of segregation (BS4). De novo status assigns strong (PS2) or moderate (PM6) evidence when parental samples are available and the variant is absent in both parents. Clinical information incorporates specific phenotype match (PP4) and prevalence considerations.\nThe framework combines these evidence types through defined rules. Pathogenic classification requires either one very strong criterion plus one strong, or two strong criteria, with additional supporting evidence. Likely pathogenic requires somewhat less evidence. Most variants end up as VUS because available evidence is insufficient for confident classification in either direction.\n\n\n26.2.2 PP3 and BP4: Computational Evidence\nComputational predictions enter the ACMG-AMP framework primarily through PP3 (pathogenic supporting evidence from computational predictions) and BP4 (benign supporting evidence). These criteria apply when multiple in silico tools agree that a variant is deleterious (PP3) or benign (BP4).\nThe original 2015 guidelines assigned these criteria only “supporting” strength, reflecting appropriate caution about computational predictions available at the time. Tools like SIFT, PolyPhen-2, and CADD had limited accuracy and concerning circularity issues (Chapter 4). ClinGen sequence variant interpretation working groups have subsequently refined how computational evidence is weighted, in some cases upgrading to moderate strength for well-calibrated predictors in specific genes.\nFoundation models raise new questions about computational evidence strength. AlphaMissense achieves substantially higher accuracy than traditional tools on held-out ClinVar variants and deep mutational scanning data. Should predictions from these models receive greater evidentiary weight? The answer is not straightforward. Higher accuracy on aggregate benchmarks does not guarantee reliability for any individual prediction. Gene-specific calibration matters: a model may perform well across all genes but poorly for genes with unusual structure or function. And the fundamental limitation remains that computational predictions estimate functional impact, not clinical pathogenicity.\nResponsible application of foundation model predictions in ACMG-AMP classification requires gene-specific and variant-type-specific calibration whenever possible, explicit acknowledgment that PP3/BP4 evidence is supporting unless upgraded by expert panel guidance, use of multiple orthogonal predictors rather than reliance on any single model, and clear documentation of which tools were applied and how predictions were interpreted.\n\n\n26.2.3 Calibrating Predictions to Evidence Strength\nThe calibration problem is central to using foundation model predictions clinically. A model outputs a continuous score; clinical classification requires discrete evidence categories. How should thresholds be set, and what strength should be assigned?\nThe ClinGen Sequence Variant Interpretation Recommendations address this through the concept of odds of pathogenicity. Supporting evidence corresponds to an odds ratio of approximately 2 (twice as likely pathogenic as benign given this evidence). Moderate evidence corresponds to odds of approximately 4, and strong evidence to odds of approximately 18. For a computational predictor to warrant upgrading from supporting to moderate strength, its predictions should demonstrably achieve odds ratios meeting these thresholds in relevant validation datasets.\nFor AlphaMissense and similar foundation models, published validation shows that the highest-scoring variants (above 0.9) achieve odds ratios exceeding the strong evidence threshold in some gene contexts. ClinGen expert panels have begun incorporating these calibrations for specific genes, allowing upgraded evidence strength when predictions meet defined criteria. Clinicians should follow gene-specific expert panel recommendations when available rather than applying uniform thresholds across all genes.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease and Variant Interpretation</span>"
    ]
  },
  {
    "objectID": "p6-ch26-rare-disease.html#family-based-analysis",
    "href": "p6-ch26-rare-disease.html#family-based-analysis",
    "title": "26  Rare Disease and Variant Interpretation",
    "section": "26.3 Family-Based Analysis",
    "text": "26.3 Family-Based Analysis\nRare disease interpretation rarely relies on proband sequence alone. Family structure provides powerful additional information through inheritance pattern constraints, de novo status determination, and segregation analysis.\n\n26.3.1 De Novo Variants\nDe novo variants arise newly in the proband and are absent in both parents. For severe, early-onset dominant conditions, de novo mutations are expected: affected individuals rarely reproduce, so the disease-causing allele must arise fresh each generation. Observing a damaging variant as de novo provides strong evidence for pathogenicity under ACMG-AMP (PS2), often sufficient to push a candidate toward likely pathogenic or pathogenic classification.\nThe informativeness of de novo status depends on the mutation rate at that site and the expected de novo rate for the variant class. The human germline mutation rate is approximately 1 to 1.5 new mutations per 100 million base pairs per generation. For protein-coding exons (approximately 30 million base pairs), each individual carries roughly one new coding variant on average. Finding a damaging de novo variant in a candidate gene is therefore much more suspicious than finding an inherited variant of similar predicted effect.\nFoundation models assist de novo interpretation by providing effect estimates that help prioritize among multiple de novo variants (typical trio sequencing identifies one to three de novo coding variants) and by identifying de novo variants in noncoding regions that might disrupt critical regulatory elements. A de novo variant in a brain-specific enhancer upstream of a known epilepsy gene, predicted by Enformer to substantially reduce gene expression, warrants investigation even though traditional pipelines might overlook noncoding de novo events.\n\n\n26.3.2 Compound Heterozygosity and Phasing\nRecessive diseases require biallelic disruption: both copies of the gene must be affected for disease to manifest. When a proband carries two different heterozygous variants in the same gene, the critical question is whether these variants are in trans (on opposite chromosomes, leading to biallelic disruption) or in cis (on the same chromosome, leaving one copy functional).\nPhasing determines which configuration applies. Several approaches are available. Physical phasing through long-read sequencing directly observes which variants occur on the same DNA molecule, providing definitive phase information when reads span both variant positions. Trio phasing infers phase from parental genotypes: if one variant is inherited from the mother and one from the father, they must be in trans. Statistical phasing uses population haplotype patterns to estimate phase, though accuracy decreases for rare variants not well-represented in reference panels.\nFor clinical interpretation, trio phasing is often the most practical approach. If both variants are confirmed in trans and both are predicted damaging, this supports pathogenicity under a recessive model. If both variants were inherited from a single parent (in cis), the gene cannot explain a recessive phenotype unless a third variant exists.\nFoundation models contribute by estimating the functional severity of each variant. A missense variant with marginal AlphaMissense score might not warrant attention alone, but paired in trans with a clear loss-of-function variant, the compound heterozygous combination could produce sufficient functional disruption to cause disease.\n\n\n26.3.3 Segregation Analysis\nIn larger families with multiple affected and unaffected individuals, segregation analysis examines whether candidate variants track with disease status. Under a dominant model, all affected individuals should carry the variant, and penetrance assumptions constrain how many unaffected carriers are expected. Under a recessive model, affected individuals should be homozygous or compound heterozygous, carriers should be heterozygous, and unaffected non-carriers should lack the variant entirely.\nStrong segregation evidence (PP1, upgradable to strong evidence with sufficient meioses) can substantially support pathogenicity classification. Equally important, failure to segregate provides benign evidence (BS4): a variant present in unaffected family members at rates inconsistent with the proposed inheritance model is unlikely to be causal.\nSegregation analysis requires accurate pedigree information, confirmed sample identities, and careful consideration of age-dependent penetrance and phenocopies. A variant might be present in an unaffected young relative who will develop disease later, or an affected relative might have a different etiology (phenocopy). These complexities require clinical judgment that no computational model can replace.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease and Variant Interpretation</span>"
    ]
  },
  {
    "objectID": "p6-ch26-rare-disease.html#somatic-variant-interpretation-in-cancer",
    "href": "p6-ch26-rare-disease.html#somatic-variant-interpretation-in-cancer",
    "title": "26  Rare Disease and Variant Interpretation",
    "section": "26.4 Somatic Variant Interpretation in Cancer",
    "text": "26.4 Somatic Variant Interpretation in Cancer\nCancer genomics presents distinct interpretive challenges. Tumor genomes accumulate mutations throughout malignant evolution, creating a mix of driver mutations (those conferring selective advantage and contributing to cancer development) and passenger mutations (bystanders with no functional consequence). The interpretive task shifts from identifying variants causing inherited disease to identifying variants driving tumor biology and predicting therapeutic response.\n\n26.4.1 Germline versus Somatic Distinction\nCancer sequencing must distinguish germline variants (present in all cells, inherited or de novo) from somatic variants (acquired in the tumor lineage). Tumor-only sequencing cannot make this distinction reliably, as rare germline variants may be mistaken for somatic events. Paired tumor-normal sequencing, comparing tumor to a non-malignant sample from the same patient, enables confident somatic variant identification.\nThis distinction has direct clinical implications. A germline pathogenic variant in BRCA1 indicates hereditary cancer predisposition affecting the patient and potentially their family members, warranting genetic counseling and possibly risk-reducing interventions. A somatic BRCA1 mutation arose in the tumor and has no implications for inherited risk, though it may still predict response to PARP inhibitors.\n\n\n26.4.2 Driver Classification\nAmong somatic mutations, identifying drivers requires different evidence than germline pathogenicity assessment. Recurrence across independent tumors suggests selective advantage: if BRAF V600E appears in 50% of melanomas, this frequency far exceeds what chance would predict, implying functional importance. Databases like COSMIC catalog somatic mutation frequencies across cancer types, enabling recurrence-based prioritization.\nFunctional impact predictions from foundation models apply somewhat differently in the somatic context. A missense variant predicted highly damaging by AlphaMissense in a tumor suppressor gene suggests loss of function consistent with a driver role. The same prediction in an oncogene might indicate loss of normal regulation, potentially activating rather than inactivating the protein. Interpretation must consider the gene’s role (oncogene versus tumor suppressor) and the specific functional consequence of the variant.\nTumor mutational burden provides context for individual variant interpretation. Hypermutated tumors (from mismatch repair deficiency or POLE mutations) may carry thousands of coding mutations, making it difficult to identify drivers against this noisy background. In such cases, restricting attention to known hotspots, truncating mutations in tumor suppressors, and variants with strong functional predictions helps prioritize the likely relevant events.\n\n\n26.4.3 Therapeutic Biomarkers\nSomatic variant interpretation increasingly focuses on therapeutic implications. Specific variants predict response to targeted therapies: EGFR exon 19 deletions and L858R mutations predict erlotinib response in lung cancer; BRAF V600E predicts vemurafenib response in melanoma; PIK3CA mutations indicate alpelisib benefit in breast cancer. These associations derive from clinical trials demonstrating differential response by mutation status.\nFoundation models do not directly predict therapeutic response, as they lack the clinical outcome data that would be required. Their contribution is in characterizing novel variants in known therapeutic target genes. A patient whose tumor carries an unusual EGFR mutation not previously characterized might be evaluated using structural models and effect predictions to estimate whether the mutation likely preserves the drug-binding site and confers similar dependency as canonical sensitizing mutations. Such analyses are hypothesis-generating rather than definitive but can inform clinical decision-making when direct trial evidence is unavailable.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease and Variant Interpretation</span>"
    ]
  },
  {
    "objectID": "p6-ch26-rare-disease.html#laboratory-validation",
    "href": "p6-ch26-rare-disease.html#laboratory-validation",
    "title": "26  Rare Disease and Variant Interpretation",
    "section": "26.5 Laboratory Validation",
    "text": "26.5 Laboratory Validation\nComputational predictions, however accurate, remain predictions. Functional assays provide direct experimental evidence of variant effects, and ACMG-AMP appropriately weights functional data (PS3 for damaging functional effect, BS3 for no functional effect) as strong evidence when assays are well-validated.\n\n26.5.1 Types of Functional Assays\nDifferent variant types require different assay approaches. For missense variants, protein function assays measure specific biochemical activities of the mutant protein: enzyme activity, DNA binding, protein-protein interactions, or cellular phenotypes in model systems. Deep mutational scanning systematically characterizes all possible amino acid substitutions at each position in a protein, creating comprehensive functional maps. These maps enable immediate lookup of functional effects for any observed missense variant, though coverage remains incomplete across the proteome.\nFor splicing variants, minigene assays clone genomic regions containing the variant into expression vectors and measure splicing patterns in cultured cells. RNA sequencing from patient tissue (when accessible) directly observes whether aberrant splicing occurs in vivo. SpliceAI predictions can be validated by these direct measurements, establishing whether computational predictions match experimental reality for specific variants.\nFor regulatory variants, reporter assays measure whether variant-containing regulatory elements drive appropriate expression patterns. Massively parallel reporter assays (MPRAs) enable testing thousands of variants simultaneously, generating the training data that informs foundation model development while also providing direct validation for specific variants of clinical interest. CRISPR-based approaches can introduce variants into endogenous genomic contexts rather than artificial reporter constructs, providing more physiologically relevant readouts.\n\n\n26.5.2 Integrating Functional Evidence\nFunctional data enters ACMG-AMP classification through PS3 (strong pathogenic evidence from functional studies showing deleterious effect) and BS3 (strong benign evidence from functional studies showing no effect). The strength assignment depends on assay validation: well-established assays measuring physiologically relevant endpoints warrant strong evidence, while novel or less-validated assays may warrant only moderate or supporting strength.\nClinGen has developed detailed recommendations for functional evidence evaluation. The specific gene and disease mechanism should guide assay selection. Controls (known pathogenic and known benign variants) should be included to validate assay performance. The biological relevance of the assay endpoint to the disease mechanism must be justified. These requirements reflect appropriate caution: not all functional assays are equally informative, and inappropriate assays can mislead classification.\nFoundation model predictions can prioritize which variants most warrant functional follow-up. When resources limit testing to a subset of VUS, selecting those with discordant computational predictions (high predicted impact but uncertain clinical classification) maximizes the information gained. Variants where functional testing might resolve classification provide greater value than variants where classification is already clear or unlikely to change regardless of functional results.\n\n\n26.5.3 Closing the VUS Loop\nThe accumulation of variants of uncertain significance represents a major challenge in clinical genetics. Patients receive results that cannot be interpreted, creating anxiety and uncertainty. As more individuals undergo sequencing, VUS prevalence grows. Systematic efforts to resolve VUS through functional characterization could dramatically improve the clinical utility of genetic testing.\nHigh-throughput functional approaches offer a path forward. Saturation genome editing applies CRISPR to introduce every possible single-nucleotide variant at clinically important loci, then measures functional consequences through cellular phenotypes or growth selection. These experiments generate comprehensive functional maps that can immediately classify any observed variant. The Brotman Baty Institute’s ongoing efforts for BRCA1, mismatch repair genes, and other clinically important loci exemplify this approach.\nFoundation models trained on these functional datasets can generalize beyond directly measured variants, predicting effects for positions or genes not yet characterized experimentally. This creates a productive cycle: functional data improves model training, improved models identify high-priority variants for follow-up, and targeted experiments fill gaps while further improving models.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease and Variant Interpretation</span>"
    ]
  },
  {
    "objectID": "p6-ch26-rare-disease.html#practical-workflow-integration",
    "href": "p6-ch26-rare-disease.html#practical-workflow-integration",
    "title": "26  Rare Disease and Variant Interpretation",
    "section": "26.6 Practical Workflow Integration",
    "text": "26.6 Practical Workflow Integration\nTranslating foundation model capabilities into clinical practice requires integration with existing laboratory and clinical workflows. The technical and interpretive steps must fit within established regulatory frameworks, electronic health record systems, and clinical team structures.\n\n26.6.1 Laboratory Workflow\nClinical sequencing laboratories operate under regulatory oversight (CLIA certification, state licensure, and potentially CAP accreditation in the United States). Validated pipelines must produce consistent, reproducible results. Introducing new computational tools requires formal validation demonstrating that the tool performs as expected on representative sample types, that outputs are interpretable and actionable by clinical staff, and that results are documented and traceable.\nFor foundation model integration, validation studies should assess performance on variants with known clinical classifications, compare predictions to existing tools to understand concordance and discordance, evaluate performance across variant types (missense, splice, regulatory) and gene categories, and document threshold selection and evidence strength assignment.\nLaboratory information management systems must capture foundation model predictions alongside other variant annotations. Reports to clinicians should clearly indicate the role of computational evidence, the specific tools applied, and the evidence strength assigned. Overreliance on computational predictions, or failure to communicate their limitations, risks inappropriate clinical decisions.\n\n\n26.6.2 Clinical Decision-Making\nVariant interpretation reports ultimately inform clinical decisions: whether to pursue additional testing, what genetic counseling to provide, whether to adjust medical management, and what surveillance or prevention strategies to recommend. These decisions rest with clinicians and genetic counselors working with patients, not with computational algorithms.\nFoundation model predictions support this process by improving the efficiency and accuracy of variant prioritization, reducing the number of VUS through more informative computational evidence, identifying potentially actionable variants in previously overlooked genomic regions, and enabling rapid assessment of novel variants not previously observed.\nThe interpretive report should convey both what computational predictions indicate and the uncertainty that remains. Clinicians must understand that even highly accurate models make errors, that predictions may be less reliable for underrepresented populations or unusual variant types, and that computational evidence is one component of a comprehensive assessment. Shared decision-making with patients should acknowledge these limitations while conveying the best current understanding.\n\n\n26.6.3 Regulatory and Ethical Considerations\nClinical use of foundation model predictions raises regulatory questions addressed more fully in Chapter 29. In the United States, laboratory-developed tests using computational predictions fall under CLIA oversight, with additional FDA jurisdiction increasingly asserted for software as a medical device. European regulations under IVDR impose their own requirements. Laboratories must navigate this evolving landscape while ensuring that clinical utility keeps pace with regulatory compliance.\nEquity concerns deserve particular attention. Foundation models trained predominantly on data from individuals of European ancestry may perform less well for other populations. If computational predictions systematically provide less informative evidence for underrepresented groups, this could widen existing disparities in diagnostic yield and clinical care. Ongoing efforts to diversify training data and evaluate performance across ancestries are essential for equitable clinical deployment.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease and Variant Interpretation</span>"
    ]
  },
  {
    "objectID": "p6-ch26-rare-disease.html#the-interpretive-partnership",
    "href": "p6-ch26-rare-disease.html#the-interpretive-partnership",
    "title": "26  Rare Disease and Variant Interpretation",
    "section": "26.7 The Interpretive Partnership",
    "text": "26.7 The Interpretive Partnership\nFoundation models transform variant interpretation by providing more accurate, comprehensive, and fine-grained predictions than previous computational approaches. Missense pathogenicity can be estimated proteome-wide with unprecedented accuracy. Regulatory variant effects can be predicted across tissues and cell types. Splicing disruption can be quantified with clinical-grade precision. These capabilities accelerate the diagnostic odyssey, enabling faster and more confident resolution for patients with rare diseases.\nYet foundation models do not replace human judgment in clinical genetics. They do not understand phenotypes, families, or therapeutic implications. They do not weigh the psychological impact of uncertain results or navigate the ethical complexities of predictive testing. They provide evidence that must be integrated within clinical frameworks designed around human decision-making.\nThe most productive framing positions foundation models as partners in interpretation: computational systems that handle pattern recognition at scales beyond human capacity, freeing clinical experts to focus on integration, communication, and decision-making where human judgment remains essential. This partnership model, rather than replacement or autonomy, defines the productive path forward for genomic foundation models in rare disease diagnosis.\nThe next chapter extends these interpretive principles to therapeutic discovery, examining how foundation models identify drug targets, predict biomarkers, and guide precision medicine approaches beyond diagnosis (Chapter 27).",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease and Variant Interpretation</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html",
    "href": "p6-ch27-drug-discovery.html",
    "title": "27  Drug Discovery and Target Identification",
    "section": "",
    "text": "27.1 The Genetic Foundation of Target Selection\nDrug discovery confronts a paradox of scale. The human genome contains roughly 20,000 protein-coding genes, yet fewer than 700 have been successfully targeted by approved drugs. Genome-wide association studies have identified thousands of disease-associated loci, but translating a statistical association into a validated therapeutic target typically requires a decade of work and hundreds of millions of dollars. The attrition rate remains brutal: over 90% of drug candidates that enter clinical trials fail, often because they targeted the wrong gene or because the patient population was too heterogeneous for a single mechanism to succeed.\nGenomic foundation models offer a path through this combinatorial explosion. Rather than treating each target identification program as a de novo effort, foundation models encode biological knowledge learned across millions of sequences, regulatory architectures, and perturbation responses into reusable representations. A variant effect predictor trained on evolutionary conservation and protein structure can score any missense mutation without task-specific retraining. A regulatory model that learned enhancer-promoter grammar from thousands of cell types can predict expression consequences of noncoding variants across tissues it never explicitly trained on. The core insight is that drug discovery programs share underlying biology, and foundation models capture that shared structure in ways that hand-crafted features cannot.\nThis chapter examines how genomic foundation models connect to the drug discovery pipeline. The focus is on four broad applications: target discovery and genetic validation, where human genetics and variant-level scores prioritize safer, more effective targets; network-aware approaches that propagate genetic signals through protein and regulatory networks to identify modules and repurposing opportunities; functional genomics screens that leverage foundation models to design, interpret, and iteratively improve large-scale perturbation experiments; and biomarker development, where model outputs inform patient stratification and trial design. Throughout, the aim is not to promise end-to-end AI drug discovery, but to show pragmatic ways that genomic foundation models can reduce risk, prioritize hypotheses, and make experiments more informative when coupled to high-quality human data.\nHuman genetics provides uniquely causal evidence for target selection. Unlike expression correlations or pathway membership, genetic associations reflect the consequences of lifelong modulation of gene activity in human populations. Multiple analyses over the past decade have demonstrated that genetically supported targets succeed in clinical trials at roughly twice the rate of targets without genetic evidence. Targets implicated by Mendelian disease genetics, GWAS hits, or functional variants show higher probabilities of success in phase II and III trials compared to targets selected through other means.\nThis empirical observation motivates building pipelines where genetic architecture serves as a first-class input to target identification. Genomic foundation models extend this logic in two directions. First, they provide richer biological context: instead of simple “variants near gene X,” foundation models encode regulatory architecture, chromatin state, three-dimensional genome interactions, cell-type specificity, and perturbation responses. Second, they enable transfer across diseases and modalities: a single model trained on diverse genomic and multi-omic data can be reused for multiple diseases and therapeutic areas, analogous to how language models transfer across domains.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#the-genetic-foundation-of-target-selection",
    "href": "p6-ch27-drug-discovery.html#the-genetic-foundation-of-target-selection",
    "title": "27  Drug Discovery and Target Identification",
    "section": "",
    "text": "27.1.1 From Variant-Level Predictions to Gene-Level Evidence\nDrug discovery teams rarely care about individual variants per se; they care about genes and pathways. The fundamental challenge in target identification is therefore aggregating variant-level information into gene-level evidence that can guide target selection.\nConsider a typical workflow. Starting from GWAS summary statistics, statistical fine-mapping methods identify credible sets of potentially causal variants at each locus. Sequence-based foundation models then score each candidate variant for regulatory or coding impact. Protein-centric variant effect predictors such as AlphaMissense, GPN-MSA, and the missense components of AlphaGenome combine protein language models, structural information, and evolutionary conservation to assess coding variants (Cheng et al. 2023; Benegas et al. 2024; Z. Avsec, Latysheva, and Cheng 2025; Brandes et al. 2023). Regulatory foundation models including Enformer, Borzoi, and long-context DNA language models predict the consequences of noncoding variants on chromatin accessibility, transcription factor binding, and gene expression (Ž. Avsec et al. 2021; Linder et al. 2025; Dalla-Torre et al. 2023; Nguyen et al. 2023).\nThe critical step is connecting variants to genes. For coding variants, this mapping is straightforward: the variant lies within a gene’s coding sequence, and protein-level scores directly inform that gene’s candidacy. For noncoding variants, the mapping requires integrating chromatin conformation data (Hi-C, promoter-capture Hi-C), enhancer-gene predictions from models like Enformer, and expression quantitative trait locus data that empirically links variants to gene expression changes. Fine-mapping approaches such as MIFM can help distinguish truly causal regulatory variants from correlated passengers, tightening the map from GWAS locus to variant to target gene (Wu et al. 2024; Rakowski and Lippert 2025).\nGene-level aggregation proceeds by summarizing variant effects across all variants linked to each gene. For a given gene, this summary might include the burden of predicted loss-of-function variants in cases versus controls, the strongest regulatory variant effect sizes predicted by foundation models, constraint metrics indicating the gene’s intolerance to damaging variation, and pleiotropy scores reflecting associations with other traits that might indicate safety liabilities or broader biological importance. From a foundation model perspective, the core idea is to treat gene-level evidence as an aggregation problem over high-dimensional variant embeddings. Rather than manually defining a handful of summary statistics, variant embeddings and predicted functional profiles can feed into downstream models that learn which patterns matter most for disease.\n\n\n27.1.2 Linking Genetics to Target Safety and Efficacy\nClassical human genetics has established several heuristics for target selection that foundation models can reinforce and extend. Human knockout individuals, people carrying biallelic loss-of-function variants, provide natural experiments on the consequences of gene inactivation. Protective variants that reduce disease risk suggest the directionality of therapeutic intervention: partial inhibition of a protein may be beneficial rather than harmful. Pleiotropy, meaning associations with many unrelated traits, may signal safety liabilities if modulating a target affects multiple physiological systems.\nFoundation models sharpen these assessments. Fine-mapping methods combined with regulatory foundation models can distinguish causal variants from those merely in linkage disequilibrium with causal variants. Variant effect scores from protein and regulatory models approximate effect sizes, helping differentiate subtle modulators from catastrophic loss-of-function mutations. Multi-task predictions across chromatin marks, transcription factor binding, expression, and splicing provide mechanistic hypotheses for how risk loci affect biology, moving beyond statistical association toward functional understanding.\nThe output of this workflow is a ranked list of candidate targets with structured evidence that can be compared across diseases and programs. Each target comes annotated with the strength of genetic evidence (effect sizes, fine-mapping probabilities), predicted mechanisms (coding versus regulatory, affected tissues), constraint information (tolerance to loss-of-function, essentiality), and druggability features (protein family, structural information, existing ligands).",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#network-aware-target-discovery-and-repurposing",
    "href": "p6-ch27-drug-discovery.html#network-aware-target-discovery-and-repurposing",
    "title": "27  Drug Discovery and Target Identification",
    "section": "27.2 Network-Aware Target Discovery and Repurposing",
    "text": "27.2 Network-Aware Target Discovery and Repurposing\nIndividual genes do not operate in isolation. Proteins interact in complexes, genes participate in pathways, and regulatory networks coordinate cellular responses. Even with excellent variant-to-gene mapping, the biological context of a target shapes its therapeutic potential. Network-aware approaches propagate genetic signals through these relational structures to identify modules, bottleneck nodes, and repurposing opportunities.\n\n27.2.1 Propagating Genetic Signals Through Networks\nThe basic intuition is that GWAS signals concentrated in a pathway or protein interaction module provide stronger evidence than isolated hits. A single gene with modest genetic support but tight functional connections to several strongly implicated genes may be a more attractive target than an isolated hit with stronger statistics but unclear biology.\nNetwork-based methods integrate noncoding GWAS loci, regulatory annotations, and protein-protein interactomes to identify disease genes and evaluate drug repurposing opportunities in complex diseases. Graph neural network architectures (see Chapter 19) can learn to propagate genetic evidence through interaction networks, scoring each gene not just by its direct genetic association but by its network context. The key methodological insight is that genes can be embedded jointly with their network neighbors, allowing the model to capture how genetic perturbations in one gene might affect functionally related genes.\nFoundation model representations enhance these network approaches. Instead of representing each gene by a sparse vector of annotations, genes can be embedded using features derived from protein language models, regulatory foundation models, and expression-based cell state encoders. These rich representations capture functional similarity beyond what interaction databases alone can provide. Two genes with similar protein language model embeddings likely share functional properties even if no direct interaction has been catalogued.\n\n\n27.2.2 Drug Repurposing Through Shared Representations\nThe same framework enables systematic drug repurposing. By representing drugs via their targets, gene expression signatures, and phenotypic effects, and representing diseases via their genetic architecture and molecular signatures, models can score drug-disease pairs based on representation similarity. If a drug’s target sits near genetically implicated genes in representation space, or if the drug’s expression signature opposes the disease signature, that drug becomes a repurposing candidate.\nNetwork proximity provides one concrete operationalization: drugs whose targets are enriched near disease-risk genes, as measured by network diffusion or embedding similarity, may have therapeutic potential for that disease. Several retrospective analyses have found that such proximity predicts reduced disease incidence among users of particular drugs, though prospective validation remains limited.\nThe caution here is fundamental: representation similarity is not causation. A drug that appears near disease genes in embedding space might act through that mechanism, or the association might reflect confounding by indication, survivorship bias, or other artifacts of observational data. Network-based repurposing generates hypotheses; Mendelian randomization, natural experiments, and prospective trials must test them.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#drug-target-interaction-prediction",
    "href": "p6-ch27-drug-discovery.html#drug-target-interaction-prediction",
    "title": "27  Drug Discovery and Target Identification",
    "section": "27.3 Drug-Target Interaction Prediction",
    "text": "27.3 Drug-Target Interaction Prediction\nBeyond identifying disease-relevant targets, foundation models can predict which molecules might modulate those targets. Drug-target interaction prediction sits at the interface between genomic and chemical foundation models, using biological representations to inform molecular design decisions.\n\n27.3.1 Representing Targets for Binding Prediction\nTraditional drug-target interaction methods rely on sequence similarity, structural docking, or chemical fingerprint matching. Foundation model approaches replace these hand-crafted features with learned representations. Protein language model embeddings from ESM-2 or similar architectures capture evolutionary and structural information that correlates with binding site properties (lin_evolutionary-scale_2023?). Ligand representations from chemical foundation models encode molecular properties relevant to binding affinity and selectivity.\nThe prediction task becomes: given a protein embedding (derived from a protein language model) and a molecule embedding (derived from a chemical language model or graph neural network), predict binding affinity or interaction probability. These models can be trained on large databases of known drug-target interactions and binding affinities, then applied to predict interactions for novel targets or molecules.\nFor genomics-focused applications, the protein representation is the critical contribution. A target identified through genetic validation can be immediately embedded using protein foundation models, enabling binding prediction without waiting for experimental structures or extensive biochemical characterization. This acceleration is particularly valuable for understudied targets where structural data is sparse.\n\n\n27.3.2 Selectivity and Off-Target Prediction\nThe same framework extends to selectivity prediction. By comparing a drug’s predicted binding across all proteins in a proteome-scale embedding space, models can flag potential off-target interactions. A compound predicted to bind its intended target but also showing high affinity for kinases with cardiovascular expression, for example, might warrant additional safety characterization before advancement.\nFoundation model representations capture protein family relationships and binding site similarities that inform off-target predictions. Two proteins with similar embeddings likely share structural features that could bind similar molecules. This information, combined with tissue expression data and phenome-wide association data (linking genes to thousands of traits), enables preliminary safety profiling before expensive preclinical experiments.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#toxicity-prediction-from-genomic-context",
    "href": "p6-ch27-drug-discovery.html#toxicity-prediction-from-genomic-context",
    "title": "27  Drug Discovery and Target Identification",
    "section": "27.4 Toxicity Prediction from Genomic Context",
    "text": "27.4 Toxicity Prediction from Genomic Context\nSafety failures represent a major cause of drug attrition, particularly in late-stage development where failures are most expensive. Genomic information provides several routes to earlier toxicity prediction.\n\n27.4.1 Genetic Evidence of Target Liabilities\nHuman genetic data offers direct evidence of target-related toxicity. If loss-of-function variants in a target gene associate with adverse phenotypes in biobank populations, those phenotypes may emerge as on-target toxicities during therapeutic inhibition. Phenome-wide association studies across biobanks link genes to thousands of traits, from laboratory values to disease diagnoses to imaging features. A target strongly associated with QT prolongation, hepatotoxicity markers, or nephrotoxicity phenotypes warrants careful safety evaluation.\nFoundation models enhance this analysis by providing more accurate variant effect predictions (distinguishing true loss-of-function from benign variants) and by integrating across evidence types. A gene might show modest individual associations with several safety-relevant traits that, when aggregated using foundation model representations, reveal a concerning pattern.\n\n\n27.4.2 Expression-Based Toxicity Prediction\nTissue expression patterns inform toxicity risk. A target expressed highly in hepatocytes poses greater hepatotoxicity risk than one expressed primarily in the target tissue. Single-cell foundation models (see Chapter 16) provide cell-type-resolved expression information, enabling predictions about which cell types might be affected by target modulation.\nMore sophisticated approaches use perturbation-response models trained on CRISPR screens and drug treatment data. Given a target knockdown or drug treatment, these models predict transcriptomic responses across cell types. If the predicted response signature resembles known toxicity signatures (mitochondrial stress, DNA damage response, inflammatory activation), that prediction informs safety risk assessment.\n\n\n27.4.3 Integrating Genomic Context with Chemical Properties\nUltimate toxicity prediction requires integrating target information with compound properties. The same molecule might be safe or toxic depending on its selectivity profile, metabolism, and tissue distribution. Foundation models provide the biological context (target properties, off-target predictions, expression patterns) that complements chemical property predictions (metabolism, reactivity, distribution) in integrated toxicity models.\nThe field remains early: prospective validation of foundation model toxicity predictions against clinical outcomes is limited. Current utility lies in prioritizing compounds for experimental toxicity testing and in generating hypotheses about liability mechanisms, rather than replacing traditional safety pharmacology.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#functional-genomics-screens-and-perturbation-models",
    "href": "p6-ch27-drug-discovery.html#functional-genomics-screens-and-perturbation-models",
    "title": "27  Drug Discovery and Target Identification",
    "section": "27.5 Functional Genomics Screens and Perturbation Models",
    "text": "27.5 Functional Genomics Screens and Perturbation Models\nWhile human genetics offers observational evidence, drug discovery relies heavily on perturbation experiments that directly test hypotheses. CRISPR knockout and knockdown screens, saturation mutagenesis of protein domains, massively parallel reporter assays (MPRAs) of regulatory elements, and Perturb-seq experiments linking genetic perturbations to single-cell transcriptomic responses all generate data that both validates targets and improves models.\n\n27.5.1 Designing Informative Perturbation Libraries\nTraditional pooled screens use simple design rules: one guide RNA per exon, or tiling a regulatory region at fixed spacing. Foundation models enable smarter library design by providing priors over which perturbations are likely to be informative.\nVariant effect scores from protein foundation models can prioritize which amino acid positions are most likely to reveal functional differences when mutated. Positions predicted to be highly constrained and structurally important warrant more thorough coverage than positions predicted to be mutationally tolerant. Regulatory foundation models can highlight which enhancer or promoter regions are predicted to have the largest expression effects in the cell type of interest, focusing screening effort on high-impact regions.\nBeyond prioritization, foundation models can guide combinatorial design. Model uncertainty, the degree to which a model is confident in its predictions, identifies regions where experimental data would be most informative. Positions where the model makes uncertain predictions are precisely those where experimental measurement adds the most value. Active learning strategies that select perturbations to maximize expected information gain can dramatically improve the efficiency of screening campaigns.\n\n\n27.5.2 Perturb-seq and Transcriptomic Readouts\nPerturb-seq experiments combine pooled genetic screens with single-cell RNA sequencing, linking each perturbation to its transcriptomic consequences. These data are exceptionally rich: rather than a single phenotypic readout (viability, fluorescence), each cell provides a high-dimensional expression profile reflecting how the perturbation affected cellular state.\nFoundation models trained on Perturb-seq data learn to predict transcriptomic responses to genetic perturbations. Given a gene knockdown, these models predict which other genes will be up- or down-regulated, providing a functional signature for each target. Similar signatures suggest similar biology; divergent signatures suggest distinct mechanisms even for targets in the same pathway.\nThe drug discovery application is perturbation matching. Given a disease state characterized by a transcriptomic signature (perhaps derived from patient samples or disease models), foundation models can identify perturbations whose predicted response signature would move the system toward a healthier state. If knocking down gene X reverses the disease signature, X becomes a candidate therapeutic target. If treating with drug Y produces a signature similar to knocking down gene X, Y becomes a candidate molecule for that mechanism.\n\n\n27.5.3 Closing the Loop: Lab-in-the-Loop Refinement\nPerhaps the most powerful application of foundation models in functional genomics is iterative refinement. Screen outcomes provide labeled examples that can fine-tune sequence-to-function models for the specific biological context of interest.\nConsider an MPRA that assays thousands of enhancer variants for their effects on reporter gene expression in a disease-relevant cell type. These sequence-activity pairs directly supervise expression-prediction foundation models, dramatically improving their accuracy for that locus and tissue. The refined model then makes better predictions for the next round of experiments, suggesting which additional variants would be most informative to test.\nThis lab-in-the-loop cycle accelerates discovery while improving model accuracy in disease-relevant regions of sequence space. Foundation models provide the prior (general knowledge about sequence-function relationships); experiments provide the likelihood (specific measurements in the system of interest); and the posterior (updated model) makes better predictions for subsequent experiments.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#biomarker-development-and-patient-stratification",
    "href": "p6-ch27-drug-discovery.html#biomarker-development-and-patient-stratification",
    "title": "27  Drug Discovery and Target Identification",
    "section": "27.6 Biomarker Development and Patient Stratification",
    "text": "27.6 Biomarker Development and Patient Stratification\nEven when a target is well validated, many programs fail in clinical trials because the right patients were not enrolled, the right endpoints were not measured, or the treatment effect was diluted across a heterogeneous population. Foundation model representations provide new tools for defining and validating biomarkers.\n\n27.6.1 From Polygenic Scores to Foundation Model Features\nClassical polygenic scores summarize additive effects of common variants on disease risk. These scores have proven useful for patient enrichment in cardiovascular and metabolic disease trials, selecting patients at highest genetic risk who might benefit most from intervention. Deep learning methods extend this approach by learning nonlinear genotype-phenotype mappings that capture interactions and nonadditive effects.\nFoundation models enhance polygenic prediction in several ways. Instead of using raw genotypes as inputs, models can use variant effect scores, regulatory predictions, or gene-level embeddings derived from foundation models. This captures biological context that simple genotypes miss. Models trained on variant embeddings rather than binary genotype calls can capture subtle differences between variants at the same position, distinguishing a mildly damaging missense from a severely damaging one even when both are heterozygous.\nTransfer across populations represents a particular strength. Foundation models trained on diverse genomes provide representations that may generalize more robustly across ancestries than models trained on individual cohorts. Fine-mapping-aware approaches that use foundation model features can reduce dependence on linkage disequilibrium patterns that vary across populations, potentially improving the portability of genetic risk predictors.\n\n\n27.6.2 Multi-Omic Biomarker Discovery\nBeyond germline genetics, drug development increasingly leverages somatic genomics, transcriptomics, proteomics, and other molecular readouts. Tumor sequencing combined with expression profiling characterizes the molecular landscape of each patient’s cancer. Single-cell multiome data (RNA plus ATAC) reveal cell-state heterogeneity that bulk assays miss.\nFoundation models trained on these data types provide embeddings that capture patient-level molecular profiles. Set-based architectures that treat each patient’s genomic features as a set (rather than assuming fixed feature positions) can handle the heterogeneity of tumor genomes, where different patients have different mutations. Gene regulatory network inference models trained on atlas-scale single-cell data can extract pathway activity scores that serve as mechanistically interpretable biomarkers.\nThe key shift is that biomarkers are no longer limited to a handful of hand-picked variants or expression markers. They become functions over high-dimensional genomic and multi-omic embeddings, learned in a data-driven way yet grounded in biological priors from foundation models. A biomarker might be a region of embedding space corresponding to patients with particular molecular subtypes, defined by the model rather than by manual curation.\n\n\n27.6.3 Trial Design and Endpoint Selection\nFoundation model predictions inform trial design at multiple stages. Patient enrichment uses genetic risk scores or molecular subtypes to select patients most likely to respond, increasing statistical power and reducing required sample sizes. Adaptive designs use intermediate biomarker responses to modify randomization or dosing during the trial. Endpoint selection uses molecular signatures to define pharmacodynamic biomarkers that indicate target engagement, supporting dose selection and early efficacy signals.\nRegulatory agencies increasingly accept genomic biomarkers for patient selection in oncology and are developing frameworks for other therapeutic areas. The challenge is validation: demonstrating that foundation model predictions actually stratify patient outcomes requires prospective trials or well-designed retrospective analyses with appropriate controls for confounding.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#industry-workflows-and-infrastructure",
    "href": "p6-ch27-drug-discovery.html#industry-workflows-and-infrastructure",
    "title": "27  Drug Discovery and Target Identification",
    "section": "27.7 Industry Workflows and Infrastructure",
    "text": "27.7 Industry Workflows and Infrastructure\nFor pharmaceutical and biotechnology organizations, the challenge is not whether they can access a foundation model but how to integrate these models into existing data platforms, governance structures, and decision-making processes.\n\n27.7.1 Building Model Infrastructure\nIn mature organizations, foundation models should be treated as shared infrastructure rather than ad hoc scripts developed by individual project teams. A well-organized model catalog contains DNA language models (Nucleotide Transformer, HyenaDNA, GENA-LM), sequence-to-function models (Enformer, Borzoi), and variant effect predictors (AlphaMissense, GPN-MSA, CADD) with documented capabilities, limitations, and appropriate use cases (Dalla-Torre et al. 2023; Nguyen et al. 2023; Fishman et al. 2025; Ž. Avsec et al. 2021; Linder et al. 2025; Cheng et al. 2023; Benegas et al. 2024; Schubach et al. 2024).\nFeature services provide centralized APIs that accept variants, genomic intervals, or genes as input and return embeddings, predicted functional profiles, or risk features. Centralization enables consistency across programs and avoids redundant computation. Logging and versioning ensure that analyses can be reproduced even as models and data evolve.\nData governance maintains clear separation between models trained on public data versus sensitive internal cohorts. Internal data, including proprietary clinical trial data, patient samples, and collaborator contributions, requires careful handling. Guardrails define where internal data can be used for fine-tuning and how resulting models can be shared or published.\n\n\n27.7.2 Strategic Choices: Build, Buy, or Fine-Tune\nOrganizations face three strategic options when adopting foundation models. Using external models as-is offers low upfront cost and benefits from community benchmarking, but may not capture organization-specific populations, assays, or therapeutic areas. A model trained primarily on European ancestry populations may perform poorly on a company’s Asian-focused programs; a model trained on common cell lines may miss biology relevant to rare disease indications.\nFine-tuning open-source foundation models on internal data retains powerful general representations while adapting to local data distributions. This approach requires computational investment and careful privacy controls, but often provides the best balance of generality and specificity. A company with large internal biobank data can fine-tune a general variant effect predictor on that cohort, improving predictions for its patient populations without sacrificing the broad knowledge captured during pretraining.\nTraining bespoke internal models from scratch offers maximum control and allows alignment of pretraining objectives with specific use cases. A company focused on rare diseases might pretrain on sequences and phenotypes particularly relevant to that space. The cost is substantial: pretraining requires significant compute, data engineering, and machine learning expertise. There is also risk of overfitting to narrow internal datasets if the pretraining corpus is not sufficiently diverse.\nIn practice, most organizations adopt hybrid strategies. They start with public foundation models for early exploration and non-sensitive applications, gradually fine-tune on internal data as value becomes clear, and reserve from-scratch training for cases where unique data assets justify the investment. Lightweight model-serving infrastructure handles latency-sensitive applications such as clinical decision support, while heavier offline systems support large-scale research workloads.\n\n\n27.7.3 Industry Context: Timelines and Decision Gates\nAcademic machine learning research optimizes benchmark performance; drug discovery optimizes probability of clinical and commercial success under time and resource constraints. Understanding industry context helps foundation model practitioners contribute effectively.\nDrug discovery programs progress through gates: target validation, candidate selection, investigational new drug filing, and clinical trial phases. Each gate requires specific evidence: biological rationale, efficacy data, safety data, manufacturing feasibility. Foundation model contributions must align with gate requirements. A beautiful embedding space is valueless if it cannot be translated into evidence that advances a program through its next gate.\nTimelines matter. A prediction that takes six months to validate experimentally may be worthless if the program decision must be made in three months. Foundation models that enable faster experiments (through better library design, prioritization, or interpretation) create more value than models that provide incrementally better predictions but require the same experimental timeline to validate.\nBiotechnology companies and pharmaceutical companies operate differently. Biotechs often focus on single programs with limited resources, prioritizing speed and risk-taking. Pharma companies manage portfolios across therapeutic areas, prioritizing consistency and scalability. Foundation model infrastructure that serves one context may not serve the other. A boutique biotech might prefer lightweight, single-purpose models; a large pharma might invest in comprehensive infrastructure serving dozens of programs.\n\n\n27.7.4 Intellectual Property and Data Considerations\nFoundation models raise new questions around intellectual property, data sharing, and regulatory expectations that organizations must navigate.\nModels trained on proprietary data can be valuable assets but are difficult to patent directly. The model architecture is typically based on published methods; the weights reflect training data that may include public and proprietary components. Downstream discoveries, including specific targets, biomarkers, and therapeutic hypotheses derived from foundation model analyses, are more clearly protectable but require careful documentation of inventive contribution.\nCollaborative model development across institutions may require federated learning or model-to-data paradigms, especially for patient-level data. Genomic data carries re-identification risk; sharing raw data, even for model training, requires appropriate consent and data use agreements. Federated approaches that train on local data without centralizing raw information can enable multi-institutional collaboration while respecting privacy constraints.\nFor regulatory submissions, foundation models used in drug development create documentation requirements. If a model informed target selection, patient stratification, or safety assessment, regulators may request information about model training, validation, and performance across subgroups. The confounding and interpretability challenges discussed in Chapter 22 and Chapter 24 become acute when models inform pivotal decisions in drug development. Clear documentation trails from model prediction to program decision support regulatory review.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#evaluation-and-validation",
    "href": "p6-ch27-drug-discovery.html#evaluation-and-validation",
    "title": "27  Drug Discovery and Target Identification",
    "section": "27.8 Evaluation and Validation",
    "text": "27.8 Evaluation and Validation\nEvaluating foundation model contributions to drug discovery requires carefully separating model performance from scientific and clinical validity. A model that achieves high benchmark scores may still fail to improve drug discovery outcomes; a model with modest benchmarks may provide actionable insights that advance programs.\n\n27.8.1 Benchmark Limitations\nMany published benchmarks draw targets and drugs from the same databases used to pretrain models, creating risk of leakage that inflates performance estimates. Repurposing success stories often rely on retrospective data mining with limited prospective validation. The ultimate test of a foundation model for drug discovery is whether it identifies targets that succeed in clinical trials, a test that takes years and confounds model contribution with countless other factors.\nConfounding pervades drug discovery data. Models trained on observational clinical and genomic data inherit confounders from those data. Drug-disease associations learned by foundation models may reflect treatment patterns rather than true causal relationships. Confounding by indication (sicker patients receive different treatments), survivorship bias (only patients who survived long enough enter certain analyses), and healthcare access patterns all threaten validity. Genetic instruments and careful epidemiologic designs remain essential for causal claims that foundation model predictions cannot provide alone.\n\n\n27.8.2 From Prediction to Validation\nFoundation model predictions are hypotheses, not conclusions. A target ranked highly by genetic evidence and foundation model scoring still requires experimental validation. The value of foundation models lies in prioritizing which hypotheses to test, not in replacing experimental testing.\nValidation strategies depend on the application. Target predictions can be validated through functional genomics screens that test whether predicted targets affect disease-relevant phenotypes. Biomarker predictions require retrospective validation on held-out cohorts or prospective validation in clinical trials. Repurposing predictions require real-world evidence analyses or prospective trials.\nThe timeline for validation matters. Some predictions can be tested in weeks (cell-based assays for target validation); others require years (clinical outcomes for biomarkers). Foundation model contributions should be assessed on timescales relevant to drug discovery decisions, not just immediate benchmark performance.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#connections-to-molecular-design",
    "href": "p6-ch27-drug-discovery.html#connections-to-molecular-design",
    "title": "27  Drug Discovery and Target Identification",
    "section": "27.9 Connections to Molecular Design",
    "text": "27.9 Connections to Molecular Design\nWhile this chapter focuses on target identification and indication selection, foundation model representations connect downstream to molecular design. The bridge between genomic and molecular foundation models typically involves using target context as conditioning signals for molecule generation. Gene-level embeddings from genomic foundation models, reflecting genetic evidence, tissue specificity, and network context, can condition chemistry models that propose molecules targeting that gene.\nMulti-modal foundation models jointly trained on DNA, RNA, proteins, structures, small molecules, and phenotypic readouts learn representations that span these modalities. Such models can predict not just whether a molecule binds a target, but how target modulation in a particular genetic context might affect cellular phenotypes. Closed-loop optimization uses genomic foundation models to predict target relevance and liability, chemistry and protein foundation models to propose molecules, and experimental feedback to update both model types in active learning cycles.\nThe detailed treatment of molecular design belongs to Chapter 28. From a target identification perspective, the key point is that genomic foundation models determine whether a target is worth pursuing; downstream models then optimize how to hit it. The investment in accurate target identification and validation pays dividends throughout the drug discovery pipeline by ensuring that optimization efforts focus on targets with the highest probability of clinical success.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch27-drug-discovery.html#summary",
    "href": "p6-ch27-drug-discovery.html#summary",
    "title": "27  Drug Discovery and Target Identification",
    "section": "27.10 Summary",
    "text": "27.10 Summary\nGenomic foundation models connect to drug discovery at multiple stages and in multiple ways. Target discovery and genetic validation workflows aggregate variant-level scores from foundation models into gene-level evidence, integrating fine-mapping, variant effect prediction, and regulatory modeling to prioritize targets with strong genetic and mechanistic support. Network-aware approaches propagate genetic signals through protein and regulatory networks to identify modules, bottleneck nodes, and repurposing opportunities. Drug-target interaction prediction and toxicity assessment use foundation model representations of targets to predict binding, selectivity, and safety liabilities. Functional genomics screens leverage foundation models for library design, perturbation response modeling, and iterative lab-in-the-loop refinement. Biomarker development uses foundation model features for patient stratification, risk prediction, and trial enrichment.\nThroughout these applications, foundation models serve as tools that reduce risk, prioritize hypotheses, and make experiments more informative. They do not replace experimental validation, clinical trials, or regulatory review. The value proposition is acceleration and prioritization: foundation models help identify the most promising targets, design the most informative experiments, and select the patients most likely to benefit. Programs that would have taken years to reach the right hypothesis can get there faster. Experiments that would have required exhaustive screening can focus resources on high-priority candidates.\nThe chapters on clinical risk prediction (?sec-clinical-risk) and rare disease variant interpretation (Chapter 26) apply related concepts in more specialized contexts. The subsequent chapter on sequence design (Chapter 28) extends these ideas to generative applications, where foundation models not only analyze existing sequences but propose new ones optimized for therapeutic function. Together, these applications illustrate how the representational advances of genomic foundation models connect to the realities of translational research, drug discovery, and patient care.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nFishman, Veniamin, Yuri Kuratov, Aleksei Shmelev, Maxim Petrov, Dmitry Penzar, Denis Shepelin, Nikolay Chekanov, Olga Kardymon, and Mikhail Burtsev. 2025. “GENA-LM: A Family of Open-Source Foundational DNA Language Models for Long Sequences.” Nucleic Acids Research 53 (2): gkae1310. https://doi.org/10.1093/nar/gkae1310.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray, Peter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping Improves Identification of Causal Variants.” Research Square, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery and Target Identification</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html",
    "href": "p6-ch28-design.html",
    "title": "28  Sequence Design and Engineering",
    "section": "",
    "text": "28.1 The design formalism\nThe previous chapters established that foundation models can predict the consequences of genetic and protein variation with increasing accuracy. A model trained on millions of protein sequences can estimate whether a missense variant disrupts function; a regulatory model can forecast how a promoter mutation alters expression across cell types. These predictive capabilities represent genuine scientific advances. Yet prediction alone cannot create a therapeutic protein that nature never evolved, design a promoter that drives expression only in diseased tissue, or engineer an mRNA vaccine against a novel pathogen. The gap between reading genomes and writing them defines one of the central challenges in translational biology: we can now characterize biological sequences with unprecedented resolution, but translating that understanding into designed molecules remains largely empirical, expensive, and slow.\nThis asymmetry between predictive power and generative capability reflects a fundamental mismatch. Evolution optimizes for reproductive fitness over geological timescales, producing sequences that satisfy survival constraints under ancestral conditions. Therapeutic applications demand sequences optimized for entirely different objectives: binding a specific epitope with high affinity, expressing at therapeutic levels in a particular tissue, or evading immune recognition while retaining function. The sequences we need often lie far from natural evolutionary trajectories, in regions of sequence space that foundation models have never observed during training. Navigating this terra incognita requires not just accurate oracles that score candidate sequences, but principled strategies for proposing, testing, and refining designs in domains where model reliability is uncertain.\nFoundation models have begun to address this challenge by providing both generative priors over plausible sequences and differentiable oracles that guide optimization. Protein language models sample novel sequences that respect the statistical patterns of natural proteins. Structure-aware diffusion models generate backbones and sequences simultaneously, enabling design of proteins with specified geometries. Regulatory sequence models predict expression outcomes across thousands of candidate promoters, enabling gradient-based optimization toward desired activity profiles. When coupled with high-throughput experimental assays in closed-loop design cycles, these capabilities are transforming how we approach biological engineering. The same models that predict variant effects (see Chapter 14) become oracles guiding iterative improvement; the same representations that enable transfer learning (see Chapter 9) provide priors that regularize designs toward biologically plausible regions.\nSequence design inverts the standard prediction problem. Where prediction maps from sequence to function (given sequence \\(x\\), estimate property \\(f(x)\\)), design maps from desired function to sequence (given target property \\(y^\\star\\), find sequence \\(x^\\star\\) such that \\(f(x^\\star) \\approx y^\\star\\)). This inversion is computationally challenging because biological sequence spaces are astronomically large. A 200-residue protein admits \\(20^{200}\\) possible sequences, vastly exceeding the number of atoms in the observable universe. Even a modest 500-base-pair regulatory element spans \\(4^{500}\\) possibilities. Exhaustive enumeration is impossible; intelligent search strategies are essential.\nThe design objective can take several mathematical forms depending on the application. Optimization problems seek sequences that maximize (or minimize) a scalar objective, such as finding \\(x^\\star = \\arg\\max_x f_\\theta(x)\\) where \\(f_\\theta\\) might represent predicted binding affinity, expression level, or stability. Conditional generation problems sample sequences from a distribution conditioned on desired properties, drawing \\(x \\sim p_\\theta(x \\mid y)\\) where \\(y\\) specifies structural constraints, functional requirements, or context. Constrained optimization problems combine objective maximization with explicit constraints, seeking \\(x^\\star = \\arg\\max_x f_\\theta(x)\\) subject to \\(c(x) \\leq 0\\), where constraints \\(c\\) might enforce GC content limits, avoid restriction sites, or maintain similarity to natural sequences.\nFoundation models contribute to design through multiple mechanisms. As generative priors, they assign higher probability to sequences resembling natural biology, regularizing optimization toward plausible regions of sequence space. As differentiable oracles, they enable gradient-based optimization where sequence modifications are guided by gradients of predicted properties. As embedding functions, they map discrete sequences into continuous spaces where interpolation and optimization become tractable. The challenge lies in searching enormous combinatorial spaces while remaining within regimes where these model-based estimates remain reliable.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html#protein-design-with-language-models",
    "href": "p6-ch28-design.html#protein-design-with-language-models",
    "title": "28  Sequence Design and Engineering",
    "section": "28.2 Protein design with language models",
    "text": "28.2 Protein design with language models\nProtein language models (PLMs) trained on evolutionary sequence databases have emerged as powerful tools for protein design, providing both generative sampling capabilities and fitness estimation for candidate sequences. The success of these approaches stems from a key insight: evolution has conducted billions of years of experiments on protein sequence space, and PLMs trained on the surviving sequences implicitly encode constraints on what works.\n\n28.2.1 Sequence generation from language model priors\nAutoregressive protein language models such as ProGen and ProtGPT2 generate novel protein sequences by sampling tokens sequentially from learned distributions (Madani et al. 2023; Ferruz, Schmidt, and Höcker 2022). Given a partial sequence, the model predicts probability distributions over the next amino acid, enabling iterative extension until a complete protein emerges. This generation process can be unconditional (sampling from the full learned distribution) or conditional on control signals such as protein family annotations, organism of origin, or functional keywords.\nThe quality of generated sequences depends critically on how closely the sampling distribution matches functional proteins. Sequences sampled at low temperature (more deterministic) tend to resemble common protein families but may lack novelty. Sequences sampled at high temperature (more stochastic) exhibit greater diversity but risk straying into nonfunctional regions. Practical design workflows often generate large libraries of candidates across temperature ranges, then filter using downstream oracles for structure, stability, or function.\nMasked language models like ESM-2 support design through a different mechanism. Rather than generating sequences de novo, these models estimate the probability of each amino acid at each position given the surrounding context. Design proceeds by iterative refinement: starting from an initial sequence, positions are masked and resampled according to model predictions, gradually shifting the sequence toward higher-likelihood regions. This Gibbs-sampling-like procedure can be biased toward specific objectives by combining PLM likelihoods with scores from downstream predictors.\nThe key advantage of PLM-based design lies in data efficiency. Because models are pretrained on millions of natural sequences, they generalize to design tasks with minimal task-specific data. A PLM fine-tuned on a few hundred functional variants can propose candidates across sequence space, extrapolating far beyond the training examples. This contrasts with traditional directed evolution approaches that require extensive experimental screening to navigate sequence space.\n\n\n28.2.2 Structure-aware design with diffusion models\nStructure-aware design addresses a fundamental limitation of sequence-only approaches: proteins function through three-dimensional structures, and sequence optimization without structural guidance may produce sequences that fail to fold correctly. The advent of accurate structure prediction (AlphaFold2, ESMFold) enables new design paradigms that jointly consider sequence and structure.\nRFdiffusion exemplifies this approach by generating protein backbones through a diffusion process in three-dimensional coordinate space (Watson et al. 2023). Starting from random noise, the model iteratively denoises toward plausible backbone geometries, conditioned on design specifications such as target binding interfaces, desired topology, or symmetric assembly requirements. The resulting backbones represent novel structures not observed in nature but predicted to be physically realizable.\nConverting designed backbones to sequences requires inverse folding models that predict amino acid sequences likely to adopt a given structure. ProteinMPNN operates on this principle, taking backbone coordinates as input and outputting probability distributions over sequences predicted to fold onto that backbone (Dauparas et al. 2022). The model can generate thousands of candidate sequences for a single backbone, enabling selection based on additional criteria such as expression likelihood or immunogenicity.\nThis two-stage pipeline (structure diffusion followed by inverse folding) has demonstrated remarkable success in creating novel proteins. Designed binders targeting challenging therapeutic targets, de novo enzymes with specified active site geometries, and symmetric protein assemblies with precise nanoscale dimensions have all been realized experimentally. The key insight is that structure provides a powerful intermediate representation: rather than searching directly in the vast space of sequences, design proceeds through the more constrained space of physically realizable structures.\n\n\n28.2.3 Functional conditioning and multi-objective optimization\nReal therapeutic or industrial applications rarely optimize a single objective. A designed enzyme must not only be catalytically active but also stable at process temperatures, expressible in the production host, and resistant to proteolytic degradation. A therapeutic antibody must bind its target with high affinity while avoiding off-target interactions, maintaining solubility, and minimizing immunogenicity. These competing demands create multi-objective optimization problems where no single sequence optimizes all criteria simultaneously.\nMulti-objective design produces Pareto frontiers of solutions representing different trade-offs among objectives. A sequence might achieve exceptional binding affinity at the cost of reduced stability, while another balances moderate affinity with excellent developability properties. Practitioners must select among Pareto-optimal solutions based on application-specific priorities, and foundation models increasingly support this selection by providing diverse oracles across multiple property dimensions.\nFoundation models contribute to multi-objective design in three ways. Generative priors propose candidate sequences that satisfy basic plausibility constraints (foldability, expressibility) before optimization begins. Multiple differentiable oracles (for binding, stability, immunogenicity) enable gradient-based optimization toward Pareto frontiers. Embedding spaces support interpolation between sequences with different property profiles, enabling exploration of intermediate trade-offs. The combination of these capabilities makes foundation models central to modern protein design pipelines.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html#regulatory-sequence-design",
    "href": "p6-ch28-design.html#regulatory-sequence-design",
    "title": "28  Sequence Design and Engineering",
    "section": "28.3 Regulatory sequence design",
    "text": "28.3 Regulatory sequence design\nGenomic foundation models trained on chromatin accessibility, transcription factor binding, and gene expression data enable design of synthetic regulatory elements with specified activity profiles. Unlike protein design where the sequence-to-function mapping operates through three-dimensional structure, regulatory design must account for the genomic and cellular context in which elements function.\n\n28.3.1 Promoter and enhancer engineering\nMassively parallel reporter assays (MPRAs) have generated training data for models that predict expression levels from promoter and enhancer sequences (Boer et al. 2020). These models learn sequence determinants of regulatory activity, including transcription factor binding sites, spacing constraints between elements, and context-dependent interactions. Once trained, the same models serve as oracles for design: by evaluating expression predictions across millions of candidate sequences, optimization algorithms can identify synthetic regulatory elements with desired properties.\nGradient-based design treats the sequence-to-expression model as a differentiable function. Starting from an initial sequence, gradients of predicted expression with respect to input positions indicate which mutations would increase (or decrease) activity. Because sequences are discrete while gradients are continuous, optimization requires relaxation strategies that operate on “soft” sequence representations before projecting back to discrete nucleotides. These approaches leverage the same saliency map computations used for model interpretation (see Chapter 24), running the analysis in reverse to guide design rather than explain predictions.\nDesign objectives for regulatory elements extend beyond maximizing expression in a target context. Cell-type-specific enhancers should drive high expression in desired tissues while remaining inactive elsewhere. Inducible promoters should respond to specific signals while maintaining low basal activity. Compact regulatory elements are preferred for gene therapy applications where vector capacity is limited. These constraints transform simple optimization into multi-objective problems requiring careful balancing of competing requirements.\nGenerative models trained directly on regulatory sequences offer an alternative to optimization-based approaches. Autoregressive or diffusion models learn to sample novel enhancers and promoters that match the statistical properties of natural regulatory elements. Conditioning on cell type labels, chromatin state annotations, or other metadata enables generation of elements with targeted activity profiles. The advantage of generative approaches lies in their ability to produce diverse candidate libraries for experimental screening, rather than converging on a single optimized sequence that may exploit model artifacts rather than genuine biology.\n\n\n28.3.2 Splicing and RNA processing elements\nModels trained on splicing outcomes (SpliceAI and related architectures described in Chapter 6) enable design of sequences that modulate RNA processing. Therapeutic applications include correcting pathogenic splice site mutations by strengthening weak splice sites or weakening aberrant ones, designing antisense oligonucleotides that redirect splicing to skip exons containing disease-causing mutations, and engineering alternative splicing outcomes to produce desired protein isoforms.\nThe design space for splicing elements encompasses splice site sequences themselves (the canonical GT-AG dinucleotides and surrounding intronic and exonic enhancers and silencers), branch point sequences, and auxiliary sequences that recruit splicing regulatory proteins. Foundation models that predict splicing patterns from local sequence context serve as oracles for evaluating candidate modifications, while gradient-based optimization identifies changes predicted to shift splicing toward therapeutic outcomes.\nDesign of splicing modulators requires particular attention to off-target effects. The splicing code is highly context-dependent, and sequence modifications intended to affect one splice site may inadvertently alter recognition of others. Genome-wide splicing models that predict effects across all splice sites provide essential off-target assessment, flagging candidate designs that would disrupt normal splicing at unintended locations.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html#mrna-design-and-optimization",
    "href": "p6-ch28-design.html#mrna-design-and-optimization",
    "title": "28  Sequence Design and Engineering",
    "section": "28.4 mRNA design and optimization",
    "text": "28.4 mRNA design and optimization\nThe clinical success of mRNA vaccines has intensified interest in systematic approaches to mRNA sequence design. Unlike protein or regulatory element design where the primary challenge is achieving desired function, mRNA design must simultaneously optimize translation efficiency, molecular stability, immune evasion, and manufacturing tractability. Foundation models increasingly contribute to each of these objectives.\n\n28.4.1 Codon optimization principles\nThe genetic code is degenerate: sixty-one sense codons encode twenty amino acids, meaning that any protein sequence can be encoded by many different mRNA sequences. These synonymous sequences differ in translation efficiency, mRNA stability, and immunogenicity despite producing identical proteins. Codon optimization exploits this redundancy to improve therapeutic mRNA performance.\nTraditional codon optimization relied on codon adaptation indices derived from highly expressed genes in target organisms. Codons frequently used in abundant proteins were assumed to be efficiently translated, leading to optimization strategies that maximize use of preferred codons. However, this approach oversimplifies the complex relationship between codon choice and expression. Translation elongation rate varies with codon-anticodon interactions, tRNA abundance, mRNA secondary structure, and ribosome queuing effects. Local codon context matters: rare codons following abundant ones may be translated efficiently, while runs of preferred codons can cause ribosome collisions.\nMachine learning models trained on ribosome profiling data and reporter assays have begun to capture these context-dependent effects. These models predict translation efficiency from sequence features including codon frequencies, local secondary structure, and amino acid properties. Using such models as oracles, optimization algorithms can search for mRNA sequences that maximize predicted translation while avoiding problematic sequence features. The resulting designs often differ substantially from simple codon-frequency optimization, incorporating rare codons at specific positions to optimize local translation dynamics.\n\n\n28.4.2 Stability engineering and UTR design\nmRNA stability in the cytoplasm determines the duration of protein production and thus the dose required for therapeutic effect. Stability is governed by multiple sequence features: the 5’ and 3’ untranslated regions (UTRs) that flank the coding sequence, the presence of destabilizing sequence motifs recognized by RNA-binding proteins, and secondary structures that protect against or expose the molecule to nucleases.\nUTR engineering represents a particularly active area of foundation model application. Natural UTRs contain binding sites for regulatory proteins and microRNAs, sequences that affect ribosome recruitment, and structures that influence mRNA localization and stability. Foundation models trained on expression data across diverse UTR sequences learn which features promote stability and efficient translation. Design algorithms then search for synthetic UTRs that maximize these properties while avoiding sequences that trigger immune recognition or rapid degradation.\nChemical modifications of mRNA (pseudouridine, N1-methylpseudouridine, and other nucleoside analogs) dramatically improve stability and reduce immunogenicity. These modifications alter the sequence-function relationship in ways that current foundation models, trained primarily on natural RNA, may not fully capture. Emerging models that incorporate modification information promise to enable joint optimization of sequence and modification patterns.\n\n\n28.4.3 Immunogenicity considerations\nExogenous mRNA triggers innate immune responses through pattern recognition receptors including Toll-like receptors (TLR3, TLR7, TLR8) and cytosolic sensors (RIG-I, MDA5). While some immune activation may be beneficial for vaccine applications, excessive inflammation limits dosing and causes adverse effects. For protein replacement therapies where repeated dosing is required, minimizing immunogenicity is essential.\nThe immunostimulatory potential of mRNA depends on sequence features including GC content, specific sequence motifs recognized by pattern receptors, and secondary structures that resemble viral replication intermediates. Foundation models that predict immunogenicity from sequence enable design of mRNAs that evade innate immune detection. These predictions must be balanced against other objectives: modifications that reduce immunogenicity may also reduce translation efficiency, creating multi-objective trade-offs that characterize mRNA design more broadly.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html#antibody-and-vaccine-design",
    "href": "p6-ch28-design.html#antibody-and-vaccine-design",
    "title": "28  Sequence Design and Engineering",
    "section": "28.5 Antibody and vaccine design",
    "text": "28.5 Antibody and vaccine design\nAntibody engineering represents one of the most commercially significant applications of computational protein design. The modular architecture of antibodies (framework regions that maintain structural integrity surrounding hypervariable complementarity-determining regions that mediate antigen recognition) creates a well-defined design problem: optimize CDR sequences to achieve desired binding properties while maintaining framework stability and developability.\n\n28.5.1 CDR optimization and humanization\nAntibodies discovered through animal immunization or phage display often require optimization before therapeutic use. Non-human framework sequences may trigger immune responses in patients, necessitating humanization that replaces framework residues with human equivalents while preserving antigen binding. CDR sequences may require affinity maturation to achieve therapeutic potency or specificity optimization to reduce off-target binding.\nFoundation models support antibody optimization through multiple mechanisms. Antibody-specific language models trained on paired heavy and light chain sequences learn the structural and functional constraints on CDR sequences. These models predict which mutations are compatible with the antibody fold and which are likely to disrupt structure. Given a parental antibody sequence, the models can propose libraries of variants enriched for functional candidates, reducing the experimental screening burden required to identify improved variants.\nStructure-aware approaches enable more targeted design. Given a structure of the antibody-antigen complex (determined experimentally or predicted computationally), optimization focuses on residues at the binding interface. Computational saturation mutagenesis predicts the effect of every possible amino acid substitution at each interface position, identifying combinations expected to improve affinity. These predictions guide the construction of focused libraries that explore the most promising region of sequence space.\n\n\n28.5.2 Vaccine antigen design\nVaccine development increasingly employs computational design to create immunogens that elicit protective immune responses. The challenge differs from therapeutic protein design: rather than optimizing for direct biological activity, vaccine antigens must be recognized by the immune system and induce antibodies or T cells that protect against pathogen challenge.\nFoundation models contribute to vaccine design in several ways. Epitope prediction models identify regions of pathogen proteins most likely to be recognized by antibodies or T cells, guiding selection of vaccine targets. Structural models predict how mutations affect epitope conformation, enabling design of stabilized antigens that maintain native epitope structure during manufacturing and storage. Glycan shielding analysis predicts which epitopes will be accessible on the pathogen surface versus hidden by glycosylation, focusing vaccine design on exposed regions.\nThe rapid development of mRNA vaccines against SARS-CoV-2 demonstrated the potential of computational approaches to accelerate vaccine design. Structure-guided stabilization of the prefusion spike conformation, optimization of mRNA sequences for expression and stability, and prediction of variant effects on vaccine efficacy all benefited from computational modeling. Future vaccine development will increasingly integrate foundation model predictions throughout the design process.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html#closed-loop-designbuildtestlearn-cycles",
    "href": "p6-ch28-design.html#closed-loop-designbuildtestlearn-cycles",
    "title": "28  Sequence Design and Engineering",
    "section": "28.6 Closed-loop design–build–test–learn cycles",
    "text": "28.6 Closed-loop design–build–test–learn cycles\nFoundation models achieve their full potential when integrated into iterative experimental workflows. The design–build–test–learn (DBTL) paradigm treats computational predictions as hypotheses to be tested experimentally, with results feeding back to improve both the designed molecules and the models that guide design.\n\n28.6.1 Active learning for efficient exploration\nExperimental validation remains the bottleneck in biological design. Even high-throughput assays can test at most thousands to millions of variants, a tiny fraction of possible sequences. Active learning strategies select which experiments to perform by balancing two competing objectives: exploiting current model predictions to test sequences likely to succeed, and exploring regions of uncertainty to gather data that will improve the model.\nBayesian optimization provides a principled framework for this trade-off. A surrogate model (often a Gaussian process or neural network with uncertainty estimates) approximates the sequence-to-function mapping. Acquisition functions such as expected improvement or upper confidence bound combine predicted function values with uncertainty estimates to select informative test sequences. After each experimental round, the surrogate model is updated with new data, and the process repeats.\nFoundation models enhance active learning by providing informative priors and features. Rather than learning sequence-to-function mappings from scratch, surrogate models can operate on PLM embeddings that capture evolutionary relationships and structural constraints. These embeddings provide a meaningful notion of sequence similarity even before any task-specific data is available, accelerating the early rounds of optimization when labeled data is scarce.\n\n\n28.6.2 Integration with high-throughput experimentation\nModern experimental platforms generate data at scales well-matched to foundation model training. Deep mutational scanning (DMS) systematically characterizes thousands of single-mutant variants of a protein, mapping the functional landscape around a parental sequence. Massively parallel reporter assays test tens of thousands of regulatory element variants in a single experiment. CRISPR screens introduce perturbations across the genome and measure phenotypic consequences.\nThese assays generate dense local maps of sequence-function relationships that complement the global patterns captured by foundation models. The integration is bidirectional: model predictions prioritize which variants to include in experimental libraries, and experimental results fine-tune models for improved accuracy in relevant sequence neighborhoods. After several DBTL cycles, the combined system (fine-tuned model plus accumulated experimental data) can often design sequences that substantially outperform the parental molecule.\nThe design of experiments themselves benefits from computational guidance. Rather than testing all possible single mutants, active learning identifies the most informative subset. Rather than random library construction, computational analysis identifies epistatic interactions that should be explored through combinatorial variants. The cost of DNA synthesis and high-throughput assays makes efficient experimental design increasingly important as design ambitions grow.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html#validation-requirements-and-failure-modes",
    "href": "p6-ch28-design.html#validation-requirements-and-failure-modes",
    "title": "28  Sequence Design and Engineering",
    "section": "28.7 Validation requirements and failure modes",
    "text": "28.7 Validation requirements and failure modes\nComputational design generates hypotheses; experimental validation determines whether those hypotheses are correct. The gap between predicted and observed performance represents the ultimate test of design methods, and understanding where predictions fail is essential for improving both models and design strategies.\n\n28.7.1 The validation hierarchy\nDesigned sequences must pass through multiple validation stages before achieving real-world impact. Computational validation confirms that designs satisfy specified constraints and achieve predicted scores, filtering obvious failures before synthesis. In vitro validation tests whether designed proteins express, fold, and exhibit predicted activities in simplified experimental systems. In vivo validation assesses function in cellular or animal contexts where additional complexity may reveal unanticipated problems. Clinical validation, for therapeutic applications, determines whether designs are safe and effective in human patients.\nSuccess rates decline at each stage of this hierarchy. Computationally promising designs often fail to express or fold correctly. Designs that succeed in vitro may lose activity in cellular contexts due to incorrect localization, unexpected degradation, or off-target interactions. Molecules that perform well in model organisms may fail in human clinical trials due to immunogenicity, toxicity, or pharmacokinetic limitations. The attrition from computational design to clinical success remains substantial, motivating continued improvement in predictive accuracy and earlier identification of failure modes.\n\n\n28.7.2 Characteristic failure patterns\nFoundation model-guided design exhibits systematic failure modes that practitioners must recognize and mitigate. Distribution shift occurs when optimization pushes sequences into regions where model predictions are unreliable. A model trained on natural proteins may produce confident but incorrect predictions for designed sequences that lie far from training data. Regularization toward natural sequence statistics and uncertainty quantification help identify when designs have strayed beyond reliable prediction regimes.\nMode collapse in generative models produces designs that are variants of training sequences rather than genuinely novel molecules. When generated sequences can be matched to close homologs in training data, the design process has failed to create anything new. Novelty filters and diversity requirements during generation help ensure that computational design adds value beyond database retrieval.\nReward hacking occurs when optimization exploits model artifacts rather than genuine sequence-function relationships. A model might predict high expression for sequences containing spurious features that happen to correlate with expression in training data but have no causal effect. Ensemble methods, where designs must score highly across multiple independently trained models, provide some protection against hacking individual model weaknesses.\nThe most insidious failures involve properties that models cannot predict because they were absent from training data. A designed protein might aggregate under manufacturing conditions never encountered during model development. A regulatory element might be silenced by chromatin modifications specific to the therapeutic context. These failures can only be identified through experimental validation in relevant conditions, motivating the closed-loop DBTL approach that continuously tests designs in application-relevant settings.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html#practical-constraints-on-design",
    "href": "p6-ch28-design.html#practical-constraints-on-design",
    "title": "28  Sequence Design and Engineering",
    "section": "28.8 Practical constraints on design",
    "text": "28.8 Practical constraints on design\nBeyond achieving desired function, practical design must satisfy numerous constraints arising from manufacturing, safety, and deployment requirements.\n\n28.8.1 Manufacturing and developability\nDesigned proteins must be producible at scale in expression systems such as bacteria, yeast, or mammalian cells. Expression levels, solubility, and purification behavior determine manufacturing feasibility and cost. Foundation models trained on expression data can predict which sequences are likely to express well, enabling design pipelines that optimize not only for function but for manufacturability.\nFor therapeutic proteins, developability encompasses additional properties including stability during storage, compatibility with formulation requirements, and behavior during analytical characterization. Aggregation propensity, chemical degradation sites (oxidation, deamidation), and glycosylation patterns all affect developability. Computational tools increasingly predict these properties from sequence, enabling their incorporation as design constraints.\n\n\n28.8.2 Safety and biosecurity considerations\nThe same capabilities that enable beneficial design applications also raise biosecurity concerns. Generative models trained on pathogen sequences might in principle be used to design enhanced pathogens or reconstruct dangerous organisms. The dual-use potential of biological design technology requires ongoing attention to safety practices and governance frameworks.\nCurrent foundation models do not provide straightforward paths to bioweapon development; designing a functional pathogen requires capabilities far beyond predicting sequence properties. However, as models improve and integrate with automated synthesis and testing platforms, the barrier to misuse may decrease. Responsible development practices, including careful consideration of training data, model access policies, and monitoring for concerning use patterns, are essential components of the foundation model ecosystem. These considerations connect to the broader discussion of safety and ethics in Chapter 29.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html#algorithmic-approaches-to-search-and-optimization",
    "href": "p6-ch28-design.html#algorithmic-approaches-to-search-and-optimization",
    "title": "28  Sequence Design and Engineering",
    "section": "28.9 Algorithmic approaches to search and optimization",
    "text": "28.9 Algorithmic approaches to search and optimization\nDesign algorithms must navigate vast sequence spaces to identify candidates with desired properties. Several algorithmic paradigms have proven effective, each with characteristic strengths and limitations.\nGradient-based optimization treats foundation models as differentiable functions and computes gradients of objectives with respect to input sequence representations. Because sequences are discrete while gradients are continuous, optimization operates on relaxed representations (probability distributions over nucleotides or amino acids) that are projected back to discrete sequences for evaluation. This approach efficiently navigates high-dimensional spaces but can produce adversarial sequences that exploit model weaknesses rather than achieving genuine biological function.\nEvolutionary algorithms maintain populations of candidate sequences that undergo mutation, recombination, and selection based on fitness scores from foundation model oracles or experimental assays. This approach naturally handles discrete sequence spaces and can maintain diversity to avoid local optima. Multi-objective evolutionary algorithms explicitly construct Pareto frontiers of solutions trading off competing objectives.\nBayesian optimization models the sequence-to-fitness mapping with a probabilistic surrogate (typically a Gaussian process or ensemble neural network) and uses acquisition functions to balance exploration of uncertain regions with exploitation of predicted optima. This approach is particularly effective when experimental evaluations are expensive and each design round must be carefully chosen.\nMonte Carlo methods sample sequences from distributions defined by foundation model likelihoods, optionally biased toward high-scoring regions through importance weighting or Markov chain Monte Carlo. These approaches naturally integrate foundation model priors with task-specific objectives and can generate diverse candidate sets for experimental screening.\nThe choice among algorithmic approaches depends on the specific design problem, available computational resources, and experimental constraints. Many practical pipelines combine multiple approaches: generative sampling to produce initial candidate pools, gradient-based refinement to optimize specific objectives, and active learning to select informative experimental tests.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch28-design.html#looking-forward",
    "href": "p6-ch28-design.html#looking-forward",
    "title": "28  Sequence Design and Engineering",
    "section": "28.10 Looking forward",
    "text": "28.10 Looking forward\nSequence design represents the frontier where foundation models transition from tools for understanding biology to engines for creating it. The field has advanced from designing individual stable proteins to engineering complex molecular machines, from optimizing isolated regulatory elements to programming cellular behavior, from incremental improvement of existing molecules to de novo creation of functions not found in nature.\nSeveral technical challenges remain. Current models struggle with long-range dependencies in regulatory design, where element activity depends on chromosomal context extending far beyond the designed sequence itself. Multi-component design (protein complexes, genetic circuits, metabolic pathways) requires models that capture interactions among designed elements. Temporal dynamics, including how designed proteins respond to cellular conditions over time, remain difficult to predict. Addressing these challenges will require both improved foundation models and tighter integration with mechanistic understanding of biological systems.\nThe validation bottleneck persists as perhaps the most fundamental limitation. Computational design can propose candidates faster than experiments can test them, creating pressure to improve both predictive accuracy (reducing false positives that waste experimental resources) and experimental throughput (enabling more designs to be evaluated). Automated laboratories, standardized assay platforms, and improved experimental design methods will all contribute to accelerating the DBTL cycle.\nFoundation models have transformed what is possible in biological design. The constraints of natural evolution no longer bound the sequences we can consider; the statistical patterns of existing biology provide priors that guide exploration of novel territory. Yet the transition from prediction to design also amplifies both the potential benefits and the risks of these technologies. Ensuring that designed biology serves human flourishing while minimizing potential harms requires not just technical advances but also thoughtful governance, inclusive deliberation about applications, and ongoing attention to safety. These broader considerations connect design to the regulatory, ethical, and societal themes explored in Chapter 29.\n\n\n\n\nBoer, Carl G. de, Eeshit Dhaval Vaishnav, Ronen Sadeh, Esteban Luis Abeyta, Nir Friedman, and Aviv Regev. 2020. “Deciphering Eukaryotic Gene-Regulatory Logic with 100 Million Random Promoters.” Nature Biotechnology 38 (1): 56–65. https://doi.org/10.1038/s41587-019-0315-8.\n\n\nDauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. “Robust Deep Learning–Based Protein Sequence Design Using ProteinMPNN.” Science 378 (6615): 49–56. https://doi.org/10.1126/science.add2187.\n\n\nFerruz, Noelia, Steffen Schmidt, and Birte Höcker. 2022. “ProtGPT2 Is a Deep Unsupervised Language Model for Protein Design.” Nature Communications 13 (1): 4348. https://doi.org/10.1038/s41467-022-32007-7.\n\n\nMadani, Ali, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P. Mohr, James M. Holton, Jose Luis Olmos, et al. 2023. “Large Language Models Generate Functional Protein Sequences Across Diverse Families.” Nature Biotechnology 41 (8): 1099–1106. https://doi.org/10.1038/s41587-022-01618-2.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. “De Novo Design of Protein Structure and Function with RFdiffusion.” Nature 620 (7976): 1089–1100. https://doi.org/10.1038/s41586-023-06415-8.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design and Engineering</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html",
    "href": "p6-ch29-future.html",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "",
    "text": "29.1 Regulatory Frameworks for Genomic AI\nThe first genomic foundation model to receive FDA clearance as a medical device will face a peculiar regulatory challenge: demonstrating that a system trained on millions of sequences from research biobanks, academic databases, and public repositories can safely inform clinical decisions for individual patients who never consented to such use. Clinical-grade variant interpretation tools already incorporate deep learning predictions, yet the regulatory frameworks governing their deployment were designed for deterministic software with traceable decision logic, not for neural networks whose internal representations resist simple explanation. As of 2024, more than 500 AI-enabled medical devices have received FDA authorization, but fewer than a dozen involve genomic interpretation, and none yet deploys a foundation model at the scale described in this book. The gap between technical capability and regulatory readiness defines one of the central tensions facing the field.\nThis asymmetry between what models can do in silico and what they may do in clinical practice shapes every translational decision. A variant effect predictor achieving 0.95 AUROC on a curated benchmark may fail unpredictably on the rare variants that matter most for diagnosis. A regulatory sequence model that accurately predicts chromatin accessibility in well-characterized cell lines may hallucinate effects in patient-derived tissues never seen during training. The technical achievements documented in earlier chapters represent necessary but insufficient conditions for clinical impact. Realizing the benefits of genomic foundation models while managing their risks requires navigating regulatory pathways designed for different technologies, building governance structures for data that spans generations and continents, and confronting ethical questions that genomics and artificial intelligence raise independently but compound when combined.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html#regulatory-frameworks-for-genomic-ai",
    "href": "p6-ch29-future.html#regulatory-frameworks-for-genomic-ai",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "",
    "text": "29.1.1 The Software as Medical Device Paradigm\nRegulatory agencies worldwide classify AI-based clinical tools as software as a medical device (SaMD), a category that applies when software itself constitutes the medical device rather than merely controlling hardware. The International Medical Device Regulators Forum defines SaMD risk tiers based on the seriousness of the health condition and the role software plays in clinical decision-making: software that provides information to drive clinical management of a serious condition receives higher scrutiny than software that merely informs decisions about non-serious conditions.\nGenomic foundation models typically fall into higher-risk categories. A tool that classifies variants as pathogenic or benign directly influences diagnostic conclusions for conditions ranging from hereditary cancer syndromes to rare developmental disorders. The consequences of misclassification can be severe: a false negative may delay life-saving interventions, while a false positive may trigger unnecessary prophylactic surgery or cascade into family-wide psychological harm. Regulators accordingly require substantial evidence of analytical validity (does the model measure what it claims to measure?), clinical validity (does measurement correlate with the clinical outcome?), and in some cases clinical utility (does using the model improve patient outcomes?).\nThe FDA’s approach to AI-enabled devices has evolved considerably since the agency cleared the first autonomous diagnostic AI in 2018. The agency now distinguishes between “locked” algorithms whose behavior is fixed at approval and “adaptive” algorithms that continue learning from new data after deployment. Most foundation models fall into neither category cleanly: their weights are frozen after pretraining, but their outputs depend on prompts, fine-tuning, or downstream heads that may change across applications. This architectural ambiguity creates regulatory uncertainty. A foundation model serving as the backbone for multiple clinical applications might require separate submissions for each use case, or a single submission might cover the shared backbone while individual fine-tuned heads receive separate clearances.\n\n\n29.1.2 European and Global Regulatory Landscapes\nThe European Union’s approach differs from the FDA’s in several respects relevant to genomic AI. The EU Medical Device Regulation (MDR), which fully replaced prior directives in 2021, classifies standalone software according to similar risk principles but places greater emphasis on conformity assessment by notified bodies rather than centralized agency review. For high-risk software, manufacturers must demonstrate compliance with essential safety and performance requirements through technical documentation, quality management systems, and post-market surveillance plans. The AI Act, which entered force in 2024, adds another regulatory layer: high-risk AI systems (including those used in medical diagnosis) must meet transparency, robustness, and human oversight requirements that go beyond device-specific regulations.\nRegulatory divergence across jurisdictions creates practical challenges for global deployment. A genomic foundation model cleared by the FDA may require separate CE marking for European markets, TGA approval in Australia, and PMDA review in Japan, each with distinct evidentiary standards and submission formats. Harmonization efforts through the International Medical Device Regulators Forum provide common frameworks for definitions and risk classification, but substantive requirements continue to differ. Companies developing clinical-grade genomic AI must either design validation programs that satisfy the most stringent jurisdiction or pursue market-by-market strategies that delay access in some regions.\nThe regulatory landscape for laboratory-developed tests (LDTs) further complicates matters in the United States. Clinical laboratories have historically been able to develop and offer tests under their own validation without FDA premarket review, relying instead on CLIA certification and state licensure. Many clinical genomics laboratories use in-house bioinformatics pipelines, variant callers, and annotation tools that incorporate machine learning components without seeking FDA clearance. Recent FDA guidance signals intent to assert greater oversight over LDTs, particularly those using complex algorithms, but the boundary between regulated devices and unregulated laboratory procedures remains contested.\n\n\n29.1.3 Validation Requirements for Clinical Genomic AI\nRegulatory submissions for genomic AI devices require validation evidence spanning multiple dimensions. Analytical validation typically involves demonstrating that the model performs consistently across different sequencing platforms, library preparation methods, and sample types. For a variant effect predictor, this might include showing that scores remain calibrated when inputs come from whole-genome sequencing versus targeted panels, from fresh blood versus archived FFPE tissue, or from healthy individuals versus cancer patients with complex somatic variation.\nClinical validation connects model outputs to clinical outcomes. For a variant classifier, clinical validation might assess concordance with expert panel classifications, correlation with functional assay results, or agreement with segregation patterns in affected families. The choice of reference standard is itself contentious: ClinVar classifications, which many models use as training labels, reflect historical expert consensus that may lag behind accumulating evidence, and circular validation using the same database for training and evaluation produces misleadingly optimistic results (see Chapter 22).\nSome regulators also require evidence of clinical utility, demonstrating that model use improves patient outcomes compared to standard practice. This higher bar is difficult to meet for genomic AI tools that operate as components within larger clinical workflows. A variant effect predictor may improve prioritization efficiency without changing ultimate diagnoses, or may enable earlier diagnosis that only translates to better outcomes when appropriate treatments exist. Designing trials that isolate the model’s contribution from confounding workflow factors requires careful attention to study design and endpoint selection.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html#data-governance-and-consent",
    "href": "p6-ch29-future.html#data-governance-and-consent",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "29.2 Data Governance and Consent",
    "text": "29.2 Data Governance and Consent\n\n29.2.1 The Consent Problem at Scale\nFoundation model training requires data at scales that challenge traditional consent paradigms. A protein language model trained on UniRef encompasses sequences from millions of organisms, including many species for which consent concepts do not apply and human sequences contributed under varied research protocols. A model trained on human genomic data from multiple biobanks aggregates information collected under different consent frameworks, some permitting broad secondary research use and others restricting use to specific studies.\nThe legal and ethical status of such aggregated training depends on how consent documents were written, how thoroughly participants understood the scope of future use, and how jurisdictions interpret secondary use provisions. European GDPR provisions treat genetic data as a special category requiring explicit consent, but may permit research use under legitimate interest or public interest provisions with appropriate safeguards. United States regulations under the Common Rule permit secondary research on properly deidentified data, but genomic data resist complete deidentification given the uniqueness of individual genomes.\nEven when consent technically permits model training, broader ethical questions remain. Participants who consented to genomic research in 2005 could not have anticipated that their data might train AI systems capable of generating novel sequences or predicting sensitive traits. The temporal gap between data collection and model development strains the fiction of informed consent. Dynamic consent systems that allow ongoing engagement and preference updates address some concerns but are difficult to retrofit onto legacy collections and impose burdens on participants and institutions alike.\n\n\n29.2.2 Biobank Governance Models\nLarge biobanks have developed varied governance approaches that shape how their data can be used for foundation model development. UK Biobank, which combines genomic data with extensive phenotypic information on approximately 500,000 participants, permits registered researchers to use data for health-related research under terms that explicitly anticipate computational and AI applications. Access requires application review, data security commitments, and agreement to publish results. The model has enabled substantial foundation model research while maintaining participant trust through transparent policies and active communication.\nOther biobanks operate under more restrictive frameworks. Some disease-specific registries limit use to research on particular conditions. Some indigenous and community biobanks require tribal or community approval for research access, reflecting concerns about historical exploitation and the importance of data sovereignty. The tension between open science norms that favor broad data sharing and community governance norms that prioritize local control creates friction for foundation model developers seeking diverse training data.\nFederated learning and other privacy-preserving techniques offer partial solutions by enabling model training without centralizing raw data. Under federated approaches, each data custodian trains local models that share only gradients or model updates with a central coordinator. The approach protects against centralization risks but introduces technical complexity, may reduce model quality compared to centralized training, and does not eliminate all privacy risks (gradient updates can sometimes reveal individual-level information). Practical federated training for genomic foundation models remains an active research area with limited deployment experience.\n\n\n29.2.3 Secondary Use and Data Futures\nThe genomic data collected today may be used for applications not yet imagined. A variant database assembled for pharmacogenomic research might later inform ancestry inference tools with implications for immigration enforcement. Chromatin accessibility data generated for cancer biology might reveal aging signatures relevant to insurance underwriting. Foundation models trained on diverse genomic data acquire emergent capabilities that their creators did not anticipate and may not recognize.\nGovernance structures must therefore address not just present uses but future possibilities. Some institutions adopt broad consent models that authorize essentially unlimited research use, relying on institutional review and public benefit assessments rather than individual authorization for each application. Others implement tiered consent allowing participants to authorize some uses while restricting others. Still others propose data trusts or cooperatives that hold data on participants’ behalf and negotiate access terms collectively.\nNo consensus has emerged on optimal governance structures for genomic foundation model development. The field operates within a patchwork of institutional policies, national regulations, and community norms that permit some training configurations while prohibiting others. Researchers building foundation models must navigate this landscape carefully, documenting data provenance, respecting access restrictions, and anticipating how governance norms may evolve as AI capabilities advance.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html#privacy-and-genomic-data",
    "href": "p6-ch29-future.html#privacy-and-genomic-data",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "29.3 Privacy and Genomic Data",
    "text": "29.3 Privacy and Genomic Data\n\n29.3.1 The Re-identification Challenge\nGenomic data pose fundamental privacy challenges because genomes are simultaneously unique identifiers and richly informative biological records. A person’s genome can be matched against public genealogy databases, research repositories, or forensic databases to establish identity with high confidence. Once identified, the genomic record reveals information about disease predisposition, ancestry, family relationships, and other sensitive attributes that the person may not wish to disclose.\nConventional anonymization techniques that remove names and obvious identifiers provide limited protection. Research has demonstrated re-identification of individuals from genomic data alone, from genomic data combined with demographic information, and even from aggregate genomic statistics under certain conditions. Foundation models compound these concerns by potentially extracting and recombining information in ways that defeat simple deidentification. A model trained on sequences from many individuals might, under adversarial prompting, generate outputs that reveal information about specific training examples.\nTechnical safeguards include differential privacy (which adds calibrated noise to training procedures to bound individual-level information leakage), secure multi-party computation (which enables joint computation over distributed data without revealing inputs), and synthetic data generation (which produces training data that preserves statistical properties without corresponding to real individuals). Each approach involves tradeoffs between privacy protection and model utility. Differential privacy with strong guarantees may degrade model performance substantially. Secure computation adds computational overhead and complexity. Synthetic data may fail to capture rare variants or unusual correlations essential for clinical applications.\n\n\n29.3.2 Family and Relational Privacy\nGenomic privacy extends beyond individuals to families and communities. A person’s genome reveals information about biological relatives who may not have consented to any data collection. Identifying a carrier of a hereditary cancer mutation implies elevated risk for parents, siblings, and children. Revealing ancestry information for one family member constrains inferences about relatives. These relational dimensions mean that individual consent cannot fully protect the interests of those affected by genomic disclosure.\nFoundation models trained on family data, or capable of inferring family relationships from population-level patterns, create new relational privacy risks. A model that accurately predicts recessive disease carrier status from sequence alone could identify at-risk couples without explicit testing. A model that infers extended pedigree structure from population genetics signals could reveal family secrets or create legal complications. Governance frameworks must consider not just the rights of data subjects but the interests of biological relatives who cannot meaningfully consent.\nSome jurisdictions have begun addressing relational genomic privacy through legislation. The Genetic Information Nondiscrimination Act (GINA) in the United States prohibits health insurers and employers from using genetic information discriminatorily, providing partial protection for individuals whose relatives have been tested. European GDPR provisions on special category data extend some protections to inferred genetic information. But legal frameworks lag behind technical capabilities, and enforcement mechanisms remain limited.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html#intellectual-property-and-ownership",
    "href": "p6-ch29-future.html#intellectual-property-and-ownership",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "29.4 Intellectual Property and Ownership",
    "text": "29.4 Intellectual Property and Ownership\n\n29.4.1 Who Owns Genomic Sequence Data?\nLegal frameworks for sequence data ownership vary across jurisdictions and remain contested. In the United States, the Supreme Court’s 2013 Myriad decision held that naturally occurring DNA sequences cannot be patented, eliminating one barrier to data sharing but leaving property rights in datasets unclear. Databases may receive limited copyright protection for their selection and arrangement, but individual sequences typically cannot be copyrighted as facts or natural phenomena. Contractual restrictions, such as data use agreements attached to biobank access, provide the primary mechanism for controlling sequence data use.\nThe situation differs for synthetic or engineered sequences, which may qualify for patent protection if they meet novelty, utility, and non-obviousness requirements. Foundation models that generate novel sequences thus operate in complex IP territory: sequences generated by the model may be patentable if sufficiently innovative, but determining inventorship (human researcher versus AI system) raises unresolved legal questions. Courts and patent offices are only beginning to address AI-generated inventions, with varying approaches across jurisdictions.\nFor foundation model developers, the key practical questions concern what restrictions apply to training data and what rights attach to model outputs. Training on publicly available sequences may be permissible under database terms of use, research exemptions, or fair use principles depending on jurisdiction and use context. Commercial deployment of models trained on restricted-access data may require additional authorization. Outputs generated by models may be freely usable by the model operator, or may carry through restrictions from training data, depending on legal interpretation and contractual provisions.\n\n\n29.4.2 Model Weights as Assets\nFoundation model weights represent substantial investments of compute, data, and expertise, creating obvious commercial value. Companies training large genomic models face decisions about whether to release weights openly, provide API access without weight release, or restrict access entirely. Each approach carries different implications for scientific progress, commercial competition, and safety management.\nOpen release of weights enables independent research, reproduction, and adaptation but forfeits commercial control and complicates responsibility for misuse. API access maintains control while enabling broad use but creates dependencies and may restrict scientific scrutiny. Restricted access protects competitive advantage and may enhance safety oversight but limits beneficial applications and concentrates power.\nThe genomics community has historically favored open data sharing, with major databases and biobanks making data freely available under permissive terms. Whether this norm extends to foundation model weights is contested. Arguments for openness emphasize scientific reproducibility, broad access benefits, and the difficulty of maintaining meaningful restrictions given technical capabilities for weight reconstruction or distillation. Arguments for restriction emphasize dual-use risks from highly capable generative models, commercial incentives necessary to sustain development investment, and the potential for open models to be fine-tuned for harmful purposes.\n\n\n29.4.3 Prediction Ownership and Liability\nWhen a foundation model generates a clinically relevant prediction (this variant is likely pathogenic, this regulatory sequence will increase expression), questions arise about who owns that prediction and who bears responsibility if it proves wrong. The model developer, the clinical laboratory using the model, the health system employing the laboratory, and the clinician acting on results all have potential roles and potential liability.\nCurrent legal frameworks generally hold clinicians responsible for clinical decisions, with laboratories liable for test quality and medical device manufacturers liable for product defects. How these responsibilities apply when decisions incorporate foundation model outputs remains uncertain. If a model developer provides a variant classifier as SaMD, the developer likely bears some responsibility for the classifier’s performance. If a laboratory integrates foundation model embeddings into a proprietary pipeline, the laboratory may assume primary responsibility for overall system performance. If a clinician overrides a model recommendation based on clinical judgment, liability may shift toward the clinician’s decision-making.\nThese liability questions have practical implications for foundation model deployment. Developers may structure their offerings to minimize liability exposure, for instance by providing research-use-only tools that shift responsibility to users, or by limiting outputs to information that falls short of clinical recommendations. Such structuring may impede beneficial clinical applications if it creates uncertainty about appropriate use or fragments responsibility in ways that leave harms uncompensated.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html#responsible-development-practices",
    "href": "p6-ch29-future.html#responsible-development-practices",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "29.5 Responsible Development Practices",
    "text": "29.5 Responsible Development Practices\n\n29.5.1 Transparency and Documentation\nResponsible foundation model development requires transparency about training data, model capabilities, limitations, and intended use. Model cards, datasheets, and similar documentation frameworks provide structured approaches to capturing this information. For genomic foundation models, relevant documentation includes:\nTraining data composition encompasses which species are represented, what genomic regions are covered, which populations contribute human data, what functional annotations are included, and how data were filtered or preprocessed. Data provenance documentation traces sources, access conditions, and any restrictions on use or redistribution. Evaluation results cover performance across relevant benchmarks, disaggregated by ancestry, variant type, gene family, and other relevant strata. Limitation disclosure identifies known failure modes, out-of-distribution detection capabilities, and contexts where model outputs should not be trusted.\nThe challenge is ensuring that documentation reaches users who need it and influences their decisions. A detailed model card published alongside model weights may be ignored by users seeking quick results. Clinical deployments may strip away documentation as models are integrated into larger systems. Effective transparency requires not just producing documentation but designing workflows that surface relevant information at decision points and verifying that users understand limitations.\n\n\n29.5.2 Fairness and Performance Equity\nGenomic foundation models inherit biases from their training data. If training corpora over-represent European ancestry populations, models may perform worse on variants common in other populations, on regulatory elements active in non-European tissues, or on genes under different selective pressures across populations. If functional annotations derive primarily from well-funded research programs focused on common diseases, models may underperform on rare diseases or conditions affecting underserved populations.\nFairness assessment requires disaggregated evaluation across relevant population strata, not just aggregate performance metrics. A variant effect predictor achieving 0.92 AUROC overall might achieve 0.95 in European populations and 0.82 in African populations, a disparity masked by aggregate reporting. A regulatory model might perform well on cell types common in training data (lymphocytes, hepatocytes) while failing on less-studied cell types (specialized neurons, rare immune subsets) that matter for particular diseases.\nMitigation approaches include diversifying training data, applying reweighting or resampling strategies during training, and developing adaptation techniques that improve performance on underrepresented groups. But data diversification has limits when underlying resources remain skewed, and post-hoc corrections may trade off overall performance for equity gains. The deeper solution involves changing incentive structures to prioritize diverse data collection and equitable performance from the outset.\n\n\n29.5.3 Human Oversight and Decision Support\nEven highly capable foundation models should operate as decision support tools rather than autonomous decision-makers in clinical contexts. Human oversight serves multiple functions: catching model errors that fall outside training distribution, integrating clinical context that models cannot access, navigating value trade-offs where technical optimization is insufficient, and maintaining accountability structures that enable error correction and redress.\nEffective oversight requires that model outputs be interpretable enough for humans to exercise meaningful judgment. If a variant classifier provides only a pathogenic/benign label without supporting evidence, the overseeing clinician has no basis for assessing whether the model’s reasoning applies to the case at hand. If a regulatory effect predictor reports a large effect without indicating uncertainty, the user may not know when skepticism is warranted. Interpretability tools discussed in Chapter 24 support oversight by revealing internal model reasoning, but interpreting such explanations requires expertise and time that may not be available in clinical workflows.\nSystem design must also prevent automation bias, the tendency for human operators to defer to automated recommendations even when independent judgment would lead to different conclusions. Training clinicians to use AI tools effectively, designing interfaces that prompt critical evaluation rather than passive acceptance, and monitoring for over-reliance patterns are all components of responsible oversight architecture.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html#dual-use-and-biosecurity",
    "href": "p6-ch29-future.html#dual-use-and-biosecurity",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "29.6 Dual Use and Biosecurity",
    "text": "29.6 Dual Use and Biosecurity\n\n29.6.1 Generative Models and Pathogen Enhancement\nFoundation models capable of generating functional biological sequences raise biosecurity concerns distinct from those posed by predictive models. A protein language model trained to generate functional enzymes might, in principle, be prompted to design proteins with enhanced pathogenic properties. A regulatory sequence model might generate promoters optimized for expression in human tissues of concern. A generative DNA model might propose sequences that evade detection by standard diagnostics.\nThe severity of these risks depends on technical factors that remain uncertain. Current generative models often produce sequences that are theoretically functional but fail in experimental validation; the gap between computational generation and biological realization provides a natural barrier. Specialized knowledge required to translate generated sequences into actual biological threats remains substantial, though it may decrease as wetlab automation advances. Many dangerous sequences are already documented in public databases, making novel generation less marginal than it might appear.\nNonetheless, responsible development requires attention to dual-use potential. Strategies include capability evaluation (probing models for ability to generate concerning sequences before release), staged deployment (limiting access to highly capable generative models while monitoring for misuse indicators), and output filtering (screening generated sequences against known hazard databases). The optimal balance between open scientific exchange and biosecurity restriction remains contested, with reasonable experts holding divergent views on where lines should be drawn.\n\n\n29.6.2 Access Controls and Responsible Release\nFoundation model developers must decide how to release models in ways that enable beneficial use while limiting potential for harm. Complete openness maximizes beneficial applications but foregoes control over misuse. Complete restriction limits misuse but also limits beneficial applications and may prove impossible to maintain as model capabilities become reproducible. Graduated access models attempt to balance these considerations by providing broader access to less capable models while restricting access to more capable systems.\nAccess controls can operate at multiple levels: restricting weight access while providing API availability, limiting API capabilities through output filtering, requiring applications and use agreements for access, or monitoring usage patterns for indicators of concerning applications. Each control imposes costs on legitimate users and may prove circumventable by determined malicious actors. The effectiveness of controls depends on the specific model, the capability of concern, and the technical sophistication of potential misusers.\nFor genomic foundation models specifically, the biosecurity risks are generally lower than for models capable of synthesizing pathogen sequences from scratch, but concerns about privacy violations, discriminatory applications, and scientific misconduct remain. A model capable of inferring sensitive traits from genomic data might be misused for unauthorized health prediction. A model capable of generating realistic synthetic genomic data might be used to fabricate research results. Responsible release strategies must consider these diverse risk profiles.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html#open-technical-problems",
    "href": "p6-ch29-future.html#open-technical-problems",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "29.7 Open Technical Problems",
    "text": "29.7 Open Technical Problems\n\n29.7.1 Scaling and Efficiency\nThe largest foundation models in natural language processing now exceed a trillion parameters and were trained on trillions of tokens. Genomic foundation models remain substantially smaller, with typical models ranging from hundreds of millions to low billions of parameters. Whether genomic applications require comparable scale remains uncertain. The human genome spans 3 billion base pairs and encompasses perhaps 20,000 protein-coding genes, a smaller and more constrained space than natural language. But capturing the full complexity of gene regulation, protein structure, and cellular context may require parameter counts that approach or exceed language model scale.\nScaling genomic foundation models faces several bottlenecks. Training data availability constrains scale when models exhaust unique sequences and must rely on data augmentation or repetition. Compute costs remain prohibitive for most academic groups and limit experimentation with truly large architectures. Long sequence lengths required for genomic context (regulatory elements can span hundreds of kilobases) create quadratic attention costs that limit practical context windows despite architectural innovations.\nEfficiency improvements that reduce compute requirements without sacrificing capability are thus particularly valuable for genomic applications. Approaches include sparse attention patterns that avoid full quadratic costs, state space models that process sequences in linear time, knowledge distillation that transfers capability from large models to smaller ones, and quantization that reduces precision requirements for inference. Each approach involves trade-offs between efficiency gains and capability preservation that must be evaluated empirically on genomic tasks.\n\n\n29.7.2 Context and Multi-Scale Integration\nBiological phenomena span scales from nucleotides to ecosystems. Foundation models must integrate information across these scales to capture biological reality: local sequence motifs, regulatory element architecture, chromosome-level organization, cellular context, tissue environment, organism-level physiology, and population-level variation all contribute to genotype-phenotype relationships.\nCurrent approaches typically focus on single scales or model multi-scale relationships implicitly through large training datasets rather than explicitly through architectural design. A DNA language model processes sequence tokens without explicit representation of chromatin structure. A single-cell model embeds cells without explicit representation of tissue organization. A regulatory model predicts expression without explicit representation of 3D genome contacts.\nArchitectures that explicitly integrate across scales remain a frontier. Hierarchical models that compose representations at different resolutions, graph neural networks that encode biological relationships across scales, and hybrid systems that combine modality-specific encoders with cross-modal attention layers all represent active research directions. Success will require not just architectural innovation but appropriate training data that captures multi-scale relationships and evaluation protocols that probe multi-scale reasoning.\n\n\n29.7.3 Causality and Mechanism\nThe distinction between correlation and causation pervades genomic analysis. A variant associated with disease in GWAS may be causal, in linkage disequilibrium with a causal variant, or confounded by population structure or other factors. A regulatory element predicted to affect expression may directly drive transcription or may merely co-occur with other causal elements. Foundation models, like other statistical learners, capture patterns in training data without distinguishing causal from correlational relationships.\nProgress toward causal and mechanistic reasoning in genomic AI likely requires integrating diverse evidence types. Perturbation experiments (CRISPR knockouts, drug treatments, environmental exposures) provide interventional data that can distinguish causal effects from correlations. Mendelian randomization approaches leverage genetic instruments to estimate causal effects from observational data. Structural causal models provide formal frameworks for encoding and reasoning about causal relationships.\nIncorporating causal structure into foundation models is technically challenging. Causal relationships are often unknown, contested, or context-dependent. Training objectives that encourage causal reasoning must balance causal accuracy against predictive performance on tasks where correlation suffices. Evaluation of causal reasoning requires benchmarks with known causal ground truth, which are scarce for complex biological systems.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html#emerging-directions",
    "href": "p6-ch29-future.html#emerging-directions",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "29.8 Emerging Directions",
    "text": "29.8 Emerging Directions\n\n29.8.1 Multimodal Integration\nThe foundation models discussed throughout this book largely operate on single modalities: DNA sequence, protein sequence, gene expression counts, chromatin accessibility signals. Biological reality is irreducibly multimodal, with information flowing across modalities through transcription, translation, signaling, and metabolism. The next generation of genomic foundation models will need to integrate across modalities more deeply.\nEarly multimodal genomic models combine encoders trained separately on different modalities, using cross-attention or shared embedding spaces to enable cross-modal reasoning. More ambitious architectures train end-to-end on multimodal data, learning unified representations that capture relationships between sequence and structure, expression and chromatin state, genotype and phenotype. The data requirements for such training are substantial, requiring aligned measurements across modalities at scale.\nClinical applications particularly benefit from multimodal integration. A diagnostic model that combines genomic variants with electronic health record data, imaging findings, and laboratory values can capture patterns invisible to any single modality. A prognostic model that integrates germline genetics with tumor transcriptomics and treatment history can personalize predictions in ways that purely genetic models cannot. Building such systems requires not just technical capability but also data governance frameworks that permit multimodal combination while protecting privacy.\n\n\n29.8.2 Agentic and Closed-Loop Systems\nFoundation models have traditionally operated as passive tools: given an input, they produce an output, and humans decide what to do with it. Emerging agentic architectures allow models to take actions, observe outcomes, and adapt behavior based on feedback. In genomic contexts, agentic systems might design experiments, interpret results, revise hypotheses, and iterate toward biological goals with minimal human intervention.\nClosed-loop systems couple computational prediction with experimental validation in automated cycles. A design model proposes sequences optimized for a target function. An automated synthesis and screening platform tests proposed sequences. Results feed back to update the model or guide subsequent proposals. Such systems can explore sequence space far more efficiently than sequential human-directed experimentation.\nThe promise of agentic and closed-loop approaches is accelerated discovery: identifying functional sequences, characterizing biological mechanisms, and optimizing therapeutic candidates faster than traditional workflows. The risks include models pursuing objectives that diverge from human intent, experimental systems generating safety hazards, and accountability gaps when autonomous systems make consequential errors. Realizing benefits while managing risks requires careful attention to objective specification, monitoring and oversight mechanisms, and safety boundaries that constrain autonomous action.\n\n\n29.8.3 Clinical Integration and Learning Health Systems\nThe ultimate test of genomic foundation models is whether they improve health outcomes. Moving from research demonstrations to clinical impact requires integration into care workflows, evidence of benefit from prospective studies, regulatory clearance, and sustainable business models that support ongoing development and maintenance.\nLearning health systems provide a framework for continuous improvement: clinical use generates data that feeds back into model refinement, creating virtuous cycles where models improve as they serve more patients. Such systems raise governance questions about who controls the learning process, how improvements are validated before deployment, and how benefits and risks are distributed across patients, providers, and technology developers.\nThe foundation model paradigm offers particular advantages for learning health systems. Pretrained models can be adapted to local populations and practices through fine-tuning on institutional data. Improvements demonstrated at one institution can potentially transfer to others through shared model updates. Common architectures enable comparison across sites and accumulation of evidence across diverse populations.\nRealizing this vision requires infrastructure for secure data sharing, governance frameworks that enable learning while protecting privacy, regulatory pathways that accommodate evolving systems, and clinical workflows that support appropriate use and oversight. The technical capabilities described in this book are necessary but not sufficient. Genomic foundation models will achieve their potential only through sustained collaboration among technologists, clinicians, patients, policymakers, and communities working together to build systems that are both capable and trustworthy.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "p6-ch29-future.html#visual-recommendations",
    "href": "p6-ch29-future.html#visual-recommendations",
    "title": "29  Regulatory, Ethical, and Future Considerations",
    "section": "29.9 Visual Recommendations",
    "text": "29.9 Visual Recommendations\nFigure 29.1: Regulatory pathway comparison. Three-column diagram comparing FDA (United States), CE marking (European Union), and representative other jurisdictions (Japan, Australia) for genomic AI medical devices. Show classification tiers, required evidence types, and typical timelines. Annotate with examples of genomic AI tools at each regulatory stage.\nFigure 29.2: Data governance models. Schematic comparing traditional consent (participant → researcher), broad consent (participant → biobank → multiple researchers), dynamic consent (ongoing participant engagement), and federated/privacy-preserving approaches (data remains local, only aggregates shared). Include arrows showing data flow and control points.\nFigure 29.3: Dual-use risk assessment framework. Two-dimensional matrix with axes for “capability for harm” (low to high) and “accessibility of knowledge required” (low to high). Position example applications (variant interpretation, regulatory design, protein generation, sequence optimization) in appropriate quadrants. Annotate with suggested safeguards for each quadrant.\nFigure 29.4: Multi-scale integration architecture. Hierarchical diagram showing biological scales (nucleotide, element, gene, chromosome, cell, tissue, organism) and corresponding computational representations. Use connecting arrows to indicate information flow between scales. Highlight current model limitations at scale boundaries.\nFigure 29.5: Closed-loop design system. Circular workflow diagram showing: generative model proposes sequences → automated synthesis produces candidates → high-throughput assay measures function → results update model or guide selection → iteration. Annotate with where safety filters, human oversight, and stopping criteria can be applied.",
    "crumbs": [
      "Part VI — Translation and Application",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regulatory, Ethical, and Future Considerations</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green,\nAlexander Pritzel, Olaf Ronneberger, et al. 2024.\n“[AlphaFold3] Accurate Structure\nPrediction of Biomolecular Interactions with AlphaFold\n3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky,\nAnna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev.\n2010. “A Method and Server for Predicting Damaging Missense\nMutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nAll of Us Research Program Investigators, All of Us; 2019. “The\n‘All of Us’ Research\nProgram.” New England Journal of Medicine\n381 (7): 668–76. https://doi.org/10.1056/NEJMsr1809937.\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F.\nScott, and Ada Hamosh. 2015. “OMIM.org:\nOnline Mendelian Inheritance in\nMan (OMIM®), an Online Catalog of Human Genes\nand Genetic Disorders.” Nucleic Acids Research 43 (D1):\nD789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAuton, Adam, Gonçalo R. Abecasis, David M. Altshuler, Richard M. Durbin,\nGonçalo R. Abecasis, David R. Bentley, Aravinda Chakravarti, et al.\n2015. “A Global Reference for Human Genetic Variation.”\nNature 526 (7571): 68–74. https://doi.org/10.1038/nature15393.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A.\nGrabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet\nKohli, and David R. Kelley. 2021. “[Enformer]\nEffective Gene Expression Prediction from Sequence by\nIntegrating Long-Range Interactions.” Nature Methods 18\n(October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025.\n“AlphaGenome: AI for Better\nUnderstanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S.\nSong. 2024. “GPN-MSA: An Alignment-Based\nDNA Language Model for Genome-Wide Variant Effect\nPrediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023.\n“[GPN] DNA Language Models Are Powerful\nPredictors of Genome-Wide Variant Effects.” Proceedings of\nthe National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025.\n“[TraitGym] Benchmarking\nDNA Sequence Models for\nCausal Regulatory Variant\nPrediction in Human\nGenetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBoer, Carl G. de, Eeshit Dhaval Vaishnav, Ronen Sadeh, Esteban Luis\nAbeyta, Nir Friedman, and Aviv Regev. 2020. “Deciphering\nEukaryotic Gene-Regulatory Logic with 100 Million Random\nPromoters.” Nature Biotechnology 38 (1): 56–65. https://doi.org/10.1038/s41587-019-0315-8.\n\n\nBommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran\nArora, Sydney von Arx, Michael S. Bernstein, et al. 2022. “On the\nOpportunities and Risks of\nFoundation Models.” arXiv. https://doi.org/10.48550/arXiv.2108.07258.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and\nVasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant\nEffects with a Deep Protein Language Model.” Nature\nGenetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg\nBrockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025.\n“[Evo 2] Genome Modeling and Design\nAcross All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nBrowning, Brian L., Xiaowen Tian, Ying Zhou, and Sharon R. Browning.\n2021. “Fast Two-Stage Phasing of Large-Scale Sequence\nData.” American Journal of Human Genetics 108 (10):\n1880–90. https://doi.org/10.1016/j.ajhg.2021.08.005.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T.\nElliott, Kevin Sharp, Allan Motyer, et al. 2018. “The\nUK Biobank Resource with Deep Phenotyping and\nGenomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T.\nHiggins-Chen, Steve Horvath, and Bo Wang. 2024.\n“CpGPT: A Foundation Model\nfor DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE]\nMulti-Omics Single-Cell Data Integration and Regulatory\nInference with Graph-Linked Embedding.” Nature\nBiotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nChen, Jiayang, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze\nYu, Licheng Zong, et al. 2022. “[RNA-FM]\nInterpretable RNA Foundation\nModel from Unannotated Data for\nHighly Accurate RNA\nStructure and Function\nPredictions.” arXiv. https://doi.org/10.48550/arXiv.2204.00300.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou.\n2022. “[DeepSEA Sei] A\nSequence-Based Global Map of Regulatory Activity for Deciphering Human\nGenetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė,\nTaylor Applebaum, Alexander Pritzel, et al. 2023.\n“[AlphaMissense] Accurate Proteome-Wide\nMissense Variant Effect Prediction with\nAlphaMissense.” Science 381 (6664):\neadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nCheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu\nYang, Lei Li, and Jian Ma. 2024. “DNALONGBENCH:\nA Benchmark Suite\nFor Long-Range DNA\nPrediction Tasks,” October. https://openreview.net/forum?id=opv67PpqLS.\n\n\nChoi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O’Reilly. 2020.\n“[PRS] Tutorial: A Guide to Performing\nPolygenic Risk Score Analyses.” Nature Protocols 15 (9):\n2759–72. https://doi.org/10.1038/s41596-020-0353-1.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus\nWahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024.\n“[DeepRVAT] Integration of Variant\nAnnotations Using Deep Set Networks Boosts Rare Variant Association\nTesting.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan\nDuan, and Bo Wang. 2024. “scGPT:\nToward Building a Foundation Model for Single-Cell Multi-Omics Using\nGenerative AI.” Nature Methods 21 (8):\n1470–80. https://doi.org/10.1038/s41592-024-02201-0.\n\n\nDabernig-Heinz, Johanna, Mara Lohde, Martin Hölzer, Adriana Cabal, Rick\nConzemius, Christian Brandt, Matthias Kohl, et al. 2024. “A\nMulticenter Study on Accuracy and Reproducibility of Nanopore\nSequencing-Based Genotyping of Bacterial Pathogens.” Journal\nof Clinical Microbiology 62 (9): e00628–24. https://doi.org/10.1128/jcm.00628-24.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez\nCarranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago,\net al. 2023. “Nucleotide Transformer: Building and\nEvaluating Robust Foundation Models for Human Genomics.”\nNature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F.\nMilles, B. I. M. Wicky, et al. 2022. “Robust Deep Learning–Based\nProtein Sequence Design Using ProteinMPNN.”\nScience 378 (6615): 49–56. https://doi.org/10.1126/science.add2187.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper,\nArend Sidow, and Serafim Batzoglou. 2010. “Identifying a\nHigh Fraction of the Human\nGenome to Be Under Selective\nConstraint Using GERP++.”\nPLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nDePristo, Mark A., Eric Banks, Ryan Poplin, Kiran V. Garimella, Jared R.\nMaguire, Christopher Hartl, Anthony A. Philippakis, et al. 2011.\n“A Framework for Variation Discovery and Genotyping Using\nNext-Generation DNA Sequencing Data.” Nature\nGenetics 43 (5): 491–98. https://doi.org/10.1038/ng.806.\n\n\nDuncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R.\nPeterson, and B. Domingue. 2019. “Analysis of Polygenic Risk Score\nUsage and Performance in Diverse Human Populations.” Nature\nCommunications 10 (1): 3328. https://doi.org/10.1038/s41467-019-11112-0.\n\n\nEdgar, Ron, Michael Domrachev, and Alex E. Lash. 2002. “Gene\nExpression Omnibus: NCBI Gene\nExpression and Hybridization Array Data Repository.” Nucleic\nAcids Research 30 (1): 207–10. https://doi.org/10.1093/nar/30.1.207.\n\n\nElnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi,\nYu Wang, Llion Jones, Tom Gibbs, et al. 2021.\n“ProtTrans: Towards\nCracking the Language of Life’s\nCode Through\nSelf-Supervised Deep\nLearning and High Performance\nComputing.” arXiv. https://doi.org/10.48550/arXiv.2007.06225.\n\n\nFerruz, Noelia, Steffen Schmidt, and Birte Höcker. 2022.\n“ProtGPT2 Is a Deep Unsupervised Language Model for\nProtein Design.” Nature Communications 13 (1): 4348. https://doi.org/10.1038/s41467-022-32007-7.\n\n\nFishman, Veniamin, Yuri Kuratov, Aleksei Shmelev, Maxim Petrov, Dmitry\nPenzar, Denis Shepelin, Nikolay Chekanov, Olga Kardymon, and Mikhail\nBurtsev. 2025. “GENA-LM: A Family of\nOpen-Source Foundational DNA Language Models for Long\nSequences.” Nucleic Acids Research 53 (2): gkae1310. https://doi.org/10.1093/nar/gkae1310.\n\n\nFrankish, Adam, Mark Diekhans, Anne-Maud Ferreira, Rory Johnson, Irwin\nJungreis, Jane Loveland, Jonathan M Mudge, et al. 2019.\n“GENCODE Reference Annotation for the Human and Mouse\nGenomes.” Nucleic Acids Research 47 (D1): D766–73. https://doi.org/10.1093/nar/gky955.\n\n\nFrazer, Jonathan, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K.\nMin, Kelly Brock, Yarin Gal, and Debora S. Marks. 2021.\n“[EVE] Disease Variant Prediction with\nDeep Generative Models of Evolutionary Data.” Nature 599\n(7883): 91–95. https://doi.org/10.1038/s41586-021-04043-8.\n\n\nGamazon, Eric R., Heather E. Wheeler, Kaanan P. Shah, Sahar V.\nMozaffari, Keston Aquino-Michaels, Robert J. Carroll, Anne E. Eyler, et\nal. 2015. “A Gene-Based Association Method for Mapping Traits\nUsing Reference Transcriptome Data.” Nature Genetics 47\n(9): 1091–98. https://doi.org/10.1038/ng.3367.\n\n\nGarrison, Erik, Jouni Sirén, Adam M. Novak, Glenn Hickey, Jordan M.\nEizenga, Eric T. Dawson, William Jones, et al. 2018. “Variation\nGraph Toolkit Improves Read Mapping by Representing Genetic Variation in\nthe Reference.” Nature Biotechnology 36 (9): 875–79. https://doi.org/10.1038/nbt.4227.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024.\n“Delphi: A Deep-Learning\nMethod for Polygenic Risk\nPrediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nGoodwin, Sara, John D. McPherson, and W. Richard McCombie. 2016.\n“Coming of Age: Ten Years of Next-Generation Sequencing\nTechnologies.” Nature Reviews Genetics 17 (6): 333–51.\nhttps://doi.org/10.1038/nrg.2016.49.\n\n\nGrešová, Katarína, Vlastimil Martinek, David Čechák, Petr Šimeček, and\nPanagiotis Alexiou. 2023. “Genomic Benchmarks: A Collection of\nDatasets for Genomic Sequence Classification.” BMC Genomic\nData 24 (1): 25. https://doi.org/10.1186/s12863-023-01123-8.\n\n\nGuo, Fei, Renchu Guan, Yaohang Li, Qi Liu, Xiaowo Wang, Can Yang, and\nJianxin Wang. 2025. “Foundation Models in Bioinformatics.”\nNational Science Review 12 (4): nwaf028. https://doi.org/10.1093/nsr/nwaf028.\n\n\nGusev, Alexander, Arthur Ko, Huwenbo Shi, Gaurav Bhatia, Wonil Chung,\nBrenda W. J. H. Penninx, Rick Jansen, et al. 2016. “Integrative\nApproaches for Large-Scale Transcriptome-Wide Association\nStudies.” Nature Genetics 48 (3): 245–52. https://doi.org/10.1038/ng.3506.\n\n\nIoannidis, Nilah M., Joseph H. Rothstein, Vikas Pejaver, Sumit Middha,\nShannon K. McDonnell, Saurabh Baheti, Anthony Musolf, et al. 2016.\n“REVEL: An Ensemble\nMethod for Predicting the\nPathogenicity of Rare Missense\nVariants.” The American Journal of Human\nGenetics 99 (4): 877–85. https://doi.org/10.1016/j.ajhg.2016.08.016.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F.\nMcRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A.\nKosmicki, et al. 2019. “[SpliceAI]\nPredicting Splicing from Primary\nSequence with Deep\nLearning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021.\n“DNABERT: Pre-Trained Bidirectional\nEncoder Representations from\nTransformers Model for DNA-Language in\nGenome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nJiang, Tao, Yongzhuang Liu, Yue Jiang, Junyi Li, Yan Gao, Zhe Cui,\nYadong Liu, Bo Liu, and Yadong Wang. 2020. “Long-Read-Based Human\nGenomic Structural Variation Detection with cuteSV.” Genome Biology 21 (1):\n189. https://doi.org/10.1186/s13059-020-02107-y.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael\nFigurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.\n“[AlphaFold2] Highly Accurate Protein\nStructure Prediction with AlphaFold.”\nNature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nJurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge,\nand René Jäkel. 2024. “SetQuence &\nSetOmic: Deep Set Transformers for Whole\nGenome and Exome Tumour Analysis.” BioSystems 235\n(January): 105095. https://doi.org/10.1016/j.biosystems.2023.105095.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A.\nSloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data\nNavigation on the ENCODE Portal.” Nature\nCommunications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B.\nCummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020.\n“The Mutational Constraint Spectrum Quantified from Variation in\n141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKelley, David R. 2020. “[Basenji2]\nCross-Species Regulatory Sequence Activity\nPrediction.” PLOS Computational Biology 16 (7):\ne1008050. https://doi.org/10.1371/journal.pcbi.1008050.\n\n\nKelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger,\nCory Y. McLean, and Jasper Snoek. 2018. “[Basenji2]\nSequential Regulatory Activity Prediction Across\nChromosomes with Convolutional Neural Networks.” Genome\nResearch 28 (5): 739–50. https://doi.org/10.1101/gr.227819.117.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory\nM. Cooper, and Jay Shendure. 2014. “A General Framework for\nEstimating the Relative Pathogenicity of Human Genetic Variants.”\nNature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nKrusche, Peter, Len Trigg, Paul C. Boutros, Christopher E. Mason,\nFrancisco M. De La Vega, Benjamin L. Moore, Mar Gonzalez-Porta, et al.\n2019. “Best Practices for Benchmarking\nGermline Small Variant\nCalls in Human Genomes.”\nNature Biotechnology 37 (5): 555–60. https://doi.org/10.1038/s41587-019-0054-x.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela\nYen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015.\n“Integrative Analysis of 111 Reference Human Epigenomes.”\nNature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nKurki, Mitja I., Juha Karjalainen, Priit Palta, Timo P. Sipilä, Kati\nKristiansson, Kati M. Donner, Mary P. Reeve, et al. 2023.\n“FinnGen Provides Genetic Insights from a\nWell-Phenotyped Isolated Population.” Nature 613 (7944):\n508–18. https://doi.org/10.1038/s41586-022-05473-8.\n\n\nLambert, Samuel A., Laurent Gil, Simon Jupp, Scott C. Ritchie, Yu Xu,\nAnnalisa Buniello, Aoife McMahon, et al. 2021. “The\nPolygenic Score Catalog as an\nOpen Database for Reproducibility and Systematic Evaluation.”\nNature Genetics 53 (4): 420–25. https://doi.org/10.1038/s41588-021-00783-5.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen\nChao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018.\n“ClinVar: Improving Access to Variant Interpretations\nand Supporting Evidence.” Nucleic Acids Research 46\n(D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam,\nAmit R. Majithia, and Trey Ideker. 2025. “[G2PT]\nA Genotype-Phenotype Transformer to Assess and Explain\nPolygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Hao, Zebei Han, Yu Sun, Fu Wang, Pengzhen Hu, Yuang Gao, Xuemei Bai,\net al. 2024. “CGMega: Explainable Graph Neural\nNetwork Framework with Attention Mechanisms for Cancer Gene Module\nDissection.” Nature Communications 15 (1): 5997. https://doi.org/10.1038/s41467-024-50426-6.\n\n\nLi, Heng. 2013. “Aligning Sequence Reads, Clone Sequences and\nAssembly Contigs with BWA-MEM.” arXiv.\nhttps://doi.org/10.48550/arXiv.1303.3997.\n\n\n———. 2014. “Towards Better Understanding\nof Artifacts in Variant Calling\nfrom High-Coverage\nSamples.” Bioinformatics 30 (20): 2843–51.\nhttps://doi.org/10.1093/bioinformatics/btu356.\n\n\n———. 2018. “Minimap2: Pairwise Alignment for Nucleotide\nSequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLi, Sizhen, Saeed Moayedpour, Ruijiang Li, Michael Bailey, Saleh Riahi,\nMilad Miladi, Jacob Miner, et al. 2023. “CodonBERT:\nLarge Language Models for mRNA Design and Optimization.” bioRxiv. https://doi.org/10.1101/2023.09.09.556981.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and\nYunping Zhu. 2022. “MoGCN: A\nMulti-Omics Integration\nMethod Based on Graph\nConvolutional Network for Cancer\nSubtype Analysis.” Frontiers in\nGenetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nLi, Zehui, Akashaditya Das, William A. V. Beardall, Yiren Zhao, and\nGuy-Bart Stan. 2023. “Genomic Interpreter:\nA Hierarchical Genomic\nDeep Neural Network with\n1D Shifted Window\nTransformer.” arXiv. https://doi.org/10.48550/arXiv.2306.05143.\n\n\nLi, Zehui, Vallijah Subasri, Yifei Shen, Dongsheng Li, Yiren Zhao,\nGuy-Bart Stan, and Caihua Shan. 2025. “Omni-DNA:\nA Unified Genomic\nFoundation Model for\nCross-Modal and\nMulti-Task Learning.”\narXiv. https://doi.org/10.48550/arXiv.2502.03499.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness,\nGlenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome\nReference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting\nLu, Allan dos Santos Costa, et al. 2022. “[ESM-2]\nLanguage Models of Protein Sequences at the Scale of\nEvolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and\nDavid R. Kelley. 2025. “[Borzoi]\nPredicting RNA-Seq Coverage from\nDNA Sequence as a Unifying Model of Gene\nRegulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang,\nYucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025.\n“Life-Code: Central Dogma\nModeling with Multi-Omics\nSequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nLoh, Po-Ru, Petr Danecek, Pier Francesco Palamara, Christian\nFuchsberger, Yakir A Reshef, Hilary K Finucane, Sebastian Schoenherr, et\nal. 2016. “Reference-Based Phasing Using the\nHaplotype Reference Consortium\nPanel.” Nature Genetics 48 (11): 1443–48. https://doi.org/10.1038/ng.3679.\n\n\nMadani, Ali, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P.\nMohr, James M. Holton, Jose Luis Olmos, et al. 2023. “Large\nLanguage Models Generate Functional Protein Sequences Across Diverse\nFamilies.” Nature Biotechnology 41 (8): 1099–1106. https://doi.org/10.1038/s41587-022-01618-2.\n\n\nMallal, Simon, Elizabeth Phillips, Giampiero Carosi, Jean-Michel Molina,\nCassy Workman, Janez Tomažič, Eva Jägel-Guedes, et al. 2008.\n“HLA-B*5701 Screening for\nHypersensitivity to Abacavir.” New\nEngland Journal of Medicine 358 (6): 568–79. https://doi.org/10.1056/NEJMoa0706135.\n\n\nManzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025.\n“Comparative Analysis of Deep\nLearning Models for Predicting\nCausative Regulatory\nVariants.” bioRxiv: The Preprint Server for\nBiology, June, 2025.05.19.654920. https://doi.org/10.1101/2025.05.19.654920.\n\n\nMarees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan,\nEmmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018.\n“[GWAS] A Tutorial on Conducting\nGenome-Wide Association Studies: Quality Control and\nStatistical Analysis.” International Journal of Methods in\nPsychiatric Research 27 (2): e1608. https://doi.org/10.1002/mpr.1608.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen,\nDennis Pultz, Ole Winther, and Wouter Boomsma. 2024.\n“BEND: Benchmarking DNA\nLanguage Models on Biologically Meaningful\nTasks.” arXiv. https://doi.org/10.48550/arXiv.2311.12570.\n\n\nMárquez-Luna, Carla, Steven Gazal, Po-Ru Loh, Samuel S. Kim, Nicholas\nFurlotte, Adam Auton, and Alkes L. Price. 2021. “Incorporating\nFunctional Priors Improves Polygenic Prediction Accuracy in\nUK Biobank and 23andMe Data\nSets.” Nature Communications 12 (1): 6052. https://doi.org/10.1038/s41467-021-25171-9.\n\n\nMaurano, Matthew T., Richard Humbert, Eric Rynes, Robert E. Thurman,\nEric Haugen, Hao Wang, Alex P. Reynolds, et al. 2012. “Systematic\nLocalization of Common\nDisease-Associated Variation in\nRegulatory DNA.” Science 337\n(6099): 1190–95. https://doi.org/10.1126/science.1222794.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill\nVishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel,\nRonnie Rajan, and Shadab Khan. 2025. “BioToken and\nBioFM – Biologically-Informed\nTokenization Enables Accurate and\nEfficient Genomic Foundation\nModels.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and\nAlexander Rives. 2021. “[ESM-1v]\nLanguage Models Enable Zero-Shot Prediction of the Effects\nof Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nMorales, Joannella, Shashikant Pujar, Jane E. Loveland, Alex Astashyn,\nRuth Bennett, Andrew Berry, Eric Cox, et al. 2022. “A Joint\nNCBI and EMBL-EBI Transcript Set\nfor Clinical Genomics and Research.” Nature 604 (7905):\n310–15. https://doi.org/10.1038/s41586-022-04558-8.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy\nSchwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021.\n“An Open Approach to Systematically Prioritize Causal Variants and\nGenes at All Published Human GWAS Trait-Associated\nLoci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nMukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom\nSoare, Raghav Tandon, David Amar, et al. 2024.\n“EmbedGEM: A Framework to Evaluate the Utility of\nEmbeddings for Genetic Discovery.” Bioinformatics\nAdvances 4 (1). https://doi.org/10.1093/bioadv/vbae135.\n\n\nNaghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B.\nMacdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow\nCoyote-Maestas, and Hani Goodarzi. 2024. “[cdsFM - EnCodon/DeCodon]\nA Suite of Foundation\nModels Captures the Contextual\nInterplay Between Codons.”\nbioRxiv. https://doi.org/10.1101/2024.10.10.617568.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT:\nPredicting Amino Acid Changes That Affect Protein\nFunction.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum\nBirch-Sykes, Michael Wornow, Aman Patel, et al. 2023.\n“HyenaDNA: Long-Range\nGenomic Sequence Modeling at\nSingle Nucleotide\nResolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nNielsen, Rasmus, Joshua S. Paul, Anders Albrechtsen, and Yun S. Song.\n2011. “Genotype and SNP Calling from Next-Generation\nSequencing Data.” Nature Reviews. Genetics 12 (6):\n443–51. https://doi.org/10.1038/nrg2986.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk,\nSteffanie Paul, Han Spinner, Nathan Rollins, et al. 2023.\n“ProteinGym: Large-Scale\nBenchmarks for Protein Fitness\nPrediction and Design.” Advances in\nNeural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V.\nBzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The\nComplete Sequence of a Human Genome.” Science 376\n(6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Connell, Jared, Deepti Gurdasani, Olivier Delaneau, Nicola Pirastu,\nSheila Ulivi, Massimiliano Cocca, Michela Traglia, et al. 2014. “A\nGeneral Approach for Haplotype\nPhasing Across the Full Spectrum\nof Relatedness.” PLOS Genetics 10 (4):\ne1004234. https://doi.org/10.1371/journal.pgen.1004234.\n\n\nO’Leary, Nuala A., Mathew W. Wright, J. Rodney Brister, Stacy Ciufo,\nDiana Haddad, Rich McVeigh, Bhanu Rajput, et al. 2016. “Reference\nSequence (RefSeq) Database at NCBI: Current\nStatus, Taxonomic Expansion, and Functional Annotation.”\nNucleic Acids Research 44 (D1): D733–45. https://doi.org/10.1093/nar/gkv1189.\n\n\nOrenbuch, Rose, Courtney A. Shearer, Aaron W. Kollasch, Aviv D. Spinner,\nThomas Hopf, Lood van Niekerk, Dinko Franceschi, Mafalda Dias, Jonathan\nFrazer, and Debora S. Marks. 2025. “[popEVE] Proteome-Wide Model for Human\nDisease Genetics.” Nature Genetics, November, 1–10. https://doi.org/10.1038/s41588-025-02400-1.\n\n\n“PacificBiosciences/Pbsv.” 2025. PacBio. https://github.com/PacificBiosciences/pbsv.\n\n\nPasaniuc, Bogdan, and Alkes L. Price. 2016. “Dissecting the\nGenetics of Complex Traits Using Summary Association Statistics.”\nNature Reviews Genetics 18 (2): 117–27. https://doi.org/10.1038/nrg.2016.142.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006.\n“Population Structure and\nEigenanalysis.” PLOS Genetics 2 (12): e190.\nhttps://doi.org/10.1371/journal.pgen.0020190.\n\n\nPe’er, Itsik, Roman Yelensky, David Altshuler, and Mark J. Daly. 2008.\n“Estimation of the Multiple Testing Burden for Genomewide\nAssociation Studies of Nearly All Common Variants.” Genetic\nEpidemiology 32 (4): 381–85. https://doi.org/10.1002/gepi.20303.\n\n\nPearce, James D., Sara E. Simmonds, Gita Mahmoudabadi, Lakshmi Krishnan,\nGiovanni Palla, Ana-Maria Istrate, Alexander Tarashansky, et al. 2025.\n“[TranscriptFormer]\nCross-Species Generative\nCell Atlas Across 1.5\nBillion Years of Evolution:\nThe TranscriptFormer Single-Cell\nModel.” bioRxiv. https://doi.org/10.1101/2025.04.25.650731.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel,\nSean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022.\n“Calibration of Computational Tools for Missense Variant\nPathogenicity Classification and ClinGen Recommendations\nfor PP3/BP4 Criteria.” American\nJournal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nPoplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas\nColthurst, Alexander Ku, Dan Newburger, et al. 2018.\n“[DeepVariant] A Universal\nSNP and Small-Indel Variant Caller Using Deep Neural\nNetworks.” Nature Biotechnology 36 (10): 983–87. https://doi.org/10.1038/nbt.4235.\n\n\nPrice, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E.\nWeinblatt, Nancy A. Shadick, and David Reich. 2006. “Principal\nComponents Analysis Corrects for Stratification in Genome-Wide\nAssociation Studies.” Nature Genetics 38 (8): 904–9. https://doi.org/10.1038/ng1847.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.\n“Exploring the Limits of Transfer\nLearning with a Unified\nText-to-Text Transformer.”\narXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025.\n“[MIFM] Multiple Instance Fine-Mapping:\nPredicting Causal Regulatory Variants with a Deep Sequence\nModel.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\n“RealTimeGenomics/Rtg-Core.” 2025. Real Time\nGenomics. https://github.com/RealTimeGenomics/rtg-core.\n\n\nRegev, Aviv, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe\nBenoist, Ewan Birney, Bernd Bodenmiller, et al. 2017. “The\nHuman Cell Atlas.” Edited\nby Thomas R Gingeras. eLife 6 (December): e27041. https://doi.org/10.7554/eLife.27041.\n\n\nRehm, Heidi L., Jonathan S. Berg, Lisa D. Brooks, Carlos D. Bustamante,\nJames P. Evans, Melissa J. Landrum, David H. Ledbetter, et al. 2015.\n“ClinGen — The Clinical\nGenome Resource.” New England\nJournal of Medicine 372 (23): 2235–42. https://doi.org/10.1056/NEJMsr1406261.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and\nMartin Kircher. 2019. “CADD: Predicting the\nDeleteriousness of Variants Throughout the Human Genome.”\nNucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin,\nJason Liu, Demi Guo, et al. 2021. “[ESM-1b]\nBiological Structure and Function Emerge from Scaling\nUnsupervised Learning to 250 Million Protein Sequences.”\nProceedings of the National Academy of Sciences of the United States\nof America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nRobinson, James, Dominic J Barker, Xenia Georgiou, Michael A Cooper,\nPaul Flicek, and Steven G E Marsh. 2020.\n“IPD-IMGT/HLA\nDatabase.” Nucleic Acids Research 48 (D1):\nD948–55. https://doi.org/10.1093/nar/gkz950.\n\n\nSakaue, Saori, Saisriram Gurajala, Michelle Curtis, Yang Luo, Wanson\nChoi, Kazuyoshi Ishigaki, Joyce B. Kang, et al. 2023. “Tutorial: A\nStatistical Genetics Guide to Identifying HLA Alleles\nDriving Complex Disease.” Nature Protocols 18 (9):\n2625–41. https://doi.org/10.1038/s41596-023-00853-4.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch.\n2024. “[GROVER] DNA Language Model\nGROVER Learns Sequence Context in the Human Genome.”\nNature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and\nVolodymyr Kuleshov. 2024. “Caduceus:\nBi-Directional Equivariant\nLong-Range DNA\nSequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and\nMartin Kircher. 2024. “CADD V1.7: Using Protein\nLanguage Models, Regulatory CNNs and Other Nucleotide-Level\nScores to Improve Genome-Wide Variant Predictions.” Nucleic\nAcids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nShafin, Kishwar, Trevor Pesout, Pi-Chuan Chang, Maria Nattestad, Alexey\nKolesnikov, Sidharth Goel, Gunjan Baid, et al. 2021.\n“Haplotype-Aware Variant Calling with\nPEPPER-Margin-DeepVariant Enables\nHigh Accuracy in Nanopore Long-Reads.” Nature Methods 18\n(11): 1322–32. https://doi.org/10.1038/s41592-021-01299-w.\n\n\nSherry, S. T., M.-H. Ward, M. Kholodov, J. Baker, L. Phan, E. M.\nSmigielski, and K. Sirotkin. 2001. “dbSNP: The NCBI Database of Genetic\nVariation.” Nucleic Acids Research 29 (1): 308–11. https://doi.org/10.1093/nar/29.1.308.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs,\nMinmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005.\n“[PhastCons] Evolutionarily Conserved\nElements in Vertebrate, Insect, Worm, and Yeast Genomes.”\nGenome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.\n\n\nSirugo, Giorgio, Scott M. Williams, and Sarah A. Tishkoff. 2019.\n“The Missing Diversity in\nHuman Genetic Studies.”\nCell 177 (1): 26–31. https://doi.org/10.1016/j.cell.2019.02.048.\n\n\nSmolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W.\nHorner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024.\n“Detection of Mosaic and Population-Level Structural Variants with\nSniffles2.” Nature Biotechnology 42 (10):\n1571–80. https://doi.org/10.1038/s41587-023-02024-y.\n\n\nSollis, Elliot, Abayomi Mosaku, Ala Abid, Annalisa Buniello, Maria\nCerezo, Laurent Gil, Tudor Groza, et al. 2023. “The\nNHGRI-EBI GWAS\nCatalog: Knowledgebase and Deposition Resource.”\nNucleic Acids Research 51 (D1): D977–85. https://doi.org/10.1093/nar/gkac1010.\n\n\nSong, Li, Gali Bai, X. Shirley Liu, Bo Li, and Heng Li. 2022.\n“T1K: Efficient and Accurate KIR and\nHLA Genotyping with Next-Generation Sequencing\nData.” bioRxiv. https://doi.org/10.1101/2022.10.26.513955.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N.\nPhan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023.\n“Leveraging Base-Pair Mammalian Constraint to Understand Genetic\nVariation and Human Disease.” Science 380 (6643):\neabn2937. https://doi.org/10.1126/science.abn2937.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and\nCathy H. Wu. 2007. “UniRef: Comprehensive and\nNon-Redundant UniProt Reference Clusters.”\nBioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nThe GTEx Consortium. 2020. “The GTEx\nConsortium Atlas of Genetic Regulatory Effects Across Human\nTissues.” Science 369 (6509): 1318–30. https://doi.org/10.1126/science.aaz1776.\n\n\nThe Tabula Sapiens Consortium. 2022. “The Tabula\nSapiens: A Multiple-Organ, Single-Cell\nTranscriptomic Atlas of Humans.” Science 376 (6594):\neabl4896. https://doi.org/10.1126/science.abl4896.\n\n\nTheodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina\nR. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023.\n“[Geneformer] Transfer Learning Enables\nPredictions in Network Biology.” Nature 618 (7965):\n616–24. https://doi.org/10.1038/s41586-023-06139-9.\n\n\nVan der Auwera, Geraldine A., Mauricio O. Carneiro, Christopher Hartl,\nRyan Poplin, Guillermo del Angel, Ami Levy-Moonshine, Tadeusz Jordan, et\nal. 2018. “From FastQ Data to\nHigh-Confidence Variant\nCalls: The Genome\nAnalysis Toolkit Best\nPractices Pipeline.” Current\nProtocols in Bioinformatics 43 (1): 11.10.1–33. https://doi.org/10.1002/0471250953.bi1110s43.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You\nNeed.” arXiv. https://doi.org/10.48550/arXiv.1706.03762.\n\n\nVilhjálmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev,\nSara Lindström, Stephan Ripke, Giulio Genovese, et al. 2015.\n“Modeling Linkage Disequilibrium\nIncreases Accuracy of Polygenic\nRisk Scores.” American Journal of\nHuman Genetics 97 (4): 576–92. https://doi.org/10.1016/j.ajhg.2015.09.001.\n\n\nVõsa, Urmo, Annique Claringbould, Harm-Jan Westra, Marc Jan Bonder,\nPatrick Deelen, Biao Zeng, Holger Kirsten, et al. 2021.\n“Large-Scale Cis- and Trans-eQTL\nAnalyses Identify Thousands of Genetic Loci and Polygenic Scores That\nRegulate Blood Gene Expression.” Nature Genetics 53 (9):\n1300–1310. https://doi.org/10.1038/s41588-021-00913-z.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L.\nTrippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023.\n“De Novo Design of Protein Structure and Function with\nRFdiffusion.” Nature 620 (7976): 1089–1100.\nhttps://doi.org/10.1038/s41586-023-06415-8.\n\n\nWenger, Aaron M., Paul Peluso, William J. Rowell, Pi-Chuan Chang,\nRichard J. Hall, Gregory T. Concepcion, Jana Ebler, et al. 2019.\n“Accurate Circular Consensus Long-Read Sequencing Improves Variant\nDetection and Assembly of a Human Genome.” Nature\nBiotechnology 37 (10): 1155–62. https://doi.org/10.1038/s41587-019-0217-9.\n\n\nWhirl-Carrillo, M, E M McDonagh, J M Hebert, L Gong, K Sangkuhl, C F\nThorn, R B Altman, and T E Klein. 2012. “Pharmacogenomics\nKnowledge for Personalized\nMedicine.” Clinical Pharmacology &\nTherapeutics 92 (4): 414–17. https://doi.org/10.1038/clpt.2012.96.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray,\nPeter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping\nImproves Identification of Causal Variants.” Research\nSquare, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.\n\n\nYang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan\nSalakhutdinov, and Quoc V. Le. 2020. “XLNet:\nGeneralized Autoregressive\nPretraining for Language\nUnderstanding.” arXiv. https://doi.org/10.48550/arXiv.1906.08237.\n\n\nYeo, Gene, and Christopher B. Burge. 2004. “Maximum\nEntropy Modeling of Short\nSequence Motifs with Applications\nto RNA Splicing Signals.”\nJournal of Computational Biology 11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll,\nand Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls\nUsing DeepVariant and GLnexus.”\nBioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei\nSun, Chen-Hao Chen, et al. 2019. “Cistrome Data\nBrowser: Expanded Datasets and New Tools for Gene\nRegulatory Analysis.” Nucleic Acids Research 47 (D1):\nD729–35. https://doi.org/10.1093/nar/gky1094.\n\n\nZheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam,\nand Ruibang Luo. 2022. “Symphonizing Pileup and Full-Alignment for\nDeep Learning-Based Long-Read Variant Calling.” Nature\nComputational Science 2 (12): 797–803. https://doi.org/10.1038/s43588-022-00387-x.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K.\nWong, and Olga G. Troyanskaya. 2018. “[Expecto]\nDeep Learning Sequence-Based Ab Initio Prediction of\nVariant Effects on Expression and Disease Risk.” Nature\nGenetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA]\nPredicting Effects of Noncoding Variants with Deep\nLearning–Based Sequence Model.” Nature Methods 12 (10):\n931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and\nHan Liu. 2024. “DNABERT-2: Efficient\nFoundation Model and Benchmark\nFor Multi-Species\nGenome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.\n\n\nZook, Justin M., Jennifer McDaniel, Nathan D. Olson, Justin Wagner,\nHemang Parikh, Haynes Heaton, Sean A. Irvine, et al. 2019. “An\nOpen Resource for Accurately Benchmarking Small Variant and Reference\nCalls.” Nature Biotechnology 37 (5): 561–66. https://doi.org/10.1038/s41587-019-0074-6.\n\n\nZvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang,\nCindy Orozco Bohorquez, Austin Clyde, et al. 2022.\n“GenSLMs: Genome-Scale Language Models\nReveal SARS-CoV-2 Evolutionary\nDynamics.” bioRxiv. https://doi.org/10.1101/2022.10.10.511571.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "app-a-dl.html",
    "href": "app-a-dl.html",
    "title": "Appendix A — Deep Learning Primer",
    "section": "",
    "text": "A.1 From Linear Models to Deep Networks\nThis appendix gives a compact introduction to deep learning for readers who are comfortable with genomics but less familiar with modern neural networks. The goal is not to replace a full machine learning textbook, but to provide enough background to make the models in Chapters 5–19 feel intuitive rather than magical.\nWe focus on:\nWhere possible, we connect directly to the genomic case studies in the main text (DeepSEA, ExPecto, SpliceAI, Enformer, genomic language models, and GFMs).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#from-linear-models-to-deep-networks",
    "href": "app-a-dl.html#from-linear-models-to-deep-networks",
    "title": "Appendix A — Deep Learning Primer",
    "section": "",
    "text": "A.1.1 Models as Functions\nAt its core, a predictive model is just a function:\n\\[\nf_\\theta: x \\mapsto \\hat{y}\n\\tag{A.1}\\]\nwhere:\n\n\\(x\\) is an input (e.g., a one-hot encoded DNA sequence, variant-level features, or a patient feature vector).\n\n\\(\\hat{y}\\) is a prediction (e.g., probability of a histone mark, gene expression level, disease risk).\n\n\\(\\theta\\) are the parameters (weights) of the model.\n\nIn classical genomics workflows, \\(f_\\theta\\) might be:\n\nLogistic regression (for case–control status)\n\nLinear regression (for quantitative traits)\n\nRandom forests or gradient boosting (for variant pathogenicity scores)\n\nDeep learning keeps the same basic structure but allows \\(f_\\theta\\) to be a much more flexible, high-capacity function built by composing many simple operations.\n\n\nA.1.2 Linear Models vs Neural Networks\nA simple linear model for classification looks like:\n\\[\n\\hat{y} = \\sigma(w^\\top x + b),\n\\]\nwhere \\(w\\) and \\(b\\) are parameters and \\(\\sigma(\\cdot)\\) is a squashing nonlinearity (e.g., the logistic function). The model draws a single separating hyperplane in feature space.\nA neural network generalizes this by stacking multiple linear transformations with nonlinear activation functions:\n\\[\n\\begin{aligned}\nh_1 &= \\phi(W_1 x + b_1) \\\\\nh_2 &= \\phi(W_2 h_1 + b_2) \\\\\n&\\vdots \\\\\n\\hat{y} &= g(W_L h_{L-1} + b_L)\n\\end{aligned}\n\\]\nwhere:\n\nEach \\(W_\\ell, b_\\ell\\) is a layer’s weight matrix and bias.\n\n\\(\\phi(\\cdot)\\) is a nonlinear activation (e.g., ReLU).\n\n\\(g(\\cdot)\\) is a final activation (e.g., sigmoid for probabilities, identity for regression).\n\nThe key idea:\n\nBy composing many simple nonlinear transformations, deep networks can approximate very complex functions.\n\nIn Chapters 5–7, DeepSEA, ExPecto, and SpliceAI implement exactly this pattern, but with convolutional layers (Section 4) tailored to 1D DNA sequence instead of dense matrix multiplications (Zhou and Troyanskaya 2015; Zhou et al. 2018; Jaganathan et al. 2019).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#training-deep-models",
    "href": "app-a-dl.html#training-deep-models",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.2 Training Deep Models",
    "text": "A.2 Training Deep Models\n\nA.2.1 Data, Labels, and Loss Functions\nTo train a model, we need:\n\nA dataset of examples \\(\\{(x_i, y_i)\\}_{i=1}^N\\)\n\nA model \\(f_\\theta\\)\n\nA loss function \\(L(\\hat{y}, y)\\) that measures how wrong a prediction is\n\nCommon loss functions:\n\nBinary cross-entropy (for yes/no labels, e.g., “is this ChIP–seq peak present?”):\n\\[\nL(\\hat{p}, y) = -\\big(y \\log \\hat{p} + (1-y)\\log(1-\\hat{p})\\big)\n\\]\nMulticlass cross-entropy (for one-of-K labels)\n\nMean squared error (MSE) (for continuous outputs, e.g., gene expression)\n\nThe training objective is to find \\(\\theta\\) that minimizes the average loss:\n\\[\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N L\\big(f_\\theta(x_i), y_i\\big).\n\\]\n\n\nA.2.2 2.2 Gradient-Based Optimization\nDeep networks may have millions to billions of parameters. We can’t search over all possibilities, but we can follow the gradient of the loss with respect to \\(\\theta\\):\n\nGradient descent updates: \\[\n\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta),\n\\] where \\(\\eta\\) is the learning rate.\n\nIn practice, we use:\n\nMini-batch stochastic gradient descent (SGD): Compute gradients on small batches of examples (e.g., 128 sequences at a time) for efficiency and better generalization.\nAdaptive optimizers like Adam, which adjust learning rates per parameter.\n\nYou never compute gradients by hand; modern frameworks (PyTorch, JAX, TensorFlow) use automatic differentiation to efficiently compute \\(\\nabla_\\theta \\mathcal{L}\\) even for very complex architectures.\n\n\nA.2.3 Backpropagation in One Sentence\nBackpropagation is just the chain rule of calculus applied efficiently through the layers of a network. It propagates “blame” from the output back to each weight, telling us how changing that weight would change the loss.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#generalization-overfitting-and-evaluation",
    "href": "app-a-dl.html#generalization-overfitting-and-evaluation",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.3 Generalization, Overfitting, and Evaluation",
    "text": "A.3 Generalization, Overfitting, and Evaluation\n\nA.3.1 Train / Validation / Test Splits\nDeep networks can memorize training data if we’re not careful. To evaluate generalization, we typically split data into:\n\nTraining set – used to fit parameters\n\nValidation set – used to tune hyperparameters (learning rate, depth, etc.) and perform early stopping\n\nTest set – held out until the end to estimate performance on new data\n\nIn genomics, how we split matters as much as how much data we have:\n\nSplitting by locus or chromosome (to test cross-locus generalization)\n\nSplitting by individual or cohort (to avoid leakage between related samples)\n\nSplitting by species or ancestry when evaluating transfer\n\nThese issues are developed in more depth in the evaluation and confounding chapters (Chapter 21 and Chapter 22).\n\n\nA.3.2 Overfitting and Regularization\nSigns of overfitting:\n\nTraining loss keeps decreasing, but validation loss starts increasing.\n\nMetrics like AUROC or AUPRC plateau or drop on validation data even as they improve on training data.\n\nCommon regularization techniques:\n\nWeight decay / L2 regularization – penalize large weights.\n\nDropout – randomly zero out activations during training.\n\nEarly stopping – stop training when validation performance stops improving.\n\nData augmentation – generate more training examples by transforming inputs, e.g.:\n\nReverse-complement augmentation for DNA sequences (treat sequence and its reverse complement as equivalent).\n\nWindow jittering: randomly shifting the sequence window around a target site.\n\n\n\n\nA.3.3 Basic Metrics\nYou’ll encounter metrics such as:\n\nAUROC (Area Under the ROC Curve) – how well the model ranks positives above negatives.\n\nAUPRC (Area Under the Precision–Recall Curve) – more informative when positives are rare.\n\nCalibration metrics (e.g., Brier score) and reliability diagrams – especially for clinical risk prediction (?sec-clinical-risk).\n\nThe model and application chapters provide details about which metrics are appropriate for which tasks. See Chapter 21 for more on evaluation metrics.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#convolutional-networks-for-genomic-sequences",
    "href": "app-a-dl.html#convolutional-networks-for-genomic-sequences",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.4 Convolutional Networks for Genomic Sequences",
    "text": "A.4 Convolutional Networks for Genomic Sequences\nConvolutional neural networks (CNNs) are the workhorse architecture in early genomic deep learning models like DeepSEA, ExPecto, and SpliceAI (Zhou and Troyanskaya 2015; Zhou et al. 2018; Jaganathan et al. 2019).\n\nA.4.1 1D Convolutions as Motif Detectors\nFor a 1D DNA sequence encoded as a matrix \\(X \\in \\mathbb{R}^{L \\times 4}\\) (length \\(L\\), 4 nucleotides), a convolutional layer applies a set of filters (kernels) of width \\(k\\):\n\nEach filter is a small matrix \\(K \\in \\mathbb{R}^{k \\times 4}\\).\n\nAt each position, the filter computes a dot product between \\(K\\) and the corresponding \\(k\\)-length chunk of \\(X\\).\n\nSliding the filter along the sequence creates an activation map that is high wherever the motif encoded by \\(K\\) is present.\n\nIntuitively:\n\nA 1D convolutional filter learns to recognize sequence motifs (e.g., transcription factor binding sites) directly from data.\n\n\n\nA.4.2 Stacking Layers and Receptive Fields\nDeeper convolutional layers allow the model to “see” longer-range patterns:\n\nFirst layer: short motifs (e.g., 8–15 bp).\n\nHigher layers: combinations of motifs, motif spacing, and local regulatory grammar.\n\nPooling layers (e.g., max pooling) reduce spatial resolution while aggregating features, increasing the receptive field.\n\nIn DeepSEA, stacked convolutions and pooling allow the model to use hundreds of base pairs of context around a locus to predict chromatin state (Zhou and Troyanskaya 2015). ExPecto extends this idea by mapping sequence to tissue-specific expression predictions (Zhou et al. 2018). SpliceAI uses very deep dilated convolutions to reach ~10 kb of context for splicing (Jaganathan et al. 2019).\n\n\nA.4.3 Multi-Task Learning\nEarly sequence-to-function CNNs are almost always multi-task:\n\nA single input sequence is used to predict many outputs simultaneously (e.g., hundreds of TF ChIP–seq peaks, histone marks, DNase hypersensitivity tracks).\n\nShared convolutional layers learn common features, while the final layer has many output units (one per task).\n\nBenefits:\n\nEfficient use of data and compute\n\nBetter regularization: related tasks constrain each other\n\nNatural interface for variant effect prediction: you can see how a mutation affects many functional readouts at once",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#beyond-cnns-recurrent-networks-briefly",
    "href": "app-a-dl.html#beyond-cnns-recurrent-networks-briefly",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.5 Beyond CNNs: Recurrent Networks (Briefly)",
    "text": "A.5 Beyond CNNs: Recurrent Networks (Briefly)\nBefore Transformers dominated sequence modeling, recurrent neural networks (RNNs)—especially LSTMs and GRUs—were the default architecture for language and time series.\nConceptually:\n\nAn RNN processes a sequence one position at a time.\n\nIt maintains a hidden state that is updated as it moves along the sequence.\n\nIn principle, it can capture arbitrarily long-range dependencies.\n\nIn practice, for genomic sequences:\n\nVery long-range dependencies (tens to hundreds of kilobases) are difficult to learn with standard RNNs.\n\nTraining can be slow and unstable on very long sequences.\n\nCNNs and attention-based models have largely displaced RNNs in genomic applications.\n\nYou may still see RNNs in some multi-modal or temporal settings (e.g., modeling longitudinal clinical data), but they are not central to this book’s architectures.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#transformers-and-self-attention",
    "href": "app-a-dl.html#transformers-and-self-attention",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.6 Transformers and Self-Attention",
    "text": "A.6 Transformers and Self-Attention\nTransformers, introduced in natural language processing, have become the dominant architecture for sequence modeling. In this book, they underpin protein language models, DNA language models (DNABERT and successors), and long-range models like Enformer (Ji et al. 2021; Avsec et al. 2021).\n\nA.6.1 The Idea of Self-Attention\nIn a self-attention layer, each position in a sequence can directly “look at” and combine information from every other position.\nFor an input sequence represented as vectors \\(\\{x_1, \\dots, x_L\\}\\):\n\nEach position is mapped to query (\\(q_i\\)), key (\\(k_i\\)), and value (\\(v_i\\)) vectors via learned linear projections.\n\nThe attention weight from position \\(i\\) to position \\(j\\) is:\n\\[\n\\alpha_{ij} \\propto \\exp\\left(\\frac{q_i^\\top k_j}{\\sqrt{d}}\\right),\n\\]\nfollowed by normalization so that \\(\\sum_j \\alpha_{ij} = 1\\).\nThe new representation of position \\(i\\) is a weighted sum of all value vectors:\n\\[\nz_i = \\sum_{j=1}^L \\alpha_{ij} v_j.\n\\]\n\nKey properties:\n\nContent-based: Interactions are determined by similarity of representations, not just distance.\n\nGlobal context: Each position can, in principle, attend to any other position.\n\nPermutation-aware via positional encodings: Additional information (sinusoidal or learned) encodes position so the model knows order.\n\n\n\nA.6.2 Multi-Head Attention and Transformer Blocks\nReal Transformer layers use multi-head attention:\n\nThe model runs self-attention in parallel with multiple sets of \\((Q,K,V)\\) projections (heads).\n\nDifferent heads can specialize in different patterns (e.g., local motif combinations, long-range enhancer–promoter contacts).\n\nA typical Transformer block has:\n\nMulti-head self-attention\n\nAdd & layer normalization\n\nPosition-wise feed-forward network\n\nAnother add & layer normalization\n\nStacking many blocks yields a deep Transformer.\n\n\nA.6.3 Computational Cost and Long-Range Genomics\nNaive self-attention has \\(O(L^2)\\) cost in sequence length \\(L\\). For genomic sequences, where we might want 100 kb–1 Mb contexts, this is expensive.\nLong-range genomic models like Enformer and HyenaDNA address this with:\n\nHybrid designs (CNNs + attention) to reduce sequence length before applying global attention (Avsec et al. 2021).\n\nStructured state space models (SSMs) and related architectures that scale more gracefully with length (Nguyen et al. 2023).\n\nThese details are treated in depth in the long-range modeling chapters; here it suffices to know that Transformers give flexible global context at the cost of higher computational complexity.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#self-supervised-learning-and-pretraining",
    "href": "app-a-dl.html#self-supervised-learning-and-pretraining",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.7 Self-Supervised Learning and Pretraining",
    "text": "A.7 Self-Supervised Learning and Pretraining\nA central theme of this book is pretraining: training a large model once on a broad, unlabeled or weakly-labeled task, then re-using it for many downstream problems.\n\nA.7.1 Supervised vs Self-Supervised\n\nSupervised learning: Each input \\(x\\) comes with a label \\(y\\). Examples:\n\nPredicting chromatin marks from sequence (DeepSEA).\n\nPredicting splice junctions (SpliceAI).\n\nPredicting disease risk from features (?sec-clinical-risk).\n\nSelf-supervised learning: The model learns from raw input data without explicit labels, using some pretext task constructed from the data itself. Examples:\n\nMasked token prediction (BERT-style): hide some nucleotides and train the model to predict them from surrounding context.\n\nNext-token prediction (GPT-style): predict the next base given previous ones.\n\nDenoising or reconstruction tasks.\n\n\nIn genomics, self-supervised models treat DNA sequences as a language and learn from the vast amount of genomic sequence without needing curated labels.\n\n\nA.7.2 Masked Language Modeling on DNA\nDNABERT applied BERT-style masked language modeling to DNA sequences tokenized as overlapping k-mers (Ji et al. 2021). The model:\n\nReads sequences as k-mer tokens.\n\nRandomly masks a subset of tokens.\n\nLearns to predict the masked tokens given surrounding context.\n\nBenefits:\n\nUses essentially unlimited unlabeled genomic data.\n\nLearns rich representations that can be fine-tuned for tasks like promoter prediction, splice site detection, and variant effect prediction.\n\nChapter 11 generalizes this story to broader DNA foundation models, including alternative tokenization schemes and architectures.\n\n\nA.7.3 Pretraining, Fine-Tuning, and Probing\nAfter pretraining, we can use a model in several ways:\n\nFine-tuning: Initialize with pretrained weights, then continue training on a specific downstream task with task-specific labels.\n\nLinear probing: Freeze the pretrained model, extract embeddings, and train a simple linear classifier on top.\n\nPrompting / adapters: Add small task-specific modules (adapters) while keeping most of the model fixed.\n\nThese patterns reappear across protein LMs, DNA LMs, variant effect models, and GFMs in Chapters 9–16.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#foundations-for-evaluation-and-reliability",
    "href": "app-a-dl.html#foundations-for-evaluation-and-reliability",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.8 Foundations for Evaluation and Reliability",
    "text": "A.8 Foundations for Evaluation and Reliability\nWhile the main book has dedicated chapters for evaluation (Chapter 21), confounding (Chapter 22), and clinical metrics (?sec-clinical-risk), it’s useful to have a few basic concepts in mind.\n\nA.8.1 Distribution Shift\nA model is trained under some data distribution (e.g., certain assays, cohorts, ancestries) and then deployed under another (e.g., a different hospital system or population). When these differ, we have distribution shift, which can degrade performance.\nTypical genomic shifts include:\n\nNew sequencing technologies or lab protocols\n\nNew ancestries or populations\n\nNew tissues, diseases, or phenotypes\n\n\n\nA.8.2 Data Leakage\nData leakage occurs when information from the test set “leaks” into training (e.g., through overlapping loci or related individuals), leading to overly optimistic estimates of performance. Chapter 21 and Chapter 22 discuss strategies for leak-resistant splits in detail.\n\n\nA.8.3 Calibration and Uncertainty\nFor many applications, especially in the clinic, we care not just about whether the model is correct, but whether its probabilities are well calibrated and whether we know when the model is uncertain. Calibration and uncertainty quantification are covered in ?sec-clinical-risk; here, the main takeaway is that perfect AUROC does not imply perfect clinical utility.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#a-minimal-recipe-for-a-genomic-deep-learning-project",
    "href": "app-a-dl.html#a-minimal-recipe-for-a-genomic-deep-learning-project",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.9 A Minimal Recipe for a Genomic Deep Learning Project",
    "text": "A.9 A Minimal Recipe for a Genomic Deep Learning Project\nTo make the abstractions more concrete, here is a lightweight “recipe” that roughly mirrors what the case-study chapters do.\n\nDefine the prediction problem\n\nInput: e.g., 1 kb sequence around a variant, or patient-level features.\n\nOutput: e.g., presence of a chromatin mark, change in expression, disease risk.\n\nChoose an input representation\n\nOne-hot encoding or tokenization scheme for sequences (see Chapter 5).\n\nEncodings for variants, genes, or patients (e.g., aggregate from per-variant features).\n\nPick a model family\n\nCNN for local sequence-to-function (Chapters 5–7).\n\nTransformer or SSM for long-range or language model-style tasks (Chapters 8–11).\n\nPretrained GFM + small task-specific head (Chapters 12–16).\n\nSpecify the loss and metrics\n\nCross-entropy for binary classification, MSE for regression, etc.\n\nMetrics like AUROC, AUPRC, correlation, calibration.\n\nSet up data splits and evaluation\n\nDecide whether to split by locus, individual, cohort, or species.\n\nHold out a test set and use validation data to tune hyperparameters.\n\nTrain with regularization and monitoring\n\nUse an optimizer (SGD or Adam-like) with a learning rate schedule.\n\nApply regularization (dropout, weight decay, augmentation).\n\nMonitor training and validation curves for overfitting.\n\nInspect and stress-test\n\nCheck performance across subgroups (e.g., ancestries, assays, cohorts).\n\nUse interpretability tools (Chapter 24) to see what patterns the model is using.\n\nRun robustness checks and ablations.\n\nIterate\n\nAdjust architecture, add more data, refine labels, or incorporate pretrained backbones.\n\nMove from model-centric tuning to system-level considerations (data quality, deployment environment, feedback loops).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-a-dl.html#how-this-primer-connects-to-the-rest-of-the-book",
    "href": "app-a-dl.html#how-this-primer-connects-to-the-rest-of-the-book",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.10 How This Primer Connects to the Rest of the Book",
    "text": "A.10 How This Primer Connects to the Rest of the Book\nThis appendix gives you the minimum vocabulary to navigate the rest of the text:\n\nChapters 5–7 show how CNNs on one-hot sequence learn regulatory code, expression, and splicing.\n\nChapters 8–11 extend these ideas to richer sequence representations, Transformers, and long-range sequence models.\n\nChapters 12–16 frame these models as genomic foundation models, introduce evaluation, interpretability, and multi-omics.\n\nChapters 17–19 show how these ingredients are assembled into clinical, discovery, and biotech applications.\n\nYou don’t need to internalize every detail here. The goal is simply that when you see terms like “convolution,” “attention,” “pretraining,” or “fine-tuning” in the main chapters, they feel like familiar tools rather than mysterious jargon.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html",
    "href": "app-b-compute.html",
    "title": "Appendix B — Model Deployment",
    "section": "",
    "text": "B.1 From Linear Models to Deep Networks\nThis appendix gives a compact introduction to deep learning for readers who are comfortable with genomics but less familiar with modern neural networks. The goal is not to replace a full machine learning textbook, but to provide enough background to make the models in Chapters 5–19 feel intuitive rather than magical.\nWe focus on:\nWhere possible, we connect directly to the genomic case studies in the main text (DeepSEA, ExPecto, SpliceAI, Enformer, genomic language models, and GFMs).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#from-linear-models-to-deep-networks",
    "href": "app-b-compute.html#from-linear-models-to-deep-networks",
    "title": "Appendix B — Model Deployment",
    "section": "",
    "text": "B.1.1 Models as Functions\nAt its core, a predictive model is just a function:\n\\[\nf_\\theta: x \\mapsto \\hat{y}\n\\tag{B.1}\\]\nwhere:\n\n\\(x\\) is an input (e.g., a one-hot encoded DNA sequence, variant-level features, or a patient feature vector).\n\n\\(\\hat{y}\\) is a prediction (e.g., probability of a histone mark, gene expression level, disease risk).\n\n\\(\\theta\\) are the parameters (weights) of the model.\n\nIn classical genomics workflows, \\(f_\\theta\\) might be:\n\nLogistic regression (for case–control status)\n\nLinear regression (for quantitative traits)\n\nRandom forests or gradient boosting (for variant pathogenicity scores)\n\nDeep learning keeps the same basic structure but allows \\(f_\\theta\\) to be a much more flexible, high-capacity function built by composing many simple operations.\n\n\nB.1.2 Linear Models vs Neural Networks\nA simple linear model for classification looks like:\n\\[\n\\hat{y} = \\sigma(w^\\top x + b),\n\\]\nwhere \\(w\\) and \\(b\\) are parameters and \\(\\sigma(\\cdot)\\) is a squashing nonlinearity (e.g., the logistic function). The model draws a single separating hyperplane in feature space.\nA neural network generalizes this by stacking multiple linear transformations with nonlinear activation functions:\n\\[\n\\begin{aligned}\nh_1 &= \\phi(W_1 x + b_1) \\\\\nh_2 &= \\phi(W_2 h_1 + b_2) \\\\\n&\\vdots \\\\\n\\hat{y} &= g(W_L h_{L-1} + b_L)\n\\end{aligned}\n\\]\nwhere:\n\nEach \\(W_\\ell, b_\\ell\\) is a layer’s weight matrix and bias.\n\n\\(\\phi(\\cdot)\\) is a nonlinear activation (e.g., ReLU).\n\n\\(g(\\cdot)\\) is a final activation (e.g., sigmoid for probabilities, identity for regression).\n\nThe key idea:\n\nBy composing many simple nonlinear transformations, deep networks can approximate very complex functions.\n\nIn Chapters 5–7, DeepSEA, ExPecto, and SpliceAI implement exactly this pattern, but with convolutional layers (Section 4) tailored to 1D DNA sequence instead of dense matrix multiplications (Zhou and Troyanskaya 2015; Zhou et al. 2018; Jaganathan et al. 2019).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#training-deep-models",
    "href": "app-b-compute.html#training-deep-models",
    "title": "Appendix B — Model Deployment",
    "section": "B.2 Training Deep Models",
    "text": "B.2 Training Deep Models\n\nB.2.1 Data, Labels, and Loss Functions\nTo train a model, we need:\n\nA dataset of examples \\(\\{(x_i, y_i)\\}_{i=1}^N\\)\n\nA model \\(f_\\theta\\)\n\nA loss function \\(L(\\hat{y}, y)\\) that measures how wrong a prediction is\n\nCommon loss functions:\n\nBinary cross-entropy (for yes/no labels, e.g., “is this ChIP–seq peak present?”):\n\\[\nL(\\hat{p}, y) = -\\big(y \\log \\hat{p} + (1-y)\\log(1-\\hat{p})\\big)\n\\]\nMulticlass cross-entropy (for one-of-K labels)\n\nMean squared error (MSE) (for continuous outputs, e.g., gene expression)\n\nThe training objective is to find \\(\\theta\\) that minimizes the average loss:\n\\[\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N L\\big(f_\\theta(x_i), y_i\\big).\n\\]\n\n\nB.2.2 2.2 Gradient-Based Optimization\nDeep networks may have millions to billions of parameters. We can’t search over all possibilities, but we can follow the gradient of the loss with respect to \\(\\theta\\):\n\nGradient descent updates: \\[\n\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta),\n\\] where \\(\\eta\\) is the learning rate.\n\nIn practice, we use:\n\nMini-batch stochastic gradient descent (SGD): Compute gradients on small batches of examples (e.g., 128 sequences at a time) for efficiency and better generalization.\nAdaptive optimizers like Adam, which adjust learning rates per parameter.\n\nYou never compute gradients by hand; modern frameworks (PyTorch, JAX, TensorFlow) use automatic differentiation to efficiently compute \\(\\nabla_\\theta \\mathcal{L}\\) even for very complex architectures.\n\n\nB.2.3 Backpropagation in One Sentence\nBackpropagation is just the chain rule of calculus applied efficiently through the layers of a network. It propagates “blame” from the output back to each weight, telling us how changing that weight would change the loss.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#generalization-overfitting-and-evaluation",
    "href": "app-b-compute.html#generalization-overfitting-and-evaluation",
    "title": "Appendix B — Model Deployment",
    "section": "B.3 Generalization, Overfitting, and Evaluation",
    "text": "B.3 Generalization, Overfitting, and Evaluation\n\nB.3.1 Train / Validation / Test Splits\nDeep networks can memorize training data if we’re not careful. To evaluate generalization, we typically split data into:\n\nTraining set – used to fit parameters\n\nValidation set – used to tune hyperparameters (learning rate, depth, etc.) and perform early stopping\n\nTest set – held out until the end to estimate performance on new data\n\nIn genomics, how we split matters as much as how much data we have:\n\nSplitting by locus or chromosome (to test cross-locus generalization)\n\nSplitting by individual or cohort (to avoid leakage between related samples)\n\nSplitting by species or ancestry when evaluating transfer\n\nThese issues are developed in more depth in the evaluation and confounding chapters (Chapter 21 and Chapter 22).\n\n\nB.3.2 Overfitting and Regularization\nSigns of overfitting:\n\nTraining loss keeps decreasing, but validation loss starts increasing.\n\nMetrics like AUROC or AUPRC plateau or drop on validation data even as they improve on training data.\n\nCommon regularization techniques:\n\nWeight decay / L2 regularization – penalize large weights.\n\nDropout – randomly zero out activations during training.\n\nEarly stopping – stop training when validation performance stops improving.\n\nData augmentation – generate more training examples by transforming inputs, e.g.:\n\nReverse-complement augmentation for DNA sequences (treat sequence and its reverse complement as equivalent).\n\nWindow jittering: randomly shifting the sequence window around a target site.\n\n\n\n\nB.3.3 Basic Metrics\nYou’ll encounter metrics such as:\n\nAUROC (Area Under the ROC Curve) – how well the model ranks positives above negatives.\n\nAUPRC (Area Under the Precision–Recall Curve) – more informative when positives are rare.\n\nCalibration metrics (e.g., Brier score) and reliability diagrams – especially for clinical risk prediction (?sec-clinical-risk).\n\nThe model and application chapters provide details about which metrics are appropriate for which tasks. See Chapter 21 for more on evaluation metrics.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#convolutional-networks-for-genomic-sequences",
    "href": "app-b-compute.html#convolutional-networks-for-genomic-sequences",
    "title": "Appendix B — Model Deployment",
    "section": "B.4 Convolutional Networks for Genomic Sequences",
    "text": "B.4 Convolutional Networks for Genomic Sequences\nConvolutional neural networks (CNNs) are the workhorse architecture in early genomic deep learning models like DeepSEA, ExPecto, and SpliceAI (Zhou and Troyanskaya 2015; Zhou et al. 2018; Jaganathan et al. 2019).\n\nB.4.1 1D Convolutions as Motif Detectors\nFor a 1D DNA sequence encoded as a matrix \\(X \\in \\mathbb{R}^{L \\times 4}\\) (length \\(L\\), 4 nucleotides), a convolutional layer applies a set of filters (kernels) of width \\(k\\):\n\nEach filter is a small matrix \\(K \\in \\mathbb{R}^{k \\times 4}\\).\n\nAt each position, the filter computes a dot product between \\(K\\) and the corresponding \\(k\\)-length chunk of \\(X\\).\n\nSliding the filter along the sequence creates an activation map that is high wherever the motif encoded by \\(K\\) is present.\n\nIntuitively:\n\nA 1D convolutional filter learns to recognize sequence motifs (e.g., transcription factor binding sites) directly from data.\n\n\n\nB.4.2 Stacking Layers and Receptive Fields\nDeeper convolutional layers allow the model to “see” longer-range patterns:\n\nFirst layer: short motifs (e.g., 8–15 bp).\n\nHigher layers: combinations of motifs, motif spacing, and local regulatory grammar.\n\nPooling layers (e.g., max pooling) reduce spatial resolution while aggregating features, increasing the receptive field.\n\nIn DeepSEA, stacked convolutions and pooling allow the model to use hundreds of base pairs of context around a locus to predict chromatin state (Zhou and Troyanskaya 2015). ExPecto extends this idea by mapping sequence to tissue-specific expression predictions (Zhou et al. 2018). SpliceAI uses very deep dilated convolutions to reach ~10 kb of context for splicing (Jaganathan et al. 2019).\n\n\nB.4.3 Multi-Task Learning\nEarly sequence-to-function CNNs are almost always multi-task:\n\nA single input sequence is used to predict many outputs simultaneously (e.g., hundreds of TF ChIP–seq peaks, histone marks, DNase hypersensitivity tracks).\n\nShared convolutional layers learn common features, while the final layer has many output units (one per task).\n\nBenefits:\n\nEfficient use of data and compute\n\nBetter regularization: related tasks constrain each other\n\nNatural interface for variant effect prediction: you can see how a mutation affects many functional readouts at once",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#beyond-cnns-recurrent-networks-briefly",
    "href": "app-b-compute.html#beyond-cnns-recurrent-networks-briefly",
    "title": "Appendix B — Model Deployment",
    "section": "B.5 Beyond CNNs: Recurrent Networks (Briefly)",
    "text": "B.5 Beyond CNNs: Recurrent Networks (Briefly)\nBefore Transformers dominated sequence modeling, recurrent neural networks (RNNs)—especially LSTMs and GRUs—were the default architecture for language and time series.\nConceptually:\n\nAn RNN processes a sequence one position at a time.\n\nIt maintains a hidden state that is updated as it moves along the sequence.\n\nIn principle, it can capture arbitrarily long-range dependencies.\n\nIn practice, for genomic sequences:\n\nVery long-range dependencies (tens to hundreds of kilobases) are difficult to learn with standard RNNs.\n\nTraining can be slow and unstable on very long sequences.\n\nCNNs and attention-based models have largely displaced RNNs in genomic applications.\n\nYou may still see RNNs in some multi-modal or temporal settings (e.g., modeling longitudinal clinical data), but they are not central to this book’s architectures.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#transformers-and-self-attention",
    "href": "app-b-compute.html#transformers-and-self-attention",
    "title": "Appendix B — Model Deployment",
    "section": "B.6 Transformers and Self-Attention",
    "text": "B.6 Transformers and Self-Attention\nTransformers, introduced in natural language processing, have become the dominant architecture for sequence modeling. In this book, they underpin protein language models, DNA language models (DNABERT and successors), and long-range models like Enformer (Ji et al. 2021; Avsec et al. 2021).\n\nB.6.1 The Idea of Self-Attention\nIn a self-attention layer, each position in a sequence can directly “look at” and combine information from every other position.\nFor an input sequence represented as vectors \\(\\{x_1, \\dots, x_L\\}\\):\n\nEach position is mapped to query (\\(q_i\\)), key (\\(k_i\\)), and value (\\(v_i\\)) vectors via learned linear projections.\n\nThe attention weight from position \\(i\\) to position \\(j\\) is:\n\\[\n\\alpha_{ij} \\propto \\exp\\left(\\frac{q_i^\\top k_j}{\\sqrt{d}}\\right),\n\\]\nfollowed by normalization so that \\(\\sum_j \\alpha_{ij} = 1\\).\nThe new representation of position \\(i\\) is a weighted sum of all value vectors:\n\\[\nz_i = \\sum_{j=1}^L \\alpha_{ij} v_j.\n\\]\n\nKey properties:\n\nContent-based: Interactions are determined by similarity of representations, not just distance.\n\nGlobal context: Each position can, in principle, attend to any other position.\n\nPermutation-aware via positional encodings: Additional information (sinusoidal or learned) encodes position so the model knows order.\n\n\n\nB.6.2 Multi-Head Attention and Transformer Blocks\nReal Transformer layers use multi-head attention:\n\nThe model runs self-attention in parallel with multiple sets of \\((Q,K,V)\\) projections (heads).\n\nDifferent heads can specialize in different patterns (e.g., local motif combinations, long-range enhancer–promoter contacts).\n\nA typical Transformer block has:\n\nMulti-head self-attention\n\nAdd & layer normalization\n\nPosition-wise feed-forward network\n\nAnother add & layer normalization\n\nStacking many blocks yields a deep Transformer.\n\n\nB.6.3 Computational Cost and Long-Range Genomics\nNaive self-attention has \\(O(L^2)\\) cost in sequence length \\(L\\). For genomic sequences, where we might want 100 kb–1 Mb contexts, this is expensive.\nLong-range genomic models like Enformer and HyenaDNA address this with:\n\nHybrid designs (CNNs + attention) to reduce sequence length before applying global attention (Avsec et al. 2021).\n\nStructured state space models (SSMs) and related architectures that scale more gracefully with length (Nguyen et al. 2023).\n\nThese details are treated in depth in the long-range modeling chapters; here it suffices to know that Transformers give flexible global context at the cost of higher computational complexity.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#self-supervised-learning-and-pretraining",
    "href": "app-b-compute.html#self-supervised-learning-and-pretraining",
    "title": "Appendix B — Model Deployment",
    "section": "B.7 Self-Supervised Learning and Pretraining",
    "text": "B.7 Self-Supervised Learning and Pretraining\nA central theme of this book is pretraining: training a large model once on a broad, unlabeled or weakly-labeled task, then re-using it for many downstream problems.\n\nB.7.1 Supervised vs Self-Supervised\n\nSupervised learning: Each input \\(x\\) comes with a label \\(y\\). Examples:\n\nPredicting chromatin marks from sequence (DeepSEA).\n\nPredicting splice junctions (SpliceAI).\n\nPredicting disease risk from features (?sec-clinical-risk).\n\nSelf-supervised learning: The model learns from raw input data without explicit labels, using some pretext task constructed from the data itself. Examples:\n\nMasked token prediction (BERT-style): hide some nucleotides and train the model to predict them from surrounding context.\n\nNext-token prediction (GPT-style): predict the next base given previous ones.\n\nDenoising or reconstruction tasks.\n\n\nIn genomics, self-supervised models treat DNA sequences as a language and learn from the vast amount of genomic sequence without needing curated labels.\n\n\nB.7.2 Masked Language Modeling on DNA\nDNABERT applied BERT-style masked language modeling to DNA sequences tokenized as overlapping k-mers (Ji et al. 2021). The model:\n\nReads sequences as k-mer tokens.\n\nRandomly masks a subset of tokens.\n\nLearns to predict the masked tokens given surrounding context.\n\nBenefits:\n\nUses essentially unlimited unlabeled genomic data.\n\nLearns rich representations that can be fine-tuned for tasks like promoter prediction, splice site detection, and variant effect prediction.\n\nChapter 11 generalizes this story to broader DNA foundation models, including alternative tokenization schemes and architectures.\n\n\nB.7.3 Pretraining, Fine-Tuning, and Probing\nAfter pretraining, we can use a model in several ways:\n\nFine-tuning: Initialize with pretrained weights, then continue training on a specific downstream task with task-specific labels.\n\nLinear probing: Freeze the pretrained model, extract embeddings, and train a simple linear classifier on top.\n\nPrompting / adapters: Add small task-specific modules (adapters) while keeping most of the model fixed.\n\nThese patterns reappear across protein LMs, DNA LMs, variant effect models, and GFMs in Chapters 9–16.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#foundations-for-evaluation-and-reliability",
    "href": "app-b-compute.html#foundations-for-evaluation-and-reliability",
    "title": "Appendix B — Model Deployment",
    "section": "B.8 Foundations for Evaluation and Reliability",
    "text": "B.8 Foundations for Evaluation and Reliability\nWhile the main book has dedicated chapters for evaluation (Chapter 21), confounding (Chapter 22), and clinical metrics (?sec-clinical-risk), it’s useful to have a few basic concepts in mind.\n\nB.8.1 Distribution Shift\nA model is trained under some data distribution (e.g., certain assays, cohorts, ancestries) and then deployed under another (e.g., a different hospital system or population). When these differ, we have distribution shift, which can degrade performance.\nTypical genomic shifts include:\n\nNew sequencing technologies or lab protocols\n\nNew ancestries or populations\n\nNew tissues, diseases, or phenotypes\n\n\n\nB.8.2 Data Leakage\nData leakage occurs when information from the test set “leaks” into training (e.g., through overlapping loci or related individuals), leading to overly optimistic estimates of performance. Chapter 21 and Chapter 22 discuss strategies for leak-resistant splits in detail.\n\n\nB.8.3 Calibration and Uncertainty\nFor many applications, especially in the clinic, we care not just about whether the model is correct, but whether its probabilities are well calibrated and whether we know when the model is uncertain. Calibration and uncertainty quantification are covered in ?sec-clinical-risk; here, the main takeaway is that perfect AUROC does not imply perfect clinical utility.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#a-minimal-recipe-for-a-genomic-deep-learning-project",
    "href": "app-b-compute.html#a-minimal-recipe-for-a-genomic-deep-learning-project",
    "title": "Appendix B — Model Deployment",
    "section": "B.9 A Minimal Recipe for a Genomic Deep Learning Project",
    "text": "B.9 A Minimal Recipe for a Genomic Deep Learning Project\nTo make the abstractions more concrete, here is a lightweight “recipe” that roughly mirrors what the case-study chapters do.\n\nDefine the prediction problem\n\nInput: e.g., 1 kb sequence around a variant, or patient-level features.\n\nOutput: e.g., presence of a chromatin mark, change in expression, disease risk.\n\nChoose an input representation\n\nOne-hot encoding or tokenization scheme for sequences (see Chapter 5).\n\nEncodings for variants, genes, or patients (e.g., aggregate from per-variant features).\n\nPick a model family\n\nCNN for local sequence-to-function (Chapters 5–7).\n\nTransformer or SSM for long-range or language model-style tasks (Chapters 8–11).\n\nPretrained GFM + small task-specific head (Chapters 12–16).\n\nSpecify the loss and metrics\n\nCross-entropy for binary classification, MSE for regression, etc.\n\nMetrics like AUROC, AUPRC, correlation, calibration.\n\nSet up data splits and evaluation\n\nDecide whether to split by locus, individual, cohort, or species.\n\nHold out a test set and use validation data to tune hyperparameters.\n\nTrain with regularization and monitoring\n\nUse an optimizer (SGD or Adam-like) with a learning rate schedule.\n\nApply regularization (dropout, weight decay, augmentation).\n\nMonitor training and validation curves for overfitting.\n\nInspect and stress-test\n\nCheck performance across subgroups (e.g., ancestries, assays, cohorts).\n\nUse interpretability tools (Chapter 24) to see what patterns the model is using.\n\nRun robustness checks and ablations.\n\nIterate\n\nAdjust architecture, add more data, refine labels, or incorporate pretrained backbones.\n\nMove from model-centric tuning to system-level considerations (data quality, deployment environment, feedback loops).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-b-compute.html#how-this-primer-connects-to-the-rest-of-the-book",
    "href": "app-b-compute.html#how-this-primer-connects-to-the-rest-of-the-book",
    "title": "Appendix B — Model Deployment",
    "section": "B.10 How This Primer Connects to the Rest of the Book",
    "text": "B.10 How This Primer Connects to the Rest of the Book\nThis appendix gives you the minimum vocabulary to navigate the rest of the text:\n\nChapters 5–7 show how CNNs on one-hot sequence learn regulatory code, expression, and splicing.\n\nChapters 8–11 extend these ideas to richer sequence representations, Transformers, and long-range sequence models.\n\nChapters 12–16 frame these models as genomic foundation models, introduce evaluation, interpretability, and multi-omics.\n\nChapters 17–19 show how these ingredients are assembled into clinical, discovery, and biotech applications.\n\nYou don’t need to internalize every detail here. The goal is simply that when you see terms like “convolution,” “attention,” “pretraining,” or “fine-tuning” in the main chapters, they feel like familiar tools rather than mysterious jargon.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Model Deployment</span>"
    ]
  },
  {
    "objectID": "app-c-data-curation.html",
    "href": "app-c-data-curation.html",
    "title": "Appendix C — Training Data Curation",
    "section": "",
    "text": "Label: @sec-apx-data-curation\nOld source: NEW\nPurpose: Guide to constructing training sets for genomic FMs\nContent: - Data sources and access - Quality filtering strategies - Deduplication and redundancy reduction - Contamination detection - Data provenance and versioning - Bias assessment",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Training Data Curation</span>"
    ]
  },
  {
    "objectID": "app-d-models.html",
    "href": "app-d-models.html",
    "title": "Appendix D — Referenced Models",
    "section": "",
    "text": "D.1 Category Definitions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Referenced Models</span>"
    ]
  },
  {
    "objectID": "app-d-models.html#category-definitions",
    "href": "app-d-models.html#category-definitions",
    "title": "Appendix D — Referenced Models",
    "section": "",
    "text": "DNA LM: DNA language models using self-supervised pretraining on genomic sequences\nPLM: Protein language models trained on protein sequences\nSeq→Func: Supervised sequence-to-function models predicting chromatin/expression from DNA\nSplice: Specialized splice site prediction models\nVEP: Variant effect predictors (various paradigms)\nGFM: Genomic foundation model (broad, reusable representations)\nPGS: Polygenic score or risk prediction models\nGNN: Graph neural network for gene/pathway analysis\n\n\n\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. “[AlphaFold3] Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky, Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev. 2010. “A Method and Server for Predicting Damaging Missense Mutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024. “[DeepRVAT] Integration of Variant Annotations Using Deep Set Networks Boosts Rare Variant Association Testing.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper, Arend Sidow, and Serafim Batzoglou. 2010. “Identifying a High Fraction of the Human Genome to Be Under Selective Constraint Using GERP++.” PLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. “Delphi: A Deep-Learning Method for Polygenic Risk Prediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “[AlphaFold2] Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nKelley, David R. 2020. “[Basenji2] Cross-Species Regulatory Sequence Activity Prediction.” PLOS Computational Biology 16 (7): e1008050. https://doi.org/10.1371/journal.pcbi.1008050.\n\n\nKelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y. McLean, and Jasper Snoek. 2018. “[Basenji2] Sequential Regulatory Activity Prediction Across Chromosomes with Convolutional Neural Networks.” Genome Research 28 (5): 739–50. https://doi.org/10.1101/gr.227819.117.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam, Amit R. Majithia, and Trey Ideker. 2025. “[G2PT] A Genotype-Phenotype Transformer to Assess and Explain Polygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Hao, Zebei Han, Yu Sun, Fu Wang, Pengzhen Hu, Yuang Gao, Xuemei Bai, et al. 2024. “CGMega: Explainable Graph Neural Network Framework with Attention Mechanisms for Cancer Gene Module Dissection.” Nature Communications 15 (1): 5997. https://doi.org/10.1038/s41467-024-50426-6.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and Yunping Zhu. 2022. “MoGCN: A Multi-Omics Integration Method Based on Graph Convolutional Network for Cancer Subtype Analysis.” Frontiers in Genetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT: Predicting Amino Acid Changes That Affect Protein Function.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs, Minmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005. “[PhastCons] Evolutionarily Conserved Elements in Vertebrate, Insect, Worm, and Yeast Genomes.” Genome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.\n\n\nYeo, Gene, and Christopher B. Burge. 2004. “Maximum Entropy Modeling of Short Sequence Motifs with Applications to RNA Splicing Signals.” Journal of Computational Biology 11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Referenced Models</span>"
    ]
  },
  {
    "objectID": "app-e-resources.html",
    "href": "app-e-resources.html",
    "title": "Appendix E — Additional Resources",
    "section": "",
    "text": "E.1 Genomics & Human Genetics",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "app-e-resources.html#genomics-human-genetics",
    "href": "app-e-resources.html#genomics-human-genetics",
    "title": "Appendix E — Additional Resources",
    "section": "",
    "text": "Thompson & Thompson Genetics and Genomics in Medicine (9th ed.)\nRonald Cohn, Stephen Scherer, Ada Hamosh. Clinical-focused overview of human genetics and genomics for medicine, great for grounding in clinical genomics.\nHuman Molecular Genetics (5th ed.)\nTom Strachan, Andrew Read. Higher-level molecular genetics/genomics text with strong coverage of mechanisms, technologies, and disease applications.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "app-e-resources.html#immunology",
    "href": "app-e-resources.html#immunology",
    "title": "Appendix E — Additional Resources",
    "section": "E.2 Immunology",
    "text": "E.2 Immunology\n\nJaneway’s Immunobiology (10th ed.)\nKenneth M. Murphy, Casey Weaver, Leslie J. Berg. Standard comprehensive immunology textbook, excellent for understanding immune system biology relevant to genomics and disease.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "app-e-resources.html#machine-learning-deep-learning",
    "href": "app-e-resources.html#machine-learning-deep-learning",
    "title": "Appendix E — Additional Resources",
    "section": "E.3 Machine Learning & Deep Learning",
    "text": "E.3 Machine Learning & Deep Learning\n\nDeep Learning\nIan Goodfellow, Yoshua Bengio, Aaron Courville. Comprehensive deep learning textbook; free online: https://www.deeplearningbook.org/\nDive into Deep Learning (D2L)\nAston Zhang et al. Interactive deep learning book with Jupyter notebooks and multi-framework code; free online: https://d2l.ai/\nAn Introduction to Statistical Learning (ISLR, 2nd ed.)\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Gentle introduction to statistical learning methods used in ML, available free online: https://www.statlearning.com/\nThe Elements of Statistical Learning (ESL)\nTrevor Hastie, Robert Tibshirani, Jerome Friedman. More advanced, theory-heavy companion to ISLR; free PDF: https://hastie.su.domains/ElemStatLearn/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Additional Resources</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html",
    "href": "app-f-glossary.html",
    "title": "Appendix F — Glossary",
    "section": "",
    "text": "F.1 CH 01",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-01",
    "href": "app-f-glossary.html#ch-01",
    "title": "Appendix F — Glossary",
    "section": "",
    "text": "F.1.1 Sequencing Technologies & Data\n\nNext-generation sequencing (NGS)\nHigh-throughput DNA sequencing technologies that allow rapid…stretches of DNA, producing millions of short reads in parallel….\n\n\nIllumina sequencing\nA widely used NGS technology that utilizes reversible dye-terminators to sequence DNA by synthesis\n\n\nShort reads / Paired-end reads\nDNA sequences generated by NGS technologies, typically rangi… a DNA fragment, providing additional information for alignment.\n\n\nLong-read sequencing (PacBio HiFi, Oxford Nanopore)\nDNA sequencing technologies that produce longer reads, typic…ases, allowing for better resolution of complex genomic regions.\n\n\nCircular consensus sequencing\nA sequencing method used by PacBio to generate highly accura… long reads by repeatedly sequencing the same DNA molecule.\n\n\nBase calling\nThe process of determining the nucleotide sequence from raw sequencing data.\n\n\nFASTQ\nA file format that stores both nucleotide sequences and their corresponding quality scores.\n\n\nRead depth / Coverage (e.g., 30×, 100×)\nThe number of times a particular nucleotide is sequenced, indicating the reliability of the sequencing data.\n\n\n\nF.1.2 Targeting Strategies\n\nTargeted gene panel\nA sequencing approach that focuses on a specific set of gene…or cost-effective analysis of known disease-associated variants.\n\n\nWhole-exome sequencing (WES)\nA sequencing approach that targets all protein-coding region…the genome, providing a comprehensive view of coding variants.\n\n\nWhole-genome sequencing (WGS)\nA sequencing approach that captures the entire genome, inclu…ng regions, providing the most comprehensive view of genetic var\n\n\nCapture efficiency\nThe effectiveness of a targeted sequencing approach in enric…interest, impacting the overall quality and coverage of the data.\n\n\n\nF.1.3 Alignment & Processing\n\nRead alignment / Mapping\nThe process of aligning sequencing reads to a reference genome to determine their genomic origin.\n\n\nSeed-and-extend alignment\nAn algorithmic approach for read alignment that first identi…ences (seeds) and then extends the alignment around these seeds.\n\n\nPCR duplicates\nIdentical sequencing reads that originate from the same DNA …esulting from PCR amplification, which can bias variant calling.\n\n\nBase quality score recalibration (BQSR)\nA process that adjusts the quality scores of sequencing read…or systematic errors made by the sequencer.\n\n\nMapping quality\nA measure of the confidence that a read is correctly aligned to the reference genome.\n\n\nReference bias\nThe tendency for sequencing and alignment processes to preferentially detect alleles present in the reference genome.\n\n\n\nF.1.4 Variant Calling\n\nVariant calling\nThe process of identifying variants from sequencing data by comparing it to a reference genome.\n\n\nGenotype likelihood\nThe probability of observing the sequencing data given a particular genotype.\n\n\nPair-HMM (pair hidden Markov model)\nA statistical model used in variant calling to calculate the likelihood of different alignments between reads and the reference genome.\n\n\nJoint genotyping / Cohort calling\nThe process of simultaneously calling variants across multiple samples to improve accuracy and consistency.\n\n\ngVCF (genomic VCF)\nA variant call format that includes information about both va…sites, allowing for joint genotyping.\n\n\nVCF (variant call format)\nA standardized file format for storing variant information, including SNPs, indels, and structural variants.\n\n\nVQSR (Variant Quality Score Recalibration)\nA method for improving the accuracy of variant calls by mode…lationship between variant quality scores and various annotatio\n\n\nPileup\nA summary of the base calls at each position in a set of align…ing reads, used for variant calling and visualization.\n\n\n\nF.1.5 Phasing\n\nHaplotype phasing\nThe process of determining which variants are inherited together on the same chromosome.\n\n\nRead-backed phasing\nA method of phasing that uses sequencing reads that span multiple variants to determine their phase.\n\n\nStatistical phasing\nA method of phasing that uses population-level genotype data and statistical models to infer haplotypes.\n\n\nCompound heterozygosity\nThe presence of two different variants at a particular gene locus, one on each chromosome of a pair.\n\n\nCis vs. trans configuration\nDescribes the relative arrangement of two variants on the same chromosome (cis) or on different chromosomes (trans).\n\n\n\nF.1.6 Variant Types\n\nSNV (single nucleotide variant)\nA variation in a single nucleotide that occurs at a specific position in the genome.\n\n\nIndel\nAn insertion or deletion of bases in the genome of an organism.\n\n\nStructural variant\nA large-scale alteration in the genome, such as a deletion, duplication, inversion, or translocation.\n\n\nMulti-nucleotide variant (MNV)\nA variation that affects multiple consecutive nucleotides in the genome.\n\n\nMosaic variant\nA genetic variant that is present in some but not all cells of an organism, often arising during development.\n\n\nSomatic variant\nA genetic variant that occurs in non-germline cells and is not inherited, often associated with cancer.\n\n\nGermline variant\nA genetic variant that is present in the egg or sperm and can be passed on to offspring.\n\n\nDe novo variant\nA genetic variant that arises spontaneously in an individual and is not inherited from either parent.\n\n\n\nF.1.7 Difficult Regions\n\nSegmental duplication\nLarge, highly similar sequences in the genome that can complicate read alignment and variant calling.\n\n\nParalog / Paralogous gene\nA gene that is related to another gene in the same organism due to a duplication event.\n\n\nHomopolymer\nA sequence of identical nucleotides in a row, which can be prone to sequencing errors.\n\n\nLow-complexity region\nA region of the genome with a simple sequence composition, which can be challenging for alignment and variant calling.\n\n\nHLA region / MHC\nThe human leukocyte antigen (HLA) region, also known as the major histocompatibility complex (MHC), is a highly polymorphic region involved in immune response.\n\n\n\nF.1.8 Benchmarking\n\nPrecision (positive predictive value)\nThe proportion of true positive variant calls among all positive calls.\n\n\nRecall (sensitivity)\nThe proportion of true positive variant calls detected among all actual variants.\n\n\nF1 score\nThe harmonic mean of precision and recall, providing a single metric for evaluating variant calling performance.\n\n\nTrue positive (TP) / False positive (FP) / False negative (FN)\nMetrics used to evaluate the accuracy of variant calls, where TP represents correctly identified variants, FP represents incorrectly identified variants, and FN represents missed variants.\n\n\nHigh-confidence region\nA region of the genome where variant calls are considered to b… reliable, often used for benchmarking and validation.\n\n\n\nF.1.9 Key Resources/Tools (may warrant brief glossary entries)\n\nGIAB (Genome in a Bottle)\nA consortium that develops reference materials and data for benchmarking genome sequencing and variant calling.\n\n\nDeepVariant\nA deep learning-based variant caller developed by Google that identifies genetic variants from sequencing data.\n\n\nGLnexus\nA tool for joint variant calling across multiple samples, designed to work with DeepVariant outputs.\n\n\nHaplotypeCaller\nA variant caller from the Genome Analysis Toolkit (GATK) that uses local de-novo assembly of haplotypes to call variants.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-02",
    "href": "app-f-glossary.html#ch-02",
    "title": "Appendix F — Glossary",
    "section": "F.2 CH 02",
    "text": "F.2 CH 02\n\nF.2.1 Reference & Coordinate Systems\n\nReference genome/assembly\nA digital nucleic acid sequence database, assembled as a repre…example of a species’ set of genes. Multiple versions exist.\n\n\nGRCh37\nThe 37th version of the Genome Reference Consortium human genome assembly.\n\n\nGRCh38\nThe 38th version of the Genome Reference Consortium human genome assembly.\n\n\nT2T-CHM13\nThe Telomere-to-Telomere (T2T) CHM13 human genome assembly, r…ting a complete, gapless sequence of a human genome.\n\n\nPangenome reference\nA reference that represents the genetic diversity of a species, rather than a single individual.\n\n\nGene model\nA representation of the structure of a gene, including its exons, introns, and regulatory elements.\n\n\nCanonical transcript\nThe most biologically relevant transcript of a gene, often used as the reference for annotation.\n\n\nAlternative transcript/isoform\nDifferent versions of a transcript produced from the same gene due to alternative splicing or other mechanisms.\n\n\nMANE Select\nMatched Annotation from NCBI and EMBL-EBI (MANE) Select is a …cripts that are consistently annotated across databases.\n\n\n\nF.2.2 Variant Types & Properties\n\nAllele frequency\nThe proportion of a specific allele among all alleles of a gene in a population.\n\n\nMAF (minor allele frequency)\nThe frequency at which the less common allele occurs in a given population.\n\n\nrsID\nA unique identifier assigned to a single nucleotide polymorphism (SNP) in the dbSNP database.\n\n\nLoss-of-function (LoF) variant\nA genetic variant that results in reduced or abolished protein function.\n\n\nUltra-rare variant\nA genetic variant that is extremely uncommon in the population, often with a frequency of less than 0.01%.\n\n\n\nF.2.3 Population Genetics Metrics\n\nLinkage disequilibrium\nA non-random association of alleles at different loci in a given population.\n\n\npLI (probability of being loss-of-function intolerant)\nA metric that estimates the likelihood that a gene is intolerant to loss-of-function variants.\n\n\nLOEUF (loss-of-function observed/expected upper bound fraction)\nA metric that quantifies the observed versus expected number of loss-of-function variants in a gene.\n\n\nConstraint metrics\nMetrics that assess the tolerance of a gene to functional genetic variation.\n\n\nImputation\nThe process of inferring unobserved genotypes in a study sample based on observed genotypes and a reference panel.\n\n\n\nF.2.4 Functional Genomics\n\nChIP-seq\nChromatin Immunoprecipitation followed by sequencing, a method used to analyze protein-DNA interactions.\n\n\nDNase-seq\nA method to identify regions of open chromatin by sequencing DNA fragments generated by DNase I digestion.\n\n\nATAC-seq\nAssay for Transposase-Accessible Chromatin using sequencing, a technique to study chromatin accessibility.\n\n\nHi-C\nA method to study the three-dimensional architecture of genomes by capturing chromatin interactions.\n\n\nChromatin accessibility\nThe degree to which DNA is exposed and available for binding by proteins, often assessed by DNase-seq or ATAC-seq.\n\n\nHistone modification\nChemical modifications to histone proteins that can influence chromatin structure and gene expression.\n\n\nPeak calling\nThe process of identifying regions of the genome with signific…ment of sequencing reads, often used in ChIP-seq and ATAC-seq analyses.\n\n\nSignal track\nA graphical representation of sequencing data across the genom… intensity of signals such as read coverage or enrichment.\n\n\n\nF.2.5 Expression Genetics\n\neQTL (expression quantitative trait locus)\nA genomic locus that explains variation in gene expression levels.\n\n\nSplicing QTL\nA genomic locus that affects the splicing of pre-mRNA.\n\n\nMolecular QTL\nA quantitative trait locus that influences molecular traits such as gene expression, protein levels, or metabolite concentrations.\n\n\nCis-regulatory\nReferring to regulatory elements, such as promoters or enhanc…ated on the same molecule of DNA as the gene they regulate.\n\n\nColocalization\nThe occurrence of two or more genetic signals at the same genomic location, suggesting a shared causal variant.\n\n\nDropout (single-cell context)\nThe failure to detect a transcript in a single-cell RNA-seq ex…often due to low mRNA capture efficiency.\n\n\n\nF.2.6 Clinical Interpretation\n\nACMG/AMP criteria\nA set of guidelines developed by the American College of Medic…ogy (AMP) for the interpretation of sequence variants. These c…vide a framework for classifying variants into categories such path\n\n\nPathogenicity\nThe ability of a genetic variant to cause disease.\n\n\nHaploinsufficiency\nA condition in which a single functional copy of a gene is in…maintain normal function, leading to a disease phenotype.\n\n\nTriplosensitivity\nA condition in which an extra copy of a gene leads to a disease phenotype.\n\n\nGene-disease validity\nThe strength of evidence supporting a relationship between a gene and a disease.\n\n\nPharmacogenomics\nThe study of how genetic variation affects an individual’s response to drugs.\n\n\nDiplotype\nThe combination of alleles at multiple loci on a single chromosome that are inherited together.\n\n\n\nF.2.7 Study Designs & Statistics\n\nGWAS summary statistics\nAggregated data from genome-wide association studies, typicall…ormation on the association between genetic variants and traits across the genome.\n\n\nFine-mapping\nThe process of identifying the specific causal variants within…omic region associated with a trait.\n\n\nEffect size\nA measure of the strength of the relationship between a genetic variant and a trait.\n\n\nAscertainment bias\nA systematic distortion in the estimation of genetic effects d…non-random sampling of individuals or variants.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-03",
    "href": "app-f-glossary.html#ch-03",
    "title": "Appendix F — Glossary",
    "section": "F.3 CH 03",
    "text": "F.3 CH 03",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-04",
    "href": "app-f-glossary.html#ch-04",
    "title": "Appendix F — Glossary",
    "section": "F.4 CH 04",
    "text": "F.4 CH 04",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-05",
    "href": "app-f-glossary.html#ch-05",
    "title": "Appendix F — Glossary",
    "section": "F.5 CH 05",
    "text": "F.5 CH 05",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-06",
    "href": "app-f-glossary.html#ch-06",
    "title": "Appendix F — Glossary",
    "section": "F.6 CH 06",
    "text": "F.6 CH 06",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-07",
    "href": "app-f-glossary.html#ch-07",
    "title": "Appendix F — Glossary",
    "section": "F.7 CH 07",
    "text": "F.7 CH 07",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-08",
    "href": "app-f-glossary.html#ch-08",
    "title": "Appendix F — Glossary",
    "section": "F.8 CH 08",
    "text": "F.8 CH 08",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-09",
    "href": "app-f-glossary.html#ch-09",
    "title": "Appendix F — Glossary",
    "section": "F.9 CH 09",
    "text": "F.9 CH 09",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-10",
    "href": "app-f-glossary.html#ch-10",
    "title": "Appendix F — Glossary",
    "section": "F.10 CH 10",
    "text": "F.10 CH 10",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-11",
    "href": "app-f-glossary.html#ch-11",
    "title": "Appendix F — Glossary",
    "section": "F.11 CH 11",
    "text": "F.11 CH 11",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-12",
    "href": "app-f-glossary.html#ch-12",
    "title": "Appendix F — Glossary",
    "section": "F.12 CH 12",
    "text": "F.12 CH 12",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-13",
    "href": "app-f-glossary.html#ch-13",
    "title": "Appendix F — Glossary",
    "section": "F.13 CH 13",
    "text": "F.13 CH 13",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-14",
    "href": "app-f-glossary.html#ch-14",
    "title": "Appendix F — Glossary",
    "section": "F.14 CH 14",
    "text": "F.14 CH 14",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-15",
    "href": "app-f-glossary.html#ch-15",
    "title": "Appendix F — Glossary",
    "section": "F.15 CH 15",
    "text": "F.15 CH 15",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-16",
    "href": "app-f-glossary.html#ch-16",
    "title": "Appendix F — Glossary",
    "section": "F.16 CH 16",
    "text": "F.16 CH 16",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-17",
    "href": "app-f-glossary.html#ch-17",
    "title": "Appendix F — Glossary",
    "section": "F.17 CH 17",
    "text": "F.17 CH 17",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-18",
    "href": "app-f-glossary.html#ch-18",
    "title": "Appendix F — Glossary",
    "section": "F.18 CH 18",
    "text": "F.18 CH 18",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-19",
    "href": "app-f-glossary.html#ch-19",
    "title": "Appendix F — Glossary",
    "section": "F.19 CH 19",
    "text": "F.19 CH 19",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#ch-20",
    "href": "app-f-glossary.html#ch-20",
    "title": "Appendix F — Glossary",
    "section": "F.20 CH 20",
    "text": "F.20 CH 20",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#apx-a",
    "href": "app-f-glossary.html#apx-a",
    "title": "Appendix F — Glossary",
    "section": "F.21 APX A",
    "text": "F.21 APX A",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "app-f-glossary.html#apx-b",
    "href": "app-f-glossary.html#apx-b",
    "title": "Appendix F — Glossary",
    "section": "F.22 APX B",
    "text": "F.22 APX B",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  }
]