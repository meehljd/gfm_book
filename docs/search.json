[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Genomic Foundation Models",
    "section": "",
    "text": "Introduction\nA single fertilized egg divides into trillions of cells sharing essentially the same genome, yet these cells differentiate into over two hundred distinct types, each with characteristic patterns of gene expression, chromatin accessibility, and regulatory state. The instructions for this differentiation are written in the genome itself: in enhancers and silencers distributed across hundreds of megabases, in splice sites that determine which exons join to form mature transcripts, in three-dimensional chromatin contacts that bring distant regulatory elements together. Reading these instructions requires understanding a regulatory grammar that evolution wrote over billions of years but never documented.\nClassical computational approaches attacked this problem piecemeal. One model predicted splice sites from local sequence context. Another identified transcription factor binding motifs. A third scored variant pathogenicity using evolutionary conservation. Each required hand-crafted features, curated training sets, and careful validation within a narrow domain. Insights rarely transferred: a model trained to recognize promoters knew nothing about enhancers, and neither could predict how a single nucleotide change might alter splicing. The result was a fragmented landscape where each biological question demanded its own specialized tool.\nFoundation models represent a fundamentally different approach. By training on vast corpora of genomic sequence with self-supervised objectives, these models learn representations that capture regulatory logic without explicit supervision on any particular task. The same model that predicts masked nucleotides can, after minimal adaptation, predict chromatin accessibility, identify splice sites, score variant effects, and distinguish pathogenic mutations from benign polymorphisms. This capacity for transfer learning suggests that foundation models have learned something general about how genomes encode function. Understanding what they have learned, how to deploy them effectively, and where they still fail defines the central challenge for practitioners in this field.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-foundation-models-for-genomics",
    "href": "index.html#why-foundation-models-for-genomics",
    "title": "Genomic Foundation Models",
    "section": "Why Foundation Models for Genomics?",
    "text": "Why Foundation Models for Genomics?\nTraditional genomic modeling has been overwhelmingly task-specific. A variant caller is tuned to distinguish sequencing errors from true variants in a particular sequencing platform and sample type. A supervised convolutional network predicts a fixed set of chromatin marks for a specific cell line. A polygenic risk score is fit for one trait, in one ancestry group, using data from one biobank. These models can achieve excellent performance in the settings they were designed for, but they often transfer poorly to new assays, tissues, ancestries, or institutions. When the input distribution shifts, whether because of a new sequencing chemistry, a different population, or a novel cell type, performance degrades in ways that are difficult to anticipate.\nFoundation models address this fragility through three interrelated strategies. First, they leverage scale: training on massive, heterogeneous datasets spanning multiple assays, tissues, species, and cohorts forces the model to learn representations that capture shared biological structure rather than dataset-specific artifacts. Second, they employ self-supervised objectives that do not require manual labels, allowing them to exploit the vast quantities of unlabeled sequence data, perturbation screens, and population variation that genomics generates. Third, they are designed for reusability: rather than training a new model for each task, practitioners probe, adapt, or fine-tune a shared backbone, amortizing the cost of representation learning across many downstream applications.\nThe extent to which this paradigm delivers on its promises in genomics remains an active research question. Some tasks benefit dramatically from pretrained representations; others show marginal improvement over strong classical baselines. Transfer across species, cell types, and assays works better in some settings than others. The computational costs of training and deploying large models create practical constraints that vary across research and clinical environments. Foundation models are not the answer to every genomic problem. Effective practice requires frameworks to evaluate when these approaches help, when simpler methods suffice, and how to design analyses that exploit the strengths of modern architectures while remaining alert to their limitations.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#recurring-themes",
    "href": "index.html#recurring-themes",
    "title": "Genomic Foundation Models",
    "section": "Recurring Themes",
    "text": "Recurring Themes\nSeveral threads run through the book, and individual chapters can be read as different perspectives on the same underlying questions.\nThe co-evolution of data and architecture is one such thread. Early variant effect predictors relied on hand-engineered features and shallow models trained on modest curated datasets. Convolutional networks enabled direct learning of regulatory motifs and local grammar from raw sequence, but their fixed receptive fields limited their reach. Transformers and other long-context architectures opened the door to capturing broader regulatory neighborhoods and chromatin structure. Foundation models push toward representations that span multiple assays, tissues, and organisms. At each stage, the question is not simply whether the model is more sophisticated, but how the available data constrain what the model can sensibly learn.\nScaling laws and emergent capabilities represent a related concern. As models grow larger and train on more data, certain capabilities appear discontinuously rather than gradually. The relationship between parameters, training data, and compute follows predictable patterns that inform practical decisions about model development. Understanding these scaling dynamics helps practitioners decide when to train larger models, when existing models suffice, and what capabilities to expect at different scales.\nContext length and genomic geometry present persistent challenges. Many genomic phenomena are intrinsically non-local: enhancers regulate genes across hundreds of kilobases, chromatin loops bring distal elements into contact, and polygenic effects distribute risk across thousands of variants genome-wide. How models represent these long-range dependencies, what architectural choices enable or constrain their reach, and what is gained or lost as context windows scale remain central questions for genomic deep learning.\nThe distinction between prediction and design cuts across multiple chapters. Most current models are used as predictors: given a sequence and context, what molecular or phenotypic outcome is expected? The same models can also be embedded in design workflows, from variant prioritization and library construction to therapeutic sequence optimization. Foundation models change where the boundary lies between analysis and experimental planning, and they introduce new failure modes when generative or optimization objectives are misspecified.\nEvaluation connects benchmark performance to real-world decisions. Benchmark scores are seductive and easy to compare, but biological and clinical decisions are messy, multi-objective, and constrained by data drift, confounding, and poorly specified endpoints. A recurring theme is the gap between state-of-the-art metrics on held-out test sets and actual impact in research or clinical deployment. Careful evaluation, confounder analysis, uncertainty quantification, and calibration can narrow that gap, but only when practitioners understand what their metrics actually measure.\nInterpretability and mechanism warrant sustained attention. Interpretability is not optional decoration but a design constraint that shapes how models should be built and evaluated. Saliency maps, motif extraction, and mechanistic analyses can deepen understanding of what a model has learned, but they can also provide false comfort when applied to confounded or brittle representations. Distinguishing genuine biological insight from pattern-matching artifacts requires both technical tools and careful experimental design.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#typography-and-formatting",
    "href": "index.html#typography-and-formatting",
    "title": "Genomic Foundation Models",
    "section": "Typography and Formatting",
    "text": "Typography and Formatting\nComputational biology, machine learning, and clinical genomics each have distinct conventions for technical terminology. ML researchers recognize VCF as a file format; genomicists know BRCA1 as a tumor suppressor gene; clinicians understand gnomAD as a variant database. Typographic conventions distinguish these categories, helping specialists navigate unfamiliar domains while respecting established standards.\nThe typography system identifies canonical terms that appear in the glossary, distinguishes biological entities from computational infrastructure, and maintains clean prose that does not overwhelm with formatting. Each format choice must earn its place by genuinely aiding comprehension rather than adding visual noise. Databases like gnomAD appear constantly throughout genomic analyses; italicizing every mention would create clutter without improving clarity. In contrast, model names like Enformer and gene names like BRCA1 function as subjects in the narrative: proper nouns that benefit from visual distinction.\nThe hierarchy is simple: Bold marks glossary terms on first mention only. Italics marks proper nouns that function as subjects or actors in the narrative (models, genes, mathematical variables, Latin terms). Regular text with careful capitalization handles databases, consortia, and resources. Monospace signals computational infrastructure (file formats, code, command-line tools). Most prose remains unformatted, with typography providing navigation aids rather than constant emphasis.\nGlossary terms appear in bold on first mention only: “The transformer architecture revolutionized sequence modeling.” Subsequent mentions use regular text. This applies to machine learning concepts (attention mechanism, fine-tuning, embeddings), genomic concepts (single nucleotide polymorphism (SNP), enhancer, phasing), clinical terms (variant of uncertain significance (VUS), penetrance), and statistical concepts (area under ROC curve (auROC), calibration).\nModel names use italics throughout: Enformer, DNABERT, AlphaFold, DeepVariant, SpliceAI. Gene and protein names follow biological convention with italics: BRCA1, TP53, CFTR, CYP2D6. Mathematical variables in prose also use italics: “where n represents sequence length” or “attention score between positions i and j”. Latin and foreign terms are italicized: in silico, de novo, ab initio, in trans.\nMonospace formatting signals computational elements. File formats use monospace: VCF, BAM, FASTA, FASTQ, BED, GTF. Code elements including function names (batch_size, forward()), packages (transformers, torch), and command-line tools (bedtools, samtools, GATK) also use monospace.\nDatabases, consortia, and resources use regular text with careful capitalization: gnomAD, ClinVar, ENCODE, GTEx, UniProt. Sequencing technologies (Illumina, PacBio HiFi, Oxford Nanopore) and biochemical assays (ATAC-seq, ChIP-seq, RNA-seq) similarly use regular text. This reduces visual clutter in passages that reference multiple data sources while preserving clarity through distinctive capitalization patterns.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#structure-and-organization",
    "href": "index.html#structure-and-organization",
    "title": "Genomic Foundation Models",
    "section": "Structure and Organization",
    "text": "Structure and Organization\nSix parts span twenty-nine chapters, with six appendices providing supplementary material. Each part can be read independently, but the progression is cumulative.\nPart I: Genomic Foundations lays the genomic and statistical groundwork that later models rest on. 1  From Reads to Variants introduces next-generation sequencing, alignment, and variant calling, highlighting sources of error and the evolution from hand-crafted pipelines to learned variant callers. 2  Data Landscape surveys the core data resources that underlie most modern work: reference genomes, population variation catalogs, clinical variant databases, and functional genomics consortia such as ENCODE and GTEx. 3  GWAS and Polygenic Scores reviews genome-wide association studies, linkage disequilibrium, fine-mapping, and polygenic scores, emphasizing what these variant-to-trait associations do and do not tell us about mechanism. 4  Classical Variant Prediction covers conservation-based and machine-learning-based variant effect predictors such as CADD, including their feature sets, label construction, and issues of circularity and dataset bias. Together, these chapters answer a foundational question: what data and pre-deep-learning tools form the backdrop that any genomic foundation model must respect, integrate with, or improve upon?\nPart II: Deep Learning for Sequences introduces the conceptual and technical foundations of modern sequence modeling. 5  Tokens and Embeddings examines how genomic and protein sequences are converted into model-compatible representations, covering one-hot encodings, k-mers, byte-pair encodings, learned embeddings, and position encodings, showing how these choices shape downstream model behavior. 6  Convolutional Networks examines convolutional approaches that established the field of genomic deep learning, including DeepSEA, Basset, and SpliceAI, analyzing what they learn about motifs and regulatory grammar and where their fixed receptive fields impose limitations that motivate attention-based architectures. 7  Transformers and Attention provides a detailed treatment of attention mechanisms, position encodings, and transformer architectures, with emphasis on how these ideas translate from language to biological sequence. 8  Pretraining Strategies covers pretraining objectives, from masked language modeling and next-token prediction to contrastive and generative approaches, examining how self-supervision extracts structure from unlabeled biological data. 9  Transfer and Adaptation addresses transfer learning, domain adaptation, and few-shot learning, asking when and how pretrained representations generalize to new tasks, species, and data modalities.\nPart III: Foundation Models for Biology surveys the major foundation model families, organized by modality, and establishes variant effect prediction as the integrating application. 10  Foundation Model Paradigm develops a working definition and taxonomy of foundation models in genomics, distinguishing them from earlier supervised approaches and examining scaling laws that characterize how model capabilities change with size and data. 11  DNA Language Models covers DNA language models such as DNABERT, Nucleotide Transformer, HyenaDNA, and Evo, tracing their training corpora, objectives, evaluation suites, and current capabilities. 12  Protein Language Models describes large protein language models trained on evolutionary sequence databases, their emergent structure and function representations, and applications to structure prediction and design. 13  Regulatory Models covers hybrid CNN-transformer and related architectures designed for long genomic contexts, such as Enformer and Borzoi, which predict regulatory readouts over tens to hundreds of kilobases. 14  Variant Effect Prediction serves as a capstone that integrates these model families, examining how protein-based approaches such as AlphaMissense and DNA-based approaches such as splicing and regulatory models combine to address variant effect prediction, the central interpretive challenge that motivates the field.\nPart IV: Multi-Scale and Systems Modeling examines how foundation model principles extend beyond one-dimensional sequence to embrace cellular and systems-level biology. 15  RNA Structure and Function extends beyond splicing to RNA structure prediction and RNA foundation models, examining how secondary structure and functional context inform representation learning. 16  Single-Cell Models covers foundation models for single-cell transcriptomics and epigenomics, showing how transformer architectures adapt to the unique characteristics of these data types. 17  3D Genome Organization addresses the three-dimensional organization of the genome, from chromatin loops and TAD boundaries to emerging spatial transcriptomics foundation models, examining how 3D structure provides the missing link between sequence and regulatory function. 18  Graph and Network Models turns to graph neural networks and network-based approaches, framing these not as alternatives to sequence models but as higher-level reasoning systems that consume foundation model embeddings as node features. 19  Multi-Omics Integration broadens the view to multi-omics integration, exploring how models can jointly represent genomic, transcriptomic, proteomic, and clinical information to connect sequence variation to phenotype across multiple layers of biological organization.\nPart V: Evaluation and Reliability develops frameworks for assessing what models actually learn and how reliably they perform. 20  Benchmarks surveys existing benchmarks for genomic foundation models, analyzing their construction, coverage, and limitations. 21  Evaluation Principles presents evaluation principles and proper methodology, covering data splitting, metric choice, and the link between benchmark performance and real-world utility. 22  Confounders and Leakage details sources of confounding and data leakage, from batch effects and ancestry structure to label bias and covariate shift, offering practical strategies for detection and mitigation. 23  Uncertainty Quantification addresses uncertainty quantification, examining calibration, epistemic versus aleatoric uncertainty, and practical methods such as ensembles and conformal prediction that help models express when they do not know. 24  Interpretability explores interpretability tools from classical motif discovery and attribution methods to emerging mechanistic approaches, asking when these tools reveal genuine biological mechanisms and when they provide false comfort.\nPart VI: Translation moves from methods to end-to-end workflows in research and clinical practice. 25  Clinical Risk Prediction discusses clinical risk prediction that combines genomic features with electronic health records and environmental data, focusing on discrimination, calibration, fairness, and deployment in health systems. 26  Rare Disease Diagnosis examines how foundation models fit into rare disease and cancer workflows, including variant prioritization pipelines, integration with family and tumor-normal data, and laboratory validation. 27  Drug Discovery looks at how GFMs intersect with target discovery, functional genomics screens, and biomarker development in pharmaceutical and biotechnology settings. 28  Sequence Design covers generative applications, from protein design and therapeutic sequence optimization to synthetic biology and bioengineering workflows. 29  Ethics and Frontiers concludes with regulatory and ethical considerations, open problems, emerging directions, and considerations for responsible development and deployment of genomic AI systems.\nSix appendices provide supporting material. Appendix A — Deep Learning Primer offers a compact introduction to neural networks, CNNs, transformers, training, and evaluation for readers who want enough machine learning background to engage with the main chapters without consulting external references. Appendix B — Deployment and Compute covers practical considerations for deploying genomic foundation models, including computational requirements, hardware selection, and infrastructure concerns. Appendix C — Data Curation provides guidance on constructing training datasets, covering data sources, quality filtering, deduplication, and contamination detection. Appendix D — Model Reference provides a comprehensive reference table of models discussed in the main chapters, with architecture summaries, training data, and key citations. Appendix E — Resources offers a curated collection of datasets, software tools, courses, and papers for deeper exploration. Appendix F — Glossary defines key terms spanning genomics, machine learning, and clinical applications.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#a-framework-not-a-snapshot",
    "href": "index.html#a-framework-not-a-snapshot",
    "title": "Genomic Foundation Models",
    "section": "A Framework, Not a Snapshot",
    "text": "A Framework, Not a Snapshot\nGenomic foundation models represent a moving target: architectures evolve, datasets expand, and evaluation standards shift. A book of the state of the art in 2024 would be obsolete before publication. The goal instead is to provide a framework for reasoning about new models as they appear, grounding readers in principles stable enough to outlast any particular architecture or benchmark.\nReaders who work through this material should be equipped to place new models in the landscape of data, architecture, objective, and application. They should be able to design analyses that use foundation models as components (whether as feature extractors, priors, or simulators) without overclaiming what the models can do. They should recognize pitfalls in training, evaluation, and deployment, especially in clinical settings where errors have real consequences. And they should be able to decide where foundation models genuinely add value and where simpler methods remain sufficient.\nThe journey begins with foundations: how raw reads become variants, how variants become the datasets on which all subsequent models depend, and where errors in this upstream process create systematic challenges that propagate through everything built upon them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why I Wrote This Book\nWorking on genomic foundation models means context-switching constantly: debugging data artifacts one week, reproducing a transformer-based variant effect predictor the next, and arguing about clinical patient cohorts the week after. The knowledge required is scattered across textbooks, methods papers, and tribal folklore - genomics on one shelf, deep learning on another, clinical deployment in someone else’s head entirely.\nThis book is my attempt to put those pieces in one place: to connect the mature, statistically grounded tradition of human genetics with the rapidly changing ecosystem of deep learning and foundation models, and to make that transition legible for people who live in one corner of the triangle and are trying to get oriented to the others.\nI wrote it first for myself and my collaborators: as a way to organize wiki pages, markdown files, and half-finished slide decks into something coherent. Over time it became clear that turning those notes into a book might be useful to others navigating the same landscape.\nWhat I wanted, but could not find, was a conceptual throughline:\nThis book is my best attempt at answering those questions in a way that is historically grounded, technically honest, and practically oriented.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#why-i-wrote-this-book",
    "href": "preface.html#why-i-wrote-this-book",
    "title": "Preface",
    "section": "",
    "text": "How do we get from reads to variants in a way that a deep model can trust?\nHow should we think about polygenic scores, fine-mapping, and functional assays in the era of foundation models?\nWhen we say a model “understands” regulatory grammar or protein function, what does that actually mean?\nAnd what does it take to move from a promising preprint to a tool that can support decisions about real patients?",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#how-this-book-came-together",
    "href": "preface.html#how-this-book-came-together",
    "title": "Preface",
    "section": "How This Book Came Together",
    "text": "How This Book Came Together\nThe structure of the book reflects the way these ideas evolved in my own work.\nEarly sections grew out of teaching and mentoring conversations: explaining next-generation sequencing, variant calling, and pre-deep-learning interpretation methods to new team members who were strong in statistics or ML but new to genomics (and vice versa).\nThe middle sections emerged from a series of “journal club + experiments” cycles, where we:\n\nread papers on sequence-to-function CNNs, protein language models, and genomic transformers,\ntried to reproduce key results or adapt them to key datasets,\nand documented the pain points—data formats, training instabilities, evaluation pitfalls, which never quite fit into a methods section.\n\nThe later parts were shaped by collaborations around clinical prediction, variant interpretation pipelines, and larger multi-omic models. Many of the examples and caveats come directly from these projects: places where a model that looked excellent on paper behaved in surprising ways when exposed to real-world data, or where simple baselines outperformed much fancier architectures once confounding and distribution shift were handled correctly.\nBecause of that origin, the book has a particular bias: it is written from the perspective of someone who spends much of their time trying to get models to work in messy, high-stakes settings. You will see this in the emphasis on data quality, evaluation, and clinical translation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#how-to-read-this-book",
    "href": "preface.html#how-to-read-this-book",
    "title": "Preface",
    "section": "How to Read This Book",
    "text": "How to Read This Book\nThis is not a genomics textbook, a complete review of every DNA or protein model, or a deep-learning-from-scratch course. Instead, it is meant to be:\n\na roadmap to the main kinds of data, models, and objectives that matter for genomic foundation models today\na bridge between classical statistical genetics and modern representation learning\na practical guide to the kinds of failure modes and design choices that matter in real applications.\n\nYou do not need to read the book cover-to-cover in order.\n\nIf your background is in genomics or statistical genetics, you may want to skim the early deep-learning motivations and focus more on the sections that introduce convolutional models, transformers, and self-supervision, then move on to evaluation and applications.\nIf you come from machine learning, it may be more helpful to start with the genomic data and pre-deep-learning methods, then dive into the sequence-to-function and transformer-based chapters with an eye toward how the data and objectives differ from text or images.\nIf you are a clinician or translational researcher, you might care most about the reliability, confounding, and clinical deployment discussions, dipping back into the modeling parts as needed to interpret results or communicate with technical collaborators.\n\nThe book is organized into six parts:\n\nPart I introduces genomic data and pre-deep-learning interpretation methods, from sequencing and variant calling to early pathogenicity scores and polygenic models.\nPart II focuses on supervised sequence-to-function models, with an emphasis on convolutional architectures, regulatory prediction, and splicing.\nPart III turns to transformer-based models and self-supervision, covering protein and DNA language models and hybrid architectures that combine CNNs and transformers.\nPart IV discusses what makes a model a foundation model in genomics, including multi-omic architectures, variant effect modeling, and emergent capabilities.\nPart V examines reliability, evaluation, confounding, and interpretability—how we know whether a model is learning what we think it is, and how to detect when it is not.\nPart VI looks at applications: clinical and risk prediction, variant interpretation workflows, and early steps toward drug discovery and biotech use cases.\n\nWithin each part, the goal is not to catalogue every paper, but to highlight representative examples and the design principles they illustrate. References are there to give you starting points, not to serve as a comprehensive literature review.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#what-this-book-assumes-and-what-it-does-not",
    "href": "preface.html#what-this-book-assumes-and-what-it-does-not",
    "title": "Preface",
    "section": "What This Book Assumes (and What It Does Not)",
    "text": "What This Book Assumes (and What It Does Not)\nThe book assumes:\n\nbasic familiarity with probability and statistics (regression, hypothesis testing, effect sizes),\ncore genomics concepts (genes, variants, linkage disequilibrium, GWAS at a high level),\nand some exposure to machine learning ideas (training versus test data, overfitting, loss functions).\n\nIt does not assume that you have implemented deep learning models yourself, or that you are fluent in every area. When a chapter leans heavily on a particular background (for example, causal inference or modern self-supervised learning), it will either provide a brief refresher or point you to an appendix or external resource.\nIf you are missing some of this background, that is fine. The intent is for you to be able to read actively: to pause, look up side topics, and then return to the main arc without feeling lost.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#a-note-on-scope-and-opinions",
    "href": "preface.html#a-note-on-scope-and-opinions",
    "title": "Preface",
    "section": "A Note on Scope and Opinions",
    "text": "A Note on Scope and Opinions\nGenomic foundation models are evolving quickly. Any snapshot is, by definition, incomplete and slightly out of date.\nRather than chasing every new architecture or benchmark, the book focuses on durable ideas:\n\nhow different data types fit together,\nwhat kinds of objectives encourage useful representations,\nhow evaluation can fail in genomics-specific ways,\nand where deep models complement (rather than replace) classical approaches.\n\nInevitably, there are judgment calls about which papers, methods, and perspectives to emphasize. Those choices reflect my own experiences and biases. They are not an official position of any institution I work with, and they will certainly differ from other reasonable views in the field.\nYou should treat the book as one opinionated map of the landscape, not the landscape itself.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#acknowledgements",
    "href": "preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis book exists because of many generous people who shared their time, ideas, and encouragement.\nFirst, I owe a deep debt of gratitude to my colleagues in the Mayo Clinic GenAI and broader data science community. The day-to-day conversations, whiteboard sessions, and “what went wrong here?” post-mortems with this group shaped much of the perspective and many of the examples in the chapters.\nI am especially grateful to the principal investigators and clinicians whose questions kept the focus on real patients and real decisions:\nDr. Shant Ayanian, Dr. Elena Myasoedova, and Dr. Alexander Ryu.\nTo leadership at Mayo Clinic who supported the time, computing resources, and institutional patience needed for both the models and this book:\nDr. Matthew Callstrom, Dr. Panos Korfiatis, and Matt Redlon.\nTo my data science and machine learning engineering colleagues, whose work and feedback directly shaped many of the workflows and case studies:\nBridget Toomey, Carl Molnar, Zach Jensen, and Marc Blasi.\nI am also grateful for the architectural creativity, hardware insight, and willingness to experiment from our collaborators at Cerebras:\nNatalia Vassilieva, Jason Wolfe, Omid Shams Solari, Vinay Pondenkandath, Bhargav Kanakiya, and Faisal Al-khateeb.\nAnd to our collaborators at GoodFire, whose partnership helped push these ideas toward interpretable and deployable systems:\nDaniel Balsam, Nicholas Wang, Michael Pearce, and Mark Bissell.\nI would also like to thank my former colleagues at LGC for foundational work and conversations around protein language models and large-scale representation learning:\nPrasad Siddavatam and Robin Butler.\nBeyond these named groups, I owe a broader debt to the geneticists, molecular biologists, statisticians, clinicians, and engineers whose work this book draws on. The field moves forward because people share code, publish honest benchmarks, and insist that models be connected back to biologically meaningful questions. Thank you for setting that standard.\nFinally, I am grateful to my wife, Alyssa, and our two kids for their patience with the evenings and weekends this book consumed. You gave me the space to finish it and the reasons to step away from it.\nIf this book helps you connect a new model to a real biological question, design a more robust evaluation, or communicate more clearly across disciplinary boundaries then it will have done its job.\n— Josh Meehl",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part_1/p1--foundations.html",
    "href": "part_1/p1--foundations.html",
    "title": "Part I: Data Foundations",
    "section": "",
    "text": "Every genomic foundation model inherits the biases of its training data. A model trained on European-dominated biobanks will miscalibrate predictions for other populations. A variant effect predictor learning from ClinVar inherits whatever ascertainment biases clinical laboratories embedded in those classifications. A regulatory model trained on ENCODE cell lines may fail on primary tissues absent from the training compendium. Foundation models do not transcend their data sources; they compress and reflect them. Understanding what data resources contain, what they systematically miss, and what assumptions they encode is prerequisite to understanding what foundation models can and cannot accomplish.\nGenomic foundation models inherit both the power and the limitations of the technologies that generate their training data. Next-generation sequencing and variant calling (1  From Reads to Variants) transform biological samples into the VCF files that serve as inputs to nearly all downstream analysis. Understanding these technologies reveals their remarkable power alongside their systematic blind spots: reference bias, missing structural variants, and error patterns that propagate into every model trained on their outputs.\nPublic resources underpin modern computational genomics, serving simultaneously as training data, evaluation benchmarks, and sources of prior biological knowledge (2  Data Landscape): reference genomes, population variation catalogs like gnomAD, functional genomics consortia such as ENCODE and Roadmap Epigenomics, and biobank-scale cohorts including the UK Biobank and GTEx. Genome-wide association studies and polygenic scores (3  GWAS and Polygenic Scores) provide both baselines against which deep models are measured and conceptual frameworks that inform their design. Pre-deep-learning variant effect prediction through CADD and related methods (4  Classical Variant Prediction) establishes what careful feature engineering achieved and where its limitations motivated the learned representations developed in subsequent parts.",
    "crumbs": [
      "Part I: Data Foundations"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html",
    "href": "part_1/p1-ch01-ngs.html",
    "title": "1  From Reads to Variants",
    "section": "",
    "text": "1.1 NGS Data Challenges\nEvery polygenic risk score, every variant pathogenicity prediction, every clinical interpretation of a patient’s genome begins with a prior assumption: that the variants being analyzed are real. Researchers download VCF files from biobanks, filter by allele frequency, and feed variants into predictive models without questioning whether those variants truly exist in the original biological sample. Yet variant calling is not observation; it is inference. The sequencer produces millions of short reads with characteristic error profiles, alignment algorithms place those reads against a reference genome with varying confidence, and variant callers integrate the evidence into genotype calls that may or may not reflect reality. When this inference fails, every downstream analysis inherits errors that propagate silently through the entire computational pipeline.\nShort-read sequencing technologies produce reads of 100 to 300 base pairs with error rates near one percent, creating fundamental ambiguities in repetitive regions, segmental duplications, and regions of high sequence complexity. Classical variant callers addressed these challenges through probabilistic models that integrate multiple evidence types: base quality scores, mapping quality, strand balance, and population allele frequencies. Deep learning approaches, exemplified by DeepVariant, reformulated variant calling as image classification, learning to distinguish true variants from artifacts in ways that generalize across sequencing platforms and sample types.\nThe representational approach of encoding pileups as images anticipates the embedding strategies discussed in Section 5.6. DeepVariant’s CNN architecture shares design principles with regulatory models like DeepSEA (Section 6.2), though applied to different input modalities. Where DeepVariant learns variant-specific patterns from labeled training data, foundation models (Section 10.2) learn sequence representations from unlabeled genomes.\nUnderstanding where variant calling succeeds and fails is essential context for genomic foundation models. The challenging regions of the genome (tandem repeats, segmental duplications, the major histocompatibility complex) that confound variant callers also challenge models trained on variant calls. The systematic errors introduced at this stage create systematic blind spots that no amount of sophisticated downstream modeling can correct. The evaluation challenges created by these blind spots are examined in detail in Section 21.2.\nThe human genome contains approximately three billion base pairs, yet no instrument can read this sequence in one continuous stretch. Next-generation sequencing (NGS) instead fragments DNA molecules into short pieces, sequences each fragment independently, and produces tens to hundreds of gigabases of sequence data per run (Goodwin, McPherson, and McCombie 2016). The typical output consists of paired-end Illumina reads spanning 100 to 300 base pairs each, with each base assigned a quality score reflecting the instrument’s confidence in that call. This abundance comes at a cost: every read carries non-trivial measurement uncertainty, including substitutions from miscalled bases, context-specific errors near homopolymers, and quality degradation toward read ends.\nLong-read technologies from Pacific Biosciences and Oxford Nanopore extend the observable space dramatically, producing reads of 10 kilobases to over a megabase in length (Wenger et al. 2019; Dabernig-Heinz et al. 2024). These platforms access genomic territory invisible to short reads, including complex structural variants, segmental duplications, and repetitive regions. They carry their own characteristic error profiles, however, and the choice of sequencing platform fundamentally shapes which variants are discoverable and which systematic biases enter downstream analyses. A variant residing within a repetitive element may be invisible to short reads but readily detected by long reads that span the entire repeat.\nThe central problem is deceptively simple in statement but profound in consequence: how do we turn raw reads into a reliable list of genomic variants? Answering this question requires disentangling three fundamentally different sources of signal that manifest identically as mismatches between reads and reference. Sequencing errors arise from instrument noise and PCR artifacts during library preparation, creating false variants that never existed in the original DNA. Alignment artifacts occur when reads are mapped to incorrect genomic locations, particularly in repetitive regions and paralogous gene families, causing true variants to appear at wrong positions or disappear entirely. Genuine biological variation encompasses germline variants inherited from parents, somatic mutations acquired during cellular division, and mosaicism where only a fraction of cells carry a particular change. Historically, complex modular pipelines combining probabilistic models and hand-crafted heuristics addressed this separation (Nielsen et al. 2011). Deep learning now plays an important role in simplifying and improving parts of this stack, but understanding the classical pipeline remains essential for interpreting what downstream models actually learn.\nGermline variant calling in whole-exome and whole-genome sequencing presents the core technical challenge underlying most genomic deep learning applications. Somatic variant calling in cancer and RNA-seq-specific variant calling share many parallels but require additional considerations addressed elsewhere.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-targeting",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-targeting",
    "title": "1  From Reads to Variants",
    "section": "1.2 Targeting Strategies: Panels, Exomes, and Genomes",
    "text": "1.2 Targeting Strategies: Panels, Exomes, and Genomes\nDifferent clinical and scientific goals demand different sequencing strategies. A patient presenting with sudden cardiac arrest at age 35 needs deep, reliable coverage of KCNQ1, KCNH2, SCN5A, and other ion channel genes associated with long QT syndrome; sequencing her entire genome to find these variants would waste resources and delay clinical decisions. A biobank building training data for polygenic risk scores across hundreds of thousands of participants needs genome-wide coverage, even if individual sites have modest depth. A family searching for the cause of their child’s undiagnosed developmental delay needs comprehensive coverage that leaves no coding exon unexamined. These competing demands drive the choice between targeted panels, whole-exome sequencing, and whole-genome sequencing.\n\n1.2.1 Targeted and Panel Sequencing\nWhen clinicians already know which genes to examine, targeted gene panels capture tens to hundreds of genes selected for a specific clinical indication. Panels for cardiomyopathy, hereditary cancer syndromes, or epilepsy restrict sequencing to regions of known clinical relevance. By limiting the target to a small number of loci, panels achieve very deep coverage (often exceeding 500×) at modest cost, enabling sensitive detection of rare variants and some degree of mosaicism.\nThe narrow scope of panels limits their utility for deep learning and population-scale analysis. Panels miss novel disease genes outside their predefined targets, cannot be easily repurposed for new traits, and often have heterogeneous content across laboratories that complicates data aggregation. For large-scale genomic foundation models, panel data serve better as richly phenotyped anchors than as primary training material: they provide clean labels for specific variants but sparse genomic coverage overall. The sparse coverage of panel data creates specific challenges for transfer learning, as discussed in Section 9.8.1.\n\n\n1.2.2 Whole-Exome Sequencing\nProtein-coding sequence represents approximately 1 to 2 percent of the genome but harbors a disproportionate share of variants with known functional consequences. Whole-exome sequencing (WES) enriches coding exons and some splice-adjacent regions through hybridization probes that pull down targeted DNA, followed by short-read sequencing. Typical coverage ranges from 80 to 150× for exonic targets, sufficient for confident heterozygous variant calling in most regions.\nWES has driven Mendelian disease gene discovery for over a decade and powered early biobank-scale efforts, including the exome subsets of gnomAD and many hospital-based cohorts (Karczewski et al. 2020). The capture-based approach introduces systematic biases that propagate into downstream analyses, however. Certain exons consistently fail to capture efficiently, particularly those with extreme GC content, high repetitive content, or unusual length. A variant in the first exon of HTT (the Huntington disease gene) might be missed entirely due to extreme GC richness, and a variant effect predictor trained on WES data will never encounter variants in poorly captured regions. These blind spots are invisible in standard benchmarks but can have substantial clinical consequences. Batch effects tied to reagent lots and evolving panel designs further complicate multi-cohort analyses. Technical confounding from batch effects is systematically analyzed in Section 22.2.2, with mitigation strategies in Section 22.9.\n\n\n1.2.3 Whole-Genome Sequencing\nNoncoding variants contribute substantially to human disease, and structural variants often span boundaries between exonic and intronic sequence. Whole-genome sequencing (WGS) samples nearly all bases in the genome at typical coverage of 30 to 60×, encompassing both coding and noncoding regions without the biases introduced by capture chemistry. Because there is no enrichment step, WGS produces more uniform depth than WES and enables detection of noncoding regulatory variants, structural variants, and copy-number changes alongside single nucleotide variants (SNVs) and indels (insertions and deletions).\nWGS has become increasingly favored for new large cohorts and rare disease studies. The UK Biobank’s release of 500,000 whole genomes and gnomAD’s expansion to include diverse populations both rely on WGS as the primary data type (Bycroft et al. 2018; Karczewski et al. 2020). The data are reusable for many downstream analyses, including GWAS (Chapter 3), polygenic score development, and rare variant burden tests, and the simplified pipeline eliminates the need to track changing capture designs across time and centers. Whole-genome models typically assume access to WGS-based variant calls, even when actual training sets combine WES and WGS data for practical reasons.\n\n\n1.2.4 Long-Read Sequencing Technologies\nShort reads face a fundamental limitation rooted in information theory: sequences shorter than local repeats, segmental duplications, or structural variants cannot unambiguously resolve these features. Consider the SMN1 and SMN2 genes, which differ by only five nucleotides across their entire coding regions. Distinguishing them is clinically critical for diagnosing spinal muscular atrophy, yet short reads routinely fail this task because a 150-bp read maps equally well to either genomic location.\nPacific Biosciences (PacBio) HiFi sequencing produces reads of 10 to 25 kilobases with per-base accuracy exceeding 99.9% through circular consensus sequencing, where the same molecule is read multiple times to correct random errors (Wenger et al. 2019). Oxford Nanopore Technologies (ONT) instruments generate reads ranging from a few kilobases to over a megabase in length, with rapidly improving raw accuracy and unique capabilities including portable sequencers suitable for field deployment, direct RNA sequencing without reverse transcription, and real-time base calling during sequencing (Dabernig-Heinz et al. 2024). These technologies played central roles in the telomere-to-telomere (T2T) assembly of a complete human genome and in emerging human pangenome references that capture population diversity beyond what any single linear reference can represent (Nurk et al. 2022; Liao et al. 2023).\nLong reads transform variant calling by traversing low-complexity and repetitive regions essentially invisible to short-read technologies. Dedicated variant callers such as PEPPER-Margin-DeepVariant, Clair3, Sniffles2, pbsv, and cuteSV exploit read length and alignment patterns to detect insertions, deletions, inversions, and complex rearrangements (Shafin et al. 2021; Zheng et al. 2022; Smolka et al. 2024; “PacificBiosciences/Pbsv” 2025; Jiang et al. 2020). Single molecules spanning multiple heterozygous sites provide direct phasing information for haplotype resolution without statistical inference. Long reads also inform graph-based references and pangenomes that better represent population diversity than traditional linear references (Liao et al. 2023).\nShort-read pipelines remain the workhorse for large human cohorts due to cost and throughput advantages that will persist for years. Downstream models must accommodate variants discovered by either technology and must be evaluated on composite callsets that integrate short- and long-read information.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-classical",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-classical",
    "title": "1  From Reads to Variants",
    "section": "1.3 Classical Variant Calling Pipelines",
    "text": "1.3 Classical Variant Calling Pipelines\nUnderstanding classical approaches matters not merely for historical completeness but because deep learning models like DeepVariant still operate within this overall framework. They replace specific components rather than rebuilding from scratch. The GATK Best Practices, first formalized around 2011 and refined continuously since, represent accumulated wisdom from a decade of methodological development (DePristo et al. 2011; Van der Auwera et al. 2018). These pipelines encode expert intuition about which quality metrics matter, how to balance sensitivity against specificity, and when borderline evidence should be trusted. Modern deep learning approaches inherit this structure even as they replace individual components with learned alternatives.\n\n1.3.1 From Sequencer to Aligned Reads\nErrors introduced at the earliest stages of data processing propagate through every downstream analysis; a miscalled base becomes a false variant, and a misaligned read places true variants at incorrect positions. The journey from DNA sample to variant calls begins when instrument software converts fluorescent images or electrical signals to base calls and quality scores through base calling. Reads are demultiplexed by sample barcode into FASTQ files, each containing millions of short sequences with associated Phred-scaled quality scores. These files serve as the raw material for all subsequent analysis, encoding both the sequence content and the instrument’s confidence in each base.\nEach read must then find its position in a reference genome. Alignment algorithms use seed-and-extend strategies to map short sequences against references such as GRCh38 (the traditional linear reference) or T2T-CHM13 (the first complete telomere-to-telomere assembly), with tools like BWA-MEM and minimap2 handling the computational challenge (Li 2013, 2018). The difficulty is substantial: algorithms must cope with mismatches arising from both true variants and sequencing errors, small indels that shift the alignment frame, and repetitive sequences where multiple genomic locations match equally well. When a read could plausibly originate from several locations, the aligner must either choose one (potentially incorrectly), report multiple candidates, or assign a mapping quality score reflecting its uncertainty about the true origin.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 1.1: [Essential] End-to-end schematic showing the journey from DNA sample to VCF file. Major stages: (1) DNA fragmentation and library preparation, (2) sequencing (distinguish short-read Illumina vs long-read PacBio/ONT icons), (3) base calling producing FASTQ, (4) read alignment to reference (show reads stacking against genome), (5) post-alignment processing (duplicate marking, BQSR), (6) per-sample variant calling producing gVCF, (7) joint genotyping across cohort, (8) variant filtering producing final VCF. Annotate data formats at each arrow (FASTQ, BAM/CRAM, gVCF, VCF). Include approximate file sizes for a 30× WGS to ground scale.\n\n\n\nBefore variant calling can begin, systematic artifacts require correction. PCR duplicates arise when multiple reads are amplified from the same original DNA fragment during library preparation; these inflate apparent coverage and can amplify sequencing errors into false variants that appear well-supported by independent evidence. Duplicate marking identifies and flags these reads based on identical alignment coordinates. Quality scores also require adjustment: base quality score recalibration (BQSR) models systematic errors by comparing observed mismatches to databases of known variants, producing scores that better reflect true error rates in each sequence context (DePristo et al. 2011). Older pipelines also performed local realignment around indels, though modern callers have largely internalized this step.\n\n\n1.3.2 Per-Sample Variant Calling\nAt each position in the genome, the fundamental question is: given the reads overlapping this site, what is the most likely genotype? For diploid humans at biallelic sites, three possibilities exist: homozygous reference (0/0), heterozygous (0/1), or homozygous alternate (1/1). The task is to compute genotype likelihoods that quantify the probability of the observed read data under each possible genotype, then combine these likelihoods with prior expectations to estimate posterior probabilities.\nGATK HaplotypeCaller approaches this by first identifying regions with evidence of variation, then locally assembling candidate haplotypes from the reads spanning that region, and finally computing likelihoods for each possible diploid genotype. The core calculation uses a pair hidden Markov model (pair-HMM) to marginalize over possible alignments between each read and each candidate haplotype, incorporating base quality scores to weight the contribution of each base (DePristo et al. 2011; Li 2014).\nThe mathematical framework is Bayesian. At a given site, the posterior probability of genotype G given read data D follows from Bayes’ theorem:\n\\[\nP(G \\mid D) \\propto P(G) \\prod_{r \\in \\text{reads}} P(r \\mid G)\n\\]\nThe prior P(G) often assumes Hardy-Weinberg equilibrium with a specified allele frequency, while the likelihood P(r | G) captures the probability of observing read r given that the true genotype is G. This formulation assumes conditional independence of reads given the genotype, an assumption violated in practice by systematic sequencing errors, read pair correlations, and library-level artifacts that create dependencies among observations. Classical pipelines attempt to correct for these violations through BQSR and ad hoc filters. Deep learning-based callers can learn these dependencies implicitly by processing entire pileups simultaneously, one of their key advantages.\nThe per-read likelihoods aggregate into genotype likelihoods, which combine with priors to yield posterior probabilities. These posteriors become the genotype quality (GQ) scores that downstream analyses often treat as ground truth. Per-sample results are output as gVCF files encoding both variant calls and “reference blocks” with estimated confidence at non-variant positions, enabling later joint analysis across samples.\n\n\n\n\n\n\nNoteThe VCF Format\n\n\n\nThe VCF Format The Variant Call Format (VCF) is the standard file format for storing variant calls. Each variant occupies one row with mandatory columns: CHROM and POS specify genomic location; REF and ALT give the reference and alternate alleles; QUAL provides a Phred-scaled quality score; FILTER indicates whether the variant passed quality filters; and INFO contains semicolon-delimited annotations such as allele frequency or functional predictions. For multi-sample files, a FORMAT column defines per-sample fields (typically GT for genotype, GQ for genotype quality, DP for depth), followed by one column per sample containing those values. Genotypes are encoded as allele indices separated by / (unphased) or | (phased), where 0 represents the reference allele and 1 the first alternate. A heterozygous call appears as 0/1; a phased heterozygote as 0|1 or 1|0 depending on which allele was inherited maternally versus paternally.\n\n\n\n\n1.3.3 Cohort Calling and Filtering\nIndividual samples rarely provide sufficient information for confident rare variant calling. A variant observed in only one sample with modest supporting reads might be a true rare variant or a systematic artifact; examining a single sample cannot distinguish these possibilities. Examining the same site across thousands of samples resolves this ambiguity: true variants appear in multiple individuals following population genetic expectations, while artifacts show patterns inconsistent with inheritance and population structure.\nJoint genotyping combines gVCFs across many samples to produce a multi-sample VCF. This process ensures that all samples are evaluated at the same candidate sites, avoiding the problem of comparing different variant lists, and pools information across carriers to improve sensitivity for rare variants. A variant with marginal evidence in three individuals gains credibility when those individuals share ancestry and the variant frequency matches population expectations from external databases.\nFiltering strategies separate high-confidence variants from probable artifacts. Early approaches applied independent thresholds on quality metrics such as depth, mapping quality, and strand bias, but these hard filters poorly captured the complex, multivariate patterns distinguishing true variants from errors. Variant Quality Score Recalibration (VQSR) instead trains a Gaussian mixture model on known true positives from validated resources (HapMap, 1000 Genomes) and likely false positives, learning a composite quality score that integrates multiple annotation dimensions (DePristo et al. 2011). This approach dominated large-scale variant calling for a decade before machine learning methods began to replace it.\n\n\n1.3.4 Sample-Level Quality Control\nBefore any downstream analysis or model training, variant callsets must pass through sample-level quality control. Sex checks compare reported sex to X chromosome heterozygosity and Y chromosome coverage to detect sample swaps or sex chromosome aneuploidy. Contamination analysis estimates whether DNA from multiple individuals mixed during sample preparation, which would create apparent heterozygosity at sites where the individual is actually homozygous. Relatedness detection identifies unexpected relatives or duplicate sequencing of the same individual, both of which confound association analyses and inflate apparent sample sizes. Ancestry inference estimates genetic ancestry using principal component analysis or model-based clustering, which matters for controlling population stratification in downstream analyses (Chapter 22).\nThese QC steps determine which samples enter training sets, how models are stratified by ancestry, and which samples must be excluded due to technical artifacts. Any callset used for model training or evaluation implicitly assumes that careful QC has already been applied.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-phasing",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-phasing",
    "title": "1  From Reads to Variants",
    "section": "1.4 Haplotype Phasing",
    "text": "1.4 Haplotype Phasing\nThe clinical stakes of phasing emerge clearly in compound heterozygosity. Consider a child who inherits two rare, potentially pathogenic variants in CFTR, the cystic fibrosis gene. If both variants reside on the chromosome inherited from the mother (in cis), the child retains one functional copy from the father and may be unaffected or merely a carrier. If the variants are on opposite chromosomes (in trans), no functional copy exists and the child will develop cystic fibrosis. This scenario is examined in detail in Section 26.3.2. Graph-based approaches to phasing connect to the network methods discussed in Section 18.1.\nStandard VCF genotypes cannot distinguish these scenarios, encoding only that heterozygous genotypes exist at two positions without specifying which alleles travel together on the same physical chromosome. The clinical implications are entirely different, yet the data appear identical without phase information. Diploid organisms carry two copies of each autosomal chromosome, one inherited from each parent. Haplotype phasing resolves the ambiguity in unphased genotype calls by assigning each allele to a specific parental chromosome, transforming genotypes such as 0/1 into phased representations like 0|1 or 1|0 where the delimiter indicates that phase has been determined.\n\n1.4.1 Clinical and Analytical Importance\nThe distinction between cis and trans configurations drives clinical decisions for recessive conditions across hundreds of disease genes, from metabolic disorders to hearing loss to retinal degeneration. Beyond compound heterozygosity, phased haplotypes enable several critical analyses. Haplotype-specific expression studies reveal allelic imbalance where one parental copy is preferentially transcribed, a phenomenon with implications for imprinting disorders and variable penetrance. Accurate modeling of linkage disequilibrium (LD) structure in population genetics depends on knowing which alleles are inherited together. Reference panels used for genotype imputation are stored as phased haplotypes; inaccurate phasing in these panels propagates errors to every study that uses them for imputation.\nFor deep learning applications, phasing determines whether models receive unordered genotype pairs or structured, haplotype-resolved representations. This choice affects model architecture, training procedures, and ultimately performance. A model that processes phased haplotypes can learn patterns spanning multiple variant sites that would appear as noise in unphased data.\n\n\n1.4.2 Phasing Methods\nDifferent data types enable different phasing strategies, each with characteristic strengths and resolution. Read-backed phasing uses sequencing reads that span multiple heterozygous sites to assign alleles to the same physical molecule. Short reads typically phase variants within tens to hundreds of base pairs, limited by fragment length. Long reads extend this range to tens of kilobases or more, providing direct physical evidence of haplotype structure.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 1.2: [High] Three-panel figure illustrating the clinical importance of phase information. Panel A: Maternal and paternal chromosomes carrying the CFTR gene, with two pathogenic variant positions marked. Panel B (cis configuration): Both variants on maternal chromosome; paternal copy functional; child is carrier (unaffected). Panel C (trans configuration): One variant on each chromosome; no functional copy; child has cystic fibrosis. Include VCF notation showing how unphased (0/1) vs phased (0|1, 1|0) genotypes encode this information.\n\n\n\n\n\n1.4.3 Phasing Approaches\nWhen read-based phasing reaches its limits, two complementary strategies extend phase inference across longer distances. Population-based methods exploit the fact that chromosomes are inherited in long blocks that persist across generations. Tools such as SHAPEIT, Eagle, and Beagle compare study samples against reference panels of previously phased haplotypes, identifying shared segments that reveal which alleles travel together (O’Connell et al. 2014; Loh et al. 2016; Browning et al. 2021). When an individual’s genotypes match a known haplotype pattern, the algorithm can confidently assign alleles to chromosomes even without direct read evidence spanning the variants. These statistical approaches work well for common variation where linkage disequilibrium provides informative context but struggle with rare variants that lack haplotype representation in reference panels.\nFamily data, when available, resolves phase through Mendelian logic rather than statistical inference. An allele present in a child and one parent but absent in the other must have been inherited from that parent. This deterministic reasoning makes pedigree-based phasing the gold standard for accuracy, though its applicability depends on recruiting family members for sequencing. The two approaches are complementary: statistical phasing provides genome-wide coverage for any individual, while family-based phasing offers higher confidence for the specific variants segregating within a pedigree.\nModern pipelines often combine these approaches, using statistical phasing anchored by a large reference panel, augmented by read-backed evidence where available, and refined by trio data when present. The resulting phase accuracy varies by variant frequency, local recombination rate, and representation in reference panels.\n\n\n1.4.4 Genotype Imputation and Refinement\nSequencing every individual at high coverage is expensive, but statistical inference from population structure can fill gaps at much lower cost. Genotype imputation matches a cohort with incomplete genotype data (from array genotyping or low-coverage sequencing) against a reference panel of densely phased haplotypes. Statistical models infer missing genotypes and refine uncertain calls by leveraging LD patterns and shared haplotype segments with individuals in the reference (Browning et al. 2021).\nTwo related processes deserve distinction. Imputation of untyped variants infers genotypes at positions not directly observed in the study cohort but present in the reference panel, dramatically increasing variant density without additional sequencing. A genotyping array measuring 500,000 SNPs can yield tens of millions of imputed variants through comparison with panels like the Haplotype Reference Consortium. Genotype refinement improves quality at sites where genotypes were already measured but with uncertainty, particularly useful in low-coverage WGS or WES where stochastic sampling creates noisy calls. Here the same statistical machinery strengthens existing calls rather than generating new ones.\nLow-coverage sequencing presents a distinct opportunity for imputation-based refinement. At 1x genome coverage, any individual position may have zero, one, or a handful of supporting reads, creating high uncertainty in direct genotype calls. Tools like GLIMPSE leverage reference panels to convert these noisy read pileups into high-confidence genotype calls, reducing false call rates by approximately 90% compared to unrefined calls from the same low-coverage data (Rubinacci et al. 2021). The approach works because while any single position has sparse coverage, the aggregate pattern of reads across a haplotype block provides substantial information when matched against reference haplotypes. This principle underlies the growing interest in low-pass whole genome sequencing as a cost-effective alternative to arrays for population-scale studies.\n\n\n1.4.5 Hybrid Sequencing and Coverage Boosting\nModern clinical sequencing increasingly combines multiple data types to balance cost, coverage, and completeness. Hybrid approaches blend deep whole exome sequencing with low-pass whole genome sequencing in a single assay. The exome component (typically 80-100x coverage) provides high-confidence variant calls in coding regions, while the low-pass genome backbone (1-5x coverage) enables genome-wide imputation of common variants comparable to traditional arrays (Cirulli et al. 2020). Helix’s Exome+ assay exemplifies this design: clinical-grade exome sequencing combined with approximately 300,000 genome-wide SNPs that support imputation accuracy equivalent to 0.5x whole genome sequencing.\nA separate concept, often also called “boosting,” refers to assay design choices that enhance sequencing depth in specific regions. Coverage boosting through customized capture probe design targets regions prone to poor coverage in standard exome sequencing, including GC-rich exons, repetitive elements, and clinically critical genes like CYP2D6 where complete star allele calling requires comprehensive coverage. Helix’s assay achieves greater than 99.5% call rates across approximately 600 medically relevant genes by boosting capture probe density in these regions. [Citation Needed] This physical enhancement of sequencing depth differs fundamentally from statistical imputation: boosted coverage produces more supporting reads at the problematic positions, while imputation infers genotypes from population patterns without additional sequencing.\nThe distinction matters for downstream applications. Imputed genotypes carry well-calibrated uncertainty (typically reported as dosages between 0 and 2) that reflects confidence in the inference, whereas directly sequenced variants from boosted coverage regions produce conventional genotype likelihoods based on read evidence. Imputation accuracy degrades systematically for rare variants (minor allele frequency below 1%) because population reference panels provide less information about uncommon haplotypes. Coverage boosting, in contrast, performs equivalently for rare and common variants within the targeted regions since it operates on direct sequencing evidence. For clinical applications requiring rare variant detection in specific genes, boosted coverage provides more reliable results than imputation; for genome-wide association studies leveraging common variants, imputation offers greater cost efficiency.\nFor downstream deep learning, these approaches have distinct implications. Imputation dramatically increases the number of variants available as input features for genotype-based models and produces well-calibrated dosages that probabilistic models can exploit rather than forcing hard calls. Imputation ties model performance to the composition and ancestry representation of the reference panel, however: imputation errors are systematically larger when target individuals come from populations underrepresented in the panel, reinforcing themes of bias and confounding addressed in Chapter 22. Coverage-boosted regions provide high-quality calls independent of reference panel ancestry, but only within the targeted regions. Models consuming hybrid sequencing data must account for this heterogeneity: a variant in a boosted exonic region carries different uncertainty characteristics than an imputed intronic variant, even when both appear in the same VCF with comparable quality scores.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-errors",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-errors",
    "title": "1  From Reads to Variants",
    "section": "1.5 Sources of Error and Uncertainty",
    "text": "1.5 Sources of Error and Uncertainty\nEven with sophisticated pipelines, variant calls remain imperfect measurements of biological reality. These errors concentrate in specific genomic contexts and variant types, creating systematic blind spots in training data that downstream models inherit without warning (Li 2014). Understanding where errors arise, and why they cluster where they do, is essential for interpreting model performance and designing robust training strategies.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 1.3: [High] Taxonomy diagram organizing error sources hierarchically. Three major branches: (1) Sequencing artifacts (subdivide: homopolymer errors, PCR duplicates, index hopping, strand bias, quality score miscalibration); (2) Alignment artifacts (subdivide: mapping ambiguity in repeats, reference bias favoring ref allele, misalignment in segmental duplications); (3) Biological complexity (subdivide: somatic mosaicism, low allele fraction, complex variants like MNVs). For each leaf, include 1-sentence description and indicate whether it creates false positives, false negatives, or both.\n\n\n\n\n1.5.1 Mapping Ambiguity and Reference Bias\nWhen reads align almost equally well to multiple genomic locations, no algorithm can confidently determine their true origin. Segmental duplications, paralogous gene families, and repetitive elements create these ambiguous contexts throughout the genome. The consequences flow in both directions: misassigned reads create false positive variants at incorrect locations, while correctly placed reads may be discarded or down-weighted due to mapping uncertainty, creating false negatives.\nReference bias compounds these problems by systematically favoring detection of reference alleles over alternate alleles. A read carrying a non-reference variant may align slightly worse than an identical read matching the reference due to the mismatch penalty, leading to preferential retention of reference-supporting evidence. This bias causes systematic undercalling of alternate alleles, particularly in highly polymorphic regions or for variants that substantially alter local sequence context. Populations divergent from the reference genome experience more severe reference bias, creating ancestry-correlated error patterns.\n\n\n1.5.2 Systematic Sequencing Artifacts\nSequencing chemistry introduces predictable error patterns that differ qualitatively from random noise. Homopolymer runs (stretches of identical nucleotides such as AAAAAAA or GGGGGG) cause polymerase slippage during synthesis, generating false indels at rates far exceeding substitution errors. Certain sequence motifs, particularly those with extreme GC content, exhibit systematically elevated error rates that persist even at high coverage. PCR amplification during library preparation can introduce errors early in the process; these errors then propagate into multiple reads, creating correlated false positives that appear well-supported by independent evidence.\nIndex hopping occurs when sample barcodes are misassigned during multiplexed sequencing, causing variants from one sample to appear spuriously in others sharing the same flow cell. Strand bias, where variant-supporting reads cluster on one strand orientation, often indicates systematic artifact rather than true variation. These patterns create correlated errors that cluster by batch, lane, or library preparation method, and they are difficult to distinguish from rare true variants precisely because they can appear in multiple reads with reasonable quality scores.\n\n\n1.5.3 Coverage Gaps and Allelic Imbalance\nStochastic sampling means some genomic regions receive fewer reads than average purely by chance, even when capture or sequencing is nominally uniform. In these low-coverage regions, one or both alleles may be missed entirely, and allelic balance can deviate substantially from the expected 50:50 ratio in heterozygotes. A heterozygous site with 20× coverage and 10 reads supporting each allele is confidently called; the same site with 4× coverage might show 4 reference reads and 0 alternate reads by chance alone, leading to a false homozygous reference call with no indication that an allele was missed.\nAllelic imbalance extends beyond random sampling to encompass systematic biases introduced throughout the sequencing workflow. PCR amplification during library preparation preferentially amplifies certain alleles over others, particularly where one allele creates more stable secondary structure or where GC content differs between alleles. Capture-based enrichment exhibits similar biases: hybridization probes designed against the reference sequence bind reference alleles more efficiently than alternate alleles. These technical biases compound across steps, so a heterozygous variant might consistently show 60:40 or even 70:30 allelic ratios across independent samples processed through the same pipeline.\nThe clinical consequences become acute in contexts where allele fractions carry diagnostic meaning. Loss of heterozygosity in tumor samples, where one allele is deleted or silenced, produces genuine biological imbalance that must be distinguished from technical artifacts. A tumor suppressor gene showing 80:20 allelic ratio could indicate LOH with implications for prognosis and therapy, or could reflect nothing more than capture bias at a technically difficult locus. Without matched normal tissue or population-level calibration of expected allelic ratios, these scenarios remain ambiguous.\nSomatic mosaic variants present at low allele fractions face similar detection challenges. A variant present in 10% of cells produces reads indistinguishable in individual quality from sequencing errors at typical coverage depths. The expected number of alternate reads follows a binomial distribution: at 30× coverage with 10% allele fraction, one expects only 3 alternate reads on average, with substantial probability of observing 0, 1, or 2 reads by chance. Distinguishing this signal from a sequencing error rate of 0.1% to 1% per base requires either much deeper sequencing or molecular techniques like unique molecular identifiers that distinguish true low-frequency variants from PCR duplicates carrying the same error.\nModern variant callers address these challenges by estimating site-specific or sample-specific bias parameters from the data itself, learning from known heterozygous sites throughout the genome rather than assuming uniform 50:50 ratios. These statistical innovations improve sensitivity for mosaic and somatic variant detection but require careful calibration to avoid introducing new sources of systematic error.\n\n\n1.5.4 Complex Variants and Representation\nSmall indels near homopolymers, multi-nucleotide variants (MNVs), and overlapping indels present representation challenges beyond simple detection. The same biological event can often be encoded in multiple equivalent ways depending on alignment and normalization conventions. The variant chr1:100 AT&gt;A might alternatively appear as chr1:101 T&gt;-, with different callers and normalization tools potentially choosing different representations for the identical underlying mutation. These equivalent representations complicate comparisons across pipelines and benchmarks; two callsets may disagree on representation while agreeing on biology, or may appear to agree while representing different events.\nDeep learning models inherit all these errors and uncertainties as their input. If a variant never enters the VCF, no model trained on VCFs can learn its effect. If genotype qualities are miscalibrated, models trained on hard calls may be systematically overconfident in regions where input data are fundamentally noisy. These inherited limitations propagate silently through the analysis chain.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-difficult",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-difficult",
    "title": "1  From Reads to Variants",
    "section": "1.6 Difficult Regions: The Limits of Short-Read Calling",
    "text": "1.6 Difficult Regions: The Limits of Short-Read Calling\nCertain genomic regions resist accurate variant calling regardless of algorithmic sophistication, with their difficulty stemming from fundamental properties of sequence structure that challenge alignment and assembly. These regions are disproportionately responsible for discordant calls between pipelines and technologies (Li 2014). Their clinical importance often exceeds their representation in training data.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 1.4: [Enhancing] Ideogram-style genome overview highlighting regions where short-read variant calling systematically fails. Mark: (1) Segmental duplications (show CYP2D6 region on chr22 as exemplar); (2) HLA complex on chr6p21 (shade entire region); (3) Centromeric and pericentromeric regions; (4) Subtelomeric regions; (5) Example tandem repeat loci (e.g., HTT, FMR1). Inset panel shows approximate percentage of genome in each difficulty class. Second inset shows how long reads resolve a region invisible to short reads.\n\n\n\n\n1.6.1 Segmental Duplications and Gene Families\nThe CYP2D6 gene illustrates how sequence complexity creates clinical blind spots. This gene encodes a cytochrome P450 enzyme responsible for metabolizing approximately 25% of clinically used drugs, including codeine, tamoxifen, and many antidepressants. [Citation Needed] It resides in a complex genomic region alongside two pseudogenes (CYP2D7 and CYP2D8) sharing over 90% sequence identity. Short reads from one copy map almost equally well to another, producing ambiguous alignments that either receive arbitrarily assigned positions or inflated mapping quality scores that mask the underlying uncertainty.\nVariant callers operating in this region face an impossible choice between sensitivity and specificity. Conservative approaches undercall true variation to avoid false positives; aggressive approaches call spurious variants in the wrong paralog. A patient’s CYP2D6 metabolizer status, critical for drug dosing decisions that can mean the difference between therapeutic efficacy and serious adverse events, may be incorrectly inferred from short-read data alone.\n\n\n1.6.2 Low-Complexity and Repetitive Sequence\nHomopolymers, short tandem repeats (STRs), and other low-complexity regions challenge both sequencing chemistry and alignment algorithms. Indel error rates are especially elevated in these contexts, and many pipelines mask or flag these regions as low confidence. Yet variation in repeats can be biologically critical. Triplet repeat expansion disorders including Huntington disease, fragile X syndrome, and myotonic dystrophy arise from unstable repeat sequences that standard short-read pipelines handle poorly. Models trained on callsets that exclude these regions inherit blind spots at clinically important loci.\n\n\n1.6.3 HLA Region Complexity\nThe human leukocyte antigen (HLA) locus on chromosome 6p21 exemplifies both the biological importance and technical difficulty of complex genomic regions. HLA genes including HLA-A, HLA-B, HLA-C, and HLA-DRB1 encode proteins central to immune recognition and represent some of the most polymorphic sequences in the human genome. The region spans several megabases of near-identical sequences interspersed with gene conversions, copy number variation, and pseudogenes.\nStandard reference-based alignment fails in HLA because the extreme polymorphism means reads carrying common, well-characterized alleles may match the linear reference genome poorly. A read from the HLA-B*57:01 allele (clinically important for predicting abacavir hypersensitivity in HIV treatment) may fail to align or align with low mapping quality, causing systematic undercalling of this medically actionable variant (Mallal et al. 2008). The same problems affect HLA typing for transplant matching, autoimmune disease association studies, and pharmacogenomic testing across diverse therapeutic areas (Robinson et al. 2020; Sakaue et al. 2023).\nSpecialized tools address these challenges through alternative strategies. HLA imputation methods use dense reference panels to infer HLA alleles from array genotypes, enabling large-scale association studies that would otherwise require expensive targeted sequencing (Sakaue et al. 2023). Sequence-based typing tools such as T1K perform HLA and KIR (killer immunoglobulin-like receptor) genotyping directly from WES, WGS, or RNA-seq data by aligning reads against allele databases rather than the linear reference (Song et al. 2022). Graph-based approaches incorporate known HLA alleles as alternate paths through the region, improving both alignment and variant calling (Garrison et al. 2018; Liao et al. 2023).\nHLA exemplifies a broader principle: regions that are biologically rich and clinically actionable are often technically difficult. Deep models trained on callsets that downweight or exclude these regions inherit their absence, creating blind spots precisely where accurate genotyping matters most.\nThese challenging regions create systematic evaluation problems for foundation models. The homology-aware data splitting strategies required to avoid leakage in these regions are detailed in Section 21.2. HLA complexity motivates specialized network-based approaches discussed in Section 18.1.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-benchmarks",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-benchmarks",
    "title": "1  From Reads to Variants",
    "section": "1.7 Benchmarking and Ground Truth",
    "text": "1.7 Benchmarking and Ground Truth\nEvaluating variant callers requires high-confidence truth sets and standardized comparison tools. The challenge is that “ground truth” for variant calling is not actually true in any absolute sense; it represents consensus derived from multiple imperfect observations using different technologies and algorithms. Without careful benchmarking design, it is easy to overfit to specific datasets, underestimate errors in difficult regions, or misinterpret the practical significance of small metric improvements. Benchmark construction principles and their limitations for foundation model evaluation are examined comprehensively in Section 20.5 and Chapter 21.\n\n1.7.1 GIAB Reference Samples\nThe Genome in a Bottle (GIAB) Consortium, coordinated by NIST, provides extensively characterized reference samples with validated variant calls across most of the genome (Zook et al. 2019). The primary sample is NA12878 (also known as HG001), a female of European ancestry from the CEPH/Utah pedigree with the longest history of multi-platform characterization. Additional samples span ancestral diversity and family structures: HG002 through HG004 comprise an Ashkenazi Jewish trio enabling trio-based validation, while HG005 through HG007 provide a Han Chinese trio.\nFor each sample, GIAB provides high-confidence variant calls representing consensus from multiple sequencing technologies and variant callers that constitute the best current estimate of true genotypes. Equally important are the high-confidence regions, genomic intervals where the truth set is believed to be reliable. Performance outside these regions remains formally unmeasured. Benchmarking tools such as hap.py and RTG Tools enable standardized comparison of test callsets against truth, implementing reproducible calculation of precision, recall, and F1 metrics by variant type (Krusche et al. 2019; “RealTimeGenomics/Rtg-Core” 2025).\n\n\n1.7.2 Metrics and Their Meaning\nSmall differences in benchmark metrics can obscure large differences in clinical utility, while large headline improvements may concentrate in regions with little practical importance. Standard metrics for variant calling include recall (sensitivity), the fraction of true variants in the benchmark successfully identified by the caller; precision (positive predictive value), the fraction of called variants that are present in the benchmark truth set; and F1 score, the harmonic mean of precision and recall providing a single summary when both matter equally. These metrics are typically reported separately for SNVs and indels and may be stratified by genomic context to reveal where performance degrades.\nMetrics can be defined at different levels: per-variant (did we identify the correct alternate allele?), per-genotype (did we correctly determine zygosity?), or per-site (did we recognize variation at this position regardless of allele?). For downstream models, genotype-level accuracy and sample-level completeness often matter more than simply counting variant matches. A model that receives incorrect genotypes at common regulatory variants will learn corrupted associations even if overall variant-level metrics appear strong.\n\n\n1.7.3 Limitations of Benchmarks\nGIAB truth sets derive primarily from a small number of deeply sequenced samples, predominantly of European ancestry in early releases, and initially focused on genomic regions where high confidence was achievable. High-confidence regions cover approximately 85 to 90 percent of the genome, leaving performance in excluded regions formally unknown. Performance in underrepresented ancestries, in complex structural variant regions, and for novel variant classes may differ substantially from headline GIAB metrics (Zook et al. 2019; Liao et al. 2023).\nWhen benchmarks are reused extensively for method development, the risk of overfitting to benchmark-specific patterns becomes substantial. Pipelines may be tuned to maximize F1 on GIAB-like samples without improving performance on real-world cohorts with different ancestry composition, sequencing protocols, or variant spectra. For deep learning-based callers with large capacity to absorb quirks in training data, this risk is especially acute. Similar concerns apply to benchmarking and evaluation of deep models more broadly (Chapter 20, Chapter 21).\nOngoing efforts from the T2T Consortium and the Human Pangenome Reference Consortium are expanding benchmark scope to include complete genome assemblies and diverse haplotype collections that better represent human genetic diversity (Nurk et al. 2022; Liao et al. 2023).",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-deepvariant",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-deepvariant",
    "title": "1  From Reads to Variants",
    "section": "1.8 DeepVariant: Variant Calling as Image Classification",
    "text": "1.8 DeepVariant: Variant Calling as Image Classification\nClassical variant calling pipelines encode accumulated expert intuition through hand-crafted features and carefully tuned heuristics developed over years of experience with sequencing data. DeepVariant, introduced by Google in 2018, posed a different question: what if we let the model learn these patterns directly from data? The key insight was not better probabilistic modeling of sequencing errors but rather a reformulation of the problem itself. Variant calling becomes image classification, and convolutional neural networks learn to distinguish true variants from artifacts through the same pattern recognition that enables them to classify natural images (Poplin et al. 2018).\n\n1.8.1 Pileup Images as Input\nRepresenting reads as image-like tensors allows the model to learn from raw evidence rather than predefined summary statistics. Patterns invisible to hand-crafted heuristics (strand-biased support clustered at read ends, quality degradation in specific contexts) become learnable. Around each candidate variant site, DeepVariant constructs a multi-channel tensor resembling an image. Each row corresponds to a read overlapping the site, with columns indexing positions relative to the candidate variant. Channels encode multiple features: match or mismatch with the reference, Phred-scaled base quality, mapping quality, strand orientation, support for different alleles, and additional alignment characteristics. The reference sequence and candidate alleles are overlaid as additional channels providing context.\nThis representation transforms the variant calling problem fundamentally. Rather than computing summary statistics (depth, allelic balance, strand bias) and feeding them to a classifier with predefined decision rules, DeepVariant presents the raw evidence to a neural network. The model learns that strand-biased support clustered at read ends looks different from balanced support distributed across read positions without anyone explicitly defining these features or their relative importance. Patterns invisible to hand-crafted heuristics become learnable.\ntest\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 1.5: [Essential] Annotated example of a DeepVariant-style pileup tensor. Show a candidate variant site (e.g., a heterozygous SNV) with approximately 30 reads stacked vertically. Label the multiple channels: (1) read bases with matches in gray, mismatches in color-coded nucleotides; (2) base quality as intensity gradient; (3) mapping quality as separate channel; (4) strand orientation (forward/reverse); (5) reference sequence overlay at top. Include the candidate alleles. Show corresponding genotype posterior output (e.g., P(0/0)=0.02, P(0/1)=0.96, P(1/1)=0.02).\n\n\n\n\n\n1.8.2 Architecture and Training\nThe reformulation of variant calling as image classification enables borrowing architectures proven effective for natural images. DeepVariant uses an Inception-style CNN architecture originally developed for natural image classification. The convolutional architecture that makes this possible is examined in detail in Chapter 6. The network processes the pileup tensor through multiple convolutional layers, pooling operations, and nonlinearities, outputting posterior probabilities over three genotype classes (homozygous reference, heterozygous, homozygous alternate) for each candidate site (Poplin et al. 2018).\nTraining uses high-confidence truth sets such as GIAB genomes. The model observes many examples of true variants and non-variants along with their associated pileup images, learning complex decision boundaries that integrate base quality, mapping quality, local sequence context, and read-level patterns. Where VQSR fits a separate model on hand-selected annotations after initial calling, DeepVariant processes raw evidence directly during the primary classification step.\nThe end-to-end training produces well-calibrated genotype likelihoods across a range of sequencing chemistries, instruments, and read lengths, particularly when fine-tuned for specific experimental contexts (Yun et al. 2021). Once trained, the same architecture generalizes across whole-genome versus whole-exome data, PCR-free versus PCR-amplified libraries, and different sequencing instruments. This adaptability contrasts with classical pipelines where calibration is often a separate, post hoc step requiring platform-specific tuning by experts.\n\n\n1.8.3 Cohort Calling with GLnexus\nDeepVariant operates primarily at the per-sample level, producing a gVCF of genotype likelihoods for each individual sample. To generate a multi-sample VCF suitable for population-scale analysis, these per-sample results must be combined through joint genotyping.\nGLnexus provides this cohort-level integration for DeepVariant gVCFs (Yun et al. 2021). The system merges per-sample likelihoods, applies cohort-level priors informed by observed allele frequencies, and performs multi-sample genotype refinement and filtering. Together, DeepVariant and GLnexus form a modular pipeline where deep learning replaces the per-sample likelihood engine while the overall architecture (per-sample calls, joint genotyping, cohort filtering) remains structurally similar to classical approaches.\nJoint calling improves sensitivity for rare variants by pooling evidence across carriers, ensures consistent variant representation across all samples in a cohort, and enables cohort-level quality filters that identify systematic artifacts visible only across many samples. This combination has become a de facto standard for large WES and WGS projects, including recent releases from gnomAD and the UK Biobank (Karczewski et al. 2020; Bycroft et al. 2018).\n\n\n1.8.4 Comparison with Classical Approaches\nThe fundamental difference between DeepVariant and classical pipelines lies in how evidence is combined. HaplotypeCaller uses pair-HMM models with explicit assumptions about read independence and then applies VQSR to recalibrate quality scores using hand-selected annotation features. DeepVariant processes entire pileups simultaneously, implicitly learning correlations among reads that violate the independence assumptions built into classical probabilistic models.\nThis end-to-end approach offers several practical advantages. Calibration emerges from training rather than requiring separate recalibration steps with their own parameter tuning. Transfer across platforms and even species often succeeds with modest fine-tuning rather than complete redevelopment. The model can detect subtle artifact patterns that escape hand-crafted filters, learning representations of error modes that human experts never explicitly described.\nBoth approaches share important limitations. Neither handles structural variants well; both focus primarily on SNVs and small indels. Both operate within the same overall pipeline framework: alignment, duplicate marking, and joint genotyping remain largely unchanged regardless of the per-sample caller used. DeepVariant is best understood as a drop-in replacement for the per-sample calling step, not a complete reimagining of variant discovery from raw data.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-implications",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-implications",
    "title": "1  From Reads to Variants",
    "section": "1.9 Implications for Genomic Deep Learning",
    "text": "1.9 Implications for Genomic Deep Learning\nNGS and variant calling establish the foundation for genomic deep learning. They determine what data downstream models receive, where coverage exists, and where systematic blind spots remain hidden. Understanding how variants are called, and where that process fails, is essential for interpreting the performance and limitations of every model built on this foundation.\n\n1.9.1 Variants as Atomic Units\nThe output of WES and WGS pipelines (a VCF of SNVs, indels, and inferred genotypes) defines the atomic units that many downstream models operate on. Polygenic risk scores treat variants as weighted features summed across the genome (Section 3.5). GWAS summary statistics quantify associations at individual variant positions. Variant annotation tools classify each site by predicted functional consequence. Foundation models that operate on genotypes rather than raw sequence inherit the variant catalog as their effective vocabulary.\nIf a variant is never called, it cannot appear in training data, and no model can learn its effect. False positives introduce noise into labels and features, teaching models to associate spurious variants with phenotypes. False negatives create blind spots where models must extrapolate from incomplete information, often without any indication that data are missing. Choices about phasing, imputation, and variant representation determine whether models see haplotype-structured inputs, unordered genotypes, or scalar dosage summaries. The quality of variant calls directly limits the quality of everything built upon them.\n\n\n1.9.2 Inherited Biases and Blind Spots\nUpstream decisions constrain what downstream models can learn. If an assay rarely observes indels in certain repeat classes, models trained on those callsets effectively learn a world where such variants do not exist. If certain ancestries are underrepresented in reference panels or truth sets, models may perform poorly for those populations while appearing well-calibrated in benchmarks dominated by European samples. High-confidence region definitions determine which variants enter training sets; variants in excluded regions are invisible to models regardless of their biological importance.\nFor regulatory sequence models and variant effect predictors (Chapter 11 for DNA language models, Section 13.2 for regulatory models, Section 6.2 for CNNs, Section 4.3 for classical approaches), upstream variant calling determines which sites appear as candidates and how often certain sequence patterns are observed in association with functional outcomes. The HLA blind spot in short-read calling means that models trained primarily on short-read callsets will systematically underperform for immune-related variants despite their substantial clinical importance for autoimmune disease, transplant rejection, and drug hypersensitivity. The accuracy ceiling imposed by variant calling quality on downstream variant effect prediction is quantified in Section 14.4, where multiple lines of evidence must be integrated despite systematic gaps in each.\n\n\n1.9.3 Effect Sizes Across the Frequency Spectrum\nVariant calling quality modulates the effective effect sizes that downstream models can detect, with different dynamics for common and rare variation. For common variants contributing to highly polygenic traits, modest genotype error acts as additional measurement noise that attenuates effect size estimates without creating spurious large effects. Improving variant calling in already “easy” genomic regions yields diminishing returns compared to simply increasing sample size.\nFor rare variants with large individual effects, the dynamics change substantially. Loss-of-function variants, damaging missense mutations, and splice-altering changes can have substantial effects on disease risk. Here, false negatives dominate the problem: if the variant is never called, its effect is invisible to association tests and to models trained on called genotypes. Small improvements in recall for clinically important rare variants can have outsized impact on gene discovery and interpretation.\nImputed variants introduce their own effect size modulation. The squared correlation between true and imputed genotypes acts as an attenuation factor: an association with true effect size \\(\\beta\\) behaves as if the effect were approximately \\(r^2 \\beta\\) in downstream analyses using imputed dosages. Improvements in imputation quality, particularly for underrepresented ancestries where current panels perform poorly, directly scale the effective signals that models can learn.\nClinically critical loci often present the most challenging technical contexts, creating a systematic mismatch between importance and data quality. Pharmacogenomic variants in CYP gene families, immune-related variants in HLA, and many other medically actionable sites reside in regions where standard pipelines perform poorly. Global accuracy metrics may change only slightly when these regions improve, but the clinical impact can be substantial.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-reliability",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-reliability",
    "title": "1  From Reads to Variants",
    "section": "1.10 Reliability Landscape",
    "text": "1.10 Reliability Landscape\nVariant calling produces the substrate on which every subsequent model operates. The quality of that substrate varies systematically: high-confidence calls in unique sequence with adequate coverage, uncertain calls in repetitive regions and structural variant breakpoints, systematic gaps where short reads cannot reach. These patterns are not random. They concentrate in genomic regions of particular biological importance: segmental duplications that drive gene family evolution, tandem repeats that modulate gene expression, HLA haplotypes that determine immune response.\nDeepVariant and related approaches demonstrate that learned representations can outperform hand-crafted heuristics, at least on established benchmarks. This paradigm recurs across genomic deep learning: sequence-to-function models that learn regulatory grammar directly from data (Chapter 13), splice predictors that discover motifs without explicit encoding (Chapter 6), language models that learn evolutionary constraint from sequence alone (Chapter 11). In each case, the central question is whether the additional model capacity captures genuine biological signal or overfits to artifacts in training data. The distinction between capturing biological signal versus overfitting to training artifacts is examined systematically in Section 24.4 for interpretability analysis and Section 21.4 for evaluation methodology.\nThe models that follow inherit whatever systematic biases exist in variant calls. A pathogenicity predictor trained on ClinVar labels inherits the sequencing technologies and population composition that generated those labels. A fine-mapping method that trusts variant calls uniformly will miscalibrate its posterior probabilities in difficult regions. Understanding where variant calling succeeds and fails is prerequisite to understanding where downstream models can be trusted.\n\n\n\n\nBrowning, Brian L., Xiaowen Tian, Ying Zhou, and Sharon R. Browning. 2021. “Fast Two-Stage Phasing of Large-Scale Sequence Data.” American Journal of Human Genetics 108 (10): 1880–90. https://doi.org/10.1016/j.ajhg.2021.08.005.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nCirulli, Elizabeth T., Simon White, Robert W. Read, Gai Elhanan, William J. Metcalf, Francisco Tanudjaja, Donna M. Fath, et al. 2020. “Genome-Wide Rare Variant Analysis for Thousands of Phenotypes in over 70,000 Exomes from Two Cohorts.” Nature Communications 11 (1): 542. https://doi.org/10.1038/s41467-020-14288-y.\n\n\nDabernig-Heinz, Johanna, Mara Lohde, Martin Hölzer, Adriana Cabal, Rick Conzemius, Christian Brandt, Matthias Kohl, et al. 2024. “A Multicenter Study on Accuracy and Reproducibility of Nanopore Sequencing-Based Genotyping of Bacterial Pathogens.” Journal of Clinical Microbiology 62 (9): e00628–24. https://doi.org/10.1128/jcm.00628-24.\n\n\nDePristo, Mark A., Eric Banks, Ryan Poplin, Kiran V. Garimella, Jared R. Maguire, Christopher Hartl, Anthony A. Philippakis, et al. 2011. “A Framework for Variation Discovery and Genotyping Using Next-Generation DNA Sequencing Data.” Nature Genetics 43 (5): 491–98. https://doi.org/10.1038/ng.806.\n\n\nGarrison, Erik, Jouni Sirén, Adam M. Novak, Glenn Hickey, Jordan M. Eizenga, Eric T. Dawson, William Jones, et al. 2018. “Variation Graph Toolkit Improves Read Mapping by Representing Genetic Variation in the Reference.” Nature Biotechnology 36 (9): 875–79. https://doi.org/10.1038/nbt.4227.\n\n\nGoodwin, Sara, John D. McPherson, and W. Richard McCombie. 2016. “Coming of Age: Ten Years of Next-Generation Sequencing Technologies.” Nature Reviews Genetics 17 (6): 333–51. https://doi.org/10.1038/nrg.2016.49.\n\n\nJiang, Tao, Yongzhuang Liu, Yue Jiang, Junyi Li, Yan Gao, Zhe Cui, Yadong Liu, Bo Liu, and Yadong Wang. 2020. “Long-Read-Based Human Genomic Structural Variation Detection with cuteSV.” Genome Biology 21 (1): 189. https://doi.org/10.1186/s13059-020-02107-y.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKrusche, Peter, Len Trigg, Paul C. Boutros, Christopher E. Mason, Francisco M. De La Vega, Benjamin L. Moore, Mar Gonzalez-Porta, et al. 2019. “Best Practices for Benchmarking Germline Small Variant Calls in Human Genomes.” Nature Biotechnology 37 (5): 555–60. https://doi.org/10.1038/s41587-019-0054-x.\n\n\nLi, Heng. 2013. “Aligning Sequence Reads, Clone Sequences and Assembly Contigs with BWA-MEM.” arXiv. https://doi.org/10.48550/arXiv.1303.3997.\n\n\n———. 2014. “Towards Better Understanding of Artifacts in Variant Calling from High-Coverage Samples.” Bioinformatics 30 (20): 2843–51. https://doi.org/10.1093/bioinformatics/btu356.\n\n\n———. 2018. “Minimap2: Pairwise Alignment for Nucleotide Sequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness, Glenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome Reference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nLoh, Po-Ru, Petr Danecek, Pier Francesco Palamara, Christian Fuchsberger, Yakir A Reshef, Hilary K Finucane, Sebastian Schoenherr, et al. 2016. “Reference-Based Phasing Using the Haplotype Reference Consortium Panel.” Nature Genetics 48 (11): 1443–48. https://doi.org/10.1038/ng.3679.\n\n\nMallal, Simon, Elizabeth Phillips, Giampiero Carosi, Jean-Michel Molina, Cassy Workman, Janez Tomažič, Eva Jägel-Guedes, et al. 2008. “HLA-B*5701 Screening for Hypersensitivity to Abacavir.” New England Journal of Medicine 358 (6): 568–79. https://doi.org/10.1056/NEJMoa0706135.\n\n\nNielsen, Rasmus, Joshua S. Paul, Anders Albrechtsen, and Yun S. Song. 2011. “Genotype and SNP Calling from Next-Generation Sequencing Data.” Nature Reviews. Genetics 12 (6): 443–51. https://doi.org/10.1038/nrg2986.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V. Bzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The Complete Sequence of a Human Genome.” Science 376 (6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Connell, Jared, Deepti Gurdasani, Olivier Delaneau, Nicola Pirastu, Sheila Ulivi, Massimiliano Cocca, Michela Traglia, et al. 2014. “A General Approach for Haplotype Phasing Across the Full Spectrum of Relatedness.” PLOS Genetics 10 (4): e1004234. https://doi.org/10.1371/journal.pgen.1004234.\n\n\n“PacificBiosciences/Pbsv.” 2025. PacBio. https://github.com/PacificBiosciences/pbsv.\n\n\nPoplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, et al. 2018. “[DeepVariant] A Universal SNP and Small-Indel Variant Caller Using Deep Neural Networks.” Nature Biotechnology 36 (10): 983–87. https://doi.org/10.1038/nbt.4235.\n\n\n“RealTimeGenomics/Rtg-Core.” 2025. Real Time Genomics. https://github.com/RealTimeGenomics/rtg-core.\n\n\nRobinson, James, Dominic J Barker, Xenia Georgiou, Michael A Cooper, Paul Flicek, and Steven G E Marsh. 2020. “IPD-IMGT/HLA Database.” Nucleic Acids Research 48 (D1): D948–55. https://doi.org/10.1093/nar/gkz950.\n\n\nRubinacci, Simone, Diogo M. Ribeiro, Robin J. Hofmeister, and Olivier Delaneau. 2021. “Efficient Phasing and Imputation of Low-Coverage Sequencing Data Using Large Reference Panels.” Nature Genetics 53 (1): 120–26. https://doi.org/10.1038/s41588-020-00756-0.\n\n\nSakaue, Saori, Saisriram Gurajala, Michelle Curtis, Yang Luo, Wanson Choi, Kazuyoshi Ishigaki, Joyce B. Kang, et al. 2023. “Tutorial: A Statistical Genetics Guide to Identifying HLA Alleles Driving Complex Disease.” Nature Protocols 18 (9): 2625–41. https://doi.org/10.1038/s41596-023-00853-4.\n\n\nShafin, Kishwar, Trevor Pesout, Pi-Chuan Chang, Maria Nattestad, Alexey Kolesnikov, Sidharth Goel, Gunjan Baid, et al. 2021. “Haplotype-Aware Variant Calling with PEPPER-Margin-DeepVariant Enables High Accuracy in Nanopore Long-Reads.” Nature Methods 18 (11): 1322–32. https://doi.org/10.1038/s41592-021-01299-w.\n\n\nSmolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W. Horner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024. “Detection of Mosaic and Population-Level Structural Variants with Sniffles2.” Nature Biotechnology 42 (10): 1571–80. https://doi.org/10.1038/s41587-023-02024-y.\n\n\nSong, Li, Gali Bai, X. Shirley Liu, Bo Li, and Heng Li. 2022. “T1K: Efficient and Accurate KIR and HLA Genotyping with Next-Generation Sequencing Data.” bioRxiv. https://doi.org/10.1101/2022.10.26.513955.\n\n\nVan der Auwera, Geraldine A., Mauricio O. Carneiro, Christopher Hartl, Ryan Poplin, Guillermo del Angel, Ami Levy-Moonshine, Tadeusz Jordan, et al. 2018. “From FastQ Data to High-Confidence Variant Calls: The Genome Analysis Toolkit Best Practices Pipeline.” Current Protocols in Bioinformatics 43 (1): 11.10.1–33. https://doi.org/10.1002/0471250953.bi1110s43.\n\n\nWenger, Aaron M., Paul Peluso, William J. Rowell, Pi-Chuan Chang, Richard J. Hall, Gregory T. Concepcion, Jana Ebler, et al. 2019. “Accurate Circular Consensus Long-Read Sequencing Improves Variant Detection and Assembly of a Human Genome.” Nature Biotechnology 37 (10): 1155–62. https://doi.org/10.1038/s41587-019-0217-9.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll, and Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls Using DeepVariant and GLnexus.” Bioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam, and Ruibang Luo. 2022. “Symphonizing Pileup and Full-Alignment for Deep Learning-Based Long-Read Variant Calling.” Nature Computational Science 2 (12): 797–803. https://doi.org/10.1038/s43588-022-00387-x.\n\n\nZook, Justin M., Jennifer McDaniel, Nathan D. Olson, Justin Wagner, Hemang Parikh, Haynes Heaton, Sean A. Irvine, et al. 2019. “An Open Resource for Accurately Benchmarking Small Variant and Reference Calls.” Nature Biotechnology 37 (5): 561–66. https://doi.org/10.1038/s41587-019-0074-6.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html",
    "href": "part_1/p1-ch02-data.html",
    "title": "2  Data Landscape",
    "section": "",
    "text": "2.1 Reference Genomes and Gene Annotations\nGenomic models learn from labels, and those labels come from somewhere. A variant effect predictor trained on ClinVar classifications learns whatever biases clinical laboratories embedded in those classifications. A chromatin accessibility model trained on Encyclopedia of DNA Elements (ENCODE) cell lines may fail on primary tissues absent from the training compendium. A constraint metric derived from European-ancestry cohorts will be poorly calibrated for variants private to other populations. Every machine learning model in genomics inherits both the signal and the systematic gaps of its training data. Understanding what genomic resources contain, and what they systematically miss, is prerequisite to interpreting what models learn.\nNo single dataset captures the complexity of genomic function. The field depends on a mosaic of complementary resources: reference genomes and gene annotations that define the coordinate system, population variant catalogs that reveal what survives in healthy individuals, biobank datasets that link genetic variation to phenotypes at scale, functional genomics atlases that map biochemical activity across cell types and conditions, and clinical databases that aggregate expert variant interpretations. Each resource contributes a different type of evidence. Reference genomes provide the scaffold against which all variants are defined. Population databases like the Genome Aggregation Database (gnomAD) establish baseline expectations for variant frequency. Functional assays from ENCODE and Roadmap Epigenomics indicate where the genome shows evidence of regulatory activity. Clinical databases like ClinVar provide ground-truth labels for pathogenic and benign variants, at least for the subset of variants that have been expertly reviewed.\nThe goal of this chapter is not encyclopedic completeness but critical literacy: the ability to recognize when training data may not represent the population, condition, or variant class at hand. This literacy becomes essential when deploying models in clinical contexts where failures have consequences. The biases introduced by population structure, label ascertainment, and technical artifacts create systematic confounding that propagates through every downstream model. These challenges receive detailed treatment, where Section 22.2.1 examines ancestry-related confounding, Section 22.2.4 addresses ascertainment patterns in clinical labels, and Section 22.2.2 covers technical batch effects. The present chapter establishes the data landscape from which those confounders arise.\nA family arrives at a genetics clinic after their newborn’s screening reveals a potential metabolic disorder. The clinical team orders whole-genome sequencing and receives a report identifying a novel variant in a gene associated with the condition. The variant’s coordinates place it at the boundary between an exon and an intron, potentially disrupting splicing. Yet whether this interpretation is correct depends on decisions made years before the child was born: which positions constitute exon boundaries, which transcript model defines the canonical gene structure, and which sequence serves as the reference against which “variant” is defined. Reference genomes and gene annotations are so foundational that their assumptions often become invisible, yet every downstream analysis inherits the choices embedded in these resources. A model cannot learn about a regulatory element for a transcript that does not exist in the annotation.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-reference",
    "href": "part_1/p1-ch02-data.html#sec-ch02-reference",
    "title": "2  Data Landscape",
    "section": "",
    "text": "2.1.1 Reference Assemblies\nA patient’s clinical sequencing reveals a potentially pathogenic variant in a duplicated region of chromosome 17. The variant calling pipeline reports a confident genotype, the annotation tool predicts a frameshift, and the clinical team prepares to discuss the finding with the family. Yet the “variant” may be an artifact of misalignment: reads from a paralogous sequence elsewhere in the genome mapped incorrectly because the reference assembly collapsed two distinct loci into one. Whether this error occurs, whether it can be detected, and whether the clinical interpretation has any foundation in biological reality all depend on the choice of reference genome.\nMost modern pipelines align reads to a small number of reference assemblies, predominantly Genome Reference Consortium Human Build 38 (GRCh38) or the newer telomere-to-telomere CHM13 assembly (T2T-CHM13) (Nurk et al. 2022). A reference genome is not simply a consensus sequence; it encodes a series of consequential decisions about how to represent duplications, alternate haplotypes, and unresolved gaps. These decisions determine which regions are mappable by short reads, how structural variants are represented, and how comparable results will be across cohorts built on different assemblies. The variant calling pipelines described in Chapter 1 depend fundamentally on these reference choices. Variant callers that rely on these reference assemblies face characteristic failures in difficult regions, as detailed in Section 1.6.\nGraph-based and pangenome references relax the assumption of a single linear reference, representing multiple haplotypes and ancestries within a unified coordinate system (Liao et al. 2023). Comparative multi-species references, such as those used in mammalian constraint maps from the Zoonomia consortium (Sullivan et al. 2023), extend this idea across species, providing evolutionary conservation scores that feed directly into deleteriousness predictors and gene-level constraint metrics discussed in Section 4.1.1 for evolutionary approaches and Section 4.3 for integrated scoring.\nFor most genomic deep learning applications, the practical reality is still GRCh37 or GRCh38 coordinates, often with incremental patches. Models trained on these resources therefore inherit their blind spots: incomplete or collapsed segmental duplications, underrepresented ancestries in pangenome construction, and uneven quality across chromosomes and regions. These limitations concentrate in precisely the regions where variant interpretation matters most (such as the HLA locus, pharmacogenes with structural variation, and segmental duplications harboring disease genes), creating a systematic mismatch between clinical importance and reference quality.\n\n\n2.1.2 Gene Models\nA child presents with developmental delay and muscle weakness. Whole-genome sequencing identifies a novel variant near the DMD gene, which encodes dystrophin and causes Duchenne muscular dystrophy when disrupted. The annotation pipeline reports the variant as intronic and unlikely to affect protein function. Yet DMD spans 2.2 megabases and includes 79 exons with complex alternative splicing; whether this variant disrupts a tissue-specific isoform depends entirely on which transcript model the annotation tool uses. The clinical implications are entirely different, yet the underlying sequence is identical: only the annotation changes.\nGene annotation databases such as GENCODE and RefSeq define the biological vocabulary overlaid on reference coordinates: exon-intron structures, canonical and alternative transcripts, start and stop codons, and untranslated regions (Frankish et al. 2019; O’Leary et al. 2016). These annotations distinguish coding from non-coding variants, identify splice-disrupting mutations, and map functional genomics signals to genes. They also establish the units (genes, transcripts, exons) that downstream models implicitly operate on. Splicing prediction models in Section 6.5 learn splice site grammar from annotated exon-intron boundaries, then apply those patterns to detect both canonical and cryptic sites.\nThe MANE Select project provides a single matched transcript per protein-coding gene that is identical between GENCODE and RefSeq, simplifying clinical interpretation and variant reporting (Morales et al. 2022). This standardization makes variant descriptions consistent across laboratories, yet it privileges a single isoform over biological complexity. In contexts where tissue-specific or developmentally regulated isoforms drive disease (alternative splicing in muscular dystrophies, isoform-specific expression in neuropsychiatric conditions), the canonical transcript may miss the relevant biology.\nNew isoforms continue to be discovered, alternative splicing remains incompletely cataloged, and cell-type-specific transcripts may be absent from bulk-derived annotations. Non-coding RNA genes and pseudogenes are even more unevenly annotated. These gaps propagate through every tool built on them: variant effect predictors cannot score consequences for transcripts that do not exist in their reference annotation, and expression models cannot predict isoforms they were never trained on.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-population",
    "href": "part_1/p1-ch02-data.html#sec-ch02-population",
    "title": "2  Data Landscape",
    "section": "2.2 Population Variant Catalogs and Allele Frequencies",
    "text": "2.2 Population Variant Catalogs and Allele Frequencies\nA clinical geneticist evaluates a child with an undiagnosed syndrome and identifies a novel missense variant in a candidate gene. The question that determines what happens next is deceptively simple: has anyone else carried this variant? If the variant appears in thousands of healthy adults, it is almost certainly benign. If it has never been observed across hundreds of thousands of sequenced genomes, that absence becomes evidence of selective pressure against the variant, strongly suggesting functional consequence. Without population-scale variant catalogs, this inference is impossible, and every rare variant would demand the same level of scrutiny regardless of its actual likelihood of causing disease.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 2.2: [High] Stacked bar chart or waffle plot showing ancestry composition across key resources: gnomAD v4, UK Biobank, ClinVar submissions, GTEx donors, GWAS Catalog participants. Highlight the persistent European overrepresentation (approximately 78% of GWAS participants as of 2019) against global population proportions (Sirugo, Williams, and Tishkoff 2019). Include a small world map inset showing which continental ancestries are represented vs underrepresented.\n\n\n\nAllele frequency, the proportion of chromosomes in a reference population carrying a given variant, serves as one of the most powerful priors in variant interpretation. Beyond simple filtering, allele frequencies inform statistical frameworks for case-control association, provide training signal for deleteriousness predictors, and enable imputation of ungenotyped variants through linkage disequilibrium (see Chapter 3). The catalogs described below have progressively expanded in sample size, ancestral diversity, and annotation depth, transforming variant interpretation from an ad hoc exercise into a quantitative discipline.\nA crucial nuance shapes model interpretation: these catalogs record variants that are compatible with being sampled in the first place. Gene-lethal variants that cause embryonic death or severe childhood disease rarely appear, even when they are biologically informative. Variants causing late-onset conditions (Alzheimer’s risk alleles, adult-onset cancer predisposition) can persist at appreciable frequencies because selection has not had time to remove them. Models trained on population data can only learn from variants present in these catalogs, which means they systematically underrepresent the most severe loss-of-function mutations.\n\n2.2.1 dbSNP and Variant Identifiers\nTwo laboratories sequence the same patient and report their findings to a tumor board. Laboratory A describes a variant using genomic coordinates on GRCh38; Laboratory B uses HGVS nomenclature relative to a specific transcript. Are they discussing the same variant? Without standardized identifiers, this simple question can consume hours of manual reconciliation. The database of Single Nucleotide Polymorphisms (dbSNP) provides the common currency that cuts through this ambiguity: stable identifiers (rsIDs) that enable integration across tools and publications (Sherry et al. 2001).\nWhen a laboratory reports a variant, when a researcher publishes a GWAS finding, and when a clinician queries a pathogenicity database, they need a common language to ensure they are discussing the same genomic position. Modern whole-exome and whole-genome sequencing routinely discovers millions of previously unseen variants per large cohort, but dbSNP identifiers remain the standard way to reference known single nucleotide polymorphisms (SNPs) and link disparate resources. When a GWAS publication reports an association at rs12345, that identifier traces back to dbSNP and enables integration with functional annotations, clinical databases, and population variant catalogs.\n\n\n2.2.2 1000 Genomes and Early Reference Panels\nGenotyping arrays measure only a sparse subset of genomic positions, yet disease-associated variants may lie anywhere in the genome. How can researchers infer variants at unmeasured positions? The answer lies in patterns of co-inheritance: variants that travel together on ancestral chromosome segments can be inferred from neighboring measured positions. This process of imputation depends entirely on having reference panels that capture the haplotype structure of the population being studied.\nThe 1000 Genomes Project provided one of the first widely used multi-population panels for imputation, sampling individuals from African, European, East Asian, South Asian, and admixed American populations (Auton et al. 2015). The resulting haplotype structure underlies many imputation servers and downstream analyses, enabling genotyping arrays with millions of markers to impute tens of millions of untyped variants through linkage disequilibrium (Yun et al. 2021). Although its sample size (approximately 2,500 individuals) is modest by current standards, 1000 Genomes established the template for how to build and distribute multi-population reference panels, and its samples continue to serve as benchmarks for variant calling performance. The role of imputation in GWAS is discussed further in Chapter 3.\n\n\n2.2.3 Genome Aggregation Database (gnomAD)\nDistinguishing genuinely rare variants from sampling gaps requires population-scale catalogs with two properties: sufficient sample size to detect low-frequency variants reliably, and sufficient ancestral diversity to avoid misclassifying variants common in underrepresented populations. A variant at 1% frequency in African populations but absent from European cohorts would be incorrectly flagged as novel by any database sampling only European individuals. The Genome Aggregation Database (gnomAD) addresses both requirements by aggregating exome and genome sequencing data from research and clinical cohorts worldwide into harmonized allele frequency resources spanning hundreds of thousands of individuals (Karczewski et al. 2020).\ngnomAD provides high-resolution allele frequencies stratified by genetic ancestry, enabling population-matched filtering that accounts for variants common in one ancestry but rare in others. This stratification matters because a variant observed at 1% frequency in African populations but absent from European cohorts would be incorrectly flagged as ultra-rare by a model trained predominantly on European data.\ngnomAD also introduced constraint metrics that have become standard features in variant prioritization. The probability of loss-of-function intolerance (pLI) and loss-of-function observed/expected upper bound fraction (LOEUF) summarize how depleted a gene is for protein-truncating variants relative to expectation. Genes essential for viability show far fewer loss-of-function variants than neutral mutation rates would predict; this depletion provides evidence of selective constraint that transfers to variant interpretation. A novel truncating variant in a highly constrained gene warrants more concern than the same variant class in an unconstrained gene. These constraint metrics serve as important features in many variant effect predictors discussed in Section 4.3 and Chapter 14.\nPopulation frequencies from gnomAD provide critical filtering steps in clinical variant interpretation pipelines, as detailed in Section 26.1.2. The constraint metrics derived from gnomAD form a foundation for variant effect prediction discussed in Section 4.1.1. Allele frequency distributions also inform fine-mapping approaches that assign causal probability to GWAS-associated variants (Section 3.3).”\n\n\n\n\n\n\nNoteInterpreting Constraint Metrics\n\n\n\npLI (probability of loss-of-function intolerance) estimates the probability that a gene falls into the class of haploinsufficient genes where loss of one copy causes disease. Scores range from 0 to 1; genes with pLI &gt; 0.9 are considered highly constrained. pLI’s categorical nature (genes are classified as tolerant, intermediate, or intolerant) limits its resolution for genes with intermediate constraint.\nLOEUF (loss-of-function observed/expected upper bound fraction) provides a continuous measure by computing the ratio of observed to expected loss-of-function variants, with a 90% confidence upper bound. Lower LOEUF values indicate stronger constraint. A gene with LOEUF of 0.2 has observed only 20% as many truncating variants as expected under neutral evolution. LOEUF has largely superseded pLI in contemporary analyses due to its continuous scale and more intuitive interpretation.\n\n\nThese resources are indispensable for filtering common variants in Mendelian disease diagnostics, distinguishing ultra-rare variants from recurrent ones, and providing population genetics priors for deleteriousness scores like CADD (Rentzsch et al. 2019; Schubach et al. 2024). At the same time, they reflect the composition of the cohorts they aggregate: ancestry representation remains uneven despite ongoing efforts, structural variants and repeat expansions are less completely cataloged than SNVs and short indels, and individuals with severe early-onset disease are underrepresented by design. These biases propagate into every model that uses gnomAD frequencies or constraint scores as features.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-biobanks",
    "href": "part_1/p1-ch02-data.html#sec-ch02-biobanks",
    "title": "2  Data Landscape",
    "section": "2.3 Biobanks and GWAS Data",
    "text": "2.3 Biobanks and GWAS Data\nA pharmaceutical company developing a new cardiac drug needs to understand which genetic variants influence drug response. A health system implementing pharmacogenomic testing needs to know which patients are at risk for adverse reactions. A researcher studying the genetics of depression needs cases and controls with standardized phenotyping. None of these questions can be answered by sequencing alone; they require linking genetic variation to phenotypes at scale, across thousands or hundreds of thousands of individuals. Yet assembling such cohorts introduces its own biases: participants must consent, provide samples, and have phenotypes recorded in standardized ways. The populations enrolled in major biobanks reflect patterns of healthcare access, research infrastructure, and historical priorities that do not represent global genetic diversity.\nThe overrepresentation of European-ancestry individuals in most major biobanks creates systematic gaps in variant discovery, effect-size estimation, and polygenic score portability that propagate through downstream analyses (Sirugo, Williams, and Tishkoff 2019). A variant common in West African populations may be absent or rare in European-dominated catalogs, rendering it invisible to association studies and underrepresented in predictive models. This tension between scientific utility and representational equity shapes every biobank-derived resource and is discussed in detail in Chapter 22.\n\n2.3.1 Large Population Cohorts\nA variant that increases heart disease risk by 5% (OR = 1.05) requires approximately 50,000 cases and 50,000 controls to detect reliably at genome-wide significance (\\(\\alpha = 5 \\times 10^{-8}\\)). A variant shifting a continuous trait like blood pressure by 0.05 standard deviations demands even larger samples, often exceeding 100,000 individuals. The fundamental constraint is statistical: detecting small effect sizes against a backdrop of millions of tested variants requires both stringent significance thresholds and massive sample sizes to achieve adequate power. Required sample size scales with the inverse square of effect size, meaning a variant with half the effect requires four times the sample. This relationship explains why genetic discovery accelerated dramatically when biobanks reached the scale of hundreds of thousands of participants. Chapter 25 provides a detailed treatment of these statistical foundations and their implications for clinical risk prediction.\nUK Biobank, with approximately 500,000 participants and deep phenotyping across thousands of traits, has become a dominant resource for methods development and benchmarking (Bycroft et al. 2018). FinnGen leverages Finland’s population history and unified healthcare records for large-scale disease association discovery (Kurki et al. 2023). The All of Us Research Program prioritizes diversity, aiming to enroll one million participants with deliberate oversampling of historically underrepresented groups (null 2019). deCODE genetics has genotyped a substantial fraction of Iceland’s population, enabling unique studies of rare variants and founder effects in a population with detailed genealogical records (Gudbjartsson et al. 2015). Additional resources include the Million Veteran Program, Mexican Biobank, BioBank Japan, China Kadoorie Biobank, and emerging African genomics initiatives such as H3Africa (Sirugo, Williams, and Tishkoff 2019).\nTogether, these efforts enable genome-wide association studies (GWAS) for thousands of traits, development and evaluation of polygenic scores, and fine-mapping of causal variants and genes (Marees et al. 2018; Mountjoy et al. 2021). From a modeling perspective, they provide the large-scale genotype-phenotype matrices that power architectures ranging from classical linear mixed models to foundation models trained on biobank-scale data. The practical reality for most GWAS and polygenic score methods in Chapter 3 is data from either array genotyping with imputation or whole-exome/whole-genome sequencing with joint calling, as in DeepVariant/GLnexus-style pipelines (Yun et al. 2021).\n\n\n2.3.2 GWAS Summary Statistics\nIndividual-level genotype and phenotype data are powerful but sensitive. Sharing such data across institutions requires complex data use agreements, institutional review board approvals, and secure computing infrastructure. These barriers would slow scientific progress if every analysis required access to raw data. Summary statistics offer an alternative: per-variant effect sizes, standard errors, and p-values that capture the essential association signal without revealing individual genotypes.\nThe GWAS Catalog compiles published results across thousands of traits, while the PGS Catalog provides curated polygenic score weights and metadata for reproducibility (Sollis et al. 2023; Lambert et al. 2021). Frameworks like Open Targets Genetics integrate fine-mapped signals with functional annotations to prioritize candidate causal genes at associated loci (Mountjoy et al. 2021).\nSummary statistics enable meta-analysis across cohorts without sharing individual-level data, transfer of genetic findings to new populations through methods like PRS-CSx, and integration with functional annotations to distinguish causal variants from linked bystanders (Ruan et al. 2022). For deep learning, summary statistics provide a sparse, trait-level view of the genome. This sparsity creates different challenges than the dense labels available in functional genomics, but also different opportunities: the genetic architecture revealed through GWAS informs polygenic score construction (Section 3.5) and indicates which variants and pathways merit follow-up with regulatory models (Chapter 13) and variant effect predictors (Chapter 14).",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-functional",
    "href": "part_1/p1-ch02-data.html#sec-ch02-functional",
    "title": "2  Data Landscape",
    "section": "2.4 Functional Genomics and Regulatory Landscapes",
    "text": "2.4 Functional Genomics and Regulatory Landscapes\nProtein-coding exons constitute roughly 1.5% of the human genome, yet most disease-associated variants from GWAS fall outside coding regions. A massive study identifies 100 loci associated with schizophrenia, but 90 of them lie in non-coding regions with no obvious connection to any gene. This mismatch creates a fundamental interpretability problem: we can identify non-coding loci that harbor disease risk, but we cannot easily determine which base pairs matter, which genes they regulate, or in which cell types they act. Understanding these non-coding variants requires mapping the regulatory logic that governs when, where, and how much each gene is expressed. Functional genomics assays provide this map, identifying transcription factor binding sites, nucleosome positioning, chromatin accessibility, histone modifications, and three-dimensional genome organization across cell types and conditions.\nFunctional genomics datasets serve a dual role in genomic deep learning. First, they supply the biological vocabulary for interpreting non-coding variants, linking sequence changes to potential regulatory consequences. Second, and more directly, they provide the training labels for sequence-to-function models. When a model learns to predict chromatin accessibility or histone marks from DNA sequence alone, it compresses into its parameters the regulatory code implicit in thousands of functional genomics experiments.\n\n2.4.1 ENCODE, Roadmap, and Related Consortia\nA single ChIP-seq experiment for one transcription factor in one cell line provides useful signal, but models that learn general regulatory grammar require thousands of such experiments spanning many factors, marks, and cell types. A researcher training a regulatory model on her own laboratory’s data will produce a model that works well in her specific experimental context but fails to generalize. The key insight behind ENCODE and Roadmap was that coordinated experimental campaigns, with standardized methods and quality control, could create reference datasets serving the entire field.\nThe Encyclopedia of DNA Elements (ENCODE) and Roadmap Epigenomics consortia designed coordinated experimental campaigns that profiled transcription factor binding (ChIP-seq), histone modifications, chromatin accessibility (DNase-seq, ATAC-seq), and chromatin conformation (Hi-C) across cell lines and primary tissues (Kagda et al. 2025; Kundaje et al. 2015). Gene Expression Omnibus (GEO) archives these and many other functional genomics datasets with standardized metadata (Edgar, Domrachev, and Lash 2002).\nThe significance of these consortia lies less in any individual experiment than in the scale and standardization they provide. By generating hundreds of assays across dozens of cell types with consistent protocols, ENCODE and Roadmap created canonical reference datasets that define the regulatory landscape for the cell types they profiled. These resources enabled multiple generations of regulatory models. DeepSEA (Section 6.2) pioneered multi-task learning on ENCODE chromatin accessibility and transcription factor binding, where each prediction task corresponds to a ChIP-seq or accessibility experiment. Enformer (Section 13.2) extended this paradigm with transformer attention mechanisms and longer context windows. The progression from convolutional to attention-based architectures reflects both the richness of ENCODE data and its limitations: models trained on these resources inherit ENCODE’s choices about which cell types, factors, and experimental conditions merit inclusion.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 2.3: [High] Heatmap-style visualization showing the ENCODE/Roadmap data compendium structure. Rows represent cell types or tissues (group by category: cell lines, primary cells, tissues). Columns represent assay types (ChIP-seq for various TFs and histone marks, DNase-seq, ATAC-seq, RNA-seq). Color intensity indicates data availability (present/absent or coverage depth). Highlight which cell types have comprehensive coverage vs sparse coverage. Annotate example cell types that are well-profiled (K562, GM12878, HepG2) vs disease-relevant tissues that remain undersampled.\n\n\n\n\n\n2.4.2 Cistrome Data Browser\nENCODE and Roadmap provide authoritative datasets for their chosen cell types and factors, but they represent only a fraction of publicly available functional genomics experiments. A researcher interested in a specific transcription factor or a disease-relevant cell type may find that ENCODE lacks the relevant data, even though dozens of laboratories have generated relevant ChIP-seq experiments. These experiments exist scattered across GEO with heterogeneous processing and quality.\nThe Cistrome Data Browser addresses this gap by aggregating thousands of human and mouse ChIP-seq and chromatin accessibility datasets from ENCODE, Roadmap, GEO, and individual publications into a uniformly reprocessed repository (Zheng et al. 2019). All datasets pass through standardized quality control and peak calling, enabling comparisons across experiments originally generated with different protocols.\nCistrome provides uniform peak calls, signal tracks, and metadata for cell type, factor, and experimental conditions. The tradeoff is heterogeneity: while reprocessing harmonizes computational steps, the underlying experiments vary in sample preparation, antibody quality, sequencing depth, and experimental design. Cistrome expands coverage at the cost of the tight experimental control found in the primary consortia, a tradeoff that matters when models learn from noisy or inconsistent labels.\n\n\n2.4.3 From Assays to Training Labels\nSequence-to-function models transform functional genomics resources into supervised learning problems. Models like DeepSEA (see Chapter 13) draw training labels from ENCODE, Roadmap, and Cistrome-style datasets: each genomic window is associated with binary or quantitative signals indicating transcription factor binding, histone modifications, or chromatin accessibility across hundreds of assays and cell types (Zhou and Troyanskaya 2015; Zhou et al. 2018).\nThe quality, coverage, and biases of these labels directly constrain what models can learn. Cell types absent from the training compendium cannot be predicted reliably. Factors with few high-quality ChIP-seq experiments will have noisier labels. Systematic differences between assay types (binary peak calls versus quantitative signal tracks) shape whether models learn to predict occupancy, accessibility, or something in between. These considerations become central when examining model architectures and training strategies in Chapter 13.\n\n\n2.4.4 Deep Mutational Scanning and Multiplexed Variant Assays\nPopulation variant catalogs tell us which variants survive in healthy individuals, but they cannot tell us what happens when a specific amino acid is changed to every possible alternative. Functional genomics experiments reveal where the genome is active, but they do not directly measure the consequence of each possible mutation. Deep mutational scanning (DMS) fills this gap by measuring the fitness or functional impact of thousands of protein or regulatory variants in a single experiment.\nThese assays systematically introduce mutations (often approaching saturation mutagenesis for a protein domain or regulatory element), subject the resulting library to selection or screening, and use sequencing to quantify the representation of each variant before and after selection. The result is dense, quantitative measurements of variant effects under controlled conditions. Benchmarks such as ProteinGym compile large DMS datasets across proteins to evaluate variant effect predictors. TraitGym curates multiplexed reporter assays and other high-throughput readouts of regulatory variant effects (Notin et al. 2023; Benegas, Eraslan, and Song 2025).\nThese resources sit at the interface between genomic and protein-level modeling. Where gnomAD and biobanks catalog sparse, naturally occurring variation, DMS datasets offer dense, quantitative functional measurements across systematic variant libraries that test most or all possible substitutions. DMS data differ fundamentally from population catalogs: they measure functional impact directly under controlled conditions rather than inferring it from population survival. Protein sequence models (Chapter 12) and regulatory variant predictors (Chapter 14) use these DMS-style datasets as key benchmarks and training sources.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-expression",
    "href": "part_1/p1-ch02-data.html#sec-ch02-expression",
    "title": "2  Data Landscape",
    "section": "2.5 Expression and eQTL Resources",
    "text": "2.5 Expression and eQTL Resources\nFunctional genomics assays reveal where transcription factors bind and which chromatin regions are accessible, but they do not directly answer the downstream question: does regulatory activity actually change how much RNA a gene produces? A transcription factor may bind a genomic region without altering expression of nearby genes; an accessible chromatin region may not contain active regulatory elements. Regulatory binding and gene expression exist in a many-to-many relationship that cannot be resolved by either measurement alone. Expression datasets complete this link, measuring transcript abundance across tissues, cell types, and genetic backgrounds.\nConnecting non-coding GWAS variants to their effector genes requires mechanistic hypotheses: some indication of which gene a regulatory variant actually regulates. Expression quantitative trait loci (eQTLs) provide exactly this connection, identifying genetic variants statistically associated with transcript-level changes. When a GWAS signal colocalizes with an eQTL for a nearby gene in disease-relevant tissue, that gene becomes a candidate effector. For model training, expression data provide quantitative labels that integrate across many regulatory inputs converging on a single promoter.\n\n2.5.1 Bulk Expression Atlases\nA GWAS identifies a locus associated with coronary artery disease in a non-coding region. Dozens of genes lie within the associated interval. Which one mediates the disease risk? If the lead variant also associates with expression of a nearby gene specifically in arterial endothelial cells, that gene becomes the prime candidate. Without tissue-specific expression data linked to genotypes, this inference is impossible.\nThe Genotype-Tissue Expression (GTEx) consortium provides the most comprehensive resource linking genetic variation to gene expression across human tissues, with RNA-seq profiles from 948 post-mortem donors across 54 tissues (THE GTEX CONSORTIUM 2020). GTEx established foundational insights that inform regulatory genomics models: most genes harbor tissue-specific eQTLs, regulatory variants typically act in cis over distances of hundreds of kilobases, and expression variation explains a meaningful fraction of complex trait heritability.\nGTEx underlies expression prediction models such as PrediXcan, which trains tissue-specific models to impute gene expression from genotypes alone (Gamazon et al. 2015). Transcriptome-wide association studies (TWAS) extend this idea to associate imputed expression with phenotypes (Gusev et al. 2016). Colocalization methods ask whether a GWAS signal and an eQTL share the same causal variant, providing evidence that the associated gene mediates the trait effect.\nThe GTEx design has limitations worth acknowledging. Post-mortem collection introduces agonal stress artifacts that may not reflect living tissue biology. Sample sizes vary considerably across tissues (hundreds for some, dozens for others), affecting statistical power. Some disease-relevant tissues, such as pancreatic islets or specific brain subregions, remain undersampled. Complementary resources like the eQTLGen Consortium aggregate eQTL results from blood across much larger sample sizes, trading tissue diversity for statistical power (Võsa et al. 2021).\n\n\n2.5.2 Single-Cell and Context-Specific Expression\nBulk RNA-seq averages expression across all cells in a tissue sample, obscuring the cell-type-specific programs that often mediate disease biology. A bulk eQTL in brain tissue might reflect astrocytes, neurons, microglia, or oligodendrocytes; the causal cell type matters for understanding mechanism. This averaging creates a fundamental resolution problem: variants may have strong effects in rare cell populations that are diluted to undetectability when mixed with other cell types.\nSingle-cell RNA-seq resolves this heterogeneity, identifying expression signatures for individual cell types, rare populations, and transitional states. Large-scale efforts including the Human Cell Atlas and Tabula Sapiens are building reference atlases that catalog cell types across organs and developmental stages (Regev et al. 2017; THE TABULA SAPIENS CONSORTIUM 2022). For variant interpretation, single-cell data enable cell-type-specific eQTL mapping, revealing that a variant may influence expression in one cell type but not others within the same tissue. Spatial transcriptomics adds anatomical context, preserving tissue architecture while measuring gene expression.\nThese technologies introduce computational challenges: sparsity from dropout effects, batch variation across samples and technologies, and massive scale with millions of cells per study. They also offer an increasingly fine-grained view of the link between genotype, regulatory state, and cellular phenotype. Multi-omics integration (Chapter 19) and systems-level modeling draw heavily on single-cell and spatial resources.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-protein-databases",
    "href": "part_1/p1-ch02-data.html#sec-ch02-protein-databases",
    "title": "2  Data Landscape",
    "section": "2.6 Protein Databases",
    "text": "2.6 Protein Databases\n… Update with PDB; UniRef; BFD; etc…",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-phenotypes",
    "href": "part_1/p1-ch02-data.html#sec-ch02-phenotypes",
    "title": "2  Data Landscape",
    "section": "2.7 Phenotype Definition and Data Quality",
    "text": "2.7 Phenotype Definition and Data Quality\nEvery model in genomics learns from labels, but phenotype labels carry their own biases distinct from variant annotations or functional genomics measurements. A GWAS for type 2 diabetes depends entirely on how diabetes is defined: by self-report, ICD-10 codes, hemoglobin A1c thresholds, medication records, or clinical adjudication. Each definition captures a different slice of the underlying biology. Self-report misses undiagnosed cases. ICD codes reflect billing practices as much as clinical reality. Laboratory thresholds impose sharp boundaries on continuous metabolic dysregulation. The “same” phenotype defined differently yields different genetic architectures, different effect sizes, and different polygenic score performance.\nThis sensitivity to phenotype definition compounds as biobanks scale. UK Biobank’s 500,000 participants enable discovery at unprecedented statistical power, but that power is limited by the precision of the phenotypes being tested. A GWAS with millions of participants but noisy case definitions may have less effective power than a smaller study with carefully adjudicated outcomes. The trade-off between sample size and phenotype quality pervades modern statistical genetics, and understanding its contours is essential for interpreting what models trained on biobank data actually learn.\nPhenotype quality issues create systematic confounding in GWAS (Section 3.8.3) and clinical risk prediction models (Section 25.4). Deep phenotyping approaches that extract richer representations from EHR data are examined in Section 3.8.4 for GWAS contexts and Section 25.3 for clinical deployment.”\n\n2.7.1 Problem of Binary Disease Definitions\nMost GWAS treat disease as binary: case or control, affected or unaffected. This simplification enables standard statistical machinery but discards information about disease severity, age of onset, trajectory, and subtype. Two patients both labeled “coronary artery disease” may differ in clinically meaningful ways: one experienced an acute myocardial infarction at age 45, the other underwent elective stenting for stable angina at 72. Collapsing this heterogeneity into a single binary label forces genetic analyses to identify variants associated with an artificial composite rather than biologically coherent disease entities.\nThe consequences extend beyond reduced statistical power. Phenotype heterogeneity can induce genetic heterogeneity, where different genetic variants predispose to different subtypes that have been artificially combined. A GWAS for “depression” that includes melancholic depression, atypical depression, and adjustment disorders will identify variants associated with the mixture rather than any specific syndrome. The resulting polygenic scores predict the mixture, potentially missing stronger associations with homogeneous subtypes and providing weaker stratification than would be achievable with cleaner phenotype definitions.\nClinical endpoints also differ in their proximity to genetic effects. Biomarkers such as LDL cholesterol or blood pressure lie closer to gene function than clinical outcomes such as myocardial infarction or stroke, which require the biomarker dysregulation to persist, interact with environmental factors, and culminate in tissue damage. Genetic effects are typically larger and more readily detected for intermediate phenotypes than for distal clinical outcomes. This motivates strategies that analyze biomarkers as outcomes in their own right, then connect genetic effects on biomarkers to disease risk through Mendelian randomization or mediation analysis.\n\n\n2.7.2 Electronic Health Record Quality and Completeness\nElectronic health records promise comprehensive phenotyping at scale: every diagnosis, procedure, medication, and laboratory result captured in structured or semi-structured form. In practice, EHR data are messy, incomplete, and shaped by processes far removed from biology. A diagnosis code reflects not just what the patient has but what the clinician chose to document, what the billing system required, and what the coding specialist interpreted. The same clinical presentation may receive different codes depending on the setting, the clinician’s documentation habits, and institutional coding policies.\nMissing data pervades EHR phenotyping. Laboratory values are measured when clinically indicated, not at random, creating informative missingness where the absence of a measurement conveys information about the patient’s health status. Patients who transfer between health systems appear to have incomplete histories. Conditions managed by specialists outside the health system may be entirely absent from the record. These gaps are not random but systematically related to patient characteristics, healthcare access, and disease severity in ways that can bias genetic analyses.\nTemporal dynamics add further complexity. Disease onset rarely corresponds to diagnosis date; patients carry pathology for years before clinical recognition. Medication records indicate prescriptions but not adherence. Procedure dates capture interventions but not the progression of disease that motivated them. Time-to-event analyses must grapple with left truncation (patients entering observation after disease onset), interval censoring (disease status observed only at discrete timepoints), and the distinction between incident and prevalent cases that confounds cross-sectional analyses.\n\n\n2.7.3 Coding Inconsistencies and Label Noise\nThe International Classification of Diseases provides a standardized vocabulary, but standardized vocabulary does not guarantee standardized application. ICD-10 contains over 70,000 codes, and clinical coders must choose among them based on physician documentation that may be ambiguous, incomplete, or inconsistent. Studies comparing chart review to coded diagnoses find substantial discordance: some patients with clear clinical disease lack corresponding codes, while others have codes without supporting clinical evidence. [Citation Needed]\nCode usage also evolves over time. The transition from ICD-9 to ICD-10 in the United States (October 2015) created discontinuities in phenotype definitions built on specific codes. Clinical practice changes alter what conditions are tested for, diagnosed, and coded. COVID-19’s emergence created entirely new codes and altered coding patterns for respiratory illness more broadly. Longitudinal analyses spanning coding transitions or practice changes must account for these artifacts or risk confusing temporal trends in coding with temporal trends in disease.\nLabel noise from coding errors propagates into every downstream analysis. A phenotype definition with 10% misclassification (5% false positives, 5% false negatives) substantially attenuates genetic effect sizes and reduces GWAS power. For rare diseases where cases are precious, false positives among controls matter less than false negatives among cases, which dilute the genetic signal. For common diseases where controls are presumed healthy, false negatives among controls (undiagnosed cases) similarly attenuate associations. The magnitude of this attenuation depends on disease prevalence, misclassification rates, and their correlation with genetic risk.\n\n\n2.7.4 Deep Phenotyping Approaches\nRecognition of these limitations has motivated deep phenotyping strategies that move beyond binary disease definitions. Quantitative phenotypes, when available, preserve information that binary thresholds discard. Rather than dichotomizing blood pressure into hypertensive versus normotensive, analyzing systolic and diastolic pressure as continuous traits captures the full distribution of genetic effects. Similarly, imaging-derived phenotypes (cardiac MRI measurements, brain volume, bone density) provide precise quantitative endpoints with higher heritability than clinical disease outcomes.\nPhenotype refinement uses clinical features to identify more homogeneous subgroups. Clustering patients by age of onset, comorbidity patterns, or biomarker profiles can reveal subtypes with distinct genetic architectures. Type 2 diabetes, for instance, has been decomposed into clusters defined by age, BMI, insulin resistance, and beta-cell function, with different clusters showing different genetic associations and different disease trajectories. [Citation Needed] Such stratification requires sufficient clinical data to define subgroups, limiting its application to well-phenotyped cohorts.\nA more radical approach abandons expert-specified phenotype criteria entirely. Instead of encoding clinical knowledge through hierarchical ontologies, embedding methods learn vector representations of clinical concepts from co-occurrence patterns in EHR data. Word2Vec models trained on ICD-10 code sequences position clinically related codes near each other in this learned space; codes that co-occur in patient records cluster together regardless of their position in the ICD ontology. Large language models can generate similar phenotype embeddings from textual descriptions, capturing semantic relationships encoded in clinical language.\nThese embeddings can serve as phenotypes themselves. Xu et al. demonstrated that GWAS conducted on EHR-embedding dimensions identified heritable components of clinical phenotype structure, with genetic correlations revealing coherent trait clusters such as cardiovascular disease risk factors (Xu et al. 2025). The embeddings capture phenotypic relationships that binary disease definitions obscure, potentially improving the power to detect genetic associations and the transferability of polygenic scores across related traits.\n\n\n2.7.5 Impact on Downstream Modeling\nPhenotype quality constraints propagate through every analysis built on biobank data, creating systematic confounding that affects both GWAS (Section 3.8.3) and clinical risk prediction models (Section 25.4). Polygenic scores trained on noisy phenotypes learn to predict the noise alongside the signal, potentially inheriting coding artifacts, temporal discontinuities, and population-specific documentation practices. Transfer learning from one biobank to another may fail not because the underlying genetic architecture differs but because the phenotype definitions differ in ways that alter what the model learned.\nFoundation models face analogous challenges. A model that learns associations between genetic variants and EHR-derived phenotypes absorbs whatever systematic distortions those phenotypes contain. If a diagnosis is more likely to be coded in patients who receive specialist care, the model learns a genetic signature for healthcare access as much as for disease biology. If a biomarker is measured only in symptomatic patients, the model learns from a biased sample that may not represent the population distribution. Deep phenotyping approaches that extract richer representations from EHR data offer partial solutions, examined in Section 3.8.4 for GWAS contexts and Section 25.3 for clinical deployment.\nThese considerations motivate careful phenotype documentation in model development. Specifying exactly how a phenotype was defined, which codes or criteria were applied, what exclusions were made, and how temporal boundaries were established enables assessment of whether findings will generalize to settings with different definitions. The goal is not perfect phenotyping, which remains unattainable, but transparent phenotyping that allows downstream users to understand what the model actually learned and where its assumptions may break down.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-clinical",
    "href": "part_1/p1-ch02-data.html#sec-ch02-clinical",
    "title": "2  Data Landscape",
    "section": "2.8 Variant Interpretation Databases and Clinical Labels",
    "text": "2.8 Variant Interpretation Databases and Clinical Labels\nA family receives whole-exome sequencing results for their child with developmental delay. The laboratory report lists 50 rare variants in genes associated with neurodevelopmental disorders. For each variant, the clinical team must answer: is this the cause? Allele frequencies tell us what variants survive in healthy populations, and functional genomics data reveal where the genome is biochemically active, but neither directly answers this question. That determination requires integrating multiple lines of evidence (family segregation, functional assays, computational predictions, phenotypic observations) into a structured framework that can be applied consistently.\nClinical variant interpretation databases aggregate these assessments from laboratories, expert panels, and research groups. These databases have become critical infrastructure for both clinical genomics and computational method development, providing labels that inform diagnostic decisions and serve as training data for machine learning models. Their labels carry biases and circularity that propagate through any analysis built on them, yet no viable alternative exists for large-scale model training and evaluation.\n\n2.8.1 ClinVar and Clinical Assertions\nA clinical laboratory sequences a patient with suspected hereditary cancer syndrome and identifies a missense variant in BRCA2. Before returning results, the laboratory searches ClinVar and finds that three other laboratories have evaluated this variant: two classified it as likely pathogenic, one as a variant of uncertain significance. How should this conflicting evidence inform the final report? ClinVar aggregates assertions of variant pathogenicity from clinical laboratories and researchers worldwide, making it the central clearinghouse for clinical variant interpretations (Landrum et al. 2018).\nClinVar provides standardized classifications following ACMG/AMP guidelines (pathogenic, likely pathogenic, benign, likely benign, variant of uncertain significance) that are central to diagnostic pipelines and to benchmarking variant effect predictors. It has become the de facto reference for variant pathogenicity labels, but its contents reflect systematic biases that affect any downstream use. These biases operate at multiple levels and warrant careful consideration.\nSubmission heterogeneity poses a fundamental challenge. Annotations come from diverse submitters, including diagnostic laboratories, research groups, expert panels, and database exports. Submitters apply varying evidentiary standards; some provide detailed supporting evidence while others offer only assertions. Conflicting interpretations are common, particularly for variants of uncertain significance.\nClassifications evolve as evidence accumulates. A variant classified as VUS in 2018 may be reclassified as likely pathogenic by 2023 based on new functional studies or additional patient observations. ClinVar releases monthly snapshots rather than maintaining formal version control, so models trained on older releases may learn outdated classifications that have since been revised. Specifying the exact ClinVar release date is essential for reproducibility.\nAncestry and gene coverage biases create uneven representation. Variants in well-studied populations (particularly European ancestry) and well-characterized disease genes are heavily overrepresented. Variants from underrepresented populations are more likely to remain classified as VUS due to insufficient evidence. This creates feedback loops: predictive models perform better on European-ancestry variants because training data is richer, reinforcing the disparity (Landrum et al. 2018).\nClinical assertions in ClinVar become training labels for variant effect predictors like CADD (Section 4.3) and evaluation benchmarks for foundation model approaches (Chapter 14). The role of ClinVar in ACMG-AMP variant classification workflows is detailed in Section 26.2. Calibration of computational scores to ClinVar pathogenicity assertions is examined in Section 14.5.3, while systematic evaluation of ClinVar as a benchmark resource appears in Section 20.3.1.\nCircularity with computational predictors represents a subtle but important concern. Clinical submissions increasingly incorporate computational scores like CADD, REVEL, and AlphaMissense as supporting evidence for pathogenicity classification. When these same ClinVar labels are then used to train or evaluate computational predictors, circularity emerges (Schubach et al. 2024). If a laboratory used a high CADD score as supporting evidence for classifying a variant as likely pathogenic, and that variant later appears as a positive label in ClinVar, models trained on ClinVar may partly learn to reproduce CADD itself rather than discovering independent signal. This circularity operates at two levels: evaluation circularity (when models are assessed on benchmarks influenced by the model’s own predictions) and training circularity (when features used in training derive from the same underlying information as the labels). Both forms inflate apparent performance without demonstrating genuine predictive power.\nVariants of uncertain significance constitute the majority of rare variant classifications, reflecting genuinely limited evidence. These variants are both targets for predictive modeling (can computational methods resolve uncertainty?) and potential pitfalls (models trained only on confidently classified variants may not generalize to VUS with different characteristics).\nDespite these limitations, ClinVar remains invaluable. The key is using it appropriately: recognizing biases when training models, accounting for version differences when comparing studies, stratifying performance by ancestry and gene coverage, and treating computational predictions as one line of evidence rather than definitive classifications.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 2.4: [High] Three-panel figure. Panel A: Pie or bar chart showing distribution of ClinVar classifications (Pathogenic, Likely Pathogenic, VUS, Likely Benign, Benign, Conflicting). Highlight that VUS dominates. Panel B: Heatmap showing classification density by gene, with well-studied genes (BRCA1, BRCA2, CFTR) having many submissions vs sparse coverage elsewhere. Panel C: Timeline showing how classifications evolve (example of a variant reclassified from VUS to Pathogenic over time, illustrating version sensitivity).\n\n\n\n\n\n2.8.2 Complementary Clinical Databases\nClinVar’s open-access model and broad submission base make it the most widely used resource, but it is not the only source of clinical variant interpretations. The Human Gene Mutation Database (HGMD) maintains a curated collection of disease-causing mutations compiled from the published literature, with particular depth in rare Mendelian disorders (Stenson et al. 2017). HGMD’s professional version includes variants not yet publicly released, and its curation emphasizes literature-reported pathogenic variants rather than the full spectrum of classifications in ClinVar. The Leiden Open Variation Database (LOVD) takes a gene-centric approach, with individual databases maintained by gene experts who curate variants according to locus-specific knowledge (Fokkema et al. 2011). LOVD instances often capture variants and functional evidence specific to particular disease communities that may not appear in broader databases.\nThese resources complement ClinVar in important ways: HGMD provides literature-derived pathogenic variants that may precede ClinVar submissions, while LOVD captures expert knowledge from disease-specific research communities. For model development and benchmarking, awareness of these alternative sources matters because training exclusively on ClinVar may miss variants documented elsewhere, and apparent novel predictions may simply reflect incomplete training data rather than genuine generalization.\n\n\n2.8.3 ClinGen and Expert Curation\nClinical laboratories submitting to ClinVar vary enormously in expertise and evidentiary standards. A submission from a general diagnostic laboratory applying ACMG guidelines to an unfamiliar gene may differ substantially from an assessment by researchers who have studied that gene for decades. The Clinical Genome Resource (ClinGen) addresses this heterogeneity by providing expert-curated assessments at multiple levels (Rehm et al. 2015).\nClinGen expert panels evaluate gene-disease validity (whether variation in a gene can cause a specific disease) and dosage sensitivity (whether haploinsufficiency or triplosensitivity leads to clinical phenotypes). These evaluations build on the catalog of Mendelian phenotypes maintained by OMIM, which provides curated gene-disease associations and clinical synopses (Amberger et al. 2015).\nClinGen also develops calibrated thresholds for computational predictors, specifying score intervals that justify different strengths of evidence (supporting, moderate, strong) for pathogenicity or benignity (Pejaver et al. 2022). The FDA has recognized these curations as valid scientific evidence for clinical validity. These calibrations directly inform how computational scores should be incorporated into variant classification workflows and are discussed further in Section 14.5.3 for score calibration to ACMG evidence levels and Section 14.4 for integration of multiple computational predictors.\n\n\n2.8.4 Pharmacogenomics Resources\nMost variant interpretation focuses on rare mutations that cause or predispose to disease. Pharmacogenomics presents a different paradigm: common polymorphisms that individually may have no disease consequences but profoundly influence how individuals respond to medications. These variants matter not because they cause disease but because they determine whether a drug will work, fail, or cause harm.\nImplementing pharmacogenomics in clinical practice requires three capabilities: curating variant-drug associations from published literature, translating that evidence into actionable dosing guidelines, and automating the path from a patient’s VCF file to a clinical report. PharmGKB addresses the first need, cataloging over 800 genes, 700 drugs, and thousands of variant-drug-phenotype relationships with evidence levels (Whirl-Carrillo et al. 2012). CPIC translates this knowledge into standardized guidelines specifying how to adjust drug selection or dosing based on metabolizer phenotype (Relling et al. 2019). PharmCAT automates annotation, taking VCF files as input and producing CPIC-compliant reports (Sangkuhl et al. 2019). ClinPGx integrates all three into a unified framework spanning variant detection through clinical recommendation (Gong et al. 2025).\n\n\n\n\n\n\nNoteStar-Allele Nomenclature\n\n\n\nPharmacogenes use a specialized nomenclature where haplotypes (combinations of variants on the same chromosome) are designated by star alleles. The reference haplotype is *1, with variant haplotypes numbered sequentially (*2, *3, etc.) as they were discovered. Each star allele represents a specific combination of SNVs, indels, or structural variants that travel together.\nFor CYP2D6, over 150 star alleles have been defined. Some reduce enzyme function (*4, *5), others increase it through gene duplication (*1xN), and many have unknown functional consequences. A patient’s diplotype (the combination of maternal and paternal star alleles) determines their metabolizer phenotype: poor, intermediate, normal, or ultrarapid.\nStar-allele calling requires phasing to determine which variants co-occur on the same chromosome, plus structural variant detection to identify gene deletions and duplications. Standard SNV-focused pipelines miss critical information, which is why specialized tools like PharmCAT exist.\n\n\nThe CYP2D6 gene exemplifies the complexity. This cytochrome P450 enzyme metabolizes approximately 25% of clinically used drugs, including codeine, tamoxifen, and many antidepressants (Nofziger et al. 2019). Patients with loss-of-function CYP2D6 variants cannot activate codeine to morphine, rendering the drug ineffective for pain relief; patients with gene duplications may convert codeine too efficiently, experiencing dangerous opioid toxicity from standard doses. The difference between these scenarios depends entirely on accurate star-allele diplotyping.\nFrom a modeling perspective, pharmacogenomic resources offer a complementary type of label linking variants to molecular and clinical outcomes through different mechanisms than Mendelian disease pathogenicity. Where ClinVar labels indicate whether a variant causes disease, PharmGKB labels indicate how a variant affects drug response in individuals who may be otherwise healthy.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-constraints",
    "href": "part_1/p1-ch02-data.html#sec-ch02-constraints",
    "title": "2  Data Landscape",
    "section": "2.9 Inherited Constraints",
    "text": "2.9 Inherited Constraints\nEvery genomic model inherits both the power and the biases of its training data. A variant effect predictor trained on ClinVar labels absorbs the ascertainment patterns of clinical sequencing: European ancestry overrepresented, rare diseases enriched, incidental findings undersampled. A chromatin model trained on ENCODE immortalized cell lines learns regulatory patterns that may not generalize to primary tissues with different epigenetic landscapes. Models that estimate genetic constraint quantify how strongly purifying selection acts against damaging variants in each gene, comparing observed variant counts to expectations. But when trained on human population databases, these models systematically miss the most severe cases: gene-lethal variants never appear because carriers do not survive to be sequenced.\nThese biases compound as data flows through analysis pipelines. GWAS summary statistics carry ancestry composition forward into polygenic scores. Conservation scores calculated from biased multiple sequence alignments propagate into variant effect predictions. Foundation model pretraining on reference genomes from limited populations shapes the representations available for all downstream applications. Each transformation amplifies some biases while masking others, making the provenance of model behavior increasingly difficult to trace.\nThe critical question is not whether models trained on these data contain biases; they do. The question is whether those biases can be characterized, bounded, and ultimately corrected. These foundational datasets appear throughout genomic AI as training labels, evaluation benchmarks, and sometimes both simultaneously. Recognizing when the same data sources serve multiple roles is essential for interpreting model performance honestly and anticipating where generalization will fail. Part VI examines these challenges in depth: data partitioning strategies that account for shared ancestry and homology (Section 21.2), population structure effects that confound genetic associations (Section 22.2.1), and ascertainment patterns that create circularity in clinical labels (Section 22.2.4).\n\n\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F. Scott, and Ada Hamosh. 2015. “OMIM.org: Online Mendelian Inheritance in Man (OMIM®), an Online Catalog of Human Genes and Genetic Disorders.” Nucleic Acids Research 43 (D1): D789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAuton, Adam, Gonçalo R. Abecasis, David M. Altshuler, Richard M. Durbin, Gonçalo R. Abecasis, David R. Bentley, Aravinda Chakravarti, et al. 2015. “A Global Reference for Human Genetic Variation.” Nature 526 (7571): 68–74. https://doi.org/10.1038/nature15393.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025. “[TraitGym] Benchmarking DNA Sequence Models for Causal Regulatory Variant Prediction in Human Genetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nEdgar, Ron, Michael Domrachev, and Alex E. Lash. 2002. “Gene Expression Omnibus: NCBI Gene Expression and Hybridization Array Data Repository.” Nucleic Acids Research 30 (1): 207–10. https://doi.org/10.1093/nar/30.1.207.\n\n\nFokkema, Ivo F. A. C., Peter E. M. Taschner, Gerard C. P. Schaafsma, J. Celli, Jeroen F. J. Laros, and Johan T. den Dunnen. 2011. “LOVD v.2.0: The Next Generation in Gene Variant Databases.” Human Mutation 32 (5): 557–63. https://doi.org/10.1002/humu.21438.\n\n\nFrankish, Adam, Mark Diekhans, Anne-Maud Ferreira, Rory Johnson, Irwin Jungreis, Jane Loveland, Jonathan M Mudge, et al. 2019. “GENCODE Reference Annotation for the Human and Mouse Genomes.” Nucleic Acids Research 47 (D1): D766–73. https://doi.org/10.1093/nar/gky955.\n\n\nGamazon, Eric R., Heather E. Wheeler, Kaanan P. Shah, Sahar V. Mozaffari, Keston Aquino-Michaels, Robert J. Carroll, Anne E. Eyler, et al. 2015. “A Gene-Based Association Method for Mapping Traits Using Reference Transcriptome Data.” Nature Genetics 47 (9): 1091–98. https://doi.org/10.1038/ng.3367.\n\n\nGong, Li, Clarissa J Klein, Kelly E Caudle, Ann M Moyer, Stuart A Scott, Michelle Whirl-Carrillo, Teri E Klein, ClinGen Pharmacogenomics Working Group (PGxWG), and on behalf of the. 2025. “Integrating Pharmacogenomics into the Broader Construct of Genomic Medicine: Efforts by the ClinGen Pharmacogenomics Working Group (PGxWG).” Clinical Chemistry 71 (1): 36–44. https://doi.org/10.1093/clinchem/hvae181.\n\n\nGudbjartsson, Daniel F., Patrick Sulem, Hannes Helgason, Arnaldur Gylfason, Sigurjon A. Gudjonsson, Florian Zink, Asmundur Oddson, et al. 2015. “Sequence Variants from Whole Genome Sequencing a Large Group of Icelanders.” Scientific Data 2 (1): 150011. https://doi.org/10.1038/sdata.2015.11.\n\n\nGusev, Alexander, Arthur Ko, Huwenbo Shi, Gaurav Bhatia, Wonil Chung, Brenda W. J. H. Penninx, Rick Jansen, et al. 2016. “Integrative Approaches for Large-Scale Transcriptome-Wide Association Studies.” Nature Genetics 48 (3): 245–52. https://doi.org/10.1038/ng.3506.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data Navigation on the ENCODE Portal.” Nature Communications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. “Integrative Analysis of 111 Reference Human Epigenomes.” Nature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nKurki, Mitja I., Juha Karjalainen, Priit Palta, Timo P. Sipilä, Kati Kristiansson, Kati M. Donner, Mary P. Reeve, et al. 2023. “FinnGen Provides Genetic Insights from a Well-Phenotyped Isolated Population.” Nature 613 (7944): 508–18. https://doi.org/10.1038/s41586-022-05473-8.\n\n\nLambert, Samuel A., Laurent Gil, Simon Jupp, Scott C. Ritchie, Yu Xu, Annalisa Buniello, Aoife McMahon, et al. 2021. “The Polygenic Score Catalog as an Open Database for Reproducibility and Systematic Evaluation.” Nature Genetics 53 (4): 420–25. https://doi.org/10.1038/s41588-021-00783-5.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness, Glenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome Reference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nMarees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan, Emmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018. “[GWAS] A Tutorial on Conducting Genome-Wide Association Studies: Quality Control and Statistical Analysis.” International Journal of Methods in Psychiatric Research 27 (2): e1608. https://doi.org/10.1002/mpr.1608.\n\n\nMorales, Joannella, Shashikant Pujar, Jane E. Loveland, Alex Astashyn, Ruth Bennett, Andrew Berry, Eric Cox, et al. 2022. “A Joint NCBI and EMBL-EBI Transcript Set for Clinical Genomics and Research.” Nature 604 (7905): 310–15. https://doi.org/10.1038/s41586-022-04558-8.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy Schwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021. “An Open Approach to Systematically Prioritize Causal Variants and Genes at All Published Human GWAS Trait-Associated Loci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nNofziger, Charity, Amy J. Turner, Katrin Sangkuhl, Michelle Whirl-Carrillo, José A. G. Agúndez, John L. Black, Henry M. Dunnenberger, et al. 2019. “PharmVar GeneFocus: CYP2D6.” Clinical Pharmacology & Therapeutics 107 (1): 154–70. https://doi.org/10.1002/cpt.1643.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. “ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design.” Advances in Neural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nnull, null. 2019. “The ‘All of Us’ Research Program.” New England Journal of Medicine 381 (7): 668–76. https://doi.org/10.1056/NEJMsr1809937.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V. Bzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The Complete Sequence of a Human Genome.” Science 376 (6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Leary, Nuala A., Mathew W. Wright, J. Rodney Brister, Stacy Ciufo, Diana Haddad, Rich McVeigh, Bhanu Rajput, et al. 2016. “Reference Sequence (RefSeq) Database at NCBI: Current Status, Taxonomic Expansion, and Functional Annotation.” Nucleic Acids Research 44 (D1): D733–45. https://doi.org/10.1093/nar/gkv1189.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel, Sean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022. “Calibration of Computational Tools for Missense Variant Pathogenicity Classification and ClinGen Recommendations for PP3/BP4 Criteria.” American Journal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nRegev, Aviv, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe Benoist, Ewan Birney, Bernd Bodenmiller, et al. 2017. “The Human Cell Atlas.” Edited by Thomas R Gingeras. eLife 6 (December): e27041. https://doi.org/10.7554/eLife.27041.\n\n\nRehm, Heidi L., Jonathan S. Berg, Lisa D. Brooks, Carlos D. Bustamante, James P. Evans, Melissa J. Landrum, David H. Ledbetter, et al. 2015. “ClinGen — The Clinical Genome Resource.” New England Journal of Medicine 372 (23): 2235–42. https://doi.org/10.1056/NEJMsr1406261.\n\n\nRelling, Mary V., Teri E. Klein, Roseann S. Gammal, Michelle Whirl-Carrillo, James M. Hoffman, and Kelly E. Caudle. 2019. “The Clinical Pharmacogenetics Implementation Consortium: 10 Years Later.” Clinical Pharmacology & Therapeutics 107 (1): 171–75. https://doi.org/10.1002/cpt.1651.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRuan, Yunfeng, Yen-Feng Lin, Yen-Chen Anne Feng, Chia-Yen Chen, Max Lam, Zhenglin Guo, Lin He, et al. 2022. “Improving Polygenic Prediction in Ancestrally Diverse Populations.” Nature Genetics 54 (5): 573–80. https://doi.org/10.1038/s41588-022-01054-7.\n\n\nSangkuhl, Katrin, Michelle Whirl-Carrillo, Ryan M. Whaley, Mark Woon, Adam Lavertu, Russ B. Altman, Lester Carter, Anurag Verma, Marylyn D. Ritchie, and Teri E. Klein. 2019. “Pharmacogenomics Clinical Annotation Tool (PharmCAT).” Clinical Pharmacology & Therapeutics 107 (1): 203–10. https://doi.org/10.1002/cpt.1568.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nSherry, S. T., M.-H. Ward, M. Kholodov, J. Baker, L. Phan, E. M. Smigielski, and K. Sirotkin. 2001. “dbSNP: The NCBI Database of Genetic Variation.” Nucleic Acids Research 29 (1): 308–11. https://doi.org/10.1093/nar/29.1.308.\n\n\nSirugo, Giorgio, Scott M. Williams, and Sarah A. Tishkoff. 2019. “The Missing Diversity in Human Genetic Studies.” Cell 177 (1): 26–31. https://doi.org/10.1016/j.cell.2019.02.048.\n\n\nSollis, Elliot, Abayomi Mosaku, Ala Abid, Annalisa Buniello, Maria Cerezo, Laurent Gil, Tudor Groza, et al. 2023. “The NHGRI-EBI GWAS Catalog: Knowledgebase and Deposition Resource.” Nucleic Acids Research 51 (D1): D977–85. https://doi.org/10.1093/nar/gkac1010.\n\n\nStenson, Peter D., Matthew Mort, Edward V. Ball, Katy Evans, Matthew Hayden, Sally Heywood, Michelle Hussain, Andrew D. Phillips, and David N. Cooper. 2017. “The Human Gene Mutation Database: Towards a Comprehensive Repository of Inherited Mutation Data for Medical Research, Genetic Diagnosis and Next-Generation Sequencing Studies.” Human Genetics 136 (6): 665–77. https://doi.org/10.1007/s00439-017-1779-6.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N. Phan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023. “Leveraging Base-Pair Mammalian Constraint to Understand Genetic Variation and Human Disease.” Science 380 (6643): eabn2937. https://doi.org/10.1126/science.abn2937.\n\n\nTHE GTEX CONSORTIUM. 2020. “The GTEx Consortium Atlas of Genetic Regulatory Effects Across Human Tissues.” Science 369 (6509): 1318–30. https://doi.org/10.1126/science.aaz1776.\n\n\nTHE TABULA SAPIENS CONSORTIUM. 2022. “The Tabula Sapiens: A Multiple-Organ, Single-Cell Transcriptomic Atlas of Humans.” Science 376 (6594): eabl4896. https://doi.org/10.1126/science.abl4896.\n\n\nVõsa, Urmo, Annique Claringbould, Harm-Jan Westra, Marc Jan Bonder, Patrick Deelen, Biao Zeng, Holger Kirsten, et al. 2021. “Large-Scale Cis- and Trans-eQTL Analyses Identify Thousands of Genetic Loci and Polygenic Scores That Regulate Blood Gene Expression.” Nature Genetics 53 (9): 1300–1310. https://doi.org/10.1038/s41588-021-00913-z.\n\n\nWhirl-Carrillo, M, E M McDonagh, J M Hebert, L Gong, K Sangkuhl, C F Thorn, R B Altman, and T E Klein. 2012. “Pharmacogenomics Knowledge for Personalized Medicine.” Clinical Pharmacology & Therapeutics 92 (4): 414–17. https://doi.org/10.1038/clpt.2012.96.\n\n\nXu, Leqi, Wangjie Zheng, Jiaqi Hu, Yingxin Lin, Jia Zhao, Gefei Wang, Tianyu Liu, and Hongyu Zhao. 2025. “Improving Polygenic Risk Prediction Performance by Integrating Electronic Health Records Through Phenotype Embedding.” The American Journal of Human Genetics 112 (12): 3030–45. https://doi.org/10.1016/j.ajhg.2025.11.006.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll, and Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls Using DeepVariant and GLnexus.” Bioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei Sun, Chen-Hao Chen, et al. 2019. “Cistrome Data Browser: Expanded Datasets and New Tools for Gene Regulatory Analysis.” Nucleic Acids Research 47 (D1): D729–35. https://doi.org/10.1093/nar/gky1094.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html",
    "href": "part_1/p1-ch03-gwas.html",
    "title": "3  GWAS and Polygenic Scores",
    "section": "",
    "text": "3.1 GWAS Framework\nGenome-wide association studies (GWAS) do not identify causal variants; they identify signposts. When a GWAS reports that a particular single nucleotide polymorphism (SNP) associates with coronary artery disease, that SNP is almost certainly not the variant that alters cardiac biology. It is correlated with the causal variant through linkage disequilibrium, the non-random association of nearby alleles that persists across generations. The statistical machinery of GWAS is exquisitely sensitive to these correlations but fundamentally agnostic about mechanism. It can identify a region of the genome that harbors trait-relevant variation without distinguishing the causal variant from its correlated neighbors, without explaining which genes or pathways are affected, and without revealing whether the same associations hold in populations with different linkage patterns.\nThis distinction between association and causation defines the central intellectual challenge of statistical genetics. GWAS have identified thousands of genomic regions associated with hundreds of complex traits, from height and blood pressure to schizophrenia and type 2 diabetes. These associations replicate across studies with remarkable consistency, confirming that the signals are real. Yet the path from associated region to biological mechanism remains obscure for most loci. The majority of GWAS signals fall in non-coding regions where there is no obvious gene to implicate. Even when a signal overlaps a gene, whether it affects expression, splicing, or protein function is rarely apparent from the association alone.\nPolygenic scores aggregate these associations into predictions, summing risk alleles across thousands of loci to estimate an individual’s genetic predisposition. For some traits, these scores achieve clinically meaningful discrimination: individuals in the top percentile of coronary artery disease risk have odds ratios comparable to monogenic familial hypercholesterolemia. Yet polygenic scores inherit all the limitations of the associations they aggregate. They predict without explaining, correlate without identifying mechanism, and transfer poorly across populations with different allele frequencies and linkage patterns. Understanding both their power and their limitations is essential for mechanistic approaches where regulatory sequence models (Chapter 13) and variant effect predictors (Chapter 14) attempt to move from statistical association to biological explanation (Khera and Kathiresan 2017). The portability challenges of polygenic scores across populations motivate the fairness and equity considerations examined in Section 22.2.1 and Section 25.8.\nConsider a clinician counseling a patient about cardiovascular disease risk. Traditional risk factors (age, smoking, cholesterol, blood pressure) explain roughly 50% of the variation in who develops disease (Khera and Kathiresan 2017). Family history suggests that genetics contributes substantially to the remainder, but which genetic variants matter, and how much does each contribute? GWAS provide a systematic approach to answering these questions by testing each of millions of variants for association with the trait of interest.\nThe scale required for well-powered GWAS explains why large-scale biobanks (Section 2.3 for comprehensive biobank descriptions, Section 2.9 for data composition biases) have become essential infrastructure for statistical genetics. UK Biobank, with its 500,000 participants genotyped across hundreds of thousands of variants and linked to extensive phenotypic data, has enabled GWAS for thousands of traits at sample sizes that were unimaginable a decade ago. Similar resources, including the Million Veteran Program, FinnGen, and All of Us, continue to expand the scope of discoverable associations. The biobank paradigm of combining dense genotyping with rich phenotyping at population scale has transformed GWAS from underpowered fishing expeditions into reliable discovery engines.\nThe core logic is straightforward. For each variant in turn, researchers ask whether individuals carrying more copies of a particular allele tend to have higher or lower values of the phenotype (for quantitative traits) or higher or lower probability of disease (for binary outcomes). They estimate an effect size, compute a test statistic under the null hypothesis of no association, and record a p-value. After testing millions of variants, those exceeding a stringent significance threshold are identified, the associated loci reported, and interpretation begins regarding which genes and pathways might be involved.\nThis apparently simple procedure requires careful attention to study design, quality control, and statistical modeling. The phenotype must be measured consistently across individuals. The genotypes must be accurate and the variants well-defined. Confounders that correlate with both genotype and phenotype (most notably population structure) must be controlled. Multiple testing across millions of variants demands stringent significance thresholds. Only after addressing these challenges can GWAS results be trusted and translated into downstream applications.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-gwas-framework",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-gwas-framework",
    "title": "3  GWAS and Polygenic Scores",
    "section": "",
    "text": "3.1.1 Association Models for Quantitative Traits\nChoosing the wrong statistical model for a GWAS does not merely introduce imprecision; it distorts effect size estimates in ways that propagate through every downstream analysis, from fine-mapping to polygenic scores to drug target prioritization. A height GWAS and a schizophrenia GWAS require fundamentally different approaches because one outcome is continuous and the other binary. Applying linear regression to a binary outcome produces fitted values outside the 0-1 probability range and residuals that violate normality assumptions.\nFor continuous phenotypes such as height, LDL cholesterol, or blood pressure, the standard approach is linear regression. Let \\(y_i\\) denote the phenotype for individual \\(i\\), and let \\(g_{ij}\\) denote the genotype dosage at variant \\(j\\), encoded as \\(0\\), \\(1\\), or \\(2\\) copies of the alternative allele (or as a fractional value for imputed genotypes). The model is:\n\\[\ny_i = \\alpha + \\beta_j g_{ij} + \\gamma^\\top c_i + \\varepsilon_i\n\\]\nThe coefficient \\(\\beta_j\\) represents the expected change in phenotype per additional copy of the alternative allele, holding covariates \\(c_i\\) fixed. When phenotypes are standardized to zero mean and unit variance, \\(\\beta_j\\) is expressed in standard deviation units per allele. The vector \\(c_i\\) typically includes age, sex, genotyping batch, and principal components capturing ancestry (discussed below). The residual \\(\\varepsilon_i\\) captures unexplained variation, assumed to be independent and identically distributed across individuals.\nFor each variant, a test statistic is computed for the null hypothesis \\(H_0: \\beta_j = 0\\). In large samples, the t-statistic follows approximately a standard normal distribution under the null, yielding a two-sided p-value. With \\(M\\) variants tested (typically \\(10^6\\) to \\(10^7\\) after imputation), multiple comparison correction is essential. The conventional genome-wide significance threshold of \\(5 \\times 10^{-8}\\) approximates a Bonferroni correction for roughly one million effectively independent tests, accounting for correlation among variants due to linkage disequilibrium (Risch and Merikangas 1996; Pe’er et al. 2008).\n\n\n3.1.2 Association Models for Disease Outcomes\nBinary outcomes create a specific statistical problem that, if ignored, systematically distorts effect size estimates in ways that compound through downstream applications. When the phenotype is disease status (affected or unaffected), linear regression produces nonsensical predictions: fitted values outside the 0-1 probability range and residuals that violate normality assumptions. The consequence extends beyond statistical inelegance. Effect sizes estimated under the wrong model propagate into polygenic scores and risk prediction, potentially misclassifying patients who sit near clinical decision thresholds where intervention recommendations change.\nFor binary phenotypes, logistic regression replaces linear regression. The model relates genotype to the log-odds of disease:\n\\[\n\\log \\frac{P(y_i = 1)}{P(y_i = 0)} = \\alpha + \\beta_j g_{ij} + \\gamma^\\top c_i\n\\]\nHere \\(\\beta_j\\) is the log-odds ratio per allele, and \\(\\exp(\\beta_j)\\) gives the odds ratio (OR). An odds ratio of \\(1.2\\) means that each additional copy of the alternative allele increases the odds of disease by \\(20\\%\\). For rare diseases (prevalence below approximately \\(10\\%\\)), odds ratios approximate relative risks, but the distinction matters for common conditions and when communicating absolute risk to patients.\nCase-control sampling, in which cases are enriched relative to their population frequency, distorts absolute risk estimates but preserves the validity of odds ratio estimation. This mathematical property explains why GWAS conducted in case-control designs can still produce effect sizes useful for polygenic scores, provided downstream applications account for baseline disease incidence. The likelihood function conditions on disease status, making the odds ratio identifiable regardless of sampling scheme.\n\n\n3.1.3 Manhattan Plots and Q-Q Plots\nThe Manhattan plot has become the iconic visualization of GWAS results, named for its resemblance to the New York City skyline. Each point represents a tested variant, with genomic position along the x-axis (ordered by chromosome) and negative log-transformed p-value on the y-axis. Variants with stronger associations rise higher; those exceeding the genome-wide significance threshold of \\(5 \\times 10^{-8}\\) (typically drawn as a horizontal line at \\(-\\log_{10}(5 \\times 10^{-8}) \\approx 7.3\\)) are considered significant hits.\nThe Manhattan plot reveals both the successes and limitations of GWAS at a glance. Prominent peaks indicate genomic regions harboring trait-associated variants, but each peak typically contains dozens or hundreds of correlated variants rather than a single causal nucleotide. The width of peaks reflects local linkage disequilibrium structure: broader peaks indicate regions where many variants are correlated with the lead signal. The height reflects statistical strength, which depends on effect size, allele frequency, and sample size. Tall, narrow peaks suggest strong, well-localized signals; broad peaks spanning megabases indicate that fine-mapping will be challenging.\nQuantile-quantile (Q-Q) plots complement Manhattan plots by assessing whether the observed p-value distribution matches theoretical expectations under the null hypothesis. Systematic deviation from the diagonal (genomic inflation) suggests either true polygenic signal or residual confounding from population structure. The genomic inflation factor λ quantifies this deviation, with values substantially above 1.0 warranting investigation of potential confounders.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 3.1: [Essential] Annotated Manhattan plot from a real or realistic GWAS (e.g., height or CAD). Key annotations: (1) Genome-wide significance threshold at \\(-\\log_{10}(5 \\times 10^{-8}) \\approx 7.3\\); (2) Example peak with lead SNP labeled; (3) Width of peak showing LD extent (many correlated variants, not just one); (4) Chromosomes color-alternated along x-axis; (5) Inset zoom on one peak showing how multiple variants exceed threshold, illustrating that GWAS identifies loci, not causal variants. Include Q-Q plot inset showing expected vs observed p-value distribution with genomic inflation factor \\(\\lambda\\) annotated.\n\n\n\n\n\n3.1.4 Population Structure Control\nPopulation structure poses a fundamental challenge to GWAS interpretation because it can generate association signals indistinguishable from true biological effects. If allele frequencies differ systematically across subpopulations and the phenotype also varies across these groups for non-genetic reasons (differences in environment, diet, healthcare access, socioeconomic status), naive association testing will detect variants that mark ancestry rather than causal biology. A variant that is simply more common in one population will appear associated with any trait that differs between populations, regardless of biological mechanism. The resulting false positives waste resources on follow-up studies and, more insidiously, can embed ancestry-related confounding into polygenic scores that are then deployed as if they measured pure genetic risk.\nPrincipal component analysis (PCA) on the genotype matrix captures the major axes of genetic variation across individuals (Price et al. 2006; Patterson, Price, and Reich 2006). The leading principal components often correspond to continental ancestry gradients or finer-scale population structure within a study. Including these PCs as covariates in the regression model attenuates spurious associations driven by ancestry stratification.\nThis correction is imperfect. Subtle structure not captured by the included PCs, cryptic relatedness among individuals, and the interweaving of genetic ancestry with environmental exposures all complicate interpretation. The challenges extend far beyond technical statistical adjustment: ancestry is entangled with healthcare access, environmental exposures, and socioeconomic factors in ways that simple covariate correction cannot fully resolve. These issues become critical when translating GWAS results to clinical applications and when evaluating whether polygenic scores perform equitably across populations. The full complexity of ancestry as a confounder is addressed in Chapter 22.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-heritability",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-heritability",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.2 Heritability: What Genetics Can Explain",
    "text": "3.2 Heritability: What Genetics Can Explain\nBefore GWAS can identify specific variants, a more fundamental question must be answered: how much of the variation in a trait is attributable to genetics at all? A trait entirely determined by environment would yield no GWAS hits regardless of sample size. A trait entirely determined by genetics would, in principle, be fully predictable from genotype. Heritability quantifies where traits fall along this spectrum, but the concept is more subtle than it first appears, and different estimation methods yield systematically different answers.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 3.2: [High] Conceptual diagram showing nested components of phenotypic variance. Outer ring: Total phenotypic variance (\\(100\\%\\)). First partition: Genetic vs Environmental components (e.g., \\(80/20\\) for height). Within genetic: Additive (narrow-sense \\(h^2\\)) vs non-additive (dominance, epistasis). Within additive: SNP-heritability (what GWAS can capture) vs “missing heritability” (rare variants, structural variants, imperfect tagging). Annotate with approximate values for a well-studied trait like height.\n\n\n\n\n3.2.1 Pedigree Heritability\nClassical genetics estimated heritability by comparing phenotypic similarity among relatives. Identical twins share all their genetic variation; fraternal twins share on average half; full siblings also share half; parents and offspring share half; cousins share one-eighth. If genetic variation influences a trait, closer relatives should be more similar. The correlation structure across relationship types allows partitioning of phenotypic variance into genetic and environmental components.\nNarrow-sense heritability (\\(h^2\\)) represents the proportion of phenotypic variance attributable to additive genetic effects. For height, pedigree studies consistently estimate \\(h^2\\) around \\(0.80\\), meaning that \\(80\\%\\) of the variation in height across individuals in the studied population can be attributed to genetic differences (Visscher, Hill, and Wray 2008). For schizophrenia, twin studies estimate \\(h^2\\) around \\(0.80\\) as well (Hilker et al. 2018). For body mass index, estimates cluster around \\(0.50\\) to \\(0.80\\) depending on the population, age group, and study design (Elks et al. 2012).\nThese high heritability estimates established that genetics substantially influences most traits of biomedical interest, motivating the search for specific causal variants. If 80% of height variation is genetic, then genetic variants collectively must explain most of that variation. Finding those variants became the goal of GWAS.\n\n\n3.2.2 SNP-Heritability and the Missing Heritability Problem\nGWAS delivered a puzzle. For height, even the largest studies with hundreds of significant hits explained only a fraction of the heritability estimated from family studies. Early GWAS collectively explained perhaps 5% of height variance when pedigree studies suggested 80% should be genetic. This gap, termed missing heritability, sparked intense debate about where the remaining genetic variance might hide (Manolio et al. 2009).\nThe concept of SNP-heritability (\\(h^2_{\\mathrm{SNP}}\\)) emerged to parse this puzzle into more tractable components. Rather than asking how much variance is explained by genome-wide significant variants, researchers asked how much variance is explained by all common SNPs on genotyping arrays, including those that fail to reach significance. Methods such as GCTA-GREML estimate this quantity by modeling phenotypic similarity as a function of genetic similarity computed across all SNPs (Yang et al. 2010). For height, SNP-heritability estimates reach approximately \\(0.50\\) to \\(0.60\\), substantially higher than variance explained by significant hits alone but still below pedigree estimates.\nThis intermediate value revealed that missing heritability actually comprises two distinct gaps. The first gap separates SNP-heritability from variance explained by GWAS-significant variants. This polygenic gap reflects the architecture of complex traits: thousands of variants each contribute effects too small to reach genome-wide significance individually, yet they collectively explain substantial variance when modeled together. As sample sizes grow and more variants cross the significance threshold, this gap narrows. The polygenic gap is not truly “missing” heritability; the variance is captured by common SNPs, just distributed across too many variants to detect individually.\nThe second gap separates pedigree heritability from SNP-heritability. This hidden heritability reflects genetic variation genuinely absent from common SNP arrays: rare variants below minor allele frequency thresholds, structural variants poorly tagged by single nucleotide polymorphisms, copy number variations, and variants not in linkage disequilibrium with array content. Unlike the polygenic gap, this component cannot be recovered by increasing GWAS sample size; it requires different data types entirely, such as whole-genome sequencing that captures rare variation directly.\nThe distinction matters for how foundation models might contribute. Models trained on common variant data inherit the SNP-heritability ceiling; they cannot learn patterns from variation they never observe. Integrating rare variant data, structural variant calls, or multi-omic measurements represents not merely incremental improvement but access to a fundamentally different component of genetic architecture.\n\n\n3.2.3 Implications for GWAS and Polygenic Scores\nThe heritability landscape carries practical implications for what GWAS and polygenic scores can achieve. SNP-heritability sets an upper bound on the predictive accuracy of polygenic scores built from common variants: a PGS cannot explain more variance than is captured by the SNPs it uses. For height, with SNP-heritability around 0.50, the best possible common-variant PGS could explain at most half of phenotypic variance. Current PGS for height in European-ancestry populations approach this bound, explaining roughly 25% of variance with continued gains as sample sizes grow (Yengo et al. 2022).\nFor diseases, the relationship between heritability and predictive accuracy is more complex. A highly heritable disease might have low predictive accuracy if the causal variants are rare, if gene-environment interactions dominate, or if the heritability is distributed across thousands of variants each with tiny effects. Conversely, a moderately heritable disease with a few common variants of large effect might be more predictable. The architecture of genetic effects matters as much as total heritability.\nMissing heritability also motivates the integration of rare variant analysis with GWAS of common variants. Whole-genome sequencing studies can capture rare variants invisible to genotyping arrays, potentially recovering some of the genetic variance missing from common-variant analyses. Foundation models trained on sequence data, rather than genotype arrays, may ultimately capture genetic effects across the full allele frequency spectrum, a possibility explored in Chapter 14.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-ld",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-ld",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.3 Linkage Disequilibrium and the Association-Causation Gap",
    "text": "3.3 Linkage Disequilibrium and the Association-Causation Gap\nGWAS test variants one at a time, but the genome is not inherited one variant at a time. Nearby variants travel together on haplotypes and are co-inherited across generations except when recombination separates them. This correlation structure, known as linkage disequilibrium (LD), is both essential to GWAS power and the source of their fundamental interpretive limitation. Without LD, GWAS would need to genotype every variant in the genome directly; with LD, statistical association cannot distinguish cause from correlation.\nWhen a GWAS identifies a significant association at variant j, three possibilities exist. The variant itself may be causal, directly influencing the phenotype through some molecular mechanism. Alternatively, variant j may simply be correlated with a nearby causal variant \\(k\\) due to LD, with the association signal reflecting this correlation rather than direct causation. In complex regions, multiple causal variants may exist, and the observed association pattern reflects their joint effects filtered through the local LD structure. Distinguishing these scenarios from GWAS summary statistics alone is often impossible. The causal variant and its tag look identical in the association data, yet only the causal variant represents a valid drug target or mechanistic insight.\nFine-mapping methods attempt to resolve this ambiguity by modeling LD structure explicitly. Reference panels from the 1000 Genomes Project (Section 2.2.2) and gnomAD (Section 2.2.3) enable LD calculation across diverse populations, providing the correlation matrices these methods require. Statistical fine-mapping narrows the set of plausible causal variants but often cannot identify a single culprit. Incorporating functional priors from regulatory models and variant effect predictors can further prioritize candidates; this integration of foundation model representations into fine-mapping is examined in Section 14.1.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 3.3: [Essential] Three-panel figure. Panel A: Haplotype diagram showing how LD creates correlation between variants on the same chromosome segment; show example haplotypes with causal variant (star) and tag variants traveling together. Panel B: \\(r^2\\) matrix (triangular heatmap) for a genomic region showing block structure of LD. Panel C: Same causal variant shown in two populations with different LD structure: in one population, tags are highly correlated with causal variant; in another, the correlation is weaker, illustrating why portability fails.\n\n\n\n\n3.3.1 Structure of Linkage Disequilibrium\nUnderstanding why LD creates interpretive ambiguity requires understanding how LD arises and decays. Recombination during meiosis shuffles genetic material between parental chromosomes, with crossover events occurring at an average rate of roughly one per 100 megabases, meaning each chromosome arm typically experiences only one or two exchanges per generation. Over many generations, recombination breaks down long-range correlations between variants while preserving short-range structure. The timescale of this decay matters: LD between variants separated by a few kilobases persists for thousands of generations, while correlations spanning megabases decay within tens of generations. The result is a mosaic pattern: regions of high LD (haplotype blocks) where many variants are strongly correlated, interspersed with recombination hotspots where LD decays rapidly.\nRecombination does not occur uniformly across the genome. Crossover hotspots, typically spanning 1 to 2 kilobases, concentrate the majority of recombination events into a small fraction of genomic sequence. These hotspots are enriched for specific sequence motifs recognized by the zinc finger protein PRDM9, which directs the recombination machinery to particular locations. The consequence is that haplotype blocks can extend for hundreds of kilobases across regions lacking hotspots, while adjacent blocks may be separated by sharp boundaries where recombination has effectively randomized allelic associations.\nThe squared correlation coefficient \\(r^2\\) quantifies LD between pairs of variants. Unlike Pearson correlation, which measures linear relationships between continuous variables, \\(r^2\\) for LD is computed from allele frequencies and haplotype counts in a \\(2\\times 2\\) contingency table. The metric equals \\((p_{AB} - p_A p_B)^2 / (p_A p_a p_B p_b)\\), where \\(p_{AB}\\) is the frequency of the AB haplotype and \\(p_A\\), \\(p_a\\), \\(p_B\\), \\(p_b\\) are the individual allele frequencies. This formulation captures the deviation from random association expected under linkage equilibrium. The notation unfortunately overlaps with the coefficient of determination from linear regression, but the quantities measure different phenomena: regression \\(R^2\\) captures variance explained by a fitted model, while LD \\(r^2\\) captures non-random association between alleles at two loci. When \\(r^2\\) approaches \\(1\\), the two variants are nearly always observed together on the same haplotypes; when \\(r^2\\) approaches \\(0\\), they segregate independently. From a GWAS perspective, if a causal variant \\(k\\) has strong association with the phenotype and variant \\(j\\) is in high LD with \\(k\\) (high \\(r^2\\)), then variant \\(j\\) will also show strong association even if it has no direct causal role. The statistical signal propagates through LD, creating ambiguity about which variant is actually functional.\nLD patterns vary across populations because demographic history shapes which haplotypes persist and at what frequencies. Founder effects occur when a small number of individuals establish a new population, carrying only a subset of the ancestral haplotype diversity. The Finnish population, descended from a small founder group roughly 4,000 years ago, exhibits extended LD blocks and elevated frequencies of otherwise rare disease alleles. Bottlenecks produce similar effects: dramatic population contractions eliminate rare haplotypes and reduce diversity, leaving survivors with correlated genetic backgrounds. In contrast, large stable populations accumulate recombination events over many generations, breaking down LD more completely. A variant that tags a causal allele effectively in one population may be a poor proxy in another where different recombination history has decoupled the correlation. This population-specificity of LD structure is one reason polygenic scores fail to transfer across ancestries, a problem examined in detail in Section 3.7.\n\n\n3.3.2 Causal Variants, Tag Variants, and GWAS Catalogs\nThe distinction between causal and tag variants determines whether GWAS results can translate into biological insight or clinical action. A causal variant directly influences the phenotype, whether by altering protein sequence, disrupting transcription factor binding, affecting splicing, or modifying chromatin state. A tag variant is merely correlated with a causal variant through LD, serving as a statistical proxy without direct functional consequence. The distinction is invisible to GWAS: both produce association signals, and in the presence of strong LD, those signals are statistically indistinguishable.\nGWAS catalogs therefore report associated loci, not causal variants. The “lead SNP” at each locus (the variant with the smallest p-value) is often a tag rather than the causal variant, particularly when the causal variant is rare, poorly genotyped, or not present on the array. Even when a locus is robustly associated, dozens or hundreds of correlated variants may be statistically indistinguishable from the lead SNP.\nThis limitation has concrete practical consequences. Drug development requires identifying causal genes and mechanisms, not just associated regions; targeting a tag variant or the wrong gene wastes years of development effort. Clinical variant interpretation needs to distinguish functional mutations from neutral passengers; reporting a tag as pathogenic misleads patients and clinicians. Polygenic scores built on tag SNPs may lose power when applied to populations with different LD patterns, since the tag-causal correlation that made the tag useful may not hold. The gap between association and causation motivates the fine-mapping approaches considered next.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-fine-mapping",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-fine-mapping",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.4 Fine-Mapping: From Loci to Causal Variants",
    "text": "3.4 Fine-Mapping: From Loci to Causal Variants\nA pharmaceutical company evaluating a GWAS hit for drug development faces a concrete problem: the associated locus spans 500 kilobases, contains 200 correlated variants, and overlaps three genes. Which gene should they target? Which variant drives the association? Investing hundreds of millions of dollars in a program targeting the wrong gene would be catastrophic, yet GWAS summary statistics alone cannot resolve the ambiguity. Fine-mapping attempts to address this gap, moving from “this region is associated” to “these specific variants are most likely causal” by exploiting the joint behavior of correlated variants under explicit statistical models.\n\n3.4.1 Statistical Framework\nThe core insight of fine-mapping is that while multiple variants may show similar marginal association statistics, their joint behavior under a model that accounts for LD can discriminate among them. A causal variant should show association beyond what can be explained by LD with its neighbors; a tag variant should not. This distinction, invisible when variants are tested one at a time, becomes apparent when their correlations are modeled jointly.\nBayesian fine-mapping methods approach the problem by specifying a prior distribution over which variants in a region might be causal, then computing posterior probabilities given the observed association statistics and local LD structure (Maller et al. 2012; Hormozdiari et al. 2014). The key outputs are posterior inclusion probabilities (PIPs), which estimate the probability that each variant is among the causal set, and credible sets, which are minimal sets of variants that contain the true causal variant(s) with specified probability (commonly 95%).\nThe mathematical foundation rests on comparing models that differ in which variants are causal. Consider a region containing \\(m\\) variants, and let \\(\\gamma\\) denote a configuration specifying which variants are causal (a binary vector of length \\(m\\)). Under a linear model, the observed GWAS summary statistics (effect estimates \\(\\hat{\\beta}\\) and their standard errors) follow a multivariate normal distribution whose mean depends on the true causal effects and whose covariance depends on the LD matrix \\(\\Sigma\\). The likelihood of observing the data under configuration \\(\\gamma\\) can be written as:\n\\[P(\\hat{\\beta} \\mid \\gamma, \\Sigma) = \\int P(\\hat{\\beta} \\mid \\beta_\\gamma, \\Sigma) \\, P(\\beta_\\gamma) \\, d\\beta_\\gamma\\]\nwhere \\(\\beta_\\gamma\\) represents the true effect sizes for causal variants in configuration \\(\\gamma\\), and \\(P(\\beta_\\gamma)\\) is the prior on effect sizes (typically Gaussian with variance \\(\\sigma^2\\)) (Benner et al. 2016). This integral has a closed-form solution when both the likelihood and prior are Gaussian, yielding a Bayes factor comparing each configuration to the null model of no associations.\nThe posterior probability of configuration \\(\\gamma\\) follows from Bayes’ theorem:\n\\[P(\\gamma \\mid \\hat{\\beta}, \\Sigma) = \\frac{P(\\hat{\\beta} \\mid \\gamma, \\Sigma) \\, P(\\gamma)}{\\sum_{\\gamma'} P(\\hat{\\beta} \\mid \\gamma', \\Sigma) \\, P(\\gamma')}\\]\nThe prior \\(P(\\gamma)\\) encodes assumptions about the number and distribution of causal variants. Common choices include a fixed maximum number of causal variants \\(K\\) (often 1 to 5) with uniform probability across configurations of equal size, or a binomial prior where each variant has independent probability \\(\\pi\\) of being causal (Wang et al. 2020). The denominator sums over all possible configurations, a computation that becomes intractable for large regions (with \\(m\\) variants and up to \\(K\\) causal variants, the number of configurations scales as \\(C(m, K)\\)).\nThe posterior inclusion probability for variant \\(j\\) marginalizes over all configurations in which that variant appears:\n\\[\\text{PIP}_j = \\sum_{\\gamma : j \\in \\gamma} P(\\gamma \\mid \\hat{\\beta}, \\Sigma)\\]\nThis quantity answers the question: given everything we observed, what is the probability that variant \\(j\\) is among the causal variants? Methods like FINEMAP, CAVIAR, and SuSiE differ primarily in how they approximate the intractable sum over configurations and in their prior specifications, but all produce PIPs as their primary output (Benner et al. 2016; Hormozdiari et al. 2014; Wang et al. 2020).\nCredible sets provide a complementary summary. A 95% credible set is the smallest set of variants whose cumulative PIP exceeds 0.95. When a single variant dominates (PIP above 0.95), the credible set contains only that variant. When LD distributes probability across many variants, credible sets expand accordingly. The SuSiE method produces one credible set per inferred causal signal, enabling regions with multiple independent associations to be decomposed into distinct sets (Wang et al. 2020).\nVariants with high PIPs (above 0.5 or 0.9) are strong candidates for functional follow-up. Credible sets that contain few variants are more actionable than those containing dozens. The width of credible sets reflects both the strength of the association signal and the local LD structure: tight LD means many variants remain plausible even with strong statistical evidence. In some regions, fine-mapping narrows thousands of candidates to a handful; in others, the ambiguity remains irreducible given available data.\n\n\n3.4.2 Functional Annotation Priors\nStatistical fine-mapping alone cannot resolve regions where multiple variants are in near-perfect LD; the data simply cannot distinguish variants that are always co-inherited. Functional annotations offer a path forward by incorporating biological plausibility: not all genomic positions are equally likely to harbor causal variants. Variants disrupting coding sequences, altering transcription factor binding sites, or falling within active enhancers carry higher prior probability of functional relevance than variants in unannotated intergenic regions.\nAnnotation-informed approaches update fine-mapping priors based on these external data sources. Variants in coding regions, promoters, enhancers, or regions of evolutionary constraint may be assigned higher prior probability of causality. Integration with chromatin accessibility data (from ATAC-seq or DNase-seq), transcription factor binding maps (from ChIP-seq), or expression quantitative trait loci (eQTL) can further prioritize variants with plausible regulatory mechanisms.\nThe functional scores introduced in Chapter 2 provide systematic frameworks for quantifying variant-level annotations. Scores such as CADD, DANN, and Eigen integrate diverse genomic features into single numbers that can inform fine-mapping priors (Kircher et al. 2014; Quang, Chen, and Xie 2015; Ionita-Laza et al. 2016). More recently, foundation models trained on genomic sequence have produced variant effect predictions that capture functional information beyond what traditional annotations provide (Chapter 14). These scores transform fine-mapping from a purely statistical exercise into an integrative analysis that combines association evidence with mechanistic plausibility.\nLarge-scale resources now link GWAS summary statistics, fine-mapping results, and functional genomic annotations across hundreds of traits and thousands of loci (Buniello et al. 2025; Mountjoy et al. 2021). These datasets enable systematic identification of variants that are both statistically prioritized and functionally plausible, though the biological validation required to confirm causal mechanisms remains laborious and is completed for only a small fraction of associated loci.\n\n\n3.4.3 Multi-Ancestry Fine-Mapping\nSingle-ancestry fine-mapping encounters a fundamental resolution limit: when variants are in tight LD within the study population, no amount of statistical sophistication can distinguish them. Multi-ancestry approaches break through this limit by exploiting the population-specificity of LD structure. A variant in tight LD with twenty neighbors in Europeans may have only three correlated variants in African-ancestry populations, where shorter LD blocks (reflecting larger historical effective population size) provide greater resolution.\nJoint fine-mapping across ancestries leverages these differences systematically (Kichaev et al. 2017). When a variant remains strongly associated across populations despite different local LD structure, confidence in its causal role increases. The logic is straightforward: a true causal variant should show consistent association regardless of which other variants happen to be correlated with it in any particular population. A tag variant, by contrast, may appear associated in one population (where it correlates with the causal variant) but not in another (where that correlation is absent).\nMulti-ancestry approaches grow increasingly important as large biobanks expand to include diverse populations, though they require careful attention to potential effect size heterogeneity across populations. The core assumption that causal variants produce consistent effects worldwide can be violated through several mechanisms. Gene-environment interactions represent one such mechanism: a variant’s phenotypic effect may depend on environmental exposures that differ systematically across populations. The FTO obesity-associated variants, for instance, show stronger effects in sedentary populations than in physically active ones, and lactase persistence variants in LCT produce metabolic consequences only where dairy consumption is common. When populations differ in relevant environmental contexts, effect sizes will differ even for genuinely causal variants.\nGenetic background effects present a second complication. A variant’s impact may depend on epistatic interactions with other loci, and if modifier variants differ in frequency across populations, the focal variant will appear to have population-specific effects. A causal variant might produce large effects only when a particular haplotype is present at an interacting locus; if that haplotype is common in one population but rare in another, the apparent effect of the causal variant will vary despite its genuine causal role. These complexities do not invalidate multi-ancestry fine-mapping, but they do mean that variants showing heterogeneous effects across populations should not be automatically dismissed. The method gains statistical power by assuming effect consistency, yet biological reality sometimes violates this assumption in ways that could exclude true causal variants or reduce confidence in them.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-pgs-construction",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-pgs-construction",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.5 Polygenic Score Construction",
    "text": "3.5 Polygenic Score Construction\nA 35-year-old woman with a family history of breast cancer asks her physician whether she should begin mammography screening earlier than guidelines recommend. Traditional risk models incorporate family history, age, and reproductive factors, but cannot capture the cumulative effect of thousands of common variants, each conferring small increases in risk, that together may substantially elevate her probability of disease. Polygenic scores address this gap by aggregating variant effects across the genome into a single number:\n\\[\n\\text{PGS}_i = \\sum_{j} w_j g_{ij}\n\\]\nThe weight \\(w_j\\) reflects the estimated effect of variant \\(j\\), and \\(g_{ij}\\) is the genotype dosage for individual \\(i\\). The simplest approach uses GWAS effect size estimates directly as weights; more sophisticated methods adjust for LD, apply shrinkage, or incorporate fine-mapping information. The clinical promise is substantial: for diseases with significant genetic components, polygenic scores can identify individuals at elevated risk years or decades before disease onset, potentially enabling targeted screening or prevention.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 3.4: [High] Side-by-side comparison of C+T vs LD-aware Bayesian methods. Left panel (C+T): Start with GWAS summary statistics → apply p-value threshold → clump by LD → retain independent lead SNPs → weight by effect size. Show visually how most variants are discarded. Right panel (LDpred/PRS-CS): Same summary statistics → model LD structure jointly → shrink effects toward zero based on prior → retain all variants with modulated weights. Highlight that C+T discards information while Bayesian methods model it.\n\n\n\n\n\n\n\n\n\nNoteTerminology: PGS versus PRS\n\n\n\nThe literature uses overlapping terminology. Polygenic risk score (PRS) is common in clinical contexts, emphasizing disease risk prediction. Polygenic score (PGS) is more general, encompassing both disease and quantitative trait prediction. Genomic risk score and related terms also appear, often interchangeably. This book uses PGS as the default, adding “risk” when specifically discussing disease outcomes. Methodological overviews provide detailed guidance on construction and evaluation (Choi, Mak, and O’Reilly 2020).\n\n\nIntegration of polygenic scores with foundation model features represents an emerging approach to clinical risk prediction. Feature extraction from genomic foundation models (Section 9.3) can provide complementary signals to traditional PGS. The integration strategies for combining PGS with deep learning approaches are examined in Section 25.1.\n\n3.5.1 Clumping and Thresholding\nThe challenge of constructing a useful polygenic score is not mathematical but statistical: GWAS provide noisy estimates of millions of effects, many of which are correlated through LD, and naive summation produces scores dominated by noise rather than signal. Clumping and thresholding (C+T) represents the simplest solution: reduce both the noise and the correlation by aggressive filtering, accepting substantial information loss in exchange for robustness.\nThe procedure begins with clumping: variants are ranked by p-value, then iteratively the most significant variant is selected and all variants within a specified window (typically \\(250\\ \\mathrm{kb}\\)) in LD above a threshold (typically \\(r^2 &gt; 0.1\\)) are removed. This yields a set of approximately independent index variants. A p-value cutoff then retains only variants below threshold. Finally, weights are set equal to the GWAS effect size estimate for retained variants, and zero otherwise.\nThe hyperparameters (LD window, \\(r^2\\) threshold, p-value threshold) are typically chosen by grid search to maximize predictive performance in a held-out validation set. This tuning introduces overfitting risk, particularly in small samples or when the validation population differs from the eventual deployment population.\nC+T is transparent and computationally simple, but it discards substantial information. Most variants are excluded, LD is handled only through coarse pruning, and variants with modest p-values that collectively explain meaningful variance may be entirely omitted. For highly polygenic traits where thousands of variants each contribute small effects, this information loss substantially degrades prediction accuracy. The method treats LD as a problem to be eliminated rather than a correlation structure to be modeled, an approach that sacrifices power for simplicity.\n\n\n3.5.2 LD-Aware Bayesian Methods\nThe information discarded by C+T is not random noise; it contains genuine signal about genetic effects distributed across correlated variants. Rather than pruning away this structure, a more principled approach models the joint distribution of effect sizes explicitly, treating the true effects \\(\\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_M)\\) as random variables drawn from a prior distribution. Given GWAS summary statistics and an LD reference panel, these methods infer posterior mean effect sizes that serve as PGS weights. The key insight is that LD becomes information rather than nuisance: correlated variants constrain each other’s likely effects, improving estimation for all.\nLDpred assumes that a fraction \\(p\\) of variants have nonzero effects drawn from a Gaussian distribution, while the remainder have zero effect (Vilhjálmsson et al. 2015). The method uses GWAS summary statistics and LD from a reference panel (computed from a subset of individuals or external dataset matching the target ancestry) to compute approximate posterior effect sizes. These posteriors shrink noisy estimates toward zero, borrow strength across correlated variants, and generally outperform C+T when properly tuned.\nPRS-CS extends this framework by placing a continuous shrinkage prior on effect sizes, which better accommodates the highly polygenic architecture of complex traits and reduces sensitivity to the sparsity hyperparameter (Ge et al. 2019). The continuous prior assigns most variants small but nonzero effects rather than forcing a binary causal/non-causal distinction. The method has shown strong performance across a range of traits and ancestries, though like all methods it requires an LD reference that reasonably matches the target population.\nRelated approaches (lassosum, SBayesR, and others) use different priors or optimization strategies but share the core insight: jointly modeling effect sizes under LD yields better predictions than pruning LD away. Performance differences among methods are often modest when each is well-tuned, and the choice may depend on computational resources, availability of validation data, and specific trait architecture.\n\n\n3.5.3 Fine-Mapping-Informed Scores\nPolygenic scores built on tag SNPs face a fundamental portability problem: the tag-causal correlation that justified including a variant may not hold in populations with different LD structure. Fine-mapping outputs, particularly posterior inclusion probabilities, offer a potential solution by identifying variants more likely to be causal. Causal variants should remain predictive regardless of population-specific LD patterns, since their effects are direct rather than mediated through correlation.\nTwo strategies incorporate fine-mapping information into PGS construction. Selection approaches retain only variants above a PIP threshold (typically 0.1 or 0.5), focusing the score on high-confidence causal candidates. Weighting approaches modulate each variant’s contribution by its PIP, downweighting likely tags while preserving information from variants with intermediate evidence.\nFine-mapping-informed approaches aim to concentrate weight on variants that are biologically meaningful rather than merely statistically associated. In principle, this should improve cross-ancestry transferability since causal variants remain causal regardless of population-specific LD patterns. In practice, gains depend on fine-mapping resolution, which is limited in regions of tight LD. The approaches remain an active area of methodological development, with potential for substantial improvement as multi-ancestry fine-mapping resources expand.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-pgs-interpretation",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-pgs-interpretation",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.6 Polygenic Score Interpretation",
    "text": "3.6 Polygenic Score Interpretation\nA polygenic score is a number, but numbers do not make clinical decisions. A patient told they are in the 95th percentile of genetic risk may interpret this as near-certain disease development, while a physician may recognize it as modest risk elevation insufficient to change management. Converting a score into actionable information requires understanding what it represents, how it relates to disease risk or trait values, and where its interpretation breaks down. Miscommunication at this stage can transform a useful risk stratification tool into a source of inappropriate anxiety or false reassurance.\n\n3.6.1 Relative Risk and Percentiles\nThe most immediate clinical question about a high polygenic score is: how much does it increase risk? Polygenic scores are most naturally interpreted in relative terms by fitting a logistic regression in a validation cohort:\n\\[\n\\log \\frac{P(y_i = 1)}{P(y_i = 0)} = \\alpha + \\theta \\cdot \\mathrm{PGS}_i + \\eta^\\top z_i\n\\]\nwhere \\(z_i\\) contains covariates and \\(\\theta\\) captures the effect of the PGS. After standardizing the score to unit variance, \\(\\exp(\\theta)\\) gives the odds ratio per standard deviation of the PGS. This metric allows statements such as “individuals one standard deviation above the mean have 1.5-fold higher odds of disease.”\nPercentile-based communication is common in clinical contexts. The risk for individuals in the top 1% or 5% of the PGS distribution can be compared to those near the median or in the bottom percentiles. For some conditions, individuals in the top percentiles have risk comparable to or exceeding that conferred by single high-penetrance mutations: the top 8% of the coronary artery disease PGS distribution has risk equivalent to familial hypercholesterolemia carriers, and the top 1% of the breast cancer PGS distribution has lifetime risk approaching that of BRCA2 mutation carriers (Khera and Kathiresan 2017; Mavaddat et al. 2019). This finding makes polygenic scores potentially relevant for clinical risk stratification, though the appropriate thresholds and clinical actions remain subjects of ongoing research and debate.\nTranslation of PGS into clinical decision-making requires careful attention to calibration and uncertainty quantification. Clinical risk prediction frameworks that integrate multiple evidence types are detailed in Section 25.2. Calibration requirements for clinical deployment appear in Section 23.2 for general principles and Section 25.6.2 for clinical-specific considerations.\n\n\n3.6.2 Absolute Risk\nA physician cannot act on relative risk alone; clinical decisions require knowing the probability that this specific patient will develop disease over a specified time horizon. Relative risk statements can mislead when baseline risk varies substantially. A 1.5-fold increase in odds for a disease with 1% baseline risk means absolute risk rises from 1% to roughly 1.5%; the same relative increase for a disease with 20% baseline risk means absolute risk rises from 20% to roughly 26%. A patient told they have “50% higher risk” may react very differently depending on whether baseline risk is low or high.\nConverting PGS to absolute risk requires combining the score with baseline incidence rates, which vary by age, sex, and other factors. The hazard ratio per standard deviation of PGS, combined with age-specific incidence curves from population registries, can yield personalized risk trajectories. Such calculations demand careful attention to calibration: the model must produce well-calibrated probabilities in the population where it will be deployed, not just the population where it was trained. A model calibrated in UK Biobank may systematically over- or under-estimate risk when applied to a U.S. clinical population with different baseline incidence rates or healthcare practices. Clinical deployment of PGS is addressed in detail in Chapter 25.\n\n\n3.6.3 Explained Variance and Discrimination\nPopulation-level performance metrics determine whether a polygenic score has any utility, but they can mask the substantial uncertainty that remains for any individual patient. For quantitative traits, the coefficient of determination (\\(R^2\\)) between PGS and phenotype provides a direct measure of explanatory power. Height PGS now explain roughly \\(25\\%\\) of phenotypic variance in European-ancestry populations, approaching the theoretical maximum given current sample sizes and the heritability of the trait (Yengo et al. 2022). For binary traits, the \\(R^2\\) on the liability scale (the underlying continuous risk) is more interpretable than the observed-scale \\(R^2\\), which depends on disease prevalence.\nArea under the receiver operating characteristic curve (auROC) measures discrimination: the probability that a randomly selected case has a higher PGS than a randomly selected control. auROC values of 0.5 indicate no discrimination (random guessing); values approaching 1.0 indicate near-perfect separation. For most complex diseases, PGS achieve auROC values in the 0.55 to 0.70 range when used alone, with incremental gains when combined with traditional risk factors (Torkamani, Wineinger, and Topol 2018; Lambert, Abraham, and Inouye 2019). These values reflect meaningful stratification at the population level but limited utility for individual prediction.\nEven a PGS that explains 10% of trait variance leaves 90% unexplained by factors genetic and environmental. High-risk individuals by PGS may never develop disease; low-risk individuals may be affected. Polygenic scores provide probabilistic risk stratification, not deterministic prediction. This distinction is critical for clinical communication and for setting appropriate expectations about what genomic risk information can and cannot offer.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-portability",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-portability",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.7 Ancestry, Portability, and Fairness",
    "text": "3.7 Ancestry, Portability, and Fairness\nThe vast majority of GWAS participants have been of European ancestry: as of 2019, approximately 78% of participants were European despite Europeans comprising roughly 16% of the global population (Martin et al. 2019). This historical imbalance has profound consequences for who benefits from polygenic scores and who may be harmed by their limitations. A technology that works well for some populations and poorly for others is not merely incomplete; deployed without appropriate caution, it risks widening existing health disparities rather than narrowing them.\nPopulation structure creates systematic portability challenges that extend beyond GWAS to all genomic models. The sources of this population stratification are detailed in Section 2.2, while systematic analysis of ancestry confounding appears in Section 22.2.1. Training data diversity requirements for foundation models that generalize across populations are discussed in Section 2.9.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 3.5: [High] Bar chart showing relative prediction accuracy (\\(R^2\\) or auROC ratio) of European-derived PGS when applied to different ancestry groups. European as reference (\\(100\\%\\)), then decreasing accuracy for East Asian, South Asian, Hispanic/Latino, and African-ancestry populations (often \\(25\\%\\)–\\(60\\%\\) of European performance). Include error bars. Overlay or annotate factors contributing to the gap: LD differences, allele frequency differences, potential effect heterogeneity, training sample size disparities.\n\n\n\n\n3.7.1 Portability Problem\nPolygenic scores derived from European-ancestry GWAS show markedly reduced performance in other populations. African-ancestry individuals typically experience 40% to 75% reductions in prediction accuracy compared to European-ancestry individuals, even for the same trait measured in the same study (Duncan et al. 2019; Martin et al. 2019). The pattern holds across traits and across methods, though the magnitude varies with genetic architecture and the degree of shared causal variants.\nSeveral factors contribute to this portability failure. LD structure differs across populations: tag SNPs that effectively proxy causal variants in Europeans may be poor proxies in populations with different recombination history. Allele frequencies differ: variants common in one population may be rare or absent in another. Effect sizes may genuinely differ across populations due to gene-environment interactions or genetic background effects. And GWAS in smaller non-European samples have less power to detect associations, yielding noisier effect estimates that further degrade prediction.\nMulti-ancestry GWAS and methods designed to leverage diverse training data offer partial solutions. Including multiple ancestries in discovery improves transferability, and methods that explicitly model ancestry-specific LD or effect sizes can enhance performance (Márquez-Luna et al. 2017). Yet even state-of-the-art approaches do not fully close the gap, and substantial research is needed before PGS perform equitably across populations.\n\n\n3.7.2 Fairness and Health Equity\nThe performance gap across ancestries is not merely a technical nuisance; it raises fundamental questions about fairness in precision medicine. If genomic models work primarily for individuals of European ancestry, deploying these models in diverse clinical populations risks exacerbating existing health disparities rather than ameliorating them. The communities historically excluded from genetic research would continue to receive inferior genomic medicine, now encoded in algorithmic form.\nConsider a scenario where PGS are used for risk-stratified screening. If the score identifies high-risk individuals more accurately in Europeans than in other groups, Europeans receive more targeted and efficient screening while others receive either under-screening (if falsely classified as low risk) or over-screening (if falsely classified as high risk). The benefits of precision medicine accrue disproportionately to those already overrepresented in research, while the costs of miscalibration fall on those historically excluded.\nThese challenges extend beyond PGS to every genomic model. Foundation models can learn to exploit ancestry signals as shortcuts, achieving high benchmark performance while performing poorly on underrepresented groups. Aggregate performance metrics mask inequities across populations. Deployment in diverse clinical settings requires explicit evaluation of performance stratified by ancestry, along with transparent reporting of limitations and appropriate caution in populations where validation is limited. These issues receive comprehensive treatment in Chapter 22, with governance and policy responses addressed in Chapter 29.\n\n\n\n\n\n\nNoteThe 78 Percent Problem\n\n\n\nAs of 2019, approximately 78% of GWAS participants were of European ancestry despite Europeans comprising roughly 16% of the global population (Martin et al. 2019). This disparity propagates through every layer of genomic medicine. Polygenic scores derived from European-ancestry GWAS show 40-75% reductions in prediction accuracy for African-ancestry individuals, even for the same trait measured in the same study. Variant databases like ClinVar contain far more pathogenic classifications for European-ancestry variants, leaving variants from underrepresented populations more likely to remain classified as VUS due to insufficient evidence. Foundation models inherit these biases at the root: a model trained on skewed data cannot be corrected post hoc to achieve the performance it would have achieved with representative training data.\nThe disparity is not merely statistical. If genomic risk scores are used for screening decisions, Europeans receive more accurate risk stratification while other populations receive either under-screening (if falsely classified as low risk) or over-screening (if falsely classified as high risk). The benefits of precision medicine accrue disproportionately to those already overrepresented in research.\nThis problem recurs throughout the book: in variant effect prediction (Chapter 14), model confounding (Chapter 22), clinical risk prediction (Chapter 25), and the governance frameworks needed to address it (Chapter 29). [Citation Needed for VUS statistics]",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-phewas",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-phewas",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.8 Phenome-Wide Association Studies",
    "text": "3.8 Phenome-Wide Association Studies\nGWAS answer a specific question: which variants associate with this phenotype? The reverse question is equally informative: which phenotypes associate with this variant, or with this set of variants aggregated into a polygenic score? Phenome-wide association studies (PheWAS) systematically test associations between genetic variants and hundreds or thousands of phenotypes, revealing pleiotropy that single-phenotype analyses cannot detect. A variant initially discovered for its association with coronary artery disease may also associate with type 2 diabetes, lipid levels, and blood pressure, connections that illuminate shared biology and inform variant interpretation.\nThis reversal of the GWAS paradigm has proven particularly valuable for understanding polygenic score biology. A polygenic score constructed for one trait often predicts other traits, sometimes to a surprising degree. The coronary artery disease PGS predicts not only heart attacks but also diabetes, hypertension, and mortality from other vascular causes. These cross-phenotype associations reflect the shared genetic architecture among related traits and the pleiotropic effects of common variants. They also reveal where phenotype definitions may be capturing overlapping constructs or where biological pathways connect seemingly distinct outcomes.\n\n3.8.1 PheWAS Framework\nPheWAS implementations parallel GWAS but with dimensions transposed. Rather than testing millions of variants against one phenotype, PheWAS tests one variant (or score) against hundreds of phenotypes. The phenotype vocabulary typically derives from EHR codes grouped into clinically meaningful categories. Phecodes collapse ICD-9 and ICD-10 billing codes into approximately 1,800 phenotype groups, aggregating related codes (such as the many ICD codes for diabetes mellitus) into unified disease concepts while distinguishing diseases that occupy nearby code ranges but represent different conditions.\nThe statistical framework mirrors GWAS: logistic regression for binary phenotypes, linear regression for quantitative traits, adjustment for covariates including age, sex, and genetic ancestry. Multiple testing correction accounts for the hundreds of tests performed; the Bonferroni threshold at \\(1{,}800\\) phecodes requires \\(p &lt; 2.8 \\times 10^{-5}\\) for significance. False discovery rate control offers a less conservative alternative appropriate when characterizing the landscape of associations rather than declaring individual findings.\nInterpretation requires attention to the correlation structure among phenotypes. A variant associated with obesity will, by mechanical consequence, associate with any phenotype more common in obese individuals. True pleiotropy (the variant affecting multiple traits through independent biological pathways) cannot be distinguished from mediated pleiotropy (the variant affecting one trait that causes others) through PheWAS alone. Colocalization analysis, conditional testing, and Mendelian randomization provide complementary evidence about whether associations reflect shared causal variants or confounded correlations.\n\n\n3.8.2 PheWAS for Polygenic Score Interpretation\nSingle variants have modest effects on complex traits, limiting the power of variant-level PheWAS for common diseases. Polygenic scores aggregate these effects across thousands of variants, providing sufficient signal for phenome-wide characterization. PRS-PheWAS tests the association between a polygenic score and each phenotype in the vocabulary, revealing the full spectrum of traits that share genetic architecture with the index phenotype.\nXu et al. applied this framework to interpret EHR-embedding-based polygenic scores, finding that scores derived from cardiovascular-related embedding dimensions associated strongly with circulatory system diagnoses across the phenome (Xu et al. 2025). The PheWAS results explained why cardiovascular traits showed the largest improvements from embedding-enhanced prediction: the embeddings captured genetic signal shared across the cardiovascular phenotype cluster. This approach provides a systematic method for understanding what a polygenic score actually predicts and whether its cross-phenotype associations match biological expectations.\nPRS-PheWAS also reveals unexpected associations that may indicate shared biology, confounding, or phenotype definition artifacts. A diabetes PGS that associates with billing codes for insulin pumps reflects healthcare utilization rather than disease biology. A depression PGS that associates with chronic pain diagnoses may indicate shared genetic liability, diagnostic conflation, or the medical consequences of depression. Distinguishing these possibilities requires domain knowledge and follow-up analyses that the PheWAS itself cannot provide.\nPRS-PheWAS for clinical interpretation of polygenic scores is examined in detail in Section 25.4.3, where phenome-wide associations inform which traits share genetic architecture and can benefit from multi-trait prediction.\n\n\n3.8.3 Phenotype Quality and PheWAS Power\nThe power of PheWAS depends critically on phenotype quality, which varies enormously across the hundreds of conditions in a typical phecode vocabulary. Well-captured phenotypes with clear diagnostic criteria (type 2 diabetes, hypothyroidism) yield stronger associations than poorly captured phenotypes that depend on documentation practices (chronic fatigue syndrome, fibromyalgia). Rare phenotypes with few cases lack power regardless of effect size. Common phenotypes with high misclassification rates suffer attenuated effects.\nThis heterogeneity complicates interpretation of phenome-wide results. The absence of an association may reflect genuine lack of pleiotropy or insufficient power to detect it. The pattern of associations across phenotype categories may reflect genuine biological clustering or differential phenotype quality across clinical domains. Cardiovascular phenotypes in hospital-based EHRs are typically well-captured because they drive admissions and procedures; psychiatric phenotypes are poorly captured because they are often managed in outpatient settings that may not feed into the research EHR.\nRecognition of these limitations has motivated phenotype quality assessment as a prerequisite for PheWAS. Metrics such as the proportion of cases with supporting laboratory values, the consistency of coding over time, and the agreement between algorithmic definitions and chart review provide evidence about which phenotypes can support reliable association testing. Restricting analyses to high-quality phenotypes improves specificity at the cost of comprehensiveness.\nThe phenotype quality challenges described here recur in multi-omic integration (Section 19.4) and clinical deployment of risk models (Section 25.6). Systematic approaches to phenotype quality assessment connect to label quality discussions in Section 22.2.4.\n\n\n3.8.4 Deep Phenotyping and Embedding-Enhanced GWAS\nThe limitations of binary phecode phenotypes have motivated alternative approaches that leverage richer phenotypic representations. Rather than testing association with categorical disease labels, these methods test association with continuous phenotypic embeddings that capture clinical similarity and co-occurrence structure. A patient’s position in embedding space reflects their full clinical profile rather than the presence or absence of specific diagnoses.\nEHR-embedding GWAS treats each dimension of a phenotype embedding as a quantitative trait, conducting standard GWAS to identify variants associated with that dimension. Xu et al. found that such embedding dimensions show significant heritability and genetic correlation with diverse clinical traits, suggesting they capture biologically meaningful phenotypic variation (Xu et al. 2025). Hierarchical clustering of traits by their genetic correlation profiles with embedding dimensions recovered clinically coherent groups, including a cardiovascular cluster comprising coronary artery disease, ischemic stroke, peripheral artery disease, type 2 diabetes, and related conditions.\nThese embedding-based approaches offer several advantages over traditional phenotype definitions. They avoid the information loss inherent in binary case-control classification. They capture phenotypic relationships that expert-defined definitions may miss. They can be constructed from the same EHR data used for genotype-phenotype analysis, requiring no additional phenotyping effort. Their limitations include interpretability (what does association with embedding dimension 7 mean biologically?) and potential circularity (if embeddings capture coding practices, GWAS may identify variants associated with healthcare utilization rather than disease). [Citation Needed for interpretability challenges]\nThe integration of phenotype embeddings with polygenic prediction represents an active research frontier. Embedding-enhanced polygenic risk scores combine traditional single-trait scores with scores derived from EHR-embedding GWAS, leveraging the genetic correlations among related phenotypes to improve prediction for the target trait. For cardiovascular outcomes where phenotypes cluster together and share genetic architecture, this integration has shown substantial improvements over single-trait scores. The approach is examined in detail in Section 25.3 for feature extraction strategies and Section 25.4 for EHR embedding approaches.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-mechanism",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-mechanism",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.9 From Association to Mechanism",
    "text": "3.9 From Association to Mechanism\nGWAS and polygenic scores have delivered thousands of robust trait associations, clinically useful risk stratification for some conditions, and fundamental insights into the polygenic architecture of complex phenotypes. They have also exposed a persistent gap between statistical association and biological understanding. Most GWAS hits lie in noncoding regions, often within enhancers, promoters, or other regulatory elements. The variant is associated; the mechanism is obscure. Fine-mapping narrows the list of candidates but rarely identifies a single causal nucleotide with confidence. Even when a variant is prioritized, the path from sequence change to molecular consequence to cellular phenotype to disease remains opaque.\nThis mechanistic gap limits translation in concrete ways. Drug development requires actionable targets, not associated regions. Clinical variant interpretation needs to explain why a variant matters, not just that it correlates with disease. Polygenic scores stratify population risk but offer little guidance on individual intervention. Multiple complementary strategies address this gap: regulatory sequence models predict how variants alter transcription factor binding and chromatin accessibility (Chapter 13), variant effect predictors assess functional impact at nucleotide resolution (Chapter 14), and multi-omics integration approaches connect genetic variation to intermediate molecular phenotypes (Chapter 19). Network-based approaches that integrate GWAS results with protein interaction networks and pathway information are examined in Section 18.4.1 for gene prioritization and Section 18.4.2 for therapeutic target identification.\nThe goal is not to replace statistical genetics but to build on it. Association provides the map of where trait-relevant variation resides; mechanistic modeling attempts to explain how that variation produces its effects. The combination of statistical association and mechanistic interpretation offers the most promising path toward genomic medicine that is both predictive and understood.\n\n\n\n\nBenner, Christian, Chris C. A. Spencer, Aki S. Havulinna, Veikko Salomaa, Samuli Ripatti, and Matti Pirinen. 2016. “FINEMAP: Efficient Variable Selection Using Summary Data from Genome-Wide Association Studies.” Bioinformatics 32 (10): 1493–1501. https://doi.org/10.1093/bioinformatics/btw018.\n\n\nBuniello, Annalisa, Daniel Suveges, Carlos Cruz-Castillo, Manuel Bernal Llinares, Helena Cornu, Irene Lopez, Kirill Tsukanov, et al. 2025. “Open Targets Platform: Facilitating Therapeutic Hypotheses Building in Drug Discovery.” Nucleic Acids Research 53 (D1): D1467–75. https://doi.org/10.1093/nar/gkae1128.\n\n\nChoi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O’Reilly. 2020. “[PRS] Tutorial: A Guide to Performing Polygenic Risk Score Analyses.” Nature Protocols 15 (9): 2759–72. https://doi.org/10.1038/s41596-020-0353-1.\n\n\nDuncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R. Peterson, and B. Domingue. 2019. “Analysis of Polygenic Risk Score Usage and Performance in Diverse Human Populations.” Nature Communications 10 (1): 3328. https://doi.org/10.1038/s41467-019-11112-0.\n\n\nElks, Cathy E., Marcel Den Hoed, Jing Hua Zhao, Stephen J. Sharp, Nicholas J. Wareham, Ruth J. F. Loos, and Ken K. Ong. 2012. “Variability in the Heritability of Body Mass Index: A Systematic Review and Meta-Regression.” Frontiers in Endocrinology 3 (February). https://doi.org/10.3389/fendo.2012.00029.\n\n\nGe, Tian, Chia-Yen Chen, Yang Ni, Yen-Chen Anne Feng, and Jordan W. Smoller. 2019. “Polygenic Prediction via Bayesian Regression and Continuous Shrinkage Priors.” Nature Communications 10 (1): 1776. https://doi.org/10.1038/s41467-019-09718-5.\n\n\nHilker, Rikke, Dorte Helenius, Birgitte Fagerlund, Axel Skytthe, Kaare Christensen, Thomas M. Werge, Merete Nordentoft, and Birte Glenthøj. 2018. “Heritability of Schizophrenia and Schizophrenia Spectrum Based on the Nationwide Danish Twin Register.” Biological Psychiatry, Novel Mechanisms in Schizophrenia Pathophysiology, 83 (6): 492–98. https://doi.org/10.1016/j.biopsych.2017.08.017.\n\n\nHormozdiari, Farhad, Emrah Kostem, Eun Yong kang, Bogdan Pasaniuc, and Eleazar Eskin. 2014. “Identifying Causal Variants at Loci with Multiple Signals of Association.” In Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics, 610–11. BCB ’14. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2649387.2660800.\n\n\nIonita-Laza, Iuliana, Kenneth McCallum, Bin Xu, and Joseph D. Buxbaum. 2016. “A Spectral Approach Integrating Functional Genomic Annotations for Coding and Noncoding Variants.” Nature Genetics 48 (2): 214–20. https://doi.org/10.1038/ng.3477.\n\n\nKhera, Amit V., and Sekar Kathiresan. 2017. “Genetics of Coronary Artery Disease: Discovery, Biology and Clinical Translation.” Nature Reviews Genetics 18 (6): 331–44. https://doi.org/10.1038/nrg.2016.160.\n\n\nKichaev, Gleb, Megan Roytman, Ruth Johnson, Eleazar Eskin, Sara Lindström, Peter Kraft, and Bogdan Pasaniuc. 2017. “Improved Methods for Multi-Trait Fine Mapping of Pleiotropic Risk Loci.” Bioinformatics 33 (2): 248–55. https://doi.org/10.1093/bioinformatics/btw615.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory M. Cooper, and Jay Shendure. 2014. “A General Framework for Estimating the Relative Pathogenicity of Human Genetic Variants.” Nature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nLambert, Samuel A, Gad Abraham, and Michael Inouye. 2019. “Towards Clinical Utility of Polygenic Risk Scores.” Human Molecular Genetics 28 (R2): R133–42. https://doi.org/10.1093/hmg/ddz187.\n\n\nMaller, Julian B., Gilean McVean, Jake Byrnes, Damjan Vukcevic, Kimmo Palin, Zhan Su, Joanna M. M. Howson, et al. 2012. “Bayesian Refinement of Association Signals for 14 Loci in 3 Common Diseases.” Nature Genetics 44 (12): 1294–1301. https://doi.org/10.1038/ng.2435.\n\n\nManolio, Teri A., Francis S. Collins, Nancy J. Cox, David B. Goldstein, Lucia A. Hindorff, David J. Hunter, Mark I. McCarthy, et al. 2009. “Finding the Missing Heritability of Complex Diseases.” Nature 461 (7265): 747–53. https://doi.org/10.1038/nature08494.\n\n\nMárquez-Luna, Carla, Po-Ru Loh, South Asian Type 2 Diabetes (SAT2D) Consortium, The SIGMA Type 2 Diabetes Consortium, and Alkes L. Price. 2017. “Multiethnic Polygenic Risk Scores Improve Risk Prediction in Diverse Populations.” Genetic Epidemiology 41 (8): 811–23. https://doi.org/10.1002/gepi.22083.\n\n\nMartin, Alicia R., Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada, Benjamin M. Neale, and Mark J. Daly. 2019. “Clinical Use of Current Polygenic Risk Scores May Exacerbate Health Disparities.” Nature Genetics 51 (4): 584–91. https://doi.org/10.1038/s41588-019-0379-x.\n\n\nMavaddat, Nasim, Kyriaki Michailidou, Joe Dennis, Michael Lush, Laura Fachal, Andrew Lee, Jonathan P. Tyrer, et al. 2019. “Polygenic Risk Scores for Prediction of Breast Cancer and Breast Cancer Subtypes.” The American Journal of Human Genetics 104 (1): 21–34. https://doi.org/10.1016/j.ajhg.2018.11.002.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy Schwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021. “An Open Approach to Systematically Prioritize Causal Variants and Genes at All Published Human GWAS Trait-Associated Loci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006. “Population Structure and Eigenanalysis.” PLOS Genetics 2 (12): e190. https://doi.org/10.1371/journal.pgen.0020190.\n\n\nPe’er, Itsik, Roman Yelensky, David Altshuler, and Mark J. Daly. 2008. “Estimation of the Multiple Testing Burden for Genomewide Association Studies of Nearly All Common Variants.” Genetic Epidemiology 32 (4): 381–85. https://doi.org/10.1002/gepi.20303.\n\n\nPrice, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E. Weinblatt, Nancy A. Shadick, and David Reich. 2006. “Principal Components Analysis Corrects for Stratification in Genome-Wide Association Studies.” Nature Genetics 38 (8): 904–9. https://doi.org/10.1038/ng1847.\n\n\nQuang, Daniel, Yifei Chen, and Xiaohui Xie. 2015. “DANN: A Deep Learning Approach for Annotating the Pathogenicity of Genetic Variants.” Bioinformatics 31 (5): 761–63. https://doi.org/10.1093/bioinformatics/btu703.\n\n\nRisch, Neil, and Kathleen Merikangas. 1996. “The Future of Genetic Studies of Complex Human Diseases.” Science 273 (5281): 1516–17. https://doi.org/10.1126/science.273.5281.1516.\n\n\nTorkamani, Ali, Nathan E. Wineinger, and Eric J. Topol. 2018. “The Personal and Clinical Utility of Polygenic Risk Scores.” Nature Reviews Genetics 19 (9): 581–90. https://doi.org/10.1038/s41576-018-0018-x.\n\n\nVilhjálmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev, Sara Lindström, Stephan Ripke, Giulio Genovese, et al. 2015. “Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores.” American Journal of Human Genetics 97 (4): 576–92. https://doi.org/10.1016/j.ajhg.2015.09.001.\n\n\nVisscher, Peter M., William G. Hill, and Naomi R. Wray. 2008. “Heritability in the Genomics Era — Concepts and Misconceptions.” Nature Reviews Genetics 9 (4): 255–66. https://doi.org/10.1038/nrg2322.\n\n\nWang, Gao, Abhishek Sarkar, Peter Carbonetto, and Matthew Stephens. 2020. “A Simple New Approach to Variable Selection in Regression, with Application to Genetic Fine Mapping.” Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (5): 1273–1300. https://doi.org/10.1111/rssb.12388.\n\n\nXu, Leqi, Wangjie Zheng, Jiaqi Hu, Yingxin Lin, Jia Zhao, Gefei Wang, Tianyu Liu, and Hongyu Zhao. 2025. “Improving Polygenic Risk Prediction Performance by Integrating Electronic Health Records Through Phenotype Embedding.” The American Journal of Human Genetics 112 (12): 3030–45. https://doi.org/10.1016/j.ajhg.2025.11.006.\n\n\nYang, Jian, Beben Benyamin, Brian P. McEvoy, Scott Gordon, Anjali K. Henders, Dale R. Nyholt, Pamela A. Madden, et al. 2010. “Common SNPs Explain a Large Proportion of the Heritability for Human Height.” Nature Genetics 42 (7): 565–69. https://doi.org/10.1038/ng.608.\n\n\nYengo, Loïc, Sailaja Vedantam, Eirini Marouli, Julia Sidorenko, Eric Bartell, Saori Sakaue, Marielisa Graff, et al. 2022. “A Saturated Map of Common Genetic Variants Associated with Human Height.” Nature 610 (7933): 704–12. https://doi.org/10.1038/s41586-022-05275-y.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html",
    "href": "part_1/p1-ch04-vep-classical.html",
    "title": "4  Classical Variant Prediction",
    "section": "",
    "text": "4.1 Conservation-Based Approaches\nConservation scores measure evolutionary constraint, not disease relevance. Protein-level predictors estimate structural disruption, not clinical pathogenicity. Splice site algorithms identify sequence motifs, not functional consequences. Every classical variant effect predictor measures a proxy for what clinicians actually need to know: will this variant cause disease in this patient? The gap between measurable signal and clinical question is irreducible. Evolutionary conservation reflects reproductive fitness, not human health; protein structure does not determine disease penetrance; and splice motifs do not guarantee splicing outcomes. Classical methods achieve what they achieve by combining multiple imperfect proxies, hoping that their convergence approximatehs clinical truth.\nThe conceptual foundations and practical methods that dominated variant interpretation before the foundation model era reveal both genuine insights and systematic blind spots. The trajectory from single-signal predictors through increasingly sophisticated ensemble methods traces a persistent gap: what we can measure differs from what clinicians need to know. Conservation scores like PhyloP and Genomic Evolutionary Rate Profiling (GERP) quantify purifying selection across evolutionary time, identifying positions where variation is depleted relative to neutral expectations. Protein-level tools like SIFT and PolyPhen-2 assess amino acid substitutions through sequence conservation and structural features. Splice predictors identify the sequence motifs that mark intron-exon boundaries. Each approach captures genuine biological signal, but each also fails in characteristic ways: conservation misses recently evolved human-specific functions, protein predictors cannot assess non-coding variants, splice algorithms miss cryptic sites and tissue-specific regulation.\nThe field’s response was to develop integrative methods that combine multiple signals into unified scores. CADD receives particular attention here because it introduced design patterns that recur throughout genomic machine learning: using evolutionary signals as proxy labels, training on large-scale genomic data, integrating dozens of diverse annotations, and precomputing scores genome-wide for downstream reuse. CADD and its successors, including REVEL, PrimateAI, and M-CAP, remain in active clinical use today. Understanding their construction and limitations illuminates both what classical methods achieved and why the field ultimately moved toward learned representations (Section 5.6 for learned representations, Chapter 7 for attention mechanisms, Section 10.2 for foundation model principles).\nA clinical geneticist evaluating a novel intronic variant faces an immediate problem: no functional annotation exists for most of the genome, and no clinical database has seen this specific change before. The variant lies outside any protein-coding region, no regulatory element overlaps it, and the patient’s phenotype offers no clear mechanistic hypothesis. Yet one source of information spans the entire genome and predates any experimental annotation by billions of years. If a genomic position has remained unchanged across species separated by hundreds of millions of years of evolution, mutations at that position are likely to be deleterious. Natural selection has already performed the largest functional screen imaginable, running experiments across countless organisms over evolutionary time, and conservation scores quantify the results.\nConservation signals derive from the population variant catalogs described in Section 2.2.3, where patterns of variation across populations reveal which positions tolerate change. The variant calling quality from Section 1.3 directly affects which variants appear in constraint calculations, potentially biasing constraint metrics in regions with systematic calling errors.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-conservation",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-conservation",
    "title": "4  Classical Variant Prediction",
    "section": "",
    "text": "FIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 4.2: [High] Multi-panel figure. Panel A: Multiple sequence alignment at a highly conserved position (same nucleotide across 30+ species) vs a neutrally evolving position (variable nucleotides). Show phylogenetic tree alongside alignment. Panel B: Distribution of phyloP or GERP scores genome-wide, with long right tail representing constrained elements. Mark threshold zones (e.g., phyloP &gt; 2 = strong evidence). Panel C: Example intronic variant at deeply conserved position with no other annotation, illustrating how conservation provides evidence in annotation-sparse regions.\n\n\n\n\n4.1.1 Evolutionary Constraint Metrics\nThe logic of conservation is straightforward: if a position matters for survival or reproduction, mutations there will be removed by selection before they can spread through the population. Quantifying this signal requires comparing sequences across species to identify positions where substitutions occur less frequently than expected under neutral evolution. Conservation scores translate this evolutionary signal into numerical values that can inform variant interpretation.\nPhyloP scores quantify the deviation of observed substitution rates from neutral expectation at individual positions(Siepel et al. 2005). The score is computed by comparing the observed pattern of bases at each alignment column against a neutral evolutionary model (typically fit to ancestral repeat sequences that are assumed to evolve without selective constraint). Positive phyloP scores indicate conservation, meaning evolution is slower than expected under neutrality. Negative scores indicate acceleration, suggesting faster evolution that may reflect positive selection. A phyloP score of 2 indicates that the observed base is approximately 100-fold more conserved than expected under neutrality, providing strong evidence that mutations at this position have been systematically removed by selection.\nGERP takes a complementary approach by estimating rejected substitutions at each position: the number of substitutions that would have been expected under neutrality but are absent from the observed alignment (Davydov et al. 2010). Large positive GERP scores indicate strong constraint. For a position conserved across 30 mammalian species, a GERP score of 5 implies that approximately five substitutions were rejected by selection over mammalian evolution. This interpretation connects directly to the biological process of purifying selection but depends on accurate neutral rate estimation and alignment quality.\nphastCons provides element-level rather than position-level conservation by identifying contiguous stretches of constrained sequence (Siepel et al. 2005). Using a hidden Markov model, phastCons classifies each position as belonging to a conserved or non-conserved state, then outputs the posterior probability of conservation. The resulting scores are smoother than position-level metrics, capturing functional elements that span multiple nucleotides even when individual positions show moderate conservation. This element-level view proves particularly valuable for identifying regulatory sequences where the overall constraint matters more than any single nucleotide.\nConservation scores from phastCons and phyloP serve as foundational features throughout computational genomics. Integrative methods like CADD (Section 4.3) combine conservation with dozens of other annotations to score variant deleteriousness. Fine-mapping algorithms use conservation to weight prior probabilities when prioritizing causal variants within GWAS loci (Section 3.3). Yet conservation measures evolutionary fitness, not disease relevance, and variants pathogenic in humans may be invisible to cross-species constraint. This evolutionary proxy problem creates fundamental limitations for clinical interpretation, examined in Section 4.1.2.\n\n\n4.1.2 What Conservation Measures Versus What Clinicians Need\nConservation scores measure evolutionary constraint: the degree to which a position has resisted substitution over millions of years. This is not the same as clinical relevance. A position can be evolutionarily constrained for functions unrelated to human disease, or clinically important despite modest conservation. The assumption underlying conservation-based interpretation is that positions under strong constraint are more likely to be functionally important and therefore more likely to cause disease when mutated. This assumption is often correct but not universally so.\nThe clinician wants to know: will this variant cause disease in my patient? Conservation provides indirect evidence: this position has been important for organismal fitness across evolutionary time. The gap between these questions creates interpretive challenges. A variant at a highly conserved position in a gene with no known disease association provides evolutionary evidence of functional importance but no direct path to clinical interpretation. Conversely, a variant at a modestly conserved position in a well-established disease gene may be clinically significant despite weak conservation signal.\n\n\n4.1.3 Clinical Application and Boundaries\nConservation scores prove particularly valuable for non-coding variant interpretation, where direct functional annotations are often incomplete or absent. A deeply conserved intronic position likely participates in splicing regulation, gene expression control, or other functional processes even if no explicit annotation overlaps it. Under ACMG-AMP guidelines for variant classification, strong conservation provides computational evidence (the PP3 criterion) supporting pathogenicity (Richards et al. 2015). A variant falling at a position with phyloP greater than 2 and GERP greater than 4 carries significantly more weight than one at an unconserved position, even when no other annotation is available. These scores remain central to clinical variant interpretation workflows, as examined in Chapter 25, where they contribute evidence alongside population frequency, functional studies, and segregation data.\nThe boundaries of conservation-based approaches are equally important to recognize, and these boundaries are not merely technical inconveniences but reflect fundamental gaps in what evolutionary signal can reveal. Conservation requires evolutionary time to accumulate signal. Recently evolved functional elements, including human-specific regulatory sequences and primate-specific genes, may show little conservation despite genuine function. A position can be functionally critical in humans yet unconserved because the function arose too recently for selection to leave a detectable signature. The fraction of the human genome showing evidence of human-specific function since the human-chimpanzee split (estimated at several percent) presents exactly this challenge: important to human biology, yet invisible to conservation metrics. [Citation Needed]\nConservation patterns vary dramatically by functional context. Neural development genes tend to be highly conserved across vertebrates, while immune genes evolve rapidly under positive selection. Critically, lack of conservation does not prove neutrality: a position may be diverging under positive selection, or may serve lineage-specific functions absent in the comparison species.\nConservation scores also face technical challenges from alignment quality. In repetitive regions, segmental duplications, and rapidly evolving gene families, reliable alignments may be impossible to construct, leaving conservation scores undefined or unreliable precisely where variant interpretation is most difficult. The HLA region, immunoglobulin loci, and centromeric sequences are clinically important yet systematically difficult to assess by conservation. The regions most difficult to interpret computationally are frequently those of greatest clinical interest.\nThese boundaries do not diminish the value of conservation; they define where that value applies. Conservation provides information largely orthogonal to population frequency (which reflects recent human history rather than deep evolutionary constraint) and to functional genomics annotations (which capture biochemical activity rather than selective importance). Integrative methods such as CADD combine conservation with these other signals to achieve better performance than any single source. Protein language models, examined in Chapter 12, learn conservation-like signals directly from sequence data without requiring explicit alignments, potentially addressing some technical limitations while introducing their own assumptions about what constitutes functional constraint.\n\n\n\n\n\n\nNoteACMG-AMP Variant Classification Framework\n\n\n\nThe American College of Medical Genetics and Genomics and the Association for Molecular Pathology jointly established a standardized framework for clinical variant interpretation (Richards et al. 2015). Variants are classified into five tiers: Pathogenic, Likely Pathogenic, Variant of Uncertain Significance (VUS), Likely Benign, and Benign. Classification combines multiple evidence types, each assigned a strength level. Pathogenic evidence ranges from very strong (PVS) through strong (PS), moderate (PM), to supporting (PP). Benign evidence follows a parallel hierarchy: stand-alone (BA), strong (BS), and supporting (BP).\nComputational predictions occupy the supporting tier: PP3 when algorithms predict a damaging effect, BP4 when they predict benign impact. This placement reflects appropriate caution. Computational evidence can contribute to classification but cannot establish pathogenicity or benign status independently. Multiple computational tools showing concordant predictions may strengthen the evidence, but the framework explicitly limits their weight. Variants classified primarily on computational grounds warrant flagging for reclassification as functional or clinical evidence emerges.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\n\n\nFigure 4.3: Caption…",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-protein-predictors",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-protein-predictors",
    "title": "4  Classical Variant Prediction",
    "section": "4.2 Protein-Level Predictors",
    "text": "4.2 Protein-Level Predictors\nA diagnostic laboratory receives exome sequencing results for a 45-year-old woman with early-onset breast cancer and a family history suggesting hereditary cancer syndrome. Among hundreds of rare variants, one stands out: a missense change in BRCA2 substituting glycine for arginine at a conserved position. Is this the explanation for her cancer, or an incidental finding? No previous case report exists for this exact variant. No functional assay has tested its effect. The question of whether this amino acid substitution disrupts BRCA2 function determines whether her siblings should be tested and whether she qualifies for PARP inhibitor therapy. The clinical stakes could not be higher, yet the evidence available is entirely computational. Protein-level predictors attempt to answer such questions by encoding biological intuition about which amino acid changes matter.\n\n4.2.1 SIFT: Sequence Homology as Functional Constraint\nConservation scores can identify constrained positions, but they cannot distinguish which substitutions at those positions are tolerated. A position might be highly conserved overall yet accept certain amino acid changes that preserve function. For missense variants specifically, the relevant question is not whether the position is constrained but whether the specific amino acid substitution disrupts function. SIFT addresses this distinction by examining which amino acids have been accepted at each position across evolutionary history (Ng and Henikoff 2003).\nSIFT collects homologous protein sequences from diverse species, constructs a multiple sequence alignment, and examines which amino acids appear at each position across the alignment. Positions that are highly conserved (showing the same or similar amino acids across species) are predicted to be functionally important; substitutions introducing amino acids not observed at that position are predicted to be deleterious.\nThe method computes a normalized probability for each possible amino acid at each position based on the diversity observed in the alignment. The SIFT score for a substitution is the probability of observing the mutant amino acid, scaled by the position’s overall diversity. Scores range from 0 to 1, with low scores (typically below 0.05) indicating predicted damage. A SIFT score of 0.01 for a particular missense variant indicates that the mutant amino acid is rarely or never observed at that position across the sequence family, suggesting functional constraint has prevented its fixation throughout evolution.\nSIFT’s simplicity is both its strength and its limitation. The method requires only protein sequence information and a database of homologs; it makes no assumptions about protein structure, physicochemistry, or mechanism of damage. This generality allows application to any protein with sufficient homologs in sequence databases. For proteins with few homologs, young gene families, or positions with limited alignment depth, predictions may be unreliable. The method captures only the evolutionary signal present in the alignment, missing functional constraints that arose recently or affect only a subset of species.\n\n\n4.2.2 PolyPhen-2: Integrating Structure and Sequence\nSIFT’s reliance on sequence alone ignores substantial information about how amino acid substitutions affect protein function. A glycine buried in a protein’s hydrophobic core will disrupt structure differently than one on a surface loop. A substitution at a catalytic site matters more than one far from any functional region. PolyPhen-2 extends sequence-based prediction by incorporating protein structure features and amino acid physicochemistry, recognizing that the same substitution can have different consequences depending on its structural context (Adzhubei et al. 2010).\nThe method uses a naive Bayes classifier trained to distinguish disease-causing mutations from neutral polymorphisms based on a collection of sequence-derived and structure-derived features. The feature set includes sequence conservation (similar to SIFT) but adds several structural descriptors when three-dimensional structure data is available: solvent accessibility (whether the position is buried or exposed), secondary structure context (α-helix, β-sheet, or random coil), and proximity to known functional sites. Amino acid physicochemical properties inform predictions about whether substitutions are conservative or radical. The Grantham distance, a measure of biochemical dissimilarity between amino acid pairs based on composition, polarity, and molecular volume, contributes to assessing substitution severity. [Citation Needed] Where Grantham distance derives from physicochemical properties, BLOSUM matrices capture empirical substitution frequencies observed across evolutionarily related proteins (Henikoff and Henikoff 1992). A glycine-to-arginine substitution (Grantham distance of 125) represents a far more radical change than a leucine-to-isoleucine substitution (Grantham distance of 5).\nPolyPhen-2 provides two models trained on different datasets: HumDiv, trained on disease-causing and neutral variants from protein sequence databases, and HumVar, trained on Mendelian disease mutations versus common human polymorphisms. The choice of training set affects score interpretation; HumVar produces more conservative predictions appropriate for clinical Mendelian disease variant classification, while HumDiv is more sensitive and may be preferable for research applications where missing a true positive is more costly than false positives.\nPolyPhen-2 scores range from 0 to 1, with higher scores indicating greater predicted deleteriousness. The output includes qualitative classifications (benign, possibly damaging, probably damaging) based on score thresholds. A PolyPhen-2 score of 0.95 with a “probably damaging” classification indicates high confidence that the substitution disrupts protein function, though the clinical significance depends on additional evidence about the specific disease context.\n\n\n4.2.3 From Sequence to Function\nPolyPhen-2, SIFT, and similar tools answer a mechanistic question: does this amino acid substitution disrupt protein function? They assess whether the new residue fits the structural context, whether the position tolerates variation across species, and whether the physicochemical change is drastic. These are genuine molecular insights. A “probably damaging” designation from PolyPhen-2 means the substitution likely impairs the protein’s normal function, but the clinical significance of that impairment requires reasoning that protein-level predictors cannot provide.\nProtein language models like ESM-2 (Section 12.1) provide alternative approaches to variant effect prediction that learn evolutionary constraint from sequence alone, without explicit multiple sequence alignments. ESM-based variant scoring is examined in Section 14.2, where zero-shot prediction paradigms avoid the limitations of supervised training on biased clinical labels\n\n\n4.2.4 Boundaries of Protein-Level Prediction\nSeveral fundamental boundaries constrain all protein-level predictors, and these boundaries are not merely technical inconveniences but reflect deep gaps in what sequence and structure analysis alone can reveal. Protein-level tools are restricted to missense variants; nonsense, frameshift, splice-altering, and non-coding variants lie entirely outside their scope. A patient’s most important variant may be intronic or synonymous, yet protein-level predictors have nothing to say about it. This constraint is absolute: these methods analyze amino acid substitutions and cannot be extended to other variant types without fundamental redesign.\nProtein-level predictors estimate impact on protein function without specifying the mechanism or clinical consequence. A variant predicted to damage function might impair enzymatic activity, disrupt protein folding, eliminate a binding interface, or alter stability. The clinical relevance depends on which function is affected and whether the phenotype results from loss of function or gain of function. A predicted-damaging variant in a tumor suppressor behaves very differently from one in an oncogene, yet protein-level predictors provide no information about this distinction. The same high score can indicate completely different clinical implications depending on biological context.\nThe terminology itself can mislead: “gain-of-function” sounds beneficial, yet these mutations cause some of the most severe human diseases. A gain-of-function variant does not improve the protein; it creates aberrant activity that the cell cannot regulate. In FGFR3, gain-of-function mutations cause the receptor to signal constitutively, driving achondroplasia by suppressing bone growth. In PIK3CA, activating mutations produce uncontrolled cell proliferation. In ion channels like SCN1A, gain-of-function can cause epileptic encephalopathy through neuronal hyperexcitability. These variants may preserve or even enhance protein structure and activity, precisely the opposite of what conservation-based predictors flag as damaging. A perfectly folded protein with enhanced catalytic activity will score as “tolerated” by methods designed to detect disruption, yet its clinical consequences may be devastating. Loss-of-function and gain-of-function mutations in the same gene can cause entirely different diseases: loss of RET function causes Hirschsprung disease (failed neural crest migration), while gain of RET function causes multiple endocrine neoplasia (tumor predisposition). Protein-level predictors cannot distinguish these mechanisms because they assess structural perturbation, not the direction of functional change.\nThese tools also provide no information about inheritance mode, penetrance, or expressivity. A strongly predicted-damaging variant in a gene with high tolerance to heterozygous loss may be clinically benign in carriers. Protein-level predictors cannot distinguish between a variant causing severe disease in homozygotes and one causing no disease at all when heterozygous. This distinction becomes critical when counseling families, where the mode of inheritance fundamentally changes recurrence risk and management recommendations.\nProtein-level predictors inherit the training data biases present in their underlying databases. Disease mutations in training sets are enriched for severe, early-onset Mendelian conditions with clear inheritance patterns. Variants causing subtle effects, incomplete penetrance, or complex phenotypes may be systematically mispredicted. The well-studied genes that dominate training data may not generalize to poorly characterized genes where variants are most difficult to interpret.\nSIFT and PolyPhen-2 remain widely used in clinical practice and serve as features within more sophisticated ensemble methods. Their scores appear in diagnostic reports, contribute to ACMG-AMP classification criteria, and inform variant prioritization in research and clinical pipelines. Understanding their construction and limitations is essential for appropriate interpretation.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-cadd",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-cadd",
    "title": "4  Classical Variant Prediction",
    "section": "4.3 CADD Framework",
    "text": "4.3 CADD Framework\nThe protein-level predictors and conservation scores examined above each capture one aspect of variant function, yet clinical interpretation requires weighing multiple lines of evidence simultaneously. A variant might fall in a conserved region, alter a moderately constrained amino acid, and overlap a predicted enhancer. How should these signals be combined? More fundamentally, how can we train a predictor when curated pathogenic variants number in the hundreds of thousands while the genome contains billions of possible mutations? These questions expose a fundamental tension: the variants we most need to interpret (rare, novel, never before seen) are precisely those for which training labels do not exist.\nCADD addressed these challenges by reframing variant effect prediction as a large-scale machine learning problem (Kircher et al. 2014; Rentzsch et al. 2019). The key insight was not better feature engineering or more sophisticated classification, but rather a reconceptualization of the labeling problem itself. Instead of training directly on small sets of known pathogenic versus benign variants, which are scarce and biased toward certain genes and variant types, CADD contrasts variants that have survived purifying selection in the human lineage with matched simulated variants that could have occurred but did not. This evolutionary proxy strategy transforms the labeling problem, yielding millions of training examples where curated datasets provide thousands.\n\n4.3.1 Evolutionary Proxy Training and Label Sources\nThe conceptual foundation of CADD rests on constructing training labels from evolutionary signal rather than clinical curation. The method builds two proxy classes of variants that serve as training labels, each designed to approximate a category that cannot be observed directly. Understanding these label sources is essential because the same proxy labeling strategies reappear throughout genomic machine learning, including in the foundation models discussed in Chapter 10 and Chapter 8.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 4.4: [Essential] Three-panel conceptual figure. Panel A (Proxy-Neutral): Human-derived alleles fixed since human-chimpanzee split; these variants survived selection, representing tolerated changes. Show evolutionary tree with human branch highlighted. Panel B (Proxy-Deleterious): Simulated variants matching human mutational processes (trinucleotide context); these represent possible-but-not-observed mutations enriched for deleterious effects. Show simulation schematic with mutation spectrum. Panel C (Classification): SVM learning to distinguish classes based on annotation features; output is “evolutionary tolerance” score, not pathogenicity directly.\n\n\n\nThe proxy-neutral class consists of variants that have been tolerated by purifying selection. CADD draws these from sequence differences that arose on the human lineage since the split from chimpanzees and became fixed or nearly fixed in modern humans. These are identified by their derived allele frequency: alleles that differ from the inferred ancestral state (typically determined by comparison to chimpanzee and other great ape sequences) and are present at very high frequency in human populations. Because these derived alleles have persisted over millions of years of evolution, most are presumed to be neutral or only weakly deleterious. This is not a perfect proxy: some observed alleles are genuinely pathogenic, particularly those with incomplete penetrance, late onset, or context-dependent effects. The proxy-neutral class is, on average, substantially enriched for tolerated alleles relative to a random sample of possible mutations.\nThe proxy-deleterious class is constructed by simulating mutations across the genome according to realistic mutational processes. The simulation matches local sequence context (typically using trinucleotide frequencies to capture the strong dependence of mutation rates on flanking bases). CpG dinucleotides, for example, have elevated mutation rates due to spontaneous deamination of methylated cytosines, and the simulation accounts for this by generating more CpG transitions. Regional variation in mutation rates, driven by factors including replication timing and chromatin state, is similarly incorporated.\nThe logic underlying this construction is subtle but powerful. Simulated variants represent changes that could plausibly occur under human mutational processes but are generally not observed at high frequency in population databases. The proxy-deleterious class as a whole is enriched for alleles disfavored by selection, because the set of possible mutations includes many that disrupt conserved elements, alter protein function, or perturb regulatory sequences. By contrasting this set with the proxy-neutral class (high derived allele frequency variants that survived selection), CADD learns to recognize the annotation signatures that distinguish variants under purifying selection from those that have been tolerated.\nThis proxy labeling strategy has important implications. CADD does not learn to distinguish pathogenic from benign variants directly; it learns to distinguish tolerated-by-evolution from possible-but-not-observed. The assumption is that variants depleted by selection are enriched for functional effects and therefore enriched for disease relevance. This assumption is often correct but introduces a systematic gap between what CADD measures (evolutionary tolerance) and what clinicians need (disease causation).\n\n\n4.3.2 Feature Integration\nConservation scores measure evolutionary constraint. Protein-level predictors assess amino acid substitution severity. Regulatory annotations mark biochemically active regions. Each signal captures genuine biology, but no single annotation captures the full complexity of variant function. A missense variant in a constrained gene might be tolerated if it falls in an unconserved loop region; a synonymous variant might be pathogenic if it disrupts splicing. The power of CADD lies in learning how these heterogeneous signals interact, upweighting annotations that distinguish proxy-deleterious from proxy-neutral variants and downweighting those that do not.\nCADD integrates more than 60 features, far exceeding what explicit combination rules could accommodate. Gene model annotations describe the local transcript and coding context of each variant. The most fundamental is the predicted sequence consequence: whether a variant is synonymous, missense, nonsense, frameshift, splice-site disrupting, or located in untranslated or intronic regions. Distance to exon-intron boundaries and proximity to canonical splice sites provide additional context. Gene-level attributes including constraint metrics (pLI, LOEUF from gnomAD) quantify how tolerant each gene is to damaging variation.\nThese constraint metrics derive from a fundamental concept in human genetics: haploinsufficiency. Most genes tolerate heterozygous loss because a single functional copy produces sufficient protein for normal function. Haploinsufficient genes are different; they require both copies to maintain adequate protein levels, making heterozygous loss-of-function variants pathogenic. The probability of loss-of-function intolerance (pLI) estimates the likelihood that a gene falls into this category based on the observed depletion of truncating variants in population databases. [Citation Needed] A pLI approaching 1.0 indicates that loss-of-function variants are nearly absent from healthy individuals, strong evidence that such variants cause disease or embryonic lethality. LOEUF (loss-of-function observed/expected upper bound fraction) refines this estimate by quantifying the ratio of observed to expected truncating variants with confidence bounds. Genes like MECP2 (Rett syndrome) and many transcription factors exhibit strong haploinsufficiency; genes encoding metabolic enzymes often tolerate 50% reduction without clinical consequence. By incorporating these gene-level constraint metrics, CADD can weight identical variants differently depending on whether they occur in dosage-sensitive or dosage-tolerant genes.\nCADD draws on three families of evidence. Evolutionary constraint from phyloP, GERP, and phastCons provides the conservation signals described earlier; incorporating multiple metrics computed from different alignments captures complementary aspects of selective pressure. For coding variants, amino acid substitution predictions from SIFT and PolyPhen-2 assess structural and functional disruption, supplemented by physicochemical properties, Grantham distances, and domain annotations from Pfam. Non-coding variants receive context from ENCODE and Roadmap Epigenomics annotations capturing chromatin accessibility, histone modifications, and transcription factor binding.\nAdditional features capture local sequence context (GC content, CpG density), genomic architecture (segmental duplications, repetitive elements), and chromosomal position. The model learns how to weight and combine these heterogeneous signals from the data rather than from expert specification.\nThe feature engineering approach reaches a performance ceiling because manually designed features encode only what biologists already know. This limitation motivates the shift to learned representations examined in Section 5.6 for tokenization strategies and Section 9.3 for foundation model features. The contrast between hand-crafted features and learned representations illuminates the paradigm shift toward foundation models discussed in Section 10.1\n\n\n4.3.3 Model Architecture and Scoring\nRaw classifier outputs are not directly interpretable as probabilities or biological effect sizes. A clinician presented with a support vector machine decision value has no intuitive understanding of what that number means. To address this, CADD defines PHRED-scaled scores based on the rank of each variant among all possible single-nucleotide substitutions in the reference genome. A scaled score of 10 indicates that a variant falls in the top 10% of predicted deleteriousness. A score of 20 indicates the top 1%, and a score of 30 indicates the top 0.1%. This rank-based transformation ensures comparability across CADD versions and provides immediate interpretability: a clinician can understand that a score of 25 places this variant among the most extreme 0.3% of possible mutations without needing to understand the underlying classifier.\nCADD’s classifier operates on the high-dimensional feature vector assembled for each variant. The original CADD model used a linear support vector machine trained to discriminate proxy-neutral and proxy-deleterious variants based on approximately 30 million training examples. The choice of a linear model was deliberate and pragmatic: with tens of millions of training examples and dozens of features, a linear classifier is computationally tractable while capturing the main structure of the classification problem. CADD version 1.7 transitioned from the SVM to logistic regression, which offers comparable discriminative performance while providing native probability outputs and faster scoring of new variants. [Citation Needed]\nIn clinical laboratories, CADD scaled scores commonly serve as filters to enrich for potentially pathogenic variants. Typical thresholds range from 15 (top 3%) to 20 (top 1%) or higher. Variants with scores at or above 20 are considered moderately high deleteriousness candidates, while scores at or above 30 are frequently interpreted as strongly enriched for functional impact. A diagnostic pipeline might use CADD greater than or equal to 20 as an initial filter, reducing 25,000 exome variants to several hundred candidates for expert review. These filters serve as prioritization tools that reduce the variant burden to a manageable number rather than as definitive pathogenicity calls.\n\n\n4.3.4 Evolutionary Proxy Problem\nCADD’s training signal derives entirely from evolutionary history: variants that survived natural selection versus those depleted by it. This creates a fundamental mismatch with clinical questions. Evolution optimizes for reproductive fitness across populations and timescales; clinical genetics asks whether a specific variant causes disease in a specific patient.\nThe mismatch manifests in predictable ways. Constraint varies across tissues and developmental stages, but CADD assigns a single genome-wide score. A variant in a deeply conserved neural enhancer receives a high score regardless of whether the patient presents with a cardiac or neurological phenotype. The constraint is real, but its clinical relevance depends on context CADD cannot assess.\nMore fundamentally, purifying selection only removes variants that reduce fitness before or during reproductive years. Late-onset diseases like Alzheimer’s or many cancers exert minimal selective pressure; variants causing these conditions may show little evolutionary depletion despite clear pathogenicity. Gain-of-function mutations present an even sharper challenge. A variant that creates a novel toxic function has no evolutionary precedent to deplete; CADD’s framework cannot recognize pathogenic mechanisms that evolution never encountered.\nThese limitations are not failures of implementation but consequences of the proxy relationship between evolutionary constraint and disease causation. CADD measures what evolution preserved and eliminated. Whether that corresponds to clinical pathogenicity depends on whether the disease mechanism falls within evolution’s purview.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-ensemble-methods",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-ensemble-methods",
    "title": "4  Classical Variant Prediction",
    "section": "4.4 Other Ensemble Methods",
    "text": "4.4 Other Ensemble Methods\nThe clinical geneticist focused exclusively on rare missense variants in Mendelian disease faces a different optimization problem than the researcher screening the entire genome for regulatory variants. CADD’s genome-wide generality may sacrifice accuracy within specific variant classes, accepting modest performance everywhere to achieve coverage anywhere. For diagnostic laboratories where missense variants in known disease genes dominate the caseload, specialized ensemble methods offer an alternative: models trained directly on curated disease variants, optimized for the specific task rather than general prioritization. This tension between generality and specialization recurs throughout computational biology, and different clinical contexts demand different tradeoffs.\nEnsemble principles for combining multiple predictors connect to deep ensemble approaches for uncertainty quantification in Section 23.4.1. Integration strategies that combine classical scores with foundation model predictions are examined in Section 14.4\n\n4.4.1 REVEL\nA missense variant in a known disease gene presents a narrower interpretive challenge than an arbitrary variant anywhere in the genome. The variant is protein-coding, the gene has established disease associations, and the question is specifically whether this amino acid substitution is pathogenic. This focused scope permits a different training strategy than CADD’s evolutionary proxy approach.\nREVEL represents a missense-specific ensemble predictor widely used in clinical laboratories (Ioannidis et al. 2016). Rather than training on evolutionary proxy labels, REVEL directly discriminates pathogenic missense variants (curated from HGMD and other disease databases) from rare putatively neutral missense variants observed in population datasets.\nREVEL integrates predictions from a panel of individual tools: SIFT, PolyPhen-2, PROVEAN, MutationAssessor, FATHMM, GERP++, phyloP, and phastCons, among others. A random forest model learns to combine these scores, weighting each according to its discriminative power for the pathogenic versus neutral classification. The training set is carefully constructed to avoid label contamination, excluding variants present in both disease and population databases.\nREVEL scores range from 0 to 1, with higher values implying greater pathogenicity likelihood. Common interpretation thresholds treat scores above 0.5 as supporting evidence for pathogenicity, with scores above 0.75 providing stronger evidence. REVEL is restricted to missense single-nucleotide variants, making it more specialized than CADD but often more accurate within its scope. This specialization reflects a deliberate choice: by giving up coverage of non-coding and structural variants, REVEL gains the ability to train on directly relevant labels rather than evolutionary proxies.\n\n\n4.4.2 M-CAP\nDiagnostic laboratories evaluating potential Mendelian disease variants face asymmetric consequences for errors. Calling a benign variant pathogenic can lead to unnecessary surgeries, psychological burden, and inappropriate cascade testing of family members. Missing a pathogenic variant delays diagnosis but typically permits later reclassification as evidence accumulates. This asymmetry argues for prioritizing specificity over sensitivity in clinical settings.\nM-CAP addresses specifically the challenge of distinguishing pathogenic from benign rare missense variants in Mendelian disease contexts, with explicit attention to this asymmetry (Jagadeesh et al. 2016). The method uses gradient boosting on a feature set including conservation scores, protein structure features, and amino acid properties.\nM-CAP was explicitly designed to minimize false positives while maintaining reasonable sensitivity. The developers tuned their classifier to achieve less than 5% false positive rate on known pathogenic variants, accepting some reduction in sensitivity as a tradeoff. This design philosophy differs from methods that balance sensitivity and specificity equally, reflecting M-CAP’s intended use in diagnostic settings where false positive pathogenicity calls have serious consequences.\n\n\n4.4.3 Comparison and Selection\nNo single ensemble method dominates across all variant types and clinical contexts. CADD provides the broadest coverage (genome-wide, all variant types) but may sacrifice accuracy within specific variant classes to achieve this generality. REVEL often outperforms CADD on missense-only benchmarks, reflecting its focused training objective. M-CAP prioritizes specificity over sensitivity, appropriate for clinical settings where avoiding false positives is paramount.\nClinical variant interpretation typically incorporates multiple computational scores rather than relying on any single predictor. Different scores may agree, providing stronger evidence, or disagree, flagging variants requiring careful manual review. A variant with CADD greater than or equal to 25, REVEL greater than or equal to 0.8, and M-CAP “possibly pathogenic” presents a consistent computational picture; one where CADD and REVEL disagree prompts closer examination of the underlying features. Understanding the construction, training data, and intended use case of each method is essential for appropriate interpretation. The integration of these scores into clinical workflows is examined in detail in Chapter 25 and Chapter 26, where computational evidence must be weighed alongside functional studies, segregation data, and clinical presentation.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 4.5: [High] ROC curves (or precision-recall curves) comparing CADD, REVEL, and M-CAP on a held-out benchmark of missense variants. Include curves for individual component scores (SIFT, PolyPhen-2, phyloP) to show improvement from integration. Mark operating points corresponding to common clinical thresholds (CADD ≥ 20, REVEL ≥ 0.75, M-CAP “possibly pathogenic”). Annotate sensitivity and specificity at each threshold. Include note about benchmark circularity caveats.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-circularity",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-circularity",
    "title": "4  Classical Variant Prediction",
    "section": "4.5 Circularity and Ascertainment Bias",
    "text": "4.5 Circularity and Ascertainment Bias\nA diagnostic laboratory classifies a novel missense variant as pathogenic based partly on its high CADD score. That classification enters ClinVar. Two years later, a benchmarking study evaluates CADD performance on ClinVar pathogenic variants and reports excellent accuracy. Is the high performance genuine, or has the benchmark been contaminated by the predictor’s own influence on the labels it is evaluated against? This scenario illustrates the first of the pervasive problems affecting all variant effect predictors: circularity between scores and clinical databases. Compounding this circularity is ascertainment bias in available training and testing variants. These issues do not invalidate classical scores, but they counsel appropriate humility about performance claims and careful attention in both development and application. The broader methodological concerns around benchmark design and confounding are examined in Chapter 22 and Chapter 20.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 4.6: [High] Circular diagram illustrating the feedback loop between computational predictors and clinical databases. Show cycle: (1) Computational score (e.g., high CADD) influences clinical classification; (2) Classified variant enters ClinVar as “Pathogenic”; (3) Benchmarking study evaluates CADD on ClinVar variants; (4) High benchmark performance encourages clinical adoption; (5) Return to step 1. Indicate intervention points: temporal holdouts, functional assay ground truth, prospective evaluation. Use visual metaphor of self-reinforcing spiral.\n\n\n\n\n4.5.1 Circularity Problem\nClinVar and similar clinical databases increasingly incorporate computational predictions as evidence supporting variant classification. When a clinical laboratory classifies a variant as pathogenic, the CADD score, PolyPhen-2 prediction, or other computational evidence may have contributed to that determination. When CADD is subsequently evaluated on ClinVar pathogenic variants, its performance is artificially inflated: the benchmark contains variants that were labeled partly because CADD assigned them high scores. The predictor appears to perform well because it was already part of the labeling process.\nThis circularity operates through multiple pathways. Direct use occurs when clinical laboratories explicitly cite computational scores in their classifications. Indirect influence arises when computational predictions shape clinical suspicion, affecting which variants receive functional testing or expert review. Selection bias in benchmark construction can compound the problem: benchmark creators may preferentially include variants with strong computational evidence, excluding ambiguous cases that would provide a more stringent test.\nThe consequence is that benchmark performance may overestimate real-world utility. A method that has been widely adopted will appear to perform well on benchmarks populated by variants classified using that method, even if its true discriminative power is more limited. This concern applies to all established computational tools, including conservation scores, protein-level predictors, and ensemble methods. The more influential a method becomes, the more its benchmark performance becomes self-reinforcing.\nAddressing circularity requires careful benchmark construction. Temporal holdouts (using only classifications made before a method’s widespread adoption) can reduce but not eliminate the problem. Functional assays that directly measure variant effects provide ground truth independent of computational predictions but are available for only a small fraction of variants. Prospective evaluation on newly classified variants offers the cleanest test but requires patience and ongoing data collection. The foundation model evaluations discussed in Chapter 14 face these same challenges, and the confounding issues examined in Chapter 22 show how these problems persist and evolve as methods become more sophisticated.\n\n\n4.5.2 Ascertainment Bias\nBeyond circularity lies a more fundamental problem: the variants available for training and evaluation represent a systematically skewed sample of disease-causing mutations. Clinical databases are dominated by variants in well-studied genes, particularly those causing severe Mendelian phenotypes with clear inheritance patterns. Protein-coding variants are overrepresented because they are easier to interpret and more often tested. Variants in genes associated with common diagnostic panels appear frequently; those in rarely tested genes are sparse.\nThis ascertainment bias shapes what models learn and how they perform. A predictor trained predominantly on variants in constrained genes may learn that gene-level constraint is the primary signal for pathogenicity. When applied to variants in less constrained genes (where pathogenic variants also occur, but less frequently), the model may systematically underestimate risk. Similarly, models trained on European-ancestry samples may encode population-specific patterns that transfer poorly to other populations, compounding health disparities by providing less accurate predictions for underrepresented groups.\nThe consequences extend to evaluation. Benchmark variants inherit the ascertainment biases of their source databases. Strong performance on benchmark sets may not translate to the rare genes, unusual variant types, or underrepresented populations encountered in real clinical practice. Variants that are “easy” to classify (stop-gains in highly constrained genes) are overrepresented in benchmarks, while diagnostically challenging variants (missense variants in moderate-constraint genes, non-coding variants) are underrepresented. The benchmark tells us how well the method performs on the easy cases; it may reveal little about the hard cases where computational assistance is most needed.\nAscertainment bias in sequencing creates blind spots in difficult genomic regions (Section 1.6). The circularity between training labels and evaluation benchmarks affects both classical and modern methods, as examined systematically in Section 22.5. Benchmark construction strategies that mitigate these issues appear in Section 20.5\n\n\n4.5.3 Implications for Clinical Use\nThese limitations do not render classical scores useless, but they counsel appropriate humility in interpretation. Computational predictions provide one line of evidence among several in variant interpretation. Strong scores in expected directions support clinical suspicion; unexpected scores prompt careful review. No computational score should override clear clinical or functional evidence, and borderline scores in complex cases may warrant agnosticism rather than confident prediction.\nThe ACMG-AMP framework for variant classification appropriately treats computational predictions as supporting evidence (PP3 for predictions supporting pathogenicity, BP4 for predictions supporting benign status) rather than standalone criteria (Richards et al. 2015). Multiple lines of computational evidence may be combined, but the weight assigned should reflect the limitations outlined here. Variants classified primarily on computational grounds should be flagged for potential reclassification as additional evidence emerges. The ACMG-AMP framework’s integration of computational evidence is detailed in Section 26.2 for complete clinical workflows. Calibration of computational scores to ACMG evidence strength levels is examined in Section 14.5.3 for foundation model approaches.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-feature-limitations",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-feature-limitations",
    "title": "4  Classical Variant Prediction",
    "section": "4.6 Limitations of the Feature Engineering Paradigm",
    "text": "4.6 Limitations of the Feature Engineering Paradigm\nClassical variant effect prediction achieved substantial success in prioritizing potentially pathogenic variants and established conceptual foundations that persist in modern methods. The integration of diverse annotations, use of evolutionary signals as proxy labels, and genome-wide precomputation all anticipate contemporary practices. Yet a fundamental tension remains: manually designed features encode only what biologists already know, and the complexity of genotype-phenotype relationships exceeds what explicit feature engineering can capture. The features are not wrong; they are incomplete in ways that cannot be remedied by adding more of the same.\n\n4.6.1 Feature Ceiling\nFeature-engineered methods encode human knowledge about which genomic properties matter for variant function. Conservation scores capture evolutionary constraint; protein-level predictors encode structural intuitions; regulatory annotations mark biochemically active regions. This encoded knowledge is valuable but necessarily incomplete. Biologists cannot specify all relevant patterns in advance, and the interactions between features may be too complex for simple combination rules to capture.\nThe performance of feature-engineered methods is therefore bounded by the quality and completeness of the features themselves. Adding more features provides diminishing returns as the most informative signals are exhausted. Interactions between features (a variant in a conserved enhancer within a constrained gene) may require explicit specification or rely on simple combination rules that miss nonlinear relationships. The linear SVM at the heart of CADD cannot represent the complex feature interactions that characterize biological regulation.\n\n\n4.6.2 Limited Context\nClassical features typically describe variants in isolation or with minimal context. Conservation scores examine each position independently. Protein-level predictors consider amino acid substitutions without full protein context. Gene-level features apply uniformly across entire genes regardless of position-specific effects. This limited context prevents classical methods from learning the complex sequence patterns that determine variant effects.\nA variant disrupting a critical transcription factor binding motif may escape detection if the motif is not annotated, even though the underlying sequence pattern is learnable from data. A missense variant at a protein-protein interface may be more damaging than one in a loop region, but capturing this distinction requires understanding protein structure and interactions that feature engineering incompletely represents. The local sequence context surrounding a variant often matters, but which contexts matter and how requires learning from data rather than specification by experts.\n\n\n4.6.3 Persistent Gap Between Measurement and Need\nEach classical method measures something related to but distinct from clinical pathogenicity. Conservation scores measure evolutionary constraint. Protein-level predictors measure functional disruption. CADD measures evolutionary tolerance. Each provides genuine biological signal, but none directly answers the clinical question: will this variant cause disease in this patient?\nThis gap is not merely a limitation of specific methods but reflects something deeper about the variant interpretation problem. The clinically relevant question depends on context (which tissue, which genetic background, which environmental exposures) that no current method captures. Even perfect prediction of functional disruption would not resolve questions of penetrance, expressivity, and disease mechanism. Classical methods provide important evidence for variant interpretation, but they cannot substitute for the integrative clinical reasoning examined in Chapter 25.\n\n\n4.6.4 From Features to Representations\nThe transition from CADD to deep learning methods represents a fundamental shift in how variant effect prediction is approached: from encoding biological knowledge as hand-crafted features to learning representations directly from sequence data. Where SIFT computes conservation from explicit multiple sequence alignments, ESM-2 learns similar signals implicitly through masked language modeling on protein sequences. Where PolyPhen-2 engineers features from solved protein structures, AlphaFold2 learns geometric constraints from evolutionary covariation (Chapter 12). The tokenization strategies and embedding approaches that enable this representation learning are detailed in Section 5.6. The shift is real and consequential.\nYet learned representations do not automatically overcome the limitations that constrain classical methods. The circularity between training labels and evaluation benchmarks affects transformer-based predictors exactly as it affects logistic regression. A model trained on ClinVar pathogenic variants inherits ClinVar’s ascertainment biases whether it uses 100 features or 100 million parameters. Rare variants remain difficult because they are rare in training data. Novel mechanisms remain invisible because no labeled examples exist. The variant effect prediction problem is fundamentally difficult, and methodology alone cannot resolve difficulties rooted in data availability and biological complexity. The zero-shot scoring paradigm (Section 14.1.1) offers a partial escape from label circularity by deriving variant scores from unsupervised pretraining objectives rather than explicit pathogenicity annotations, though this approach introduces its own assumptions about what pretraining captures.\nClassical methods remain valuable: as baselines that establish the performance floor modern methods must exceed, as interpretable components when understanding matters as much as prediction, and as reminders of what the field has learned about which signals carry predictive information. Foundation models have advanced variant effect prediction (Chapter 14), but honest evaluation requires understanding what classical methods achieved and where all approaches, classical and modern alike, continue to struggle. The evaluation methodology required to fairly assess both classical and learned approaches is detailed in Chapter 21, with genomics-specific metric considerations in Section 21.5.\n\n\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky, Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev. 2010. “A Method and Server for Predicting Damaging Missense Mutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper, Arend Sidow, and Serafim Batzoglou. 2010. “Identifying a High Fraction of the Human Genome to Be Under Selective Constraint Using GERP++.” PLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nHenikoff, S, and J G Henikoff. 1992. “Amino Acid Substitution Matrices from Protein Blocks.” Proceedings of the National Academy of Sciences 89 (22): 10915–19. https://doi.org/10.1073/pnas.89.22.10915.\n\n\nIoannidis, Nilah M., Joseph H. Rothstein, Vikas Pejaver, Sumit Middha, Shannon K. McDonnell, Saurabh Baheti, Anthony Musolf, et al. 2016. “REVEL: An Ensemble Method for Predicting the Pathogenicity of Rare Missense Variants.” The American Journal of Human Genetics 99 (4): 877–85. https://doi.org/10.1016/j.ajhg.2016.08.016.\n\n\nJagadeesh, Karthik A., Aaron M. Wenger, Mark J. Berger, Harendra Guturu, Peter D. Stenson, David N. Cooper, Jonathan A. Bernstein, and Gill Bejerano. 2016. “M-CAP Eliminates a Majority of Variants of Uncertain Significance in Clinical Exomes at High Sensitivity.” Nature Genetics 48 (12): 1581–86. https://doi.org/10.1038/ng.3703.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory M. Cooper, and Jay Shendure. 2014. “A General Framework for Estimating the Relative Pathogenicity of Human Genetic Variants.” Nature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT: Predicting Amino Acid Changes That Affect Protein Function.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie Gastier-Foster, Wayne W. Grody, et al. 2015. “Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology.” Genetics in Medicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs, Minmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005. “[PhastCons] Evolutionarily Conserved Elements in Vertebrate, Insect, Worm, and Yeast Genomes.” Genome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_2/p2--principles.html",
    "href": "part_2/p2--principles.html",
    "title": "Part II: Sequence Architectures",
    "section": "",
    "text": "Every neural network architecture encodes assumptions about biology. Convolutional networks assume that local patterns matter and that the same motifs are meaningful regardless of genomic position. Attention mechanisms assume that distant positions can interact directly without passing information through intermediate representations. Pretraining objectives assume that certain patterns in unlabeled sequence provide useful supervision in the absence of functional labels. These assumptions, embedded in architectural choices made before any training begins, determine which biological phenomena the model can capture and which remain invisible to it.\nArchitectural choices made before training begins constrain everything a model can learn. Tokenization choices (5  Tokens and Embeddings) propagate through model design, from one-hot encoding through byte-pair encoding to biologically informed vocabularies. Convolutional neural networks (6  Convolutional Networks) first demonstrated that deep learning could outperform handcrafted features for regulatory genomics by learning sequence-to-function mappings directly from data. Self-attention mechanisms and transformer architecture (7  Transformers and Attention) enable both local pattern recognition and long-range dependency modeling across genomic sequences.\nSelf-supervised objectives shape what models learn from unlabeled sequence (8  Pretraining Strategies). Masked language modeling, next-token prediction, and denoising approaches each encourage models to discover different biological patterns and produce representations with distinct properties. Adapting pretrained models to downstream tasks (9  Transfer and Adaptation) through fine-tuning, few-shot learning, and deployment strategies completes the path from raw sequence to useful prediction.",
    "crumbs": [
      "Part II: Sequence Architectures"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html",
    "href": "part_2/p2-ch05-representations.html",
    "title": "5  Tokens and Embeddings",
    "section": "",
    "text": "5.1 One-Hot Encoding: The CNN Foundation\nBefore a genomic model learns any parameters, before it sees any training data, before architecture choices are made, a prior decision has already constrained what it can discover: how will sequence be represented? This decision is not merely technical. A tokenization scheme that merges nucleotides into coarse multi-base units may obscure the single-nucleotide resolution needed to detect pathogenic splice variants. An embedding strategy that encodes local context may lose the long-range dependencies that connect distal enhancers to their target genes. A position encoding that assumes fixed sequence length may fail on the variable-length inputs that clinical applications require. These choices propagate through every subsequent design decision, shaping what patterns the model can detect, what resolution it can achieve, and ultimately which biological questions it can answer.\nThe challenge is that biological structure operates at multiple scales simultaneously, and no single representation captures all scales equally well. Transcription factor binding sites span 6 to 12 nucleotides; the regulatory grammar linking multiple sites extends over hundreds of base pairs. Coding sequences follow a strict three-nucleotide codon structure; noncoding regions have no such constraint. A splice acceptor consists of just two nucleotides (the AG dinucleotide marking exon boundaries), yet splice regulation depends on sequences spanning the entire intron. Any representation scheme must navigate these biological realities while remaining computationally tractable for sequences that dwarf typical language model inputs by orders of magnitude.\nAn analogy to natural language processing illuminates the fundamental tradeoffs. Training a language model on English text requires deciding how to segment the continuous character stream into discrete tokens. Character-level tokenization preserves maximum resolution but creates sequences too long for efficient processing. Word-level tokenization compresses the sequence but loses information about morphology and subword structure. Learned subword vocabularies (byte-pair encoding, SentencePiece) balance these concerns by letting corpus statistics guide segmentation. DNA presents similar choices but with critical differences: only four letters rather than dozens, no natural word boundaries, and the biological structure operating at multiple scales that language lacks. The representation strategies that genomic foundation models employ range from fixed \\(k\\)-mer vocabularies through learned tokenization schemes, each choice shaping what downstream models can learn.\nA child inherits a DMD variant from her mother. Whether this variant causes Duchenne muscular dystrophy or remains clinically silent depends on its exact position relative to the exon-intron boundary: one nucleotide can determine whether the splicing machinery recognizes the junction. This is why single-nucleotide resolution is not a technical nicety but a clinical necessity. The earliest deep learning approaches to genomic sequence modeling recognized this requirement and adopted the simplest representation capable of preserving it: one-hot encoding, where each nucleotide becomes a sparse binary vector with a single active element indicating its identity. Adenine is encoded as \\([1, 0, 0, 0]\\), cytosine as \\([0, 1, 0, 0]\\), guanine as \\([0, 0, 1, 0]\\), and thymine as \\([0, 0, 0, 1]\\). A sequence of length \\(L\\) thus becomes a matrix of dimensions \\(4 \\times L\\), interpretable as four channels analogous to the RGB channels of an image plus one.\nThe properties that made one-hot encoding dominant in the convolutional neural network (CNN) era stem from this simple design. The representation is lossless, preserving every nucleotide explicitly without information compression. It maintains single-nucleotide resolution, enabling detection of effects from individual SNPs. The encoding exhibits translation equivariance, meaning convolutional filters learn position-invariant motifs recognizable anywhere in the sequence. And it requires no preprocessing, vocabulary construction, or tokenizer training, making implementation straightforward. DeepSEA, ExPecto, and SpliceAI all employed one-hot encoding without modification, with convolutional layers learning to detect sequence patterns directly from the binary representation. These convolutional architectures and their learned pattern detectors are examined in Chapter 6.\nThe key insight underlying CNN success with one-hot encoding is that convolutions process sequences through local operations. Each filter examines only a small window of positions at a time, and the sparse, orthogonal nature of one-hot vectors poses no obstacle to this local processing. First-layer filters effectively learn position weight matrices that score short \\(k\\)-mer patterns, while deeper layers capture combinations and spatial arrangements of these primitive motifs. The representation worked because it aligned with the architectural inductive bias of convolutions: local pattern detection does not require global sequence compression.\nFor transformer architectures, one-hot encoding creates a fundamental mismatch. Transformers compute attention between all pairs of positions, with computational cost scaling quadratically with sequence length. A 10 kb sequence requires 100 million pairwise attention computations per layer, quickly becoming prohibitive for the long sequences genomic applications require. The problem compounds because transformers learn dense embeddings for each token, but with only four possible nucleotides, the embedding layer has minimal opportunity for rich representation learning.\nThis mismatch forces an impossible choice between the long contexts needed for regulatory modeling and computational tractability. Transformer context windows of 512 to 4,096 tokens translate to only 512 to 4,096 base pairs when using one-hot encoding, a tiny fraction of genes or regulatory regions. Compare this to Enformer’s 200 kb receptive field or SpliceAI’s 10 kb context, both achieved through architectural innovations operating on one-hot encoded sequence (Chapter 6). Sub-quadratic architectures like HyenaDNA resolve this tension through a different approach: maintaining single-nucleotide tokenization while replacing attention with operations that scale more gently with sequence length (Section 5.4). For standard transformer architectures, however, the quadratic barrier motivated the search for alternative representations that compress genomic sequences into fewer tokens while preserving biological information.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-kmer",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-kmer",
    "title": "5  Tokens and Embeddings",
    "section": "5.2 K-mer Tokenization: The DNABERT Approach",
    "text": "5.2 K-mer Tokenization: The DNABERT Approach\nThe computational constraints of one-hot encoding for transformers led researchers to explore sequence compression through \\(k\\)-mer tokenization. This approach treats overlapping subsequences of length \\(k\\) as tokens, drawing an analogy between k-mers and words in natural language. Just as sentences compose words carrying meaning through sequence and combination, genomic sequences might be understood as \\(k\\)-mer “words” encoding biological function through their arrangement. DNABERT pioneered this approach for genomic transformers in 2021, using 6-mers as tokens and training a BERT-style masked language model on human reference sequences (Ji et al. 2021).\nThe \\(k\\)-mer vocabulary has a fixed size of \\(4^k\\) possible tokens. For 6-mers, this yields 4,096 distinct tokens, comparable to vocabulary sizes in some natural language models. Each token represents six consecutive nucleotides, creating direct correspondence between subsequence and token identity. DNABERT used overlapping k-mers: for a sequence like ACGTACGT, successive 6-mer tokens share five nucleotides with their neighbors. The sequence position advances by one nucleotide at a time, generating one token per position (minus the \\(k\\)-1 positions at the sequence end where a complete \\(k\\)-mer cannot form).\nDNABERT provided valuable proof of concept for genomic transformers. It demonstrated that self-supervised pretraining on raw DNA sequences could improve performance over training from scratch, that learned embeddings could capture biologically meaningful regularities even when trained only on the reference genome, and that BERT-style architectures could transfer across multiple downstream tasks. DNABERT achieved strong performance on promoter prediction, splice site identification, and transcription factor binding site recognition after fine-tuning with relatively small amounts of task-specific labeled data. The model’s architecture and subsequent developments are examined in Chapter 11, while the transfer learning approaches that enable adaptation to specific tasks are treated in Chapter 9.\nSubsequent analysis revealed fundamental limitations rooted in the overlapping design. DNABERT-2 articulated these problems clearly in 2024 (Zhou et al. 2024). Overlapping k-mers provide no sequence compression: the number of tokens equals the number of nucleotides (minus a small constant), so context window limitations persist unchanged. A 10 kb sequence still requires approximately 10,000 tokens, and the quadratic attention complexity remains prohibitive for long sequences. The very design that seemed to add biological meaning through \\(k\\)-mer structure failed to address the computational bottleneck motivating the approach.\nThe overlapping design creates additional complications beyond computational cost. A single nucleotide contributes to \\(k\\) different tokens (each \\(k\\)-mer containing that position), complicating interpretation of which token drives any given prediction. This ambiguity becomes particularly problematic for variant effect interpretation, where understanding how a specific nucleotide change alters model predictions is essential. The effect of a single substitution propagates through \\(k\\) different tokens in ways that can be difficult to disentangle. The model must also learn that overlapping tokens share nucleotides, a relationship obvious from the tokenization scheme but requiring discovery through training. This redundancy consumes model capacity that could otherwise capture more complex biological patterns. The fixed 4^\\(k\\) vocabulary does not adapt to corpus statistics; frequent and rare k-mers receive equal representation capacity in the embedding table despite potentially differing importance for prediction.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-bpe",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-bpe",
    "title": "5  Tokens and Embeddings",
    "section": "5.3 Byte Pair Encoding: Learning the Vocabulary",
    "text": "5.3 Byte Pair Encoding: Learning the Vocabulary\nThe limitations of \\(k\\)-mer tokenization raise a question: what if the vocabulary itself could be learned from data? Byte Pair Encoding (BPE) addresses this by constructing vocabulary through iterative discovery of frequent subsequences rather than defining tokens through a fixed rule. The algorithm, originally developed for data compression, builds vocabulary through a simple procedure. BPE initializes the vocabulary with single nucleotides: {A, C, G, T}. It then scans the training corpus to count all adjacent token pairs, identifies the most frequent pair, merges this pair into a new token added to the vocabulary, and replaces all instances in the corpus with the merged token. The process repeats through many iterations (typically thousands), building a vocabulary of variable-length tokens capturing frequently occurring sequence patterns. [Citation Needed]\nThe critical difference from \\(k\\)-mer tokenization is that BPE produces genuine sequence compression through non-overlapping tokens. Unlike overlapping k-mers where each nucleotide generates its own token, BPE creates tokens spanning multiple nucleotides without overlap. A 10 kb sequence might compress to 2,000 or 3,000 tokens depending on its repetitive structure, enabling transformers to process substantially longer sequences within the same context window.\nDNABERT-2 replaced 6-mer tokenization with BPE and demonstrated dramatic improvements (Zhou et al. 2024). The new model achieved comparable performance to state-of-the-art approaches while using 21 times fewer parameters and requiring approximately 92 times less graphics processing unit (GPU) time in pretraining. The Nucleotide Transformer (Chapter 11) similarly employs BPE tokenization, as do protein language models that must handle amino acid sequences with different compositional properties (Chapter 12). These efficiency gains stem directly from non-overlapping tokenization: actual sequence compression enables processing longer sequences with the same computational budget, and eliminating overlapping token redundancy allows the model to focus capacity on learning biological patterns rather than token relationships.\nThe BPE vocabulary learns corpus statistics through its construction process. Repetitive elements appearing frequently throughout the genome (such as Alu sequences or common regulatory motifs) receive dedicated tokens spanning many nucleotides. These long tokens enable efficient representation of repetitive regions while preserving single-nucleotide resolution for unique sequences. Rare sequences that BPE never encountered during vocabulary construction are represented as concatenations of shorter subunits, maintaining the ability to encode any sequence while allocating more representation capacity to common patterns.\nGROVER (Genome Rules Obtained Via Extracted Representations) extended this approach by training BPE specifically on the human genome and selecting vocabulary using a custom next-\\(k\\)-mer prediction task (Sanabria et al. 2024). Analysis of the resulting token embeddings revealed that the learned vocabulary encodes biologically meaningful structure without explicit supervision. Common tokens cluster separately from rare ones in embedding space. GC-rich tokens segregate from AT-rich tokens, reflecting the different properties of these sequence compositions. Token length correlates with specific embedding dimensions, allowing the model to represent both the content and extent of each token. Some tokens appear primarily in repetitive regions while others distribute broadly across the genome, and this localization pattern emerges in the learned representations.\nBPE introduces complications of its own that matter for clinical applications. Variable-length tokens mean that variant positions fall at different locations relative to token boundaries depending on local sequence context. A SNP might fall in the middle of a long token in one sequence context but at a token boundary in another, potentially affecting how the model represents and processes the variant. The same nucleotide change may alter different numbers of tokens depending on surrounding sequence, creating inconsistent input representations for what should be comparable biological events. The tradeoff between compression and interpretability becomes a design choice depending on intended application.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-single-nucleotide",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-single-nucleotide",
    "title": "5  Tokens and Embeddings",
    "section": "5.4 Single-Nucleotide Tokenization: Maximum Resolution",
    "text": "5.4 Single-Nucleotide Tokenization: Maximum Resolution\nWhile \\(k\\)-mer and BPE tokenization compress sequences to enable longer context windows, they sacrifice the single-nucleotide resolution essential for variant effect prediction. A single nucleotide polymorphism (SNP) can completely alter protein function through mechanisms ranging from amino acid substitution to splice site disruption to regulatory element ablation. When a pathogenic variant and a benign variant differ by one nucleotide position, multi-nucleotide tokens obscure exactly where variants fall and how they relate to the boundaries of biological features.\nHyenaDNA took the opposite approach in 2023, using single-nucleotide tokens with no compression whatsoever (Nguyen et al. 2023). Each nucleotide (A, C, G, T) becomes a separate token, maintaining maximum possible resolution. Every nucleotide is independently represented, SNP effects can be isolated to specific token positions without ambiguity, and no tokenization artifacts depend on surrounding sequence context.\nThe challenge is sequence length. A 1 Mb region requires 1 million tokens, far beyond standard transformer capacity. HyenaDNA addressed this through architectural innovation rather than tokenization compromise. The Hyena architecture replaces the attention mechanism with implicit convolutions (long convolutions parameterized by a small neural network) that scale sub-quadratically with sequence length. Where attention computes explicit pairwise interactions between all positions, Hyena achieves similar representational power through operations whose cost grows only slightly faster than linearly. This enables processing sequences hundreds of times longer than attention-based transformers within the same computational budget. The architectural principles underlying Hyena and related sub-quadratic approaches are examined in detail in Chapter 7.\nThe practical impact was substantial: a 500-fold increase in context length over dense attention models while maintaining single-nucleotide resolution. HyenaDNA could process 1 Mb sequences where DNABERT was limited to approximately 500 bp and the Nucleotide Transformer to approximately 6 kb. On the Nucleotide Transformer benchmarks, HyenaDNA reached state-of-the-art performance on 12 of 18 datasets with orders of magnitude fewer parameters and less pretraining data. On GenomicBenchmarks, it surpassed prior state-of-the-art on 7 of 8 datasets by an average of 10 accuracy points.\nHyenaDNA also demonstrated the first use of in-context learning in genomics. The model could perform tasks based on examples provided in the context window without any fine-tuning (conditioning on demonstration sequences rather than updating parameters). This capability, familiar from large language models, had not previously been shown for genomic sequences and suggests that very long context combined with high resolution enables qualitatively new forms of biological reasoning. See section Section 9.9.3 for more on in-context learning.\nThe development of sub-quadratic architectures including Hyena, Mamba, and state space models has fundamentally changed the tokenization calculus [Citations Needed]. When computational constraints no longer force a choice between resolution and context length, single-nucleotide tokenization becomes the natural choice for applications requiring precise variant interpretation. The architectural innovations examined in Chapter 7 effectively decouple the resolution decision from the context length decision, eliminating what had seemed like an inherent tradeoff. HyenaDNA and Caduceus, examined in Chapter 11, demonstrate how these architectures enable million-base contexts at single-nucleotide resolution.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-biological-tokenization",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-biological-tokenization",
    "title": "5  Tokens and Embeddings",
    "section": "5.5 Biologically-Informed Tokenization",
    "text": "5.5 Biologically-Informed Tokenization\nStandard tokenization schemes treat DNA as a homogeneous string of characters, ignoring the biological reality that different genomic regions serve fundamentally different functions and follow different structural rules. Coding sequences obey a strict codon structure where every three nucleotides encode an amino acid; noncoding regions have no such constraint. Treating these regions identically wastes an opportunity to build biological knowledge directly into the representation.\nFor protein-coding regions, the natural unit of sequence is the codon rather than the individual nucleotide. GenSLMs pioneered codon-level tokenization for genomic foundation models in 2022, treating each three-nucleotide codon as a single token and exploiting the fact that codons are the biologically meaningful units of protein-coding sequence (Zvyagin et al. 2022). The 64-codon vocabulary captures the complete space of possible genetic code words, with each token corresponding to either an amino acid or a stop signal. This alignment with translation semantics means that mutations affecting amino acid identity (nonsynonymous changes) alter the token sequence, while synonymous mutations within a codon alter the specific token used but maintain broader codon-family structure.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 5.2: [Enhancing] Comparison of standard vs. biologically-informed tokenization on a gene structure diagram. Show: (1) Standard BPE tokenizing across codon boundaries in coding regions; (2) Codon-aware tokenization (GenSLMs/Life-Code) respecting reading frame with 64-codon vocabulary; (3) BioToken-style representation with explicit variant tokens, regulatory element markers, and structural annotations. Highlight how biological structure can be encoded directly into tokenization.\n\n\n\nLife-Code extended codon-aware tokenization to broader genomic contexts in 2025, encoding coding and noncoding regions in a way that respects reading frame and local biological function (Liu et al. 2025). Coding regions are tokenized by codons, aligning token boundaries with the fundamental unit of protein translation. Noncoding regions, lacking codon structure, are tokenized by learned patterns capturing regulatory motifs and other functional elements. This biologically-informed design enables Life-Code to learn protein structure through knowledge distillation from protein language models, capture interactions between coding and noncoding regions within a unified framework, and achieve state-of-the-art results across tasks involving DNA, RNA, and protein.\nBioToken extends tokenization further to include explicit genomic structural annotations (Medvedev et al. 2025). Rather than treating variants as implicit changes in the sequence string, BioToken creates tokens explicitly representing SNPs, insertions, and deletions. Known regulatory elements receive dedicated tokens encoding their presence and type. Gene structure, chromatin state, and other functional annotations integrate directly into the token representation. This approach treats tokens as rich entities bundling nucleotides with positional, functional, or experimental context.\nVariant-aware representations hold particular promise for clinical applications, where the input is often “reference plus variant” rather than a generic sequence. By incorporating biological inductive biases directly into tokenization, BioToken’s associated model achieves competitive or superior performance to specialized models like Enformer and SpliceAI with significantly fewer parameters. This efficiency suggests that appropriate representation can partially substitute for model scale by making the learning problem easier through informed structure.\nThe broader principle is that tokenization can and should incorporate biological structure when that structure is known and relevant. BPE learns statistical patterns from the corpus, but those patterns need not correspond to biological units. Codon tokenization imposes biological semantics directly, at the cost of applicability to noncoding regions. Future approaches might combine these strategies: codon-aware tokenization for coding regions, BPE or single-nucleotide tokens for noncoding sequence, and explicit variant tokens for clinical interpretation tasks.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 5.3: [High] UMAP or t-SNE visualization of \\(k\\)-mer or BPE token embeddings from a trained DNA language model (e.g., DNABERT-2 or GROVER). Color points by: Panel A: GC content (gradient from AT-rich to GC-rich). Panel B: Token frequency (common vs. rare). Panel C: Genomic context (coding, regulatory, repetitive). Show how biologically meaningful structure emerges without explicit supervision.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-embeddings",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-embeddings",
    "title": "5  Tokens and Embeddings",
    "section": "5.6 From Tokens to Embeddings: Learning Representations",
    "text": "5.6 From Tokens to Embeddings: Learning Representations\nA patient’s genome contains a variant of uncertain significance (VUS) in SCN5A, a cardiac ion channel gene. Whether this variant affects protein function depends on subtle sequence features that determine how the protein folds, where it localizes, and how it interacts with other cellular components. The clinical question is binary (pathogenic or benign), but the biological answer emerges from continuous biophysical properties. Classical methods for variant interpretation (Chapter 4) capture some of these relationships through hand-crafted features; learned embeddings offer an alternative approach where relevant patterns emerge from data. This gap between discrete genetic variation and continuous biological effect is precisely what embedding layers must bridge: transforming discrete tokens into dense numerical representations that neural networks can process and from which they can learn.\nThe operation itself is simple: a lookup table assigns each token to a learned vector. The embedding layer maintains a matrix \\(E\\) of dimensions \\(V \\times d\\), where \\(V\\) is vocabulary size and d is embedding dimension. Each token maps to a row of this matrix, and during training, backpropagation adjusts the embedding vectors to support downstream prediction. This simplicity belies its importance; the distinction between discrete tokens and their dense representations shapes what models can learn.\nConsider the difference between one-hot encoding and learned embeddings. A one-hot representation treats each nucleotide as maximally distinct from every other: the dot product between any two different nucleotides is zero, providing no information about their relationships. Adenine and thymine are equally different from each other as adenine and guanine, despite the biological reality that purines (A, G) share structural properties distinct from pyrimidines (C, T), and that complementary base pairs (A-T, G-C) have special significance for DNA structure and function.\nLearned embeddings allow the model to discover such relationships from data. If distinguishing purines from pyrimidines helps the model predict regulatory function, the embedding space will organize to reflect this distinction. If complementary relationships matter, they will emerge in the geometry of the learned space.\nThe embedding dimension \\(d\\) controls representational capacity. Small embeddings of 32 to 64 dimensions suffice for simple tokenization schemes like single nucleotides, where only four vectors must be distinguished. Larger vocabularies require larger embeddings: DNABERT-2’s BPE tokens use 768-dimensional embeddings, comparable to natural language models. The choice involves a tradeoff between expressiveness and efficiency, as larger embeddings increase both model capacity and computational cost. [Citation Needed]\nAnalysis of trained DNA language models reveals that embedding spaces organize around biologically meaningful properties even without explicit supervision. GC content, often considered a nuisance variable in genomics, emerges as a major axis of variation in embedding space because it correlates with many functional properties including gene density, chromatin accessibility, and mutation rate. Repetitive elements cluster together in embedding space. Coding sequence embeddings differ systematically from noncoding embeddings, even when the tokenization scheme makes no explicit distinction between these region types. [Citation Needed]\nThis emergent organization has practical implications. The structure learned in the embedding layer propagates through all subsequent computations. If embeddings fail to capture relevant distinctions, later layers must learn them from scratch. If embeddings encode spurious correlations, the model may exploit them inappropriately. Understanding what embeddings learn, and whether that learning aligns with biological reality, becomes an important diagnostic for model behavior. Systematic probing of these learned representations (Section 9.3) reveals what patterns models have captured, while interpretability methods (Chapter 24) trace how these representations influence downstream predictions.\nThe relationship between tokenization and embedding deserves emphasis. Coarse tokenization through large k-mers or aggressive BPE creates more token types, each with room for rich embedding representations but requiring the model to learn more parameters. Fine tokenization through single nucleotides creates fewer token types with simpler embeddings but forces the model to build complex representations through composition across layers. Neither approach is uniformly superior; the optimal choice depends on available training data, model scale, and task requirements.\n\n5.6.1 Position in Sequence\nTokenization converts sequence to discrete symbols, but genomic function depends on where those symbols appear. A transcription factor binding site has entirely different effects depending on whether it sits in a promoter, an enhancer, or a gene body. The same variant at position -30 relative to a transcription start site carries different implications than at position +500. Transformers are inherently permutation-invariant: shuffling token order changes nothing about how attention weights are computed. Position must be explicitly encoded.\nPositional encodings address this by injecting location information into token representations. Strategies range from learned embeddings (which assign a trainable vector to each position) to mathematical schemes like sinusoidal encodings or rotary position embeddings that can extrapolate to sequence lengths beyond training. The choice of positional encoding determines what spatial relationships a model can learn and how well it generalizes across genomic scales. Detailed treatment of positional encoding strategies appears in Section 7.2, where they are examined alongside the attention mechanisms they enable.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-biological-special",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-biological-special",
    "title": "5  Tokens and Embeddings",
    "section": "5.7 Special Considerations for Biological Sequences",
    "text": "5.7 Special Considerations for Biological Sequences\nThe double-stranded nature of DNA creates an ambiguity that has no parallel in natural language: should a model treat the forward and reverse complement strands as the same sequence, different sequences, or related-but-distinct entities? A transcription factor binding site for p53 functions when bound to either strand, yet the gene it regulates is transcribed from only one. This strand ambiguity ripples through every aspect of model design, from data augmentation to architectural constraints to output interpretation.\nA sequence ACGT on the forward strand corresponds to ACGT read 5’ to 3’, but also implies the reverse complement TGCA on the opposite strand read in the opposite direction. Some biological features are strand-specific: a gene on the forward strand is transcribed from that strand only. Other features are strand-agnostic: many transcription factor binding sites function identically on either strand. Representation schemes must decide whether to treat strands as equivalent through data augmentation with reverse complements, as distinct through explicit strand encoding, or as related-but-different through equivariant architectures processing both strands jointly.\nThe Nucleotide Transformer addressed strand by including both orientations during training, using data augmentation to ensure the model sees sequences from both directions (Dalla-Torre et al. 2023). Caduceus introduced a more elegant solution in 2024: a bidirectional architecture processing forward and reverse complement strands simultaneously through shared computation (Schiff et al. 2024). The model outputs are equivariant to reverse complementation (reversing and complementing the input produces correspondingly transformed outputs). This inductive bias ensures consistent treatment of strand without requiring augmentation or doubling computational cost.\nCircular genomes present another topological consideration. Bacterial chromosomes and plasmids, mitochondrial DNA, and many viral genomes are circular, with no natural start or end position. Linear position encodings impose arbitrary boundaries on these sequences. Some models address this through circular position encodings that wrap around at sequence boundaries, while others process circular genomes as linear sequences with the understanding that boundary effects may introduce artifacts. [Citation Needed]\nGenomic coordinates carry information absent from raw sequence. The position chr17:41,276,045 refers to a specific location in the BRCA1 gene, and variants at this position have been extensively studied. Knowing the genomic coordinate enables lookup of prior knowledge: population frequencies from gnomAD, clinical interpretations from ClinVar, functional annotations from ENCODE. Some representation schemes incorporate coordinate information explicitly, enabling models to learn position-specific patterns and integrate with external databases. Others deliberately exclude coordinates to force models to learn purely from sequence, trading prior knowledge for generalization to novel sequences or other species.\nMultiple sequence inputs arise frequently in genomic applications. Variant effect prediction requires comparing reference and alternate alleles. Comparative genomics involves aligned sequences from multiple species. Some regulatory predictions require input from multiple genomic regions, such as promoter plus enhancer. Representation schemes must accommodate these multi-sequence inputs through concatenation, paired encoding, or specialized architectures processing multiple sequences jointly.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-tradeoffs",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-tradeoffs",
    "title": "5  Tokens and Embeddings",
    "section": "5.8 Tradeoffs and Practical Guidance",
    "text": "5.8 Tradeoffs and Practical Guidance\nThe choice between tokenization strategies involves multiple competing considerations depending on the intended application. Understanding these tradeoffs enables informed design decisions rather than arbitrary choices.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 5.4: [High] Two-axis plot with “Sequence Compression” on x-axis (tokens per kilobase) and “Nucleotide Resolution” on y-axis. Position different approaches: One-hot/single-nucleotide (no compression, full resolution), overlapping k-mers (no compression, \\(k\\)-nucleotide resolution), non-overlapping k-mers (\\(k\\)-fold compression, \\(k\\)-nucleotide resolution), BPE (variable compression, variable resolution). Annotate practical context lengths achievable with standard transformers (~4K tokens) for each approach. Include callout showing clinical implication: “Single SNP affects…” with number of tokens per approach.\n\n\n\n\n5.8.1 Resolution Versus Compression\nThe tension between compression and resolution represents the fundamental tradeoff. Higher compression enables longer context windows within fixed computational budgets but loses precision for identifying exactly where variants fall and how they relate to biological features. One-hot encoding and single-nucleotide tokenization provide no compression but maintain full resolution. Non-overlapping k-mers achieve approximately \\(k\\)-fold compression at the cost of \\(k\\)-nucleotide resolution. BPE provides variable compression depending on sequence repetitiveness, with correspondingly variable resolution. For variant effect prediction (Chapter 14), where single nucleotide changes can have dramatic phenotypic consequences, resolution is paramount and the computational costs of long single-nucleotide sequences are often justified.\n\n\n5.8.2 Vocabulary Size and Model Capacity\nVocabulary size affects both model capacity and efficiency in ways that interact with embedding design. Larger vocabularies require bigger embedding tables but may capture more complex patterns directly in the token representation. Smaller vocabularies are parameter-efficient but require the model to learn compositional structure through multiple layers. One-hot encoding’s vocabulary of four tokens (plus special tokens) minimizes embedding parameters but maximizes the compositional learning burden. K-mer vocabularies scale exponentially with \\(k\\), reaching 4,096 for 6-mers. BPE vocabularies are tunable, typically ranging from 4,096 to 32,000 tokens for genomic applications. [Citation Needed]\n\n\n5.8.3 Computational Efficiency\nComputational efficiency depends on both tokenization and architecture in ways that have shifted as new architectures have emerged. For standard attention with \\(O(L^2)\\) complexity, any compression directly reduces cost: non-overlapping k-mers reduce attention cost by a factor of \\(k^2\\), and BPE with average compression \\(c\\) reduces cost by \\(c^2\\). Sub-quadratic architectures like Hyena and Mamba change this calculus entirely, making single-nucleotide tokenization computationally feasible at long contexts and eliminating the need to trade resolution for efficiency (Chapter 7).\n\n\n5.8.4 Variant Interpretation Requirements\nVariant interpretation has specific requirements favoring certain representation choices. Single-nucleotide tokens enable clean comparison of reference and alternate alleles at the same token position with no ambiguity about effect localization. K-mer tokens complicate matters because a single SNP changes \\(k\\) overlapping tokens, requiring aggregation across affected tokens and introducing potential boundary effects. BPE tokens create context-dependent effects where the same variant may fall at different positions relative to token boundaries depending on surrounding sequence. Foundation model approaches to variant effect prediction (Chapter 14) must navigate these tokenization constraints when scoring single-nucleotide changes.\n\n\n5.8.5 Practical Heuristics\nSeveral heuristics have emerged from practical experience. Single-nucleotide tokens work best when variant-level reasoning or high-resolution interpretability is central to the application. K-mers or BPE provide advantages when context length is the primary bottleneck and tasks do not require base-level precision. Biologically-informed tokens merit consideration when integrating multi-modal or annotation-rich data. Position encoding should match task requirements: relative encodings for tasks where absolute position is arbitrary, coordinate-aware encodings for clinical applications requiring integration with external databases (Section 7.2).",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-foundation",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-foundation",
    "title": "5  Tokens and Embeddings",
    "section": "5.9 Representation as Foundation",
    "text": "5.9 Representation as Foundation\nThese choices propagate through every subsequent modeling decision. Position encodings in transformers must align with token boundaries. Convolutional receptive fields span tokens, not nucleotides, making effective genomic range dependent on tokenization (Chapter 6). Transfer learning inherits the tokenization of the pretrained model, constraining how representations can be adapted to new tasks (Chapter 9). A model pretrained with 6-mer tokenization cannot be fine-tuned for single-nucleotide variant interpretation without architectural modification.\nThe field has moved from treating tokenization as fixed preprocessing to recognizing it as a fundamental design decision shaping what models can learn. Some architectures now learn tokenization jointly with prediction, discovering representations optimized for specific tasks rather than fixed in advance. As contexts extend to chromosome scale and models grow to billions of parameters, the representation problem will remain central to genomic foundation model design.\n\n\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.\n\n\nZvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. “GenSLMs: Genome-Scale Language Models Reveal SARS-CoV-2 Evolutionary Dynamics.” bioRxiv. https://doi.org/10.1101/2022.10.10.511571.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html",
    "href": "part_2/p2-ch06-cnn.html",
    "title": "6  Convolutional Networks",
    "section": "",
    "text": "6.1 Convolutions as Sequence Pattern Detectors\nIn 2015, a convolutional neural network trained on ENCODE chromatin data learned to recognize transcription factor binding motifs that matched entries in the JASPAR database, despite never seeing those motifs during training (Zhou and Troyanskaya 2015). The network had discovered, through gradient descent on raw sequence, patterns that experimental biologists had spent decades cataloging. This was not merely a demonstration that deep learning could match human-curated databases. The deeper insight was that learned representations could transcend existing annotations: predicting regulatory effects for any sequence, in any genomic context, including regions never assayed in any experiment. For the first time, computational methods could move beyond annotating known regulatory elements to predicting the functional consequences of sequence variation genome-wide.\nThe early convolutional models established paradigms that persist in modern genomic AI. DeepSEA demonstrated that CNNs could predict chromatin marks and transcription factor binding directly from DNA sequence, enabling variant effect prediction without requiring experimental measurements for every variant of interest. Basset extended this approach to chromatin accessibility across cell types, learning representations that transferred to new cellular contexts. SpliceAI achieved clinical-grade accuracy for splice site prediction, demonstrating that deep learning could match or exceed hand-crafted algorithms developed over decades. Each model followed a common pattern: train on functional genomics data, learn sequence features through convolutional filters, and apply to variant interpretation. The success was substantial; the paradigm seemed complete.\nYet these models revealed a fundamental architectural limitation. Convolutional networks integrate information only within their receptive fields, the local region of input that contributes to each output position. Genomic regulation routinely operates across distances that exceed practical receptive field sizes: enhancers control genes across tens of kilobases, topologically associating domains span megabases, GWAS variants often lie far from the genes they affect. A model analyzing a variant 50 kilobases from a gene promoter cannot connect the variant to its target using local convolutions alone. Understanding both what CNNs achieved and where they reached this architectural ceiling establishes the foundation for the attention mechanisms examined in Chapter 7.\nA variant in an enhancer 50 kilobases from its target gene cannot be connected to that gene by a model that sees only 1,000 base pairs of context. Consider a patient with familial hypercholesterolemia whose whole-genome sequencing reveals a novel variant upstream of LDLR. The variant sits within a known enhancer region, but the enhancer and the LDLR promoter lie beyond the window any convolutional layer can span. The model might correctly identify regulatory features at the variant position, but it cannot learn that those features regulate LDLR rather than some other gene. This receptive field constraint, inherent to convolutional architectures, determines what relationships these networks can and cannot discover. The constraint is not a limitation of training data or compute; it is architectural.\nA convolutional filter slides across an input sequence, computing similarity scores at each position. For genomic applications, the input is typically one-hot encoded DNA: a binary matrix with four rows (A, C, G, T) and columns for each position (see Chapter 5 for detailed treatment of sequence encoding strategies). Filters learn weight patterns that respond to specific nucleotide arrangements. A filter of width 8 nucleotides, for instance, computes a weighted sum of the underlying nucleotides at each position, producing high activation when the sequence matches its learned pattern and low activation otherwise. This operation is mathematically equivalent to scanning a position weight matrix across the sequence, but with a crucial difference: the filter weights are learned during training rather than derived from aligned binding site sequences.\nThe first layer of a genomic CNN typically contains hundreds of such filters, each learning to detect different local patterns. Analysis of trained filters consistently reveals correspondence to known transcription factor binding motifs. The CTCF insulator motif, the ETS family consensus sequence, the AP-1 binding site: these patterns emerge from training on chromatin data without any explicit motif supervision. The network identifies them because they predict the training labels. Supervision on chromatin state induces discovery of the sequence patterns that create chromatin state, providing unsupervised motif learning as a byproduct of supervised prediction.\nDeeper layers operate on the output of earlier layers rather than raw sequence. A second-layer filter might learn to detect specific arrangements of first-layer motifs: two ETS sites within 20 base pairs, or a CTCF motif flanked by particular spacing patterns. This hierarchical feature learning enables CNNs to capture regulatory grammar beyond individual motifs, including spacing constraints, orientation preferences, and combinatorial requirements that govern transcription factor cooperativity.\nBetween convolutional layers, spatial resolution must decrease while the receptive field expands. Pooling operations achieve this tradeoff: max pooling selects the strongest activation within a window, achieving position-invariant detection where the network responds to a motif’s presence somewhere in a region rather than its exact position. This property suits regulatory genomics, where binding site positions within an enhancer often matter less than their presence and combination.\nThe receptive field of a convolutional network defines how much input sequence can influence a single output prediction. For a network with kernel width \\(k\\), pooling factor \\(p\\), and \\(L\\) layers, the receptive field grows with depth but remains fundamentally limited by architecture. A three-layer network with typical parameters might integrate information from 200 to 1,000 base pairs. Reaching further requires either more layers (increasing computational cost and training difficulty) or dilated convolutions that space filter weights to sample larger regions. When biological dependencies span tens of kilobases, this receptive field ceiling becomes the fundamental constraint that no amount of training data can overcome.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-convolutions",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-convolutions",
    "title": "6  Convolutional Networks",
    "section": "",
    "text": "FIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 6.1: [Essential] Three-panel figure showing convolution mechanics. Panel A: Single convolutional filter (width 8) sliding across one-hot encoded DNA, producing activation scores at each position; show high activation where filter matches a motif. Panel B: Learned filter weights visualized as sequence logo (PWM-style), aligned to corresponding JASPAR motif showing the biological pattern discovered. Panel C: Multiple filters from first convolutional layer, each detecting different motifs (CTCF, ETS, AP-1), showing diverse pattern detection.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-deepsea",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-deepsea",
    "title": "6  Convolutional Networks",
    "section": "6.2 DeepSEA: Regulatory Prediction from Sequence",
    "text": "6.2 DeepSEA: Regulatory Prediction from Sequence\nA patient presents with a rare disease phenotype, and whole-genome sequencing reveals a novel variant in an intron 15 kilobases from the nearest exon. The variant does not disrupt any annotated regulatory element. No prior patient in any database carries this exact change. The clinician must decide: is this variant pathogenic, or is it an irrelevant passenger? Annotation-based methods offer no guidance. The variant overlaps nothing cataloged, so overlap-based interpretation returns nothing useful. Yet introns harbor splice regulatory elements, and 15 kilobases places the variant well within range of enhancers that might control the adjacent gene.\nExisting approaches to noncoding variant interpretation relied on this overlap paradigm. If a variant fell within a ChIP-seq peak or DNase hypersensitive site, it might be flagged as potentially regulatory. The strategy grounded predictions in experimental observations, but it could not predict whether a variant would strengthen or weaken regulatory activity, could not score variants in regions lacking experimental coverage, and provided no mechanism for quantifying effect magnitude. A variant might fall within an enhancer, but would it matter? The data indicated where regulatory elements existed; they did not indicate how sequence changes would affect them.\nDeepSEA, introduced by Zhou and Troyanskaya, reframed the problem: rather than asking whether a variant overlaps known annotations, ask what regulatory activities a sequence encodes and how mutations would alter them (Zhou and Troyanskaya 2015). The shift from annotation lookup to sequence-based prediction enabled scoring any variant in any genomic context, including regions never assayed in any experiment. This reframing would prove more consequential than any specific architectural choice.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 6.2: [High] Three-panel figure. Panel A: Architecture schematic showing input (1000bp one-hot), three convolutional layers with pooling (320→480→960 filters), fully connected layer, and 919 sigmoid outputs for chromatin features. Panel B: First-layer filter aligned to JASPAR motif (e.g., CTCF), demonstrating learned = known biology. Panel C: Scatter plot of predicted vs. observed allelic imbalance for DNase-seq, showing correlation that validates variant effect prediction.\n\n\n\n\n6.2.1 Architecture and Training\nThe clinical scenario described above demands a model that can predict function from sequence alone. DeepSEA’s architecture was deliberately simple by contemporary standards, placing the emphasis on the learning framework rather than architectural complexity.\nInput sequences of 1,000 base pairs, one-hot encoded, passed through three convolutional layers with 320, 480, and 960 filters respectively. Max pooling after each convolution compressed spatial dimensions. A fully connected layer with 925 units integrated information across the compressed representation, and a final output layer with 919 sigmoid units produced independent probability predictions for each chromatin profile.\nTraining data came from ENCODE and Roadmap Epigenomics (see Chapter 2 for comprehensive treatment of these resources): 690 transcription factor binding profiles, 104 histone modification profiles, and 125 DNase I hypersensitivity profiles spanning diverse cell types (Kagda et al. 2025; Kundaje et al. 2015). For each 1,000 bp input, the model predicted whether the central 200 bp region exhibited each chromatin feature. Chromosome 8 was held out for evaluation.\nThe multi-task learning formulation proved essential for generalization. Predicting 919 features simultaneously forced the network to learn shared representations useful across many prediction problems. The first convolutional layer learns general sequence patterns (GC content, common dinucleotides, ubiquitous motifs); these representations then feed task-specific combinations in later layers. Joint training provides implicit regularization, preventing overfitting to any single task while amortizing the cost of learning basic sequence features across all outputs.\n\n\n6.2.2 Learned Representations and Biological Validation\nAny sequence model faces a fundamental question: do learned features correspond to biological reality, or do they exploit statistical shortcuts that happen to correlate with labels? DeepSEA provided the first large-scale evidence that deep learning could recover genuine regulatory logic.\nAnalysis of first-layer filters revealed learned patterns matching known transcription factor motifs. The network had independently recovered sequence preferences cataloged in JASPAR and TRANSFAC, confirming that the training objective (predicting chromatin state) induced biologically meaningful feature extraction. This interpretability distinguished deep learning from prior black-box approaches and suggested that the models captured genuine regulatory logic rather than spurious correlations. Systematic methods for extracting and visualizing these learned representations, from filter analysis to attribution mapping, are examined in Chapter 24.\nDeeper layers combined first-layer patterns into more complex representations, capturing motif spacing requirements, orientation preferences, and cooperative binding arrangements. The network encoded relationships between sequence features that position weight matrices, operating independently at each motif, could not represent.\nDeepSEA outperformed gkm-SVM (gapped \\(k\\)-mer support vector machines) on nearly all transcription factor binding prediction tasks [Citation Needed]. The pattern of improvement revealed something fundamental: gkm-SVM showed no benefit from longer input sequences, while DeepSEA performance improved substantially with additional context. K-mer methods tally motif occurrences but cannot learn relationships between patterns at different positions. Hierarchical feature learning enables exactly what \\(k\\)-mer methods cannot provide: representations of combinatorial regulatory logic.\n\n\n6.2.3 Variant Effect Prediction\nWith a trained sequence-to-chromatin model, variant scoring becomes straightforward: predict chromatin profiles for reference and alternative sequences, compute the difference. This in silico mutagenesis produces a 919-dimensional vector describing predicted changes across all features. The model never encounters variant data during training; effect prediction emerges from learned sequence-function relationships applied to mutations the model has never seen.\nValidation used allelic imbalance data from digital genomic footprinting. For variants showing allele-specific DNase I sensitivity, DeepSEA predictions correlated with experimentally observed biases: variants predicted to increase accessibility tended to show higher accessibility on the corresponding allele. This correlation would not exist if the model merely learned coarse sequence features insensitive to point mutations.\nSystematic characterization of regulatory elements requires more than single-variant scoring. In silico saturation mutagenesis predicts effects of all possible substitutions across a regulatory element, identifying positions where mutations most strongly perturb function. These critical positions typically correspond to transcription factor binding motifs, providing motif discovery that emerges from learned representations rather than explicit sequence alignment. The approach enables characterization of any regulatory element, including those in cell types or conditions never experimentally profiled. Foundation models extend these principles to longer contexts and richer representations (Chapter 14), while clinical integration requires calibration approaches that map model outputs to actionable categories (Chapter 26).",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-basset",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-basset",
    "title": "6  Convolutional Networks",
    "section": "6.3 Cell-Type Specificity and Regulatory Grammar",
    "text": "6.3 Cell-Type Specificity and Regulatory Grammar\nA variant that disrupts cardiac-specific gene regulation may be lethal in the heart but entirely silent in neurons. A regulatory element active during embryonic development may be permanently silenced in adult tissues. Clinical variant interpretation therefore requires models that capture not just what sequence patterns predict regulatory activity, but how those predictions vary across the dozens of cell types and developmental stages where a variant might act. DeepSEA’s 919 chromatin features spanned multiple cell types, but the question remained: could architectural modifications better capture cell-type-specific programs or learn richer representations of the combinatorial grammar governing transcription factor cooperativity?\nBasset, introduced by Kelley et al. in 2016, focused specifically on predicting chromatin accessibility from sequence (Kelley, Snoek, and Rinn 2016). Rather than DeepSEA’s diverse chromatin features, Basset predicted DNase-seq peaks across 164 cell types, enabling detailed analysis of cell-type-specific regulatory activity. The architectural refinements Basset introduced would influence subsequent models: batch normalization after convolutional layers stabilized training and enabled deeper networks, while larger filters in early layers (19 nucleotides in the first layer) captured longer motifs directly rather than requiring the network to compose them from smaller patterns.\nThe key contribution was demonstrating that in silico saturation mutagenesis profiles from trained models could identify causal variants underlying disease-associated haplotypes. GWAS identifies associated regions but cannot distinguish the causal variant from nearby variants in linkage disequilibrium (see Chapter 3 for the statistical foundations of association studies). Basset’s saturation mutagenesis provided a principled approach: the variant with the strongest predicted regulatory effect within an associated haplotype is the most likely causal candidate. Foundation models like Enformer (Chapter 13) extend this principle to longer contexts that capture more distal regulatory influences on GWAS signals. This moved beyond simple peak overlap toward mechanistic variant prioritization, and subsequent validation studies confirmed that model-prioritized variants showed higher rates of experimental confirmation than lead SNPs selected purely by association strength [Citation Needed].\nDanQ explored whether regulatory grammar involves sequential dependencies that convolutions alone might miss, combining convolutional layers with bidirectional LSTMs to integrate motif detections across the input window (Quang and Xie 2016). The hybrid architecture achieved modest improvements on chromatin prediction benchmarks, though the recurrent components introduced costs examined in Section 6.7.\nThese variations illustrated a broader principle: multiple architectures could learn useful regulatory representations from sequence. The specific choices (filter sizes, layer depths, recurrent components) mattered less than the fundamental framework of learning from one-hot encoded sequence to predict chromatin labels. This robustness suggested that the underlying signal, sequence determinants of regulatory activity, was strong enough to be captured by diverse architectural approaches. For clinical applications, prediction quality depends more on training data quality and task definition than on architectural details within the CNN family.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-expecto",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-expecto",
    "title": "6  Convolutional Networks",
    "section": "6.4 ExPecto: From Chromatin to Expression",
    "text": "6.4 ExPecto: From Chromatin to Expression\nA patient’s tumor harbors a somatic variant in a putative enhancer region. Chromatin profiling in matching tissue shows the region is accessible. The variant is predicted to disrupt a transcription factor binding site. Yet the clinician’s question remains unanswered: does this variant actually change expression of a target gene? Which gene? By how much? In which tissues?\nChromatin accessibility and transcription factor binding are intermediate phenotypes, means rather than ends. The ultimate functional readout for most regulatory variants is their effect on gene expression. A variant might disrupt a binding site, but sites can be redundant, effects can be buffered, and the relationship between binding and expression is not one-to-one. Predicting expression change from sequence requires integrating regulatory signals across distances that determine which enhancers control which promoters. A variant that disrupts binding but does not alter expression is unlikely to be pathogenic, while a variant with modest chromatin effects but strong expression consequences may drive disease.\nExPecto, introduced by Zhou et al. in 2018, addressed these questions by extending sequence-to-chromatin prediction toward tissue-specific gene expression (Zhou et al. 2018). The framework predicts expression levels across 218 tissues and cell types by integrating predicted chromatin signals across a 40 kb promoter-proximal window. This context expansion, from DeepSEA’s 1 kb to ExPecto’s 40 kb, represented a significant architectural commitment: expression prediction requires integrating regulatory signals from distances far exceeding typical motif sizes.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 6.3: [Enhancing] Three-component pipeline diagram. Component 1: Beluga CNN scanning 40kb window around TSS with sliding 2kb windows, producing chromatin predictions at 200 spatial positions. Component 2: Spatial transformation with exponential decay functions (upstream and downstream), reducing to ~20,000 features. Component 3: 218 tissue-specific linear regression models, producing per-tissue expression predictions. Show example delta scores for variant effect.\n\n\n\n\n6.4.1 Modular Architecture\nExPecto comprises three sequential components, each addressing a distinct computational challenge. The separation proved essential: jointly optimizing all components end-to-end would be computationally prohibitive, and the modular design enables interpretability at each stage.\nThe first component, an enhanced CNN called Beluga, predicts 2,002 chromatin profiles from 2,000 bp input sequences. Beluga incorporated architectural improvements over DeepSEA: six convolutional layers with residual connections, expanded chromatin targets, and broader cell-type coverage. This CNN scans the 40 kb region surrounding each transcription start site with a moving window, generating chromatin predictions at 200 spatial positions and producing over 400,000 features per gene.\nThe second component transforms these high-dimensional features through spatial aggregation. Ten exponential decay functions, applied separately to upstream and downstream regions, encode the prior belief that nearby elements contribute more than distant ones. This transformation reduces dimensionality while preserving spatial relationships, producing approximately 20,000 features per gene that capture both which chromatin features are predicted and where they occur relative to the TSS.\nThe final component comprises 218 L₂-regularized linear regression models, one per tissue, predicting log expression from spatially-transformed features. Linear models were chosen deliberately: they provide interpretability, prevent overfitting given the high-dimensional feature space, and enable coefficient analysis to identify which chromatin features drive expression in each tissue. The combination of a shared sequence-to-chromatin CNN with separate tissue-specific linear heads cleanly separates sequence-level regulatory grammar from tissue-specific regulatory programs.\n\n\n6.4.2 Expression Prediction and Variant Effects\nExPecto achieved 0.819 median Spearman correlation between predicted and observed expression across tissues. Analysis of model coefficients revealed automatic learning of cell-type-relevant features: the liver expression model weighted HepG2-derived transcription factor features most heavily; breast tissue models emphasized estrogen receptor features from breast cancer cell lines. These tissue-specific patterns emerged purely from learning to predict expression, without tissue identity information provided to the chromatin model.\nVariant effect prediction follows the same logic as DeepSEA: compare expression predictions for reference and alternative sequences. Because the model never trains on variant data, predictions are unconfounded by linkage disequilibrium, a critical distinction from association-based methods (see Chapter 22 for detailed treatment of confounding in genomic models). ExPecto correctly predicted expression change direction for 92% of the strongest GTEx eQTL variants, and experimental validation confirmed that model-prioritized variants (not the GWAS lead SNPs) showed allele-specific regulatory activity in reporter assays [Citation Needed].\nThe 40 kb window represents an empirically optimized trade-off. Smaller windows decreased performance; larger windows showed negligible improvement. Most promoter-proximal regulatory information lies within 40 kb of the TSS, at least within ExPecto’s linear modeling framework. Distal enhancers beyond this window, while biologically important, require architectural approaches that can model longer-range dependencies. This limitation points toward the transformer architectures and hybrid models examined in Chapter 13.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-spliceai",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-spliceai",
    "title": "6  Convolutional Networks",
    "section": "6.5 SpliceAI: Clinical-Grade Splicing Prediction",
    "text": "6.5 SpliceAI: Clinical-Grade Splicing Prediction\nA child presents with developmental delay and dysmorphic features consistent with a known genetic syndrome. Clinical exome sequencing reveals no pathogenic coding variants in the implicated gene. The case is signed out as “unsolved,” the family left without answers. Three years later, research RNA sequencing identifies aberrant splicing in the syndromic gene: an intronic variant 150 base pairs from the nearest exon creates a cryptic splice site, inserting a premature stop codon. The diagnosis was hiding in plain sight, invisible to methods that only examine canonical splice dinucleotides.\nThis scenario, replicated across thousands of unsolved rare disease cases, illustrates a systematic blind spot in clinical genomics. Splice-disrupting mutations represent a major mechanism of Mendelian disease (see Chapter 26 for broader treatment of rare disease diagnosis), yet variants affecting splicing outside canonical GT/AG dinucleotides are systematically underascertained. Prior splice prediction methods captured essential splice site motifs but could not model the long-range determinants contributing to splicing specificity. MaxEntScan operates on approximately 9 bp of context around donor and acceptor sites (Yeo and Burge 2004). The method established the paradigm of quantitative splice site scoring using maximum entropy distributions and remains a standard baseline for variant interpretation, but its narrow window fundamentally limits what biology it can capture. These methods produced many false positives and missed variants acting through distal mechanisms: branch points, exonic splicing enhancers, and intron length constraints that previous models could not see.\nSpliceAI, introduced by Jaganathan et al. in 2019, demonstrated that deep neural networks could learn sequence-intrinsic splicing rules sufficient to predict the majority of splice sites used by the spliceosome (Jaganathan et al. 2019). The model predicts splice site locations directly from pre-mRNA sequence using 10,000 nucleotides of context, an order of magnitude beyond prior methods. This context expansion enabled recognition of distant splicing determinants invisible to annotation-based approaches.\n\n6.5.1 Architecture: Depth and Dilation\nLearning splicing rules from 10 kb of sequence context requires an architecture that can integrate information across this entire span while maintaining nucleotide-level resolution. SpliceAI achieves this through two innovations: extreme depth enabled by residual connections, and dilated convolutions that expand receptive fields without proportional parameter growth.\nSpliceAI employs an ultra-deep residual network with 32 convolutional layers. Residual connections address the vanishing gradient problem that otherwise prevents training at this depth:\n\\[\n\\text{output} = \\text{input} + F(\\text{input})\n\\]\nBy learning residual functions rather than direct mappings, the network can propagate gradients through dozens of layers. Skip connections from every fourth residual block feed directly to the penultimate layer, further stabilizing training dynamics.\nDilated convolutions expand the receptive field efficiently. A dilated convolution with rate d samples input positions at intervals of d rather than consecutively. Stacking convolutions with increasing dilation rates (1, 2, 4, 8, 16, and so on) allows the network to integrate information across the full 10 kb window while maintaining sensitivity to local patterns. Standard convolutions with small kernels would require impractical depth to achieve equivalent receptive fields.\nFor each position in the pre-mRNA sequence, SpliceAI outputs three probabilities: splice acceptor, splice donor, or neither. This per-position classification enables fine-grained predictions across entire transcripts. Training used GENCODE annotations, with odd and even chromosomes split for training and testing.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 6.4: [High] Two-panel figure. Panel A: Diagram showing how dilated convolutions expand receptive field without proportional parameter growth. Show dilation rates (1, 2, 4, 8, 16…) with gaps between filter taps, illustrating how 32 layers with dilation reach 10kb context. Panel B: SpliceAI’s residual block structure with skip connections from every 4th block to output, enabling gradient flow through 32 layers.\n\n\n\n\n\n6.5.2 Performance and Validation\nSpliceAI achieved 95% top-\\(k\\) accuracy for splice site identification (compared to 57% for MaxEntScan) and 0.98 precision-recall area under the curve (auPRC). Complex genes exceeding 100 kb are often reconstructed to nucleotide precision. Performance improved dramatically with context length:\n\n\n\nModel Variant\nContext (each side)\nauPRC\n\n\n\n\nSpliceAI-80nt\n40 bp\n0.87\n\n\nSpliceAI-400nt\n200 bp\n0.93\n\n\nSpliceAI-2k\n1,000 bp\n0.96\n\n\nSpliceAI-10k\n5,000 bp\n0.98\n\n\n\nThis progression confirms that distal sequence features contribute meaningfully to splicing decisions. The diminishing returns above 2 kb suggest that most splicing determinants lie within this range, though the additional context still provides measurable benefit.\nThe delta score quantifies variant effects by comparing predictions for reference and alternative sequences:\n\\[\n\\Delta\\text{score} = \\max_{|p - v| \\leq 50} \\left| P_{\\text{alt}}(p) - P_{\\text{ref}}(p) \\right|\n\\]\nValidation against GTEx RNA-seq showed that mutations with higher delta scores showed higher validation rates at novel splice junctions: approximately \\(50\\%\\) at \\(\\Delta \\geq 0.2\\), \\(75\\%\\) at \\(\\Delta \\geq 0.5\\), and \\(85\\%\\) at \\(\\Delta \\geq 0.8\\). Population genetics provided orthogonal support: predicted cryptic splice variants showed \\(78\\%\\) depletion at common allele frequencies, nearly matching the depletion of frameshift and stop-gain variants. Natural selection treats these variants as deleterious, confirming their functional impact.\n\n\n6.5.3 Clinical Impact\nSpliceAI’s most significant contribution may be quantifying cryptic splice mutations as a major, previously underappreciated cause of rare genetic disorders. Analysis of de novo mutations in over \\(4{,}000\\) individuals with intellectual disability found significant enrichment of predicted splice-disrupting variants compared to unaffected controls (\\(1.51\\)-fold, \\(p = 4.2 \\times 10^{-4}\\)). Approximately \\(9\\%\\) of pathogenic de novo mutations in intellectual disability act through cryptic splicing [Citation Needed]. Including these variants in gene discovery analyses identified additional candidate genes that would have fallen below discovery thresholds when considering only protein-coding mutations.\nThis clinical utility explains SpliceAI’s rapid adoption. Illumina integrated SpliceAI into their annotation pipelines. Clinical genetics laboratories worldwide use delta scores to flag potential splice-affecting variants for RNA-seq follow-up. The model exemplifies how task-specific deep learning can achieve clinical-grade accuracy on well-defined problems. SpliceAI’s integration into modern variant interpretation workflows is examined in Chapter 14, and its role in rare disease diagnosis pipelines appears in Chapter 26.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-receptive-field",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-receptive-field",
    "title": "6  Convolutional Networks",
    "section": "6.6 Receptive Field Ceiling",
    "text": "6.6 Receptive Field Ceiling\nConsider a 45-year-old woman with early-onset breast cancer and a family history suggesting hereditary risk. Whole-genome sequencing identifies a novel variant 80 kilobases upstream of BRCA1, within an established enhancer region. The enhancer is known to regulate BRCA1 expression in mammary epithelium. Does this variant reduce BRCA1 expression enough to increase cancer risk? DeepSEA can predict whether the variant disrupts transcription factor binding at that position. SpliceAI confirms no splice effects. ExPecto’s 40 kb window cannot reach from the variant to the BRCA1 promoter. No convolutional model can connect the enhancer variant to its target gene because the distance exceeds their receptive fields. The clinical question remains unanswered.\nThis case illustrates a fundamental limitation rooted in architecture: convolutional networks can only integrate information within their receptive fields. DeepSEA’s three-layer architecture effectively considers roughly 1 kb of context. ExPecto’s Beluga component operates on 2 kb windows, aggregated across a 40 kb region by the spatial transformation layer. SpliceAI pushes to 10 kb through dilated convolutions and 32 layers. Each expansion required significant architectural engineering, and each reached a practical ceiling beyond which further expansion yielded diminishing returns or became computationally prohibitive.\nThe limitation matters because genomic regulation routinely operates across distances these models cannot reach. Enhancers regulate promoters 50 to 500 kilobases away. The beta-globin locus control region sits 40 to 60 kb from the genes it activates. Polycomb-mediated repression involves chromatin contacts spanning megabases. Topologically associating domains organize regulatory interactions across hundreds of kilobases (see Chapter 17 for detailed treatment of chromatin architecture). When regulatory elements and their targets lie beyond a model’s receptive field, the model cannot learn their relationship regardless of how much training data is available. The constraint is architectural, not statistical. Attention mechanisms (Chapter 7) provide the architectural solution, while hybrid models like Enformer (Chapter 13) combine convolutional motif detection with transformer-based long-range integration.\nThis creates a systematic mismatch between biological importance and computational accessibility. A variant within a distal enhancer may have profound effects on gene expression, but a model with a 10 kb receptive field cannot connect the enhancer sequence to its target promoter. The model might correctly predict that the enhancer sequence contains regulatory features, but it cannot predict which gene those features regulate or how strongly. We can predict local regulatory potential, yet we cannot predict long-range regulatory effects.\nThe architectural response to this challenge evolved through two stages. Recurrent networks initially seemed promising, carrying context through hidden states rather than expanding receptive fields. When recurrence proved insufficient, attention mechanisms provided the architectural solution that modern genomic models required.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 6.5: [Essential] Horizontal genome diagram comparing effective context windows across CNN architectures. Show: DeepSEA (~1 kb), ExPecto/Beluga (2 kb windows, 40 kb aggregated), SpliceAI (10 kb), and for contrast, Enformer (200 kb). Overlay biologically relevant distances: typical TF binding site (~10bp), promoter region (~1kb), enhancer-gene distance (10-100kb), TAD size (~1Mb). Highlight the gap: “Most enhancer-promoter interactions exceed CNN receptive fields.”",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-sequential",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-sequential",
    "title": "6  Convolutional Networks",
    "section": "6.7 Sequential Approaches and Their Costs",
    "text": "6.7 Sequential Approaches and Their Costs\nIf convolutional networks cannot reach far enough, why not simply carry information forward through the sequence? Recurrent neural networks offered an intuitive solution to the receptive field problem: maintain a hidden state that accumulates context as the network processes each position in turn. Where a convolutional filter sees only its local window, a **recurrent neural network (RNN)’s hidden state can, in principle, carry information from the beginning of a sequence to its end. For biological sequences, this seemed natural. DNA is read by polymerases in one direction; transcripts are processed sequentially by ribosomes; regulatory elements exert effects that propagate through chromatin. A computational architecture that mirrors this sequential logic appeared well-suited to genomic modeling.\nThe hidden state mechanism works as follows. At each position t, the network combines the current input \\(x_t\\) with the previous hidden state \\(h_{t-1}\\) to produce a new hidden state \\(h_t\\). This recurrence allows information from early positions to influence computations at later positions through the chain of hidden states. A regulatory element at position 1,000 can, in theory, affect predictions at position 50,000 because its influence persists in the hidden state across all intervening positions. No receptive field limits this reach; the constraint becomes whether information survives the journey.\n\n6.7.1 Vanishing Gradient Problem\nInformation rarely survives. Training RNNs requires backpropagating gradients through time, computing how errors at late positions depend on parameters applied at early positions. These gradients pass through the same recurrent weight matrix at each step. When gradients are multiplied through hundreds or thousands of steps, they either explode (growing exponentially) or vanish (shrinking toward zero). The vanishing gradient problem makes it nearly impossible for RNNs to learn dependencies spanning more than a few dozen positions. A regulatory element 10,000 base pairs upstream might as well not exist: by the time gradients propagate backward through 10,000 recurrent steps, they have decayed to numerical insignificance.\nGating mechanisms that control information flow resolved the vanishing gradient problem. Long Short-Term Memory (LSTM) networks achieve this through a separate cell state with learned gates that determine what information to store, what to forget, and what to output (Hochreiter and Schmidhuber 1997). The forget gate can preserve information indefinitely by setting its value near one, allowing gradients to flow through the cell state without repeated multiplication by small values. Gated Recurrent Units (GRUs) simplified this design by combining gates while retaining the core insight: learned gating prevents gradient decay (Cho et al. 2014).\nThese gated architectures extended effective memory from tens to hundreds of positions, sometimes thousands. For natural language, where most dependencies span fewer than 50 words, LSTMs proved transformative. For genomic sequences, where relevant context can span tens of kilobases (tens of thousands of nucleotides), even gated recurrence falls short. The mathematics of gradient propagation through recurrent connections imposes limits that no gating mechanism fully overcomes.\n\n\n6.7.2 DanQ: Combining Convolutions and Recurrence\nThe DanQ model represented the most influential attempt to apply recurrent architectures to regulatory genomics (Quang and Xie 2016). Rather than replacing convolutions entirely, DanQ combined them: convolutional layers first extracted local sequence motifs, then a bidirectional LSTM integrated these motif detections across the 1,000 base pair input window. The architecture recognized that convolutions excel at detecting local patterns while recurrence might capture their long-range relationships.\nDanQ processed sequences in both directions simultaneously (bidirectional recurrence), allowing each position to incorporate context from both upstream and downstream. Training on the same DeepSEA chromatin prediction task, DanQ achieved modest improvements over the purely convolutional baseline, with the LSTM component learning to weight motif combinations based on their relative positions and co-occurrence patterns.\nThe improvement was real but limited. Within a 1,000 base pair window, convolutional receptive fields already capture most relevant dependencies, leaving less room for recurrence to contribute. The fundamental problem remained: neither convolutions nor recurrence could reach the 50 to 100 kilobase distances where enhancers regulate their target genes. DanQ demonstrated that hybrid architectures could outperform pure convolutions, but the gains did not justify the added complexity for most applications. The model saw limited adoption compared to simpler convolutional alternatives.\n\n\n6.7.3 Sequential Bottleneck\nEven if recurrence could maintain gradients across genomic distances, a more fundamental constraint would remain. RNNs process sequences one position at a time. Each hidden state \\(h_t\\) depends on the previous hidden state \\(h_{t-1}\\), creating an inherently sequential computation that cannot be parallelized. Training on a 100,000 base pair sequence requires 100,000 sequential steps, each waiting for the previous step to complete. Modern GPUs achieve their speed through massive parallelism; sequential dependencies eliminate this advantage.\nThis computational bottleneck made RNNs impractical for the long contexts that genomic applications require. A transformer processes all positions simultaneously, computing attention scores in parallel across the entire sequence. For a 100 kilobase context, a transformer performs one parallel operation where an RNN would require 100,000 sequential steps. The difference in training time is not incremental; it is the difference between feasible and infeasible. When Enformer extended genomic modeling to 200 kilobase contexts (see Chapter 13), recurrent architectures were not considered. The sequential bottleneck had already disqualified them.\nThe attention mechanism resolved both limitations simultaneously. Self-attention computes direct interactions between all positions without sequential dependencies, enabling parallel processing across arbitrary context lengths. Attention weights are computed through matrix operations that GPUs execute efficiently, and gradients flow directly between any two positions without passing through intermediate states. The path from position 1 to position 100,000 involves a single attention computation rather than 100,000 recurrent steps. This architectural shift, examined in Chapter 7, enabled the long-range modeling that genomic applications demand.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-specialization",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-specialization",
    "title": "6  Convolutional Networks",
    "section": "6.8 Specialization and Its Limits",
    "text": "6.8 Specialization and Its Limits\nThe convolutional models examined here established paradigms that persist in modern genomic AI. End-to-end learning from one-hot encoded sequence demonstrated that gradient descent on functional labels could discover regulatory patterns without encoding human assumptions about what matters. Multi-task training across hundreds of chromatin features showed that shared representations improve both accuracy and generalization. In silico mutagenesis, comparing predictions for reference and alternative sequences, established the dominant approach for deep learning-based variant effect prediction: scoring variants without training on variant labels, thereby avoiding the ascertainment biases that confound association-based methods (see Chapter 22).\nThese principles carry forward into foundation model architectures. What CNNs could not resolve was the receptive field limitation. Genomic regulation operates across scales that exceed practical convolutional depth: enhancers modulating genes across hundreds of kilobases, topologically associating domains spanning megabases. Dilated convolutions and deeper networks extend reach but cannot fundamentally escape the constraint that convolutions aggregate local information through hierarchical composition.\nYet specialization retains value even as general-purpose models advance. SpliceAI achieves clinical-grade splice site prediction that broader foundation models have not matched. When the prediction target is well-defined, training data abundant, and the relevant context fits within architectural constraints, task-specific models remain competitive with or superior to general-purpose approaches. This tension between specialized accuracy and general capability defines architectural choices across genomic AI. For clinical deployment requiring high reliability on specific tasks, specialized architectures may remain preferred. For discovery applications requiring broad coverage across diverse molecular mechanisms, the foundation model paradigm (see Chapter 10) offers different trade-offs. Attention mechanisms provide the architectural substrate for long-range modeling while inheriting the end-to-end learning principles that convolutional networks established. The tension between specialized accuracy and general capability reappears throughout this book: for splice prediction and motif detection where CNNs excel, task-specific architectures remain competitive; for regulatory prediction requiring long-range integration, the foundation model approaches in Part III offer different tradeoffs (Chapter 10).\n\n\n\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. “On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.” arXiv. https://doi.org/10.48550/arXiv.1409.1259.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data Navigation on the ENCODE Portal.” Nature Communications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKelley, David R., Jasper Snoek, and John L. Rinn. 2016. “Basset: Learning the Regulatory Code of the Accessible Genome with Deep Convolutional Neural Networks.” Genome Research 26 (7): 990–99. https://doi.org/10.1101/gr.200535.115.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. “Integrative Analysis of 111 Reference Human Epigenomes.” Nature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nQuang, Daniel, and Xiaohui Xie. 2016. “DanQ: A Hybrid Convolutional and Recurrent Deep Neural Network for Quantifying the Function of DNA Sequences.” Nucleic Acids Research 44 (11): e107. https://doi.org/10.1093/nar/gkw226.\n\n\nYeo, Gene, and Christopher B. Burge. 2004. “Maximum Entropy Modeling of Short Sequence Motifs with Applications to RNA Splicing Signals.” Journal of Computational Biology 11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html",
    "href": "part_2/p2-ch07-attention.html",
    "title": "7  Transformers and Attention",
    "section": "",
    "text": "7.1 Self-Attention Mechanism\nWhere convolutional networks ask “what local pattern exists here,” attention asks a different question: “what distant information matters here?” This reformulation changed what genomic models could learn. The convolutional architectures examined in Chapter 6 detect motifs, chromatin marks, and regulatory grammar with high fidelity, but they remain blind to dependencies beyond their receptive field. Attention computes direct interactions between all positions simultaneously, allowing a nucleotide near a gene promoter to attend to an enhancer 100 kilobases away without information passing through intermediate layers. The shift is not merely architectural; it reflects a different assumption about how sequence encodes function. Local patterns matter, but so do long-range relationships that convolutions cannot capture.\nThe attention mechanism, introduced for machine translation by Vaswani et al., resolved a tension that had constrained sequence modeling for years (Vaswani et al. 2023). Recurrent networks could maintain context across arbitrary distances but processed sequences one position at a time, creating training bottlenecks that limited practical sequence length. Convolutional networks processed sequences in parallel but could only integrate information within fixed receptive fields. Attention achieves both: parallel computation across all positions with direct modeling of arbitrary-range dependencies. For genomic sequences, where enhancer-promoter interactions span tens of kilobases and topologically associating domains organize contacts across megabases, this capacity for long-range modeling proved transformative.\nAttention enables capabilities that convolutions cannot achieve. Regulatory interactions spanning distances beyond any practical convolutional neural network (CNN) receptive field become directly modelable when every position can attend to every other. Different attention heads specialize for different relationship types: some learn to focus on nearby positions for local motif context, others attend across tens of kilobases to capture enhancer-promoter relationships, and still others exhibit periodic patterns suggestive of nucleosome spacing or chromatin organization. These specializations emerge from training without explicit supervision. The transformer architecture that houses these attention mechanisms has become the dominant substrate for genomic foundation models, from DNABERT’s regulatory sequence representations to Enformer’s expression predictions to ESM-2’s protein embeddings. Yet attention mechanisms still struggle with the quadratic computational cost that limits context length, creating a tension between the long-range modeling that genomic applications require and the practical constraints of current hardware.\nA 28-year-old woman presents with dilated cardiomyopathy and a variant of uncertain significance in the LMNA gene’s promoter region. Her clinician needs to determine whether this variant disrupts regulatory elements that control LMNA expression in cardiac tissue. The relevant information spans thousands of base pairs: transcription factor binding sites flanking the variant, enhancers that drive cardiac-specific expression, and insulators that constrain regulatory interactions. A model that can only aggregate local context (through convolutional or recurrent operations) must pass information through many intermediate layers, each adding noise and limiting what survives the journey. When this information pathway fails, the variant appears as noise rather than the pathogenic regulatory disruption it may represent. The fundamental question is how to let any position in a sequence directly access information from any other position, regardless of distance.\nSelf-attention answers this question by computing all pairwise interactions simultaneously, allowing the model to directly relate any position to any other regardless of distance. Where convolutions apply fixed filters uniformly across the sequence, attention performs dynamic routing: each position queries the entire sequence and aggregates information based on content-dependent relevance scores. The routing changes for every input because attention weights depend on what the sequence contains, not just where positions sit relative to each other. For the LMNA variant, this means the model can directly assess whether the variant position interacts with known cardiac enhancers without that signal degrading through layer after layer of local aggregation.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-self-attention",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-self-attention",
    "title": "7  Transformers and Attention",
    "section": "",
    "text": "FIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER E\n\n\n\n\n\n\nFigure 7.1: [Essential] Step-by-step visualization of self-attention on a short regulatory sequence. Panel A: Input embeddings for ~10 positions. Panel B: Query, Key, Value projections (show W^Q, W^K, W^V matrices). Panel C: Attention score matrix (query-key dot products, pre-softmax). Panel D: Attention weight matrix (post-softmax, showing which positions attend to which). Panel E: Weighted sum of values producing output. Annotate the key equation at each step.\n\n\n\n\n\n7.1.1 Query, Key, and Value Vectors\nAt each position in the input sequence, self-attention computes three vectors: a query, a key, and a value. These vectors emerge from multiplying the input embedding at that position by three learned weight matrices \\(W^Q\\), \\(W^K\\), and \\(W^V\\). The query represents what information this position seeks from other positions. The key represents what information this position offers to queries from elsewhere. The value represents the actual information this position contributes when attended to. This query-key-value structure separates the question of “which positions should interact” (determined by query-key similarity) from “what information flows between them” (determined by values).\nThe attention mechanism computes similarity scores between each query and all keys. For position \\(i\\), we compute the dot product between its query \\(q_i\\) and every key \\(k_j\\) across all positions \\(j = 1, \\ldots, L\\), where \\(L\\) is sequence length. These scores are scaled by \\(\\sqrt{d_k}\\) (the square root of the key dimension) to prevent the dot products from growing large in high dimensions, which would push softmax outputs toward extreme values and create vanishing gradients:\n\\[\n\\text{score}(q_i, k_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\n\\]\nA softmax function converts these scores into attention weights \\(\\alpha_{ij}\\) that form a probability distribution over positions:\n\\[\n\\alpha_{ij} = \\frac{\\exp(\\text{score}(q_i, k_j))}{\\sum_{j'=1}^L \\exp(\\text{score}(q_i, k_{j'}))}\n\\]\nThese weights determine how strongly position \\(i\\) attends to each other position. High weight means position \\(i\\) aggregates substantial information from position \\(j\\); low weight means position \\(j\\) contributes little to the output at position \\(i\\). The final output at position \\(i\\) is a weighted sum of all value vectors:\n\\[\n\\text{output}_i = \\sum_{j=1}^L \\alpha_{ij} v_j\n\\]\nThis weighted aggregation forms the core of self-attention. Each output position receives a mixture of information from across the entire sequence, with mixture proportions learned through backpropagation. For genomic sequences, this means a position near a splice site can attend to both the upstream exon and downstream intron, integrating context that determines whether splicing occurs. A position in a promoter can attend to distant enhancers, learning which distal elements influence expression at this gene. When predicting the pathogenicity of a variant in the SCN5A promoter (mutations in which cause Brugada syndrome and long QT syndrome, affecting approximately 1 in 2,000 individuals), the model can simultaneously consider the core promoter elements, upstream enhancers that drive cardiac-specific expression, and downstream regulatory regions that modulate expression levels.\n\n\n7.1.2 Multi-Head Attention\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 7.2: [High] Panel showing 4-6 attention heads from a trained genomic transformer, each displaying different learned patterns. Head 1: Local attention (attending to nearby positions). Head 2: Periodic attention (nucleosome spacing ~200bp). Head 3: Motif-specific attention (attending to CTCF sites). Head 4: Long-range attention (enhancer-promoter). Annotate the biological interpretation of each pattern.\n\n\n\nA patient presenting with a complex arrhythmia may carry variants affecting both a cardiac ion channel’s coding sequence and its distal enhancer. Understanding this case requires the model to simultaneously track local splice site context around the coding variant and enhancer-promoter relationships spanning 50 kilobases. A single attention operation cannot capture both patterns effectively: when forced to learn one pattern of position interactions, the model faces an impossible choice between attending strongly to nearby positions for local regulatory context or attending to distant positions for enhancer-gene relationships. Genomic sequences exhibit multiple types of dependencies simultaneously, and forcing all these interaction types through a single attention pattern creates destructive competition.\nMulti-head attention extends the basic mechanism by running multiple attention operations in parallel, each with independent learned projections (Vaswani et al. 2023). If we use H heads, we split the model dimension d into H subspaces of dimension d/H, compute separate queries, keys, and values for each head, run attention independently, concatenate outputs, and project back to dimension d. Different heads can specialize in different interaction types without competing for attention capacity.\nIn genomic models, one head might attend to nearby positions (capturing local motif context) while another attends to positions at characteristic distances (capturing nucleosome spacing or enhancer-promoter loops). Empirical analysis of trained genomic transformers reveals diverse attention patterns: some heads attend locally regardless of content, others attend to specific sequence motifs like TATA boxes or CTCF binding sites, and still others show distance-dependent patterns suggestive of chromatin organization (Avsec et al. 2021). This specialization emerges from training without explicit supervision, reflecting the model’s discovery that different types of interactions require different aggregation patterns. Methods for visualizing and interpreting these learned attention patterns are examined in Chapter 24.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 7.3: [Essential] Heatmap visualization of attention weights from a trained genomic transformer (e.g., Enformer). Panel A: Attention pattern showing strong weights between a promoter region and a distal enhancer ~50kb away, demonstrating learned enhancer-promoter relationships. Panel B: Same attention overlaid on linear genome diagram showing the biological interpretation. Panel C: Different attention head showing local patterns (attending to nearby positions), demonstrating head specialization.\n\n\n\nThe multi-head structure also provides redundancy that aids training. If one head fails to learn useful patterns, others can compensate. Gradient flow through multiple parallel paths stabilizes optimization. For genomic applications where training data may be limited compared to natural language corpora, this redundancy helps prevent individual heads from overfitting to spurious correlations. The number of heads represents a design choice: too few heads limit the diversity of learnable patterns, while too many heads reduce the dimensionality available to each head, potentially limiting their individual expressiveness. Most genomic transformers use 8 to 16 heads, balancing diversity against per-head capacity.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-positional-encoding",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-positional-encoding",
    "title": "7  Transformers and Attention",
    "section": "7.2 Positional Encoding",
    "text": "7.2 Positional Encoding\nA patient with hypertrophic cardiomyopathy carries a variant in the MYH7 gene’s promoter region. Determining pathogenicity requires knowing precisely where the variant sits relative to the transcription start site: a variant at position -30 (where the TATA box resides) carries entirely different implications than the same sequence at position +500 (within the 5’ UTR). Position is not merely bookkeeping for genomic sequences; it encodes biological function. The canonical TATA box must appear 25 to 30 base pairs upstream of transcription initiation to function; the same sequence elsewhere carries no regulatory significance. Splice site recognition depends on the invariant GT and AG dinucleotides appearing at precise distances from exon boundaries. Enhancer-promoter interactions require specific distance relationships that vary by locus and cell type. A model that cannot distinguish position 100 from position 10,000 cannot learn the positional grammar that governs gene regulation.\nSelf-attention, by design, computes interactions based purely on content: the attention weight between positions depends only on their query and key vectors, not on where they sit in the sequence. The basic concepts of position encoding were introduced in Section 5.6.1; here we examine the specific implementations that transformer architectures employ. Shuffling input token order changes nothing about how attention weights are computed. The model has no inherent notion of sequence order, a property called permutation invariance. For genomic data where position matters fundamentally, this blindness to order would be catastrophic. DNA has 5’ to 3’ directionality that determines transcription direction. Distance from transcription start sites determines promoter versus enhancer classification. Strand orientation distinguishes sense from antisense transcription. Positional encodings inject information about token positions into the model, breaking permutation invariance by making the model aware of where each token sits in the sequence.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 7.4: [High] Four-panel comparison of position encoding approaches. Panel A (Absolute/Learned): Heatmap showing learned position embeddings across dimensions, with note about fixed maximum length. Panel B (Sinusoidal): Wave patterns at different frequencies across positions, showing how different dimensions capture different scales. Panel C (ALiBi): Attention bias matrix showing linear decay with distance, highlighting the implicit local preference. Panel D (RoPE): 2D rotation visualization in embedding subspace showing how relative position is encoded through rotation angle.\n\n\n\n\n7.2.1 Absolute Position Encodings\nThe original transformer used sinusoidal functions with different frequencies for each embedding dimension (Vaswani et al. 2023). For position \\(pos\\) and dimension \\(i\\):\n\\[\n\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_\\text{model}}}\\right)\n\\]\n\\[\n\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_\\text{model}}}\\right)\n\\]\nThese fixed patterns have useful properties. They are deterministic (the same for all sequences), allow the model to learn to attend by relative positions (since \\(\\mathrm{PE}(pos+k)\\) can be expressed as a linear function of \\(\\mathrm{PE}(pos)\\)), and generalize to sequence lengths not seen during training. The different frequencies across dimensions create a unique “fingerprint” for each position: lower-frequency components capture coarse position while higher-frequency components capture fine position.\nMany genomic models use learned positional embeddings instead: lookup tables where each position has a learned vector added to the input embedding. DNABERT and Nucleotide Transformer both employ learned positional embeddings, allowing the model to discover position-dependent patterns specific to genomic data (Ji et al. 2021; Dalla-Torre et al. 2023). The trade-off is that learned embeddings do not automatically extrapolate to longer sequences. A model trained with maximum sequence length of 512 tokens has no learned embedding for position 513, creating a hard boundary on sequence length at inference time. This fixed maximum context proves particularly restrictive for genomics, where biological phenomena span scales from individual binding sites to megabase regulatory domains.\n\n\n7.2.2 Relative Position Encodings\nAbsolute encodings treat position 1,000 and position 1,001 as having different representations even though their relative relationship (adjacent positions) may matter more than their absolute locations. For genomic applications, relative distance often carries more biological meaning than absolute coordinates: nucleosomes are spaced approximately 200 base pairs apart regardless of genomic location, and enhancer-promoter interactions depend on distance rather than absolute position. A transcription factor binding site 50 bases upstream of a transcription start site has similar effects whether the TSS sits at genomic position 1,000 or 1,000,000. Relative positional encodings address this by encoding distances between positions rather than absolute coordinates.\nT5-style relative position bias adds a learnable scalar to attention scores based on the distance between query and key positions (Raffel et al. 2023). This bias helps the model learn that nearby positions often interact more strongly than distant ones while remaining agnostic about absolute position. The learned biases can capture genomic-specific distance preferences, such as the characteristic spacing of regulatory elements or the periodicity of nucleosome positioning.\nExtrapolation to longer sequences than seen during training poses a persistent challenge for learned position embeddings. A model trained on 1-kilobase sequences may behave unpredictably when asked to process 10-kilobase sequences at inference, since the position embeddings for distant positions were never optimized. Attention with Linear Biases (ALiBi) addresses this limitation by adding a fixed linear penalty to attention scores based on distance, without learned parameters (Press, Smith, and Lewis 2022). For a head with slope \\(m\\), attention between positions separated by distance \\(|i - j|\\) is penalized by \\(m|i - j|\\). Different heads use different slopes, encouraging some to focus locally and others globally. Because the linear penalty extrapolates naturally, ALiBi generalizes well beyond training lengths, making it attractive for genomic applications where sequence length varies dramatically. The linear distance penalty may not perfectly capture biological relationships (where some regulatory interactions span consistent long distances while others operate locally), but the simplicity and extrapolation properties have proven valuable.\nAn alternative approach encodes position through geometric transformation rather than additive bias. Rotary Position Embeddings (RoPE) multiply query and key vectors by rotation matrices whose angles depend on position (Su et al. 2024). The dot product between rotated query and key then depends on their relative distance, combining benefits of relative encoding with efficient implementation. RoPE has become standard in recent large language models and appears increasingly in genomic transformers, including variants of Nucleotide Transformer, offering a balance between the flexibility of learned embeddings and the extrapolation capability of fixed schemes. The choice between ALiBi and RoPE often depends on whether the application prioritizes aggressive length extrapolation (favoring ALiBi) or compatibility with pretrained language model architectures (favoring RoPE).\n\n\n7.2.3 Genomic Position Considerations\nGenomic sequences impose requirements on positional encoding beyond what natural language demands. DNA has strand directionality: ACGT on the forward strand has different regulatory meaning than the same sequence on the reverse strand. Positional encodings should enable the model to learn strand-specific patterns. Some genomic transformers encode both strands separately and combine predictions; others rely on the model learning strand orientation from sequence content alone.\nGenomic coordinates pose another design choice. Should position 1 correspond to a fixed genomic landmark (transcription start site, gene start) or simply indicate sequence order without biological reference? Models predicting regulatory activity often center sequences on promoters, using positions relative to the TSS. Foundation models trained on random genomic segments typically use positional encodings reflecting sequence order without genomic coordinate reference. The choice affects what the model can learn: TSS-relative positions enable learning of distance-dependent regulatory patterns, while sequence-order positions require the model to learn these patterns implicitly from content.\nAbsolute genomic coordinates carry accumulated knowledge with no linguistic analog. The position chr17:41,276,045 indexes decades of clinical observations, population frequencies, and functional studies that inform variant interpretation before a model processes a single nucleotide. Some recent models have explored incorporating these absolute coordinates, allowing models to learn position-specific patterns like centromeric sequences or telomeric regions. Circular genomes like mitochondrial DNA and bacterial chromosomes create additional complexity: they have no beginning or end, creating wraparound relationships that linear position encodings cannot naturally represent. These adaptations illustrate that position encoding is not merely a technical detail but a design choice shaping what biological patterns a model can capture.\nThe choice of positional encoding interacts with tokenization strategies (see Chapter 5). K-mer tokenization reduces sequence length (and thus attention cost) but changes what “position” means: position 1 might represent nucleotides 1 through 6 rather than a single base. A model using 6-mer tokens with learned positional embeddings learns different position-dependent patterns than one using single-nucleotide tokens, even if both cover the same genomic region.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-transformer-block",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-transformer-block",
    "title": "7  Transformers and Attention",
    "section": "7.3 Transformer Block",
    "text": "7.3 Transformer Block\nA clinician interpreting a BRCA1 variant needs a model that does more than identify isolated motifs or single long-range interactions. The variant’s pathogenicity depends on how multiple regulatory signals integrate: local splice site grammar, enhancer contacts from 20 kilobases upstream, and transcription factor binding sites whose effects depend on chromatin context. Single attention layers identify pairwise relationships, but understanding complex regulatory logic requires building hierarchical representations where simple patterns combine into compound signals. This hierarchical integration emerges from stacking transformer blocks, the modular units that combine attention and nonlinear processing to build increasingly abstract representations through repeated application.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 7.5: [High] Detailed diagram of a single transformer block with pre-norm configuration. Show: Input → Layer Norm → Multi-Head Attention → Residual Add → Layer Norm → Feed-Forward Network (expand 4x, GELU, project back) → Residual Add → Output. Annotate dimension changes. Include small inset showing 2-3 stacked blocks to illustrate depth.\n\n\n\n\n7.3.1 Block Components\nEach transformer block accomplishes two distinct functions: enabling positions to share information across the sequence, and transforming that aggregated information through nonlinear processing. The multi-head self-attention layer handles global communication, allowing each position to gather information from the entire sequence. The position-wise feed-forward network processes each position independently, applying nonlinear transformations to the aggregated information. Separating these functions into distinct components allows each to be optimized independently and provides clear computational semantics: attention determines which positions are relevant to each other (the “what to consider” question), while the feed-forward network determines how to combine that information (the “what to conclude” question).\nThe feed-forward network consists of two linear transformations with a nonlinearity between them. Typically, this expands the dimension by a factor of four (from model dimension d to 4d), applies GELU or similar activation, then projects back to dimension d. This expansion allows processing through a high-dimensional nonlinear transformation before producing output for the next layer. The position-wise nature means each position is transformed identically but independently; cross-position information flows only through attention.\nDeep transformer networks would be untrainable without mechanisms to stabilize gradient flow. Layer normalization addresses activation scale by normalizing across the feature dimension at each position, preventing the explosive growth or collapse of activations that would otherwise occur across dozens of layers. Two conventions exist for placement: post-norm applies normalization after each sublayer, while pre-norm applies it before. Pre-norm has become standard because it improves training stability for deep networks, though post-norm can achieve slightly better final performance with careful tuning (Xiong et al. 2020).\nNormalization alone cannot solve the vanishing gradient problem that plagues deep networks. Residual connections provide the second essential stabilization mechanism by adding each sublayer’s input directly to its output, creating gradient highways that bypass the transformations within attention and feed-forward operations. These connections serve two critical functions. First, they allow gradients to flow directly through many layers without repeated transformation, enabling training of very deep networks. Second, they create an inductive bias toward incremental refinement: each layer makes small adjustments to the representation rather than constructing entirely new representations from scratch. For genomic models, this incremental refinement maps naturally onto biological interpretation, where early layers might identify motifs, middle layers might recognize motif combinations, and later layers might integrate these patterns into regulatory predictions.\n\n\n7.3.2 Information Flow and Depth\nThe flow through a pre-norm transformer block proceeds as follows. Input X is normalized, processed by multi-head attention to produce X’, and added back via residual connection, yielding X + X’. This sum is normalized, passed through the feed-forward network to produce X’’, and added via another residual connection, yielding final output X + X’ + X’’. Each layer thus adds refinements to the representation while preserving information from earlier processing.\nStacking depth determines how many times this refinement occurs. Shallow transformers (6 layers or fewer) are parameter-efficient but may lack capacity for complex hierarchical patterns. Deep transformers (12 to 24 layers) can learn sophisticated representations that capture how promoter elements, enhancer contacts, and chromatin state combine to determine expression. Most genomic transformers use 6 to 24 layers, varying by application. Models for short sequences (small RNAs, individual binding sites) might use fewer layers, while foundation models for long genomic contexts often use deeper stacks to build representations that integrate information across multiple biological scales.\nThe choice of depth balances capacity against trainability. Deeper networks learn more complex functions but are harder to optimize, prone to overfitting without sufficient data, and more expensive at training and inference. For genomic models, depth often correlates with the complexity of patterns being modeled. Simple motif recognition tasks might benefit more from wider layers (larger d) than deeper stacks, while tasks requiring hierarchical integration (understanding how promoter-enhancer-insulator relationships determine expression) may benefit from additional depth that builds increasingly abstract representations layer by layer.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-scaling",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-scaling",
    "title": "7  Transformers and Attention",
    "section": "7.4 Scaling to Genomic Sequences",
    "text": "7.4 Scaling to Genomic Sequences\nA 52-year-old patient presents with unexplained cardiomyopathy, and whole-genome sequencing reveals a structural variant spanning 500 kilobases on chromosome 14, disrupting the MYH7 locus and several upstream regulatory elements. The clinical team needs to assess whether this variant explains the patient’s phenotype. Standard transformers cannot help: the quadratic complexity of self-attention makes 500-kilobase contexts computationally intractable. This gap between clinical need and computational capability defines a central challenge for genomic AI. The attention mechanism enables long-range modeling in principle, but practical constraints on memory and computation limit what contexts can actually be processed. Effective application of transformers to genomics requires strategies for managing these constraints.\n\n7.4.1 Quadratic Barrier\nComputing all pairwise attention scores requires \\(O(L^2)\\) operations, where L is sequence length. For a 10-kilobase sequence tokenized at single-nucleotide resolution, this means 100 million attention computations per layer. A 200-kilobase sequence requires 40 billion computations per layer. Memory requirements scale similarly because the attention matrix must be stored for backpropagation.\nThis scaling constraint directly limits what clinical questions transformers can address. The HLA region (critical for transplant matching and autoimmune disease risk in the approximately \\(40{,}000\\) organ transplants performed annually in the United States) spans approximately \\(4\\) megabases and contains the most polymorphic genes in the human genome. Modeling this region with standard self-attention would require \\(16 \\times 10^{12}\\) attention computations per layer, far exceeding practical limits. Structural variant detection often requires analyzing megabase-scale contexts to identify breakpoints and assess functional impact, yet these contexts remain computationally intractable for standard transformers. A patient with a suspected chromosomal translocation cannot benefit from transformer-based analysis when the relevant context exceeds computational capacity.\n\n\n7.4.2 Parameter Considerations\nThe number of parameters a transformer can effectively utilize depends on both training data quantity and the complexity of patterns to be learned. Transformer parameters come primarily from two sources. Width (model dimension d) determines embedding and hidden state sizes; increasing width allows more complex pattern representation at each position but increases parameters quadratically because weight matrices scale as d × d. Depth (number of layers) determines how many refinement steps occur; increasing depth allows hierarchical abstractions through repeated processing but increases parameters linearly.\nScaling laws from natural language processing suggest performance improves smoothly with increased parameters, data, and compute (Kaplan et al. 2020). Similar principles apply to genomics, though optimal ratios may differ. Genomic sequences are less compressible than natural language: each nucleotide carries less predictable information than words in structured sentences. The entropy of DNA sequence is higher than English text, meaning more parameters may be needed to model the same sequence length. This asymmetry suggests genomic models might benefit relatively more from depth (more processing of high-entropy information) than from width (more dimensions per position when each position carries limited structure).\nThe architectural landscape of genomic foundation models reveals distinct design philosophies. Table 7.1 compares representative models across key architectural dimensions, illustrating how different approaches balance model capacity, context length, and computational efficiency.\n\n\n\nTable 7.1: Architectural comparison of genomic foundation models. Width refers to model/hidden dimension; depth to number of layers. Context length is reported in base pairs (bp) where applicable. Models marked with † use non-transformer architectures.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nArchitecture\nWidth (d)\nDepth (Layers)\nHeads\nParameters\nContext\n\n\n\n\nDNABERT\nEncoder-only\n768\n12\n12\n110M\n~0.5 kb\n\n\nDNABERT-2\nEncoder-only\n768\n12\n12\n117M\nVariable\n\n\nNucleotide Transformer (500M)\nEncoder-only\n1,024\n24\n20\n500M\n6 kb\n\n\nNucleotide Transformer (2.5B)\nEncoder-only\n2,560\n32\n20\n2.5B\n6 kb\n\n\nNT-v2 (250M)\nEncoder-only\n—\n—\n—\n250M\n12 kb\n\n\nHyenaDNA †\nHyena stack\n128–256\n2–8\n—\n0.4M–6.6M\nup to 1M bp\n\n\nCaduceus †\nBiMamba\n256\n16\n—\n~7M\n131 kb\n\n\nEvo 2 (7B) †\nStripedHyena 2\n—\n—\n—\n7B\n1M bp\n\n\nEvo 2 (40B) †\nStripedHyena 2\n—\n—\n—\n40B\n1M bp\n\n\n\n\n\n\nSeveral patterns emerge from this comparison. First, traditional transformer-based models like DNABERT and Nucleotide Transformer inherit architectures closely resembling their NLP counterparts, with DNABERT using the same 12-layer, 768-dimension configuration as BERT-base (Ji et al. 2021). The Nucleotide Transformer family scales this approach, with the 500M variant using 24 layers and 20 attention heads, and the 2.5B variant expanding to 32 layers while maintaining the same head count (Dalla-Torre et al. 2023). This scaling primarily increases width (hidden dimension grows from 1,024 to 2,560) rather than dramatically increasing depth, following conventional transformer scaling practices.\nSecond, models designed for long-range genomic modeling adopt fundamentally different architectures to circumvent attention’s quadratic complexity. HyenaDNA replaces attention entirely with implicit long convolutions, achieving million-base-pair contexts with models containing only 2 to 8 layers and a few million parameters (Nguyen et al. 2023). Caduceus extends the Mamba state-space architecture with bidirectional processing and reverse-complement equivariance, using 16 layers to achieve 131 kb context (Schiff et al. 2024). Evo 2 represents the current frontier, with 7B and 40B parameter variants achieving million-token context through the StripedHyena 2 architecture (Brixi et al. 2025). These non-transformer approaches demonstrate that architectural innovation can achieve genomic-scale context lengths that remain computationally intractable for standard attention mechanisms.\nThird, the relationship between parameter count and context length is not straightforward. HyenaDNA achieves the longest context among early models (1 million bp) with fewer than 7 million parameters, while Nucleotide Transformer 2.5B processes only 6 kb despite 400 times more parameters. This inversion reflects a fundamental tradeoff: dense attention captures rich pairwise interactions but scales poorly with sequence length, while subquadratic alternatives like Hyena operators and state-space models sacrifice some interaction modeling capacity for computational tractability. The optimal choice depends on whether the task requires capturing dense local interactions or sparse long-range dependencies.\nThe relationship between parameter count and downstream task performance is not always monotonic: a well-trained smaller model can outperform a poorly trained larger one, and task-specific fine-tuning often matters more than pretraining scale for focused clinical applications. Nucleotide Transformer v2 demonstrated this principle dramatically, achieving comparable or superior performance to the 2.5B-parameter v1 models with only 250M parameters by incorporating architectural improvements and training for more tokens (Dalla-Torre et al. 2023). Similarly, HyenaDNA achieves state-of-the-art results on 12 of 18 benchmarks from the Nucleotide Transformer suite while using 1,500 times fewer parameters. These results suggest that architectural efficiency and training strategy may matter as much as raw parameter count for genomic applications.\n\n\n7.4.3 Context Length Strategies\nStandard self-attention’s \\(O(L^2)\\) complexity becomes prohibitive for long genomic contexts, forcing architectural choices that trade expressiveness for tractability. The strategies employed reflect different assumptions about which interactions matter most for genomic modeling.\nThe quadratic complexity of full attention becomes prohibitive for genomic sequences, but different applications tolerate different trade-offs between efficiency and expressiveness. When most relevant interactions are local (as often holds for regulatory sequences where nearby elements interact more strongly than distant ones), restricting attention to fixed windows reduces complexity to \\(O(Lw)\\) where w is window size. For clinical variant interpretation in coding sequences, where splice sites and reading frame context typically lie within a few hundred bases, local attention may capture the relevant biology. The trade-off is missing long-range interactions that fall outside windows, potentially critical for understanding distal enhancer effects or structural variant consequences.\nHierarchical approaches recover some long-range capability while maintaining efficiency. Lower layers can use local windows to capture fine-grained patterns, while upper layers attend to every \\(k\\)-th position to capture global structure. Hybrid models like Enformer apply CNNs to downsample sequences before transformer layers, reducing the effective sequence length that attention must handle (Avsec et al. 2021). A 200-kilobase genomic region might be compressed to roughly 1,500 positions after CNN processing, making full attention tractable at the cost of single-nucleotide resolution in transformer layers.\nMathematical approximations offer yet another path, preserving dense connectivity while reducing computational cost. Linformer approximates the attention matrix through low-rank decomposition, reducing complexity to linear in sequence length (Wang et al. 2020). Performer uses random feature methods to approximate attention scores without explicitly computing the full \\(L\\timesL\\) matrix (Choromanski et al. 2022). These approximations trade some expressiveness for efficiency and may miss certain long-range dependencies that low-rank structure cannot capture. The choice among sparse patterns, hierarchical designs, and mathematical approximations depends on whether the target biology demands single-nucleotide resolution, long-range connectivity, or both.\n\n\n7.4.4 Memory and Precision\nMemory requirements compound computational challenges for genomic transformers. Training requires storing activations for backpropagation, and attention matrices are particularly memory-intensive. A \\(100\\)-kilobase sequence with \\(16\\) attention heads and \\(12\\) layers requires storing \\(16 \\times 12 \\times 100{,}000 \\times 100{,}000\\) attention weights, approximately \\(2\\) terabytes at \\(32\\)-bit precision before considering other activations.\nMemory constraints often limit model size and sequence length more than computational budget. Recomputing activations during the backward pass rather than storing them (a technique called gradient checkpointing) trades additional computation for reduced memory footprint, enabling training of larger models or longer sequences on fixed hardware at the cost of 20 to 30 percent additional training time. Further memory savings come from reducing numerical precision: using 16-bit floating point for most computations while maintaining 32-bit precision for critical operations like loss computation and optimizer updates. Modern GPUs accelerate 16-bit arithmetic substantially, providing near 2× speedup with minimal precision loss. Flash Attention implements memory-efficient attention computation that avoids materializing the full attention matrix, enabling longer contexts within fixed memory budgets (Dao et al. 2022). Together, these optimizations determine the practical limits of what can be trained on available hardware.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-variants",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-variants",
    "title": "7  Transformers and Attention",
    "section": "7.5 Architectural Variants for Genomics",
    "text": "7.5 Architectural Variants for Genomics\nFoundation model architectures encode assumptions about what matters. A model optimized for scoring existing variants differs fundamentally from one designed to generate novel sequences, which differs again from one predicting molecular interactions. The choice of architecture shapes what questions can be asked.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 7.6: [Enhancing] Three-panel comparison. Panel A (Encoder-only, e.g., BERT/DNABERT): Bidirectional attention pattern (full matrix), typical use for classification/embedding. Panel B (Decoder-only, e.g., GPT/Evo): Causal attention pattern (lower triangular), typical use for generation. Panel C (Hybrid CNN-Transformer, e.g., Enformer): CNN downsampling followed by transformer, showing how hybrid reduces sequence length before attention.\n\n\n\n\n7.5.1 Encoder-Only Transformers\nWhen a clinical laboratory queries a pathogenicity database for a novel missense variant, they need a model that integrates information from the entire protein sequence: upstream domains that establish structural context, downstream regions that complete functional units, and evolutionary patterns that distinguish tolerated from deleterious changes. Encoder-only transformers process sequences bidirectionally, allowing each position to attend to all other positions including those that follow in the sequence. This bidirectional context produces richer representations than unidirectional processing because each position’s representation incorporates information from the entire sequence.\nDNABERT exemplifies this architecture, trained with masked language modeling objectives where random tokens are masked and predicted from bidirectional context (Ji et al. 2021). The model learns to predict held-out k-mers based on surrounding sequence, implicitly learning sequence patterns and constraints that transfer to downstream tasks. Nucleotide Transformer follows similar principles at larger scale (Dalla-Torre et al. 2023). These models excel at representation learning: producing embeddings that capture biological properties useful for variant effect prediction, function classification, or other tasks that require fixed-length representations of variable-length sequences.\nBidirectional attention suits tasks where both upstream and downstream context matters for understanding a position. Transcription factor binding depends on flanking sequence in both directions. Splice site recognition requires seeing both exonic and intronic context. Variant pathogenicity may depend on protein domain context from both N-terminal and C-terminal directions. The limitation is that encoder-only architectures cannot generate sequences autoregressively because they require seeing the full sequence to produce any output; they answer “what does this sequence mean” rather than “what sequence should come next.”\n\n\n7.5.2 Decoder-Only Transformers\nGenerating synthetic genomic sequences for therapeutic design, creating diverse antibody libraries for drug discovery, or sampling from learned regulatory grammars all require models that produce sequences one token at a time. Decoder-only transformers use causal attention where each position attends only to itself and preceding positions. This structure enables autoregressive generation: the model produces sequences one token at a time, conditioning each new token on all previous tokens.\nGenSLM applies this architecture to genomic data, training on next-token prediction to learn sequence distributions (Zvyagin et al. 2022). The model learns to predict the next nucleotide or \\(k\\)-mer given all preceding context, implicitly learning the statistical regularities of genomic sequence. This objective aligns naturally with generation tasks: sampling proceeds by repeatedly predicting the next token and appending it to the sequence. Causal attention is essential for generation because the model must produce each position before it can condition subsequent positions on that output.\nThe trade-off is that causal attention produces less rich representations for fixed sequences because each position sees only partial context. Position 100 in a 1000-position sequence has access to only the first 100 positions, not the remaining 900 that might provide relevant information. For variant effect prediction where downstream context matters, this limitation can be substantial. The choice between encoder and decoder architectures reflects a fundamental tension: representation learning benefits from bidirectional context, while generation requires causal structure.\n\n\n7.5.3 Encoder-Decoder Transformers\nSome genomic tasks require transforming one sequence into another of different length or structure. Predicting protein sequence from coding DNA, generating variant descriptions from sequence context, or translating between sequence representations all involve input-output relationships that neither pure encoder nor pure decoder architectures handle naturally. Encoder-decoder architectures combine bidirectional encoding with autoregressive decoding (Vaswani et al. 2023).\nThe encoder processes an input sequence with full bidirectional attention, producing contextualized representations. The decoder then generates output tokens autoregressively, attending both to its own previous outputs (through causal self-attention) and to encoder representations (through cross-attention). This cross-attention allows each decoder position to query the full encoded input when generating output, combining the benefits of bidirectional understanding with autoregressive generation.\nEncoder-decoder models are less common in genomic applications than encoder-only or decoder-only variants because most genomic tasks either need representations (favoring encoders) or generation (favoring decoders), not both simultaneously. Machine translation exemplifies the encoder-decoder use case: encode a sentence in one language, decode into another. Genomic analogs might include predicting protein sequences from codon-optimized DNA or generating clinical variant reports from sequence features, but these applications remain less developed than pure representation or generation tasks.\n\n\n7.5.4 Hybrid CNN-Transformer Models\nThe most successful genomic transformers combine convolutional and attention mechanisms rather than using transformers alone. This hybrid approach exploits CNNs’ efficiency for local pattern extraction while using transformers for long-range integration, matching the multi-scale structure of genomic regulation where both local motifs and distal interactions determine function.\nEnformer and Borzoi apply convolutional stems to long sequences, downsampling through pooling, then pass compressed representations through transformer layers (Avsec et al. 2021). The CNN layers handle motif recognition, nucleosome positioning signals, and local chromatin features with parameter efficiency that pure transformers cannot match. Transformer layers then integrate across the broader regulatory landscape, learning enhancer-promoter relationships and TAD boundary effects. This division of labor achieves state-of-the-art performance on regulatory prediction tasks while remaining computationally tractable for 200-kilobase contexts. The regulatory sequence models in Chapter 13 examine Enformer and Borzoi comprehensively, including their applications to variant effect prediction and expression modeling.\nThe hybrid approach also addresses the quadratic attention bottleneck indirectly. By downsampling sequences before transformer layers (often by factors of 128 or more), hybrids reduce effective sequence length and thus attention cost. A 200-kilobase genomic region compressed to 1,500 positions requires only 2.25 million attention computations per layer rather than 40 billion for the uncompressed sequence. The cost is loss of single-nucleotide resolution in transformer layers, though the CNN stem preserves local detail that attention layers integrate but do not need to resolve. Chapter 13 examines Enformer and related hybrid architectures in detail.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-training",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-training",
    "title": "7  Transformers and Attention",
    "section": "7.6 Training Dynamics",
    "text": "7.6 Training Dynamics\nWhen a model trained to predict pathogenic variants misclassifies a disease-causing BRCA1 mutation as benign, the consequences extend beyond benchmark metrics. Clinical laboratories may return incorrect results; patients may forego preventive surgeries that could save their lives. Training failures matter clinically because they determine what models learn and what they miss. The evaluation methodology in Chapter 21 examines how to detect such failures before clinical deployment, while Chapter 22 addresses systematic biases that cause models to fail on clinically important subgroups. A model that overfits to common polymorphisms in training data will fail on the rare variants that matter most for diagnosis. A model whose gradients vanish during training will never learn the subtle regulatory patterns that distinguish pathogenic from benign promoter variants. Understanding training dynamics helps predict and prevent these failures.\n\n7.6.1 Optimization\nGenomic transformers inherit their training foundations from natural language processing but require adjustments for biological data. The Adam optimizer and its variant AdamW remain standard, using adaptive learning rates that maintain per-parameter estimates adjusted based on gradient statistics (Loshchilov and Hutter 2019). AdamW applies weight decay directly to parameter updates rather than to the loss function, improving generalization and training stability.\nLearning rate schedules typically use warmup (linearly increasing learning rate from near-zero to peak over the first several thousand steps) followed by decay (linear or cosine decrease over the remaining training). Warmup addresses a specific instability: transformers with random initialization can produce extreme gradients early in training, and adaptive optimizers need time to build accurate gradient statistics. Warmup allows the optimizer to stabilize before applying full learning rates. Skipping warmup often causes training collapse in the first few hundred steps, manifesting as loss spikes or NaN values.\nFor genomic data, learning rate tuning may require adjustment from NLP defaults. Regulatory sequences with highly conserved motifs (TATA boxes, splice site dinucleotides) create strong signals that models can overfit quickly; lower learning rates may prevent latching onto these patterns before learning subtler regulatory grammar. Protein sequences exhibit weaker positional conservation than regulatory DNA, potentially benefiting from higher rates that encourage broader exploration of the loss landscape. Empirically, genomic transformers often use peak learning rates of 1e-4 to 5e-4 (Avsec et al. 2021), somewhat lower than the 1e-3 to 3e-3 common in language modeling.\n\n\n7.6.2 Regularization\nRegularization prevents overfitting, particularly important when training data is limited relative to model size. Genomic datasets, while growing rapidly, remain smaller than the trillion-token corpora used for large language models. A model with 100 million parameters trained on 10 billion nucleotides faces different overfitting risks than one trained on 1 trillion tokens.\nTransformers’ high parameter counts create substantial overfitting risk, particularly for genomic applications where labeled training data may be limited. Two complementary regularization strategies have proven essential. Randomly zeroing activations during training (dropout) forces the network to learn robust features that remain informative even when some information pathways are blocked. Applying dropout to attention weights specifically prevents over-reliance on particular position pairs, encouraging distributed representations. Genomic transformers, like DNABERT, use standard dropout rates of 0.1 to 0.2.\nConstraining parameter magnitudes provides orthogonal regularization. Weight decay penalizes large parameter values, encouraging smaller weights that generalize better to unseen data. For transformers, this penalty is typically applied to all parameters except biases and layer normalization parameters. The coefficient requires careful tuning: too little provides insufficient regularization; too much constrains capacity and reduces model expressiveness. Values of 0.01 to 0.1 are common, with higher values for smaller datasets where overfitting risk is greater. The interaction between dropout and weight decay means that optimal settings for each depend on the other, requiring joint tuning rather than independent optimization.\n\n\n7.6.3 Gradient Stability\nGradient issues plague deep network training and require specific attention for genomic transformers. Vanishing gradients occur when gradients become extremely small through many layers, preventing learning in early layers. Exploding gradients are the opposite: gradients grow exponentially and destabilize training. Transformers mitigate vanishing gradients through residual connections that provide direct gradient paths, allowing gradients to flow from output to early layers without passing through potentially attenuating transformations. Exploding gradients are addressed through gradient clipping, which rescales gradients when their norm exceeds a threshold.\nFor genomic transformers, gradient issues manifest differently than in language models. Natural language has nested grammatical organization (words form phrases, phrases form clauses, clauses form sentences), and this hierarchy creates structural landmarks that concentrate attention on syntactically meaningful positions. Genomic sequences lack equivalent organization: functional elements like promoters, splice sites, and enhancers are scattered without consistent positional relationships, so attention patterns must be learned entirely from data without structural priors to guide gradient flow. This absence of hierarchy interacts with imbalanced token frequencies to compound the problem. Common k-mers receive large gradients from frequent occurrence while rare but biologically important tokens (such as k-mers containing the stop codon TAG) receive small gradients from infrequent appearance. In language, frequent function words are grammatically constrained in their attention patterns; in genomic sequences, common k-mers have no such constraints, allowing their gradients to dominate through frequency alone rather than biological importance. Addressing these imbalances may require loss reweighting or adaptive sampling that ensures rare tokens appear frequently enough for effective learning.\n\n\n7.6.4 Distributed Training\nThe computational scale of genomic foundation models typically exceeds single-graphics processing unit (GPU) capacity, requiring distributed training strategies. Data parallelism replicates the model across GPUs, splitting batches across devices and aggregating gradients. This approach scales well up to batch sizes limited by convergence requirements but does not help when the model itself exceeds GPU memory. Model parallelism splits the model across devices, necessary when parameters exceed single-GPU memory. Pipeline parallelism divides layers across devices and pipelines forward and backward passes, interleaving computation to improve device utilization.\nBatch size selection involves competing considerations. Larger batches provide more stable gradient estimates and better GPU utilization but require more memory and may reduce generalization. Genomic transformers often use gradient accumulation to simulate large batches: small batches process sequentially, gradients accumulate, then a single parameter update occurs. This strategy provides large-batch gradient stability without the memory cost, though it increases training time proportionally. Effective batch sizes of 256 to 4096 sequences are common for genomic transformers, achieved through accumulation over many smaller physical batches (e.g., Nucleotide Transformer’s 2.5B model accumulated gradients from physical batches of just 2 sequences to reach 1 million tokens per update (dallatorre_nucleotide_2024?)).",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-limitations",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-limitations",
    "title": "7  Transformers and Attention",
    "section": "7.7 Limitations and Emerging Alternatives",
    "text": "7.7 Limitations and Emerging Alternatives\nA 48-year-old patient presents with a suspected Lynch syndrome diagnosis, and genetic testing reveals a structural variant spanning 3 megabases on chromosome 2 that may disrupt the MSH2 gene and its upstream regulatory region. The clinical team needs to determine whether this variant explains the patient’s early-onset colorectal cancer and guides surveillance recommendations for family members. Standard transformers cannot address this question: the quadratic complexity of self-attention makes 3-megabase contexts computationally intractable. Current models can span 200 kilobases with hybrid architectures, yet the structural variants and chromosomal rearrangements that cause many inherited cancer syndromes remain beyond reach. This gap between clinical need and computational capability defines the frontier of genomic AI.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 7.7: [High] Log-log plot showing computational cost (FLOPs or memory) vs. sequence length for different architectures. Show: Standard attention (quadratic curve, \\(O(L^2)\\)), sparse/local attention (sub-quadratic), state space models like Hyena/Mamba (linear). Annotate biologically relevant context lengths: promoter (\\(\\sim 1\\ \\mathrm{kb}\\)), gene (\\(\\sim 10\\ \\mathrm{kb}\\)), enhancer-promoter (\\(\\sim 100\\ \\mathrm{kb}\\)), TAD (\\(\\sim 1\\ \\mathrm{Mb}\\)), chromosome arm (\\(\\sim 100\\ \\mathrm{Mb}\\)). Draw vertical lines at each scale showing which architectures are tractable.\n\n\n\n\n7.7.1 Quadratic Ceiling\nThe quadratic complexity of self-attention remains transformers’ most severe limitation for genomics. Computing all pairwise attention scores requires \\(O(L^2)\\) operations and memory. For genomic contexts exceeding \\(100\\) kilobases (roughly \\(100{,}000\\) single-nucleotide tokens), this becomes prohibitive. Even with sparse approximations and efficient implementations, transformers struggle at megabase scales where many regulatory interactions occur and structural variants manifest their effects.\n\n\n\n\n\n\nNoteBig-O Notation\n\n\n\nBig-O notation describes how computational cost scales with input size. For genomic models, the input size L is typically sequence length. When we say attention has \\(O(L^2)\\) complexity, we mean that doubling the sequence length quadruples the computation: a 10,000 bp sequence requires 100 million pairwise comparisons. Sub-quadratic approaches like Hyena’s \\(O(L \\log L)\\) scale far more gently: doubling the sequence length only slightly more than doubles the cost. For chromosome-scale sequences of millions of base pairs, this difference determines whether analysis is feasible at all.\n\n\nRecent models have pushed context lengths substantially. Enformer handles 200 kilobases; emerging models approach 1 megabase. But these achievements rely on hybrid architectures with significant downsampling or hierarchical windowing that may miss certain long-range patterns or single-nucleotide resolution details. Pure transformers without such modifications remain limited to shorter contexts. The fundamental constraint shapes what questions transformers can address and motivates alternatives that escape quadratic scaling.\n\n\n7.7.2 State Space Models\nState space models (SSMs) address the quadratic barrier directly by achieving linear complexity while maintaining long-range modeling capability. Rather than computing all pairwise interactions, SSMs represent sequences as continuous-time dynamical systems, maintaining memory through recurrent state updates that propagate information across positions without explicit pairwise computation.\nArchitectures like S4, Hyena, and Mamba have demonstrated competitive or superior performance to transformers on various sequence modeling tasks while scaling to much longer contexts (Gu et al. 2022; Poli et al. 2023; Gu and Dao 2024). For genomics, this capability enables whole-chromosome or potentially whole-genome modeling that remains intractable for standard transformers. HyenaDNA processes sequences up to 1 million nucleotides at single-nucleotide resolution, enabling analysis of structural variants and long-range regulatory interactions that transformers cannot approach (Nguyen et al. 2023). The Evo model extends this further, achieving context lengths sufficient for bacterial genome-scale modeling (Nguyen et al. 2024). The DNA language model architectures in Chapter 11 examine HyenaDNA, Caduceus, and Evo in detail, exploring how linear complexity enables new categories of genomic analysis including sequence design applications (Chapter 28).\n\n\n7.7.3 Choosing Architectures\nThe choice between transformers and alternatives depends on the biological question and computational constraints. Transformers excel when global context matters but sequences are not extremely long (under 10 to 50 kilobases depending on computational resources). Attention maps provide interpretability, showing which positions the model considers relevant for predictions. Transformers benefit from extensive tooling and pretrained models from NLP that transfer readily to genomics.\nCNNs remain preferable when computational efficiency is paramount and local patterns dominate. For splice site prediction or promoter classification where relevant context spans at most a few hundred base pairs, a well-designed CNN may outperform transformers while using far fewer parameters. The inductive bias toward local patterns also regularizes against overfitting when training data is limited. The convolutional architectures examined in Chapter 6 established these design principles.\nHybrid approaches often achieve the best practical results for intermediate-scale problems. Models combining CNNs for local feature extraction with transformers for long-range integration outperform pure architectures on regulatory prediction tasks, as Chapter 13 demonstrates with Enformer and related models. The optimal combination depends on the specific biological question and the scale of relevant interactions.\nThe transition toward sub-quadratic architectures continues. Early results suggest SSMs match or exceed transformers on some genomic benchmarks while scaling to longer contexts. The question is no longer whether alternatives to quadratic attention exist, but which tasks benefit most from linear-complexity architectures and which retain advantages from explicit pairwise attention computation.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-conclusion",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-conclusion",
    "title": "7  Transformers and Attention",
    "section": "7.8 Capacity Without Direction",
    "text": "7.8 Capacity Without Direction\nThe transformer architecture provides the computational substrate for modern genomic foundation models, but architecture alone does not determine what models learn. Attention mechanisms enable pairwise interaction modeling across arbitrary sequence distances. Position encodings break permutation invariance to preserve the sequential structure essential to regulatory grammar. Stacked blocks build hierarchical representations through iterative refinement. These components create capacity; training objectives and data determine how that capacity is used.\nSelf-supervised pretraining transforms architectural capacity into biological knowledge. Masked language modeling teaches models to predict held-out tokens from context, implicitly learning the sequence patterns and evolutionary constraints that determine biological function. Next-token prediction in autoregressive models captures sequential dependencies required for sequence generation. Applied to massive genomic datasets, these objectives enable transformers to learn representations that transfer across diverse downstream tasks without task-specific supervision. Foundation models like DNABERT for regulatory sequence, ESM-2 for proteins, and Enformer for expression prediction each demonstrate that transformers trained on biological sequence capture patterns that generalize beyond their training objectives. The pretraining objectives that shape these learned representations are examined in Chapter 8.\nAttention introduced a paradigm shift in how genomic models access context. Where convolutional networks aggregate local information through hierarchical composition, attention enables direct communication between any two positions regardless of distance. The computational challenge shifts from extending receptive fields to managing the quadratic complexity of pairwise attention. State space models and linear attention variants address this bottleneck while maintaining long-range capability, and whether these alternatives ultimately complement or displace standard transformers remains an open question. What is clear is that attention-based architectures have become the default substrate for genomic foundation models, with the pretraining objectives examined in Chapter 8 determining what biological knowledge they acquire.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nChoromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2022. “Rethinking Attention with Performers.” arXiv. https://doi.org/10.48550/arXiv.2009.14794.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDao, Tri, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.” Advances in Neural Information Processing Systems 35 (December): 16344–59. https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.\n\n\nGu, Albert, and Tri Dao. 2024. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” In. https://openreview.net/forum?id=tEYskw1VY2.\n\n\nGu, Albert, Karan Goel, Ankit Gupta, and Christopher Ré. 2022. “On the Parameterization and Initialization of Diagonal State Space Models.” Advances in Neural Information Processing Systems 35 (December): 35971–83. https://proceedings.neurips.cc/paper_files/paper/2022/hash/e9a32fade47b906de908431991440f7c-Abstract-Conference.html.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. “Scaling Laws for Neural Language Models.” arXiv. https://doi.org/10.48550/arXiv.2001.08361.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2019. “Decoupled Weight Decay Regularization.” arXiv. https://doi.org/10.48550/arXiv.1711.05101.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence Modeling and Design from Molecular to Genome Scale with Evo.” Science 386 (6723): eado9336. https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nPoli, Michael, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. 2023. “Hyena Hierarchy: Towards Larger Convolutional Language Models.” In Proceedings of the 40th International Conference on Machine Learning, 28043–78. PMLR. https://proceedings.mlr.press/v202/poli23a.html.\n\n\nPress, Ofir, Noah A. Smith, and Mike Lewis. 2022. “Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation.” arXiv. https://doi.org/10.48550/arXiv.2108.12409.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” arXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nSu, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” Neurocomputing 568 (February): 127063. https://doi.org/10.1016/j.neucom.2023.127063.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://doi.org/10.48550/arXiv.1706.03762.\n\n\nWang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. “Linformer: Self-Attention with Linear Complexity.” arXiv. https://doi.org/10.48550/arXiv.2006.04768.\n\n\nXiong, Ruibin, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. 2020. “On Layer Normalization in the Transformer Architecture.” In Proceedings of the 37th International Conference on Machine Learning, 10524–33. PMLR. https://proceedings.mlr.press/v119/xiong20b.html.\n\n\nZvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. “GenSLMs: Genome-Scale Language Models Reveal SARS-CoV-2 Evolutionary Dynamics.” bioRxiv. https://doi.org/10.1101/2022.10.10.511571.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html",
    "href": "part_2/p2-ch08-pretraining.html",
    "title": "8  Pretraining Strategies",
    "section": "",
    "text": "8.1 Masked Language Modeling\nThe choice of pretraining objective is not merely technical; it encodes assumptions about what matters in biological sequence. Masked language modeling encourages bidirectional context integration: the model learns to predict missing tokens using information from both upstream and downstream sequence. Next-token prediction builds autoregressive capabilities: the model learns to generate sequence one position at a time, enabling design of novel proteins or regulatory elements. Contrastive learning teaches invariance: the model learns that functionally equivalent sequences should map to similar representations regardless of species or polymorphism. Each objective produces a different model, and those differences propagate to downstream performance. A model pretrained with masked language modeling may excel at variant effect prediction (where context on both sides matters) but struggle at sequence generation. A model pretrained for generation may produce plausible sequences but provide representations less suited for classification tasks. Understanding what each objective teaches, and what assumptions each encodes, is prerequisite to selecting the right foundation model for a given application.\nSelf-supervised pretraining addresses a fundamental asymmetry in genomic data. Reference genomes span billions of nucleotides across thousands of species. Population sequencing projects catalog genetic variation in millions of individuals. Functional genomics consortia measure chromatin accessibility and gene expression across hundreds of cell types (see Chapter 2 for comprehensive treatment of these resources). Yet experimental labels remain sparse: for any given sequence, we typically lack direct measurements of its regulatory function, its effect on splicing, or its contribution to disease risk. Self-supervised objectives extract training signal from the sequences themselves, without requiring experimental labels. The resulting models learn representations that capture evolutionary constraints, sequence grammar, and functional relationships, all from the patterns present in unlabeled data. When these representations are applied to downstream tasks with limited labels, the pretrained knowledge makes scarce labeled data go further.\nConsider predicting whether a splice site variant in DMD will cause exon skipping in Duchenne muscular dystrophy. The model must recognize the canonical GT-AG splice signals, encode how flanking sequences modulate splicing efficiency, and integrate information from both the upstream exon and downstream intron. A model trained only on labeled splice variants would see perhaps a few hundred DMD examples across the entire clinical literature. A model pretrained on billions of nucleotides learns splice grammar across the entire genome, then applies that knowledge to the specific clinical question. Masked language modeling provides this pretraining by teaching models to predict missing sequence content from surrounding context, and the bidirectional attention it requires captures exactly the upstream-downstream integration that splice prediction demands.\nMLM treats sequences as partially observed and trains models to reconstruct missing content. The procedure is straightforward: randomly mask portions of an input sequence, feed the corrupted sequence to the model, and train the model to predict the original tokens at masked positions. A masking strategy replaces selected tokens with a special [MASK] token, leaving the surrounding context intact. The model processes the masked sequence through its layers and produces predictions for the masked positions, typically optimizing cross-entropy loss over the vocabulary at each masked location.\nThe key insight is that accurate prediction requires learning genuine sequence structure. To predict a masked position in a transcription factor binding site, the model must recognize the surrounding motif context. To predict masked splice donor sequences, the model must encode the consensus GT dinucleotide and the flanking patterns that modulate splicing strength. Over millions of training examples, models build distributed representations of motifs, compositional rules, and sequence constraints that transfer to tasks never seen during pretraining. The DMD splice variant can be evaluated using patterns learned from every splice site in the genome.\nMLM encourages bidirectional context integration, and this bidirectionality has direct clinical relevance. Unlike autoregressive models that condition only on preceding tokens, MLM models see both left and right context when predicting masked positions. For genomics, this matches biological reality: regulatory function depends on patterns both upstream and downstream of any given position. A transcription factor binding site is recognized through flanking sequences on both sides. Splicing signals require coordination between donor and acceptor sites separated by hundreds of bases. Missense variants disrupt protein function through effects that depend on the entire domain context, not just the preceding amino acids. The bidirectional attention mechanisms examined in Chapter 7 naturally capture these dependencies.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-mlm",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-mlm",
    "title": "8  Pretraining Strategies",
    "section": "",
    "text": "8.1.1 Masking Strategies and Their Implications\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 8.2: [High] Two-panel visualization comparing masking strategies on a regulatory sequence containing a TF binding motif. Panel A (Random token masking): Individual tokens masked throughout, including partial motif masking; show how local context allows easy prediction. Panel B (Span masking): Entire motif region masked as contiguous span; show how prediction requires reasoning from distal regulatory context. Include attention patterns for each, showing how span masking forces longer-range dependencies.\n\n\n\nPredicting whether a regulatory variant disrupts an entire transcription factor binding site or merely alters its affinity requires models that learn compositional patterns, not just local nucleotide statistics. The tension between local and compositional learning plays out in masking strategy design.\nRandom masking of individual tokens creates predictions that are relatively local: each masked position can often be inferred from immediately adjacent nucleotides. This approach is efficient but may not force models to learn higher-order structure. Span masking, which masks contiguous blocks of tokens, forces models to infer longer-range dependencies and compositional patterns. If an entire transcription factor binding motif is masked, the model cannot rely on partial motif information and must instead recognize the motif’s role from surrounding regulatory context. For clinical variant interpretation, span masking may better capture the compositional grammar that determines whether a regulatory variant disrupts an entire binding site or merely modulates its affinity.\nMasking rates present a fundamental tradeoff between supervision density and prediction difficulty. Higher masking rates (30-40% of tokens) provide more supervision per sequence but make prediction harder and may destabilize training. Since each masked token becomes a prediction target, higher rates extract more learning signal from a single forward pass through the model. Lower masking rates (10-15%) produce more stable training but require more data to achieve equivalent coverage. The standard 15% rate from BERT represents a reasonable compromise, though genomic models have explored values ranging from 10% to 40% depending on context length and tokenization granularity (Devlin et al. 2019). DNABERT used 15% masking on 6-mer tokens, while later models have experimented with adaptive masking rates that increase as training progresses, starting conservatively and becoming more aggressive as the model’s predictions improve (Ji et al. 2021).\nTokenization interacts with masking in ways that affect what biological patterns models learn (see Chapter 5 for comprehensive treatment of tokenization strategies). DNABERT pioneered MLM for genomic sequences by applying it to overlapping \\(k\\)-mer tokens: rather than treating DNA as individual nucleotides, DNABERT tokenizes sequences into all possible 6-mers with overlapping windows (Ji et al. 2021). Masking then operates at the \\(k\\)-mer level, with entire 6-mers masked as units. This design encourages learning of \\(k\\)-mer level patterns corresponding to transcription factor binding motifs (typically 6-12 base pairs) and other short functional elements. DNABERT-2 adopted byte-pair encoding (BPE) tokenization, which learns a vocabulary of variable-length subword units from the training corpus (Zhou et al. 2024). BPE tokens represent single nucleotides, common motifs, or repeated elements depending on their frequency. MLM with BPE balances flexibility with compositional structure, though the learned vocabulary may not align with biological functional units in interpretable ways.\nThe design decisions explored by DNABERT and DNABERT-2 established patterns that subsequent DNA language models have built upon and refined. Chapter 11 examines how these architectural and tokenization choices have evolved as the field has scaled to longer contexts and larger training corpora.\n\n\n8.1.2 What Masked Language Models Learn\nMLM objectives drive models to capture multiple levels of sequence organization, from local nucleotide statistics to long-range regulatory grammar. At the lowest level, models learn base composition and local constraints: CpG dinucleotide frequencies, GC content biases, and simple repeat patterns. These basic properties are necessary but not sufficient for biological function prediction.\nAt higher levels, MLM captures motif patterns and sequence grammar. Predicting masked positions in regulatory regions requires recognizing transcription factor binding sites, understanding how motifs combine in enhancers and promoters, and learning context-dependent usage patterns. If certain transcription factor motifs co-occur at specific distances (as they do in developmental enhancers where factors like HOX proteins bind cooperatively), masking one motif and predicting it from the other reinforces this grammatical relationship. This compositional learning is difficult to achieve with supervised learning alone, which typically provides coarse binary labels (“enhancer” versus “non-enhancer”) rather than fine-grained structural information about sequence organization.\nMLM also captures evolutionary conservation patterns implicitly, and this has direct relevance for clinical variant interpretation. Conserved sequences are constrained because mutations would disrupt function. By learning to predict conserved patterns from surrounding context, models encode which sequence features are under selection. This knowledge transfers to variant effect prediction, where the model recognizes when a mutation disrupts a learned conserved pattern. A variant that replaces a highly predictable position (one the model confidently fills in during MLM) is more likely to be damaging than one at a position where the model is uncertain. The connection between pretraining on raw sequence and downstream variant interpretation illustrates how self-supervised objectives capture biologically meaningful structure without explicit functional labels. The variant effect prediction approaches in Chapter 14 leverage these learned patterns directly, while probing methods (Section 9.3) reveal what specific patterns models have captured.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-autoregressive",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-autoregressive",
    "title": "8  Pretraining Strategies",
    "section": "8.2 Next-Token Prediction",
    "text": "8.2 Next-Token Prediction\nDesigning a novel promoter sequence for gene therapy requires generating DNA that respects learned regulatory grammar while achieving specific expression characteristics. Masked language modeling can evaluate whether a candidate sequence looks “natural,” but it cannot generate sequences from scratch. A gene therapy team optimizing a CAR-T construct needs promoter variants to test; they cannot simply evaluate candidates one by one when the search space spans \\(4^{500}\\) possible 500-base-pair sequences. Next-token prediction provides the generative capability missing from MLM, learning to predict each token given only preceding tokens and thereby acquiring the ability to sample coherent novel sequences that respect learned biological constraints.\nNext-token prediction represents an alternative paradigm where models learn to predict each token in a sequence given only the preceding tokens. This autoregressive approach, popularized by GPT-style language models, treats sequence generation as a core capability rather than a secondary feature. For a sequence of length \\(T\\), the model predicts token \\(t\\) from tokens \\(1\\) through \\(t-1\\), maximizing the likelihood of the observed sequence under the model’s learned distribution. The probability of a sequence factors as the product of conditional probabilities for each token given its predecessors:\n\\[P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_1, \\ldots, x_{t-1})\\]\nAlgorithmically, next-token prediction requires causal masking in the attention mechanism. Each position attends only to earlier positions, ensuring predictions at position \\(t\\) depend exclusively on positions \\(1\\) through \\(t-1\\). The loss function is cross-entropy over the vocabulary, computed at every position rather than only at masked locations. During training, teacher forcing allows efficient parallel computation: the model predicts all positions simultaneously by feeding in the ground truth sequence shifted by one position. Generation at inference time is inherently sequential, predicting one token at a time and conditioning each prediction on all previous outputs.\nAutoregressive models develop systematic positional bias during training. Early positions in a sequence are predicted from minimal context: the first token has no conditioning information at all, the second token conditions only on the first, and so on. Later positions benefit from increasingly rich context as the full preceding sequence informs each prediction. This creates asymmetric representation quality across the sequence, with early positions learned less reliably than later ones. For natural language, this asymmetry is partially justified by syntactic structure (sentence openings are more formulaic than continuations), but genomic sequences have no such directional bias. Position 1 of a regulatory element carries as much functional information as position 500. Training dynamics that systematically disadvantage early positions introduce artifacts unrelated to biology.\nSeveral strategies mitigate positional bias. Training on both forward and reverse-complement sequences ensures that each position appears early in some training examples and late in others, averaging out directional effects. Prefix conditioning provides bidirectional context for an initial segment before autoregressive generation begins, giving all generated positions access to rich conditioning information. Some architectures incorporate bidirectional “warm-up” layers before causal attention, building position-independent representations that subsequent autoregressive layers can condition on. The severity of positional bias depends on sequence length and model capacity; for short sequences (under 1000 tokens), the effect is modest, but for genome-scale contexts exceeding 100 kilobases, early positions may be substantially underrepresented in learned distributions.\nThe fundamental difference from MLM lies in what the model can see during prediction. Autoregressive models build representations from unidirectional context, learning to generate sequences that respect learned constraints. This makes autoregressive pretraining attractive for sequence design applications (see Chapter 28). Sampling new sequences proceeds naturally: predict the first token, condition on it to predict the second, and continue token by token. The generation process directly uses the learned conditional distributions without requiring additional architectural modifications or iterative refinement procedures.\n\n8.2.1 Genomic Applications\nDNA sequences present a complication that natural language does not: they have no inherent directionality. Both strands encode information, and regulatory function is often strand-agnostic. A transcription factor binding site functions identically whether read 5’-to-3’ or on the reverse complement strand. This contrasts with natural language, where left-to-right reading order carries meaning. Early autoregressive genomic models addressed this by training separate models for forward and reverse strands or by augmenting training data with reverse-complement sequences. More recent approaches treat strand symmetry as an architectural constraint, ensuring that forward and reverse complement sequences produce equivalent representations through weight sharing or explicit symmetrization.\nEvo represents a large-scale autoregressive genomic model trained on whole genomes with long-context architectures (Nguyen et al. 2024). Using StripedHyena layers to achieve contexts exceeding 100 kilobases, Evo learns long-range dependencies including gene structure, repeat organization, and regulatory architecture spanning tens of kilobases. This enables generating coherent synthetic genomes that respect higher-order structure, not just local motif patterns. For therapeutic applications, Evo’s generative capability could design synthetic regulatory circuits, generate diverse candidate sequences for directed evolution, or produce training data through synthetic augmentation when real labeled data is scarce. The Evo architecture is examined in detail in Chapter 11, while sequence design applications are treated in Chapter 28.\nProtein sequence models trained autoregressively typically generate N-terminus to C-terminus, matching ribosomal synthesis and co-translational folding. Whether this biological asymmetry meaningfully improves learned representations remains unclear: autoregressive models learn conditional sequence distributions, not physical processes, and bidirectional masked language models like ESM-2 perform excellently despite having no inherent directionality. For design applications, generation direction is likely a second-order effect. ESM models and protein design systems like ProtGPT2 predict amino acid sequences autoregressively, learning protein grammar and evolutionary constraints that transfer to structure prediction and function annotation (Ferruz, Schmidt, and Höcker 2022). For designing therapeutic proteins (antibodies, enzymes, peptide drugs), autoregressive generation produces candidates that respect learned constraints on foldability and function. Chapter 12 examines these protein language models in detail.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-comparison",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-comparison",
    "title": "8  Pretraining Strategies",
    "section": "8.3 MLM and Autoregressive Comparison",
    "text": "8.3 MLM and Autoregressive Comparison\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 8.3: [Essential] Two-panel figure showing information flow during prediction. Panel A (MLM): Position in center of sequence with arrows coming from BOTH left and right context, showing bidirectional conditioning. Annotate: “Sees full context → better for understanding.” Panel B (Autoregressive): Same position with arrows only from left (preceding tokens), showing causal restriction. Annotate: “Sees only past → enables generation.”\n\n\n\nThe tension between bidirectional understanding and generative capability represents the fundamental tradeoff between these objectives. For tasks requiring understanding of full sequence context, MLM’s bidirectional attention provides richer representations. Predicting transcription factor binding at a specific location benefits from seeing both upstream and downstream sequence, information that autoregressive models cannot access during inference. Variant effect prediction similarly benefits from full context: a missense variant’s impact depends on the entire domain, not just the preceding residues.\nAutoregressive models offer more principled generation. Their sequential prediction structure matches the generation process exactly, whereas generating from MLM models requires iterative masking and filling procedures that were not part of pretraining. A promoter design task using MLM would require starting with random sequence, masking positions, predicting fills, remasking, and iterating until convergence. This procedure is ad hoc and may not produce sequences that lie on the learned distribution. Autoregressive generation is direct: sample token by token from learned conditionals.\nTraining efficiency differs between objectives in ways that affect practical decisions. MLM predicts only 15% of tokens per sequence but uses bidirectional context for each prediction. Autoregressive models predict all tokens but with unidirectional context. The effective supervision per sequence is higher for autoregressive training, but each prediction is less informed. For fixed compute budgets, the tradeoffs roughly balance, with optimal choice depending on downstream applications rather than training efficiency alone.\nTask-specific performance depends on alignment between pretraining and downstream objectives. If the downstream task involves predicting missing information from context (variant effect prediction, binding site identification, conservation scoring), MLM pretraining provides better transfer. If the downstream task involves generation or sequential decision-making (sequence design, sampling from conditional distributions, therapeutic protein generation), autoregressive pretraining aligns more naturally. For applications requiring both understanding and generation, hybrid architectures that combine bidirectional encoding with autoregressive decoding offer a middle ground, though these add complexity. Chapter 9 examines how different pretraining objectives interact with downstream task requirements.\nThis alignment extends beyond task type to affect how practitioners extract and use model representations. Encoder models trained with MLM produce final-layer embeddings that work reliably across diverse downstream tasks because the pretraining objective shaped representations for general utility. Decoder models trained with next-token prediction specialize their final layers for vocabulary prediction, often making intermediate layers superior for classification and regression tasks. This layer hunting problem adds hyperparameter search burden when using decoder models for non-generative applications, sometimes requiring evaluation across all layers to identify where task-relevant information concentrates. The practical implications for model deployment are examined in Section 9.5.\n\n8.3.1 Hybrid Architectures\nThe dichotomy between MLM and autoregressive objectives is not absolute. Hybrid architectures attempt to capture bidirectional understanding while retaining generative capability, though they add complexity and training cost.\nPermutation language modeling, introduced in XLNet, trains on all possible token orderings rather than a fixed left-to-right sequence (yang_xlnet_2019?). For each training example, the model samples a random permutation of positions and predicts tokens in that order, with each position attending only to positions earlier in the sampled permutation. Across many permutations, every token eventually conditions on every other token, achieving bidirectional context in expectation while maintaining autoregressive structure for any single forward pass. The approach is elegant but computationally expensive: the permutation sampling and bookkeeping add overhead, and generation still requires committing to a specific ordering. For genomic applications, permutation LM could address strand symmetry naturally (forward and reverse orderings are equally likely), but implementations remain rare in the biological literature. One example is ProtXLNet examined in Section 12.2.\nPrefix language modeling offers a more practical hybrid. The model processes an initial prefix bidirectionally, building rich contextualized representations, then switches to autoregressive generation for the remainder. This architecture underlies encoder-decoder models like T5 and has been adapted for protein design, where a conditioning context (desired function, scaffold structure, or homologous sequences) is encoded bidirectionally before generating novel sequence autoregressively. ProGen2 applies this pattern to conditional protein generation, encoding functional annotations or partial sequences as prefix context before sampling completions (nijkamp_progen2_2023?). The prefix provides the “understanding” that guides generation, combining MLM-style bidirectional encoding where context is known with autoregressive sampling where novelty is needed. For therapeutic design, this enables specifying desired properties (binding target, expression level, stability) as encoded context while generating diverse candidate sequences that respect those constraints.\nThe cost of hybrid approaches is architectural complexity and training overhead. Pure MLM and pure autoregressive models have simpler implementations and clearer training dynamics. Whether the benefits of hybridization justify the costs depends on application requirements: tasks demanding both rich understanding and flexible generation may warrant the complexity, while tasks emphasizing one capability over the other are better served by the appropriate pure objective.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-denoising",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-denoising",
    "title": "8  Pretraining Strategies",
    "section": "8.4 Span Corruption and Denoising",
    "text": "8.4 Span Corruption and Denoising\nClinical variant interpretation must be robust to sequencing errors, population polymorphisms, and batch effects between discovery and validation cohorts. A pathogenic variant identified in a research study must remain classifiable as pathogenic when sequenced on a different platform in a clinical laboratory, surrounded by different technical artifacts and population-specific polymorphisms. A model trained only on pristine reference sequence may fail when encountering the noise and variation present in real patient data. Denoising objectives address this by training models on corrupted inputs, building tolerance to the kinds of perturbations that occur in clinical genomics pipelines.\nSpan corruption generalizes masked language modeling by introducing more complex forms of input degradation. The T5 model popularized this approach for natural language (Raffel et al. 2023), and the principles transfer to genomic sequences with biological adaptations. Rather than masking individual tokens, span corruption masks contiguous spans of variable length and replaces each span with a single sentinel token. The model then generates the original content of all masked spans in sequence, learning to reconstruct substantial missing regions rather than isolated positions.\nThis objective teaches different aspects of sequence structure than standard MLM. Reconstructing entire spans requires understanding longer-range dependencies and compositional patterns. If a span encompasses an entire transcription factor binding motif (typically 6-12 base pairs), the model cannot infer the motif from partial information and must instead reason about the motif’s role from surrounding regulatory context. Span lengths are typically sampled from a distribution (geometric or uniform) with a mean around 3-5 tokens, creating a mix of short and long reconstruction challenges within each training example.\n\n8.4.1 Corruption Beyond Masking\nDenoising objectives extend beyond masking to include other forms of corruption that mirror real-world data degradation. Token substitution replaces input tokens with random tokens from the vocabulary, creating corrupted sequences that resemble sequencing errors or natural variation. The model learns to distinguish correct from incorrect tokens based on surrounding context, encouraging representations that capture local consistency and motif structure. Deletion and insertion corruptions remove or add tokens at random positions, teaching models about position-invariant features that remain identifiable despite surrounding changes. For genomics, insertions and deletions are biologically realistic mutation types (indels account for approximately 15% of pathogenic variants in ClinVar (Landrum et al. 2018)), and models that handle them during pretraining may better predict their effects downstream.\n\n\n8.4.2 Biologically Motivated Corruption\nThe most effective corruption strategies mirror actual sources of noise in clinical genomics data. Simulating sequencing errors provides corruption strategies that match experimental reality. Base miscalls follow platform-specific patterns: Illumina sequencing shows characteristic substitution biases (favoring certain nucleotide transitions over transversions, with error rates of 0.1-1% depending on read position and quality score), while nanopore sequencing exhibits distinct error profiles concentrated in homopolymer regions where the signal for consecutive identical bases becomes ambiguous [Citation Needed]. Training with corruptions that mimic these error patterns may improve generalization to real sequencing data with platform-specific artifacts. The sequencing technologies producing these error patterns are examined in Chapter 1, while the confounding effects of platform-specific artifacts on model evaluation appear in Chapter 22.\nVariant augmentation introduces biologically realistic sequence changes based on population variation. Randomly substituting alleles at known polymorphic sites or injecting variants from databases like gnomAD creates corrupted sequences reflecting natural genetic diversity (Karczewski et al. 2020). This teaches models that common polymorphisms are normal variation rather than errors to be corrected, potentially improving robustness for variant effect prediction where distinguishing pathogenic variants from benign polymorphisms is the central challenge. A model trained only on reference sequence might flag any deviation as potentially damaging; a model trained with variant augmentation learns which deviations are within normal population variation.\nStructural variation simulation models larger-scale genomic changes: tandem duplications, copy number variation, and segmental rearrangements. These corruptions are harder to implement but capture realistic sources of genomic diversity beyond single-nucleotide changes. Models trained with structural variation corruptions may better understand how gene dosage changes, enhancer duplications, or domain boundary disruptions affect function. For clinical applications involving copy number variants (which underlie conditions ranging from developmental disorders like DiGeorge syndrome to cancer predisposition in hereditary breast cancer), this training signal could improve predictive accuracy.\nThe benefit of denoising pretraining extends to robustness under distribution shift. If downstream applications involve sequences from different populations, experimental platforms, or tissue contexts than the pretraining corpus, models pretrained with appropriate corruptions can maintain performance despite distribution mismatch. This matters in clinical genomics, where validation cohorts often differ from discovery cohorts in ancestry composition, sequencing technology, or phenotyping protocols. A model trained with corruptions spanning these sources of variation generalizes more reliably than one trained only on pristine reference sequence. The confounding and evaluation challenges arising from such distribution shifts are examined in Chapter 22 and Chapter 21.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-contrastive",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-contrastive",
    "title": "8  Pretraining Strategies",
    "section": "8.5 Contrastive Learning",
    "text": "8.5 Contrastive Learning\nCross-population generalization presents a persistent challenge in clinical genomics. A variant classifier trained on European ancestry cohorts may perform poorly on African ancestry patients due to different patterns of linkage disequilibrium and background polymorphism. The classifier learned to recognize pathogenic variants against a European genetic background; African genomes present the same functional variants but surrounded by different neutral polymorphisms. Contrastive learning addresses this by teaching models to recognize functional equivalence despite sequence-level differences, producing representations where a regulatory element is recognizable regardless of the population-specific variants surrounding it.\nContrastive learning takes a fundamentally different approach to self-supervised pretraining than reconstruction-based objectives. Rather than recovering corrupted inputs, contrastive objectives train models to produce similar representations for different views of the same sequence while distinguishing them from representations of unrelated sequences. The intuition is that augmented versions of a sequence (with minor corruptions, reverse complementation, or variants) should map to nearby points in representation space, while unrelated sequences should map to distant points. This teaches invariance to transformations that do not change function.\nThe algorithmic framework constructs positive pairs and negative samples. For a given anchor sequence, positive pairs are created through augmentation: reverse complementation, random cropping, variant injection, or other transformations that preserve functional identity. Negative samples are drawn from other sequences in the training batch. The model produces embeddings for all sequences, and the contrastive loss encourages anchor and positive embeddings to be similar (high cosine similarity) while pushing apart anchor and negative embeddings.\nInfoNCE loss is the most common contrastive objective (Oord, Li, and Vinyals 2019). For an anchor embedding \\(z_i\\) and positive embedding \\(z_i^{+}\\), InfoNCE maximizes:\n\\[\n\\mathcal{L} = -\\log \\frac{\\exp\\!\\left(z_i \\cdot z_i^{+} / \\tau\\right)}{\\sum_{j} \\exp\\!\\left(z_i \\cdot z_{j} / \\tau\\right)}\n\\]\nwhere the sum runs over the positive and all negative samples, and \\(\\tau\\) is a temperature parameter controlling the sharpness of the distribution. Lower temperatures make the model more discriminative, requiring cleaner separation between positives and negatives. The objective is equivalent to classifying the positive pair among all possible pairs, and the model learns representations that make this classification easy.\n\n8.5.1 Augmentation Design for Genomic Sequences\nA CTCF binding site must be recognizable whether it appears on a European or African genetic background, whether read on the forward or reverse strand, and whether the surrounding sequence contains common polymorphisms or reference alleles. Augmentation design is critical for contrastive learning because augmentations must preserve functional identity while introducing variability. If augmentations change function, the contrastive objective will learn meaningless invariances. Several augmentation strategies are biologically grounded and preserve the functional relationships that matter for downstream clinical applications.\nThe double-stranded nature of DNA provides the simplest and most reliable augmentation. Many regulatory elements function identically on either strand, and a model that fails to recognize this symmetry has learned an incomplete representation of genomic function. Reverse complementation trains the model to treat forward and reverse complement sequences as equivalent, capturing strand symmetry inherent in molecular biology. This augmentation is universally applicable and introduces no risk of changing functional identity; a TATA box is a TATA box regardless of which strand is reported.\nPosition invariance presents a subtler challenge. A transcription factor binding site should be recognizable regardless of where it falls within an input window, yet models naturally learn position-specific features. Random cropping addresses this by extracting overlapping windows from longer sequences. If a binding site appears in multiple cropped windows at different positions, the model learns that the site itself is the functionally relevant feature, not its coordinates. This proves particularly useful for tasks where genomic location matters less than local sequence content. The augmentation also provides practical benefits: a single long sequence becomes many training examples, increasing effective data diversity without collecting new data.\nPopulation diversity creates perhaps the most clinically consequential augmentation challenge. A classifier trained only on reference sequence may treat any deviation as potentially significant, when in fact most human genetic variation is neutral. Variant injection addresses this by introducing common polymorphisms or simulated mutations as augmentation. If the variants are neutral (common variants from gnomAD with high allele frequency, which are unlikely to be damaging; see Chapter 2 for gnomAD resource details), treating variant and reference sequences as positive pairs teaches robustness to genetic background. This is particularly valuable for cross-population generalization, where models must recognize functional elements despite surrounding sequence polymorphism that differs between ancestry groups. A model trained with variant augmentation learns that a CTCF binding site is functionally equivalent whether it appears on European or African genetic background.\nThe choice of negative samples shapes what distinctions the model learns to make. Random genomic sequences provide straightforward negatives but may be too easy to distinguish: any functional regulatory sequence is readily separable from random intergenic sequence. Harder negatives force more informative learning. Sequences from paralogous genes share evolutionary history but have diverged in function; distinguishing them requires learning subtle functional signatures. Pseudogenes resemble their functional counterparts but lack activity; recognizing this difference teaches the model what makes a gene functional. Orthologous regions in distant species test whether the model has learned species-invariant features. The difficulty of negatives should match the granularity of distinctions required for downstream tasks.\n\n\n8.5.2 Cross-Species Contrastive Learning\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 8.4: [Enhancing] Illustration of contrastive pretraining using orthologous sequences. Show: Human enhancer sequence and mouse ortholog (aligned, showing nucleotide divergence but conserved functional elements). Both mapped to embedding space as nearby points (positive pair). Non-orthologous sequence mapped to distant point (negative). Include phylogenetic context showing ~75 million years of divergence. Annotate: “Same function despite sequence divergence → learns species-invariant features.”\n\n\n\nLeveraging evolutionary relationships for self-supervision enables a distinctive form of contrastive learning. Orthologous sequences from different species share functional identity despite nucleotide divergence accumulated over millions of years of evolution. Treating orthologous pairs as positives and non-orthologous pairs as negatives teaches the model to extract species-invariant functional features. A human enhancer and its mouse ortholog should map to similar embeddings despite 75 million years of sequence divergence, while unrelated sequences should map to distant embeddings.\nThis approach has direct implications for drug development and therapeutic translation. Many drug targets are validated in mouse models before human clinical trials; roughly 95% of cancer drugs that succeed in mouse models fail in human trials, often because the models do not adequately capture human biology [Citation Needed]. A model pretrained with human-mouse contrastive pairs may generalize better to predicting drug response in humans based on mouse efficacy data, or to transferring regulatory circuit designs from model organisms to human cell types. The evolutionary record provides implicit labels about functional equivalence that would be expensive to obtain through direct experimental annotation.\nSequence embedding quality improves with contrastive pretraining in ways that benefit clinical applications. Models trained contrastively produce embedding spaces where functionally similar sequences cluster together, enabling nearest-neighbor search for annotating novel variants (finding similar characterized variants), sequence retrieval for identifying regulatory homologs, and unsupervised clustering of regulatory elements. For variant effect prediction, contrastive pretraining improves robustness: if the model learns that sequences differing only by neutral variants are functionally equivalent, it will better distinguish truly disruptive variants from benign polymorphisms.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-multitask",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-multitask",
    "title": "8  Pretraining Strategies",
    "section": "8.6 Multi-Task Pretraining",
    "text": "8.6 Multi-Task Pretraining\nPredicting variant pathogenicity requires integrating multiple lines of evidence: evolutionary conservation, protein structure effects, splicing changes, and regulatory disruption. A variant in TTN (the gene encoding titin, mutated in 25% of dilated cardiomyopathy cases [Citation Needed]) might be pathogenic because it disrupts protein folding, because it alters splicing, or because it affects regulatory binding sites. No single assay captures all these dimensions. Multi-task pretraining addresses this by jointly optimizing for diverse prediction tasks, learning representations that capture the multiple facets of genomic function relevant to clinical interpretation.\nMulti-task pretraining combines multiple related objectives during the same training run, jointly optimizing for several prediction tasks. Different tasks provide complementary supervision signals: masking captures local sequence patterns, chromatin prediction captures regulatory function, conservation scoring captures evolutionary constraint, and expression prediction captures transcriptional consequences. Representations that satisfy all tasks simultaneously develop richer and more general features than any single objective alone.\n\n8.6.1 Task Selection and Architecture\nThe first design decision is which tasks to include. Ideally, tasks should be diverse enough to provide distinct supervision signals but related enough to benefit from shared representations. For genomic models, effective combinations include masked language modeling for general sequence structure, chromatin accessibility prediction for regulatory function, gene expression prediction for transcriptional output, evolutionary conservation scoring for functional constraint, and variant frequency prediction from population databases. Each task operates on the same input sequence but predicts different outputs using task-specific head layers. The shared backbone encoder processes the sequence into intermediate representations, and separate prediction heads map these representations to task-specific outputs.\n\n\n8.6.2 Loss Weighting and Balancing\nOnce tasks are selected, their relative contributions to the total loss must be determined. With \\(\\mathcal{L}_1, \\ldots, \\mathcal{L}_K\\) representing individual task losses, the multi-task loss combines them:\n\\[\\mathcal{L}_{\\text{total}} = \\sum_{k=1}^K w_k \\mathcal{L}_k\\]\nwhere w_k are task weights. Equal weighting is simple but may lead to imbalanced learning if tasks have different scales or difficulties. A task with high variance loss may dominate gradient updates, starving other tasks of learning signal. Dynamic weighting approaches adjust weights during training based on learning progress, using uncertainty estimation, gradient norms, or task-specific validation performance as signals for rebalancing. Uncertainty-based weighting learns task weights as parameters, treating high-loss tasks as inherently more uncertain and down-weighting their contribution. Gradient-based methods normalize gradients across tasks to prevent any single task from dominating updates.\n\n\n8.6.3 Large-Scale Multi-Task Examples\nEnformer exemplifies large-scale multi-task pretraining for genomics (Avsec et al. 2021). The model predicts over 5,000 genomic assays simultaneously: ChIP-seq signals for hundreds of transcription factors and histone marks, DNase-seq and ATAC-seq accessibility across cell types, CAGE transcription initiation profiles, and more. This massive multi-task objective (covering 674 DNase-seq, 4,675 ChIP-seq, and 638 CAGE experiments from ENCODE and Roadmap Epigenomics (Kagda et al. 2025)) forces the model to learn representations capturing diverse regulatory signals.\nThe task diversity in Enformer provides supervision far richer than any single assay. A model trained only on DNase-seq learns general accessibility patterns but misses transcription factor specificity: it cannot distinguish which factors bind to accessible regions. A model trained only on H3K27ac ChIP-seq captures active enhancers but misses repressive marks that indicate silenced regulatory elements. Training on all assays jointly allows the model to disentangle overlapping and complementary signals, learning representations that generalize across regulatory contexts. For clinical variant interpretation, this means Enformer can predict how a regulatory variant affects enhancer activity, chromatin state, transcription factor binding, and gene expression simultaneously. Chapter 13 examines Enformer and related regulatory models in detail.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 8.5: [High] Diagram showing shared encoder with multiple prediction heads. Structure: Sequence input → Convolutional layers → Transformer layers (labeled “Shared Backbone”) → Branching to separate heads for: Chromatin accessibility (DNase-seq, ATAC-seq), Histone modifications (H3K27ac, H3K4me3, etc.), Transcription factor binding (hundreds of factors), Gene expression (CAGE). Annotate approximate task counts from ENCODE: “674 DNase + 4,675 ChIP-seq + 638 CAGE.”\n\n\n\nBorzoi extends this paradigm to full RNA-seq coverage prediction, jointly modeling transcription initiation, splicing, and transcript abundance (Linder et al. 2025). By predicting continuous coverage across gene bodies rather than just expression levels, Borzoi captures splicing patterns that are invisible to models predicting only total expression. This has direct clinical relevance: many pathogenic variants act through splicing disruption rather than protein-coding changes, and models that capture splicing patterns can identify variants that traditional expression-based approaches miss.\nCombining MLM with functional prediction represents another multi-task configuration. The model predicts masked tokens through a language modeling head while simultaneously predicting chromatin accessibility or other functional readouts through regression heads. This hybrid objective balances sequence-level pretraining with functional supervision. The MLM component ensures the model learns general sequence patterns even in regions without functional annotations (the majority of the genome lacks chromatin or expression measurements in any given cell type), while the functional prediction component focuses learning on biologically relevant features.\n\n\n8.6.4 When Multi-Task Learning Fails\nTask interference presents the primary concern with multi-task learning. If tasks require conflicting representations, jointly optimizing for both may compromise performance on each compared to single-task baselines. In genomics, this might occur if one task benefits from very local features (splice site prediction, which depends on short consensus sequences spanning roughly 10 base pairs) while another requires long-range context (enhancer activity prediction, which depends on distant promoter interactions spanning 100 kilobases). The shared backbone must compromise, potentially learning suboptimal representations for both.\nNegative transfer occurs when adding a task actually hurts downstream performance compared to training without it. This can happen if the additional task introduces noise (poorly measured assays with high experimental variance), if task weights are poorly balanced (causing one task to dominate gradients), or if the auxiliary task shifts learned representations away from features useful for target applications. The risk of negative transfer increases with task diversity: distantly related tasks are more likely to require conflicting representations.\nThe benefits of multi-task pretraining are largest when tasks are complementary and data for individual tasks is limited. If chromatin data is sparse for a particular cell type but gene expression data is abundant, jointly training on both may improve performance on both compared to single-task models. The shared representations allow information to flow between tasks, compensating for data scarcity in any single modality. When functional labels exist at scale and tasks are genuinely related, multi-task pretraining consistently outperforms single-task alternatives.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-staged",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-staged",
    "title": "8  Pretraining Strategies",
    "section": "8.7 Staged Pretraining Strategies",
    "text": "8.7 Staged Pretraining Strategies\nTraining a foundation model in a single phase rarely produces optimal results. The computational constraints that make pretraining expensive also make experimentation prohibitive: once committed to training at scale, practitioners cannot easily adjust hyperparameters, data mixtures, or objectives mid-run. Staged pretraining addresses this by decomposing training into sequential phases, each optimized for different learning goals. A model might learn basic sequence statistics from shorter contexts before extending to long-range dependencies, or acquire general sequence grammar before specializing to regulatory regions. These staged approaches improve both training stability and final model quality compared to monolithic training on the full data and context from the start.\nThe biological rationale mirrors curriculum learning in human education: master fundamentals before tackling advanced material. A medical student learns anatomy before pathology; a genomic model might learn local motif structure before enhancer-promoter interactions spanning 100 kilobases. When HyenaDNA attempted direct training on million-base contexts, optimization diverged. Progressive context extension, starting at shorter windows and gradually increasing, proved essential for stable learning (Nguyen et al. 2023). This curriculum effect appears across architectures: the structure of what is learned first shapes what can be learned later.\n\n8.7.1 Context Length Curricula\nLong-range genomic dependencies present a fundamental training challenge. Attention mechanisms scale quadratically with context length, making training on long sequences orders of magnitude more expensive than short sequences. Beyond computational cost, optimization dynamics change with context length: models processing thousands of tokens face different gradient distributions than those processing hundreds. Context length curricula address both challenges by training first on tractable short contexts, then progressively extending to longer sequences.\nHyenaDNA exemplifies this approach. Initial pretraining used contexts of a few thousand bases, allowing rapid iteration through the genome and stable optimization. As training progressed, context windows expanded through intermediate stages (8 kilobases, then 32 kilobases) until reaching the target of one million bases. Each stage inherited weights from the previous stage, with learning rate warmup to accommodate the new context regime. The curriculum proved necessary for convergence: ablations attempting direct long-context training without warmup phases showed instability and degraded final performance.\nGene42 extended this pattern with explicit continuous pretraining stages (gene42_2024?). The model trained initially at 4,096 tokens, then continued pretraining at 8,192, 16,384, 32,768, 65,536, and finally 192,000 tokens. Each context extension required adjustments to positional encodings (specifically, modifications to rotary position embedding parameters to prevent distant-token interactions from collapsing). The staged approach enabled dense attention at scales where training from scratch would be computationally prohibitive. Notably, the longest-context checkpoints required only incremental compute beyond the shorter-context stages, amortizing the total training cost across the curriculum.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 8.6: [High] Diagram showing context length curriculum progression. X-axis: Training steps (or compute). Y-axis: Context length (log scale). Show stepped increases from 1kb → 4kb → 16kb → 64kb → 256kb, with annotations at each stage showing: “Stable optimization at short context” → “Weight initialization from previous stage” → “Learning rate warmup” → “Extended training at new context.” Include inset showing attention pattern differences at short vs. long context to illustrate why progressive extension helps.\n\n\n\nThe mechanism underlying context curricula relates to how attention patterns develop. Early in training, attention distributions are nearly uniform (each position attends similarly to all others). As learning progresses, sparse, structured attention patterns emerge: promoter positions attend to enhancer regions; splice site positions attend to branch points. These structured patterns require many training steps to develop. Starting at long contexts forces the model to learn both basic sequence statistics and long-range structure simultaneously, competing objectives that can interfere. The curriculum separates these learning phases: master local patterns first (at short context), then learn to integrate them across distance (at extended context).\n\n\n8.7.2 Domain-Adaptive Pretraining\nWhen should a genomic model build on existing pretrained weights versus train from scratch? The question parallels a broader tension in NLP, where domain-adaptive pretraining (continuing training from a general-domain checkpoint on domain-specific data) competes with from-scratch domain pretraining (training exclusively on domain-specific data). The answer depends on data abundance, domain distance, and whether vocabulary transfer is feasible.\nBioBERT pioneered domain-adaptive pretraining for biomedical text, initializing from general-domain BERT weights and continuing pretraining on PubMed abstracts (lee_biobert_2020?). This approach leverages general language understanding (syntax, semantics, common knowledge) acquired during the initial pretraining phase, requiring only adaptation to domain-specific vocabulary and concepts. The strategy proved effective when biomedical data was limited and general-domain pretraining captured useful structure.\nPubMedBERT challenged this assumption by demonstrating that from-scratch pretraining on biomedical text alone could outperform domain-adaptive approaches when sufficient domain data exists (gu_pubmedbert_2021?). The key insight was vocabulary mismatch: general-domain tokenizers fragment biomedical terms into meaningless subwords (“lymphoma” becomes “l”, “##ym”, “##ph”, “##oma”), forcing the model to reconstruct meaning from pieces rather than representing concepts directly. Training from scratch with a domain-specific vocabulary eliminated this overhead. When domain-specific data is abundant (as with PubMed’s millions of abstracts), the benefits of general-domain initialization may not justify the vocabulary mismatch cost.\nFor genomic foundation models, these lessons translate directly. DNA sequence tokenizers (k-mers, BPE, single nucleotides) differ fundamentally from text tokenizers, making vocabulary transfer impossible. A general-purpose language model cannot serve as initialization for a DNA model; sequence statistics must be learned from genomic data. The relevant decision becomes whether to continue pretraining a genomic model on new data (adding species, adding functional annotations) or train a new model from scratch.\n\n\n8.7.3 Continued Pretraining on Expanded Data\nAs new genomic data becomes available (additional reference genomes, expanded population sequencing, new functional assays), practitioners face a choice: retrain from scratch incorporating all data, or continue pretraining the existing model on new data. Continued pretraining offers computational efficiency but risks catastrophic forgetting, where learning new patterns overwrites previously acquired knowledge.\nThe risk of catastrophic forgetting is real but manageable. When DNABERT checkpoints are continued on new species, performance on original species may degrade if the new training distribution differs substantially. Mitigation strategies include replay (mixing old and new data during continued pretraining), elastic weight consolidation (penalizing changes to weights important for prior tasks), and modular architectures that isolate new learning from established representations (McCloskey and Cohen 1989).\nContinued pretraining makes sense when new data complements rather than contradicts prior training. Adding closely related species to a model pretrained on mammals will likely transfer well; adding bacterial genomes with fundamentally different GC content, codon usage, and regulatory logic may require more careful integration or separate models. The decision should be guided by biological similarity between old and new data distributions.\n\n\n8.7.4 Multi-Objective Schedules\nBeyond data and context curricula, the pretraining objective itself can be staged. A model might train with masked language modeling to learn sequence statistics, then switch to or add contrastive objectives to learn functional similarity, then incorporate task-specific prediction heads. Each objective teaches different aspects of sequence function.\nDNABERT-S demonstrates staged objective curricula (zhou_dnabert-s_2024?). The model’s Curriculum Contrastive Learning (C²LR) strategy divides training into two phases. Phase I applies standard contrastive learning (distinguishing similar from dissimilar sequences using straightforward positive and negative pairs). Phase II introduces harder anchors through Manifold Instance Mixup, creating challenging training examples by mixing hidden representations at random layers. The curriculum ensures the model first masters basic discrimination before tackling the more difficult mixed-representation task.\nMulti-task schedules represent another form of objective staging. Rather than training all tasks jointly from the start, some practitioners introduce tasks sequentially: begin with the primary self-supervised objective, then add auxiliary tasks once representations have stabilized. This staging prevents auxiliary tasks from dominating early learning when the model has not yet acquired basic sequence understanding. The optimal schedule depends on task interactions: complementary tasks (MLM plus chromatin prediction) may benefit from joint training, while potentially conflicting tasks (short-range splice prediction plus long-range enhancer prediction) may benefit from staging.\n\n\n8.7.5 Data Complexity Curricula\nNot all genomic sequences present equal learning difficulty. Repetitive regions offer little to learn beyond detecting repeats; complex regulatory regions require learning combinatorial motif grammar; intergenic regions provide evolutionary constraint signal distinct from coding regions. Data complexity curricula order training examples from simple to complex, allowing models to build representations progressively.\nComplexity ordering can be implicit or explicit. Implicit ordering emerges from data sampling: if training oversamples certain regions early (promoters, conserved sequences), the model learns those patterns first. Explicit ordering requires defining complexity metrics (sequence entropy, motif density, evolutionary conservation, expression variability) and scheduling examples accordingly. While less explored in genomics than context curricula, data complexity scheduling offers potential for improving sample efficiency, particularly when some sequence classes are over-represented in training corpora.\n\n\n8.7.6 Practical Considerations\nStaged pretraining introduces complexity that must be weighed against benefits. Each stage requires decisions about duration (training steps or epochs), transition criteria (loss plateaus, validation metrics), learning rate schedules (warmup for each stage, decay patterns), and checkpoint selection (which intermediate checkpoint to continue from). Poor choices at stage transitions can negate the benefits of staging.\nDiagnostic monitoring becomes more important with staged training. Track not only aggregate loss but per-stage metrics: does performance on short-context tasks degrade when extending to long contexts? Do earlier-stage representations remain useful? Does adding new data cause forgetting of prior patterns? These diagnostics require evaluation infrastructure beyond simple loss tracking but provide essential feedback for curriculum design.\nThe computational tradeoffs favor staging in most scenarios. Training a single long-context model from scratch requires expensive long-sequence batches for the entire training run. Staged training front-loads cheap short-context training, investing in expensive long-context training only after the model has learned basic patterns. The total compute may be similar or even higher with staging, but the amortized cost per useful representation is often lower because more learning happens during the efficient early stages.\nWhen staged pretraining fails, the causes typically involve poor stage transitions or misaligned curricula. If later stages require unlearning early-stage representations (because the curriculum taught the wrong patterns first), staging may harm rather than help. Careful alignment between curriculum structure and intended final capabilities remains essential. The goal is not staging for its own sake but decomposing a difficult learning problem into tractable sequential subproblems.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-data",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-data",
    "title": "8  Pretraining Strategies",
    "section": "8.8 Data Strategies for Pretraining",
    "text": "8.8 Data Strategies for Pretraining\nCorpus construction establishes the foundation for pretraining and determines what patterns the model can learn. A clinical variant classifier is only as good as the evolutionary and population diversity captured in its pretraining corpus. If the training data underrepresents African genetic variation (African populations harbor more genetic diversity than all other continental populations combined, yet constitute a small fraction of most reference panels [Citation Needed]), the resulting model will underperform on African ancestry patients. These data decisions have direct consequences for health equity and clinical utility.\n\n8.8.1 Reference Genomes and Population Diversity\nHuman genome assemblies like GRCh38 provide the standard starting point, offering high-quality, contiguous sequence spanning all chromosomes (roughly 3.1 billion base pairs of assembled sequence, representing about 92% of the full genome before telomere-to-telomere completion [Citation Needed]). Training on the reference genome allows models to learn patterns characteristic of human DNA: base composition, repeat structure, gene organization, and regulatory architecture. The reference genome represents a single haploid consensus, missing variation present in human populations, but provides the foundation for most pretraining approaches.\nPopulation-scale variation can be incorporated through variant databases. Rather than training only on reference sequence, injecting variants at observed population frequencies creates synthetic diploid genomes reflecting real genetic diversity. This teaches models that common polymorphisms are normal variation, potentially improving robustness and variant effect prediction. gnomAD provides allele frequencies across over 800,000 individuals spanning diverse ancestries, enabling population-aware training. Pan-genome approaches extend this by representing multiple high-quality assemblies from diverse individuals, capturing structural variation and population-specific haplotypes that a single reference cannot represent (Karczewski et al. 2020). Chapter 2 examines these data resources and their construction in detail.\n\n\n8.8.2 Repeat Handling\nRepeat handling impacts pretraining in ways that depend on downstream applications. Simple repeats, tandem repeats, and transposable elements occupy roughly half of the human genome but contribute less directly to protein-coding function than unique sequences [Citation Needed]. Hard-masking repeats (replacing repetitive bases with N characters, rendering ATCGATCGATCG as NNNNNNNNNNNN) reduces training data but may discard information relevant to some tasks; many regulatory elements derive from transposable elements, and some disease-associated repeats (like the CGG expansion in FMR1 causing Fragile X syndrome, or the CAG expansion in HTT causing Huntington disease) are clinically important. Soft-masking retains sequence information while using lowercase to flag repetitive regions (atcgatcgatcg), allowing models to learn differential representations for repeats and unique sequences. Tools like RepeatMasker produce these annotations, and training pipelines can be configured to treat masked regions differently: exclude them entirely, downweight their contribution to loss, or process them normally while preserving the distinction in tokenization.\n\n\n8.8.3 Multi-Species and Augmentation Strategies\nIncorporating genomes from model organisms and related species enables models to learn evolutionary conservation patterns and may improve transfer between species. Including mouse, zebrafish, and other commonly used experimental organisms provides training signal about which sequence features are functionally constrained across evolution. For therapeutic development that relies on animal model data, multi-species pretraining provides the foundation for cross-species generalization.\nData augmentation strategies (see Section 8.5.1) complement multi-species training by artificially increasing diversity within species. These augmentations are typically applied on-the-fly during training rather than pre-computed, maintaining flexibility in the training pipeline and ensuring the model sees different augmented versions across epochs.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-optimization",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-optimization",
    "title": "8  Pretraining Strategies",
    "section": "8.9 Optimization and Scaling",
    "text": "8.9 Optimization and Scaling\nTraining a model to predict variant effects in genes like BRCA1 requires not just the right objective but also stable optimization that converges to useful representations. A model that diverges during training or gets stuck in poor local minima will fail clinically regardless of how well-designed its architecture may be. The optimization details that seem merely technical have direct consequences for whether the final model can reliably distinguish pathogenic from benign variants.\n\n8.9.1 Optimization Hyperparameters\nStable training requires careful attention to learning rate scheduling, gradient management, and numerical precision. Learning rate warmup gradually increases the learning rate from near-zero over the first several thousand steps, preventing early training instability when the model has random initializations and large gradient variance. After warmup, cosine decay schedules reduce the learning rate following a cosine curve from peak to near-zero over training, providing aggressive learning early when gradients are most informative and gentle refinement late as the model approaches convergence.\nGradient clipping (see Section 7.6.3) uses a norm threshold of 1.0 in most genomic pretraining configurations. Without clipping, a single anomalous batch can destabilize training irreversibly.\nModern pretraining relies on mixed precision arithmetic (float16 or bfloat16 instead of float32) to reduce memory consumption and accelerate computation on modern GPUs. Loss scaling prevents numerical underflow in float16, and careful handling of gradient updates ensures stability. Mixed precision is now standard for large-scale pretraining, roughly doubling throughput with minimal impact on model quality.\n\n\n8.9.2 Scaling Laws and Emergence\nPretraining scales with model size, sequence length, and dataset size in predictable ways that have profound implications for what models can learn. Larger models with more parameters capture more complex patterns but require more data and compute to train. ESM-2’s largest variant has 15 billion parameters (Lin et al. 2022) (roughly one parameter for every two amino acids in its training corpus), enabling it to capture subtle evolutionary constraints invisible to smaller models. Longer sequence contexts enable learning of long-range dependencies but increase memory requirements quadratically for standard attention. More diverse training data improves generalization but requires proportionally more training time.\nThe relationships between scale and capability follow power laws that predict optimal resource allocation (Hoffmann et al. 2022). For a fixed computational budget, there exists an optimal balance between model size and training data: models that are too large undertrain on available data, while models that are too small cannot capture the complexity present in abundant data. These scaling laws, first characterized systematically for language models (Kaplan et al. 2020), appear to hold for genomic foundation models as well, though the precise exponents and constants differ. Understanding these relationships guides decisions about when to scale up versus when to improve data quality or model architecture. Chapter 10 examines these scaling relationships in detail, formalizing the observations introduced here into quantitative laws that define the foundation model paradigm.\nThese empirical scaling laws contradict classical intuitions from statistical learning theory. The Vapnik-Chervonenkis framework predicts generalization error scaling as \\(O(1/\\sqrt{N})\\): halving the error requires quadrupling the training data, with model complexity strictly bounded by available examples (vapnik_statistical_1998?). Models with parameters vastly exceeding training examples should memorize rather than generalize. Yet foundation models operate precisely in this “overparameterized” regime and still improve predictably with scale. This benign overfitting reflects properties of gradient descent and high-dimensional loss landscapes that classical worst-case bounds did not anticipate (zhang_understanding_2017?; belkin_reconciling_2019?).\nBeyond smooth improvements in loss, scale produces qualitative changes in model capabilities that were absent at smaller scales. Language models exhibit emergent behaviors (in-context learning, chain-of-thought reasoning, few-shot generalization) that appear only above certain parameter thresholds [Citation Needed]. Whether genomic models exhibit analogous emergent capabilities remains an active research question with early evidence suggesting they do. ESM-2, trained on evolutionary sequence databases containing hundreds of millions of protein sequences from UniRef (Suzek et al. 2007), develops structural understanding of proteins despite receiving no explicit structural supervision: the three-dimensional contacts emerge from predicting amino acid sequences alone. Evo, trained autoregressively on genomes, learns to generate sequences with realistic gene structure and regulatory organization. These emergent properties cannot be predicted by extrapolating from smaller models, making them both scientifically interesting and practically difficult to anticipate.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-diagnostics",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-diagnostics",
    "title": "8  Pretraining Strategies",
    "section": "8.10 Training Diagnostics",
    "text": "8.10 Training Diagnostics\nA two-week pretraining run that begins diverging on day three but is not detected until day thirteen wastes ten days of compute and forces rollback to earlier checkpoints. The failure is not losing everything; it’s continuing to train a model that stopped learning useful representations long before anyone noticed. Early detection of training issues is essential for avoiding wasted computation and ensuring models achieve the representations necessary for clinical utility.\n\n8.10.1 Monitoring Loss and Gradients\nTraining loss curves should decrease smoothly in early stages, eventually plateauing as the model approaches convergence. Sudden spikes suggest numerical instability (often from learning rate issues or gradient explosion), inappropriate optimization hyperparameters, or corrupted data batches. Persistent plateaus may indicate insufficient model capacity, inappropriate objectives, or learning rates that prevent further improvement. Tracking loss on held-out validation data monitors generalization: if training loss decreases while validation loss increases, the model is overfitting to the training corpus.\nGradient norms indicate whether optimization is proceeding normally. Very small gradients suggest the vanishing gradient problem, preventing effective learning in early layers. Very large gradients suggest instability that gradient clipping should catch. Tracking per-layer gradient norms helps diagnose where problems originate in deep networks; if early layers show vanishing gradients while later layers have healthy magnitudes, the architecture may need residual connections or different initialization.\n\n\n8.10.2 Functional Probing\nProbing tasks provide functional sanity checks during pretraining that loss curves alone cannot capture. Simple downstream evaluations (predicting known splice sites, identifying transcription factor binding motifs, distinguishing exons from introns) can be run periodically on intermediate checkpoints to verify that learned representations capture biologically meaningful patterns. If probing performance plateaus or degrades while pretraining loss continues improving, the model may be learning patterns that do not transfer to downstream tasks. This dissociation between pretraining loss and probe performance signals a problem with the pretraining objective or data that would otherwise go undetected until final evaluation.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-selection",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-selection",
    "title": "8  Pretraining Strategies",
    "section": "8.11 Strategy Selection",
    "text": "8.11 Strategy Selection\nA clinician asking “will this BRCA1 variant cause disease?” needs a model pretrained with objectives that capture protein function and evolutionary constraint. A synthetic biologist asking “can you design me a promoter with 10-fold higher expression?” needs generative capabilities that MLM does not provide. Selecting a pretraining approach involves matching computational investment to the clinical or research questions the model must ultimately answer.\nFor most general-purpose DNA or protein models, MLM pretraining provides a strong default. It learns bidirectional context, scales efficiently, and transfers well to diverse downstream tasks. DNABERT and DNABERT-2 exemplify this approach for genomics, while ESM models demonstrate its effectiveness for proteins. Start with MLM unless there is a specific reason to prefer alternatives.\nNext-token prediction is preferred when generation is the primary goal. If designing sequences from scratch (therapeutic proteins, synthetic promoters, regulatory circuits), sampling from autoregressive models produces coherent outputs respecting learned grammar. Evo and similar models demonstrate this for genomic sequence generation. The autoregressive structure makes conditional generation straightforward, enabling design applications that MLM does not naturally support.\nMulti-task pretraining makes sense when functional labels are available at scale and tasks are complementary. Enformer’s success with thousands of chromatin assays demonstrates the power of multi-task learning when data supports it. The infrastructure requirements are higher (handling heterogeneous data, balancing losses across tasks, maintaining separate prediction heads), but the resulting representations capture functional information that pure sequence-based objectives miss.\nContrastive learning is valuable for cross-species applications or when robustness to variation is critical. If transferring models trained on model organisms to related species, or improving robustness to genetic polymorphism across human populations, contrastive pretraining on orthologous pairs or variant-augmented sequences provides targeted benefits.\nWhen deciding whether to pretrain from scratch or start from existing models, starting from pretrained checkpoints is almost always preferable if an appropriate model exists. Fine-tuning a DNABERT-2 checkpoint on a new task is faster and more data-efficient than training from scratch. Pretraining from scratch is necessary when using new tokenization schemes (incompatible vocabularies prevent weight transfer), targeting species without suitable existing models, or experimenting with fundamentally different architectures where pretrained weights cannot transfer. Chapter 9 examines these adaptation strategies in detail.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-case-studies",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-case-studies",
    "title": "8  Pretraining Strategies",
    "section": "8.12 Pretraining in Practice: Case Studies",
    "text": "8.12 Pretraining in Practice: Case Studies\nExamining how successful models were pretrained provides concrete lessons and design patterns that inform new projects. Each case study illustrates how architectural choices, data decisions, and optimization strategies combine to produce models with distinct capabilities.\n\n8.12.1 DNABERT\nDNABERT introduced MLM pretraining to genomics by adapting BERT’s architecture to DNA sequences with overlapping \\(k\\)-mer tokenization (Ji et al. 2021). The model was pretrained on the human genome with 6-mer tokens, masking 15% of tokens at random. Standard BERT hyperparameters proved effective: AdamW optimizer with warmup, dropout regularization, and layer normalization. The key lessons include the importance of tokenization choice (k-mers capture motif-level patterns better than single nucleotides for regulatory prediction), the value of reverse complement augmentation for strand symmetry, and the transferability of representations across tasks never seen during pretraining. The full DNABERT architecture and its subsequent developments (DNABERT-2, DNABERT-S) are examined in Chapter 11.\n\n\n8.12.2 HyenaDNA\nHyenaDNA demonstrated that efficient long-range architectures enable pretraining on extremely long contexts (Nguyen et al. 2023). By using Hyena layers with subquadratic complexity, HyenaDNA scaled to contexts spanning one million bases (compared to typical transformer limits of a few thousand bases), far beyond standard transformers. Pretraining used single-nucleotide next-token prediction with a curriculum that progressively increased context length from shorter windows to full million-base sequences. This curriculum learning proved essential: training directly on long contexts without warmup led to instability. The lessons include the feasibility of million-base contexts with appropriate architectures, the benefits of curriculum learning for context scaling, and the emergence of long-range regulatory patterns when models have sufficient receptive field.\n\n\n8.12.3 Enformer\nEnformer pioneered multi-task chromatin prediction at scale (Avsec et al. 2021). The model was pretrained jointly on over 5,000 assays from ENCODE, Roadmap Epigenomics, and related consortia, using a hybrid convolutional-transformer architecture with 200 kilobase context (spanning typical enhancer-promoter distances in mammalian genomes). Task weighting was balanced to prevent any single assay from dominating. Key insights include the power of large-scale multi-task learning for capturing diverse regulatory signals, the effectiveness of combining convolutions for local patterns with transformers for long-range interactions, and the interpretability benefits of attention patterns that reveal learned enhancer-promoter relationships. Chapter 13 examines Enformer’s architecture and regulatory predictions in detail.\n\n\n8.12.4 ESM-2\nESM-2 represents the state of the art for protein language models, scaling to 15 billion parameters trained on UniRef databases containing sequences from hundreds of millions of protein families (Lin et al. 2022). Pretraining used standard MLM on amino acid sequences at unprecedented scale. The lessons include the continued benefit of scaling (larger models and more data improve even at billions of parameters, with no plateau in sight), the value of evolutionary diversity (pretraining on distinct protein families captures constraints invisible in any single genome), and the emergence of structural understanding from sequence alone (ESM-2 representations encode three-dimensional contacts despite no explicit structural supervision during pretraining). Chapter 12 examines ESM-2 and related protein language models comprehensively.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-open-questions",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-open-questions",
    "title": "8  Pretraining Strategies",
    "section": "8.13 Open Questions",
    "text": "8.13 Open Questions\nDespite rapid progress, fundamental questions about genomic pretraining remain open, and resolving them will determine whether the next generation of models can achieve clinical-grade reliability.\nOptimal objective combinations remain unclear: should we jointly train with MLM and chromatin prediction, or train sequentially? How many auxiliary tasks help before diminishing returns? Do contrastive and generative objectives complement each other or interfere? These questions have different answers for different downstream applications, and systematic characterization is incomplete.\nIncorporating biological priors versus learning from scratch presents a design tension. Known motifs, pathway structure, and evolutionary constraints could be encoded in model architecture or initialization. Hand-engineered features risk encoding false assumptions, but pure data-driven learning may rediscover basic biology inefficiently. Hybrid approaches combining priors with learned representations remain underexplored.\nContinual pretraining as new data arrives is increasingly relevant. As sequencing technologies improve and new assays emerge, updating pretrained models without catastrophic forgetting of prior knowledge presents challenges. Online learning and elastic weight consolidation are potential solutions that remain largely untested in genomics at scale.\nThe relationship between pretraining scale and downstream performance follows predictable patterns that are still being characterized for genomic models. Understanding these relationships more precisely would guide resource allocation and set realistic expectations for what different scales of pretraining can achieve. These scaling considerations connect to the broader foundation model paradigm examined in Chapter 10.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-sequence-to-knowledge",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-sequence-to-knowledge",
    "title": "8  Pretraining Strategies",
    "section": "8.14 From Sequence Statistics to Biological Knowledge",
    "text": "8.14 From Sequence Statistics to Biological Knowledge\nThe fundamental insight underlying self-supervised pretraining is that patterns relevant to biological function are embedded in sequence statistics themselves. A model that learns to predict masked nucleotides must implicitly capture the evolutionary constraints, regulatory grammar, and structural requirements that determine what sequences are viable. A model that learns to generate plausible protein sequences must internalize the constraints that distinguish functional proteins from random polymers. These objectives extract biological knowledge from sequence without requiring explicit functional labels, transforming abundant unlabeled data into learned representations that improve data efficiency for downstream applications.\nThe choice of pretraining objective shapes what models learn in ways that propagate to clinical utility. Masked language modeling teaches bidirectional sequence understanding, making it the natural choice for variant interpretation and regulatory prediction where full flanking context informs the prediction. Next-token prediction teaches generative capabilities essential for therapeutic protein design and synthetic sequence generation. Contrastive learning teaches invariance to perturbations, building robustness that transfers across species and populations. Aligning pretraining objectives with intended applications improves transfer; misalignment creates representational gaps that fine-tuning may struggle to bridge.\nSelf-supervised pretraining has become the default approach for building genomic foundation models. The DNA language models in Chapter 11, protein language models in Chapter 12, and regulatory sequence models in Chapter 13 each employ variants of these objectives tailored to their sequence modalities and downstream applications. The transfer learning methods examined in Chapter 9 determine how effectively pretrained representations can be adapted to specific clinical and research tasks, completing the pipeline from raw sequence through learned representation to deployed application.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. https://doi.org/10.48550/arXiv.1810.04805.\n\n\nFerruz, Noelia, Steffen Schmidt, and Birte Höcker. 2022. “ProtGPT2 Is a Deep Unsupervised Language Model for Protein Design.” Nature Communications 13 (1): 4348. https://doi.org/10.1038/s41467-022-32007-7.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2203.15556.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data Navigation on the ENCODE Portal.” Nature Communications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. “Scaling Laws for Neural Language Models.” arXiv. https://doi.org/10.48550/arXiv.2001.08361.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nMcCloskey, Michael, and Neal Cohen. 1989. “Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem.” In Psychology of Learning and Motivation, 24:109–65. Academic Press. https://doi.org/10.1016/S0079-7421(08)60536-8.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence Modeling and Design from Molecular to Genome Scale with Evo.” Science 386 (6723): eado9336. https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nOord, Aaron van den, Yazhe Li, and Oriol Vinyals. 2019. “Representation Learning with Contrastive Predictive Coding.” arXiv. https://doi.org/10.48550/arXiv.1807.03748.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” arXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. “UniRef: Comprehensive and Non-Redundant UniProt Reference Clusters.” Bioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html",
    "href": "part_2/p2-ch09-transfer.html",
    "title": "9  Transfer and Adaptation",
    "section": "",
    "text": "9.1 Source and Target Domains\nTransfer learning fails as often as it succeeds, and the failures are silent. A protein language model trained on human sequences may confidently score variants in mouse orthologs, producing predictions that look reasonable but reflect human-specific evolutionary pressures irrelevant to mouse biology. A foundation model pretrained on coding sequences may extract features actively misleading for noncoding regulatory elements. A classifier achieving 90% accuracy on common variants may collapse to chance performance on the rare variants that matter most clinically. Nothing in the model’s outputs signals these failures. The predictions look the same whether transfer has succeeded or catastrophically failed. This asymmetry between confident outputs and actual reliability creates the central methodological challenge of applying pretrained models: detecting when transfer works and when it does not, before the predictions reach clinical applications where failures have consequences.\nThe promise of transfer learning is substantial. Foundation models trained on billions of evolutionary sequences learn representations that capture protein structure, functional constraints, and sequence grammar without task-specific supervision (see Chapter 8). When these representations are applied to downstream tasks with limited labeled data, they can achieve performance that would be impossible for models trained from scratch. A variant effect predictor fine-tuned from ESM-2 can classify novel missense mutations using patterns learned from the entire protein universe, not just the handful of variants with clinical annotations. This capacity to generalize from abundant unlabeled data to rare clinical scenarios has driven much of the enthusiasm for genomic foundation models.\nThe reality requires careful navigation. Every adaptation decision involves tradeoffs: preserving pretrained knowledge versus enabling task-specific learning, computational efficiency versus model flexibility, rapid deployment versus careful validation. Full fine-tuning updates all parameters, risking catastrophic forgetting of pretrained knowledge. Feature extraction freezes all pretrained parameters, limiting adaptation to task-specific patterns. Parameter-efficient methods (adapters, LoRA, prompt tuning) navigate between these extremes, but each makes different assumptions about where adaptation should occur.\nWhen a cardiologist requests variant interpretation for a patient with hypertrophic cardiomyopathy, the clinical need (classifying a specific MYH7 variant) differs fundamentally from the data available during model development (millions of protein sequences sampled across all of evolution). Bridging this gap requires understanding what properties of pretraining determine whether transfer will succeed. When this bridge fails, patients receive confident predictions based on patterns irrelevant to their clinical context.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-source-target",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-source-target",
    "title": "9  Transfer and Adaptation",
    "section": "",
    "text": "FIGURE PLACEHOLDER\n\n\n\n\nFigure 9.1: [Essential] Schematic illustrating domain shift in genomic transfer learning. Left panel (Source Domain): Diverse genomic sequences during pretraining, with learned representations capturing statistical regularities (local motifs, composition, conservation). Right panel (Target Domain): Sparse labeled examples for clinical task (e.g., pathogenic variants, tissue-specific enhancers), highlighting distributional differences. Center: Representation space showing well-transferred features (local motifs, conservation patterns) connected by solid arrows vs. poorly-transferred features (long-range regulatory logic, tissue-specific patterns) with dashed arrows indicating transfer failure.\n\n\n\n\n9.1.1 Gap Between Pretraining and Deployment\nThe source domain encompasses the data and objectives used during pretraining. For DNA foundation models, source domains typically include reference genomes, pan-genomic collections spanning population diversity, or metagenomic assemblies sampling environmental sequence space (Ji et al. 2021; Dalla-Torre et al. 2023). For protein models, databases like UniRef provide billions of sequences representing the diversity of evolutionary history (Suzek et al. 2007). Pretraining objectives (masked language modeling, next-token prediction, contrastive learning) encourage models to capture statistical regularities that help predict held-out tokens: local motifs, compositional patterns, and the signatures distinguishing functional from random sequence (see Chapter 8 for detailed treatment of these objectives). These learned regularities become the representations that might transfer to downstream tasks.\nThe target domain presents a fundamentally different challenge. Rather than abundant unlabeled sequence, the target domain offers sparse labeled examples of a specific clinical or biological question: a few thousand enhancer sequences with luciferase measurements, several hundred variants with expert pathogenicity classifications, chromatin profiles across a handful of disease-relevant cell types. The target distribution often looks nothing like pretraining data. Pathogenic variants are rare outliers, not typical protein sequences. Tissue-specific enhancers exhibit patterns that genome-wide pretraining may never emphasize. Disease-associated regulatory elements may have been systematically underrepresented in reference data (Kircher et al. 2014).\n\n\n9.1.2 Recognizing Transfer Outcomes\nNot all transfer helps, and distinguishing outcomes requires explicit validation. Positive transfer accelerates learning or improves final performance beyond training from scratch. Negative transfer occurs when pretraining actively hurts, either because learned features conflict with task requirements or because pretrained initialization creates optimization difficulties (Z. Wang et al. 2019). Neutral transfer describes situations where pretraining neither helps nor hurts, wasting computational resources on pretrained models without benefit. When a cardiology team adapts a DNA language model for KCNQ1 long QT syndrome variant classification, they must empirically verify which outcome applies to their specific task rather than assuming transfer will help because it helped elsewhere.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-transfer-factors",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-transfer-factors",
    "title": "9  Transfer and Adaptation",
    "section": "9.2 Factors Determining Transfer Success",
    "text": "9.2 Factors Determining Transfer Success\nFour factors determine whether this distributional gap can be bridged. Task relatedness measures whether target predictions depend on patterns the model learned during pretraining; predicting transcription factor binding after sequence pretraining succeeds because both involve local motif recognition, while predicting three-dimensional chromatin contacts may require spatial relationships the pretraining objective never captured (see Chapter 17 for chromatin contact prediction approaches). Target data quantity constrains which adaptation strategies avoid overfitting; with thousands of labeled examples, aggressive fine-tuning can reshape representations, but with dozens, only the lightest approaches remain viable. Model expressiveness influences adaptation flexibility, as larger models encode richer internal representations that can potentially serve more diverse downstream tasks but also risk memorizing small target datasets. Distribution overlap between source and target determines how much learned knowledge applies; human regulatory elements share patterns with mouse elements (enabling cross-species transfer) but diverge in species-specific enhancers (limiting it).\nUnderstanding why transfer succeeds or fails requires examining four interacting factors that collectively determine whether pretrained representations serve a new task. These factors are not independent: a highly related task may still fail with insufficient data, while abundant data cannot rescue transfer when source and target distributions fundamentally diverge. Practitioners must evaluate all four before committing to a transfer learning approach.\n\n9.2.1 Task Relatedness\nTransfer succeeds when target predictions depend on patterns the model learned during pretraining. This dependency is not always obvious from surface-level task descriptions. A model pretrained on DNA sequence using masked language modeling learns to predict nucleotides from context, which implicitly requires learning motifs, sequence composition, and local dependencies. Predicting transcription factor binding sites succeeds because binding depends on sequence motifs that the pretraining objective directly rewarded the model for recognizing. Predicting three-dimensional chromatin contacts typically fails because spatial relationships between distant genomic loci depend on protein-mediated interactions, chromatin accessibility, and nuclear architecture that sequence statistics alone cannot capture (see Chapter 17 for approaches that explicitly model chromatin structure).\nThe key question is not whether source and target tasks share a domain (both involve genomics) but whether they share relevant features. Protein language models pretrained on evolutionary sequences learn representations that capture structural constraints, functional domains, and evolutionary conservation. Variant effect prediction succeeds because pathogenic variants often disrupt these same structural and functional properties. Protein-protein interaction prediction may succeed partially (interaction surfaces correlate with evolutionary conservation) but fail for interaction specificity (which residues determine which partners bind), because the pretraining objective never distinguished between interacting and non-interacting proteins.\nPractitioners can estimate task relatedness before committing to transfer through three approaches. First, linear probing (see Section 9.3.1) reveals whether frozen pretrained representations contain task-relevant information; if a simple classifier on frozen embeddings outperforms random features, the pretraining objective captured something useful. Second, examining what the pretraining objective explicitly rewards clarifies what patterns the model was incentivized to learn; masked language modeling rewards local context prediction, contrastive learning rewards distinguishing related from unrelated sequences, and next-token prediction rewards sequential dependencies. Third, consulting the literature for related transfer attempts provides empirical guidance; if similar transfers have failed for this model class, success is unlikely without architectural or data modifications.\nWhen task relatedness is low, three strategies may salvage transfer. Intermediate fine-tuning on a related auxiliary task can build bridge representations: a model pretrained on general DNA sequence might be fine-tuned on chromatin accessibility prediction before the final adaptation to enhancer-gene linking, because chromatin accessibility provides intermediate features more relevant to regulatory relationships than raw sequence statistics. Multi-task fine-tuning that includes the target task alongside related tasks can encourage the model to extract shared features. Alternatively, practitioners may conclude that transfer is inappropriate for this task and proceed with from-scratch training, which remains a valid choice when pretrained representations offer no advantage.\n\n\n9.2.2 Target Data Quantity\nAvailable labeled data constrains which adaptation strategies avoid overfitting, creating a fundamental limit on adaptation complexity. The thresholds are approximate but provide useful guidance: with fewer than 500 labeled examples, only linear probing remains viable because any approach that updates pretrained parameters will overfit catastrophically. Between 500 and 5,000 examples, parameter-efficient methods like LoRA introduce enough flexibility to improve over frozen features while maintaining implicit regularization through low-rank constraints and frozen backbone parameters. Above 10,000 examples, full fine-tuning becomes feasible for adapting pretrained representations to fundamentally different target distributions.\nThese thresholds interact with data quality in ways that complicate simple counting. Five thousand noisy labels from high-throughput screening contribute less information than five hundred expert-curated annotations. Class imbalance matters: a dataset with 10,000 examples split 9,900 negative and 100 positive effectively provides only hundreds of examples for learning positive class features. Redundancy in training data (multiple variants from the same gene, or cells from the same patient) reduces effective sample size because nominally independent examples share confounding factors. The relevant quantity is not raw example count but effective information content for the target task.\nData augmentation can stretch limited examples further, but augmentation strategies must preserve task-relevant properties. Reverse-complementing DNA sequences provides valid augmentation for tasks with strand-symmetric biology (transcription factor binding is typically strand-symmetric) but introduces noise for tasks with strand-specific signals (RNA secondary structure depends on transcript strand). Random nucleotide masking followed by model infilling can generate plausible sequence variants, but these variants may not span the relevant distribution of task-specific variation. The safest augmentation strategies involve domain knowledge about what transformations preserve task labels.\nWhen data is severely limited (dozens of examples), practitioners face a choice between three imperfect options. Linear probing on frozen features provides the most stable approach but may miss task-specific patterns not captured in pretrained representations. Few-shot learning methods (see Section 9.9.1) attempt to adapt with minimal examples by leveraging structured prompts or metric learning, but success varies dramatically across tasks. Collecting more data, though often expensive, may be the only path to reliable adaptation.\n\n\n9.2.3 Model Expressiveness\nLarger models encode richer internal representations that can potentially serve more diverse downstream tasks, but this expressiveness creates a tension with overfitting risk. A 3-billion parameter protein language model captures subtle evolutionary signals invisible to smaller models, encoding relationships between distant residues, complex motif interactions, and nuanced conservation patterns. These rich representations enable zero-shot transfer to tasks the model was never explicitly trained for, because the pretraining objective forced the model to learn features that happen to correlate with task-relevant properties. ESM-2 at 15 billion parameters predicts protein structure contact maps despite never seeing structure labels during training, because evolutionary constraints that determine which sequences survive (the pretraining signal) are the same constraints that determine which structures fold stably (the transfer target).\nThe same expressiveness that enables rich transfer creates memorization risk when adaptation data is limited. A highly expressive model can memorize thousands of training examples without learning generalizable patterns, achieving perfect training accuracy while failing entirely on held-out data. This risk scales with model capacity relative to dataset size: a 3-billion parameter model fine-tuned on 500 variants will almost certainly overfit, while the same model fine-tuned on 500,000 variants may generalize effectively.\nParameter-efficient methods mitigate this tension by constraining which model behaviors can change during adaptation. LoRA restricts updates to low-rank subspaces, limiting the effective capacity available for memorization while preserving the rich pretrained representations for transfer. Adapter layers introduce small trainable modules between frozen layers, enabling task-specific computation without overwriting general knowledge. The rank, placement, and number of adapted parameters become hyperparameters that balance adaptation flexibility against overfitting risk.\nModel selection thus involves matching expressiveness to available data and task complexity. For tasks with abundant data and substantial divergence from pretraining, larger models provide more capacity to learn task-specific representations. For tasks with limited data that closely align with pretraining objectives, smaller models may transfer more reliably because their simpler representations leave less room for spurious memorization. The optimal model size depends on the interaction between all four transfer factors, not on model quality in isolation.\n\n\n9.2.4 Distribution Overlap\nThe degree of overlap between source and target distributions determines how much learned knowledge applies directly versus requires adaptation. Human and mouse genomes share regulatory syntax for housekeeping genes whose expression patterns were established before the mammalian radiation, enabling direct transfer of core promoter recognition, splice site identification, and basic transcriptional logic. Human-specific enhancers that evolved after the human-mouse divergence (roughly 75 million years ago) have no mouse counterparts from which to transfer, creating blind spots for human enhancer prediction based on mouse training data.\nDistribution overlap operates at multiple scales that practitioners must evaluate separately. At the sequence level, nucleotide composition, k-mer frequencies, and local motif distributions may diverge between source and target. Protein sequences from thermophilic organisms differ systematically in amino acid composition from mesophilic training data, potentially confusing models that implicitly learned composition-dependent features. At the feature level, the relationship between sequence patterns and biological function may shift: a motif that indicates enhancer activity in one cell type may be repressive in another due to cofactor availability. At the label level, the definition of positive and negative examples may differ: “pathogenic” variants in ClinVar reflect clinical ascertainment patterns that differ systematically from the evolutionary selection captured in pretraining.\nCross-species transfer illustrates distribution overlap challenges concretely. Models pretrained on human sequences and applied to non-human primates succeed for conserved elements (core promoters, splice sites, essential genes) because evolutionary proximity ensures feature preservation. Application to more distant species (zebrafish, Drosophila, plants) succeeds only for deeply conserved features and fails progressively for lineage-specific innovations. Kelley demonstrated that training simultaneously on human and mouse data improves regulatory prediction for both species compared to single-species training, because shared evolutionary history provides implicit labels about functional conservation while species-specific examples reveal where that conservation breaks down (Kelley 2020).\nDetecting distribution shift requires comparing source and target distributions before deployment (see Section 9.8.2 for methods). Statistical divergence measures quantify distribution differences numerically; embedding visualizations reveal whether target examples occupy familiar or novel regions of representation space; canary examples that should always be predicted correctly provide early warning of catastrophic shift. When shift is detected, practitioners must choose between domain adaptation techniques (which attempt to bridge the gap), acceptance that certain target subpopulations cannot be served by this model, or collection of target-distribution training data to enable proper adaptation.\n\n\n9.2.5 Factor Interactions\nThe four factors interact in ways that preclude simple rules. High task relatedness cannot rescue transfer when target data is too limited for any adaptation; abundant data cannot overcome fundamental distribution mismatch; an expressive model provides no advantage when pretrained representations lack task-relevant features. Practitioners must evaluate all four factors jointly, using the linear probing and validation approaches described in subsequent sections to empirically determine whether transfer succeeds for their specific combination of model, task, and data.\nThe most reliable path forward is conservative escalation: establish frozen feature baselines first to assess task relatedness and distribution overlap; try parameter-efficient methods next if frozen features show promise but leave room for improvement; reserve full fine-tuning for cases where simpler methods demonstrably fail and sufficient data exists to justify the risk; and maintain from-scratch training as a valid comparison throughout. Each escalation step provides information about which factors limit transfer, guiding both immediate decisions and future model development.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-feature-extraction",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-feature-extraction",
    "title": "9  Transfer and Adaptation",
    "section": "9.3 Feature Extraction and Representation Analysis",
    "text": "9.3 Feature Extraction and Representation Analysis\nClinical laboratories processing hundreds of variants daily cannot afford to fine-tune models for each new gene or variant class. When a novel gene enters diagnostic panels, classifiers must be deployed rapidly using whatever labeled examples exist. A molecular diagnostics team with 200 annotated RYR1 variants for malignant hyperthermia risk prediction cannot fine-tune a 500-million parameter model; they need an approach that works with minimal data while avoiding adaptation risk entirely.\nFrozen feature extraction addresses this constraint by treating pretrained models as fixed representation engines. All backbone parameters remain frozen; only a lightweight classifier trained on the extracted representations learns from labeled data. The backbone never changes, eliminating catastrophic forgetting entirely and enabling deployment within hours rather than weeks. The fundamental tradeoff is clear: frozen features sacrifice adaptation flexibility for speed, safety, and efficiency.\n\n9.3.1 Linear Probing\nWhy does the simplest possible classifier often suffice? If pretrained representations already encode task-relevant features in linearly separable form, adding complexity provides no benefit and risks overfitting. Linear probing tests this hypothesis by introducing only \\(d \\times c\\) parameters (where \\(d\\) is the embedding dimension and \\(c\\) is the number of output classes). Pass input sequences through the frozen model to obtain embeddings, typically from the final layer or from a designated [CLS] token aggregating sequence information, then train a linear classifier mapping embeddings to task labels.\nJi et al. demonstrated that DNABERT embeddings paired with linear probes achieve competitive chromatin accessibility prediction from a few hundred positive and negative examples, matching convolutional neural network baselines requiring far more labeled data (Ji et al. 2021). Dalla-Torre et al. showed similar results with Nucleotide Transformer, where linear probes on frozen embeddings approached fine-tuned performance for promoter detection and splice site recognition (Dalla-Torre et al. 2023). These successes reflect alignment between pretraining objectives (predicting masked tokens from local context) and target tasks (distinguishing sequences based on motif patterns the model already learned to recognize).\n\n\n9.3.2 When Linear Probing Fails\nLinear probes fail when relevant information exists in embeddings but requires nonlinear transformation to extract. Shallow multilayer perceptrons (one or two hidden layers) extend linear probing by enabling more complex decision boundaries while maintaining computational efficiency. With several thousand labeled examples, shallow MLPs on HyenaDNA embeddings improve splice site prediction over linear probes by capturing interactions between features that linear models cannot represent (Nguyen et al. 2023). The additional expressiveness helps when task-relevant patterns are distributed across embedding dimensions in ways that linear combination cannot capture.\nThe more fundamental limitation cannot be addressed by classifier complexity: performance caps at how well pretrained representations already encode task-relevant features. If the pretraining objective emphasized patterns irrelevant to the downstream task, or if required features were actively suppressed during pretraining, frozen features will underperform models trained from scratch regardless of classifier sophistication. A model pretrained exclusively on coding sequence may encode features misleading for noncoding regulatory prediction; no linear probe can overcome representations that point in the wrong direction.\n\n\n9.3.3 Probing Representations\nA variant effect predictor built on ESM embeddings achieves 85% accuracy in initial testing, but the team deploying it needs to understand why. Does the model genuinely capture evolutionary constraint relevant to pathogenicity, or has it learned spurious correlations that will fail on out-of-distribution variants? Before committing computational resources to adaptation, practitioners benefit from understanding what the pretrained model actually learned.\nProbing classifiers answer these diagnostic questions by systematically interrogating representations before deployment. The methodology converts the abstract question “will transfer help?” into concrete evidence about representation content: train lightweight classifiers to predict properties of interest from frozen embeddings, then examine how accurately different properties can be decoded. If chromatin accessibility can be predicted with 85% accuracy from a linear probe, the representations already encode accessibility-relevant features and frozen feature extraction will likely succeed. If transcription factor binding requires a deep nonlinear classifier to reach the same accuracy, relevant information exists but is not linearly separable, suggesting PEFT might help by reorganizing representations for easier extraction. If a property cannot be predicted above chance even with flexible classifiers, the representations may lack necessary information entirely, and transfer to this task may fail regardless of adaptation strategy.\n\n\n9.3.4 What Probing Reveals About Pretrained Models\nSystematic probing reveals what models learn during pretraining. Rives et al. demonstrated that ESM protein embeddings encode secondary structure so thoroughly that linear probes achieve near state-of-the-art helix/sheet/coil prediction accuracy (Rives et al. 2021). Contact prediction (which residues are spatially close in folded structure) requires nonlinear probes but still achieves strong performance, indicating that tertiary structure information is present but requires transformation to extract. DNA language models show similar patterns: local motif information is recoverable by linear probes while long-range dependencies require multi-layer networks (Ji et al. 2021). The ESM family and its learned structural knowledge are examined in Chapter 12, while DNA language model probing appears in Chapter 11.\nLayer-wise probing reveals how information transforms through the model. Early layers typically encode local compositional features (\\(k\\)-mer frequencies, simple motifs, sequence statistics) while later layers capture more abstract patterns (regulatory signatures, evolutionary constraints, functional classifications) (Jawahar, Sagot, and Seddah 2019). For tasks depending on local features, representations from early or middle layers may outperform final-layer embeddings that have abstracted away relevant details. Layer selection becomes another hyperparameter to optimize during adaptation.\n\n\n9.3.5 Probe-Guided Adaptation\nThe diagnostic value extends beyond predicting which adaptation strategy to use. When probing reveals that required features are absent from pretrained representations, practitioners face a choice: commit to full fine-tuning with sufficient target data (hoping the model can learn missing features), switch to a different foundation model whose pretraining objective better aligns with task requirements, or proceed with from-scratch training that does not inherit inappropriate inductive biases. The investment in probing before adaptation often saves months of wasted effort on transfer that was doomed from the start.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-peft",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-peft",
    "title": "9  Transfer and Adaptation",
    "section": "9.4 Parameter-Efficient Fine-Tuning",
    "text": "9.4 Parameter-Efficient Fine-Tuning\nA research hospital developing tissue-specific expression predictors faces an impossible choice. Frozen features from Enformer provide reasonable baselines, but full fine-tuning for each of fifty tissue types would require months of GPU time and risk overfitting the thousands of tissue-specific training examples. The team needs an intermediate approach: enough flexibility to improve over frozen features, enough constraint to prevent overfitting, enough efficiency to iterate across dozens of tissues.\nParameter-efficient fine-tuning (PEFT) methods resolve this tension by updating a small subset of parameters while keeping the majority frozen, enabling task-specific adaptation without the computational expense or overfitting risk of modifying all weights (Houlsby et al. 2019). The key insight is that useful adaptation often requires changing only a small subspace of model behavior, not rewriting everything the model learned during pretraining.\n\n9.4.1 Low-Rank Adaptation\nLow-Rank Adaptation (LoRA) has emerged as the dominant PEFT technique in genomic applications because it directly operationalizes this insight. Rather than updating a large weight matrix \\(W\\) directly, LoRA introduces two smaller matrices \\(A\\) and \\(B\\) whose product approximates the desired weight change: \\(W' = W + BA\\) (Hu et al. 2021). During fine-tuning, \\(W\\) remains frozen while only \\(A\\) and \\(B\\) receive gradient updates. The rank of these matrices (typically 8 to 64 for genomic models) controls adaptation expressiveness: lower ranks introduce fewer parameters and stronger implicit regularization; higher ranks enable more flexible task-specific modification at greater overfitting risk.\nThe efficiency gains prove substantial. A transformer with 500 million parameters might require updating only 2 to 5 million LoRA parameters (representing the low-rank decompositions applied to attention weight matrices), reducing memory requirements by an order of magnitude compared with full fine-tuning. This efficiency enables training on consumer GPUs for models that would otherwise require specialized infrastructure, and enables systematic hyperparameter search that would be prohibitive with full parameter updates. Zhou et al. demonstrated that LoRA adapters on Nucleotide Transformer enable tissue-specific chromatin accessibility prediction, where separate low-rank matrices capture tissue-specific regulatory patterns while the pretrained backbone encodes general sequence understanding (Zhou et al. 2024). Clinical applications of parameter-efficient fine-tuning for risk prediction appear in Chapter 25.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 9.2: [Essential] Schematic of LoRA adaptation showing original frozen weight matrix W alongside low-rank decomposition matrices A and B. Indicate parameter counts (e.g., 500M frozen vs. 2-5M trainable). Show how the effective weight becomes W + BA during forward pass.\n\n\n\n\n\n9.4.2 Configuring Low-Rank Adaptation\nSelecting LoRA hyperparameters requires balancing expressiveness against overfitting risk, with optimal choices depending on task alignment and available data. The rank parameter controls how many dimensions of modification are possible. Ranks of 4 to 16 typically suffice for tasks closely aligned with pretraining objectives, where small perturbations to pretrained weights capture the required adaptation. When target tasks diverge more substantially from pretraining, ranks of 32 to 64 may prove necessary, though higher ranks approach the parameter count where full fine-tuning becomes competitive. Empirical comparison across ranks on held-out validation data remains the most reliable selection method; theoretical guidance for optimal rank given task characteristics does not yet exist.\nThe question of which layers to adapt depends critically on whether the foundation model uses encoder or decoder architecture. Encoder models like DNABERT and Nucleotide Transformer process entire sequences bidirectionally, building representations that integrate context from both directions at every layer. For these models, middle and later layers typically encode the most task-relevant features: early layers capture local sequence patterns (motifs, k-mer statistics) while deeper layers integrate these into higher-order representations (see Chapter 8 for discussion of layer-wise representation learning). Adapting only the final third of transformer layers often achieves most of the performance gain at a fraction of the parameter cost. Linear probing experiments across layers can identify where task-relevant information concentrates before committing to adapter placement.\nDecoder models like HyenaDNA in autoregressive mode and GPT-style genomic models present different considerations. These architectures process sequences left-to-right, with each position attending only to preceding context. The causal attention mask means that later layers have seen more integrated context, but the unidirectional flow creates different feature hierarchies than bidirectional encoders. For decoder models, adapting attention layers proves particularly important because the causal structure means attention patterns determine what contextual information flows forward. Practitioners often find that adapting both attention projections (queries, keys, values, and output) and feed-forward layers in decoder models yields better results than attention-only adaptation that works well for encoders.\nWithin layers, LoRA can be applied to query, key, value, and output projection matrices in attention, and to the two weight matrices in feed-forward blocks. Attention weight adaptation alone often suffices for encoder models on classification tasks, where the key adaptation involves changing what information the model attends to. Feed-forward adaptation becomes more important when the required transformation involves learning new feature combinations rather than reweighting existing attention patterns. When computational budget permits, adapting all weight matrices with lower rank often outperforms adapting fewer matrices with higher rank.\nThese heuristics provide starting points, not guarantees. The interaction between model architecture, pretraining objective, target task, and available data creates a combinatorial space that resists simple rules. Systematic hyperparameter search over rank, layer selection, and weight matrix targeting, guided by validation performance on data matching the deployment distribution, remains the most reliable path to effective adaptation.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-layer-selection",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-layer-selection",
    "title": "9  Transfer and Adaptation",
    "section": "9.5 Layer Selection for Embedding Extraction",
    "text": "9.5 Layer Selection for Embedding Extraction\nA research team attempting to use HyenaDNA for splice site classification discovers an unexpected problem. Following standard practice from encoder models, they extract embeddings from the final transformer layer and train a linear classifier. Performance barely exceeds random guessing. Frustrated, they try layer 6 of 12 on a hunch and accuracy jumps by 15 percentage points. Layer 4 performs better still for their particular task. The team has stumbled onto a systematic challenge that distinguishes decoder-based foundation models from their encoder counterparts: the optimal layer for embedding extraction varies dramatically by task, and the final layer is often the worst choice.\nThis phenomenon, sometimes called the layer hunting problem, arises from a fundamental asymmetry between how encoder and decoder models are trained. Encoder models like DNABERT and Nucleotide Transformer are optimized to produce representations useful for reconstructing masked tokens from bidirectional context. Every layer contributes to this reconstruction, and the final layer aggregates information specifically designed to support prediction. The [CLS] token or mean-pooled final layer representations work reliably across diverse downstream tasks because the pretraining objective directly shaped these representations for general utility.\nDecoder models face a different optimization pressure. The next-token prediction objective trains the final layer specifically to predict vocabulary distributions over the next token. This specialization is precisely what enables fluent generation, but it creates representations optimized for a narrow purpose rather than general-purpose embeddings. The final layer learns to transform rich intermediate representations into the specific format needed for token prediction, discarding information irrelevant to that task but potentially critical for downstream classification or regression.\n\n9.5.1 The Encoder Advantage\nJawahar et al. (Jawahar, Sagot, and Seddah 2019) demonstrated that BERT develops an interpretable layer hierarchy: lower layers encode surface features (word length, capitalization), middle layers capture syntactic structure (constituency, dependency relations), and upper layers represent semantic content (coreference, semantic roles). This progression means practitioners can make principled choices about layer selection based on task requirements. Tasks requiring syntactic understanding benefit from middle layers; tasks requiring semantic similarity benefit from upper layers. Crucially, the final layer remains a reasonable default because it integrates information from all levels while retaining semantic content useful for most applications.\nThe bidirectional attention mechanism ensures that every position’s representation incorporates information from the entire sequence at every layer. A nucleotide’s representation in layer 12 reflects constraints from both upstream promoter elements and downstream coding sequence. This global integration makes encoder representations naturally suited for tasks where context on both sides matters, which describes most genomic classification problems. Variant effect prediction, transcription factor binding, and splice site recognition all benefit from knowing what lies both before and after the position of interest.\nEncoder models also exhibit relatively stable layer-wise performance for frozen feature extraction. While middle layers sometimes outperform final layers for specific tasks, the differences are typically modest (a few percentage points) and the final layer rarely fails catastrophically. Practitioners can extract final-layer embeddings with reasonable confidence that performance will be competitive, reserving layer search for optimization rather than treating it as a requirement for basic functionality.\n\n\n9.5.2 The Decoder Dilemma\nDecoder models trained with causal attention create fundamentally different representation hierarchies. Each layer can only integrate information from preceding positions due to the causal mask. Position 500 in a 1000-token sequence has rich representations of the first 499 tokens but no information about the following 500. This asymmetry propagates through layers, creating representations that emphasize historical context over global sequence properties.\nThe next-token prediction objective compounds this asymmetry by specializing the final layers for a specific output format. Consider what the final layer must learn: transform the current hidden state into a probability distribution over vocabulary tokens. This transformation discards information about the input sequence that is irrelevant for predicting the immediate next token. Evolutionary conservation patterns 200 positions upstream, motif co-occurrence statistics, and global sequence composition may all inform intermediate representations but contribute nothing to next-token prediction and can be safely discarded by the final layer.\nEmpirically, practitioners using decoder models for classification consistently find that intermediate layers outperform final layers, often dramatically. For HyenaDNA on regulatory element classification, layers in the middle third of the network frequently achieve the best linear probing accuracy. For GPT-style genomic models, the optimal layer can vary by 30-50% of network depth depending on the specific downstream task. A splice site classifier might perform best with layer 4 representations while a promoter classifier using the same model achieves optimal performance at layer 8. The task-dependence of optimal layer selection adds a hyperparameter dimension that does not exist for encoder models.\n\n\n9.5.3 Practical Consequences\nThe layer hunting problem creates concrete challenges for deploying decoder-based foundation models. First, it increases computational cost: practitioners must evaluate downstream performance across all layers (or a representative subset) before committing to an adaptation strategy. A 12-layer model requires 12 separate linear probing experiments rather than one. Second, it complicates model comparison: reporting results from the best layer for each model can obscure whether the improvement comes from the model or from more thorough hyperparameter search. Third, it limits reproducibility: papers that report only final-layer performance for decoder models may dramatically underestimate achievable accuracy, while papers that report best-layer performance without specifying the layer make replication difficult.\nThe problem intensifies when decoder models grow deeper. A 12-layer model has 12 candidate extraction points; a 48-layer model has 48. The search space grows linearly with depth, and there is no theoretical guidance for narrowing the search a priori. Heuristics like “try middle layers first” help but do not eliminate the need for empirical validation on each new task.\n\n\n9.5.4 Layer Averaging and Weighted Combinations\nSeveral strategies address the layer hunting problem without exhaustive search. Layer averaging computes embeddings as the mean across all layers (or a subset), combining information from different levels of abstraction. This approach works surprisingly well in practice because it captures both syntactic features from early layers and more abstract features from later layers. The cost is that averaging dilutes task-specific signal present in particular layers, sometimes underperforming the optimal single layer by several percentage points.\nWeighted layer combinations learn task-specific weights for each layer’s contribution to the final embedding. Given layer representations \\(h_1, h_2, \\ldots, h_L\\), the combined representation is \\(h = \\sum_{l=1}^{L} \\alpha_l h_l\\) where \\(\\alpha_l\\) are learned weights (often softmax-normalized to sum to one). This approach was popularized by ELMo (peters_deep_2018?) and remains effective for foundation model adaptation. The weights themselves become informative: high weights on early layers suggest the task relies on surface features; high weights on late-middle layers suggest reliance on contextual integration.\nLearned layer weights add minimal parameters (one scalar per layer) while substantially reducing the manual hyperparameter search. The weights can be trained jointly with the downstream classifier using the same labeled data, requiring no additional supervision. For decoder models where optimal layer varies by task, learned combinations often match or exceed single-layer performance while eliminating the need to identify the optimal layer through trial and error.\n\n\n9.5.5 Systematic Layer Probing\nWhen using decoder models for transfer learning, systematic layer probing should precede any adaptation strategy that depends on embedding quality. The procedure is straightforward: extract representations from each layer for the downstream task’s training data, train identical lightweight classifiers (linear or shallow MLP) on each layer’s representations, and evaluate on held-out validation data. The layer achieving the best validation performance indicates where task-relevant information concentrates.\nThis probing step reveals not just the optimal layer but the shape of performance across layers. A sharp peak suggests highly localized task-relevant features; broad performance across middle layers suggests distributed representation. Monotonically increasing performance toward middle layers (then decreasing toward the final layer) is the typical pattern for decoder models on classification tasks. Anomalous patterns (best performance at layer 1, or best performance at the final layer) warrant investigation: they may indicate task-pretraining alignment issues or data quality problems.\nFor genomic models specifically, probing results often correlate with task properties. Tasks requiring recognition of local sequence motifs (transcription factor binding) show best performance in earlier layers where positional patterns are most directly encoded. Tasks requiring integration of broader context (enhancer-promoter association, long-range regulatory effects) show best performance in deeper middle layers where more context has been accumulated through the causal attention stack. Tasks most aligned with the next-token prediction objective (predicting the next nucleotide) show best performance in later layers, as expected.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 9.3: [Essential] Two-panel figure comparing layer-wise probing results for encoder vs. decoder architectures on the same downstream task (e.g., splice site classification). Panel A: Encoder model showing relatively flat performance across layers with slight improvement toward final layers. Panel B: Decoder model showing inverted-U pattern with peak performance at middle layers and degraded performance at final layer. Annotate the “layer hunting problem” region for decoder models.\n\n\n\n\n\n9.5.6 Implications for Model Selection\nThe layer hunting problem should inform model architecture selection, not just adaptation strategy. When downstream applications primarily involve classification, regression, or embedding-based retrieval (most clinical genomics applications), encoder architectures offer practical advantages beyond their representational benefits. The reliable performance of final-layer embeddings simplifies deployment pipelines, reduces hyperparameter search burden, and improves reproducibility. The bidirectional context that encoders provide aligns naturally with variant interpretation, where surrounding sequence on both sides determines functional impact.\nDecoder architectures remain essential when generation is the primary goal: designing novel regulatory sequences, sampling protein variants, or producing synthetic training data. For these applications, the final layer’s specialization for next-token prediction is a feature rather than a bug. Hybrid strategies that use decoder models for generation but encoder models (or carefully selected decoder layers) for classification can capture benefits of both architectures, though at the cost of maintaining multiple models.\nWhen decoder models must be used for classification (perhaps because they offer superior long-context handling or because they are the only available pretrained model for a particular sequence type), the layer hunting cost should be budgeted explicitly. Plan for layer-wise probing experiments. Consider learned layer weighting from the start. Report which layer produced reported results, and consider reporting performance across layers to enable fair comparison with future work.\n\n\n9.5.7 Cross-Reference to Pretraining Objectives\nThe layer hunting problem is a direct consequence of pretraining objective choice, connecting this practical deployment consideration back to the foundational decisions examined in Chapter 8. Masked language modeling trains all layers to support bidirectional context integration, producing representations useful for diverse downstream tasks throughout the network. Next-token prediction trains final layers for a specific output format, creating the representation collapse that makes layer search necessary. Understanding this connection helps practitioners anticipate adaptation challenges before committing to a foundation model: if your downstream tasks are primarily predictive rather than generative, the reliable final-layer embeddings of encoder models may outweigh other architectural considerations.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-full-finetuning",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-full-finetuning",
    "title": "9  Transfer and Adaptation",
    "section": "9.6 Full Fine-Tuning",
    "text": "9.6 Full Fine-Tuning\nWhen Avsec et al. sought to predict gene expression from sequence across hundreds of cell types, they required a model capturing tissue-specific regulatory logic unavailable from any generic pretrained representation (Avsec et al. 2021). With millions of labeled examples spanning thousands of genomic tracks, they could afford to update all model parameters, reshaping internal representations entirely for their specific predictive task. Constrained adaptation would have left tissue-specific regulatory patterns unlearned.\nFull fine-tuning offers maximum flexibility: every parameter becomes tunable, enabling the model to learn whatever features the target task requires regardless of pretraining emphasis. This flexibility comes with risks proportional to its power.\n\n9.6.1 Making Full Fine-Tuning Work\nFull fine-tuning updates all model parameters during adaptation but requires careful attention to optimization dynamics. Learning rates must be substantially lower than during pretraining (often 10 to 100 times smaller) to avoid catastrophically disrupting learned representations in early training steps (Howard and Ruder 2018). Gradual unfreezing, where top layers update first and deeper layers progressively join training, helps preserve low-level features (local motifs, basic sequence statistics) while allowing high-level task-specific adjustment. Regularization through weight decay, dropout, and early stopping on validation data prevents overfitting to target datasets.\nThe approach suits scenarios when labeled datasets are large (tens of thousands of examples or more), when the target task diverges substantially from pretraining such that constrained adaptation proves insufficient, or when performance requirements justify computational investment. Enformer fine-tuning for new chromatin assays requires updating most parameters to capture assay-specific signal characteristics distinct from original training conditions. Expression prediction across novel cell types benefits from full adaptation when sufficient tissue-specific data exists.\n\n\n9.6.2 Risks of Unconstrained Adaptation\nCatastrophic forgetting occurs when fine-tuning overwrites general knowledge useful for related tasks or out-of-distribution inputs; a model fine-tuned aggressively on lymphocyte data may lose performance on epithelial cells it previously handled well (McCloskey and Cohen 1989). Overfitting afflicts small target datasets, where the model memorizes training examples rather than learning generalizable patterns. Computational expense can be prohibitive for models with billions of parameters. When negative transfer occurs (pretraining initialization actually hurts optimization), full fine-tuning may underperform training from scratch despite the additional expense.\nThe conservative strategy is to start with simpler methods and escalate only when they demonstrably fail. Establish frozen feature baselines first. If frozen features outperform random initialization, try PEFT methods before committing to full fine-tuning. Compare fine-tuned models against properly-tuned from-scratch baselines on the same target data. Monitor for overfitting through validation curves and early stopping. The goal is achieving required performance with minimal adaptation complexity.\n\n\n9.6.3 The [CLS] Token and Sequence Aggregation\nSequence classification requires a fixed-size representation regardless of input length. A promoter classifier must produce the same prediction format whether the input spans 200 or 2,000 nucleotides. A pathogenicity model must output a single score whether analyzing a 50-residue peptide or a 3,000-residue multidomain protein. Transformers produce per-position representations: for a 512-token input, the model generates 512 embedding vectors, one for each position. Converting these variable-length outputs into fixed-size vectors suitable for classification constitutes the sequence aggregation problem.\nThe [CLS] token provides one solution, introduced in BERT and adopted widely across encoder architectures including DNABERT and Nucleotide Transformer (Devlin et al. 2019). The approach prepends a special classification token to every input sequence before processing. This token participates in attention computations like any other position, attending to all sequence positions and being attended to by them. Unlike content tokens that represent actual nucleotides or amino acids, the [CLS] token carries no intrinsic meaning. Its representation emerges entirely from aggregating information across the sequence through the attention mechanism.\nThe critical insight is that training shapes the [CLS] representation specifically for sequence-level tasks. During BERT’s pretraining, the [CLS] token was used for next-sentence prediction, requiring it to encode information sufficient to determine whether two sentences were contiguous in the original text. This training pressure transforms the [CLS] position into a natural aggregation point: its final-layer representation captures sequence-level properties distilled from positional representations throughout the network. When DNABERT applies this architecture to genomic sequences, the [CLS] token learns to aggregate regulatory signals, motif patterns, and compositional features into a single vector suitable for downstream classification.\nThe computational mechanism is straightforward. For an input sequence of \\(n\\) tokens, the model prepends [CLS] to create an \\((n+1)\\)-token input. After processing through all transformer layers, the final representation at position 0 (the [CLS] position) serves as the sequence embedding. A linear classifier or shallow neural network trained on this embedding produces the final prediction. The [CLS] approach adds exactly one token to the input, creating negligible computational overhead while providing a principled aggregation mechanism shaped by pretraining.\nThe [CLS] token’s effectiveness depends on the pretraining objective. When pretraining includes explicit sequence-level tasks (next-sentence prediction in BERT, similar objectives in some genomic models), the [CLS] representation receives direct training signal to encode sequence properties. When pretraining uses only token-level objectives like masked language modeling, the [CLS] representation is shaped indirectly through its participation in attention. DNABERT used masked language modeling without an explicit sequence-level pretraining task, yet its [CLS] representations still proved effective for downstream classification, suggesting that attention-based aggregation suffices even without task-specific pretraining signal (Ji et al. 2021).\n\n\n9.6.4 Mean Pooling and Alternatives\nMean pooling offers a simpler alternative: average all per-position embeddings to obtain a single sequence representation. For a sequence with token representations \\(h_1, h_2, \\ldots, h_n\\), the pooled representation is simply \\(\\bar{h} = \\frac{1}{n}\\sum_{i=1}^{n} h_i\\). This approach requires no special tokens, no architectural modifications, and no assumptions about which position aggregates sequence information. Every position contributes equally to the final representation.\nMean pooling often matches or exceeds [CLS] performance for genomic and protein sequences, particularly when pretraining did not include explicit sequence-level objectives (naderializadeh_aggregating_2025?). The explanation lies in information distribution across positions. In natural language, sentence meaning concentrates in specific tokens: the subject, main verb, and key modifiers carry most semantic content while articles and prepositions contribute less. The [CLS] token can learn to weight positions according to their informativeness. In genomic sequences, relevant information may distribute more uniformly: every nucleotide in a transcription factor binding site contributes to recognition, every residue in a protein domain contributes to function. Mean pooling captures this distributed signal naturally, while [CLS] must learn through attention what mean pooling provides by construction.\nMax pooling takes element-wise maxima across positions, capturing the strongest activation for each embedding dimension regardless of where it occurs. For regulatory element classification, max pooling can identify whether any position contains a strong motif signal, potentially outperforming mean pooling when a single strong feature determines the label. The tradeoff is sensitivity to outliers and potential loss of information about feature co-occurrence: max pooling cannot distinguish a sequence with one strong signal from a sequence with many moderate signals.\nAttention-based pooling learns to weight positions dynamically, computing attention scores that determine each position’s contribution to the final representation (hoang_locality-aware_2025?). This generalizes both mean pooling (uniform weights) and [CLS] aggregation (learned weights through attention to a special token). Attention pooling adds parameters but can capture position-dependent importance when the downstream task requires it. For protein sequences, attention pooling has shown advantages over both [CLS] and mean pooling for tasks where specific regions (active sites, binding interfaces) determine function, allowing the model to focus on relevant positions while downweighting uninformative regions.\n\n\n9.6.5 Practical Considerations for Genomic Sequences\nGenomic sequence properties create specific considerations for aggregation strategy choice. Sequences often contain substantial length variation: promoter regions might span hundreds of base pairs while enhancer elements vary from tens to thousands. Mean pooling implicitly normalizes for length (the denominator scales with sequence size), while [CLS] representations can encode absolute length information through attention patterns. For tasks where length itself is informative, this distinction matters.\nRepetitive elements present another challenge. Genomic sequences frequently contain Alu elements, LINE repeats, and other repetitive content that may dominate mean-pooled representations simply through their abundance. The [CLS] token can learn to downweight repetitive regions if they are uninformative for the classification task, while mean pooling treats all positions equally regardless of their uniqueness or informativeness.\nThe choice between [CLS] and mean pooling often comes down to empirical validation on the specific task. For DNABERT applied to chromatin accessibility prediction, Ji et al. found that [CLS] representations achieved strong performance, but subsequent work with other DNA language models has shown comparable or superior results with mean pooling (Dalla-Torre et al. 2023). The Nucleotide Transformer evaluation suite includes comparisons across pooling strategies, generally finding modest differences that vary by task. A pragmatic approach extracts both [CLS] and mean-pooled representations during initial experiments, selecting the better performer for production deployment.\nFor protein language models, the comparison is similarly equivocal. ESM-2 embeddings work well with both [CLS] and mean pooling for most classification tasks. Recent work on optimal transport-based aggregation suggests that both standard approaches lose information present in per-residue representations, motivating more sophisticated aggregation schemes for tasks requiring fine-grained sequence understanding (naderializadeh_aggregating_2025?). These advanced methods add complexity and computational cost; whether the improvement justifies the overhead depends on task requirements and deployment constraints.\nWhen using decoder models for classification, the aggregation question becomes more complex. Decoder architectures typically lack a [CLS] token because their training objective (next-token prediction) does not require sequence-level representations. The final token’s representation aggregates information from all preceding positions due to causal attention, making it a natural candidate for sequence embedding. Mean pooling over decoder representations faces the asymmetry problem discussed in Section 9.5: later positions have richer context than earlier positions, creating systematic bias in averaged representations. Some practitioners mean-pool only the final portion of the sequence where representations have accumulated sufficient context, though optimal truncation points vary by model and task.\nThe interaction between pooling strategy and layer selection deserves attention. For encoder models, [CLS] and mean pooling both work reasonably well with final-layer representations because the pretraining objective shaped all positions for general utility. For decoder models, the optimal layer for [CLS]-style aggregation (using the final token) may differ from the optimal layer for mean pooling, adding another dimension to the hyperparameter search. When computational budget permits, systematic evaluation across both pooling strategies and layer choices provides the most reliable path to effective transfer.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-choosing-strategy",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-choosing-strategy",
    "title": "9  Transfer and Adaptation",
    "section": "9.7 Choosing an Adaptation Strategy",
    "text": "9.7 Choosing an Adaptation Strategy\nThe preceding sections described what each adaptation approach does; here we address when to use each. Two factors dominate the decision: how much labeled data exists, and how closely the target task aligns with pretraining objectives. Figure 9.4 provides a decision framework, but the underlying logic is straightforward.\nData quantity determines what is possible. With fewer than 500 labeled examples, linear probing represents the only viable approach; more complex adaptation overfits catastrophically. Between 500 and 5,000 examples, PEFT methods offer favorable tradeoffs, introducing enough flexibility to improve over frozen features while maintaining implicit regularization. Above 10,000 examples, full fine-tuning becomes viable and may be necessary when target tasks diverge substantially from pretraining. Task similarity determines what is necessary. When targets closely resemble pretraining patterns (predicting transcription factor binding after sequence pretraining), frozen features often suffice. When tasks diverge moderately (tissue-specific expression after genome-wide pretraining), PEFT enables selective adaptation. When tasks fundamentally differ from pretraining (three-dimensional chromatin contacts from sequence-only pretraining), full fine-tuning may be required to learn features the pretraining objective never emphasized. Computational constraints impose practical limits: linear probing requires minutes on CPUs, LoRA requires hours on single GPUs, and full fine-tuning of large models requires days on multiple GPUs.\nThese heuristics indicate which strategies merit trying first, but empirical validation supersedes any rule. No formula reliably predicts which approach will succeed for a specific combination of model, task, and data. The conservative path is to establish frozen feature baselines first, escalate to PEFT when frozen features prove insufficient, and reserve full fine-tuning for cases where simpler methods demonstrably fail and sufficient data exists to justify the risk.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 9.4: [Essential] Flowchart guiding adaptation strategy selection. Decision nodes: (1) “Labeled data quantity?” with branches &lt;500, 500-5000, &gt;10000. (2) “Task similarity to pretraining?” with branches high/moderate/low. (3) “Computational constraints?” with branches limited/moderate/substantial. Terminal nodes recommend: Linear probing, LoRA/Adapters, Full fine-tuning, or From-scratch training. Include expected tradeoffs at each terminal (accuracy, compute, overfitting risk).",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-domain-shift",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-domain-shift",
    "title": "9  Transfer and Adaptation",
    "section": "9.8 Domain Shift and Cross-Context Transfer",
    "text": "9.8 Domain Shift and Cross-Context Transfer\nThe CYP2D6 gene encodes a cytochrome P450 enzyme metabolizing approximately 25% of clinically used drugs, including codeine (where poor metabolizers experience no analgesic effect) and tamoxifen (where poor metabolizers show reduced breast cancer treatment efficacy) (Gaedigk et al. 2018). CYP2D6 poses particular challenges for variant interpretation due to its complex structural variation and population-specific haplotypes (see Chapter 26 for clinical workflow considerations). A foundation model trained on human genomic data and adapted for CYP2D6 variant classification might achieve 90% accuracy on common variants well-represented in training data. But the variants most important clinically are rare: novel star alleles in underrepresented populations, structural variants creating gene duplications or deletions, population-specific haplotypes absent from reference databases. Domain shift between training and deployment distributions creates systematic blind spots precisely where clinical stakes are highest.\n\n9.8.1 Types of Domain Shift in Genomics\nThree types of domain shift commonly afflict genomic transfer learning, each creating different patterns of failure.\nEvolutionary divergence creates the most fundamental barrier to cross-species transfer. When models trained on human sequences are applied to other organisms, differences in regulatory syntax, motif grammar, and functional constraints can undermine predictions entirely. Human-to-mouse regulatory prediction works reasonably for conserved housekeeping genes but fails for rodent-specific enhancers that never existed in the human training distribution. Strategies for cross-species success include pretraining on multi-species data to learn conservation patterns, fine-tuning with species-specific adapters, and focusing on highly conserved features (core promoter elements, splice site consensus sequences) that transfer more readily than species-specific innovations (Kelley 2020).\nTissue-specific regulatory programs create equally severe challenges for cross-tissue transfer. Chromatin accessibility varies dramatically across tissues, with thousands of tissue-specific enhancers and repressors controlling cell-type identity. Models trained predominantly on one tissue may miss regulatory logic specific to others. Effective approaches include shared backbones with tissue-specific prediction heads (each head learns tissue-specific transformations of shared representations), tissue-conditional models accepting tissue identity as additional input, and meta-learning frameworks training across many tissues to extract general principles applicable to novel tissue types (Avsec et al. 2021).\nPopulation structure introduces a form of domain shift with direct clinical consequences. Models trained predominantly on European-ancestry data perform systematically worse when applied to other populations, with polygenic scores showing 40 to 75 percent reductions in prediction accuracy for African-ancestry individuals (Section 3.7). The mechanisms are both technical and biological: linkage disequilibrium patterns differ across populations (making tag SNPs poor proxies for causal variants in non-training ancestries), allele frequencies shift (variants common in training data may be rare elsewhere), and effect sizes may genuinely differ due to gene-environment interactions or genetic background effects (Martin et al. 2019). Foundation models offer potential improvement by learning sequence-based features that transfer across ancestries without relying on population-specific LD patterns, but this potential remains unrealized without explicit evaluation across diverse populations. Multi-ancestry pretraining and ancestry-stratified fine-tuning represent emerging approaches, though the fundamental data imbalance (78% of GWAS participants are European despite comprising 16% of the global population) constrains what any model can learn about underrepresented groups. The broader implications of ancestry confounding receive comprehensive treatment in Section 22.2.1.\nTechnical variation across sequencing platforms, library preparation protocols, and analysis pipelines creates batch effects that masquerade as biological signal. Different instruments produce distinct error profiles; capture kits determine which regions receive adequate coverage; alignment algorithms and variant callers make different decisions at ambiguous positions. When samples from a particular batch disproportionately represent a specific label class (cases sequenced at one center, controls at another), models learn to distinguish batches rather than biology (yu_assessing_2024?). Foundation models are not immune: a DNA language model pretrained on data from one sequencing platform may encode platform-specific artifacts in its representations, producing embeddings that cluster by sequencing center rather than by biological phenotype. Detection requires comparing embeddings across batches using visualization or statistical divergence measures; mitigation strategies include explicit batch correction during preprocessing, domain-adversarial training that penalizes batch-predictive features, and careful data curation ensuring batch balance across labels (varoquaux_preventing_2021?). The relationship between batch effects and institutional confounding is explored further in Section 22.2.2.\nDifferences in molecular readout technology create a more subtle form of shift that affects cross-assay transfer. ChIP-seq and ATAC-seq both measure chromatin state but with different biochemistry, resolution, and signal characteristics. Models trained on one assay may learn assay-specific artifacts rather than underlying biology, producing predictions that fail when applied to related assays measuring the same phenomenon differently. Multi-task pretraining across assays helps models distinguish biological signal from assay-specific noise.\n\n\n9.8.2 Detecting and Mitigating Shift\nDetecting domain shift before deployment prevents silent clinical failures. Statistical divergence measures comparing source and target distributions quantify distribution differences. Embedding visualizations (t-SNE or UMAP projections) reveal whether target examples fall within the source distribution or occupy unfamiliar regions of representation space. Monitoring performance on canary examples (known easy cases that should always be predicted correctly) provides early warning of severe shift during deployment.\nWhen domain shift is detected, mitigation strategies include domain-adaptive fine-tuning, importance weighting of training examples, and explicit modeling of shift through domain-adversarial training (Ganin et al. 2016). When shift is severe and cannot be mitigated, acknowledging that transfer is inappropriate for this context prevents overconfident deployment of models that will fail.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 9.5: [High] Three-panel visualization of domain shift detection. Panel A: UMAP/t-SNE projection of embeddings showing training distribution (dense cluster) and test examples at varying distances, with out-of-distribution examples clearly separated. Panel B: Calibration curves comparing confidence vs. accuracy for in-distribution (well-calibrated diagonal) vs. out-of-distribution examples (overconfident, curve below diagonal). Panel C: Performance degradation curve showing accuracy declining as distributional distance from training data increases.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-minimal-data",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-minimal-data",
    "title": "9  Transfer and Adaptation",
    "section": "9.9 Minimal-Data and Emerging Transfer Paradigms",
    "text": "9.9 Minimal-Data and Emerging Transfer Paradigms\nA geneticist studying a newly characterized neurodevelopmental disorder has identified 15 patients with variants in a previously unstudied gene. Functional studies confirm pathogenicity for 8 variants; the remaining 7 are benign. Training a classifier from 15 examples using standard supervised learning would be absurd, yet the clinical need for variant interpretation is immediate. Parents are waiting for diagnoses. Few-shot learning and zero-shot learning address these extreme data scarcity scenarios that characterize many genomic applications, where clinical urgency outpaces data availability. The rare disease diagnosis workflow in Chapter 26 illustrates how these methods integrate into clinical practice.\n\n9.9.1 Few-Shot Learning with Minimal Examples\nWhen only 10 to 100 examples per class exist, standard adaptation overfits catastrophically. The core insight of few-shot learning is that models can be trained explicitly for rapid adaptation by optimizing across many few-shot tasks during a meta-training phase (Finn, Abbeel, and Levine 2017). Model-Agnostic Meta-Learning (MAML) exemplifies this approach by finding parameter initializations that can be fine-tuned effectively from minimal data. The initialization represents a point in parameter space from which a few gradient steps reach good task-specific solutions. At deployment, the meta-trained model adapts to new tasks from a handful of labeled examples, having learned during meta-training what features are generally useful and how to adapt quickly.\nA simpler alternative avoids gradient updates at deployment entirely. Prototypical networks learn to embed sequences such that examples from the same class cluster together (Snell, Swersky, and Zemel 2017). At inference, class prototypes are computed as the mean embedding of the few available examples per class, and novel sequences are classified based on distance to prototypes. With 10 pathogenic and 10 benign variants as prototypes, novel variants are classified by which prototype cluster they fall nearest in embedding space. The approach requires no parameter updates during deployment, only forward passes to compute embeddings and distances.\n\n\n9.9.2 Zero-Shot Transfer Without Task-Specific Data\nThe most extreme adaptation scenario eliminates task-specific examples entirely. Zero-shot transfer makes predictions using only the pretrained model’s outputs, without any task-specific adaptation. For protein variant effect prediction, ESM log-likelihood ratios score variants by how much they reduce the model’s probability of the observed sequence (Meier et al. 2021). Variants that violate the model’s learned expectations for natural proteins (disrupting conserved residues, introducing destabilizing substitutions) receive low likelihood ratios, flagging them as potentially deleterious. This approach proves competitive with supervised methods for ClinVar pathogenicity prediction because evolutionary constraint (what masked language modeling learns to predict) correlates with functional importance (what pathogenicity classification measures). The ESM variant scoring methodology and its calibration to clinical thresholds are examined in Chapter 14.\nZero-shot methods require strong alignment between pretraining objectives and target tasks. When this alignment exists (evolutionary constraint predicts pathogenicity), zero-shot approaches provide immediate predictions without any labeled data. When alignment is weaker (tissue-specific regulatory activity depends on factors beyond sequence conservation), few-shot learning with even a handful of examples typically outperforms zero-shot baselines. For most practical genomic applications, some labeled data improves predictions; few-shot rather than true zero-shot represents the realistic minimal-data regime.\n\n\n9.9.3 Emerging Approaches\nSeveral paradigms are extending the boundaries of what minimal-data transfer can achieve. Very large language models exhibit the capacity for in-context learning, performing tasks by observing demonstrations rather than through explicit fine-tuning (Brown et al. 2020). Early evidence suggests genomic foundation models at sufficient scale may exhibit similar behavior, classifying variants based on a few pathogenic and benign examples included in the input prompt. This could transform deployment: rather than training adapters or fine-tuning parameters, practitioners would provide examples of desired behavior at inference time.\nAdaptation need not be confined to training time. Test-time adaptation updates models during inference based on characteristics of test examples rather than freezing parameters after training (D. Wang et al. 2021). For genomic applications facing distribution shift between development and deployment populations, test-time adaptation could adjust model behavior to match deployment-specific characteristics without requiring labeled examples from the deployment distribution. A model developed on European-ancestry data could adapt its uncertainty calibration when encountering African-ancestry variants that differ from training distributions.\nPrivacy constraints have motivated development of federated transfer learning, which enables collaborative model development across institutions without sharing raw genomic data (Rieke et al. 2020). Institutions train local models on private patient data and share only aggregated parameter updates, enabling foundation models to learn from far more diverse data than any single institution can access while preserving patient privacy. This approach could help address the population bias in current genomic datasets by enabling contributions from institutions serving underrepresented populations (see Chapter 2 for discussion of population representation in genomic databases).\n\n\n9.9.4 Toward Theoretical Foundations\nTheoretical foundations for predicting transfer success based on measurable properties of source and target domains would reduce trial-and-error (Ben-David et al. 2010). Currently practitioners must empirically test whether transfer helps; theoretical guidance specifying when transfer will succeed based on domain divergence measures, task similarity metrics, or representation analysis could focus effort on promising applications and avoid wasted investment in doomed transfer attempts.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-label-imbalance",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-label-imbalance",
    "title": "9  Transfer and Adaptation",
    "section": "9.10 Label and Class Imbalance",
    "text": "9.10 Label and Class Imbalance\nThe clinically important variants are precisely the ones that rarely appear in training data. A pathogenicity classifier might train on thousands of benign polymorphisms but only dozens of confirmed pathogenic variants in any given gene family. A splice disruption predictor might see millions of canonical splice sites but only hundreds of cryptic sites that cause disease. This extreme imbalance is not a data curation failure but a reflection of biology: pathogenic variants are rare because purifying selection removes them from populations, and rare variants are rarely observed because they are, by definition, rare. The challenge for transfer learning is that models adapted on imbalanced target data learn to predict the majority class with high confidence while failing on the minority class that matters most.\nClass imbalance creates problems at every stage of adaptation. During fine-tuning, gradient updates are dominated by the abundant class because most training examples belong to that class. A model fine-tuned on 10,000 benign variants and 100 pathogenic variants receives 100 times more gradient signal pushing it toward correct benign predictions than toward correct pathogenic predictions. Standard cross-entropy loss weights each example equally, so the model learns that predicting “benign” is almost always correct. Early stopping based on aggregate accuracy reinforces this bias: a model that classifies everything as benign achieves 99% accuracy and appears to have converged, yet provides no clinical value whatsoever.\nThe imbalance problem compounds across the genomic landscape. ClinVar contains roughly ten times more benign than pathogenic variants for well-studied genes like BRCA1, but the ratio exceeds 100:1 for less-studied genes where expert review is rare (Landrum et al. 2018). Population databases like gnomAD contain millions of variants, the vast majority benign common polymorphisms, while disease-causing variants constitute a tiny fraction concentrated in specific functional regions. When foundation models are adapted using these resources, the inherited imbalance creates systematic underconfidence for pathogenic predictions and systematic overconfidence for benign predictions.\n\n9.10.1 Manifestations During Transfer\nImbalance affects different adaptation strategies in different ways. Linear probing on frozen representations inherits whatever class structure exists in the embedding space. If pretrained representations do not separate rare pathogenic variants from common benign variants (because the pretraining objective emphasized patterns common in the training corpus, which are by definition not rare), no amount of reweighting during probe training can recover the missing information. The probing classifier may achieve perfect separation on training data through overfitting while failing completely on held-out pathogenic variants that occupy different regions of embedding space.\nParameter-efficient fine-tuning methods like LoRA can partially address imbalance by learning task-specific transformations, but they remain susceptible when the pathogenic signal is weak relative to the benign signal. If only a small number of adapter parameters are tuned and most gradients come from the majority class, the adapters learn transformations that improve majority-class predictions without capturing minority-class patterns. Increasing adapter rank provides more capacity but also more opportunity for overfitting to the few minority examples.\nFull fine-tuning offers the most flexibility to reshape representations for imbalanced tasks but carries the greatest overfitting risk. With 100 pathogenic examples and millions of model parameters, the model can memorize every pathogenic example while learning nothing generalizable about what makes variants pathogenic. The resulting model performs perfectly on training pathogenic variants and randomly on held-out pathogenic variants, a form of catastrophic overfitting invisible to training metrics dominated by the benign class.\n\n\n9.10.2 Mitigation Strategies\nAddressing class imbalance requires intervention at data, loss, and evaluation levels. No single strategy suffices; effective pipelines combine multiple approaches.\nResampling strategies modify the training distribution to achieve more balanced class representation. Oversampling the minority class by duplicating rare examples increases their influence on gradients, though excessive oversampling causes overfitting to specific minority examples. SMOTE and related methods generate synthetic minority examples by interpolating between existing examples, but interpolation in sequence space or embedding space may produce biologically implausible variants that mislead the model (chawla_smote_2002?). Undersampling the majority class reduces imbalance but discards potentially useful information; stratified undersampling that preserves the diversity of benign variants across variant types and genomic contexts performs better than random undersampling.\nLoss reweighting assigns higher penalties to minority-class errors. Inverse frequency weighting multiplies the loss for each class by the inverse of its training frequency, so a class comprising 1% of training data receives 100 times the loss weight. Class-balanced loss variants address the diminishing returns of adding more majority-class examples, weighting by effective number of samples rather than raw counts (cui_class-balanced_2019?). Focal loss downweights easy examples (confident correct predictions) to focus learning on hard examples, many of which are minority-class instances that the model currently misclassifies (lin_focal_2017?). These loss modifications change gradient magnitudes without changing gradient directions, so they work best when the model has sufficient capacity to learn minority-class patterns if given appropriate training signal.\nThreshold adjustment and calibration address the deployment manifestation of imbalance. A model trained on imbalanced data learns decision boundaries skewed toward predicting the majority class. Adjusting the classification threshold post-training, typically by lowering the threshold for minority-class predictions, can recover sensitivity without retraining. Platt scaling and temperature scaling recalibrate predicted probabilities to match observed frequencies, essential when downstream applications depend on accurate probability estimates rather than just rankings (see Section 23.3 for calibration methods). The appropriate threshold depends on deployment prevalence, which may differ from training prevalence; a variant predictor trained on balanced batches but deployed where 0.1% of variants are pathogenic requires threshold adjustment to avoid overwhelming clinical workflows with false positives.\nTwo-stage approaches train separate models for different aspects of the problem. A first-stage model distinguishes clearly benign variants from potentially interesting variants, filtering the vast majority of benign variants at high specificity. A second-stage model, trained on the filtered subset where class balance is more favorable, distinguishes pathogenic from uncertain among the remaining candidates. This cascade architecture reduces imbalance at each stage while maintaining overall pipeline sensitivity, though it requires careful coordination to avoid error compounding across stages.\n\n\n9.10.3 Evaluation Under Imbalance\nStandard metrics obscure imbalance-driven failures. Accuracy, which measures the fraction of correct predictions, reaches 99% when a model predicts “benign” for everything in a dataset with 1% pathogenic variants. This apparent success masks complete failure on the task that matters: identifying pathogenic variants.\nThe auPRC provides a more informative view for imbalanced classification. Unlike auROC, which measures pairwise ranking between one positive and one negative example, auPRC measures precision across recall levels where precision is explicitly sensitive to the number of false positives relative to true positives. When positives are rare, achieving high precision requires correctly ranking the vast majority of negatives below positives, not just typical negatives. A model moving from a balanced validation set to a 1000:1 imbalanced deployment setting will show stable auROC but collapsing auPRC, mirroring the explosion in false discovery rate that clinical users will experience (Section 21.5.1 examines this distinction in detail).\nStratified evaluation by class reveals whether aggregate metrics hide minority-class failure. Report sensitivity (true positive rate) and specificity (true negative rate) separately rather than combining them into a single number. Report precision at clinically relevant recall thresholds: if a diagnostic pipeline requires 95% sensitivity for pathogenic variants, what precision does the model achieve at that operating point? This stratified reporting reveals the tradeoffs that aggregate metrics obscure.\nConfidence intervals matter more for minority-class metrics. With 100 pathogenic variants in the test set, sensitivity estimates have wide confidence intervals purely from sampling variation. A model with true 80% sensitivity might show anywhere from 70% to 90% sensitivity on a particular test set by chance alone. Multiple test sets, bootstrap confidence intervals, or analytic interval calculations (Wilson score intervals for proportions) provide appropriate uncertainty quantification. Presenting point estimates without intervals overstates confidence in minority-class performance.\n\n\n9.10.4 Imbalance as Fundamental Constraint\nClass imbalance in genomic transfer learning reflects a fundamental biological reality rather than a correctable data curation problem. Pathogenic variants are rare because evolution works. Deleterious mutations are removed from populations by natural selection, so the variants that remain and accumulate in databases are predominantly benign. This creates a tension: the variants we most need to classify are the variants we have the least data to learn from.\nThis constraint shapes realistic expectations for transfer learning. A foundation model pretrained on typical genomic sequence has learned patterns of typical sequence, not patterns of pathogenic deviation from typical sequence. Transfer to pathogenic variant classification asks the model to extrapolate to a distribution it has never seen. Techniques for handling class imbalance mitigate but cannot eliminate this fundamental challenge. When only 50 pathogenic variants exist for a gene family, no amount of loss reweighting or sampling strategy can substitute for the missing data. The most honest response may be appropriate uncertainty quantification and abstention on predictions where evidence is insufficient (see Section 23.7).\nThe clinical implications extend beyond model performance to workflow design. If a variant classifier has 80% sensitivity for pathogenic variants, 20% of disease-causing variants will be missed. For a rare disease diagnosis where a single pathogenic variant determines diagnosis, this false negative rate translates directly to missed diagnoses and delayed treatment. Understanding class imbalance as a structural constraint rather than a tunable hyperparameter is essential for setting appropriate clinical expectations and designing safety-net workflows that catch the variants models miss.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-diagnosing-transfer",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-diagnosing-transfer",
    "title": "9  Transfer and Adaptation",
    "section": "9.11 Diagnosing Transfer: Validation and Failure Modes",
    "text": "9.11 Diagnosing Transfer: Validation and Failure Modes\nThe research team had done everything right. They selected a state-of-the-art DNA foundation model pretrained on diverse genomic sequences. They applied LoRA adaptation using 5,000 carefully curated training examples. Validation accuracy reached 88%. But when deployed on prospectively collected samples, performance collapsed to 62%, barely better than chance for a binary classification task. Transfer had failed. For the patients whose variants were misclassified during those weeks before the failure was detected, the consequences were real: delayed diagnoses, inappropriate treatments, unnecessary anxiety. Detecting transfer failure before deployment requires understanding its root causes.\n\n9.11.1 Diagnosing Negative Transfer\nNegative transfer occurs when pretraining actively hurts performance, producing adapted models worse than those trained from scratch on target data alone. Pretraining on human coding sequences may encode codon usage patterns and amino acid preferences that create false expectations when applied to bacterial sequences with different GC content and codon bias. Pretraining on healthy tissue samples may learn features of normal cellular function that prove misleading for cancer samples where regulatory programs are fundamentally altered. The pretrained initialization, rather than providing a useful starting point, creates an optimization landscape that leads to poor task-specific solutions (Z. Wang et al. 2019).\nDiagnostic steps identify whether transfer helps or hurts. The most fundamental comparison pits the adapted model against a from-scratch baseline trained on identical target data with equivalent hyperparameter tuning; if the pretrained model does not meaningfully outperform from-scratch training, transfer provides no benefit and the computational overhead of working with large pretrained models is wasted. Adaptation complexity should also scale appropriately: if linear probing fails, full fine-tuning rarely succeeds unless target data is very large, so simpler strategies should be exhausted before investing in more complex ones. Embedding visualization using dimensionality reduction can reveal whether pretrained representations contain task-relevant features; if target task classes are not separated in embedding space, the pretrained model may lack the representations the task requires. Finally, ablating pretraining entirely by comparing against randomly initialized models of identical architecture isolates whether pretrained weights provide value or whether architectural choices alone drive performance.\n\n\n9.11.2 Remediation When Transfer Fails\nWhen diagnostics reveal fundamental mismatches, several remediation strategies apply. Task-specific pretraining on data more closely aligned with target requirements can bridge the gap; pretraining specifically on regulatory regions for regulatory prediction tasks rather than genome-wide pretraining may produce more suitable representations. Hybrid architectures combining pretrained and randomly-initialized components allow selective use of transfer where it helps while avoiding its limitations elsewhere. Trying alternative foundation models whose pretraining objectives better match task requirements may reveal that the problem was model selection rather than transfer learning generally. And accepting that transfer provides no benefit for this specific task, proceeding with from-scratch training, remains a valid conclusion when evidence supports it.\n\n\n9.11.3 Validation and Common Pitfalls\nA research group reports that their foundation model adaptation achieves 95% accuracy for splice variant classification, far exceeding previous methods. Six months later, clinical deployment reveals performance closer to 70%, with systematic failures on the rare variants that matter most. The initial evaluation was not wrong, but it was misleading. Proper validation separates genuine transfer success from evaluation artifacts that dissolve on contact with clinical reality. The benchmark landscape and its limitations are examined in Chapter 20, evaluation methodology in Chapter 21, and systematic sources of inflated performance in Chapter 22.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 9.6: [High] Multi-panel figure illustrating common validation failures. Panel A: Venn diagram showing pretraining data / test data overlap creating leakage (inflated performance). Panel B: Timeline showing temporal leakage when training on variants annotated after test set creation. Panel C: Bar chart comparing foundation model against weak baseline (large gap, misleading) vs. properly-tuned baseline (small gap, realistic). Panel D: Stratified performance showing aggregate accuracy (90%) vs. rare-variant accuracy (50%), revealing hidden failures.\n\n\n\n\n\n9.11.4 Sources of Spurious Success\nTest set overlap with pretraining data creates artificial performance inflation. Foundation models trained on massive genomic corpora may inadvertently include sequences later used for evaluation. When benchmarking on variants that appeared in pretraining data (even if unlabeled at pretraining time), the model has seen the sequences and may have memorized relevant patterns. Verifying that test sequences were excluded from pretraining requires careful provenance tracking, which published benchmarks often lack (Sainz et al. 2023). Chromosome-based splits help but do not fully address the problem when pretraining spans multiple species or includes population-level diversity (see Chapter 22 for detailed treatment of data leakage).\nTemporal leakage uses future information unavailable at prediction time. Evaluating a variant pathogenicity model on variants annotated after training data was collected creates an unrealistically favorable setting; the model may have seen related variants or learned from the same evidence that later informed annotations. Temporal splits (training on variants discovered before a cutoff, evaluating on variants discovered afterward) provide more realistic assessment of prospective performance (Landrum et al. 2018).\nInappropriate baselines inflate apparent transfer benefits. Comparing adapted foundation models against weak or poorly-tuned from-scratch baselines makes transfer look more valuable than warranted. Strong baselines require equivalent hyperparameter tuning, appropriate architectures for the task, and sufficient training on the same target data. When properly-tuned CNNs match or exceed foundation model performance for a task, the additional complexity of pretrained models may not be justified.\n\n\n9.11.5 Evaluation Practices That Reveal True Performance\nSingle-metric reporting obscures important performance characteristics. A model achieving 90% overall accuracy may show 95% accuracy on common variants and 50% accuracy on rare variants, with the clinically important rare cases hidden by aggregate metrics. Stratified evaluation by allele frequency, variant type, gene family, and other clinically relevant categories reveals whether transfer benefits generalize or concentrate in particular subgroups.\nConfidence interval reporting and multiple training runs reveal performance variability. A single training run may produce misleadingly good or bad results through random initialization effects or data sampling. Testing on multiple independent datasets rather than a single benchmark reveals whether gains generalize beyond the specific evaluation setting.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-case-studies",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-case-studies",
    "title": "9  Transfer and Adaptation",
    "section": "9.12 Case Studies in Transfer Learning",
    "text": "9.12 Case Studies in Transfer Learning\nTransfer succeeds when pretraining objectives align with downstream requirements; it fails when they diverge. Four cases illustrate the conditions that distinguish success from failure.\n\n9.12.1 DNABERT for Chromatin Accessibility\nJi et al. pretrained DNABERT using masked language modeling on \\(k\\)-mer tokenized human genomic sequence (Ji et al. 2021). For ATAC-seq peak classification, they applied linear probes to [CLS] token embeddings without updating backbone parameters. The approach achieved competitive performance with CNNs trained from scratch while requiring approximately 10 times less labeled data.\nSuccess reflected strong alignment between pretraining and target task: both involve recognizing local sequence motifs (transcription factor binding sites, nucleosome positioning signals) that determine chromatin state. The pretrained representations already encoded the relevant patterns; the linear probe simply learned to separate accessible from inaccessible regions in this well-structured embedding space.\n\n\n9.12.2 ESM for Variant Pathogenicity\nRives et al. pretrained ESM on UniRef protein sequences using masked language modeling (Rives et al. 2021). For ClinVar pathogenicity classification, Meier et al. showed that zero-shot scoring based on variant effects on sequence likelihood proved competitive with supervised methods (Meier et al. 2021). Adding a linear probe on ESM embeddings improved performance further, but the zero-shot baseline was already strong.\nSuccess reflected implicit alignment: evolutionary constraint (what masked language modeling captures) correlates with functional importance (what pathogenicity measures). The pretraining objective, though never explicitly targeting variant classification, learned representations directly relevant to it.\n\n\n9.12.3 Enformer for Cross-Tissue Expression\nAvsec et al. pretrained Enformer on thousands of chromatin and expression tracks spanning dozens of cell types (Avsec et al. 2021). Fine-tuning with tissue-specific prediction heads for individual cell types captured regulatory logic unavailable from frozen features, outperforming both frozen Enformer and from-scratch models trained on individual tissues.\nSuccess required both the large scale of pretraining (establishing general sequence-to-function mappings) and extensive fine-tuning data (enabling tissue-specific adaptation). With smaller fine-tuning datasets, the approach would have overfit; without diverse pretraining, the model would have lacked transferable regulatory knowledge.\n\n\n9.12.4 Cross-Species Regulatory Prediction\nModels pretrained on human regulatory sequences and applied to zebrafish enhancer prediction often underperform zebrafish-specific models despite the apparent relevance of regulatory sequence patterns (Kelley 2020). The failure reflects both sequence divergence (zebrafish regulatory motifs differ from human) and lineage-specific regulatory innovations (teleost-specific enhancers have no human homologs from which to transfer). Cross-species contrastive learning approaches (Section 8.5) offer one strategy for building representations that emphasize conserved features over species-specific patterns.\nThe boundary between success and failure corresponds to evolutionary conservation: patterns shared across species transfer; species-specific patterns do not. Transfer succeeds for deeply conserved elements (core promoters, splice sites) but fails for lineage-specific regulatory logic.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-conclusion",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-conclusion",
    "title": "9  Transfer and Adaptation",
    "section": "9.13 What Transfers, What Breaks",
    "text": "9.13 What Transfers, What Breaks\nTransfer learning amplifies the value of pretrained models by connecting learned representations to specific applications. A foundation model pretrained on billions of sequences encodes patterns that would require orders of magnitude more labeled data to learn from scratch. Effective transfer realizes this investment; ineffective transfer inherits hidden limitations without the promised benefits.\nThe risks are concrete. Domain shift between pretraining and deployment contexts causes silent failures: models trained on research cohorts may miscalibrate on clinical populations, models trained on one species may fail unpredictably on another, models trained on one assay technology may not generalize to its successor. These failures produce confident predictions that are systematically wrong, often in ways that correlate with clinically relevant subgroups. Detection through distribution divergence measures and embedding visualization can identify shift before deployment, but mitigation requires either domain-adaptive fine-tuning or acceptance that some shifts cannot be bridged.\nValidating transfer claims requires adversarial rigor. Test for contamination between pretraining and evaluation data through sequence-level deduplication. Implement temporal splits that respect real-world prediction scenarios. Compare against properly-tuned baselines trained from scratch with equivalent effort. Stratify performance by ancestry, variant type, and other clinically meaningful categories. The goal is establishing whether transfer provides genuine benefit under realistic deployment conditions, not optimizing for favorable benchmarks. Foundation model applications assume that transfer succeeds; the methods here determine whether that assumption holds for specific contexts.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nBen-David, Shai, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. “A Theory of Learning from Different Domains.” Machine Learning 79 (1): 151–75. https://doi.org/10.1007/s10994-009-5152-4.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=transaction&utm_medium=email&utm_campaign=linkedin_newsletter.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. https://doi.org/10.48550/arXiv.1810.04805.\n\n\nFinn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.” In Proceedings of the 34th International Conference on Machine Learning, 1126–35. PMLR. https://proceedings.mlr.press/v70/finn17a.html.\n\n\nGaedigk, Andrea, Magnus Ingelman-Sundberg, Neil A. Miller, J. Steven Leeder, Michelle Whirl-Carrillo, Teri E. Klein, and the PharmVar Steering Committee. 2018. “The Pharmacogene Variation (PharmVar) Consortium: Incorporation of the Human Cytochrome P450 (CYP) Allele Nomenclature Database.” Clinical Pharmacology & Therapeutics 103 (3): 399–401. https://doi.org/10.1002/cpt.910.\n\n\nGanin, Yaroslav, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. 2016. “Domain-Adversarial Training of Neural Networks.” Journal of Machine Learning Research 17 (59): 1–35. http://jmlr.org/papers/v17/15-239.html.\n\n\nHoulsby, Neil, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. “Parameter-Efficient Transfer Learning for NLP.” In Proceedings of the 36th International Conference on Machine Learning, 2790–99. PMLR. https://proceedings.mlr.press/v97/houlsby19a.html.\n\n\nHoward, Jeremy, and Sebastian Ruder. 2018. “Universal Language Model Fine-Tuning for Text Classification.” arXiv. https://doi.org/10.48550/arXiv.1801.06146.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2106.09685.\n\n\nJawahar, Ganesh, Benoît Sagot, and Djamé Seddah. 2019. “What Does BERT Learn about the Structure of Language?” In ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy. https://inria.hal.science/hal-02131630.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nKelley, David R. 2020. “[Basenji2] Cross-Species Regulatory Sequence Activity Prediction.” PLOS Computational Biology 16 (7): e1008050. https://doi.org/10.1371/journal.pcbi.1008050.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory M. Cooper, and Jay Shendure. 2014. “A General Framework for Estimating the Relative Pathogenicity of Human Genetic Variants.” Nature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nMartin, Alicia R., Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada, Benjamin M. Neale, and Mark J. Daly. 2019. “Clinical Use of Current Polygenic Risk Scores May Exacerbate Health Disparities.” Nature Genetics 51 (4): 584–91. https://doi.org/10.1038/s41588-019-0379-x.\n\n\nMcCloskey, Michael, and Neal Cohen. 1989. “Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem.” In Psychology of Learning and Motivation, 24:109–65. Academic Press. https://doi.org/10.1016/S0079-7421(08)60536-8.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRieke, Nicola, Jonny Hancox, Wenqi Li, Fausto Milletarì, Holger R. Roth, Shadi Albarqouni, Spyridon Bakas, et al. 2020. “The Future of Digital Health with Federated Learning.” Npj Digital Medicine 3 (1): 119. https://doi.org/10.1038/s41746-020-00323-1.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSainz, Oscar, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. “NLP Evaluation in Trouble: On the Need to Measure LLM Data Contamination for Each Benchmark.” In Findings of the Association for Computational Linguistics: EMNLP 2023, edited by Houda Bouamor, Juan Pino, and Kalika Bali, 10776–87. Singapore: Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.findings-emnlp.722.\n\n\nSnell, Jake, Kevin Swersky, and Richard Zemel. 2017. “Prototypical Networks for Few-Shot Learning.” In Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. “UniRef: Comprehensive and Non-Redundant UniProt Reference Clusters.” Bioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nWang, Dequan, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. 2021. “Tent: Fully Test-Time Adaptation by Entropy Minimization.” arXiv. https://doi.org/10.48550/arXiv.2006.10726.\n\n\nWang, Zirui, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. 2019. “Characterizing and Avoiding Negative Transfer.” In, 11293–302. https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Characterizing_and_Avoiding_Negative_Transfer_CVPR_2019_paper.html.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer and Adaptation</span>"
    ]
  },
  {
    "objectID": "part_3/p3--architectures.html",
    "href": "part_3/p3--architectures.html",
    "title": "Part III: Foundation Model Families",
    "section": "",
    "text": "Each architecture embodies a different set of assumptions about biological sequence. Convolutional models assume that local motifs and their short-range combinations are the primary carriers of regulatory information; they learn to recognize transcription factor binding sites, splice signals, and chromatin accessibility patterns from the sequence grammar immediately surrounding each position. Protein language models treat amino acid sequences as structured compositions whose meaning emerges from evolutionary context; they learn what substitutions are tolerated by observing which sequences survived natural selection. DNA language models extend this paradigm to nucleotides, learning regulatory grammar through self-supervised objectives that predict masked or next tokens. Hybrid architectures attempt to reconcile local and global perspectives, using convolutions to extract features efficiently while deploying attention to model interactions spanning tens or hundreds of kilobases. Understanding these assumptions clarifies what each model family can capture and where each will fail.\nFoundation models in genomic deep learning span distinct architectural families, each with characteristic strengths. Foundational principles and taxonomy (10  Foundation Model Paradigm) establish what defines a foundation model and provide a framework for navigating the rapidly expanding ecosystem. DNA language models (11  DNA Language Models), including DNABERT, Nucleotide Transformer, and HyenaDNA, apply self-supervised pretraining to genomic sequence, learning representations that transfer across diverse downstream tasks. Protein language models (12  Protein Language Models) achieved the earliest and most dramatic foundation model successes; ESM, ProtTrans, and their descendants emerged alongside AlphaFold2 in 2020, collectively demonstrating that deep learning could capture protein structure and function from sequence alone. AlphaFold2 revolutionized structure prediction through its Evoformer architecture, and AlphaMissense subsequently adapted that architecture for proteome-wide variant effect prediction. Hybrid architectures (13  Regulatory Models), including Enformer, Borzoi, and AlphaGenome, combine convolutional processing with transformer blocks to achieve context windows spanning hundreds of kilobases, enabling direct prediction of gene expression from sequence. Variant effect prediction (14  Variant Effect Prediction) synthesizes these approaches, translating foundation model representations into pathogenicity scores across variant types and genomic contexts.",
    "crumbs": [
      "Part III: Foundation Model Families"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html",
    "href": "part_3/p3-ch10-fm-principles.html",
    "title": "10  Foundation Model Paradigm",
    "section": "",
    "text": "10.1 From Task-Specific Models to Foundation Models\nThe deep learning era in genomics began with fragmentation. One convolutional network predicted transcription factor binding; another predicted chromatin accessibility; a third classified splice sites. Each model required its own training data, its own hyperparameter tuning, its own validation strategy. A laboratory studying gene regulation might train a dozen specialized models, none of which could inform the others. Knowledge learned for predicting enhancer activity provided no benefit for predicting splicing outcomes, even though both tasks depend on recognizing sequence patterns shaped by the same evolutionary pressures. The field accumulated tools without accumulating shared knowledge.\nFoundation models promise a different approach: train once on vast genomic data, then adapt to many downstream tasks. A single model pretrained on billions of nucleotides might provide representations useful for regulatory prediction, variant interpretation, sequence design, and cross-species analysis simultaneously. Rather than curating labeled datasets for each new question, researchers could fine-tune existing models on modest task-specific data, leveraging knowledge the model acquired during pretraining. The efficiency gains would be substantial; the conceptual shift would be larger still. Where specialized models treat each genomic question as independent, foundation models assume that shared patterns underlie diverse biological phenomena and that representations capturing those patterns should transfer.\nThis paradigm, which transformed natural language processing and protein structure prediction, carries both promise and peril for genomics. Pretraining at scale requires computational resources beyond most academic budgets. Adaptation to specific tasks demands expertise in transfer learning techniques that remain poorly understood (Chapter 9). Predictions from general-purpose models may lack the precision of specialized alternatives trained directly on task-specific data. The decision to use, adapt, or build foundation models involves tradeoffs that depend on available resources, target applications, and acceptable uncertainty.\nThe history of computational genomics reveals a consistent pattern: models become more general while maintaining or improving task-specific performance. Hand-crafted scores such as CADD and SIFT established that integration of diverse genomic annotations could improve variant pathogenicity prediction (Rentzsch et al. 2019; Schubach et al. 2024) (Chapter 4). These approaches faced a ceiling imposed by the features available for engineering, a limitation examined in Section 4.6.4. These approaches relied on expert feature engineering, combining conservation scores, functional annotations, and population frequency data through ensemble methods or logistic regression.\nTask-specific deep learning models demonstrated that neural networks could learn relevant features directly from sequence. DeepSEA predicted chromatin accessibility and transcription factor binding from 1 kb sequences using convolutional architectures (J. Zhou and Troyanskaya 2015). ExPecto extended this approach to gene expression prediction by modeling regulatory elements across multiple cell types (J. Zhou et al. 2018). Sei organized regulatory predictions into interpretable sequence classes through unsupervised clustering (Chen et al. 2022). SpliceAI achieved near-perfect splice site prediction through dilated convolutions over 10 kb contexts, though its architecture was purpose-built for this specific task and could not generalize to other regulatory prediction problems (Chapter 6). Enformer scaled sequence-to-function modeling to 200 kb windows and thousands of chromatin tracks through transformer architectures (Avsec et al. 2021).\nThese models succeeded within their specific domains but remained difficult to repurpose. Training a DeepSEA model required chromatin accessibility data. Using SpliceAI for regulatory prediction would require complete retraining on different labels. Each application domain needed its own model, trained from scratch on task-specific data. The fundamental limitation was not model capacity but training paradigm: supervised learning on narrow tasks produced narrow capabilities.\nSequence language models introduced the self-supervised pretraining paradigm (Chapter 8) to genomics. DNABERT applied masked language modeling to DNA sequences, demonstrating that general representations could be learned without task-specific labels (Ji et al. 2021) (Section 11.2). ESM and ESM-2 showed that protein language models pretrained on sequence alone could transfer effectively to structure prediction, variant effect prediction, and protein design (Rives et al. 2021; Lin et al. 2022) (Section 12.1). The Nucleotide Transformer family scaled DNA language modeling to cross-species training corpora (Dalla-Torre et al. 2023) (Section 11.3). HyenaDNA used implicit convolutions to reach million-token contexts at single-nucleotide resolution (Nguyen et al. 2023) (Section 11.5.1).\nThe transition from task-specific to foundation models changes the relationship between model developers and users. Task-specific models deliver predictions as their primary product. Foundation models deliver representations that users adapt to their own tasks through the transfer learning techniques examined in Chapter 9. This distinction affects everything from model architecture design to evaluation strategies to deployment infrastructure.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-task-specific",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-task-specific",
    "title": "10  Foundation Model Paradigm",
    "section": "",
    "text": "FIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 10.1: [Essential] Two-panel comparison showing the paradigm shift. Panel A (Task-Specific Era): Multiple independent models (DeepSEA, SpliceAI, ExPecto) shown as separate pipelines, each with its own training data, architecture, hyperparameters; arrows showing no knowledge transfer between models; Label: “Train separately, no shared learning.” Panel B (Foundation Model Era): Single large pretrained backbone at center, multiple lightweight adaptation heads branching out, downstream tasks (regulatory, splicing, expression, variant effect) as leaf nodes, shared representation layer highlighted; Label: “Pretrain once, adapt many.”",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-defining",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-defining",
    "title": "10  Foundation Model Paradigm",
    "section": "10.2 Defining Genomic Foundation Models",
    "text": "10.2 Defining Genomic Foundation Models\nThe term “foundation model” appears frequently in genomics literature, sometimes applied to any large neural network trained on biological sequences. Establishing clear criteria helps separate true genomic foundation models from ordinary deep learning approaches that happen to operate on DNA or protein sequences.\n\n10.2.1 Essential Properties\nThe defining characteristic of genomic foundation models is their capacity to serve purposes far beyond their original training objectives. This generality emerges from several interconnected properties.\nFoundation models train on entire genomes, pan-genomic sequence collections, or large assay compendia with minimal supervision. Their pretraining objectives include masked language modeling, next-token prediction, denoising, or multi-task sequence-to-function prediction. Critically, these objectives do not require dense task-specific labels for every training example. A model that requires annotated enhancers or curated pathogenic variants for every training instance does not qualify as a foundation model under this criterion.\nThe representations these models produce must prove useful across many downstream tasks. Embeddings can be extracted through forward passes and reused with simple linear probes or lightweight adapter modules rather than requiring full model retraining. The representations should encode biological information at multiple scales, from local sequence motifs to long-range regulatory grammar.\nTransfer capability extends across multiple dimensions: different assays (from chromatin accessibility to gene expression), different tissues and cell types, different species, and different variant types (from SNVs to structural variants). Evidence of broad transfer requires evaluation across multiple benchmarks rather than demonstration of performance on a single task (Chapter 20).\nFoundation models operate at a scale that would be impractical for task-specific training. Some scale context length, as HyenaDNA scales to million-token windows at single-nucleotide resolution. Others scale parameter count, as the ESM and Nucleotide Transformer families reach billions of parameters. Still others scale data diversity through pan-genomic pretraining across hundreds of species or integration of many assays and cell types. The scaling dimension chosen reflects the model’s intended applications and architectural constraints.\nFinally, foundation models typically expose consistent APIs for common operations. These include embedding extraction for sequences or variants, sequence probability scoring, and mask-based in silico mutagenesis for variant effect prediction. Models distributed through repositories such as Hugging Face often include documented recipes for downstream fine-tuning and example notebooks demonstrating common use cases.\n\n\n10.2.2 What does not Count\nMany excellent genomic models fail one or more of these criteria and should not be classified as foundation models. Early versions of DeepSEA trained specifically on chromatin accessibility data from a limited set of cell types lack the generality and standardized interface of foundation models, though later iterations that integrate many assays begin to approach foundation model territory (J. Zhou and Troyanskaya 2015). SpliceAI predicts splicing outcomes exceptionally well but was designed for that specific task and provides neither general-purpose embeddings nor easy transfer to other genomic prediction problems (Jaganathan et al. 2019). Even a very large Enformer-like model trained solely on human chromatin tracks remains bound to its specific prediction interface despite its scale and sophistication (Avsec et al. 2021).\nThe distinction matters for several reasons. It affects evaluation strategy, since foundation models must be assessed across families of tasks rather than single benchmarks (Chapter 21). It affects integration into existing pipelines, since foundation models serve as feature extractors while task-specific models typically provide end-to-end predictions. It affects how we think about model development, since foundation model training requires different infrastructure and data curation than task-specific supervised learning.\n\n\n10.2.3 Limitations of the Foundation Model Concept\nThe term “foundation” carries implications worth examining. Architectural foundations are static, load-bearing, and invisible once construction proceeds. Genomic foundation models share only the load-bearing property: they support downstream applications that would otherwise require independent construction. Yet unlike architectural foundations, these models remain visible and modifiable throughout their use. Fine-tuning adjusts the foundation itself rather than building atop an immutable base. The metaphor also implies that foundations precede and enable all subsequent work, but genomic foundation models often coexist with task-specific alternatives that outperform them on narrow benchmarks.\nA more accurate metaphor might be “foundation” in the educational sense: a broad base of knowledge that enables specialized learning but continues to develop alongside it. The pretraining phase establishes general competence; adaptation refines that competence for specific purposes without abandoning the original learning. This framing better captures the dynamic relationship between pretrained representations and downstream tasks, though the architectural metaphor has become standard terminology.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-scaling",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-scaling",
    "title": "10  Foundation Model Paradigm",
    "section": "10.3 Scaling Laws and Compute-Optimal Training",
    "text": "10.3 Scaling Laws and Compute-Optimal Training\nThe success of foundation models in natural language processing rests partly on empirical scaling laws: predictable relationships between model size, training data, computational budget, and performance. Understanding these relationships guides resource allocation and model development decisions.\n\n10.3.1 Scaling Law Framework\nScaling laws describe how model performance (typically measured as loss on held-out data) varies with three key quantities: the number of model parameters (N), the amount of training data (D), and the total compute budget (C). The Chinchilla scaling analysis demonstrated that for a fixed compute budget, there exists an optimal balance between model size and training data (Hoffmann et al. 2022). Training a model that is too large on too little data, or too small on too much data, yields worse performance than compute-optimal allocation.\nThe practical implications are significant. Many early large language models were undertrained relative to their parameter count. GPT-3’s 175 billion parameters were trained on roughly 300 billion tokens, while Chinchilla-optimal training would suggest either fewer parameters or more data. The Chinchilla model itself matched GPT-3 performance with only 70 billion parameters trained on 1.4 trillion tokens.\nFor genomic foundation models, scaling relationships are less well characterized but increasingly important. Several key questions arise: Do genomic sequences exhibit the same scaling behavior as natural language? How does the finite size of reference genomes constrain data scaling? What role does sequence diversity (cross-species, population variation) play in data scaling? Can synthetic data augmentation extend effective training data beyond natural sequences?\n\n\n10.3.2 Empirical Scaling in Genomic Models\nSeveral genomic foundation model families have reported scaling experiments, though systematic scaling laws comparable to NLP remain elusive.\nThe Nucleotide Transformer family provides perhaps the clearest genomic scaling data (Dalla-Torre et al. 2023). Performance on downstream benchmarks improves consistently with parameter count across models from 50 million to 2.5 billion parameters. The largest models (trained on multi-species data) outperform smaller models trained on human sequences alone, suggesting that cross-species data provides effective scaling even when human-specific performance is the target. Training compute scaled from approximately 10^19 to 10^21 FLOPs across the model family.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 10.2: [Essential] Three-panel figure illustrating scaling relationships. Panel A (Loss vs. Parameters): Log-log plot showing validation loss decreasing with parameter count; data points from Nucleotide Transformer family (50M to 2.5B); ESM-2 scaling curve overlaid for comparison; power law fit line with exponent annotation. Panel B (Compute-Optimal Frontier): Iso-performance contours on parameter × data plane; Chinchilla optimal line showing balanced allocation; undertrained region shaded; genomic data ceiling annotated (approximately 10^12 nucleotides). Panel C (Emergent Capabilities): Capability (y-axis) vs. scale (x-axis, log); multiple curves showing different capabilities emerging at different scales; threshold markers where capabilities “switch on.”\n\n\n\nESM-2 demonstrated similar scaling for protein language models, with performance on structure prediction and variant effect tasks improving smoothly from 8 million to 15 billion parameters (Lin et al. 2022) (Chapter 12). The largest ESM-2 models approach the structure prediction accuracy of AlphaFold2 using only single-sequence input, a capability entirely absent in smaller models. This represents a qualitative capability threshold crossed through scale.\nHyenaDNA focused on context length scaling rather than parameter scaling, demonstrating that million-token contexts at single-nucleotide resolution could be achieved through sub-quadratic architectures (Nguyen et al. 2023). The relationship between context length and downstream performance is task-dependent: some tasks (local motif recognition) show saturation at kilobase scales, while others (long-range regulatory prediction) continue improving with longer contexts.\n\n\n10.3.3 Compute-Optimal Decisions for Genomics\nThe scaling law framework has direct implications for model development decisions in genomics, though the constraints differ fundamentally from natural language processing.\nUnlike natural language, where text data is effectively unlimited, genomic sequence data faces hard constraints. Reference genomes for well-studied species total perhaps 10^11 to 10^12 nucleotides. Population-level variant data can expand this somewhat, but the effective diversity may be lower than raw counts suggest. In such data-constrained regimes, smaller models trained to convergence may outperform larger models that are undertrained.\nAcademic groups typically face stricter compute constraints than industry labs. Given fixed compute budgets, the Chinchilla framework suggests allocating resources toward longer training of smaller models rather than abbreviated training of larger models. A 500 million parameter model trained for 10 epochs on diverse genomic data may outperform a 5 billion parameter model trained for 1 epoch on the same data.\nCross-species data offers a potential path around genomic data limitations. Training on genomes from hundreds of species provides sequence diversity that human genomes alone cannot. The Nucleotide Transformer and Evo families exploit this strategy, learning evolutionary patterns from diverse genomes that improve human-specific predictions (Chapter 11). However, the transfer efficiency from non-human species to human tasks varies by application and remains poorly characterized.\n\n\n10.3.4 Emergent Capabilities\nPerhaps the most intriguing aspect of foundation model scaling is the emergence of qualitatively new capabilities at sufficient scale. Emergence refers to abilities that are absent or negligible in smaller models but appear discontinuously as models grow.\nIn large language models, emergent capabilities include multi-step reasoning, code generation, and in-context learning. These capabilities appear at model scales of roughly 10^10 parameters and above, with no clear precursor in smaller models.\nGenomic foundation models exhibit analogous emergence, though the capability thresholds are less well characterized. The most striking example involves structural understanding from sequence: ESM-2 at sufficient scale produces contact maps and secondary structure predictions from single sequences with accuracy approaching multiple sequence alignment methods (Lin et al. 2022). Smaller ESM models show no meaningful structural understanding. This capability emerges at approximately 650 million parameters and continues improving with scale.\nLarger Nucleotide Transformer models transfer more effectively to novel species not seen during training (Dalla-Torre et al. 2023). The ability to generalize beyond training species appears to require sufficient model capacity to learn abstract regulatory principles rather than memorizing species-specific patterns. Similarly, foundation models at sufficient scale can predict variant effects without task-specific fine-tuning, using only the difference in likelihood between reference and alternative sequences. This zero-shot capability requires models large enough to capture subtle sequence dependencies.\nThe most recent genomic foundation models show preliminary evidence of in-context learning: the ability to adapt to new tasks based on examples provided in the input context without parameter updates. This capability, central to the utility of large language models, remains nascent in genomic models but may emerge with further scaling.\nThe practical implication is that capability thresholds exist: models below certain scales may be fundamentally incapable of certain tasks regardless of fine-tuning. Identifying these thresholds helps guide model selection and prevents wasted effort fine-tuning models that lack necessary capacity.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-taxonomy",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-taxonomy",
    "title": "10  Foundation Model Paradigm",
    "section": "10.4 A Taxonomy of Genomic Foundation Models",
    "text": "10.4 A Taxonomy of Genomic Foundation Models\nThe landscape of genomic foundation models can be organized into four broad families. Each family exhibits distinct characteristics, strengths, limitations, and typical application domains.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\n\n\nFigure 10.3: [Essential] 2×2 grid with families as quadrants. Quadrant 1 (DNA Language Models, Blue): DNABERT, Nucleotide Transformer, HyenaDNA, Evo 2. Quadrant 2 (Protein Language Models, Purple): ESM-2, ProtTrans, ESMFold. Quadrant 3 (Sequence-to-Function Models, Green): Enformer, Borzoi, AlphaGenome. Quadrant 4 (Multi-Omic Models, Orange): Omni-DNA, cross-modal architectures. For each: representative models, input/output types, pretraining approach, strengths, limitations. Connecting lines showing where models can feed into each other.\n\n\n\n\n10.4.1 DNA Language Models\nDNA language models treat genomic sequence as a language to be modeled, learning representations from raw nucleotide strings through self-supervised objectives. Without explicit functional labels, these models discover patterns through statistical regularities in genomic sequence.\nThe pretraining objectives typically involve masked language modeling or autoregressive next-token prediction. Training draws from reference genomes or pan-genomic sequence collections spanning multiple species. The resulting models produce per-position or pooled sequence embeddings that can be extracted and used for downstream tasks. Critically, these embeddings are not tied to specific assays or cell types, making them applicable to any task that benefits from general sequence context.\nDNABERT and DNABERT-2 apply BERT-style masked language modeling to DNA sequences, using overlapping k-mers as tokens (Ji et al. 2021; Z. Zhou et al. 2024) (Section 11.2). The Nucleotide Transformer family scales this approach to larger parameter counts and cross-species training (Dalla-Torre et al. 2023) (Section 11.3). HyenaDNA achieves subquadratic complexity through implicit convolutions, enabling context lengths up to one million nucleotides (Nguyen et al. 2023) (Section 11.5.1). Caduceus incorporates bidirectional processing and reverse-complement equivariance as architectural inductive biases (Section 11.5.2). Evo 2 combines long-range attention with biological tokenization strategies (Section 11.5.3). GROVER integrates learned BPE-style tokenization with training on regulatory tracks in addition to raw sequence (Sanabria et al. 2024). These models and their architectural innovations are examined in detail in Chapter 11.\nThe primary strength of DNA language models lies in their generality: representations not bound to specific assays, cell types, or experimental conditions, capable of processing novel sequences absent from reference genomes. Their self-supervised training requires only genome sequences, making them scalable to massive corpora. The corresponding limitation is that without explicit functional grounding, they may not capture subtle regulatory patterns that manifest only under specific cellular conditions. Performance on tasks requiring fine-grained functional discrimination may lag models trained with functional supervision.\nApplications span sequence classification (promoters, enhancers, transposons), motif discovery, variant effect prediction through embedding perturbation, sequence generation for synthetic biology, and transfer learning to new species with limited labeled data.\n\n\n10.4.2 Sequence-to-Function Foundation Models\nSequence-to-function models predict molecular readouts directly from sequence through supervised or semi-supervised training on assay compendia. These models blur into foundation model territory when their output space is sufficiently broad and their internal representations prove useful for tasks beyond the original assay set.\nThese models map DNA sequences to high-dimensional vectors of molecular measurements, including chromatin accessibility, histone modifications, transcription factor binding, and gene expression levels. Training uses large collections of functional genomics assays spanning many cell types, enabling the models to learn regulatory grammar through supervised prediction of molecular phenotypes.\nEnformer predicts thousands of chromatin and expression tracks from 200 kb sequence windows through transformer attention (Avsec et al. 2021) (Section 13.2). Borzoi extends this with refined architectures and expanded RNA-seq coverage (Section 13.3). Sei organizes predictions into interpretable sequence classes through unsupervised clustering (Chen et al. 2022) (Section 13.4). Earlier models including DeepSEA and Basset established the paradigm at smaller scales (Chapter 6).\nThe explicit functional supervision in these models provides mechanistic grounding that pure language models lack. Predictions can be interpreted through comparison to experiments. The models naturally support variant effect prediction by computing reference-alternative differences. The tradeoff is that models remain tied to training assays and cell types; extension to new contexts typically requires retraining or new data collection.\nApplications center on regulatory variant interpretation in well-studied cell types, eQTL fine-mapping, enhancer identification, transcription factor binding prediction, and regulatory mechanism discovery.\n\n\n10.4.3 Variant Effect Prediction Models\nThe clinical need to interpret genetic variants has driven development of models optimized specifically for predicting functional or clinical consequences. These take a variant and predict its effect on molecular phenotypes, organismal fitness, or disease risk.\nVariant effect prediction models integrate sequence context with evolutionary information, population genetics signals, and sometimes structural or functional annotations. They output pathogenicity scores, effect size estimates, or functional consequence predictions. Training combines multiple data sources: clinical labels from ClinVar, population frequency from gnomAD, functional assays such as deep mutational scanning, and evolutionary constraint metrics.\nAlphaMissense applies protein language models to predict pathogenicity of missense variants (Cheng et al. 2023) (Section 14.2.3). ESM-1v uses evolutionary context for protein variant effect prediction (Section 14.2.1). EVE combines evolutionary and structural information (Section 14.2.2). Genomic foundation models like DNABERT and Enformer provide variant effect predictions through in silico mutagenesis (Section 14.3). The architecture, training, evaluation, and clinical deployment of variant effect predictors are covered comprehensively in Chapter 14, with integration into clinical workflows detailed in Chapter 26.\n\n\n10.4.4 Multi-Omic Foundation Models\nThe most ambitious foundation models natively integrate multiple molecular modalities, jointly processing DNA sequence, chromatin state, gene expression, protein abundance, 3D genome structure, or phenotypic descriptions.\nMulti-omic models employ architectures designed for heterogeneous input types: transformer variants with cross-attention, graph neural networks, or modality-specific encoders with fusion layers (Chapter 7, Chapter 18). Training objectives encourage cross-modal alignment through contrastive learning, joint prediction, or generative modeling of multiple data types.\nOmni-DNA uses transformer-based autoregressive models with vocabulary expansion and multi-task finetuning, unifying diverse genomic tasks under an instruction-response paradigm (Li et al. 2025). Models integrating Hi-C data capture 3D genome organization (Chapter 17). Cross-modal architectures align DNA embeddings with chromatin or expression predictions (Chapter 19).\nThe unified representations these models produce enable cross-modal queries, and joint training can improve performance through multi-task effects. Data engineering becomes substantially more complex, however, with different modalities requiring different measurement technologies and quality control. The field is early, with few models reaching production maturity.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-design-dimensions",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-design-dimensions",
    "title": "10  Foundation Model Paradigm",
    "section": "10.5 Design Dimensions",
    "text": "10.5 Design Dimensions\nWithin and across families, individual models differ along orthogonal design dimensions that affect suitability for specific tasks.\n\n10.5.1 Data Composition\nThe choice of training data shapes what patterns a model can learn. Training on human sequences alone focuses on clinically relevant patterns but limits exposure to evolutionary diversity. Cross-species training encourages learning of conserved elements and evolutionary constraints, potentially improving generalization but risking dilution of human-specific signals.\nSequence diversity presents a similar tradeoff. Training on reference genomes alone provides clean sequences but limited exposure to population variation. Incorporating variant data improves robustness but requires careful design to avoid learning spurious associations. Models may also train on raw sequence alone or incorporate functional annotations, trading generality against functional grounding. The implications of training data choices for model bias are examined in Chapter 22.\n\n\n10.5.2 Architecture Choices\nArchitectural decisions determine both computational characteristics and inductive biases. Among transformer variants, encoder-only models (DNABERT, Nucleotide Transformer) excel at classification and embedding tasks, while decoder-only models (GROVER) support generative applications (Chapter 7). Full and sparse attention patterns, linear approximations, and Flash attention implementations affect computational efficiency.\nHyena-based models and state space models achieve subquadratic scaling, enabling longer contexts than standard transformers with comparable parameters. Hybrid approaches combine local convolutions with global attention, as in Enformer, processing sequences at multiple resolutions.\n\n\n10.5.3 Context Length\nThe context window determines what genomic relationships a model can capture. Short context (under 1 kb) captures local patterns: motifs, splice sites, promoter elements. Medium context (1 to 10 kb) spans complete genes with proximal regulatory regions. Long context (10 to 200 kb) represents enhancer-promoter interactions and TAD-scale organization. Ultra-long context (over 200 kb) enables chromosomal domain modeling and complex structural variant interpretation. The effective use of long context requires appropriate tokenization and positional encoding strategies discussed in Chapter 5, with specific implementations examined in Section 11.2 (k-mer tokenization), Section 11.3 (BPE variants), and Section 7.2 (position embeddings).\n\n\n10.5.4 Tokenization\nThe representation of nucleotides as model inputs affects both computational efficiency and biological resolution. Character-level tokenization maintains single-base resolution but imposes longest sequence lengths. K-mer tokenization reduces length by a factor approaching \\(k\\), with vocabulary reaching 4,096 for 6-mers. Learned tokenization (BPE-style) discovers schemes from data, potentially allocating vocabulary more efficiently (Medvedev et al. 2025). The choice should align with both computational constraints and biological resolution requirements. Detailed discussion of tokenization strategies appears in Chapter 5.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 10.4: [High] Multi-axis comparison (radar/spider plot or parallel coordinates) showing how different models make different design choices. Axes: (1) Context Length: 512bp to 1Mb; (2) Parameter Count: 10M to 15B; (3) Species Coverage: Human-only to Pan-genomic; (4) Architecture Type: CNN to Transformer to Hybrid to SSM; (5) Tokenization: Character to \\(k\\)-mer to BPE to Codon-aware; (6) Pretraining Objective: MLM to Autoregressive to Multi-task supervised. Models plotted: DNABERT-2, HyenaDNA, Evo 2, Enformer, Nucleotide Transformer.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-build-vs-use",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-build-vs-use",
    "title": "10  Foundation Model Paradigm",
    "section": "10.6 Build Versus Use Decisions",
    "text": "10.6 Build Versus Use Decisions\nThe availability of pretrained foundation models creates strategic choices about when to use existing models, when to adapt them, and when to train from scratch.\n\n10.6.1 When to Use Existing Models\nExisting foundation models provide immediate utility when the target application aligns with model capabilities, labeled data is limited, and computational resources are constrained.\nFor tasks where general sequence representations suffice, frozen foundation model embeddings with simple downstream classifiers often perform competitively with fine-tuned alternatives. This approach requires minimal compute (single forward passes), no gradient computation through large models, and modest labeled data (hundreds to thousands of examples). Applications include sequence classification, clustering, and similarity search.\nSome foundation models support zero-shot variant effect prediction through likelihood ratio scoring. This requires no task-specific training and produces calibrated scores for novel variants immediately. Zero-shot approaches work well when the pretraining objective aligns with the target task and when fine-tuning data is unavailable or unreliable.\nFoundation model APIs also enable rapid prototyping, allowing quick assessment of whether a modeling approach is viable before committing resources to custom development. Testing variant effect prediction with ESM-1v takes hours rather than the weeks required to train a custom model.\n\n\n10.6.2 When to Adapt Existing Models\nAdaptation through fine-tuning or lightweight methods (LoRA, adapters, prefix tuning) makes sense when downstream tasks require specialized behavior beyond what frozen embeddings provide, sufficient labeled data exists (typically thousands to tens of thousands of examples), and the target domain falls within the pretraining distribution.\nParameter-efficient methods like LoRA update a small fraction of model parameters (often under 1%) while keeping the foundation model frozen (Hu et al. 2021). This preserves general knowledge while allowing task-specific adaptation. Compute requirements are modest: a few GPU-hours for most genomic tasks. The approach works well when the foundation model’s representations are largely appropriate but need refinement for specific applications. Details on parameter-efficient adaptation appear in Chapter 9.\nUpdating all parameters typically achieves the best single-task performance but requires more data (tens of thousands of examples), more compute (GPU-days to weeks), and careful regularization to prevent overfitting. Full fine-tuning makes sense for high-stakes applications where maximum accuracy justifies the investment.\n\n\n10.6.3 When to Train from Scratch\nBuilding custom foundation models requires substantial justification given the resources involved.\nNovel domains present the clearest case for custom pretraining. When target sequences differ fundamentally from existing model pretraining data (novel species, synthetic sequences, non-standard nucleotides), existing models may provide poor transfer. Applications requiring architectural features absent from existing models (specific attention patterns, custom tokenization, multi-modal inputs) similarly demand building from scratch.\nOrganizations with unique large-scale datasets (clinical biobanks, pharmaceutical screening data) may achieve better performance through custom pretraining than public models allow, though the data advantage must be substantial to justify training costs. Applications requiring larger models or longer contexts than available options face similar calculus.\n\n\n10.6.4 Cost-Benefit Analysis\nThe decision framework involves comparing expected performance against resource requirements.\nTraining a foundation model from scratch requires 10^20 to 10^22 FLOPs, translating to thousands of GPU-hours and tens of thousands of dollars at current cloud prices. Fine-tuning requires 10^16 to 10^18 FLOPs, often achievable in hours on single GPUs. Inference with frozen embeddings requires only forward passes.\nFoundation model pretraining requires billions of tokens. Fine-tuning requires thousands to tens of thousands of labeled examples. Zero-shot and embedding approaches require only evaluation data.\nFor well-studied tasks with abundant labeled data, fine-tuned models typically outperform frozen embeddings by 5 to 15% on standard metrics. Zero-shot approaches often achieve 70 to 90% of fine-tuned performance. Custom foundation models rarely outperform existing options by large margins unless the application involves genuinely novel domains.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 10.5: [High] Decision flowchart with cost-benefit annotations. Entry Point: “New genomic prediction task.” Decision nodes leading to terminal recommendations: USE (Frozen embeddings + simple classifier, Hours/$10, 70-90% of fine-tuned); ADAPT (LoRA/adapter fine-tuning, Days/$100-1000, 95% of full fine-tuning); BUILD (Custom pretraining, Months/$100K+, potentially best for domain). Annotations: “Most applications land here” pointing to USE/ADAPT paths; “Rare but sometimes necessary” pointing to BUILD.\n\n\n\nTime costs often dominate: using existing models takes hours to days, fine-tuning takes days to weeks, training from scratch takes weeks to months. For time-sensitive applications, using existing models often dominates even if custom training would eventually yield better results.\nThe practical recommendation for most applications: start with frozen embeddings from the most appropriate existing foundation model. If performance is insufficient, try parameter-efficient fine-tuning. Train from scratch only if adaptation fails and the application justifies the investment.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-evaluation",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-evaluation",
    "title": "10  Foundation Model Paradigm",
    "section": "10.7 Evaluation Principles",
    "text": "10.7 Evaluation Principles\nFoundation models resist evaluation on single tasks. Their value lies in transfer across many applications, making comprehensive evaluation substantially more complex than benchmarking task-specific models.\n\n10.7.1 Multi-Task Assessment\nA genomic foundation model should be evaluated across families of related tasks rather than isolated benchmarks. For DNA language models, this includes sequence classification tasks, variant effect prediction across multiple variant types, motif discovery, and cross-species transfer. For sequence-to-function models, evaluation should span prediction of held-out assays, transfer to novel cell types, and consistency with experimental measurements.\nThe diversity of evaluation tasks complicates comparison across models. A model excelling at promoter classification may underperform on eQTL fine-mapping. Direct comparisons require controlling for differences in training data, model scale, and evaluation protocols. Standardized benchmark suites are examined in Chapter 20.\n\n\n10.7.2 Transfer Versus Pretraining Performance\nFoundation models are intended for transfer, making pretraining loss only moderately predictive of downstream utility. A model with slightly worse masked language modeling loss may produce better embeddings if its training objective better aligns with useful representations. Evaluation should explicitly test transfer through zero-shot performance, few-shot learning, cross-domain transfer, and robustness to distribution shift.\nDetailed discussion of benchmark suites, evaluation protocols, and methodological best practices appears in Chapter 20 and Chapter 21.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-ecosystem",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-ecosystem",
    "title": "10  Foundation Model Paradigm",
    "section": "10.8 Foundation Model Ecosystem",
    "text": "10.8 Foundation Model Ecosystem\nGenomic foundation models exist within a broader ecosystem of infrastructure, community resources, and shared practices.\n\n10.8.1 Model Distribution\nMost models are distributed through centralized repositories. Hugging Face hosts many DNA and protein language models with documented APIs. GitHub repositories accompany publications with weights, code, and examples. Standardized formats reduce friction in adoption, enabling rapid benchmarking and experimentation.\n\n\n10.8.2 Documentation Requirements\nResponsible distribution requires comprehensive documentation: training data provenance, preprocessing procedures, architecture details, hyperparameters, evaluation protocols, and known limitations. Data provenance is particularly important given population-specific biases and use restrictions in genomic datasets (Chapter 22).\n\n\n10.8.3 Industry and Academic Contributions\nBoth academic and industry groups develop genomic foundation models. Academic models emphasize reproducibility and open access. Industry models may offer superior performance through proprietary data or compute but with limited transparency. Notable industry contributions include NVIDIA’s BioNeMo platform and Microsoft’s Azure genomics integration. Users should review license terms before clinical or commercial deployment.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-open-questions",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-open-questions",
    "title": "10  Foundation Model Paradigm",
    "section": "10.9 Open Questions",
    "text": "10.9 Open Questions\nDespite rapid progress, fundamental challenges remain unsolved, and the field’s trajectory remains uncertain.\nWhether genomic foundation models converge toward unified architectures or maintain specialized families is unclear. The diversity of genomic scales, resolution requirements, and functional contexts may preclude the convergence seen in NLP, where transformers now dominate across most tasks.\nExisting models learn correlations without distinguishing causal from spurious relationships. Integrating causal structure could improve robustness and enable counterfactual reasoning, but current architectures provide no principled mechanism for causal inference (Chapter 22).\nModels trained on reference genomes and common variants may not calibrate well for ultra-rare or de novo variants, precisely the variants most likely to be clinically actionable (Chapter 26). Improved integration of structural and evolutionary constraints could strengthen rare variant interpretation.\nTranslation to clinical use requires robust cross-population performance, calibrated uncertainty (Chapter 23), interpretability for clinicians (Chapter 24), prospective validation, and regulatory approval. These requirements extend well beyond benchmark performance, and the path from research model to clinical deployment remains poorly charted.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch10-fm-principles.html#sec-ch10-convergence",
    "href": "part_3/p3-ch10-fm-principles.html#sec-ch10-convergence",
    "title": "10  Foundation Model Paradigm",
    "section": "10.10 Convergence Without Consolidation",
    "text": "10.10 Convergence Without Consolidation\nFoundation models for genomics divide into families serving different needs. DNA language models learn general sequence representations from self-supervised pretraining, capturing evolutionary constraints and regulatory patterns without explicit functional labels (Chapter 11). Sequence-to-function models predict molecular phenotypes from sequence, providing quantitative outputs (expression levels, chromatin states, splice probabilities) that DNA language models alone cannot produce (Chapter 13). Variant effect models integrate sequence representations with evolutionary information to score the functional impact of genetic variants (Chapter 14). Multi-omic models combine sequence with additional data modalities to capture regulatory relationships that sequence alone cannot resolve (Chapter 19). No single family dominates; effective genomic AI requires matching model capabilities to application requirements.\nScale introduces both opportunities and constraints. Scaling laws describe predictable relationships between parameters, data, compute, and performance, enabling principled resource allocation. Some capabilities appear only at sufficient scale, creating thresholds that cannot be crossed through fine-tuning alone. The practical implication is that certain applications require institutional-scale investment, while others can leverage existing pretrained models with modest adaptation. The build-versus-use framework guides this decision: use existing models when they suffice, adapt through fine-tuning or feature extraction when needed, train from scratch only when unique data or requirements justify the investment.\nThis framework instantiates across specific domains. DNA language models (Chapter 11) and protein language models (Chapter 12) exemplify self-supervised pretraining on biological sequence. Regulatory models (Chapter 13) demonstrate sequence-to-function prediction at long-range scales. Variant effect prediction (Chapter 14) integrates multiple model families for clinical interpretation. Throughout, these principles guide model selection: what does this application require, which model family provides it, and what scale is necessary to achieve it?\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2203.15556.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2106.09685.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLi, Zehui, Vallijah Subasri, Yifei Shen, Dongsheng Li, Yiren Zhao, Guy-Bart Stan, and Caihua Shan. 2025. “Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning.” arXiv. https://doi.org/10.48550/arXiv.2502.03499.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html",
    "href": "part_3/p3-ch11-dna-lm.html",
    "title": "11  DNA Language Models",
    "section": "",
    "text": "11.1 From Task-Specific CNNs to General-Purpose Language Models\nThe transformer revolution in natural language processing rested on a simple insight: statistical patterns in unlabeled text contain information about grammar, semantics, and even world knowledge. Train a model to predict masked words from context, and it learns not just vocabulary but the structure of language itself. BERT, GPT, and their successors demonstrated that self-supervised learning on raw text yields representations useful for tasks the model was never explicitly trained to perform. Proteins proved amenable to the same approach: models trained to predict masked amino acids learned evolutionary constraints, structural properties, and functional relationships without explicit supervision (Chapter 12). DNA presents the analogous opportunity. If genomes encode a regulatory language, perhaps self-supervised learning on raw nucleotide sequence could discover its grammar.\nDNA language models import this paradigm to nucleotide sequences. Rather than training separate models for each genomic prediction task, as the convolutional neural network (CNN) era required (Chapter 6), these approaches learn general-purpose representations from unlabeled genomes that transfer across applications. A single pretrained backbone can support regulatory element classification, variant effect prediction, cross-species analysis, and sequence generation through different downstream heads or adaptation strategies. The same model that learns to predict masked nucleotides can, after fine-tuning (Chapter 9), predict chromatin accessibility in cell types it never saw during pretraining (Chapter 8), identify splice sites without splice-specific training data, and score variant effects using evolutionary patterns learned from billions of nucleotides.\nThe opportunity is substantial but not guaranteed to succeed. Protein sequences have clear functional units (domains, secondary structures, binding sites) that language model representations can capture. DNA sequences present a different challenge: regulatory grammar operates at multiple scales simultaneously, from six-nucleotide transcription factor binding sites through kilobase-scale enhancers to megabase chromatin domains. Whether self-supervised learning can discover this multi-scale grammar remains an empirical question.\nThe convolutional neural networks examined in Chapter 6 achieved strong performance on specific genomic prediction tasks, though they faced the feature ceiling limitation discussed in Section 4.6.4: performance bounded by what architectural choices and training data could capture. DeepSEA predicted chromatin marks from sequence; SpliceAI identified splice junctions with clinical utility; ExPecto estimated expression effects of variants. Each model was engineered for its particular application, with architectural choices (filter sizes, dilation patterns, pooling strategies) optimized for the task at hand.\nThis paradigm succeeded but imposed three constraints that limited scalability. Every new assay, cell type, or phenotype required fresh labeled data; a model trained on ENCODE chromatin data could not predict histone modifications in a new cell type without additional labeled examples. Model architecture was bound to specific prediction problems: SpliceAI’s dilated convolutions were tailored for splice junction detection, and ExPecto’s spatial transformation was designed for the distance-dependent relationship between regulatory elements and transcription start sites. These architectural choices, while effective, did not transfer naturally to other problems. Features learned for one task could not easily support others; a model that learned to recognize transcription factor binding sites during chromatin accessibility training could not directly apply those representations to variant effect prediction without substantial re-engineering.\nProtein language models demonstrated an alternative. ESM and related models trained on massive corpora of protein sequences using masked language modeling (predicting held-out amino acids from context) or autoregressive objectives (predicting the next amino acid). The resulting representations transferred to structure prediction, function annotation, and variant effect scoring without architecture changes (Chapter 12). DNA language models import this recipe: pretrain on large collections of genomic sequences using self-supervised objectives, then adapt the learned representations to downstream tasks through probing, fine-tuning, or zero-shot scoring.\nThe practical workflow begins with training a language model on unlabeled genomic sequences to predict masked or subsequent nucleotides. From the trained model, embeddings are extracted for sequences of interest (windows around variants, regulatory elements, or entire genes). These embeddings then support downstream tasks through probing with lightweight classifiers, fine-tuning for specific applications, or zero-shot scoring via probability comparisons. Once a sufficiently powerful backbone exists, it becomes the default starting point for nearly any DNA-level prediction problem.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-task-specific-to-general",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-task-specific-to-general",
    "title": "11  DNA Language Models",
    "section": "",
    "text": "FIGURE PLACEHOLDER\n\n\n\n\nFigure 11.1: [Essential] Horizontal timeline with milestones and architectural innovations. Key milestones: 2021 DNABERT (512 tokens, proof of concept); 2023 Nucleotide Transformer (6kb, multi-species, scaling); 2023 HyenaDNA (1 Mb, sub-quadratic); 2024 Caduceus (reverse-complement equivariance, Mamba); 2024-2025 Evo 2 (1 Mb, 7B-40B params, pan-genomic). Upper track: Context length progression (log scale). Lower track: Key architectural innovations at each stage.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-dnabert",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-dnabert",
    "title": "11  DNA Language Models",
    "section": "11.2 DNABERT: The First DNA Language Model",
    "text": "11.2 DNABERT: The First DNA Language Model\nDNABERT applied the BERT masked language modeling framework to genomic sequences, establishing proof of concept for DNA self-supervision (Ji et al. 2021). The model used overlapping k-mers (typically 6-mers) as tokens, creating a vocabulary of 4,096 tokens from the 4^6 possible hexamers. This tokenization strategy, detailed in Section 5.2, provided computational efficiency at the cost of positional ambiguity for variants. Training on the human reference genome, DNABERT learned to predict masked tokens from surrounding context using the standard BERT architecture.\nThe design choices reflected computational constraints of the time. The \\(k\\)-mer tokenization provided some sequence compression compared to single-nucleotide representations, but the overlapping nature (each nucleotide participates in multiple adjacent k-mers) meant the compression was modest and created ambiguity about precise variant positions. Context windows were limited to 512 tokens, corresponding to a few hundred base pairs of genomic sequence. The standard transformer architecture with quadratic attention complexity made longer contexts computationally prohibitive, a limitation examined in Chapter 7 and resolved by the architectural innovations in Section 11.5.1 and Section 11.5.2.\nDespite these limitations, DNABERT demonstrated several important principles. Fine-tuning on downstream tasks (promoter classification, splice site prediction, transcription factor binding site identification) achieved competitive performance with task-specific models trained from scratch. Learned embeddings captured biologically meaningful patterns, with similar sequences clustering together in embedding space even when trained only on the reference genome. The BERT-style architecture could be reused across multiple tasks with modest adaptation.\nDNABERT-2 addressed the tokenization limitations through improved approaches including BPE-style token merging that better compressed repetitive sequences (Zhou et al. 2024). The resulting model could represent longer genomic contexts within the same number of tokens, improving computational efficiency. On standardized benchmarks spanning sequence classification, regulatory element prediction, and variant effect scoring (Chapter 20), DNABERT-2 achieved consistent gains over both the original DNABERT and non-pretrained baselines. These improvements validated the importance of thoughtful tokenization design for genomic applications (see Chapter 5 for detailed discussion of tokenization strategies).\nThe DNABERT family collectively established that self-supervision on DNA works, that tokenization choices substantially affect performance, and that masked language model training produces reusable representations for diverse sequence tasks. The foundation model paradigm transfers effectively from natural language to genomic sequence.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-nucleotide-transformer",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-nucleotide-transformer",
    "title": "11  DNA Language Models",
    "section": "11.3 Nucleotide Transformer: Scaling Data and Model Diversity",
    "text": "11.3 Nucleotide Transformer: Scaling Data and Model Diversity\nDNABERT demonstrated feasibility but operated at modest scale relative to the size of genomes. The Nucleotide Transformer family pushed substantially further, emphasizing diversity in both training data and model architecture (Dalla-Torre et al. 2023).\nThe training corpus spanned genomic data from multiple species and human populations, exposing models to diverse sequence patterns, different regulatory architectures, and evolutionary constraints recurring across lineages. This cross-species pretraining mirrors the use of large multi-species alignments in protein language models but operates directly on raw DNA without explicit alignment. Context length expanded to approximately 6 kb per input sequence, representing an order-of-magnitude increase over DNABERT while still using dense transformer attention. The training objective remained masked language modeling on subsequences sampled from genomes.\nThe Nucleotide Transformer project introduced a benchmark panel that has become a standard yardstick for evaluating DNA language models. Tasks include promoter and enhancer classification, histone mark and chromatin accessibility prediction, splice site identification, and regulatory element type classification. Models are evaluated through linear probes or light fine-tuning on standardized train/validation/test splits. This benchmark infrastructure enabled systematic comparison across models and established the evaluation protocols now used throughout the field (see Chapter 20 for comprehensive discussion of genomic benchmarks).\nScaling experiments revealed predictable relationships between model size, training data, and performance. Larger models with more pretraining data and longer context windows achieved better downstream performance, following patterns observed in natural language and protein modeling. These scaling trends suggest that continued investment in larger genomic language models will yield further improvements, though the optimal allocation between parameters, data, and compute remains an active research question (Chapter 10).",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-gpn",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-gpn",
    "title": "11  DNA Language Models",
    "section": "11.4 GPN: Cross-Species Pretraining for Variant Effect Prediction",
    "text": "11.4 GPN: Cross-Species Pretraining for Variant Effect Prediction\nWhile the Nucleotide Transformer demonstrated the value of scaling, the Genomic Pretrained Network (GPN) explored a complementary direction: what can be learned from cross-species pretraining on relatively small, well-annotated genomes (Benegas, Batra, and Song 2023). Rather than scaling to maximum size, GPN asked whether self-supervision could yield useful variant effect predictors even in constrained settings.\nGPN was trained on unaligned reference genomes from Arabidopsis thaliana and seven related species within the Brassicales order using masked language modeling. Despite this modest training corpus, analysis revealed emergent encoding of gene structure (exon-intron boundaries, splice sites) and DNA sequence motifs (transcription factor binding patterns) without explicit supervision. The model discovered these patterns purely from statistical regularities of genomic sequence across related species.\nFor variant effect prediction, GPN used a likelihood ratio approach. Given reference and alternate alleles at a position, the model computes the log-likelihood of each under the learned sequence distribution. Variants that substantially reduce sequence likelihood (relative to the reference) are inferred to be more disruptive. This scoring strategy exploits the fact that constrained positions should have confident predictions for the reference allele, while unconstrained positions allow more flexibility.\nEvaluated on A. thaliana variants using allele frequencies from the 1001 Genomes Project, GPN outperformed traditional conservation scores including phyloP and phastCons (Benegas, Batra, and Song 2023). This was notable because phyloP and phastCons require explicit multiple sequence alignments and evolutionary models, while GPN learned its representations from unaligned sequences through self-supervision alone. The later GPN-MSA extended this approach to mammalian genomes by incorporating multi-species alignments, achieving strong performance on human variant benchmarks (Section 14.3.3). The success of this approach informed subsequent development of zero-shot variant scoring methods for clinical applications (Section 26.1.4).\nGPN established that cross-species pretraining captures evolutionary constraints transferable to variant effect prediction, that relatively small models trained on focused phylogenetic groups can outperform larger generic conservation measures within that group, and that the masked language modeling objective naturally produces representations suitable for variant scoring via likelihood comparisons.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-long-context",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-long-context",
    "title": "11  DNA Language Models",
    "section": "11.5 Long-Context Revolution",
    "text": "11.5 Long-Context Revolution\nQuadratic attention complexity limits transformer context to tens of kilobases at best. Processing a 100 kb sequence with dense attention requires on the order of 10^10 computations per layer. Yet regulatory phenomena routinely span larger distances: enhancer-promoter interactions extend 50-200 kb, topologically associating domains organize chromatin at the megabase scale, and some gene regulation involves even longer-range dependencies. The three-dimensional organization of chromatin that enables these long-range contacts is examined in Chapter 17; here we focus on how linear sequence models can capture information about these interactions. The mismatch between biological context and computational context represented a fundamental architectural limitation.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 11.2: [Essential] Two-panel comparison. Panel A (Computational Complexity): Log-log plot showing sequence length (x) vs. compute/memory (y); standard attention \\(O(L^2)\\) steep curve; Hyena/Mamba \\(O(L)\\) or \\(O(L \\log L)\\) much flatter; annotated points at \\(1\\ \\mathrm{kb}\\), \\(10\\ \\mathrm{kb}\\), \\(100\\ \\mathrm{kb}\\), \\(1\\ \\mathrm{Mb}\\) showing tractability. Panel B (Biological Context Coverage): Same x-axis; biological features overlaid as ranges (TF binding \\(\\sim 10\\)–\\(20\\ \\mathrm{bp}\\), promoter \\(\\sim 1\\ \\mathrm{kb}\\), gene body \\(\\sim 10\\)–\\(50\\ \\mathrm{kb}\\), enhancer-promoter \\(\\sim 20\\)–\\(200\\ \\mathrm{kb}\\), TAD \\(\\sim 100\\ \\mathrm{kb}\\)–\\(1\\ \\mathrm{Mb}\\)); vertical lines showing model context limits.\n\n\n\n\n11.5.1 HyenaDNA: Megabase Context via Implicit Convolutions\nHyenaDNA addressed this limitation by replacing attention with implicit convolutions that scale sub-quadratically (Nguyen et al. 2023). The Hyena architecture parameterizes long convolutional filters through neural networks rather than storing explicit filter weights, achieving \\(O(L \\log L)\\) complexity through efficient FFT-based convolution compared to \\(O(L^2)\\) for standard attention. The result was a 500-fold increase in context length: HyenaDNA processes sequences up to 1 Mb while maintaining single-nucleotide resolution.\nProcessing megabase-scale windows allows the model to capture entire gene bodies plus flanking regulatory regions, long-range enhancer-promoter interactions, and topologically associating domain structure. Despite the long context, single-nucleotide tokens preserve maximum resolution for variant effect prediction. Each nucleotide is independently represented without the ambiguity introduced by \\(k\\)-mer tokenization.\nOn Nucleotide Transformer benchmarks, HyenaDNA achieved state-of-the-art results on the majority of tasks with orders of magnitude fewer parameters. On GenomicBenchmarks, it surpassed prior state-of-the-art on seven of eight datasets (Nguyen et al. 2023). Perhaps most notably, HyenaDNA demonstrated in-context learning in genomics: performance improved when examples were included in the input context without updating model weights. This capability, familiar from large language models, had not previously been observed for genomic sequences and suggests that sufficient context length combined with appropriate architecture enables qualitatively new forms of biological reasoning.\n\n\n11.5.2 Caduceus: Bidirectional Processing with Reverse-Complement Equivariance\nDNA is double-stranded, and any sequence can be read from either strand. The reverse complement of a sequence encodes the same information from the opposite strand’s perspective. For many biological processes, predictions should be identical or related consistently regardless of which strand is presented. Standard neural networks can produce divergent predictions for a sequence and its reverse complement, even with data augmentation during training.\nCaduceus addressed this challenge by building reverse-complement equivariance directly into the architecture (Schiff et al. 2024). The model extends the Mamba state space architecture (which achieves O(L) complexity) to support both bidirectional processing and strand equivariance. The BiMamba component enables information flow in both directions along the sequence, while the MambaDNA block ensures mathematically related predictions for sequences and their reverse complements.\nOn downstream benchmarks, Caduceus outperformed previous long-range models. On challenging long-range variant effect prediction tasks, it exceeded models with ten times as many parameters that lacked bidirectionality or equivariance (Schiff et al. 2024). The key insight was that incorporating appropriate biological inductive biases can substitute for raw scale. Strand symmetry is a known property of DNA; building it into the architecture avoids wasting model capacity learning what could be specified directly.\n\n\n11.5.3 Evo 2: Genome-Scale Modeling Across the Tree of Life\nThe original Evo model demonstrated that DNA language models could operate at unprecedented scale (Nguyen et al. 2024). Trained on 2.7 million prokaryotic and phage genomes comprising 300 billion nucleotides, Evo processed sequences up to 131 kilobases using the StripedHyena architecture, a hybrid design combining state-space models with attention mechanisms. The 7 billion parameter model exhibited emergent biological understanding: predicting gene essentiality, identifying functional elements, and generating synthetic sequences with plausible biological properties. Crucially, Evo demonstrated that training on raw DNA sequence alone, without annotation, could yield models that captured fundamental aspects of genome organization.\nEvo 2 extends this foundation to the entire tree of life (Brixi et al. 2025). Where Evo focused primarily on prokaryotes and phages, Evo 2 incorporates eukaryotic genomes with their dramatically different organization: extensive noncoding regions, complex regulatory architectures, and intronic sequences that comprise the majority of gene length in many species. This expansion required both larger models and longer context windows to capture the sprawling regulatory landscapes characteristic of eukaryotic genomes.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 11.3: [High] Three-panel diagram. Panel A (The Problem): Input sequence and reverse complement; standard model gives potentially different predictions; annotation: “Same biological information, inconsistent predictions.” Panel B (Caduceus Architecture): BiMamba component with bidirectional arrows; MambaDNA block with weight-sharing scheme; mathematical relationship: f(revcomp(x)) = g(f(x)). Panel C (Performance Impact): Bar chart comparing models with/without equivariance on long-range tasks; annotation: “Appropriate biological inductive biases can substitute for raw scale.”\n\n\n\nThe training corpus draws from the OpenGenome2 dataset comprising 9.3 trillion DNA tokens across all domains of life, a 30-fold increase over the original Evo training data. This massive scale exposes the model to the full spectrum of genomic organization: compact prokaryotic gene arrangements, sprawling eukaryotic regulatory landscapes with extensive noncoding sequence, viral genomes with overlapping reading frames, and the diversity of regulatory architectures across evolution. The model comes in 7 billion and 40 billion parameter variants, with the larger model extending well beyond the original Evo’s scale.\nThe architecture builds on StripedHyena 2, refining the hybrid design that proved effective in the original Evo. The combination of convolutional operations with selective attention mechanisms enables processing of sequences up to 1 million nucleotides, nearly an order of magnitude beyond Evo’s 131 kilobase context. Like its predecessor, Evo 2 uses an autoregressive training objective (predicting the next base given all previous bases), which differs from the masked language modeling used in DNABERT and related models. Autoregressive training may provide complementary strengths for sequence generation and likelihood-based scoring, since the model learns to generate plausible sequences in addition to discriminating between them (Chapter 8).\nEvo 2 exhibits several forms of emergent biological knowledge despite training only on raw sequence, extending the capabilities first observed in Evo. The model learns to identify exon-intron boundaries without explicit annotation, identifies transcription factor binding site patterns matching known motifs, captures aspects of protein secondary and tertiary structure when processing coding sequences, and identifies prophage insertion regions in bacterial genomes. Where Evo demonstrated these capabilities primarily in prokaryotic contexts, Evo 2 generalizes them across eukaryotic genomes with their more complex gene structures.\nFor variant effect prediction, Evo 2 enables zero-shot scoring through likelihood ratios. Variants can be scored for consistency with learned genomic patterns by comparing model probabilities for reference versus alternate sequences. On benchmarks of pathogenic versus benign variants, zero-shot scores achieve competitive performance with specialized supervised methods, though calibration remains necessary before clinical application (Section 14.3.3). The calibration methods required for clinical deployment are examined in Section 23.2, and integration into diagnostic workflows in Section 26.1.4. The model also supports classification of variants of uncertain significance through simple classifiers trained on its embeddings.\nThe pan-species training enables cross-species applications that extend Evo’s prokaryotic focus to the full breadth of biology. Variant interpretation extends naturally to non-model organisms, supporting conservation genomics and agricultural breeding where labeled training data is scarce. Model representations cluster sequences by phylogenetic relationships even without explicit evolutionary modeling. Beyond discriminative tasks, Evo 2 demonstrates generative capabilities building on Evo’s initial demonstrations: synthesizing plausible mitochondrial genomes, prokaryotic operons, and eukaryotic regulatory regions with coherence across kilobase to megabase scales.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-training-data",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-training-data",
    "title": "11  DNA Language Models",
    "section": "11.6 Training Data and What Models Learn",
    "text": "11.6 Training Data and What Models Learn\nDNA language models are trained on diverse corpora ranging from single reference genomes to pan-genomic collections spanning the tree of life. Understanding what training data is used and what models learn from it is essential for anticipating model capabilities and limitations.\n\n11.6.1 Training Corpus Composition\nEarly models like DNABERT trained primarily on the human reference genome (GRCh38), providing exposure to approximately 3 billion nucleotides from a single individual. The Nucleotide Transformer expanded to include multiple species and human population variation from resources like the 1000 Genomes Project (Chapter 2). Evo 2 scaled to 9.3 trillion tokens spanning all domains of life, including complete bacterial chromosomes, eukaryotic genomes, viral sequences, and metagenomic assemblies.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 11.4: [High] Three-panel figure. Panel A (Training Corpus): Tree of life visualization with branch widths proportional to training data; Bacteria, Archaea, Eukaryotes, Viruses/Phages; total 9.3 trillion DNA tokens; contrast with human-only models (~3B tokens). Panel B (Architecture): StripedHyena 2 schematic with hybrid attention-convolution blocks; 1 Mb context; 7B and 40B variants. Panel C (Emergent Cross-Species Capabilities): Embedding space UMAP colored by taxonomic group; phylogenetic clustering emerging without explicit evolutionary modeling.\n\n\n\nThe composition of training data shapes what models learn. Reference-only training captures the genome’s architecture but not population variation. Multi-individual training exposes models to common polymorphisms but may underrepresent rare variants. Cross-species training provides evolutionary context (constrained regions are conserved, variable regions diverge) but may not capture species-specific regulatory patterns. Training on functional genomics data (GROVER-style approaches) teaches regulatory activity patterns but ties models to specific assays and cell types.\nA tension exists between generality and specificity. Models trained on broader corpora learn more general representations that transfer across species and contexts, but may underperform narrower models on specific applications. Models trained on focused datasets may capture task-relevant patterns more effectively but transfer less well. The optimal training strategy depends on intended applications.\n\n\n11.6.2 Probing What Models Learn\nLinear probing experiments reveal what information is encoded in model representations without task-specific fine-tuning. By training simple classifiers (logistic regression, single-layer perceptrons) on frozen embeddings to predict known annotations, researchers can assess whether models have learned biologically meaningful patterns. The methodology for such probing experiments is detailed in Section 9.3.3, with implications for interpretability examined in Section 24.4.\nDNA language models consistently learn to recognize several categories of genomic features. Models learn patterns corresponding to known transcription factor binding sites, splice signals, and other sequence motifs without explicit supervision; probing for specific motif presence shows that model embeddings can distinguish sequences containing binding sites from those lacking them. Representations also encode gene structure: models distinguish coding from noncoding regions, identify exon-intron boundaries, and recognize splice donor and acceptor sites. This knowledge emerges from sequence statistics alone, suggesting that the compositional and structural differences between genomic region types are learnable from DNA sequence.\nEvolutionary constraints are implicitly captured, particularly in models trained on multi-species data. Positions under purifying selection (constrained across evolution) show different embedding patterns than neutral positions. This provides a self-supervised analog to traditional conservation scoring, though the relationship between model-learned and alignment-based conservation measures varies across genomic contexts.\nMore complex patterns like regulatory grammar (the syntax governing how transcription factors combine to specify expression) show mixed evidence. Models capture some aspects of regulatory logic, such as the spacing preferences between binding sites, but may not fully represent the combinatorial complexity of enhancer function. Similarly, long-range dependencies (enhancer-promoter interactions across tens of kilobases) are accessible to long-context models but require extensive probing to assess whether they are actually leveraged.\n\n\n11.6.3 What Models Do Not Learn\nEqually important is recognizing what current DNA language models struggle to represent. Sequence-only models cannot capture epigenetic context: DNA methylation, histone modifications, and chromatin accessibility all affect gene regulation but are not encoded in primary sequence. Some models (like GROVER) address this by incorporating functional genomics data, but this ties them to specific cell types and experimental conditions.\nThe three-dimensional structure of chromatin affects which regulatory elements can physically interact, but linear sequence models cannot represent folding (Chapter 17). Cell-type specificity of gene regulation depends on transcription factor expression levels and chromatin state, not just sequence; models trained on sequence alone can predict potential regulatory activity but not its realization in specific contexts.\nComplex variant patterns beyond single nucleotide changes remain challenging. Indels, structural variants, repeat expansions, and epistatic interactions between distant loci are either not representable (depending on tokenization) or poorly predicted. Most benchmark tasks focus on SNVs, leaving multi-nucleotide effects underexplored.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 11.5: [High] Multi-panel figure. Panel A (Motif Recognition): First-layer attention patterns aligned with JASPAR motifs; model learns CTCF pattern from sequence statistics alone. Panel B (Gene Structure): t-SNE/UMAP of embeddings color-coded by region type (exon, intron, UTR, intergenic); clear clustering without region labels during training. Panel C (Evolutionary Constraint): Scatter plot of model uncertainty vs. phyloP conservation score; strong correlation. Panel D (What Models do not Learn): Icons with X marks for epigenetic state, 3D structure, cell-type specificity, complex variants.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-benchmarks",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-benchmarks",
    "title": "11  DNA Language Models",
    "section": "11.7 Benchmark Performance and Evaluation",
    "text": "11.7 Benchmark Performance and Evaluation\nStandardized benchmarks enable systematic comparison across DNA language models, though each benchmark captures only part of what we care about. Understanding benchmark construction and limitations is essential for interpreting performance claims.\n\n11.7.1 Major Benchmark Suites\nThe BEND (Benchmark for Nucleotide Deep learning) suite provides a unified framework with tasks including gene finding, enhancer annotation, chromatin state prediction, and variant effect scoring (Marin et al. 2024). Standardized splits and metrics enable fair comparison. BEND specifically evaluates whether models capture biologically meaningful features at different resolution scales.\nGenomic Benchmarks focus on regulatory element classification tasks: distinguishing promoters from nonpromoters, identifying active enhancers, predicting histone mark presence (gresova_genomic-benchmarks_2023?). These tasks test whether model representations encode basic genomic annotations. Most current DNA language models achieve high accuracy on these tasks, suggesting benchmark saturation for simpler classification problems.\nThe Long Range Benchmark (LRB) and DNALongBench evaluate long-context modeling capabilities (Cheng et al. 2024). Tasks include predicting distal enhancer-promoter interactions, modeling chromatin structure across hundreds of kilobases, and integrating information over extended genomic windows. These benchmarks specifically test whether long-context architectures provide meaningful advantages over shorter-context models.\nComparative evaluations across model families reveal that no single architecture dominates all tasks (Manzo, Borkowski, and Ovcharenko 2025). Performance varies substantially depending on task characteristics (local motif recognition versus long-range integration), training data composition, and architectural choices. HyenaDNA and Caduceus excel on long-range tasks where their architectural innovations matter; DNABERT-2 and Nucleotide Transformer perform well on shorter-range regulatory classification; Evo 2 shows advantages on cross-species tasks and variant effect prediction.\n\n\n11.7.2 Benchmark Limitations\nSeveral systematic issues affect benchmark interpretation. Many benchmarks have reached saturation, where multiple models achieve near-perfect performance and discriminative power disappears. This has happened for simpler classification tasks in Genomic Benchmarks. Data leakage arises when training and test sequences share homology, allowing models to succeed through memorization rather than generalization; the homology-aware splitting strategies required to prevent this are detailed in Section 21.2. Careful sequence clustering (using tools like MMseqs2 or CD-HIT) is required, but many older benchmarks lack rigorous split design. The comprehensive treatment of benchmark construction and evaluation methodology appears in Chapter 20 and Chapter 21.\nDistribution shift between benchmark data and real-world applications means strong benchmark performance may not predict deployment success. Most benchmarks derive from well-studied regions of well-characterized genomes; performance on understudied regions, rare variants, or non-European populations may differ substantially. The systematic treatment of such confounding factors appears in Chapter 22, with specific attention to ancestry-related performance disparities in Section 22.2.1.\nThe choice of evaluation metric affects what gets optimized. auROC favors discrimination regardless of calibration; Spearman correlation measures rank ordering but not absolute effect size prediction. Clinical applications may require well-calibrated probability estimates or accurate quantitative predictions, neither of which standard metrics directly assess (Chapter 23). The gap between benchmark performance and deployment utility remains substantial for most genomic applications.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 11.6: [Enhancing] Heatmap or grouped bar chart. Models (rows): DNABERT-2, Nucleotide Transformer (2.5B), HyenaDNA, Caduceus, Evo 2. Benchmark tasks (columns) grouped: Short-range (promoter detection, enhancer classification, TF binding); Long-range (enhancer-promoter interaction, chromatin state); Variant effect (SNV scoring, splice prediction). Cell values: Relative performance (color scale). Key observations annotated: “No single model dominates all tasks”; “Long-context models excel on long-range tasks.”",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-annotation-aware",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-annotation-aware",
    "title": "11  DNA Language Models",
    "section": "11.8 Annotation-Aware Extensions",
    "text": "11.8 Annotation-Aware Extensions\nRecent work explores enriching DNA language models with explicit biological structure beyond raw sequence. These approaches represent early steps toward multi-modal genomic foundation models.\nLife-Code proposes central-dogma-informed tokenization, treating coding and noncoding regions differently (Liu et al. 2025). Coding regions use codon tokens (three-nucleotide units specifying amino acids), respecting the genetic code’s fundamental structure. Noncoding regions use learned subword units optimized during training. Knowledge distillation from protein language models imports protein-level structural knowledge into DNA representations. Life-Code achieves competitive results across DNA, RNA, and protein tasks, suggesting that encoding biological structure into tokenization provides useful inductive bias (Chapter 5).\nBioToken extends tokenization to include explicit genomic annotations (Medvedev et al. 2025). Rather than representing regions purely as nucleotide strings, BioToken creates composite tokens encoding sequence content, variant presence, structural annotations (exon, intron, UTR), and functional context. The associated BioFM model achieves state-of-the-art performance across genomic benchmarks with substantially fewer parameters (265M), suggesting that annotation-aware representations improve parameter efficiency.\nThese approaches foreshadow the multi-modal foundation models discussed in Part IV (Chapter 19), where sequence is only one of many integrated information streams.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-practical-use",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-practical-use",
    "title": "11  DNA Language Models",
    "section": "11.9 Using DNA Language Models in Practice",
    "text": "11.9 Using DNA Language Models in Practice\nDNA language models support multiple usage patterns for different applications.\n\n11.9.1 Embeddings as Universal Features\nThe simplest approach extracts embeddings from a pretrained model and uses them as features for downstream classifiers. The workflow involves extracting embeddings for windows around loci of interest, pooling or selecting positions relevant to the task, and training lightweight downstream models (linear layers, shallow MLPs, gradient boosting) on the extracted features.\nThis approach supports diverse applications. Regulatory element classification distinguishes promoters, enhancers, silencers, and insulators based on learned representations. Chromatin state prediction uses sequence embeddings to predict ATAC-seq or histone mark presence. Variant effect scoring replaces or augments hand-crafted features in frameworks like CADD with language model features (analogous to CADD v1.7’s incorporation of protein language model features, as discussed in Chapter 4). The integration of these features into comprehensive variant effect prediction workflows is detailed in Section 14.4. Splicing analysis combines embeddings with specialized architectures.\nBecause the language model remains frozen, this approach is computationally efficient and avoids catastrophic forgetting when new tasks are added. The pretrained model serves as a general-purpose feature extractor supporting many downstream applications.\n\n\n11.9.2 Fine-Tuning and Adaptation\nWhen sufficient labeled data exists, fine-tuning typically outperforms frozen embedding approaches (Chapter 9). Updating all language model parameters for a specific task allows representations to specialize, achieving highest performance but requiring more compute and risking catastrophic forgetting of general knowledge.\nParameter-efficient methods like LoRA (Low-Rank Adaptation) offer a middle path, inserting small trainable modules into each layer while keeping the backbone mostly frozen (Hu et al. 2021). These approaches achieve most of the performance gains of full fine-tuning while maintaining computational efficiency and preserving general capabilities. Adapter-based methods similarly add small bottleneck modules tuned for specific tasks.\n\n\n11.9.3 Zero-Shot and Few-Shot Scoring\nFor variant interpretation, language models enable zero-shot scoring based on sequence likelihood. Compute the model’s probability for a sequence containing the reference allele, compare to probability for the sequence with the alternative allele, and interpret variants reducing probability as more disruptive. This approach requires no variant-specific training and can score any single-nucleotide variant the model can represent.\nZero-shot scoring quality depends on how well the model’s learned distribution captures biological constraints. Performance tends to improve with model scale and training data diversity (Chapter 10). Few-shot approaches include task examples in the input context, allowing in-context learning without parameter updates. HyenaDNA demonstrated this capability for genomic tasks, suggesting that sufficiently large models with long context can adapt through prompts rather than training.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-open-challenges",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-open-challenges",
    "title": "11  DNA Language Models",
    "section": "11.10 Limitations and Open Challenges",
    "text": "11.10 Limitations and Open Challenges\nDespite substantial progress, DNA language models face several fundamental limitations.\nThe tradeoff between context length and representational fidelity persists. Long-context models like HyenaDNA and Evo 2 can process megabase sequences but require efficient architectures that may not capture all the relationships dense attention would learn. Whether these architectural tradeoffs matter for specific applications remains task-dependent.\nMost tokenization schemes represent insertions and deletions awkwardly or not at all. Structural variants spanning kilobases, repeat expansions, and complex rearrangements fall outside what current models can process (Chapter 5). Epistatic interactions between variants at distant loci are not captured even by long-context models trained solely on single sequences.\nTraining data composition shapes model capabilities in underexplored ways. Models trained primarily on European-ancestry genomes may perform poorly on variants common in other populations (Chapter 22). Ascertainment bias in training databases (enrichment for coding regions, well-studied genes, specific diseases) propagates to learned representations. The field lacks systematic evaluation of performance disparities across populations.\nInterpretability remains limited (Chapter 24). While probing studies reveal what models encode, explaining why a specific variant receives a particular score in terms connecting to biological mechanism is difficult. Attention patterns and gradient-based attribution provide some insight but often fail to identify the specific sequence features driving predictions.\nIntegration with other modalities is nascent. DNA sequence provides necessary but insufficient information for predicting gene regulation. Epigenomic state, three-dimensional chromatin structure, transcription factor concentrations, and cellular context all matter. Current DNA language models cannot represent these factors; multi-modal approaches (discussed in Part IV) aim to address this limitation.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch11-dna-lm.html#sec-ch11-soft-landing",
    "href": "part_3/p3-ch11-dna-lm.html#sec-ch11-soft-landing",
    "title": "11  DNA Language Models",
    "section": "11.11 Representations Without Predictions",
    "text": "11.11 Representations Without Predictions\nDNA language models capture sequence patterns, regulatory motifs, and evolutionary constraints through self-supervised pretraining on genomic sequence. The progression from early proof-of-concept models through architectural innovations enabling megabase context demonstrates that the paradigm works: models trained to predict masked nucleotides learn representations that transfer across diverse downstream tasks. Biological inductive biases (strand symmetry, codon structure, cross-species training) can substitute for raw scale on appropriate tasks, creating opportunities for efficient models that encode domain knowledge.\nYet DNA language models have inherent limitations. They produce representations, not predictions. A language model can embed a sequence in a space where similar regulatory elements cluster together, but it cannot directly output the expression level that sequence will produce or the chromatin accessibility it will exhibit. The models capture what patterns exist in genomic sequence but not what those patterns do in cellular context. They cannot represent epigenomic state, three-dimensional chromatin organization, or cell-type-specific regulation without additional inputs beyond sequence.\nThese limitations define the complementary relationship between language models and sequence-to-function models. Where DNA language models learn representations from sequence statistics, regulatory models like Enformer and Borzoi predict molecular phenotypes from sequence context (Chapter 13). The regulatory models provide quantitative outputs (expression levels, chromatin tracks, splice probabilities) that language models alone cannot produce. For variant effect prediction (Chapter 14), both representation quality and phenotypic prediction matter: language model embeddings capture evolutionary constraint while regulatory models predict functional consequences. Understanding what each model family provides is prerequisite to combining them effectively.\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “[GPN] DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nCheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu Yang, Lei Li, and Jian Ma. 2024. “DNALONGBENCH: A Benchmark Suite For Long-Range DNA Prediction Tasks,” October. https://openreview.net/forum?id=opv67PpqLS.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2106.09685.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nManzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025. “Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory Variants.” bioRxiv: The Preprint Server for Biology, June, 2025.05.19.654920. https://doi.org/10.1101/2025.05.19.654920.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. “BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks.” arXiv. https://doi.org/10.48550/arXiv.2311.12570.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence Modeling and Design from Molecular to Genome Scale with Evo.” Science 386 (6723): eado9336. https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html",
    "href": "part_3/p3-ch12-protein-lm.html",
    "title": "12  Protein Language Models",
    "section": "",
    "text": "12.1 ESM Model Family\nEvolution is the most thorough experiment ever conducted on protein sequences. Over billions of years, natural selection tested trillions of amino acid combinations, ruthlessly eliminating sequences that failed to fold or function while preserving those that worked. The sequences populating modern databases are not random strings but successful solutions to biological problems, each implicitly encoding information about structure, stability, and function. The central insight of protein language models is that this evolutionary record, comprising hundreds of millions of sequences in databases like UniRef, contains sufficient information to learn the fundamental principles of protein biology without ever being shown a crystal structure or a functional assay.\nThis insight transformed computational biology. Traditional approaches to understanding proteins required either expensive experimental characterization or physics-based simulations that struggled with the complexity of protein behavior. Multiple sequence alignments could extract conservation patterns, but required finding homologs for each protein of interest and could not generalize beyond specific families. Protein language models changed the equation by compressing evolutionary knowledge into neural network parameters that transfer across the entire protein universe. A model trained to predict masked amino acids learns, as a byproduct, which residues contact each other in three-dimensional space, which positions tolerate variation, and which substitutions disrupt function. The physics of protein folding, selected across evolutionary time, emerges from the statistics of surviving sequences.\nThe ESM family demonstrated that transformers can learn protein structure and function from sequence alone, achieving results that rival methods requiring explicit structural supervision. Evolutionary Scale Modeling, as the name suggests, exploits the scale of evolutionary data to learn representations that generalize across proteins regardless of homology or family membership. Understanding these successes and their limitations provides essential context for genomic language models, where analogous approaches face distinct challenges arising from the multi-scale organization of regulatory information in DNA (see Chapter 11).\nThe ESM (Evolutionary Scale Modeling) family developed at Meta AI Research represents the most influential protein language model lineage, progressing from an initial proof-of-concept to models capable of predicting three-dimensional structure from sequence alone. The progression from ESM-1b through ESM-2 illustrates how scaling transformer architectures yields systematic improvements in biological knowledge extraction, while revealing what self-supervised learning on protein sequences can and cannot achieve.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-esm-family",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-esm-family",
    "title": "12  Protein Language Models",
    "section": "",
    "text": "12.1.1 ESM-1b: Establishing the Paradigm\nThe Evolutionary Scale Modeling project demonstrated that transformer language models trained on protein sequences learn biologically meaningful representations without explicit supervision (Rives et al. 2021). The approach was strikingly simple: take the BERT architecture that had revolutionized natural language processing, replace words with amino acids, and train on protein sequence databases. The resulting models learned far more than anyone expected.\nESM-1b was trained on UniRef50, a clustered database of approximately 33 million protein sequences covering the known diversity of protein families. The construction and characteristics of protein sequence databases are detailed in Section 2.6, with implications for training data curation in Chapter 2. UniRef50 clusters sequences at 50% identity, providing broad coverage while reducing redundancy that would otherwise bias the model toward overrepresented families (Suzek et al. 2007). This curation strategy ensures the model encounters diverse evolutionary solutions to protein function rather than memorizing common motifs.\nThe architecture follows the BERT-style bidirectional transformer design with 650 million parameters distributed across 33 layers, a hidden dimension of 1,280, and 20 attention heads. The maximum sequence length of 1,024 amino acids accommodates most individual protein domains and many complete proteins. The training objective is masked language modeling, the self-supervised strategy introduced in Section 8.1: randomly mask 15% of amino acids in each sequence, and train the model to predict the masked positions given surrounding context. This objective contains no information about structure, function, or evolution beyond what is implicit in the sequences themselves. This objective contains no information about structure, function, or evolution beyond what is implicit in the sequences themselves.\n\n\n12.1.2 Emergent Biological Knowledge\nThe surprise was not that ESM-1b learned to predict masked amino acids accurately, but what else it learned in the process. Despite never seeing structural or functional labels during training, ESM-1b’s internal representations encode information about protein biology at multiple levels of organization.\nSecondary structure emerges in the attention patterns. When researchers analyzed which sequence positions the model attends to when making predictions, they found that attention concentrates along patterns corresponding to alpha helices and beta sheets. The model implicitly learns that certain amino acid sequences form specific structural elements, encoding this knowledge without ever being told what secondary structure is.\nMore remarkably, ESM-1b captures residue-residue contacts. Amino acids that are distant in the linear sequence but close in three-dimensional space attend to each other in the model’s attention matrices. This emergent capability suggests the model learns aspects of protein folding purely from sequence statistics. When attention weights were converted to contact predictions, they achieved accuracy approaching dedicated contact prediction methods that were explicitly trained for that task.\nThe model’s masked token predictions correlate strongly with position-specific conservation scores derived from multiple sequence alignments. ESM effectively learns which positions tolerate variation and which are evolutionarily constrained, extracting this information from the statistical patterns across 33 million sequences rather than from explicit conservation annotations. Positions where the model confidently predicts specific amino acids correspond to positions that are conserved across protein families.\nPerhaps most striking, attention concentrates on functionally important positions. Catalytic residues, binding sites, and other sites of biological importance receive elevated attention even without explicit functional annotation in the training data. The model identifies that certain sequence positions are more informative about surrounding context, and these positions frequently correspond to sites where nature has constrained variation because they perform essential functions.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 12.1: [Essential] Multi-panel figure. Panel A (Training Objective): Protein sequence with 15% masked; model predicting from context; “No structure, function, or evolutionary labels.” Panel B (Secondary Structure in Attention): Attention heatmap with alpha helix and beta sheet regions highlighted; attention concentrating along structural patterns. Panel C (Residue Contacts from Attention): Attention weights converted to contact map; ground truth from crystal structure overlaid; strong correspondence. Panel D (Functional Site Discovery): Protein structure cartoon; positions with elevated attention highlighted; overlap with catalytic residues, binding sites.\n\n\n\n\n\n12.1.3 ESM-2: Scaling Up\nESM-2 extended the ESM approach across a range of model scales, from 8 million to 15 billion parameters, enabling systematic study of how biological knowledge scales with model capacity (Lin et al. 2022). The results confirmed a pattern familiar from natural language processing: bigger models learn more.\n\nESM-2 model family spanning four orders of magnitude in parameter count, with architecture details and relative performance on structure-related tasks.\n\n\nModel\nParameters\nLayers\nHidden Dim\nPerformance Gain\n\n\n\n\nESM-2 (8M)\n8M\n6\n320\nBaseline\n\n\nESM-2 (35M)\n35M\n12\n480\nModest\n\n\nESM-2 (150M)\n150M\n30\n640\nSubstantial\n\n\nESM-2 (650M)\n650M\n33\n1280\nLarge\n\n\nESM-2 (3B)\n3B\n36\n2560\nNear-optimal\n\n\nESM-2 (15B)\n15B\n48\n5120\nState-of-the-art\n\n\n\nPerformance scales smoothly with model size across structure prediction, contact prediction, and variant effect tasks. The scaling relationship is not linear: doubling parameters does not double accuracy. But gains remain consistent through even the largest models, suggesting that the 15-billion parameter ceiling reflects computational constraints rather than fundamental limits on what sequence statistics can teach.\nThe scaling behavior mirrors observations in natural language processing, where larger models consistently capture more nuanced patterns. This predictable relationship between scale and capability provides a roadmap for model development: if more biological knowledge is needed, train a larger model on more data. The practical implications shaped how the field approached subsequent genomic foundation models, with the scaling law framework and its implications discussed in Section 10.3.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 12.2: [Essential] Scaling analysis. Panel A (Performance vs. Parameters): Log-log plot; x-axis parameters (8M → 15B); y-axis performance on structure-related tasks; multiple curves; no sign of saturation. Panel B (Model Family Table): ESM-2 variants stacked by size; visual encoding of layers, hidden dimension, performance; annotate where capabilities emerge (~150M useful embeddings, ~650M structural understanding, ~3B near-optimal single-sequence structure, ~15B approaches MSA methods). Panel C (Capability Thresholds): Specific capabilities as step functions; contact prediction gradual; zero-shot structure emergent at ~650M.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-alternative-architectures",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-alternative-architectures",
    "title": "12  Protein Language Models",
    "section": "12.2 Alternative Architectures",
    "text": "12.2 Alternative Architectures\nThe success of ESM raised a natural question: how much depends on the specific BERT architecture versus the general approach of self-supervised learning on protein sequences? The ProtTrans family explored this question by applying multiple transformer architectures to protein modeling (Elnaggar et al. 2021).\nProtBERT applies the bidirectional encoder to protein sequences, trained on the Big Fantastic Database (BFD) comprising approximately 2.1 billion protein sequences. This training corpus, substantially larger than UniRef50, provides broader coverage at the cost of including more redundant and potentially lower-quality sequences. The architectural choices match ESM closely, enabling direct comparison of training data effects.\nProtT5 adapts the encoder-decoder architecture from T5, enabling both understanding and generation tasks (Raffel et al. 2023). The encoder processes input sequences to produce contextual representations, while the decoder can generate output sequences conditioned on those representations. This architecture proved valuable for tasks requiring sequence generation, such as structure-conditioned design or sequence completion, though the encoder-only architecture remains dominant for embedding and classification tasks.\nProtXLNet explores permutation language modeling, capturing bidirectional context without the artificial [MASK] token that BERT-style models require during training (Yang et al. 2020). By training on all possible token orderings, XLNet-style models learn to predict each token from any subset of context tokens, potentially capturing richer dependencies at the cost of more complex training.\nThese architectural variants demonstrate that the protein language modeling paradigm generalizes beyond specific design choices. All architectures learn meaningful representations when trained on sufficient data, though performance differences emerge for specific downstream tasks. Encoder-only models excel at classification and embedding tasks where the entire sequence is available. Encoder-decoder models enable generation tasks where outputs must be produced token by token.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-attention-coupling",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-attention-coupling",
    "title": "12  Protein Language Models",
    "section": "12.3 Attention and Evolutionary Coupling",
    "text": "12.3 Attention and Evolutionary Coupling\nThe emergence of contact information in ESM’s attention patterns connects to a deeper principle: evolutionary coupling. When two residues must maintain physical contact for a protein to function, mutations at one position create selective pressure for compensatory mutations at the other. Over evolutionary time, these correlated mutations leave statistical signatures in protein families that can be detected through covariance analysis of multiple sequence alignments.\nDirect Coupling Analysis (DCA) and related methods extract these coevolutionary signals to predict residue-residue contacts (Morcos et al. 2011). The approach requires constructing multiple sequence alignments, computing covariance matrices, and applying statistical corrections to distinguish direct from indirect correlations. The resulting contact predictions enabled the first accurate structure predictions for proteins lacking homologs in structural databases.\nProtein language models learn to extract similar information through a different route. Rather than computing covariance explicitly, transformers learn attention patterns that capture which positions inform predictions at other positions. When position i strongly attends to position j during masked prediction, the model has learned that knowing the amino acid at j helps predict the amino acid at i. This is precisely the signature of evolutionary coupling: positions that covary because they must maintain physical contact.\nThe attention-based approach offers several advantages over traditional covariance analysis. Language models generalize across protein families, learning shared principles that transfer to proteins with sparse evolutionary sampling. They handle the statistical challenge of distinguishing direct from indirect correlations implicitly through deep architecture rather than requiring explicit correction. And they provide rich representations beyond binary contact predictions, encoding information about the strength and nature of residue relationships.\nRao and colleagues demonstrated this connection directly by extracting attention weights from ESM and converting them to contact predictions (Rao et al. 2020). The resulting predictions approached the accuracy of dedicated contact prediction methods, despite the model never being trained to predict contacts. The attention mechanism, optimized purely for masked token prediction, discovers the coevolutionary structure of protein sequences as a byproduct.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-esmfold",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-esmfold",
    "title": "12  Protein Language Models",
    "section": "12.4 ESMFold: Structure from Sequence",
    "text": "12.4 ESMFold: Structure from Sequence\nStructure prediction has traditionally required multiple sequence alignments (MSAs) that search protein databases for evolutionary relatives, a process that can take hours per protein and fails entirely for sequences lacking detectable homologs. ESMFold demonstrated that the representations learned by ESM-2 contain sufficient evolutionary information to predict three-dimensional structure directly, eliminating the alignment requirement while maintaining competitive accuracy.\n\n12.4.1 Alignment-Free Prediction\nThe most dramatic demonstration of protein language model capabilities came with ESMFold, which predicts protein 3D structure directly from ESM-2 embeddings without requiring multiple sequence alignments (Lin et al. 2022). Traditional structure prediction, including AlphaFold2, relies heavily on MSAs constructed through computationally expensive searches against sequence databases. These searches can take hours per protein, and prediction quality depends critically on finding informative homologs.\nESMFold eliminates this requirement entirely. The architecture couples ESM-2 (using the 15-billion parameter variant) with a structure module adapted from AlphaFold2’s Evoformer and structure module. The language model embeddings replace MSA-derived features, providing the evolutionary context that the structure module needs to predict atomic coordinates. The model takes a single sequence as input and outputs predicted 3D coordinates for all atoms.\nThe computational speedup is substantial: approximately 60-fold faster than AlphaFold2 for typical proteins. This speed advantage makes it feasible to predict structures for the millions of protein sequences emerging from environmental sequencing projects, where computing MSAs would be prohibitively expensive. Metagenomic proteins, often lacking close homologs in existing databases, represent exactly the cases where MSA-based methods struggle and where single-sequence predictions become essential.\nESMFold achieves atomic-level accuracy for many proteins, though slightly below AlphaFold2 for proteins that benefit strongly from MSA information. The accuracy gap is largest for proteins with sparse evolutionary sampling, where explicit alignments provide information that single-sequence analysis cannot fully recover. For well-represented protein families, ESMFold approaches AlphaFold2 accuracy at a fraction of the computational cost.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 12.3: [High] Four-panel figure. Panel A (Architecture Pipeline): Single sequence → ESM-2 (15B) embeddings → Structure module → 3D coordinates; “No MSA required.” Panel B (Speed Comparison): Bar chart of AlphaFold2 (hours) vs ESMFold (minutes); 60× speedup. Panel C (Accuracy Comparison): Scatter plot ESMFold vs AlphaFold2 colored by MSA depth; well-represented proteins both accurate; sparse MSA proteins ESMFold more robust. Panel D (Metagenomic Application): Earth Microbiome Project proteins; many lack homologs; ESMFold enables scale.\n\n\n\n\n\n12.4.2 What ESMFold Reveals About PLMs\nESMFold’s success demonstrates that ESM-2’s internal representations encode sufficient information to determine 3D structure. The language model has learned not merely local sequence patterns but global folding principles, capturing what makes a sequence fold into a particular three-dimensional shape.\nThis has profound implications for understanding what protein language models learn. The attention patterns that emerge from masked prediction are, in some sense, learning the physics of protein folding. Residues that need to be close in 3D space to maintain stability attend to each other in the transformer’s attention matrices. The statistical patterns in protein sequences, shaped by billions of years of evolution under physical constraints, encode structural information that sufficiently powerful language models can decode.\nThe fundamental insight is that evolution has already solved the structure prediction problem, millions of times over, and recorded the solutions in sequence databases. Language models learn to read those solutions, extracting the implicit structural knowledge that selection has embedded in surviving sequences.\n\n\n\n\n\n\nNoteAlphaFold: Structure Prediction Without Foundation Models\n\n\n\nAlphaFold2’s performance at CASP14 in 2020 solved a 50-year grand challenge, predicting protein structures with accuracy competitive with experimental determination (Jumper et al. 2021). The achievement transformed structural biology and earned its creators the 2024 Nobel Prize in Chemistry. Yet AlphaFold is not a foundation model in the sense this book uses the term (see Chapter 10). Understanding why illuminates what makes PLM-based approaches distinctive.\nAlphaFold requires multiple sequence alignments as input. The Evoformer architecture processes MSA features alongside the query sequence, using attention mechanisms that operate over both the sequence dimension and the alignment dimension. Evolutionary information enters the model explicitly through database search rather than being learned implicitly from sequence data. This design choice has computational consequences: MSA construction can take hours per protein, and prediction quality depends critically on finding informative homologs. For orphan proteins lacking close relatives in sequence databases, AlphaFold’s accuracy degrades substantially.\nThe architectural innovations that enabled AlphaFold’s success differ fundamentally from the foundation model paradigm. Evoformer’s attention over MSA rows and columns, iterative recycling through the network, and the structure module’s SE(3)-equivariant operations represent expert-designed inductive biases encoding protein physics. These components were engineered specifically for structure prediction, not learned from self-supervised objectives on broad sequence data. The model excels at its designed task but does not produce general-purpose representations transferable to other problems.\nESMFold inverts this design philosophy. Rather than requiring explicit evolutionary input, ESMFold couples ESM-2 embeddings with a structure module adapted from AlphaFold’s architecture. The language model provides the evolutionary context that the structure module needs, context learned implicitly through masked token prediction on millions of protein sequences. A single sequence goes in; predicted coordinates come out. No MSA construction, no database search, no hours of preprocessing.\nThe comparison reveals what protein language models have and have not learned. ESMFold approaches AlphaFold accuracy for well-represented protein families where the language model’s training data provided dense evolutionary sampling. The gap widens for proteins where deep MSAs provide information that single-sequence analysis cannot fully recover. ESMFold runs approximately 60-fold faster than AlphaFold, enabling structure prediction at metagenomic scale for the millions of protein sequences emerging from environmental sequencing projects. The two approaches exhibit different failure modes: AlphaFold struggles with orphan proteins that lack homologs; ESMFold struggles with sequences the language model finds surprising (high perplexity), even when homologs exist.\nAlphaFold3 complicates this dichotomy (Abramson et al. 2024). The updated architecture uses diffusion-based structure generation and handles protein-ligand, protein-nucleic acid, and multi-chain complexes within a unified framework. MSA dependency is reduced in some contexts, and the model moves toward general biomolecular structure prediction rather than single-chain protein folding. Whether this represents convergence between task-specific and foundation model approaches remains an open question.\nAlphaFold demonstrated that protein structure prediction was computationally tractable; ESMFold demonstrated that foundation models had learned enough biology to solve it differently. Both insights matter. For this book’s purposes, ESMFold illustrates the foundation model paradigm: self-supervised pretraining produces representations that transfer to downstream tasks, including tasks (like structure prediction) that were not part of the training objective. AlphaFold’s success through architectural engineering rather than learned representations represents an alternative path, one that achieved the goal first but may prove less generalizable as the field matures. The AlphaMissense model discussed in Chapter 14 repurposes AlphaFold’s structure module for variant effect prediction, suggesting that even task-specific architectures can seed broader applications when their components prove useful beyond their original context.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-function-prediction",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-function-prediction",
    "title": "12  Protein Language Models",
    "section": "12.5 Function Prediction",
    "text": "12.5 Function Prediction\nBeyond structure, protein language models enable prediction of protein function directly from sequence. Function prediction encompasses multiple tasks: predicting Gene Ontology terms that describe molecular function, biological process, and cellular component; classifying enzyme activity; identifying binding sites and interaction partners; and predicting subcellular localization.\nTraditional function prediction relied on homology: proteins similar in sequence are assumed to share function. This approach fails for orphan proteins lacking characterized homologs and cannot distinguish functional differences between closely related sequences. PLM-based approaches address both limitations by learning representations that capture functional signatures beyond simple sequence similarity.\nFor Gene Ontology term prediction, PLM embeddings serve as input features to classification models that predict which GO terms apply to each protein. The embeddings capture evolutionary and structural information relevant to function, enabling accurate predictions even for proteins with limited homology to characterized sequences. Performance improves with embedding quality, suggesting that larger language models capture more functionally relevant information. These embeddings can also serve as node features in biological network analyses (Section 18.3), and the function predictions inform drug target identification workflows (Section 27.1.1). [Citation Needed]\nEnzyme classification benefits similarly from PLM representations. The Enzyme Commission hierarchy categorizes enzymes by the reactions they catalyze, from broad classes (oxidoreductases, transferases) to specific substrate preferences. PLM embeddings distinguish these categories effectively, capturing the sequence features that determine catalytic activity without requiring explicit structural analysis. [Citation Needed]\nBinding site prediction applies attention analysis to identify which residues participate in ligand binding, protein-protein interactions, or nucleic acid recognition. Positions that the model identifies as important for contextual prediction often correspond to functionally important sites, including binding pockets and catalytic residues. This capability enables rapid identification of functional sites in newly sequenced proteins. [Citation Needed]",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-variant-effects",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-variant-effects",
    "title": "12  Protein Language Models",
    "section": "12.6 Variant Effect Prediction",
    "text": "12.6 Variant Effect Prediction\nA critical clinical application of protein language models is predicting the effects of amino acid substitutions. Missense variants are the most common type of protein-coding mutation, and clinical genetics pipelines must routinely assess whether specific substitutions are likely pathogenic or benign. The traditional approach required either direct experimental characterization or computational methods trained on labeled pathogenicity data, both of which scale poorly to the millions of possible variants in each human genome (see Chapter 4 for discussion of classical approaches).\nESM-1v demonstrated that PLMs can predict variant effects without any training on variant labels (Meier et al. 2021). The approach exploits the masked language modeling objective directly: for a variant at position \\(i\\) changing amino acid \\(a\\) to amino acid \\(b\\), compute the log-likelihood ratio:\n\\[\n\\Delta \\text{score} = \\log P(b \\mid \\text{context}) - \\log P(a \\mid \\text{context})\n\\]\nIf the model assigns higher probability to the mutant amino acid than the wild-type, the variant is predicted benign; if lower, deleterious. This zero-shot prediction requires no labeled training data. The model’s evolutionary knowledge, learned from sequence databases, directly informs variant interpretation.\nThe intuition is straightforward. Evolution has shaped protein sequences such that certain positions strongly prefer certain amino acids. Substitutions that violate these preferences are more likely to disrupt function. The language model captures these preferences through training on millions of evolutionarily successful sequences. Variants that the model finds surprising are more likely to be functionally disruptive.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 12.4: [High] Four-panel figure. Panel A (The Scoring Mechanism): Protein sequence with variant position; \\(P(\\mathrm{ref}\\mid \\mathrm{context})\\) vs \\(P(\\mathrm{var}\\mid \\mathrm{context})\\); Score \\(= \\log P(\\mathrm{var}) - \\log P(\\mathrm{ref})\\). Panel B (Intuition): Evolution tested billions of substitutions; low probability variants \\(=\\) evolutionarily disfavored \\(=\\) likely disruptive. Panel C (Benchmark Performance): ROC curves ESM-1v vs classical methods on deep mutational scanning data; competitive without variant labels. Panel D (AlphaMissense Enhancement): ESM embeddings + AlphaFold2 structural features; combined model; \\(71\\text{M}\\) precomputed scores; performance boost from structure.\n\n\n\nBrandes and colleagues applied ESM-1b to predict effects for all approximately 450 million possible missense variants in the human genome, providing a precomputed resource for clinical variant interpretation (Brandes et al. 2023). On ClinVar benchmarks, ESM-1b outperformed existing methods in classifying variants as pathogenic or benign.\nAlphaMissense extended this approach by combining PLM representations with structural context from predicted protein structures (Cheng et al. 2023). The integration of sequence-based and structure-based signals improves accuracy, particularly for variants affecting protein stability or buried residues. AlphaMissense provides predictions for all approximately 71 million possible single amino acid substitutions in the human proteome.\nThe detailed comparison of variant effect prediction methods, including how PLM-based scores integrate with clinical classification frameworks, is covered in Section 14.2.3. The calibration of these scores to ACMG criteria appears in Section 14.5.3, and integration into rare disease diagnostic workflows in Section 26.1.4. Here, the key point is that protein language models provide the foundational representations that make accurate zero-shot variant prediction possible.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-structure-integration",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-structure-integration",
    "title": "12  Protein Language Models",
    "section": "12.7 Integration with Structure Prediction",
    "text": "12.7 Integration with Structure Prediction\nProtein language models exist within a broader ecosystem of computational methods for protein analysis. Understanding how PLMs relate to structure prediction systems clarifies their role and capabilities.\nAlphaFold2 achieved breakthrough accuracy in structure prediction by combining learned representations with explicit geometric modeling (Jumper et al. 2021). The architecture processes both sequence information through embeddings and evolutionary information through multiple sequence alignments, using an attention-based module (Evoformer) to integrate these signals before predicting atomic coordinates. AlphaFold2’s success depended critically on MSA quality: proteins with many homologs could be predicted accurately, while orphan proteins remained challenging.\nESMFold demonstrated that PLM embeddings can replace MSA-derived features, achieving competitive accuracy without the alignment bottleneck. This finding clarified the relationship between language models and structure prediction: PLMs learn to compress evolutionary information into representations that are functionally equivalent to explicit alignments, at least for proteins with sufficient representation in training databases.\nAlphaFold3 extended structure prediction to protein complexes, nucleic acids, and small molecules (Abramson et al. 2024). The architecture incorporates diffusion-based generation, enabling prediction of binding poses and complex assemblies. These capabilities complement PLM-based function prediction by providing structural context for interpreting functional predictions.\nGenerative protein design methods including RFDiffusion and ProteinMPNN leverage both structural and sequence information (Watson et al. 2023; Dauparas et al. 2022). RFDiffusion generates novel protein backbones through diffusion processes conditioned on design objectives. ProteinMPNN designs sequences likely to fold into specified structures. Both methods benefit from PLM representations when designing sequences with desired functional properties, demonstrating how language models integrate into the broader protein engineering pipeline (see Chapter 28 for detailed treatment of sequence design methods).\nThe trajectory from ESM to ESMFold to integration with design tools illustrates how PLMs serve as a foundation for diverse downstream applications. The representations learned through self-supervised training transfer across tasks, providing a common language for structure prediction, function annotation, variant interpretation, and protein engineering. This pattern of foundation models enabling diverse applications recurs throughout genomic AI, as discussed in Chapter 10.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-limitations",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-limitations",
    "title": "12  Protein Language Models",
    "section": "12.8 Limitations",
    "text": "12.8 Limitations\nDespite their success, protein language models face several limitations that inform the development of genomic models and guide appropriate application.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 12.5: [Enhancing] Grid of limitation categories with visual examples. Orphan Proteins: Phylogenetic tree with isolated lineage; no homologs = no evolutionary context; performance degradation curve. Novel Folds: Designed protein with non-natural topology; predictions unreliable outside training. Conformational Flexibility: Protein with multiple conformations; PLM produces single embedding. Epistasis: Two distant positions; individual mutations benign; combination deleterious; models assume independence. Interpretability: Attention correlates with biology but mechanism remains opaque.\n\n\n\n\n12.8.1 Orphan and Dark Proteins\nPLMs learn from evolutionary statistics, performing best for proteins with rich representation in training databases. Orphan proteins, those unique to specific lineages without detectable homologs, lack the evolutionary context that PLMs exploit. For these proteins, the model has no basis for distinguishing likely from unlikely amino acids at each position, and predictions degrade accordingly.\nThe problem extends to “dark” proteins that are poorly characterized despite having homologs. If an entire protein family has escaped experimental characterization, PLMs may learn statistical patterns without capturing functional relevance. The model cannot distinguish constraint imposed by function from constraint imposed by historical accident.\n\n\n12.8.2 Novel Folds\nTraining data shapes what models can predict. PLMs trained on natural protein databases learn the statistical patterns of naturally occurring folds, potentially struggling with designed proteins or hypothetical folds outside the training distribution. When researchers design proteins with novel topologies not found in nature, PLM predictions become less reliable because the relevant sequence patterns were never encountered during training. [Citation Needed]\n\n\n12.8.3 Conformational Flexibility\nMost PLM representations assume a single static structure, but many proteins adopt multiple conformations relevant to function. Allosteric proteins, intrinsically disordered regions, and proteins that undergo conformational changes upon binding present challenges for methods that embed each sequence into a single representation. The language model learns the average properties of sequences but may not capture the dynamic range that determines biological behavior.\n\n\n12.8.4 Epistasis\nMost variant effect predictions assume independence: the effect of mutation A does not depend on whether mutation B is present. Real proteins exhibit epistasis, where variant effects depend on sequence context. Two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. Current PLM-based predictors model marginal effects at each position but do not explicitly capture higher-order interactions, though the contextual embeddings may represent some epistatic relationships implicitly.\n\n\n12.8.5 Interpretability\nWhile attention patterns correlate with biological features, understanding exactly what PLMs learn remains challenging. The field is developing interpretation methods, including attention pattern analysis (Section 24.5) and probing studies (Section 24.4), but PLMs remain partially opaque. The distinction between plausible and faithful explanations, critical for clinical applications, is examined in Chapter 24. For clinical applications where explanations matter, this interpretability gap limits adoption. A prediction that a variant is pathogenic is more useful when accompanied by mechanistic insight into why the variant disrupts function.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-lessons",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-lessons",
    "title": "12  Protein Language Models",
    "section": "12.9 Lessons for Genomic Foundation Models",
    "text": "12.9 Lessons for Genomic Foundation Models\nThe success of protein language models established principles that guided subsequent development of genomic foundation models. These lessons transfer with appropriate modifications to DNA and RNA modeling.\n\n12.9.1 Self-Supervised Biological Knowledge\nPLMs demonstrated that massive amounts of biological knowledge can be learned from unlabeled sequences. The same evolutionary pressures that shape proteins also shape DNA. Purifying selection removes deleterious variants, leaving statistical signatures in sequence databases that self-supervised models can exploit. This principle underlies the entire foundation model paradigm: sufficiently large models trained on sufficiently large datasets with appropriate objectives will learn representations that capture biological function.\n\n\n12.9.2 Scaling Benefits\nPerformance improves predictably with model size through the range currently explored. The progression from 8 million to 15 billion parameters in ESM-2 showed consistent gains across structure prediction, contact prediction, and variant effect tasks. While scaling cannot continue indefinitely, current models remain in a regime where additional capacity yields reliable improvements. This relationship justified the computational investment in large genomic foundation models (see Chapter 10 for discussion of scaling laws in genomic contexts).\n\n\n12.9.3 Effective Transfer Learning\nRepresentations learned for one task (masked token prediction) transfer to other tasks (structure prediction, variant effects, function annotation). This suggests that self-supervised pretraining captures fundamental biological knowledge rather than task-specific shortcuts. A model trained to predict masked amino acids simultaneously learns about protein structure, function, evolutionary constraint, and disease relevance. The same principle motivates genomic language models: models trained to predict masked nucleotides may simultaneously learn about regulatory elements, evolutionary conservation, and variant effects. Transfer learning strategies, including fine-tuning approaches and parameter-efficient adaptation, are discussed in detail in Chapter 9, with specific guidance on choosing between these strategies in Section 9.7.\n\n\n12.9.4 Architecture-Sequence Matching\nThe BERT-style bidirectional encoder proved effective for proteins, where entire sequences are typically available and lengths rarely exceed a thousand residues. Genomic sequences present different challenges: much longer lengths spanning kilobases to megabases, different information density with coding regions being dense while intergenic regions are sparser, and structural features including reverse-complement relationships absent in proteins. These differences motivate architectural adaptations in genomic language models, as explored in Chapter 11.\n\n\n12.9.5 Integration Benefits\nAlphaMissense demonstrated that PLM embeddings combine effectively with structural and population genetics information, achieving accuracy beyond what any single information source provides. The most powerful methods integrate multiple signals, using PLMs as one component of larger systems. This principle extends to genomic foundation models, where sequence-based representations complement rather than replace functional annotations, chromatin data, and clinical information (see Chapter 14 for variant effect prediction integration strategies).",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch12-protein-lm.html#sec-ch12-conclusion",
    "href": "part_3/p3-ch12-protein-lm.html#sec-ch12-conclusion",
    "title": "12  Protein Language Models",
    "section": "12.10 Paradigm That Generalized",
    "text": "12.10 Paradigm That Generalized\nProtein language models established that transformer architectures can learn deep biological knowledge from sequence alone. ESM’s ability to predict structure, function, and variant effects without explicit labels demonstrated the power of self-supervised learning on evolutionary data. The framework validated a paradigm: treat biological sequences as language, train large models to predict masked tokens, and extract functional knowledge from learned representations. Attention patterns in these models capture evolutionary constraint, contact prediction, and structural relationships without requiring multiple sequence alignments or explicit structural supervision.\nThis success directly motivated genomic language models. If proteins constitute a language that transformers can learn, perhaps DNA does too. The DNA language models examined in Chapter 11 adapt protein language model architectures and training strategies to the distinct challenges of genomic sequences: longer contexts, different alphabets, ambiguous tokenization, and the full complexity of gene regulation beyond protein coding. RNA language models occupy an intermediate position, sharing features with both protein and DNA modeling while addressing the unique challenges of RNA structure and processing.\nThe integration path extends beyond sequence modeling. Just as protein language model representations feed into structure prediction (ESMFold) and variant effect prediction (AlphaMissense), genomic language model embeddings integrate into regulatory models (Chapter 13) and clinical applications (Chapter 26, Chapter 25). Protein design methods (Chapter 28) demonstrate how generative modeling builds on the representations that language models provide. Throughout this progression, the principle that ESM established remains: self-supervised learning on biological sequences captures knowledge that transfers across diverse applications.\n\n\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. “[AlphaFold3] Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. “Robust Deep Learning–Based Protein Sequence Design Using ProteinMPNN.” Science 378 (6615): 49–56. https://doi.org/10.1126/science.add2187.\n\n\nElnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, et al. 2021. “ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Deep Learning and High Performance Computing.” arXiv. https://doi.org/10.48550/arXiv.2007.06225.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “[AlphaFold2] Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nMorcos, Faruck, Andrea Pagnani, Bryan Lunt, Arianna Bertolino, Debora S. Marks, Chris Sander, Riccardo Zecchina, José N. Onuchic, Terence Hwa, and Martin Weigt. 2011. “Direct-Coupling Analysis of Residue Coevolution Captures Native Contacts Across Many Protein Families.” Proceedings of the National Academy of Sciences 108 (49): E1293–1301. https://doi.org/10.1073/pnas.1111471108.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” arXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nRao, Roshan, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. 2020. “Transformer Protein Language Models Are Unsupervised Structure Learners.” bioRxiv. https://doi.org/10.1101/2020.12.15.422761.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. “UniRef: Comprehensive and Non-Redundant UniProt Reference Clusters.” Bioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. “De Novo Design of Protein Structure and Function with RFdiffusion.” Nature 620 (7976): 1089–1100. https://doi.org/10.1038/s41586-023-06415-8.\n\n\nYang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. “XLNet: Generalized Autoregressive Pretraining for Language Understanding.” arXiv. https://doi.org/10.48550/arXiv.1906.08237.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html",
    "href": "part_3/p3-ch13-regulatory.html",
    "title": "13  Regulatory Models",
    "section": "",
    "text": "13.1 Long-Range Regulation Problem\nAn enhancer 80 kilobases from a promoter can determine whether a gene is expressed in liver or brain. An insulator 50 kilobases downstream can block inappropriate activation from a neighboring regulatory domain. A disease-associated variant in an intergenic region may exert its effect by disrupting a distal element that contacts its target gene through chromatin looping. Mammalian gene regulation operates across distances that dwarf the context windows of most sequence models. The convolutional architectures examined in Chapter 6 excel at detecting local motifs but cannot span these distances. A model that processes sequences in kilobase windows treats regulatory elements tens of kilobases away as if they do not exist. For understanding human gene regulation, they effectively do not.\nThe attention mechanisms introduced in Chapter 7 could theoretically model arbitrary-range dependencies, but naive transformer application to hundred-kilobase windows is computationally prohibitive. Attention scales quadratically with sequence length: doubling context length quadruples memory and computation. Processing 200 kilobases at single-nucleotide resolution would require attending over 200,000 positions simultaneously, far beyond practical limits. The field needed architectures that could span regulatory distances without the quadratic penalty.\nHybrid architectures resolve this tension by combining the strengths of both paradigms. A convolutional front-end efficiently extracts local sequence features and compresses the input to a manageable length, reducing 200 kilobases of sequence to a few thousand feature vectors. A transformer backbone then propagates information across this compressed representation through attention. The result is a new class of regulatory models that can capture enhancer-promoter interactions, predict the effects of distal variants on gene expression, and provide mechanistic hypotheses about long-range regulation. Enformer, the first widely adopted model in this class, processes 200-kilobase windows and predicts chromatin state, transcription initiation, and gene expression from sequence alone.\nConsider a canonical mammalian gene with complex tissue-specific expression. The promoter sits at the transcription start site, but the sequences that determine when and where the gene is expressed may be scattered across a 200 kilobase neighborhood. Multiple enhancers drive expression in different tissues; silencers suppress expression in inappropriate contexts; insulators demarcate regulatory domains. Chromatin looping brings these distal elements into physical proximity with the promoter, but the loops themselves are dynamic and cell-type-specific.\nShort-context models face an information-theoretic barrier in this setting. A model with a 2 kilobase receptive field cannot distinguish a variant in an enhancer 50 kilobases upstream from a variant in neutral sequence at the same distance. Both fall outside the model’s effective context. Stacking more convolutional layers or using dilated convolutions can expand the receptive field, but the computational path between distant positions grows long, and gradients attenuate over many layers. Models like Basenji2 pushed convolutional receptive fields to tens of kilobases through aggressive pooling, but purely convolutional architectures struggle to propagate information across hundreds of kilobases without impractical depth, a limitation examined in Section 6.6. [Citation Needed]\nThe scale of the problem becomes concrete when examining enhancer-promoter distances in the human genome. Median enhancer-promoter distances in many tissues span 20 to 50 kilobases, with substantial fractions exceeding 100 kilobases (Gasperini et al. 2019). Topologically associating domains (TADs), which define the neighborhoods within which regulatory elements typically interact, range from hundreds of kilobases to several megabases. A model that cannot span these distances cannot fully capture the regulatory grammar of the genome.\nAttention mechanisms offer a direct solution: by computing pairwise interactions between all positions, attention can model dependencies across arbitrary distances in a single layer. The cost is quadratic scaling with sequence length. A naive transformer operating on 200,000 base pairs at single-nucleotide resolution would require attention matrices with 40 billion entries, far exceeding practical memory limits. Hybrid architectures sidestep this constraint by using convolutions to compress the sequence before attention, reducing the effective sequence length to a few thousand tokens while preserving the information needed for long-range modeling.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html#sec-ch13-long-range",
    "href": "part_3/p3-ch13-regulatory.html#sec-ch13-long-range",
    "title": "13  Regulatory Models",
    "section": "",
    "text": "FIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 13.1: [Essential] Four-panel biological motivation. Panel A (Regulatory Element Distribution): Canonical gene locus with promoter at TSS, enhancers 20-100kb away, silencer 50kb downstream, CTCF sites; scale bar 200kb. Panel B (Model Context Windows): Same locus with overlaid windows: DeepSEA (1kb) sees promoter only; Basenji2 (40kb) sees proximal regulation; Enformer (200kb) spans most elements; AlphaGenome (1Mb) full domain. Panel C (Information-Theoretic Barrier): Variant in enhancer 50kb from gene; short-context cannot distinguish from neutral; long-context can model relationship. Panel D (Scale of Problem): Distribution of enhancer-promoter distances; median 20-50kb; substantial fraction &gt;100kb.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html#sec-ch13-enformer",
    "href": "part_3/p3-ch13-regulatory.html#sec-ch13-enformer",
    "title": "13  Regulatory Models",
    "section": "13.2 Enformer: Attention Meets Regulatory Genomics",
    "text": "13.2 Enformer: Attention Meets Regulatory Genomics\nEnformer (Ž. Avsec et al. 2021) demonstrated that combining convolutional compression with transformer attention could dramatically improve expression prediction from sequence. The model processes 200 kilobase windows of DNA and predicts thousands of chromatin and transcription tracks across cell types and species, establishing a template that subsequent models have extended and refined.\n\n13.2.1 Architecture\nThe Enformer architecture consists of three stages that progressively transform raw sequence into multi-task predictions.\nThe convolutional stem takes one-hot encoded DNA (four channels for A, C, G, T) and applies a series of convolutional blocks with residual connections. Each block includes convolutions that detect local patterns, batch normalization and nonlinearities, and pooling operations that reduce sequence length while increasing channel depth. By the end of the stem, a 200 kilobase input has been compressed to roughly 1,500 tokens, each representing approximately 128 base pairs of underlying sequence. This compression is crucial: it reduces the attention computation from quadratic in 200,000 to quadratic in 1,500, a reduction of roughly 17,000-fold in memory requirements.\nThe transformer trunk operates on the compressed sequence through a stack of self-attention layers. Each layer computes attention scores between all pairs of positions, allowing information to flow directly between any two locations in the 200 kilobase window. Relative positional encodings preserve information about the distances between elements, which matters for regulatory biology where the spacing between motifs often carries functional significance. The combination of multi-head attention and feed-forward layers enables the model to learn complex, position-dependent relationships across the full window.\nTask-specific output heads branch from the shared transformer backbone. Separate heads predict different types of outputs: DNase accessibility and ATAC-seq signal (chromatin openness), histone modifications including H3K4me3, H3K27ac, and other marks, CAGE signal reflecting transcription initiation, and additional functional genomics readouts where training data is available. Each head consists of convolutional and linear layers that transform the shared representation into track-specific predictions.\nThe multi-task design serves multiple purposes. Different assays provide complementary supervision: chromatin accessibility reflects regulatory potential, histone marks indicate active enhancers and promoters, and CAGE captures transcriptional output. Training on all assays jointly encourages the backbone to learn representations that capture the full regulatory cascade from accessible chromatin through enhancer activation to transcription initiation.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 13.2: [Essential] Detailed three-stage architecture. Stage 1 (Convolutional Stem): 200kb one-hot input → conv blocks with residual connections → pooling → ~1,500 tokens (128bp resolution); “Compresses 100×, detects local features.” Stage 2 (Transformer Trunk): Stack of self-attention layers, 8 heads, relative positional encodings; “Enables long-range interaction modeling.” Stage 3 (Task-Specific Heads): Shared backbone → multiple outputs (DNase, histone marks, CAGE, ~5,000 tracks); “Multi-task regularizes representations.” Inset: Why Hybrid Works: quadratic on 1,500 vs 200,000 tokens.\n\n\n\n\n\n13.2.2 Training Data and Cross-Species Learning\nEnformer trains on functional genomics data from both human and mouse, spanning hundreds of assays and cell types. The chromatin accessibility, histone modification, and transcription initiation assays introduced in Section 2.4.1 and Section 2.4 provide the supervision signals: DNase-seq and ATAC-seq measure regulatory potential, ChIP-seq for histone marks identifies active enhancers and promoters, and CAGE captures where transcription begins. Human training data derives largely from ENCODE and Roadmap Epigenomics consortia, supplemented by CAGE data from FANTOM and additional chromatin profiling studies [Citation Needed]. Mouse data from analogous consortia provides complementary supervision.\nCross-species training confers several advantages. Regulatory sequences that are functionally constrained evolve more slowly than neutral sequence, so mouse and human share many regulatory motifs and principles despite 80 million years of divergence. Training on both species helps the model distinguish conserved regulatory logic from species-specific noise, reduces overfitting to idiosyncrasies of human data, expands the effective training set without requiring additional human samples, and implicitly emphasizes evolutionarily conserved patterns that are more likely to be functionally important.\nThe training objective combines losses across all tracks, positions, and species. Count-based likelihoods (Poisson or negative binomial) handle sequencing-derived signals, while correlation-based objectives ensure the model captures the overall shape of coverage profiles. Per-track weighting prevents abundant assays from dominating gradients.\n\n\n13.2.3 Variant Effect Prediction\nThe clinical and scientific value of Enformer lies substantially in its ability to predict how sequence variants alter regulatory activity. The procedure follows a straightforward logic: extract a 200 kilobase window containing the variant, compute predictions for the reference allele, compute predictions for the alternative allele, and compare the outputs across all tracks and positions.\nThe resulting variant effect scores span thousands of dimensions, one for each assay and cell type. A variant might increase predicted DNase accessibility in one cell type while decreasing predicted CAGE signal in another, suggesting context-dependent regulatory effects. By aggregating predictions around gene promoters, researchers can estimate variant effects on gene expression in specific tissues.\nValidation against GTEx expression quantitative trait loci (eQTLs) demonstrated that Enformer’s predictions correlate with observed genetic effects on expression (Ž. Avsec et al. 2021). Variants with large predicted effects on promoter-proximal CAGE signal were enriched among significant eQTLs. Notably, this correlation extended to distal variants: sequence changes 50 kilobases or more from a gene’s transcription start site still showed predictive power when they fell in regions of predicted regulatory activity. This long-range predictive capacity distinguishes Enformer from short-context models and validates the architectural investment in extended context windows. These predictions integrate with classical variant effect methods (Chapter 4) and foundation model approaches (Section 14.3.2) to provide comprehensive variant interpretation, with clinical workflow integration detailed in Section 26.1.4.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 13.3: [High] Workflow diagram. Step 1: Variant position → extract 200kb window centered; reference and alternative versions. Step 2: Parallel forward passes → output 5,000+ tracks × positions. Step 3: Delta computation (alt - ref) → identify tracks/positions with largest changes. Step 4: Interpretation examples: DNase ↓ in liver (reduced accessibility), H3K27ac ↓ at enhancer (weakened activity), CAGE ↓ at target gene (predicted expression decrease). Validation inset: Correlation with GTEx eQTLs; performance on distal variants (50+ kb).",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html#sec-ch13-borzoi",
    "href": "part_3/p3-ch13-regulatory.html#sec-ch13-borzoi",
    "title": "13  Regulatory Models",
    "section": "13.3 Borzoi: From Chromatin to Transcriptome",
    "text": "13.3 Borzoi: From Chromatin to Transcriptome\nWhile Enformer predicts transcription initiation through CAGE, RNA-seq captures a richer picture of gene expression: not just where transcription begins, but how the transcript is spliced, which isoforms dominate, where transcription terminates, and how stable the resulting mRNA is. Borzoi (Linder et al. 2025) extends the hybrid architecture paradigm to predict full RNA-seq coverage profiles, enabling a unified view of how sequence variation affects the entire transcriptional program.\n\n13.3.1 Beyond Transcription Initiation\nA single gene can produce multiple transcript isoforms through alternative promoter usage, alternative splicing, and alternative polyadenylation. These isoforms may have different stabilities, different translation efficiencies, and different functions. A variant that shifts isoform ratios without changing total expression could have substantial phenotypic consequences: a switch from a cytoplasmic to a nuclear isoform, for instance, or inclusion of a premature stop codon in the predominant transcript.\nCAGE and chromatin assays cannot capture these complexities. They measure where transcription might begin and what the chromatin environment looks like, but they do not reveal how RNA polymerase traverses the gene body, where splicing occurs, or which 3’ end is selected. RNA-seq coverage profiles encode all of this information: exon boundaries appear as coverage drops at intron junctions, alternative splicing manifests as variable junction usage, and polyadenylation site choice appears in the coverage pattern near gene 3’ ends.\n\n\n13.3.2 Predicting Coverage at Nucleotide Resolution\nBorzoi builds on an Enformer-style backbone with modifications tailored to RNA-seq prediction. The convolutional stem and transformer trunk follow similar principles, compressing long input windows and propagating information through attention. Output heads predict stranded RNA-seq coverage across the window, with additional heads for complementary signals like PRO-seq (nascent transcription), CAGE, and other assays when available.\nTraining on RNA-seq coverage imposes different demands than training on chromatin marks. Coverage varies over orders of magnitude between introns and exons; the model must capture both the overall expression level and the fine structure of the coverage profile. Junction reads that span splice sites provide particularly informative supervision, as they directly constrain the model to learn splicing patterns. The loss function balances accurate prediction of coverage levels with faithful reproduction of the coverage shape, including sharp transitions at exon boundaries.\n\n\n13.3.3 Applications Beyond Expression Level\nBy predicting full RNA-seq coverage, Borzoi enables analyses that go beyond simple expression quantification. Splicing variant effects can be assessed by comparing predicted coverage at exons and junctions under reference and alternative alleles. A variant that reduces predicted junction reads for a particular exon suggests exon skipping; increased junction reads to a cryptic splice site suggests aberrant splicing. These predictions complement specialized splicing models like SpliceAI (Section 6.5), providing additional context about how splicing changes fit within the broader transcriptional program. The integration of Borzoi splicing predictions with SpliceAI scores is examined in Section 14.3.1.\nAlternative promoter usage becomes visible through coverage patterns near transcription start sites. A variant that increases coverage downstream of one TSS while decreasing it downstream of another suggests a shift in promoter preference. Such shifts can alter the 5’ UTR of the resulting transcript, affecting translation efficiency and regulatory motif content.\nPolyadenylation site choice affects 3’ UTR length and content. Shorter 3’ UTRs may escape microRNA-mediated repression; longer ones may include additional regulatory elements. Borzoi’s coverage predictions around annotated polyadenylation sites can reveal variants that shift site usage, potentially explaining effects on mRNA stability and translation that would be invisible to chromatin-based models.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 13.4: [High] Four-panel figure. Panel A (What RNA-seq Captures): Gene with alternative isoforms; CAGE sees only TSS; RNA-seq coverage shows exon structure, splice junctions, polyadenylation. Panel B (Borzoi Predictions): Example gene with complex splicing; predicted stranded coverage profile; exon boundaries visible. Panel C (Beyond Expression Level): Splicing effects (changed junction usage), alternative promoters (shifted TSS), polyadenylation (altered 3’ UTR length); each with clinical relevance. Panel D (Integration): Borzoi predictions + SpliceAI scores; complementary information.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html#sec-ch13-sei",
    "href": "part_3/p3-ch13-regulatory.html#sec-ch13-sei",
    "title": "13  Regulatory Models",
    "section": "13.4 Sei: A Regulatory Vocabulary from Sequence",
    "text": "13.4 Sei: A Regulatory Vocabulary from Sequence\nWhile Enformer and Borzoi predict continuous coverage tracks, Sei (Chen et al. 2022) takes a complementary approach: learning a discrete vocabulary of sequence classes that capture distinct regulatory activities. Rather than predicting thousands of individual assays, Sei maps sequences to a reduced set of regulatory states, each associated with characteristic chromatin and transcription patterns.\n\n13.4.1 Discrete Regulatory States\nSei builds on observations that chromatin states cluster into interpretable categories: active promoters, strong enhancers, poised enhancers, heterochromatin, and so forth. Previous methods like ChromHMM defined such states from observed chromatin marks in specific cell types [Citation Needed]. Sei learns to predict sequence class membership directly from DNA, asking what regulatory identity a sequence carries based on its intrinsic properties.\nThe model predicts 40 sequence classes derived from clustering patterns across chromatin accessibility, histone modifications, and transcription factor binding. Each class corresponds to a recognizable regulatory state: promoter-like sequences, enhancer-like sequences, CTCF binding sites, repressed regions, and various intermediate states. The output is not a single class assignment but a probability distribution over classes, reflecting the observation that many sequences have context-dependent regulatory potential.\n\n\n13.4.2 Complementary to Track Prediction\nSei and Enformer-style models serve complementary purposes. Enformer provides detailed, quantitative predictions across specific assays and cell types; Sei provides a compressed, interpretable summary of regulatory identity. For variant interpretation, both perspectives can be valuable. Enformer might reveal that a variant reduces predicted H3K27ac signal in liver but not heart; Sei might reveal that the same variant shifts sequence class membership from “strong enhancer” toward “weak enhancer,” a more immediately interpretable characterization.\nThe regulatory vocabulary approach also facilitates systematic analysis across many variants. Rather than tracking changes in thousands of individual tracks, researchers can ask how a set of variants affects the distribution of regulatory classes, identifying patterns that might be obscured in high-dimensional track space.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html#sec-ch13-alphagenome",
    "href": "part_3/p3-ch13-regulatory.html#sec-ch13-alphagenome",
    "title": "13  Regulatory Models",
    "section": "13.5 AlphaGenome: Unifying Modalities at Megabase Scale",
    "text": "13.5 AlphaGenome: Unifying Modalities at Megabase Scale\nAlphaGenome (Z. Avsec, Latysheva, and Cheng 2025) extends the hybrid modeling paradigm in two directions: longer context windows (approximately one megabase) and broader output modalities spanning chromatin, expression, splicing, and three-dimensional contacts. The goal is a single model that provides a comprehensive view of how sequence determines regulatory state.\n\n13.5.1 From 200kb to One Megabase\nThe megabase context window pushes against computational limits even with hybrid architectures. AlphaGenome addresses this through efficient attention mechanisms that reduce the quadratic cost, hierarchical processing that handles different output modalities at appropriate resolutions, and architectural refinements accumulated from Enformer and Borzoi development.\nThe output repertoire spans chromatin accessibility and histone modifications (following Enformer), gene expression and RNA coverage (following Borzoi), splicing predictions including exon inclusion and junction usage, and contact predictions reflecting three-dimensional chromatin organization.\nUnifying these modalities in a single model offers several advantages. The backbone representation must capture information relevant to all outputs, encouraging learning of features that connect chromatin state to transcription to RNA processing. Variant effect predictions become coherent across modalities: a single forward pass reveals how a variant affects chromatin, expression, splicing, and contacts, rather than requiring separate runs through independent models.\n\n\n13.5.2 Closed Weights, Open Questions\nAlphaGenome is primarily available through an API interface rather than as a downloadable model. This arrangement simplifies use for many applications: researchers can score variants without managing large model weights or specialized hardware. It also introduces constraints around data privacy, customization, and integration with local pipelines. Clinical applications that cannot send patient sequence data to external services may be unable to use API-only models directly, motivating interest in openly available alternatives.\nFrom the perspective of variant interpretation workflows, AlphaGenome provides a comprehensive set of predictions from a single query. A variant can be assessed for effects on local chromatin state, expression of nearby genes, splicing of overlapping transcripts, and potential disruption of chromatin contacts, all from the same underlying model. The challenge lies in synthesizing these multiple outputs into actionable conclusions, a topic addressed in Section 14.3.4, with practical workflow integration in Section 14.4.3.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html#sec-ch13-accomplishments",
    "href": "part_3/p3-ch13-regulatory.html#sec-ch13-accomplishments",
    "title": "13  Regulatory Models",
    "section": "13.6 What Hybrid Architectures Accomplish",
    "text": "13.6 What Hybrid Architectures Accomplish\nThe progression from DeepSEA through Enformer, Borzoi, and AlphaGenome reflects accumulating solutions to specific limitations. Each model addresses constraints that bounded its predecessor’s utility.\n\n13.6.1 Spanning Enhancer-Promoter Distances\nThe most direct contribution is enabling long-range interaction modeling. A 200 kilobase context window encompasses the distances over which most cis-regulatory interactions occur. Attention mechanisms allow the model to learn direct relationships between enhancers and promoters without requiring information to propagate through many intermediate layers. Empirically, this translates to improved prediction of expression and better correlation with eQTLs, particularly for variants in distal regulatory elements.\n\n\n13.6.2 Multi-Task Regularization\nTraining on hundreds of assays jointly constrains the model to learn representations that generalize across regulatory modalities. A feature useful only for predicting H3K4me3 in one cell type provides less gradient signal than a feature useful across chromatin, transcription, and accessibility. This multi-task pressure steers the model toward learning fundamental regulatory logic rather than assay-specific artifacts.\n\n\n13.6.3 Cross-Species Constraints\nTraining on human and mouse together further regularizes the model. Species-specific binding site variants, repetitive elements, and technical artifacts in training data affect one species but not the other. Features that generalize across species are more likely to reflect conserved regulatory mechanisms. This provides a form of evolutionary validation built into the training process.\n\n\n13.6.4 Unified Variant Effect Prediction\nPerhaps most practically valuable, hybrid models provide a unified framework for variant effect prediction on expression and related phenotypes. Rather than assembling scores from multiple specialized models, researchers can query a single model for comprehensive predictions. The outputs span cell types and assays, enabling tissue-specific interpretation of regulatory variants. This capability integrates naturally with the variant interpretation workflows described in Section 14.4 and the clinical applications examined in Chapter 26. The calibration of these multi-track predictions for clinical use is addressed in Section 14.5.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html#sec-ch13-limitations",
    "href": "part_3/p3-ch13-regulatory.html#sec-ch13-limitations",
    "title": "13  Regulatory Models",
    "section": "13.7 Limitations and Open Challenges",
    "text": "13.7 Limitations and Open Challenges\nDespite their power, long-context regulatory models face fundamental limitations that bound their current utility and define directions for future development.\n\n13.7.1 Training Data Constraints\nFunctional genomics data is biased in coverage, overrepresenting well-studied cell types (embryonic stem cells, K562, HepG2, lymphoblastoid cell lines) while leaving many tissue types and disease-relevant cell states poorly covered. Models trained on available data will perform better in represented contexts and may fail silently in underrepresented ones. Ancestry bias compounds the problem: most functional genomics studies derive from individuals of European descent, limiting the diversity of haplotypes and regulatory variants represented in training data. These data gaps are examined more comprehensively in Section 2.4.1 and Section 2.4.\nThese biases propagate to variant effect predictions. A variant in a regulatory element active primarily in pancreatic beta cells may receive poor predictions if beta cell data is sparse in training. A variant on a haplotype common in African populations but rare in Europeans may fall outside the model’s effective training distribution. Users must recognize that prediction confidence varies with representation in training data, a consideration that current models do not explicitly communicate. Chapter 22 examines how such biases can compromise model validity.\n\n\n13.7.2 Finite Context\nEven megabase-scale windows capture only local regulation. Trans-acting factors, three-dimensional contacts spanning multiple megabases, and whole-chromosome organization fall outside model context. Structural variants that rearrange large genomic segments, duplicate enhancers, or create novel fusion genes cannot be modeled within fixed-window architectures. The reference genome assumption underlying these models further limits their applicability to complex haplotypes and populations with substantial structural variation relative to the reference.\n\n\n13.7.3 Missing Three-Dimensional Context\nLinear sequence models treat DNA as a one-dimensional string, but gene regulation occurs in three-dimensional nuclear space. Chromatin loops bring distal elements into proximity; nuclear compartmentalization segregates active and repressed regions; phase-separated condensates concentrate regulatory factors. While AlphaGenome predicts some contact features, current hybrid models do not fully integrate three-dimensional chromatin organization. The relationship between linear sequence, three-dimensional structure, and regulatory output remains incompletely captured. Chapter 17 examines models that explicitly address chromatin architecture.\n\n\n13.7.4 Correlation Versus Causation\nHybrid models learn correlations between sequence and functional readouts, not causal mechanisms. A variant might receive a high predicted effect score because it disrupts a motif correlated with expression in training data, not because the motif causally drives expression. Attribution methods can identify which sequence features contribute to predictions, but attribution is not validation. High-confidence predictions require experimental confirmation through approaches like massively parallel reporter assays, CRISPR perturbation, or allelic series analysis.\n\n\n13.7.5 Interpretability Challenges\nThe scale of these models (hundreds of millions of parameters) makes mechanistic interpretation difficult. Attention patterns provide some insight into which positions the model considers related, but attention weights are not guaranteed to reflect the model’s actual computational strategy. Attribution methods (saliency maps, integrated gradients) can highlight important input positions, but the features the model constructs from those positions remain opaque. Chapter 24 examines these interpretability methods and their limitations in detail.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 13.5: [Enhancing] Honest assessment. Panel A (Training Data Bias): Pie chart of ENCODE/Roadmap cell types; overrepresented ESCs, K562, HepG2; underrepresented disease-relevant states. Panel B (Missing 3D Context): Linear sequence model vs actual looped, compartmentalized chromatin; distant elements in 3D proximity. Panel C (Correlation vs. Causation): Variant with high predicted effect; is it because motif disruption causes change, or correlation in training? Panel D (Finite Context): 200kb-1Mb windows; trans-acting factors genome-wide; SVs span megabases.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html#sec-ch13-foundation-models",
    "href": "part_3/p3-ch13-regulatory.html#sec-ch13-foundation-models",
    "title": "13  Regulatory Models",
    "section": "13.8 Relationship to Foundation Models",
    "text": "13.8 Relationship to Foundation Models\nLong-context regulatory models occupy an interesting position in the genomic foundation model landscape. They share key characteristics with foundation models: large scale, broad training data, strong performance across tasks, and utility as feature extractors for downstream applications. Yet they differ from self-supervised DNA language models (Chapter 11) in their heavy reliance on supervised, task-specific training signals.\nEnformer and its descendants can be viewed as highly specialized foundation models, pretrained on the specific task of regulatory prediction and adaptable to related applications. Their representations encode regulatory logic learned from functional genomics supervision, complementing the sequence patterns learned by self-supervised models from raw DNA. In practice, the two approaches may prove most powerful in combination: self-supervised models provide sequence representations from evolutionary context, while supervised regulatory models provide representations from functional genomics context. Integrating these representations for tasks like variant effect prediction is an active area of development, explored further in Chapter 14.\nFrom a practical standpoint, hybrid regulatory models remain among the most directly useful genomic deep learning systems for variant interpretation. They provide quantitative, tissue-specific predictions for regulatory variants, outperform short-context alternatives on distal regulatory elements, and integrate naturally into variant prioritization workflows. Their limitations are real but understood; their strengths are substantial and empirically validated.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-regulatory.html#sec-ch13-prediction-explanation",
    "href": "part_3/p3-ch13-regulatory.html#sec-ch13-prediction-explanation",
    "title": "13  Regulatory Models",
    "section": "13.9 Prediction Without Explanation",
    "text": "13.9 Prediction Without Explanation\nLong-range regulatory prediction from sequence is tractable. Enformer established that hybrid convolutional neural network (CNN)-transformer architectures could span 200 kilobases and predict expression-related chromatin features. Borzoi extended coverage to the full transcriptome with improved quantitative accuracy. AlphaGenome unified multiple regulatory modalities at megabase scale, predicting chromatin accessibility, histone modifications, transcription factor binding, and gene expression from a single architecture. Each generation captures more of the regulatory landscape with greater fidelity to experimental measurements.\nYet these models predict regulatory outcomes without explaining regulatory mechanism. They learn that certain sequence patterns associate with certain expression levels, but they do not represent enhancer-promoter contacts, transcription factor cascades, or the causal chain from sequence to phenotype. The attention patterns that span long distances may correspond to genuine regulatory interactions or may reflect confounded sequence features that happen to predict expression. Interpretability methods (Chapter 24) can probe what patterns models have learned, but high prediction accuracy does not guarantee mechanistic insight.\nThis distinction shapes how regulatory model predictions should be used. For variant effect prediction (Chapter 14), regulatory models provide one input among several: they predict whether a variant alters chromatin accessibility or expression, while protein language models (Chapter 12) assess coding consequences and evolutionary models quantify constraint. The clinical integration of these signals (Chapter 26) requires understanding what each model contributes and where each is likely to fail. Regulatory models excel at predicting noncoding variant effects when the relevant cell type is represented in training data; they struggle with cell types absent from training and with variants acting through mechanisms not captured by the output tracks they predict.\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nGasperini, Molly, Andrew J. Hill, José L. McFaline-Figueroa, Beth Martin, Seungsoo Kim, Melissa D. Zhang, Dana Jackson, et al. 2019. “A Genome-Wide Framework for Mapping Gene Regulation via Cellular Genetic Screens.” Cell 176 (1): 377–390.e19. https://doi.org/10.1016/j.cell.2018.11.029.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html",
    "href": "part_3/p3-ch14-vep-fm.html",
    "title": "14  Variant Effect Prediction",
    "section": "",
    "text": "14.1 Foundation Model Paradigm for Variant Interpretation\nClassical variant effect prediction required labels: pathogenic variants to define one class, benign variants to define another, and enough examples of each to train a classifier. This requirement created a fundamental bottleneck. The variants most important to classify, those that are rare, never before observed, and located in poorly characterized genes, were precisely those for which labels did not exist. Foundation models offer a different paradigm: score variants using patterns learned from unlabeled sequences, without ever seeing a pathogenic/benign label during pretraining. A protein language model trained only to predict masked amino acids can distinguish damaging substitutions from benign polymorphisms because evolution has already encoded this distinction in the sequences that survived. A DNA language model can identify regulatory disruptions because it learned the grammar of functional elements from billions of nucleotides. The variants that violate learned patterns are the variants that disrupt function.\nThis zero-shot capability does not eliminate the need for labeled data but changes its role. Rather than training classifiers from scratch, practitioners fine-tune foundation models on modest variant datasets, leveraging pretrained knowledge to achieve performance impossible for models that start from random initialization. The combination of self-supervised pretraining and supervised fine-tuning produces variant effect predictors that outperform classical methods across most benchmarks while requiring far less task-specific data. AlphaMissense, ESM-1v, and similar systems demonstrate that foundation model representations capture variant effects across protein families, including families with no labeled variants in training data.\nYet significant challenges remain. Foundation models predict that variants are damaging without explaining why. Calibration varies across variant types, protein families, and populations, creating uncertainty about when predictions can be trusted. The distinction between “evolutionarily unusual” and “clinically pathogenic” is real: not every rare substitution causes disease, and not every disease-causing variant appears evolutionarily constrained.\nClassical variant effect predictors operate by aggregating hand-crafted features: conservation scores computed from multiple sequence alignments, amino acid property changes, protein domain annotations, and regulatory marks at genomic loci (Chapter 4). Methods like CADD train machine learning models to distinguish pathogenic from benign variants using these features, achieving useful discrimination but ultimately limited by what features the developers chose to include. When a variant falls in a region poorly covered by existing annotations, classical methods have little to offer.\nFoundation models invert this relationship. Rather than engineering features, they learn representations from raw sequence data during pretraining, then apply those representations to variant interpretation. A protein language model trained to predict masked amino acids implicitly learns which substitutions violate evolutionary constraints. A DNA language model trained to predict nucleotides in genomic context learns which changes disrupt sequence grammar. The representations encode information about structure, function, and constraint that was never explicitly labeled during training.\nThis paradigm shift has practical consequences. Coverage extends to any variant in any gene, not just those with extensive prior annotation. Representations capture subtle patterns (co-evolution between distant residues, context-dependent motif strength) that resist manual feature engineering. Transfer learning enables rapid adaptation to new tasks and variant classes, with the specific strategies detailed in Chapter 9. The cost is interpretability: understanding why a foundation model assigns a particular score requires specialized analysis techniques rather than simple inspection of feature weights (Chapter 24).\nThree architectural families dominate current VEP applications. Protein language models (Chapter 12) encode amino acid sequences and score missense variants by measuring likelihood changes. DNA language models (Chapter 11) operate on nucleotide sequences and can score variants of any type. Regulatory models (Chapter 13) predict molecular phenotypes (chromatin accessibility, gene expression, splicing) and score variants by their predicted impact on these phenotypes. The strongest-performing systems combine elements from multiple families.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-fm-paradigm",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-fm-paradigm",
    "title": "14  Variant Effect Prediction",
    "section": "",
    "text": "FIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 14.1: [Essential] Paradigm comparison. Panel A (Classical Approach): Variant → hand-crafted features (conservation, Grantham, domains, regulatory marks) → feature vector → classifier → score; limitation: coverage limited by annotations. Panel B (Foundation Model Approach): Same variant → pretrained model → learned representations → adaptation → score; strength: extends to any position. Panel C (What Changes): Classical features explicit/interpretable/manual; foundation features learned/opaque/automatic; trade-off arrow. Panel D (Three Families): Protein LMs → missense; DNA LMs → all variants; Regulatory models → noncoding mechanism; arrow: “Combined approaches.”\n\n\n\n\n14.1.1 Zero-Shot and Supervised Approaches\nFoundation model VEP methods divide into two paradigms. Zero-shot approaches apply pretrained models directly without task-specific training: ESM-1v scores variants by comparing amino acid likelihoods, requiring no pathogenicity labels. The model’s pretraining objective (masked token prediction) implicitly teaches which substitutions violate evolutionary constraints. Supervised approaches like AlphaMissense add task-specific training layers and optimize explicitly for pathogenicity prediction using labeled examples.\nThe choice involves tradeoffs. Zero-shot methods avoid label bias entirely; they cannot learn to recapitulate existing predictor scores because they never see those scores during training. Supervised methods achieve stronger discrimination when high-quality labels exist but risk inheriting biases from training data. Zero-shot approaches generalize more reliably to novel proteins outside training distributions; supervised methods may overfit to well-studied gene families. In practice, the strongest current systems (AlphaMissense, popEVE) combine foundation model representations with some supervised adaptation, attempting to capture benefits of both paradigms.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-protein-vep",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-protein-vep",
    "title": "14  Variant Effect Prediction",
    "section": "14.2 Protein-Based Variant Effect Prediction",
    "text": "14.2 Protein-Based Variant Effect Prediction\nMissense variants (single amino acid substitutions) account for approximately half of known pathogenic variants in ClinVar, making protein-level prediction a central challenge [Citation Needed]. Foundation model approaches exploit a simple insight: evolution has already tested billions of amino acid substitutions across millions of years; variants that repeatedly survive natural selection are likely tolerable, while those never observed in homologous proteins likely disrupt function.\n\n14.2.1 Zero-Shot Scoring with Protein Language Models\nThe simplest foundation model approach to missense VEP requires no task-specific training. A protein language model trained on masked token prediction assigns probabilities to each amino acid at each position given surrounding context. Variant effect scores emerge from comparing the probability of the reference amino acid to the probability of the variant amino acid.\nESM-1v operationalizes this approach using the ESM-2 architecture fine-tuned for single-sequence variant effect prediction (Meier et al. 2021). For a variant substituting amino acid \\(a_\\text{ref}\\) with \\(a_\\text{var}\\) at position i, the score is computed as:\n\\[\\Delta \\text{LLR} = \\log P(a_\\text{var} | \\text{context}) - \\log P(a_\\text{ref} | \\text{context})\\]\nNegative scores indicate that the variant amino acid is less probable than reference in learned evolutionary context, suggesting potential deleteriousness. The model sees only the single query sequence, not multiple sequence alignments, yet achieves discrimination competitive with alignment-based methods on deep mutational scanning benchmarks. The emergence of this capability from masked token prediction, without explicit training on variant effects, exemplifies the emergent biological knowledge discussed in Section 12.1.2.\nThis zero-shot capability reflects what protein language models learn during pretraining: structural constraints (buried positions are hydrophobic), functional constraints (active sites are conserved), and co-evolutionary patterns (compensating mutations at contacting residues). The model has never seen pathogenicity labels, yet its predictions correlate with disease association because evolution and disease share underlying biology.\n\n\n14.2.2 Alignment-Based Models: EVE and popEVE\nAn alternative approach explicitly models multiple sequence alignments rather than relying on implicit evolutionary information in single-sequence representations. EVE (Evolutionary Model of Variant Effect) fits a variational autoencoder to the MSA for each protein, learning a generative model that captures position-specific and pairwise constraints (Frazer et al. 2021). Variant scores derive from the change in sequence probability under this model.\nThe EVE architecture consists of an encoder that maps sequences to a latent space and a decoder that reconstructs sequences from latent representations. Training maximizes a lower bound on sequence likelihood across the MSA. For variant scoring, EVE computes the log-likelihood ratio between mutant and wild-type sequences, capturing how surprising the substitution appears given the evolutionary record for that specific protein.\npopEVE extends this framework with improved training procedures and explicit modeling of population allele frequencies (Orenbuch et al. 2025). By incorporating frequency information, popEVE better separates rare deleterious variants from common benign polymorphisms. The model achieves strong performance on ClinVar classification while providing uncertainty estimates through ensemble disagreement.\nThe tradeoff between single-sequence and MSA-based approaches involves coverage versus depth. ESM-1v scores any protein sequence without requiring alignment construction. EVE provides stronger performance when high-quality MSAs are available but cannot score proteins lacking sufficient homologs. For well-studied protein families with deep evolutionary sampling, MSA-based methods remain competitive; for orphan proteins or rapidly evolving sequences, single-sequence models offer the only foundation model option.\n\n\n14.2.3 AlphaMissense: Structure-Informed Pathogenicity Prediction\nAlphaMissense represents the current state of the art for proteome-wide missense pathogenicity prediction, combining protein language model representations with structural information from AlphaFold2 (Cheng et al. 2023). The system provides precomputed scores for 71 million possible missense variants across the human proteome, enabling instant lookup for any variant in any protein-coding gene.\nThe architecture integrates multiple information sources. Sequence representations come from a protein language model encoding the wild-type sequence and mutation position. Structural representations derive from AlphaFold2 predictions, capturing local geometry (secondary structure, solvent accessibility, packing density) and longer-range contacts. A neural network combines these representations to produce a pathogenicity probability between 0 and 1.\nTraining uses a carefully constructed dataset that avoids the circularity plaguing earlier predictors. Rather than training on ClinVar labels (which themselves derive from computational predictions), AlphaMissense uses population frequency as a proxy for pathogenicity: variants common in gnomAD are likely benign, while variants absent from large population samples and observed in disease contexts are likely pathogenic. This approach reduces the risk of learning features that simply recapitulate existing predictor scores.\nCalibration receives explicit attention. Raw model outputs undergo isotonic regression calibration against held-out ClinVar variants, ensuring that predicted probabilities correspond to observed pathogenic proportions (Section 23.2). A score of 0.8 should mean that 80% of variants with similar scores are pathogenic, enabling meaningful clinical interpretation. AlphaMissense reports calibrated scores along with discrete classifications (likely pathogenic, likely benign, uncertain) at thresholds chosen to achieve specific precision targets.\nPerformance on independent benchmarks substantially exceeds classical predictors. On deep mutational scanning datasets (where experimental fitness measurements provide ground truth independent of clinical labels), AlphaMissense achieves correlations of 0.5 to 0.7 depending on the assay, compared to 0.3 to 0.5 for CADD or PolyPhen-2 [Citation Needed]. On ClinVar expert-reviewed variants held out from training, AlphaMissense achieves auROC values above 0.9, representing a meaningful improvement over the 0.85 to 0.88 typical of classical methods [Citation Needed].\nThe structural component proves essential for this performance. Ablation experiments removing AlphaFold2 features degrade performance substantially, particularly for variants at protein-protein interfaces and buried core positions where local geometry determines functional impact. The protein language model captures evolutionary constraint; structural information explains why that constraint exists.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 14.2: [Essential] Four-panel figure. Panel A (Input Integration): Protein sequence \\(\\rightarrow\\) ESM-like embeddings; mutation position \\(\\rightarrow\\) AlphaFold2 structural context (secondary structure, solvent accessibility, contact density); combined features. Panel B (Training Strategy): NOT trained on ClinVar; instead population frequency as pathogenicity proxy; common in gnomAD \\(\\rightarrow\\) likely benign; absent + disease context \\(\\rightarrow\\) likely pathogenic; reduces circularity risk. Panel C (Calibration): Raw outputs \\(\\rightarrow\\) isotonic regression \\(\\rightarrow\\) calibrated probabilities; reliability diagram; score \\(0.8 = 80\\%\\) pathogenic; thresholds annotated. Panel D (Performance): Deep mutational scanning benchmarks (\\(\\rho = 0.5\\)–\\(0.7\\) vs CADD \\(0.3\\)–\\(0.5\\)); ClinVar (\\(\\mathrm{auROC} &gt; 0.9\\) vs \\(0.85\\)–\\(0.88\\)); structural component essential.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-dna-vep",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-dna-vep",
    "title": "14  Variant Effect Prediction",
    "section": "14.3 DNA-Based Variant Effect Prediction",
    "text": "14.3 DNA-Based Variant Effect Prediction\nApproximately 98% of the human genome lies outside protein-coding regions, yet noncoding variants contribute substantially to disease risk through effects on gene regulation, splicing, and genome stability [Citation Needed]. Predicting the impact of these variants requires models that operate directly on DNA sequence rather than translated protein.\n\n14.3.1 Splice Variant Prediction with SpliceAI\nSplicing variants illustrate both the promise and current limitations of deep learning for noncoding VEP. Approximately 10% of pathogenic variants in ClinVar act through splicing mechanisms, disrupting the precise excision of introns from pre-mRNA [Citation Needed]. Classical approaches relied on position weight matrices matching consensus splice site sequences, achieving limited sensitivity for variants outside the core GT-AG dinucleotides.\nSpliceAI applies the dilated convolutional architecture introduced in Chapter 6 to predict splice site usage from raw DNA sequence (Jaganathan et al. 2019). The architecture processes 10,000 nucleotides of context through 32 residual blocks with dilated convolutions (dilation rates increasing from 1 to 128), enabling the receptive field to span several kilobases while maintaining nucleotide resolution. Output heads predict splice donor probability, splice acceptor probability, and junction usage at each position.\nFor variant effect prediction, SpliceAI compares predictions between reference and alternate sequences. The delta score quantifies the change in splice site probability, with positive values indicating gained splice sites and negative values indicating lost sites. Scores exceeding 0.2 correlate with experimentally validated splicing changes; scores above 0.5 have high specificity for pathogenic splicing variants [Citation Needed].\nClinical deployment has validated SpliceAI’s utility. Illumina integrated the model into their clinical interpretation pipeline, and multiple diagnostic laboratories use SpliceAI scores as supporting evidence for ACMG classification. The architectural innovations that enable this performance, including the dilated convolution strategy for expanding receptive fields, are detailed in Section 6.5. The model identifies pathogenic splicing variants missed by classical methods, particularly deep intronic variants that create novel splice sites through cryptic activation.\nLimitations reflect the model’s training data. SpliceAI learned from annotated transcripts representing major isoforms in common tissues. Tissue-specific alternative splicing, rare isoforms, and developmental stage-specific patterns fall outside the training distribution. The model also does not capture downstream consequences: whether a predicted splicing change produces a functional protein, triggers nonsense-mediated decay, or has no phenotypic effect requires additional analysis.\n\n\n14.3.2 Regulatory Variant Prediction with Enformer\nWhile SpliceAI addresses one specific noncoding mechanism, regulatory variants that alter enhancer activity, promoter function, or chromatin organization require different approaches. Enformer (Chapter 13) predicts multiple molecular phenotypes (histone modifications, transcription factor binding, chromatin accessibility, gene expression) from 196,608 base pairs of DNA sequence, providing a substrate for regulatory VEP (Ž. Avsec et al. 2021).\nVariant effect prediction with Enformer compares predicted tracks between reference and alternate sequences. For a variant in an enhancer, the model might predict reduced H3K27ac signal and decreased CAGE expression at the target promoter. These molecular predictions can be aggregated into variant effect scores, with larger predicted changes indicating greater functional impact.\nSeveral challenges complicate Enformer-based VEP. The model predicts relative effects (fold changes in predicted signal) rather than absolute deleteriousness. Calibrating these predictions against pathogenicity labels requires additional supervised training. Cell-type specificity adds complexity: a variant might strongly affect predictions in cardiac tissue while showing no effect in liver, requiring prior knowledge of relevant tissues for clinical interpretation.\nSei extends this approach by learning a regulatory vocabulary: clusters of predicted effects that correspond to interpretable categories like “active promoter,” “strong enhancer,” or “CTCF binding site” (Chen et al. 2022). Variant scores reflect shifts between these categories, providing more interpretable outputs than raw track changes. A variant that converts an enhancer prediction to a quiescent state has clearer implications than one that reduces H3K27ac by 0.3 log-fold.\n\n\n14.3.3 DNA Language Models: GPN-MSA and Evo 2\nDNA language models provide an alternative to phenotype prediction: scoring variants by how unexpected they appear in learned sequence context, analogous to protein language model approaches for missense variants.\nGPN-MSA combines DNA language modeling with multi-species sequence alignments (Benegas et al. 2024). Building on the GPN approach introduced in Section 11.4, the model processes aligned sequences from dozens of vertebrate species, learning which positions are conserved and which tolerate variation. Variant scores derive from likelihood ratios: how much less probable is the variant allele compared to reference given the alignment context? This approach captures deep evolutionary constraint missed by simple conservation scores while providing genome-wide coverage including noncoding regions.\nEvo 2 pushes context length to approximately one megabase, enabling single models to capture local motifs and long-range dependencies simultaneously (Brixi et al. 2025). The StripedHyena architecture provides computational efficiency at this scale through state-space-based sequence modeling rather than quadratic attention, as detailed in Section 11.5.3 and Chapter 7. Training on diverse genomes across the tree of life teaches general principles of sequence organization that transfer to human variant interpretation.\nZero-shot variant scoring with Evo 2 follows the standard likelihood ratio approach. Initial benchmarks show performance competitive with conservation-based scores for coding variants and potentially superior performance for noncoding variants where local sequence context matters more than position-specific conservation. The extremely long context enables modeling of effects mediated by distal elements, though whether this theoretical capability translates to improved VEP remains under investigation.\n\n\n14.3.4 AlphaGenome: Unified Multi-Omic Variant Effect Prediction\nAlphaGenome (Section 13.5) represents the most ambitious current attempt at comprehensive VEP, predicting multiple molecular phenotypes from megabase-scale DNA sequence and using those predictions to assess variant effects across modalities (Z. Avsec, Latysheva, and Cheng 2025).\nVariant effect prediction with AlphaGenome provides mechanistically interpretable outputs. A promoter variant might show reduced accessibility and decreased expression prediction. An enhancer variant might show weakened contact with its target promoter in addition to reduced local histone acetylation. A splicing variant triggers SpliceAI-like splice site changes while also affecting regulatory track predictions near the affected exon.\nThe multi-omic approach enables variant prioritization that considers multiple mechanisms simultaneously. A variant in a regulatory element that affects accessibility, expression, and chromatin contacts represents stronger evidence than one affecting only a single predicted phenotype. Conversely, variants with no predicted effect across modalities can be deprioritized despite proximity to disease genes.\nPractical deployment involves tradeoffs. Evaluating a single variant requires forward passes through the full model, incurring substantial computational cost compared to lookup-based approaches like AlphaMissense. The model may exhibit overconfidence when extrapolating beyond training cell types. Calibrating multi-dimensional predictions into single pathogenicity scores remains an open problem. These constraints position AlphaGenome as a tool for detailed mechanistic investigation of prioritized variants rather than genome-wide screening.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-combining-evidence",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-combining-evidence",
    "title": "14  Variant Effect Prediction",
    "section": "14.4 Combining Evidence Across Modalities",
    "text": "14.4 Combining Evidence Across Modalities\nNo single model addresses all variant types and mechanisms. Missense variants in protein-coding regions call for protein-level predictors; splicing variants require splice-specific models; regulatory variants benefit from long-context DNA models. Practical VEP workflows combine multiple predictors to achieve comprehensive coverage.\n\n14.4.1 Integration Strategies\nThe simplest integration approach applies different models to different variant classes. Missense variants receive AlphaMissense scores; synonymous and intronic variants near splice sites receive SpliceAI scores; promoter and enhancer variants receive Enformer or AlphaGenome predictions. This modular strategy ensures that each variant type receives predictions from an appropriate model.\nMore sophisticated integration aggregates scores across models for the same variant. A missense variant might receive both AlphaMissense (protein impact) and Enformer (regulatory impact, relevant if the codon overlaps a regulatory element) predictions. Combining these requires decisions about weighting and potential double-counting of shared information.\nBayesian approaches offer principled integration. Priors encode beliefs about variant mechanism proportions; likelihoods incorporate model predictions given mechanism; posteriors combine evidence across models while respecting uncertainty. REVEL (Rare Exome Variant Ensemble Learner) demonstrated this approach for classical predictors [Citation Needed]; extending it to foundation model outputs requires careful calibration of each component score.\n\n\n14.4.2 Avoiding Double-Counting\nFoundation models trained on overlapping data risk capturing correlated rather than independent information. AlphaMissense and ESM-1v both encode evolutionary constraint; combining their scores as independent evidence overweights evolutionary signal. Similarly, conservation-based DNA models like GPN-MSA share information with phyloP scores already incorporated in classical predictors.\nCorrelation analysis helps quantify redundancy. If two model scores correlate above 0.8 across a benchmark dataset, they likely provide similar information and should not be counted as independent evidence. Residual analysis can identify what unique signal each model contributes beyond shared components.\nFor ACMG classification, guidelines specifically address computational evidence weighting. The PP3 (computational evidence supporting pathogenicity) and BP4 (computational evidence supporting benignity) criteria apply when multiple tools agree. Using five correlated predictors that all derive from evolutionary conservation should not count as five independent pieces of evidence. Clinical laboratories develop local policies for which tools to consult and how to weight their outputs, ideally based on validation against known variants in their patient population.\n\n\n14.4.3 Practical Workflow Design\nAn effective VEP workflow balances comprehensiveness against efficiency. Genome-wide screening might use fast, zero-shot models (DNA language model likelihood scores) to identify variants deviating from expected sequence patterns. Prioritized variants then receive detailed evaluation with computationally expensive models (AlphaGenome multi-omic predictions). Final interpretation combines computational scores with population frequency, gene-level constraint metrics, segregation data, and clinical phenotype.\nThe ordering matters for efficiency. Filtering the majority of variants with fast models before applying expensive models reduces computational cost by orders of magnitude. The choice of filtering threshold trades sensitivity against specificity: strict thresholds miss true pathogenic variants; lenient thresholds burden downstream analysis with false positives. Threshold selection should match intended use: diagnostic applications prioritize sensitivity while research screening may prioritize specificity.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 14.3: [Essential] Practical integration workflow. Panel A (Model Selection by Variant Type): Missense → AlphaMissense, ESM-1v; splice-proximal → SpliceAI; deep intronic → Enformer, cryptic splice; promoter/enhancer → Enformer, AlphaGenome; synonymous → splicing + regulatory; structural → limited coverage flag. Panel B (Evidence Integration): Same variant scored by multiple models; example missense near exon boundary; AlphaMissense 0.85, SpliceAI 0.45, Enformer regulatory effect; combining without double-counting. Panel C (Correlation and Redundancy): Correlation matrix between scores; high correlation = shared info (do not count twice); residual analysis. Panel D (Practical Workflow): Tier 1 fast screening (DNA-LM likelihood); Tier 2 detailed (AlphaMissense, SpliceAI); Tier 3 mechanistic (AlphaGenome); cost/time at each tier.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-calibration",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-calibration",
    "title": "14  Variant Effect Prediction",
    "section": "14.5 Calibration and Clinical Categories",
    "text": "14.5 Calibration and Clinical Categories\nA pathogenicity score of 0.73 means nothing in isolation. If that score reflects a well-calibrated model, approximately 73% of variants receiving similar scores are truly pathogenic, and clinical decisions can proceed accordingly. If the model is miscalibrated, the true pathogenic rate could be 40% or 95%, rendering the score unreliable for clinical interpretation. Model scores become clinically useful only when they map to actionable categories through calibration, the process of ensuring that predicted probabilities match observed frequencies.\n\n14.5.1 Assessing Calibration\nCalibration plots (reliability diagrams) visualize the relationship between predicted probabilities and observed frequencies. Variants are binned by predicted score, and the proportion of pathogenic variants in each bin is plotted against the bin’s mean predicted probability. Perfect calibration falls on the diagonal: a predicted 0.8 pathogenicity corresponds to an 80% observed pathogenic rate. Points below the diagonal indicate overconfidence (predictions exceed reality), while points above indicate underconfidence.\nMost raw model outputs are poorly calibrated. Neural networks trained with cross-entropy loss tend toward overconfidence, predicting probabilities near 0 or 1 more often than warranted. Protein language model likelihood ratios produce unbounded scores requiring transformation before probability interpretation. The theoretical foundations of why deep networks and foundation models exhibit systematic miscalibration, along with formal definitions of calibration metrics including expected calibration error (ECE), are developed in Section 23.2. The specific challenges posed by foundation model miscalibration in clinical settings are examined in Section 23.2.3.\n\n\n14.5.2 Calibration Methods for Variant Effect Prediction\nPost-hoc calibration transforms raw model outputs into probabilities that match observed pathogenicity frequencies. The technical details of these transformations, including temperature scaling, Platt scaling, and isotonic regression, are developed in Section 23.3. Here we focus on their application to variant effect prediction.\nCalibration should use data representative of deployment conditions. Calibrating on ClinVar expert-reviewed variants produces reliable performance on similar variants but may not transfer to novel genes, rare populations, or variant classes underrepresented in ClinVar. A model calibrated on well-studied cancer genes may be systematically overconfident when applied to genes with fewer characterized variants. Stratified calibration by gene function, variant class, or population improves reliability at the cost of increased data requirements.\nThe systematic biases that arise from distribution shift between calibration and deployment present particular challenges for clinical genomics. Foundation models trained predominantly on European-ancestry data may exhibit differential calibration across populations, producing well-calibrated predictions for some patient groups and miscalibrated predictions for others (Chapter 22). These disparities have direct implications for equitable care, as clinical decisions based on miscalibrated predictions will be systematically worse for patients from underrepresented backgrounds. The sources and consequences of such differential calibration are examined in Section 23.2.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 14.4: [High] Four-panel figure. Panel A (The Calibration Problem): Raw model score distribution showing neural network overconfidence (clustering near 0/1); reliability diagram demonstrating miscalibration. Panel B (Calibration Methods): Conceptual overview of temperature scaling, isotonic regression, Platt scaling; before/after reliability diagrams showing improvement. Panel C (Mapping to ACMG Categories): Continuous score from 0 to 1; thresholds defining PP3, VUS, BP4 regions; visualization of threshold selection trade-offs between precision and recall. Panel D (What Uncertainty Looks Like): Intermediate scores reflecting genuine uncertainty; example variants at different score levels with clinical interpretation; annotation emphasizing that “A score of 0.73 means nothing without calibration context.”\n\n\n\n\n\n14.5.3 Mapping to ACMG Categories\nThe ACMG-AMP variant classification framework defines five categories: pathogenic, likely pathogenic, uncertain significance, likely benign, and benign (Richards et al. 2015). Computational evidence contributes to classification through specific criteria: PP3 (computational evidence supporting pathogenicity) and BP4 (computational evidence supporting benignity).\nMapping continuous foundation model scores to these discrete criteria requires threshold selection. Conservative thresholds ensure high precision at the cost of low recall: only variants with very high (or very low) scores receive computational evidence designation. Lenient thresholds increase recall but admit more false positives, potentially inflating pathogenicity classifications. The choice reflects a fundamental trade-off between missing actionable variants and overclassifying benign variants as potentially harmful.\nClinGen sequence variant interpretation working groups have developed model-specific recommendations for computational predictors, specifying score thresholds that correspond to different evidence strengths. Tavtigian and colleagues proposed a Bayesian framework for calibrating computational evidence strength based on odds ratios of pathogenicity at different score thresholds (Tavtigian et al. 2018). Under this framework, thresholds must achieve specific odds ratios (greater than 2.08 for supporting evidence, greater than 4.33 for moderate evidence) to qualify for particular ACMG evidence levels. Pejaver et al. applied this framework to calibrate 13 classical missense predictors, establishing that four tools (BayesDel, MutPred2, REVEL, VEST4) could provide up to Strong evidence for pathogenicity (Pejaver et al. 2022). In 2025, ClinGen extended these calibrations to foundation model-based predictors, demonstrating that AlphaMissense, ESM1b, and VARITY all reach Strong evidence for pathogenicity and Moderate for benignity at appropriate score thresholds (Bergquist et al. 2025). Laboratories should select tools with established calibrations and document threshold choices in variant reports.\n\n\n14.5.4 The Challenge of Uncertain Significance\nThe variant of uncertain significance (VUS) category deserves particular attention. Variants with intermediate foundation model scores genuinely reflect uncertainty: the models cannot confidently distinguish pathogenic from benign. This uncertainty may arise from limited training data for the gene or variant class, conflicting signals in the sequence context, or genuine biological ambiguity where the variant’s effect depends on factors the model cannot observe.\nForcing these variants into discrete categories by applying arbitrary cutoffs misrepresents the actual evidence. A variant scored at 0.55 is not “slightly pathogenic”; it is a variant for which the model has insufficient evidence to discriminate. Reporting calibrated probabilities alongside discrete classifications preserves information for downstream decision-making. Clinicians can then integrate computational evidence with functional studies, segregation data, and clinical presentation, appropriately weighting the computational contribution based on its expressed uncertainty.\nThe broader framework for understanding and quantifying uncertainty in foundation model predictions, including methods for distinguishing uncertainty arising from limited data (epistemic uncertainty) from uncertainty inherent in the prediction task (aleatoric uncertainty), is developed in Chapter 23. Conformal prediction methods that provide finite-sample coverage guarantees for variant classification are examined in Section 23.5.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-uncertainty",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-uncertainty",
    "title": "14  Variant Effect Prediction",
    "section": "14.6 Uncertainty Quantification",
    "text": "14.6 Uncertainty Quantification\nCalibration addresses systematic bias in probability estimates; uncertainty quantification addresses the confidence of individual predictions. A well-calibrated model might correctly estimate that 70% of variants in some category are pathogenic, but for any individual variant, we want to know whether the model’s prediction is reliable or whether the variant falls outside the model’s competence.\n\n14.6.1 Sources of Uncertainty\nEpistemic uncertainty reflects gaps in the model’s knowledge: regions of input space with sparse training data, variant types rarely observed during training, or proteins from understudied families. This uncertainty is reducible in principle by collecting more data and can be estimated by measuring model disagreement across training variations.\nAleatoric uncertainty reflects inherent noise in the prediction target: variants whose pathogenicity genuinely varies across individuals or contexts, or cases where the same score corresponds to both pathogenic and benign variants for biological rather than modeling reasons. This uncertainty is irreducible by additional training and represents fundamental limits on predictability.\nDistinguishing these uncertainty types matters for interpretation. High epistemic uncertainty suggests caution: the model has not seen similar variants and may be extrapolating unreliably. High aleatoric uncertainty suggests that the variant’s effect genuinely depends on factors not captured by sequence alone.\n\n\n14.6.2 Uncertainty Estimation Methods\nEnsemble methods train multiple models on different data subsets or with different random initializations. Prediction variance across ensemble members estimates epistemic uncertainty. Large disagreement indicates that the prediction depends strongly on training specifics rather than robust learned patterns. Deep ensembles provide well-calibrated uncertainty estimates but multiply computational cost linearly with ensemble size.\nMonte Carlo dropout approximates Bayesian inference by applying dropout at test time and averaging predictions across multiple stochastic forward passes. Variance across passes estimates uncertainty without training multiple models. This approach adds modest computational overhead and can be applied to any dropout-containing architecture.\nConformal prediction provides distribution-free uncertainty quantification with coverage guarantees (Angelopoulos and Bates 2023). Given a calibration set, conformal methods construct prediction sets guaranteed to contain the true label with specified probability (e.g., 90%). For variant classification, this might produce sets like {pathogenic, uncertain} or {benign} depending on the variant and desired coverage. Larger prediction sets indicate greater uncertainty; single-element sets indicate confident predictions. Section 23.5 examines conformal methods for genomic applications in detail.\n\n\n14.6.3 Out-of-Distribution Detection\nBeyond quantifying uncertainty for in-distribution predictions, responsible deployment requires detecting when inputs fall outside the model’s training distribution. A protein language model trained on natural proteins may produce confident but unreliable predictions for synthetic sequences or fragments. A regulatory model trained on common cell types may fail on rare developmental stages.\nLikelihood-based detection uses the model’s own representations to identify unfamiliar inputs. Sequences with low embedding density or anomalous attention patterns may fall outside the training distribution regardless of predicted scores. Flagging these inputs for manual review prevents automated classification of cases the model cannot reliably assess.\nDistance-based methods compare new inputs to training examples in representation space. Variants far from any training example in embedding space warrant skepticism even if the model produces confident predictions. Maintaining summary statistics of training representations enables efficient distance computation at deployment.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 14.5: [High] Four-panel figure. Panel A (Sources of Uncertainty): Epistemic (model has not seen similar, reducible) vs aleatoric (inherent noise, irreducible); training distribution vs query position. Panel B (Ensemble Methods): Multiple models trained differently; prediction variance; high disagreement = high epistemic uncertainty. Panel C (Conformal Prediction): Calibration set → conformal scores; prediction sets with coverage guarantee; confident {Benign} vs uncertain {P, VUS, B}. Panel D (OOD Detection): Embedding space; training examples dense region; query variant isolated = OOD; flag for manual review.\n\n\n\nChapter 23 develops uncertainty quantification methods in detail, including practical implementation guidance and evaluation metrics. For VEP applications, the key insight is that uncertainty estimates complement point predictions: high-confidence predictions can inform clinical decisions; low-confidence predictions should prompt additional evidence gathering rather than blind acceptance of model outputs.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-fm-gains",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-fm-gains",
    "title": "14  Variant Effect Prediction",
    "section": "14.7 What Foundation Models Add",
    "text": "14.7 What Foundation Models Add\nHaving surveyed current foundation model approaches, we can now directly address what they contribute beyond classical methods (Chapter 4). The answer is nuanced: substantial improvements in some domains, modest gains in others, and persistent blind spots that new architectures have not yet resolved.\n\n14.7.1 Improved Discrimination\nOn standard benchmarks, foundation model VEP methods consistently outperform classical predictors. AlphaMissense achieves auROC of 0.91 on held-out ClinVar missense variants compared to 0.85 for CADD [Citation Needed]. SpliceAI detects pathogenic splicing variants with sensitivity of 0.90 compared to 0.60 for MaxEntScan [Citation Needed]. GPN-MSA scores correlate more strongly with deep mutational scanning measurements than phyloP or GERP [Citation Needed].\nThese improvements reflect richer representations. Classical methods aggregate independent features (conservation, amino acid properties, domain annotations); foundation models learn nonlinear interactions among positions and capture patterns too subtle for manual feature engineering. The gap is largest for variants where context matters: buried core missense variants where structural environment determines impact, splice variants where cryptic site activation depends on flanking sequence, regulatory variants where motif disruption interacts with chromatin context.\n\n\n14.7.2 Extended Coverage\nClassical methods often fail silently on understudied genes, rare variant classes, or poorly annotated regions. SIFT and PolyPhen require protein alignments; variants in singleton genes without homologs receive no prediction. CADD depends on annotation features; variants in regions lacking regulatory marks receive uninformative scores.\nFoundation models degrade more gracefully. Protein language models score any amino acid sequence regardless of available homologs. DNA language models score any genomic position regardless of existing annotation. This extended coverage matters for clinical sequencing of rare diseases, where pathogenic variants often reside in less-studied genes precisely because their severe effects are incompatible with population frequency.\n\n\n14.7.3 Mechanistic Interpretability\nAlphaGenome and similar multi-output models provide predictions about mechanism rather than bare pathogenicity scores. A variant flagged as deleterious might also show predicted effects on chromatin accessibility, contact frequency, and downstream gene expression. These mechanistic predictions enable hypothesis generation and targeted experimental validation (Chapter 24).\nClassical methods offer limited mechanistic insight. CADD provides a single score without indicating whether it derives from conservation, protein impact, regulatory disruption, or other features. Decomposing the score into component contributions requires separate analysis. Foundation models that predict molecular phenotypes naturally provide this decomposition.\n\n\n14.7.4 Persistent Limitations\nFoundation models have not solved several fundamental challenges. Ancestry bias persists because training data remain skewed toward European populations; performance degrades for variants common in African or Asian populations but rare in training sets. The systematic analysis of ancestry-related confounding appears in Section 22.2.1, with broader confounding detection methods in Section 22.8. Calibration requires substantial labeled data that inherit existing biases. Rare variant classes (structural variants, complex indels, repeat expansions) lack sufficient training examples for reliable prediction.\nThe comparison to classical methods reveals diminishing returns on certain axes. For well-conserved active site variants in thoroughly studied proteins, PolyPhen-2 already achieves near-optimal performance; AlphaMissense improves marginally. The largest foundation model gains appear for difficult cases where classical features are uninformative or misleading.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 14.6: [Enhancing] Balanced assessment. Panel A (Performance Improvements): Benchmark comparison table (Method | ClinVar auROC | DMS Correlation | Coverage). Panel B (Where Improvements Largest): Difficult cases (sparse annotations, orphan genes), context-dependent effects, novel genes. Panel C (Persistent Gaps): Population bias, complex variants (SVs, repeats, phase), combinatorial effects (epistasis, compound het), tissue specificity. Panel D (The Bottom Line): Foundation models = tools for interpretation, not oracles; best use = combined with population data, functional evidence, clinical judgment.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-clinical-integration",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-clinical-integration",
    "title": "14  Variant Effect Prediction",
    "section": "14.8 Clinical Integration Considerations",
    "text": "14.8 Clinical Integration Considerations\nFoundation model VEP tools require thoughtful integration into clinical workflows. Their benchmark performance does not automatically translate without attention to deployment context, validation requirements, and human factors.\n\n14.8.1 Laboratory Validation\nBefore clinical use, laboratories should validate foundation model tools against local truth sets representing their patient population. Published benchmark performance on ClinVar may not generalize to a laboratory’s specific case mix. Validation should assess discrimination (can the tool distinguish pathogenic from benign?), calibration (do probability estimates match observed frequencies?), and utility (does incorporating the tool improve variant classification compared to existing workflows?).\nValidation requires variants with known pathogenicity independent of the computational predictions being tested. Using ClinVar variants whose classifications already incorporated CADD scores to validate CADD creates circular reasoning, a form of label circularity examined in Section 22.5. Gold-standard variants from functional studies, segregation data, or expert review provide cleaner validation targets, with detailed evaluation methodology in Chapter 21.\n\n\n14.8.2 Workflow Integration\nFoundation model predictions represent one evidence type among many. ACMG guidelines specify how computational evidence combines with population frequency, functional data, segregation, and clinical phenotype. Computational evidence alone rarely suffices for pathogenic or benign classification; it supports or weakens classifications established by other evidence types.\nLaboratory information systems require modification to display and store foundation model outputs alongside existing annotations. Analyst training ensures appropriate interpretation: understanding that high scores indicate deleteriousness without establishing causation, recognizing when scores fall outside validated ranges, and knowing when to request additional evidence for uncertain cases.\n\n\n14.8.3 Communication to Clinicians\nVariant reports communicated to ordering clinicians should present foundation model evidence appropriately. Reporting raw scores without context confuses non-specialist clinicians. Reporting discrete classifications without uncertainty may convey false confidence. Effective reporting might state: “Computational tools (AlphaMissense, SpliceAI) concordantly predict this variant is likely to affect protein function, supporting the PP3 criterion for pathogenicity classification.”\nWhen foundation model predictions conflict with other evidence, reports should acknowledge the discrepancy rather than suppressing inconvenient results. A variant segregating with disease in a family but receiving a benign computational prediction warrants explicit discussion, not quiet exclusion of the computational evidence.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-open-challenges",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-open-challenges",
    "title": "14  Variant Effect Prediction",
    "section": "14.9 Open Challenges",
    "text": "14.9 Open Challenges\nCurrent foundation model approaches leave substantial problems unsolved. These open challenges define directions for future research and areas where clinical caution remains warranted.\n\n14.9.1 Complex Variant Types\nMost current models address single nucleotide variants and small indels. Structural variants (deletions, duplications, inversions spanning kilobases to megabases) remain largely outside foundation model capabilities. Copy number variation, repeat expansions, and complex rearrangements alter genome architecture in ways current sequence models cannot represent. Extending foundation model paradigms to these variant classes requires architectural innovations beyond current approaches.\n\n\n14.9.2 Combinatorial Effects\nGenomes contain multiple variants that may interact. Compound heterozygosity (two variants affecting both copies of a gene) creates pathogenic states from individually tolerable variants, a clinical scenario examined in Section 1.4.1 and Section 26.3.2. Modifier variants in other genes modulate penetrance. Haplotype effects mean variants on the same chromosome have different consequences than variants on opposite chromosomes, with phasing methods to distinguish these scenarios detailed in Section 1.4. Current models score variants independently, ignoring these interactions that determine clinical presentation.\n\n\n14.9.3 Phenotype Specificity\nA variant pathogenic for one phenotype may be benign for another. SCN5A variants cause distinct cardiac arrhythmia syndromes depending on their specific functional effects [Citation Needed]. Foundation models trained on pathogenic/benign labels average across phenotypes, potentially obscuring clinically relevant specificity. Phenotype-specific training requires much larger datasets than currently available.\n\n\n14.9.4 Temporal and Environmental Context\nVariant effects often depend on age, environmental exposures, or physiological state. A variant pathogenic under metabolic stress may be tolerable at baseline. Foundation models capture sequence context but not the dynamic biological context determining phenotypic expression. Integrating longitudinal clinical data with sequence-level predictions remains an unsolved challenge.\n\n\n14.9.5 Equity and Access\nState-of-the-art foundation models require substantial computational resources for training and sometimes for inference. Laboratories in resource-limited settings may lack access to cutting-edge tools, creating a two-tiered system where well-funded institutions deploy sophisticated variant interpretation while others rely on simpler methods. Precomputed scores (like AlphaMissense’s proteome-wide release) partially address computational barriers, but equity concerns extend far beyond compute access.\nTraining data composition determines which patients foundation models serve well. ClinVar contains many more pathogenic variant classifications for European-ancestry individuals than for other populations (Landrum et al. 2018). Protein language models trained predominantly on sequences from well-studied organisms may capture evolutionary constraints less accurately for proteins divergent from training distributions. The consequence is systematic: variant interpretation performs best for patients who already benefit most from biomedical research, and worst for those historically excluded. A diagnostic laboratory serving a diverse urban population will encounter variants where foundation model predictions are less reliable precisely because those variants come from underrepresented ancestries.\nValidation cohorts exhibit similar biases. When foundation models are evaluated on ClinVar or gnomAD-derived benchmarks, performance metrics reflect accuracy for the populations overrepresented in those resources. A model achieving 0.95 auROC on standard benchmarks may achieve substantially lower discrimination for African-ancestry variants simply because the benchmark itself undersamples that population. Equitable deployment requires ancestry-stratified evaluation that explicitly reports performance gaps, not aggregate metrics that obscure disparities (Chapter 21). The broader implications of these biases, and governance frameworks for addressing them, receive comprehensive treatment in Chapter 29.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-vep-fm.html#sec-ch14-conclusion",
    "href": "part_3/p3-ch14-vep-fm.html#sec-ch14-conclusion",
    "title": "14  Variant Effect Prediction",
    "section": "14.10 Tools for Interpretation, Not Oracles",
    "text": "14.10 Tools for Interpretation, Not Oracles\nFoundation models have transformed variant effect prediction from feature engineering to representation learning. Protein language models capture evolutionary constraint at resolution that multiple sequence alignments cannot match. DNA language models and regulatory models extend coverage to noncoding variants across the genome. Multi-omic architectures provide mechanistic predictions enabling hypothesis generation beyond bare deleteriousness scores. The best current methods substantially outperform classical approaches on established benchmarks, particularly for rare variants and novel genes where training data are sparse.\nYet benchmark performance does not automatically translate to clinical utility. Calibration requires careful attention: a model may discriminate pathogenic from benign variants while systematically overestimating or underestimating probabilities. Uncertainty quantification remains immature; models often produce confident predictions for inputs that fall outside their training distribution. Population bias persists despite foundation model advances; improvements over classical methods are smallest for ancestry groups underrepresented in training data. Complex variant types, combinatorial effects, and tissue-specific consequences remain beyond current capabilities.\nClinical deployment demands humility alongside enthusiasm. Foundation model VEP tools are aids to human interpretation, not autonomous classifiers. Their predictions inform rather than determine variant classification, complementing population frequency data, functional assay evidence, segregation analysis, and clinical judgment. Used appropriately, they accelerate diagnosis and reduce missed findings. Used as oracles, they create false confidence and may perpetuate existing inequities in genomic medicine. Clinical workflows (Chapter 26, Chapter 25) integrate these predictions alongside uncertainty quantification (Chapter 23) and interpretability methods that probe what foundation models have learned (Chapter 24). Variant effect prediction sits at the center of genomic medicine; foundation models have raised its ceiling while the work of achieving its potential continues.\n\n\n\n\nAngelopoulos, Anastasios N., and Stephen Bates. 2023. “Conformal Prediction: A Gentle Introduction.” Foundations and Trends® in Machine Learning 16 (4): 494–591. https://doi.org/10.1561/2200000101.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBergquist, Timothy, Sarah L. Stenton, Emily A. W. Nadeau, Alicia B. Byrne, Marc S. Greenblatt, Steven M. Harrison, Sean V. Tavtigian, et al. 2025. “Calibration of Additional Computational Tools Expands ClinGen Recommendation Options for Variant Classification with PP3/BP4 Criteria.” Genetics in Medicine 27 (6): 101402. https://doi.org/10.1016/j.gim.2025.101402.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nFrazer, Jonathan, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin Gal, and Debora S. Marks. 2021. “[EVE] Disease Variant Prediction with Deep Generative Models of Evolutionary Data.” Nature 599 (7883): 91–95. https://doi.org/10.1038/s41586-021-04043-8.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nOrenbuch, Rose, Courtney A. Shearer, Aaron W. Kollasch, Aviv D. Spinner, Thomas Hopf, Lood van Niekerk, Dinko Franceschi, Mafalda Dias, Jonathan Frazer, and Debora S. Marks. 2025. “[popEVE] Proteome-Wide Model for Human Disease Genetics.” Nature Genetics, November, 1–10. https://doi.org/10.1038/s41588-025-02400-1.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel, Sean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022. “Calibration of Computational Tools for Missense Variant Pathogenicity Classification and ClinGen Recommendations for PP3/BP4 Criteria.” American Journal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie Gastier-Foster, Wayne W. Grody, et al. 2015. “Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology.” Genetics in Medicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nTavtigian, Sean V., Marc S. Greenblatt, Steven M. Harrison, Robert L. Nussbaum, Snehit A. Prabhu, Kenneth M. Boucher, and Leslie G. Biesecker. 2018. “Modeling the ACMG/AMP Variant Classification Guidelines as a Bayesian Classification Framework.” Genetics in Medicine 20 (9): 1054–60. https://doi.org/10.1038/gim.2017.210.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_4/p4--multi-scale.html",
    "href": "part_4/p4--multi-scale.html",
    "title": "Part IV: Systems and Scale",
    "section": "",
    "text": "Biology operates across scales that sequence alone cannot capture. Cells of different types read the same genome differently, activating distinct gene programs that produce neurons, hepatocytes, and immune cells from identical DNA. Genes function not in isolation but within networks of regulation and interaction, where perturbing one node propagates effects throughout the system. The three-dimensional folding of chromatin brings distal elements into contact, creating regulatory logic invisible to models that treat genomes as one-dimensional strings. Sequence foundation models ask what a genome encodes; the models in this part ask what that sequence becomes in particular cellular contexts, interaction networks, and spatial architectures.\nThis transition from sequence-centric to systems-scale modeling demands new data modalities and new computational approaches. Single-cell transcriptomics reveals the cellular heterogeneity that bulk measurements average over. Hi-C and related methods expose the spatial organization that determines which enhancers contact which promoters. Protein interaction networks and gene regulatory graphs capture relational structure that no amount of sequence analysis can infer. Foundation model principles extend naturally to these modalities: learn representations from large-scale data, then transfer to specific prediction tasks.\nRNA structure and function (15  RNA Structure and Function) extend sequence modeling beyond DNA, covering secondary structure prediction, splicing regulation, and the emerging frontier of RNA foundation models. Single-cell transcriptomics and epigenomics (16  Single-Cell Models) present distinct challenges of sparsity, noise, and scale that transformer architectures must adapt to capture. Three-dimensional genome organization (17  3D Genome Organization) adds spatial context, with models predicting chromatin contacts from sequence and connecting spatial structure to gene regulation. Graph neural networks (18  Graph and Network Models) represent biological entities and their interactions as structured graphs, integrating foundation model embeddings with relational reasoning. Multi-omics integration (19  Multi-Omics Integration) broadens the view further, jointly representing genomic, transcriptomic, proteomic, and clinical information to trace the path from genotype to phenotype.",
    "crumbs": [
      "Part IV: Systems and Scale"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html",
    "href": "part_4/p4-ch15-rna.html",
    "title": "15  RNA Structure and Function",
    "section": "",
    "text": "15.1 RNA as Molecule Versus Transcriptome Readout\nA synonymous mutation changes the DNA codon but preserves the amino acid. By the logic of protein-centric biology, such mutations should be functionally neutral: same protein sequence, same structure, same function. Yet synonymous variants can dramatically alter gene expression, affect protein folding, and cause disease. The mechanisms operate at the RNA level: altered codon optimality changes translation speed, modified mRNA secondary structure affects ribosome processivity, disrupted regulatory motifs change transcript stability. A model that sees only DNA sequence or only protein sequence misses these effects entirely. DNA foundation models learn regulatory sequence patterns; protein language models learn amino acid constraints. Neither captures RNA-level biology: secondary structure stability, RBP binding accessibility, or the coupling between splicing and decay.\nRNA occupies a distinct position in the central dogma, essential to every step from transcription to translation, yet historically receiving less computational attention than its neighbors. The disparity reflects data availability more than biological importance. Protein sequences accumulate over billions of years of evolution, providing the massive corpora that enabled ESM to learn structure from sequence (Chapter 12). DNA benefits from reference genomes, population sequencing, and functional genomics consortia generating petabytes of data (Chapter 2). RNA databases remain comparatively sparse, structural annotations cover only well-characterized families, and no equivalent of AlphaFold’s crystallographic training set exists for RNA tertiary structure. The result is a modeling landscape where RNA foundation models exist but remain immature relative to protein and DNA counterparts.\nThe foundation models examined previously all manifest their predictions through RNA intermediates: Enformer predicts RNA-seq coverage (Chapter 13), protein models predict translation products (Chapter 12), SpliceAI models spliceosome recognition of RNA (Section 6.5). RNA-specific models add a distinct layer, treating RNA not merely as a readout of DNA or a precursor to protein, but as a structured molecule with its own sequence constraints, folding landscapes, and functional roles. We examine secondary structure prediction, RNA foundation models, codon-level mRNA models, and noncoding RNA classification, while confronting the data limitations that constrain current approaches.\nTwo complementary perspectives frame computational approaches to RNA. The molecular view treats RNA as a physical object with primary sequence, secondary structure through base pairing, tertiary organization in three-dimensional space, and chemical modifications that alter its properties. In this view, modeling goals include predicting which bases pair with which, how the molecule folds, which proteins bind to it, and how synthetic RNAs might be designed with desired properties. The transcriptomic view treats RNA as a cellular readout: coverage profiles along the genome, splice junction usage, isoform abundances, expression levels that vary across cell types and conditions. Here the goal is explaining how genomic sequence and chromatin state give rise to these measurements.\nModels that predict transcriptomic signals from DNA sequence (Enformer, Borzoi, and related architectures covered in Chapter 13) operate in the second paradigm. They take genomic sequence as input and output RNA-seq or CAGE profiles as predictions. These models never see RNA sequence directly; they learn the mapping from DNA context to transcriptional output. The molecular perspective treats RNA sequence as input and predicts structure, function, or design properties.\nThe distinction parallels the difference between protein language models and proteomics prediction models. ESM takes amino acid sequences and learns structural representations (Chapter 12). A model predicting protein abundance from genomic features would be solving a different problem. Both perspectives are valuable, and both ultimately concern RNA, but they operate at different levels of the biological hierarchy and require different architectures and training strategies.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-structure-challenge",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-structure-challenge",
    "title": "15  RNA Structure and Function",
    "section": "15.2 Why Secondary Structure Creates a Distinct Modeling Challenge",
    "text": "15.2 Why Secondary Structure Creates a Distinct Modeling Challenge\nRNA secondary structure prediction differs fundamentally from protein structure prediction in ways that shape every modeling choice. Where DNA language models learn from linear sequence patterns (Chapter 11) and protein models exploit evolutionary constraints across homologs (Chapter 12), RNA models must contend with conformational dynamics that neither domain faces at comparable scale. Three interrelated challenges define the problem: thermodynamic landscapes with multiple competing minima, base-pairing interactions that span hundreds of nucleotides, and pseudoknot structures that violate the assumptions of efficient algorithms. Understanding these challenges clarifies why RNA structure remains harder to predict than protein structure despite the apparent simplicity of four bases versus twenty amino acids.\n\n15.2.1 Flat Energy Landscape Problem\nRNA’s defining computational challenge emerges from thermodynamics. Proteins fold into stable three-dimensional structures because their energy landscapes contain deep minima: the native state sits in a pronounced funnel that guides the folding process. RNA energy landscapes are remarkably flatter. Multiple conformations compete for occupancy, with free energy differences often smaller than thermal fluctuations at cellular temperatures. A given RNA sequence may adopt several alternative structures with similar stabilities, and the dominant conformation can shift in response to ion concentrations, temperature, protein binding, or chemical modifications.\nThis conformational plasticity has biological functions (riboswitches that change structure in response to ligand binding, RNA thermometers that regulate translation at different temperatures) but creates modeling difficulties. Minimum free energy (MFE) predictions, which identify the single lowest-energy structure, may miss functionally relevant alternative conformations. Partition function calculations that consider the full ensemble are more complete but computationally expensive and harder to interpret. Deep learning models that predict structure from sequence must somehow capture this many-to-many relationship between sequence and conformation, a challenge that protein structure prediction largely avoided because the sequence-to-structure mapping for most proteins is effectively one-to-one.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 15.1: [Essential] Two-panel comparison. Panel A (Protein Folding): 3D surface with deep funnel topology; clear global minimum (native state); steep energy barriers; folding trajectory descending into funnel; “Deep minimum—single stable structure.” Panel B (RNA Folding): Flatter surface with multiple shallow minima at similar energy levels; small energy differences; multiple arrows showing alternative folding paths; “Flat landscape; multiple competing structures.” Bottom panel: Same RNA sequence adopting different structures; riboswitch example; RNA thermometer.\n\n\n\n\n\n15.2.2 Base Pairing and Long-Range Dependencies\nSecondary structure arises from Watson-Crick base pairing (A-U, G-C) and wobble pairs (G-U) that create stems, loops, bulges, and internal loops. Unlike protein secondary structure, where alpha helices and beta sheets are local motifs determined by nearby residues, RNA secondary structure involves long-range contacts. A base at position i may pair with a base at position j hundreds of nucleotides away. The intervening sequence must accommodate this pairing without introducing steric clashes or thermodynamically unfavorable arrangements.\nThis long-range dependency structure differs fundamentally from protein contact prediction, where most important contacts occur between residues close in primary sequence. RNA structure prediction must consider all possible pairings across the entire sequence, evaluate their compatibility, and identify the globally optimal (or near-optimal) arrangement. The number of possible secondary structures grows exponentially with sequence length, making exhaustive enumeration intractable for long RNAs.\n\n\n15.2.3 Pseudoknots and Tertiary Complexity\nPseudoknots occur when bases in a loop pair with bases outside that loop, creating interleaved base-pairing patterns that violate the nested structure assumed by standard secondary structure algorithms. A typical pseudoknot involves two stem regions whose base pairs cross each other when drawn in standard notation. These structures are functionally important (the telomerase RNA catalytic core contains a pseudoknot essential for activity) but algorithmically challenging. Standard dynamic programming approaches for secondary structure prediction exclude pseudoknots because their inclusion increases computational complexity from \\(O(n^3)\\) to \\(O(n^6)\\) or worse. [Citation Needed]\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 15.2: [Essential] Comprehensive diagram. Panel A (Basic Elements): Stem, hairpin loop, internal loop, bulge, multi-loop/junction. Panel B (Long-Range Pairing): Linear sequence with arc diagram showing base pairs; contrast with protein local secondary structure (~10 residues); “RNA base pairs span hundreds of nucleotides.” Panel C (Pseudoknot): Interleaved pairing in notation and 3D; “Increases complexity from \\(O(n^3)\\) to \\(O(n^6)\\)”; telomerase RNA example. Panel D (Notation Systems): Dot-bracket, arc diagram, 2D graphical representation.\n\n\n\nTertiary structure involves the three-dimensional arrangement of secondary structure elements in space, including long-range interactions mediated by non-Watson-Crick base pairs, metal ion coordination, and RNA-RNA kissing loops. Predicting RNA tertiary structure remains far less developed than protein tertiary structure prediction. No RNA equivalent of AlphaFold exists, and the training data situation is dire: the Protein Data Bank contains over 200,000 protein structures but fewer than 2,000 RNA structures, many of which are ribosomal RNA fragments or tRNA variants from the same structural families. [Citation Needed]",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-classical",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-classical",
    "title": "15  RNA Structure and Function",
    "section": "15.3 Classical Approaches to Structure Prediction",
    "text": "15.3 Classical Approaches to Structure Prediction\nBefore deep learning entered the field, two complementary paradigms dominated RNA structure prediction. Thermodynamic approaches compute minimum free energy structures from experimentally calibrated energy parameters, while comparative methods infer structure from patterns of compensatory mutations across homologous sequences. Both approaches remain valuable, and understanding their strengths and limitations illuminates what deep learning models must learn to surpass them.\n\n15.3.1 Thermodynamic Folding Models\nThe dominant classical paradigm for RNA secondary structure prediction relies on nearest-neighbor thermodynamic models. These approaches assign free energy contributions to each base pair and structural element (loops, bulges, internal loops, multiloops) based on experimentally calibrated parameters. Given these parameters, dynamic programming algorithms identify the minimum free energy structure or compute the partition function over all possible structures.\nMfold and the ViennaRNA package represent the most widely used implementations. [Citation Needed] They achieve reasonable accuracy for short, well-behaved RNAs where the thermodynamic parameters are most reliable. Limitations emerge for longer RNAs where the flat energy landscape means many structures have similar energies, for RNAs in complex cellular environments where proteins and other factors alter folding, and for RNAs with modifications or non-canonical interactions not captured by standard parameter sets. These methods also assume equilibrium conditions that may not hold for co-transcriptional folding or kinetically trapped states.\n\n\n15.3.2 Comparative and Covariation Methods\nFor RNAs with sufficient homologous sequences, comparative approaches provide an orthogonal route to structure inference. If two positions exhibit compensatory mutations (G-C changing to A-U while maintaining complementarity), those positions likely base-pair. Databases like Rfam curate consensus secondary structures for RNA families based on these covariation signals. [Citation Needed]\nComparative methods are powerful but require multiple sequence alignments of homologous RNAs. Novel RNAs, rapidly evolving regulatory elements, or species-specific transcripts may lack sufficient homologs for reliable inference. The approach also assumes that structure is conserved across the aligned sequences, which breaks down for RNAs that have diverged in function or that adopt condition-specific alternative structures.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-dl-structure",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-dl-structure",
    "title": "15  RNA Structure and Function",
    "section": "15.4 Deep Learning for Secondary Structure Prediction",
    "text": "15.4 Deep Learning for Secondary Structure Prediction\nDeep learning reframes secondary structure prediction as sequence-to-structure mapping, learning the relationship directly from data rather than encoding it through thermodynamic parameters. These models can capture patterns that classical approaches miss, particularly for complex structures and pseudoknots, though they require training data that remains limited compared to protein structure prediction. Two complementary training strategies have emerged: supervised learning from experimentally determined structures and semi-supervised approaches using structure probing data.\n\n15.4.1 From Thermodynamics to Learned Patterns\nDeep learning models for RNA structure prediction frame the task as sequence-to-structure mapping, analogous to protein contact prediction (Chapter 12). Given an RNA sequence, the model predicts base-pairing probabilities for all position pairs, contact maps indicating which bases interact, or per-nucleotide structural states (paired, unpaired, in loop, in stem).\nModels like SPOT-RNA use convolutional or attention-based architectures to capture long-range dependencies in sequence. [Citation Needed] Some approaches directly predict pairing matrices as dense outputs; others output per-position classifications that are post-processed into structures. Training typically uses experimentally determined structures from databases like RNAstralign or bpRNA, supplemented by computationally predicted structures from thermodynamic models.\nPerformance on benchmark datasets often exceeds classical thermodynamic methods, particularly for RNAs with complex structures or pseudoknots where dynamic programming approaches struggle. The learned models can capture patterns beyond nearest-neighbor rules, potentially encoding longer-range sequence dependencies that contribute to folding but were not parameterized in classical approaches.\n\n\n15.4.2 Structure Probing as Supervision\nHigh-throughput structure probing experiments provide an alternative source of supervision. SHAPE (selective 2’-hydroxyl acylation analyzed by primer extension), DMS-seq, and icSHAPE measure nucleotide accessibility or flexibility across entire transcriptomes. Positions that are base-paired or buried in tertiary structure show lower reactivity than exposed positions. [Citation Needed]\nThese data offer several advantages for model training. They cover far more RNAs than crystal structures, extending beyond well-characterized families to regulatory elements and novel transcripts. They capture structure in cellular context, reflecting the influence of proteins, modifications, and physiological conditions. And they provide soft constraints rather than binary pairing assignments, potentially better matching the conformational heterogeneity of real RNA populations.\nModels trained on structure probing data learn to predict accessibility profiles from sequence. These predictions can be integrated with thermodynamic models (using predicted accessibility as constraints) or used directly for downstream tasks like predicting RNA-protein binding or designing stable constructs.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-foundation",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-foundation",
    "title": "15  RNA Structure and Function",
    "section": "15.5 RNA Foundation Models",
    "text": "15.5 RNA Foundation Models\nThe success of protein language models naturally prompted attempts to apply the same paradigm to RNA. Train large transformers on massive sequence corpora following the scaling principles examined in Section 10.3, learn representations through self-supervised objectives (Chapter 8), then transfer to downstream tasks. RNA foundation models exist and show promise, but they have not yet achieved the transformative impact of their protein counterparts. The reasons illuminate fundamental differences between protein and RNA modeling.\n\n15.5.1 Scale Gap with Protein Language Models\nRNA foundation models attempt to replicate the protein language model paradigm: train large transformers on massive sequence corpora using self-supervised objectives, then transfer learned representations to downstream tasks. The approach has produced working models, but the results lag substantially behind protein language models in both scale and demonstrated capabilities.\nThe comparison with ESM illustrates the gap. ESM-2 trained on over 65 million protein sequences from UniRef, spanning the known diversity of protein families (Chapter 12). RNA-FM, one of the more successful RNA foundation models, trained on approximately 23 million noncoding RNA sequences from RNAcentral (Chen et al. 2022). While not a trivial corpus, this represents an order of magnitude fewer sequences, and the RNA sequences span a narrower range of structural and functional diversity than proteins. The consequences appear in downstream performance: RNA-FM improves over baselines on secondary structure prediction and family classification, but shows nothing like the emergent structure prediction that made ESM-2’s attention patterns predict contact maps without supervision.\nSeveral factors explain the disparity. Protein sequences have accumulated over 4 billion years of evolution across all domains of life, with each functional protein family represented by thousands of homologs. RNA databases are biased toward well-characterized structural families (tRNAs, rRNAs, ribozymes) with sparser coverage of regulatory ncRNAs and lineage-specific transcripts. The epitranscriptomic modifications that alter RNA function are invisible in sequence databases, unlike protein post-translational modifications that at least occur at predictable sequence motifs.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 15.3: [High] Scale comparison. Panel A (Scale Comparison Table): Training sequences (65M vs 23M, 3×); Structural diversity (all protein families vs mainly tRNA/rRNA); Parameters (15B vs ~100M, 150×); Emergent structure (contacts/folding vs limited). Panel B (Training Data Composition): Protein pie chart (diverse families) vs RNA (dominated by tRNA, rRNA, ribozymes). Panel C (Emergent Capabilities Comparison): Protein LMs (structure prediction ✓, variant effects ✓) vs RNA LMs (secondary partial ⚠, tertiary missing). Panel D (The Data Challenge): PDB proteins &gt;200,000 vs RNA &lt;2,000.\n\n\n\n\n\n15.5.2 Architectures and Objectives\nMost RNA foundation models follow the masked language modeling (MLM) paradigm established by BERT (Chapter 5). RNA-FM uses a transformer encoder with nucleotide-level tokenization, predicting masked bases from surrounding context. The learned embeddings show some correspondence to secondary structure when probed with downstream tasks, though the correspondence is weaker than the structure-function relationship learned by protein language models.\nAlternative architectures explore different design choices. Some models incorporate explicit structure tokens or operate on sequence-structure graphs, learning joint representations over both modalities. Others use codon-level tokenization for coding RNAs or explore state-space models and other efficient attention variants to handle longer sequences. RNAErnie and related models experiment with multi-task objectives that combine MLM with auxiliary predictions for structural features or family classification. [Citation Needed]\nThe field remains in active development, with no clear consensus on optimal architecture, tokenization, or training strategy. Unlike protein modeling, where ESM established a dominant paradigm that subsequent work has refined, RNA modeling still explores fundamental design choices.\n\n\n15.5.3 Downstream Applications\nRNA foundation model embeddings support various downstream tasks. Secondary structure prediction fine-tunes the model to output pairing probabilities or SHAPE reactivity profiles. RNA-protein binding prediction uses CLIP-seq data to predict interactions with RNA-binding proteins. Family classification assigns sequences to Rfam families or functional categories (tRNA, rRNA, miRNA, lncRNA). Expression and stability tasks predict transcript half-life or steady-state levels from UTR sequences.\nPerformance varies substantially across tasks. For structurally constrained RNAs like tRNAs and rRNAs, where sequence motifs strongly determine structure and function, foundation model embeddings provide useful features. For regulatory lncRNAs that often lack stable secondary structures and conserved motifs, improvement over baseline methods is more modest. The diversity of RNA types and tasks complicates benchmarking (Chapter 21), and models that excel on one task may struggle on others.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-codon",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-codon",
    "title": "15  RNA Structure and Function",
    "section": "15.6 Codon-Level Models for Coding RNA",
    "text": "15.6 Codon-Level Models for Coding RNA\nCoding sequences present a modeling opportunity that neither DNA nor protein foundation models fully exploit. The genetic code’s synonymous redundancy means that mRNA sequence carries information beyond amino acid identity: codon choice affects translation speed, mRNA stability, and co-translational folding. Codon-level foundation models tokenize mRNA into three-nucleotide units, learning representations that capture these codon-specific signals invisible to protein language models.\n\n15.6.1 Beyond Nucleotide Tokenization\nCoding sequences occupy a special niche where protein and nucleic acid constraints intersect. The genetic code assigns 61 sense codons to 20 amino acids, creating synonymous redundancy where multiple codons encode the same amino acid. This redundancy is not functionally neutral: synonymous codons differ in tRNA availability, translation speed, co-translational folding effects, and mRNA stability. Protein language models, which operate on amino acid sequences, cannot capture these codon-level signals.\nCodon-level foundation models address this gap by tokenizing mRNA into codons rather than nucleotides. Models like cdsFM, EnCodon, and DeCodon treat each three-nucleotide codon as a single token, training on masked codon prediction and related objectives (Naghipourfar et al. 2024). This tokenization encodes a biological prior: codons are the fundamental units of translation, and mutations at the codon level determine amino acid changes while mutations within synonymous codons affect expression without changing protein sequence.\nThe codon vocabulary contains 61 tokens (excluding stop codons) plus special tokens for noncoding regions and boundaries. This intermediate vocabulary size (between character-level nucleotide tokenization and typical BPE vocabularies of thousands of tokens) balances resolution with context length (Chapter 5). A 300-amino-acid protein corresponds to 900 nucleotides or 300 codons, making whole-gene modeling tractable within standard transformer context windows. The therapeutic implications of codon-level modeling are examined in Section 28.4, where these representations guide mRNA vaccine and protein replacement therapy design.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 15.4: [High] Codon-level modeling. Panel A (Same Protein, Different mRNA): Amino acid sequence; multiple mRNA sequences encoding same protein; highlight synonymous codon choices. Panel B (What Codon Choice Affects): tRNA availability, translation speed, mRNA structure, stability (GC content). Panel C (Tokenization Comparison): Character-level (900 tokens for 300 AA), codon-level (300 tokens); “Encodes biological prior.” Panel D (Model Capabilities): Protein LM sees amino acids only; DNA LM sees nucleotides but no codon boundaries; Codon LM sees both.\n\n\n\n\n\n15.6.2 What Codon Models Add\nCompared to protein language models, codon-level models enable direct modeling of mRNA design problems where amino acid sequence is fixed but codon choice is variable. They capture codon usage bias and its relationship to expression, model translation elongation dynamics that affect co-translational folding, and distinguish synonymous variants that are neutral at the protein level but affect mRNA properties.\nLife-Code extends this approach into a central-dogma-wide framework, linking DNA, RNA, and protein representations through shared or aligned embedding spaces (Liu et al. 2025). CodonBERT specifically targets mRNA design for vaccines and therapeutics, training on over 10 million mRNA sequences to learn representations that predict expression, stability, and immunogenicity (Li et al. 2023).\nCodon models typically ignore mRNA secondary structure and modifications. Local structure affects ribosome access and translation rate; modifications like m6A influence stability and localization. Combining codon-aware tokenization with structure-aware representations remains an open direction, less mature than the parallel integration of sequence and structure in protein modeling (Chapter 12).",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-utr",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-utr",
    "title": "15  RNA Structure and Function",
    "section": "15.7 UTR Models and Translation Regulation",
    "text": "15.7 UTR Models and Translation Regulation\nThe untranslated regions flanking a coding sequence determine how much protein an mRNA produces and how long the message survives in the cell. These regulatory effects operate through distinct mechanisms in the 5’ and 3’ UTRs, creating opportunities for both understanding endogenous regulation and engineering synthetic mRNAs with desired expression properties.\n\n15.7.1 Why UTRs Dominate Expression Control\nThe protein output of an mRNA depends as much on its untranslated regions as on its coding sequence. A transcript’s 5’ UTR determines whether ribosomes find and engage the start codon; its 3’ UTR controls how long the message survives and where in the cell it localizes. Two mRNAs encoding identical proteins can differ by orders of magnitude in expression if their UTRs differ. This regulatory leverage makes UTR modeling essential for both understanding endogenous gene regulation and designing synthetic mRNAs for therapeutic applications.\nThe 5’ UTR spans from the transcription start site to the start codon, typically 50 to 200 nucleotides in human mRNAs. Within this region, secondary structure can occlude the start codon and impede ribosome scanning, upstream open reading frames (uORFs) can capture ribosomes before they reach the main coding sequence, and internal ribosome entry sites (IRES) can enable cap-independent translation under stress conditions. The Kozak consensus sequence surrounding the start codon influences initiation efficiency, but context extending dozens of nucleotides in either direction modulates this effect. Predicting translation efficiency from 5’ UTR sequence requires integrating these overlapping signals.\nThe 3’ UTR extends from the stop codon to the poly-A tail, ranging from under 100 nucleotides to over 10 kilobases. This region harbors binding sites for RNA-binding proteins and microRNAs that collectively determine mRNA half-life, localization, and translational status. AU-rich elements (AREs) recruit decay machinery in response to cellular signals. Pumilio and other RNA-binding proteins recognize specific motifs to repress or activate translation. The density and arrangement of miRNA binding sites create combinatorial regulatory logic that varies across cell types depending on which miRNAs are expressed.\n\n\n15.7.2 Sequence-to-Expression Models\nHigh-throughput reporter assays have enabled systematic modeling of UTR function. Massively parallel reporter assays (MPRAs) measure expression driven by thousands of UTR variants in a single experiment, providing training data at scales previously unavailable. Sample et al. used such data to train Optimus 5-Prime, a convolutional model that predicts ribosome load from 5’ UTR sequence with accuracy sufficient to guide synthetic UTR design (Sample et al. 2019). The model learned interpretable features corresponding to known regulatory elements (uORF presence, Kozak strength, secondary structure) while also capturing context-dependent interactions invisible to element-counting approaches.\nFor 3’ UTRs, models must contend with greater length and combinatorial complexity. A 2-kilobase 3’ UTR may contain dozens of potential regulatory sites whose effects depend on spacing, secondary structure context, and the expression levels of cognate binding proteins. Approaches range from motif-based models that score individual elements and sum contributions, to deep learning architectures that process entire UTR sequences and learn nonlinear interactions. Agarwal and Kelley trained models on endogenous mRNA stability measurements, demonstrating that 3’ UTR sequence features explain substantial variance in half-life across the transcriptome (Agarwal and Shendure 2020).\nTransfer learning from RNA foundation models offers a complementary approach. Rather than training UTR-specific models from scratch, pretrained representations from RNA-FM or similar models can be fine-tuned on expression prediction tasks (Chapter 5). The pretrained embeddings encode sequence context and potential structural features that may transfer to UTR function prediction, though systematic comparisons between foundation model transfer and task-specific training remain limited.\n\n\n15.7.3 Integration with mRNA Design\nUTR optimization represents a distinct component of therapeutic mRNA design, complementing codon optimization. For a vaccine or protein replacement therapy, the coding sequence determines what protein is made while the UTRs determine how much protein is made and for how long. Current mRNA therapeutics typically use UTRs borrowed from highly expressed endogenous genes (human alpha-globin and beta-globin UTRs are common choices) rather than computationally optimized sequences. [Citation Needed]\nModel-guided UTR design could improve on this approach by optimizing for specific objectives: maximizing expression in target tissues, extending mRNA half-life to reduce dosing frequency, or minimizing immunogenicity by avoiding sequences that trigger innate immune sensors. The challenge lies in the combinatorial interaction between UTRs and coding sequence. Secondary structures can span the UTR-CDS boundary, and the optimal 5’ UTR for one coding sequence may perform poorly for another. Integrated models that jointly optimize UTRs and coding sequence represent an active research direction, though experimental validation of computationally designed UTRs remains limited compared to the extensive optimization of coding sequences. The design principles and optimization strategies for therapeutic mRNAs, including COVID-19 vaccine development, are detailed in Section 28.4.2.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-mrna-design",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-mrna-design",
    "title": "15  RNA Structure and Function",
    "section": "15.8 mRNA Design and Optimization",
    "text": "15.8 mRNA Design and Optimization\nTherapeutic mRNA design requires navigating a vast sequence space where multiple objectives compete. Expression, stability, immunogenicity, and manufacturability all depend on sequence choices that interact in complex ways. The COVID-19 vaccines demonstrated that rational mRNA design can achieve clinical efficacy, while also revealing how much of current practice remains empirical rather than model-driven.\n\n15.8.1 Design Objectives and Trade-offs\nmRNA sequence design selects nucleotide sequences that encode a desired protein while optimizing expression, stability, safety, and manufacturability. For a 300-amino-acid protein, there are approximately 3^300 possible synonymous mRNA sequences (roughly the number of synonymous codons raised to the protein length). This astronomical space cannot be exhaustively searched, motivating both classical heuristics and modern machine learning approaches.\nKey objectives include high protein expression in target tissues, mRNA stability during manufacturing and in vivo, controlled translation kinetics that influence co-translational folding, and low immunogenicity for therapeutic applications. These objectives often conflict: increasing GC content may improve stability but introduce unwanted secondary structure, while avoiding rare codons may reduce expression if tRNA pools are limiting.\n\n\n15.8.2 Lessons from COVID-19 Vaccines\nThe COVID-19 mRNA vaccines provided a high-profile demonstration of mRNA design principles at unprecedented scale. The Pfizer-BioNTech and Moderna vaccines incorporated several design elements: N1-methylpseudouridine modification throughout the sequence to reduce innate immune activation, codon optimization to enhance expression in human cells, optimized 5’ and 3’ UTRs from highly expressed genes, and sequence modifications to stabilize the prefusion spike conformation. These choices drew on decades of basic research but were refined through empirical optimization rather than systematic model-based design. [Citation Needed]\nThe vaccines’ success demonstrated that rationally designed mRNAs can achieve therapeutic efficacy at scale. It also revealed limitations in current understanding: the optimal combination of modifications, codons, and UTRs for a given protein target remains partly empirical, and transferring designs across proteins or therapeutic applications requires substantial optimization.\n\n\n15.8.3 Model-Based Design Strategies\nRNA and codon foundation models enable several approaches to systematic design. Scoring and screening use pretrained models to evaluate large candidate sets for predicted expression or stability, selecting top designs for experimental validation. When models are differentiable with respect to input embeddings, gradient-based methods can guide sequence optimization toward desired objectives. Generative approaches sample diverse high-scoring sequences subject to constraints like fixed amino acid sequence or avoided motifs.\nEmpirical results suggest that deep models trained on high-throughput reporter assays or ribosome profiling can outperform classical codon adaptation indices like CAI or tAI, particularly for context-specific expression prediction. Classical indices rely on genome-wide codon frequencies that may not reflect the relevant cellular context, while deep models can learn local effects of codon pairs, mRNA structure, and regulatory elements. These models require substantial training data and may not generalize across organisms or synthetic constructs far from natural sequences.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 15.5: [High] End-to-end pipeline. Stage 1 (Target Protein): Desired sequence and structural requirements. Stage 2 (Codon Optimization): ~3^300 possibilities; objectives (expression, stability, immunogenicity); model-based scoring. Stage 3 (5’ UTR Design): Translation initiation, Kozak, secondary structure. Stage 4 (3’ UTR Design): Stability elements, miRNA avoidance, poly-A tail. Stage 5 (Modification Selection): N1-methylpseudouridine. Output: Optimized construct. Inset: COVID-19 vaccine design choices.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-ncrna",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-ncrna",
    "title": "15  RNA Structure and Function",
    "section": "15.9 Noncoding RNA Classification and Function",
    "text": "15.9 Noncoding RNA Classification and Function\nRNA that does not encode protein encompasses an extraordinary range of structures, functions, and regulatory mechanisms. Classifying these transcripts and predicting their functions presents challenges that differ from coding sequence analysis: the relevant features vary across RNA classes, functional annotations remain incomplete, and the boundary between functional ncRNA and transcriptional noise is often unclear.\n\n15.9.1 Diversity of Noncoding RNA\nRNA that does not encode protein spans an enormous functional and structural range. Housekeeping RNAs (tRNAs, rRNAs, snRNAs, snoRNAs) perform essential cellular functions with well-characterized structures. Regulatory RNAs (miRNAs, siRNAs, piRNAs, lncRNAs) control gene expression through diverse mechanisms. Structural and catalytic RNAs (ribozymes, riboswitches) adopt complex folds that enable enzymatic activity or ligand sensing. Circular RNAs (circRNAs) and other noncanonical species continue to expand the catalog of RNA diversity.\nEach class has characteristic lengths, structural motifs, genomic contexts, and functional mechanisms. tRNAs are approximately 76 nucleotides with a conserved cloverleaf structure. miRNAs are approximately 22 nucleotides processed from longer hairpin precursors. lncRNAs span thousands of nucleotides with poorly conserved sequence and often no stable secondary structure. Unifying these classes under a single modeling framework is challenging, and models that excel on one class may fail on others.\n\n\n15.9.2 From Handcrafted Features to Learned Representations\nClassical ncRNA classification relied on engineered features: k-mer frequencies, GC content, minimum free energy of predicted secondary structure, structural motif counts, and genomic context features like proximity to coding genes or chromatin marks. These features fed conventional classifiers (SVMs, random forests, shallow neural networks) that achieved reasonable performance for well-studied classes with strong sequence and structure signatures.\nThe limits of handcrafted features emerge most clearly for lncRNAs. These transcripts are defined partly by what they lack (no long open reading frame) rather than what they possess. Many lncRNAs show poor conservation, lack stable secondary structures, and have diverse, poorly characterized functions. Distinguishing functional lncRNAs from transcriptional noise remains difficult, and classical feature sets often collapse to generic statistics like length and GC content.\nFoundation model embeddings offer a more flexible approach. Per-nucleotide representations can be pooled into fixed-dimensional vectors that support classification with simple downstream heads. For ncRNAs without strong sequence motifs, the pretrained embeddings may capture subtle distributional patterns learned during self-supervised training. Few-shot learning becomes possible: given a handful of newly characterized RNAs, their embeddings can seed new clusters in representation space, guiding annotation of related sequences.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-mirna",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-mirna",
    "title": "15  RNA Structure and Function",
    "section": "15.10 miRNA Target Prediction",
    "text": "15.10 miRNA Target Prediction\nMicroRNAs regulate gene expression by guiding the RNA-induced silencing complex (RISC) to complementary sites in target mRNAs, typically in the 3’ UTR. A single miRNA can regulate hundreds of transcripts, and a single transcript can harbor binding sites for dozens of miRNAs. This regulatory network influences virtually every cellular process, and dysregulation of miRNA-target interactions contributes to cancer, cardiovascular disease, and neurodegeneration. Predicting which transcripts a given miRNA targets (and vice versa) has been a persistent computational challenge since the discovery of miRNA-mediated regulation.\nThe dominant paradigm centers on seed complementarity. Nucleotides 2 through 7 of the miRNA (the seed region) typically form perfect Watson-Crick pairs with target sites, while the remaining nucleotides contribute variably to binding affinity and regulatory effect. Classical algorithms like TargetScan identify conserved seed matches in 3’ UTRs and rank targets by evolutionary conservation, site type (8mer, 7mer-m8, 7mer-A1), and local sequence context (Agarwal and Shendure 2020). Additional features including AU content flanking the site, position within the UTR, and proximity to other miRNA sites improve prediction accuracy.\nDespite decades of refinement, target prediction remains noisy. Experimental validation rates for top predictions rarely exceed 50%, and many functional targets lack canonical seed matches. [Citation Needed] The disconnect arises partly from context dependence: a site may be accessible in one cell type but occluded by RNA structure or competing protein binding in another. It arises partly from the limitations of reporter assays that measure binding in artificial contexts rather than endogenous regulatory effects. And it arises from the biology itself, where weak individual sites combine additively and miRNA-target interactions are probabilistic rather than deterministic.\nDeep learning approaches attempt to improve on seed-based methods by learning complex sequence features from high-throughput binding data. Models trained on CLIP-seq experiments (which crosslink miRNA-target complexes and identify bound sites transcriptome-wide) can capture non-canonical binding modes and context effects invisible to seed-matching algorithms. These models often overfit to cell-type-specific binding patterns and generalize poorly across contexts (Chapter 22). The fundamental challenge is that miRNA targeting depends on factors beyond sequence: miRNA and target abundance, competition among targets for limiting RISC, and cellular state variables that no sequence-based model can capture.\nFor clinical applications, target prediction informs both the mechanism of disease-associated miRNAs and the design of therapeutic interventions. AntimiR oligonucleotides that sequester specific miRNAs have entered clinical trials for hepatitis C (targeting miR-122) and other indications. Predicting off-target effects of such therapeutics requires understanding the full network of targets that will be derepressed when a miRNA is inhibited. Similarly, miRNA mimics designed to replace lost tumor-suppressor miRNAs must be evaluated for potential regulation of unintended targets. In both cases, computational target prediction provides a starting point that experimental validation must refine.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-splicing",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-splicing",
    "title": "15  RNA Structure and Function",
    "section": "15.11 Splicing and Transcript Processing Models",
    "text": "15.11 Splicing and Transcript Processing Models\nSplicing models predict how pre-mRNA is processed into mature transcripts, a problem intimately connected to RNA biology even when the models operate on genomic DNA sequence. SpliceAI established the paradigm, but extensions address tissue specificity, branchpoint prediction, and quantitative splicing outcomes that the original model does not capture.\n\n15.11.1 Beyond SpliceAI\nSpliceAI demonstrated that deep convolutional networks could predict splice sites with near-spliceosomal precision (Section 6.5). The model’s success in identifying cryptic splice variants has made it a standard tool in clinical variant interpretation (Chapter 14). Splicing involves more than splice site recognition, and several extensions address aspects that SpliceAI does not fully capture.\nTissue-specific splicing patterns vary substantially across cell types and developmental stages. A splice site may be used in brain but skipped in liver due to differential expression of splicing factors. Models like Pangolin extend splice prediction by training on tissue-specific RNA-seq data, learning to predict not just whether a site is splice-competent but whether it is used in specific cellular contexts. [Citation Needed] These models enable variant interpretation that accounts for tissue-relevant splicing patterns rather than generic predictions. The integration of tissue-specific splice predictions into clinical variant interpretation workflows is addressed in Chapter 26.\nBranchpoint prediction identifies the adenosine residue where the lariat intermediate forms during splicing. While SpliceAI focuses on donor and acceptor sites, branchpoint recognition involves distinct sequence features (typically a degenerate YURAY motif 18-40 nucleotides upstream of the acceptor) that specialized models can capture. Combined analysis of donor, acceptor, and branchpoint predictions provides more complete characterization of splice-altering variants.\nAlternative splicing prediction moves beyond binary splice site identification to model exon inclusion rates and isoform usage. Models in this space attempt to predict not just whether an exon can be included but quantitative measures of inclusion across conditions, enabling analysis of splicing quantitative trait loci (sQTLs) and their effects on transcript diversity.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-limitations",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-limitations",
    "title": "15  RNA Structure and Function",
    "section": "15.12 Limitations and Open Challenges",
    "text": "15.12 Limitations and Open Challenges\nRNA modeling faces constraints that do not apply to protein or DNA foundation models. Data scarcity limits what can be learned from self-supervised training, functional annotations remain incomplete for most ncRNA classes, and the field has not yet achieved the breakthrough moment that AlphaFold represented for proteins. These limitations define the current frontier and point toward the advances needed for RNA foundation models to mature.\n\n15.12.1 Sparse Structural Data\nThe fundamental limitation of RNA modeling is data scarcity. Protein structure prediction benefits from over 200,000 experimentally determined structures; RNA has fewer than 2,000, heavily biased toward ribosomal RNA and tRNA. [Citation Needed] This scarcity limits supervised learning for tertiary structure prediction and constrains the emergence of structural knowledge from self-supervised pretraining. Until high-throughput methods generate RNA structures at scale comparable to protein crystallography and cryo-EM, RNA tertiary structure prediction will remain a frontier problem rather than a solved one.\nSecondary structure data is more abundant but still limited. Experimentally validated structures cover mainly well-characterized families, while computational predictions for novel sequences rely on thermodynamic models whose accuracy degrades for long RNAs and complex folds. Structure probing experiments provide genome-wide coverage but measure accessibility rather than pairing directly, requiring inference to convert reactivity profiles into structural models.\n\n\n15.12.2 Functional Annotation Gaps\nFor many ncRNA classes, function remains poorly characterized. LncRNA annotations often specify only genomic location and expression pattern without mechanistic understanding. Circular RNA functions are emerging but incompletely cataloged. Even for better-characterized classes like miRNAs, target prediction remains noisy and context-dependent.\nThis annotation gap limits supervised learning for function prediction and complicates evaluation (Chapter 21). When ground truth is uncertain, it becomes difficult to assess whether a model’s predictions reflect genuine biological insight or artifacts of incomplete training data. The field needs both experimental advances to characterize ncRNA function and computational approaches that can learn from weak or partial supervision.\n\n\n15.12.3 Maturity Gap\nRNA foundation models exist but have not achieved the transformative impact of protein language models. ESM-2 enabled ESMFold, providing structure prediction from single sequences that nearly matches AlphaFold. No comparable RNA breakthrough has occurred. The reasons include data scarcity, the conformational complexity of RNA, and the diversity of RNA classes that makes unified modeling difficult.\nThis maturity gap represents both a limitation and an opportunity. The techniques that succeeded for proteins (large-scale self-supervised learning, attention mechanisms, scaling laws) provide a roadmap (Chapter 11). Applying that roadmap to RNA requires addressing the data challenge through structure probing, synthetic data generation, or more efficient use of limited experimental structures. It requires architectural innovations that handle RNA’s long-range base pairing and conformational flexibility. It requires benchmarks and evaluation frameworks that cover the full diversity of RNA types and tasks, following the rigorous evaluation principles established in Chapter 21 and the benchmark construction guidelines in Chapter 20.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch15-rna.html#sec-ch15-bridge",
    "href": "part_4/p4-ch15-rna.html#sec-ch15-bridge",
    "title": "15  RNA Structure and Function",
    "section": "15.13 Bridge Between Sequence and Cell",
    "text": "15.13 Bridge Between Sequence and Cell\nRNA occupies a distinctive position in genomic AI, bridging the sequence-level models of Part III with the cellular perspectives that follow. Splicing models like SpliceAI operate on pre-mRNA and predict transcript processing outcomes (Section 6.5). Codon-level models capture translation dynamics invisible to protein language models. mRNA therapeutic design demonstrates practical value through codon optimization, UTR engineering, and stability prediction. These applications proceed despite the absence of the structure prediction breakthrough that transformed protein modeling; secondary structure prediction has advanced through deep learning, but tertiary structure accuracy lags protein structure by a wide margin.\nThe relationship between RNA models and other modalities reflects RNA’s position in the central dogma. RNA is the product of transcription that regulatory models predict (Chapter 13), the substrate for translation that protein models assume (Chapter 12), and the primary measurement that single-cell models use to represent cellular state (Chapter 19). Foundation models that learn from RNA sequence capture patterns distinct from those in DNA or protein: codon usage biases, secondary structure constraints, and post-transcriptional regulatory elements that neither genomic nor protein models directly represent.\nBeyond sequence, biological understanding requires cellular and tissue context. Single-cell models treat RNA expression as the primary readout of cellular state, learning representations that capture cell type identity and perturbation response (Chapter 19). Three-dimensional genome models add spatial context that influences transcription. Network models integrate gene relationships that transcend individual sequences. RNA models provide sequence-level representations that feed into these higher-level frameworks, completing the molecular arc from DNA through RNA to protein while opening the path to systems-level integration.\n\n\n\n\nAgarwal, Vikram, and Jay Shendure. 2020. “Predicting mRNA Abundance Directly from Genomic Sequence Using Deep Convolutional Neural Networks.” Cell Reports 31 (7): 107663. https://doi.org/10.1016/j.celrep.2020.107663.\n\n\nChen, Jiayang, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze Yu, Licheng Zong, et al. 2022. “[RNA-FM] Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions.” arXiv. https://doi.org/10.48550/arXiv.2204.00300.\n\n\nLi, Sizhen, Saeed Moayedpour, Ruijiang Li, Michael Bailey, Saleh Riahi, Milad Miladi, Jacob Miner, et al. 2023. “CodonBERT: Large Language Models for mRNA Design and Optimization.” bioRxiv. https://doi.org/10.1101/2023.09.09.556981.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nNaghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B. Macdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow Coyote-Maestas, and Hani Goodarzi. 2024. “[cdsFM - EnCodon/DeCodon] A Suite of Foundation Models Captures the Contextual Interplay Between Codons.” bioRxiv. https://doi.org/10.1101/2024.10.10.617568.\n\n\nSample, Paul J., Ban Wang, David W. Reid, Vlad Presnyak, Iain J. McFadyen, David R. Morris, and Georg Seelig. 2019. “Human 5′ UTR Design and Variant Effect Prediction from a Massively Parallel Translation Assay.” Nature Biotechnology 37 (7): 803–9. https://doi.org/10.1038/s41587-019-0164-5.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch16-single-cell.html",
    "href": "part_4/p4-ch16-single-cell.html",
    "title": "16  Single-Cell Models",
    "section": "",
    "text": "16.1 Single-Cell Data Landscape\nThe foundation models in Part III operate on sequence: DNA nucleotides, amino acids, RNA bases. They learn what the genome encodes, what proteins it produces, how regulatory elements respond to transcription factors. Yet sequence alone cannot explain why a neuron and a hepatocyte, carrying identical genomes, perform utterly different functions. The answer lies not in sequence but in state: which genes are active, which regulatory elements are accessible, which epigenetic marks are present. A neuron expresses synaptic genes and silences metabolic pathways; a hepatocyte does the reverse. The genome is the same; the cellular interpretation differs. Capturing this interpretation at single-cell resolution has become possible only in the past decade, and the resulting data now approach the scale that enabled language models in text.\nSingle-cell technologies decompose cellular mixtures that bulk assays average over. A tumor biopsy contains malignant cells, immune infiltrates, stromal fibroblasts, and endothelial cells in varying proportions. Bulk RNA sequencing (RNA-seq) reports average expression across this mixture, potentially masking the drug-resistant subpopulation that will cause relapse. Single-cell RNA-seq profiles each cell individually, revealing which cells express which genes and how composition shifts during disease progression. The challenge is that single-cell data are sparse, with most genes showing zero counts in most cells; noisy, as technical dropout obscures biological signal; and high-dimensional, spanning tens of thousands of features across millions of cells. Traditional analysis methods struggle with this combination; foundation models offer a path through.\nCellular language models treat gene expression profiles as documents and learn the grammar of which genes co-occur in different cellular contexts. Epigenomic models capture regulatory state encoded in DNA methylation and chromatin accessibility. Integration methods align cells across modalities when different assays are performed on different cells from the same tissue. Throughout, the central question remains: can models trained on cellular state representations learn regulatory logic that generalizes across tissues, conditions, and species? The answer determines whether single-cell foundation models can achieve the transfer learning successes that protein and DNA models have demonstrated (see Chapter 9).\nUnderstanding single-cell foundation models requires understanding the data they learn from. Single-cell technologies produce measurements fundamentally different from bulk assays: sparse, noisy, and high-dimensional, yet rich with information about cellular heterogeneity that bulk measurements obscure. The transition from bulk to single-cell resolution created new analytical challenges and new opportunities, while technical artifacts impose constraints that shape how foundation models must be designed.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch16-single-cell.html#sec-ch16-data",
    "href": "part_4/p4-ch16-single-cell.html#sec-ch16-data",
    "title": "16  Single-Cell Models",
    "section": "",
    "text": "16.1.1 From Bulk to Single-Cell Resolution\nTraditional transcriptomic studies measure gene expression in bulk tissue, producing a single measurement per gene that represents the average across thousands to millions of cells. This averaging is both a strength and a limitation. It provides robust, reproducible measurements that have powered decades of biological discovery. It also fundamentally limits what questions can be asked. If a gene appears moderately expressed in bulk, is it uniformly expressed across all cells, or highly expressed in a rare subpopulation while silent elsewhere? Bulk data cannot distinguish these scenarios.\nSingle-cell RNA sequencing (scRNA-seq) resolves this ambiguity by measuring gene expression in individual cells. The technology has evolved rapidly since its introduction in 2009 [Citation Needed]. Early methods captured hundreds of cells per experiment; current platforms routinely profile hundreds of thousands of cells, with some studies exceeding a million [Citation Needed]. Public repositories now contain tens of millions of single-cell transcriptomes spanning diverse tissues, developmental stages, disease states, and species. This scale approaches the data volumes that enabled large language models in natural language processing.\nThe analogy between cells and documents runs deeper than dataset size. In language, words combine according to grammatical rules to form sentences that convey meaning. In cells, genes combine according to regulatory programs to form expression profiles that define cellular identity. A hepatocyte expresses genes for drug metabolism, albumin synthesis, and bile production; a neuron expresses genes for synaptic transmission, ion channels, and neurotransmitter receptors. These expression programs are not random: transcription factors activate coherent sets of target genes, signaling pathways coordinate cellular responses, and developmental programs establish cell type identities through cascades of regulatory events. Just as language models learn syntax and semantics by predicting masked words (see Chapter 8), single-cell foundation models might learn regulatory logic by predicting masked genes.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 16.1: [Essential] Visual analogy. Panel A (Language Model): Sentence “The cat sat on the mat”; words as tokens; grammar rules; BERT predicts masked words. Panel B (Cellular Language Model): Cell expression profile as “sentence”; genes as tokens (~20,000 vocabulary); regulatory programs as “grammar”; Geneformer/scGPT predict masked genes. Panel C (Parallel Structure Table): NLP ↔︎ Single-Cell mapping. Panel D (What Models Learn): Language (syntax, semantics) ↔︎ Cellular (co-expression modules, cell type programs, perturbation effects).\n\n\n\n\n\n16.1.2 Technical Challenges and Data Characteristics\nSingle-cell data present distinctive challenges that shape how foundation models must be designed. Dropout is pervasive: due to inefficiencies in RNA capture and amplification, many genes that are actually expressed in a cell register as zero in the measurement. A gene with true expression may appear as zero in 50% to 90% of cells where it is actually transcribed [Citation Needed]. This zero-inflation means that absence of signal is not absence of expression.\nSparsity compounds the interpretation challenge. A typical single-cell transcriptome measures 20,000 genes, but any individual cell might have detectable expression for only 1,000 to 5,000 of them. The resulting data matrices are more than 90% zeros, requiring specialized computational approaches.\nBatch effects arise because technical variation between experiments often exceeds biological variation within them. Cells processed on different days, by different operators, or with different reagent lots may cluster by batch rather than by biological type. A model that learns batch-specific patterns rather than biological ones will fail to generalize. This challenge parallels the confounding issues examined in Chapter 22, where technical artifacts can masquerade as biological signal.\nDynamic range spans orders of magnitude, from highly expressed housekeeping genes to rare transcription factors present at a few copies per cell. Normalizing across this range while preserving biologically meaningful variation requires careful preprocessing choices that can affect downstream results.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 16.2: [Essential] Data challenges. Panel A (Dropout/Sparsity): Gene × Cell matrix mostly zero (90%+); “Zero can mean silent OR undetected.” Panel B (Batch Effects): Uniform Manifold Approximation and Projection (UMAP) colored by cell type (mixed) vs batch (clustered); “Technical variation can exceed biological.” Panel C (Dynamic Range): Histogram spanning orders of magnitude; few copies for some genes. Panel D (How Foundation Models Address): Rank-based encoding handles range; large corpus learns robust patterns; contrastive objectives for batch-invariance.\n\n\n\nDespite these challenges, the scale of available data creates opportunities. Tens of millions of cells, spanning hundreds of cell types across dozens of tissues and multiple species, provide training corpora large enough to learn general representations. The question is whether foundation model architectures can extract biological signal from noisy, sparse, high-dimensional measurements.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch16-single-cell.html#sec-ch16-clm",
    "href": "part_4/p4-ch16-single-cell.html#sec-ch16-clm",
    "title": "16  Single-Cell Models",
    "section": "16.2 Cellular Language Models",
    "text": "16.2 Cellular Language Models\nThe analogy between gene expression and language has proven remarkably productive. If cells are sentences and genes are words, then cellular regulatory programs are grammar: the rules governing which genes appear together in which contexts. Several foundation models now operationalize this analogy, treating single-cell transcriptomes as documents and learning to predict masked genes from expression context. These models differ in their tokenization strategies, pretraining objectives, and architectural choices, but share a common premise: that self-supervised learning on millions of cells can capture regulatory logic that transfers to diverse downstream tasks.\n\n16.2.1 Geneformer: Learning Network Biology\nGeneformer exemplifies the cellular language model approach, treating each cell as a sentence where genes serve as tokens (Theodoris et al. 2023). The model was pretrained on approximately 30 million single-cell transcriptomes to learn context-aware representations that capture how genes function within cellular regulatory networks. The key insight was that during pretraining, the model gained understanding of network dynamics in a completely self-supervised manner, encoding network hierarchy in its attention weights without explicit supervision on network structure.\nRather than using raw expression counts, Geneformer employs rank-based encoding that emphasizes relative expression. For each cell, genes are ranked by their expression level compared to their typical expression across the training corpus. This transformation highlights which genes are unusually active or silent in each cellular context. A gene ranked highly in a given cell is one whose expression deviates from its baseline, potentially indicating context-specific regulatory activation. The representation discards absolute counts, which vary with sequencing depth and capture efficiency, while preserving the relative ordering that reflects cellular state. This tokenization strategy differs fundamentally from the nucleotide-level approaches used in DNA language models (see Chapter 5).\nPretraining uses a masked gene prediction objective analogous to BERT-style language modeling (see Chapter 8). A fraction of genes are masked in each cell, and the model learns to predict which genes were masked based on the remaining expression context. This forces the model to learn co-expression patterns: which genes tend to appear together at high ranks in the same cells, and which genes predict each other’s presence. The objective implicitly captures regulatory modules, signaling pathways, and cell-type-specific programs.\nAfter pretraining, Geneformer supports diverse downstream applications through fine-tuning or feature extraction. Cell type annotation achieves high accuracy even with limited labeled examples, leveraging general biological knowledge acquired during pretraining. The model identified candidate therapeutic targets for cardiomyopathy by analyzing how disease-associated genes fit within learned network structure, demonstrating potential for accelerating discovery in rare diseases where large disease-specific datasets are unavailable (Theodoris et al. 2023).\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 16.3: [High] Geneformer innovations. Panel A (Rank-Based Encoding): Raw expression → ranks relative to corpus baseline → tokens; “Highlights unusually active/silent genes.” Panel B (Architecture): Ranked gene sequence → transformer encoder (BERT-style) → masked gene prediction → gene/cell embeddings. Panel C (What Attention Learns): Network diagram showing gene-gene attention; correspondence to regulatory relationships; “Network hierarchy emerges without supervision.” Panel D (Transfer Applications): Cell type annotation, therapeutic target ID, disease gene prioritization.\n\n\n\n\n\n16.2.2 scGPT: Generative Pretraining for Single-Cell Analysis\nscGPT extends the foundation model paradigm with a generative architecture trained on over 33 million cells (Cui et al. 2024). The model functions as a generalist backbone for single-cell analysis pipelines, supporting applications from cell type annotation to perturbation response prediction within a unified framework.\nThe architecture incorporates several innovations tailored to single-cell data characteristics. Gene tokens combine learnable embeddings with position encodings that can capture genomic location when relevant. Expression values are discretized into bins to handle the wide dynamic range and zero-inflation characteristic of single-cell measurements; rather than predicting continuous values, the model predicts which expression bin a gene falls into. Special tokens mark cell boundaries and indicate modality when multi-omic data are available.\nscGPT uses multiple pretraining objectives simultaneously. Masked gene prediction encourages learning of co-expression patterns, similar to Geneformer. Autoregressive generation predicts expression of one set of genes conditioned on others, enabling the model to generate synthetic expression profiles or impute missing values. Contrastive objectives push cells from the same type to cluster in embedding space while separating different types, providing discriminative signal that complements the generative objectives. This combination of pretraining objectives parallels the hybrid strategies explored in DNA and protein language models (see Chapter 8).\nThe combination of objectives enables scGPT to excel across multiple applications. Cell type annotation benefits from rich pretrained representations, including identification of fine-grained subtypes that might be missed by simpler methods. Multi-batch integration aligns cells from different experiments while preserving genuine biological variation, addressing the pervasive batch effect problem. Perturbation response prediction anticipates how cells will respond to genetic knockouts or drug treatments, providing a foundation for in silico experimentation.\n\n\n16.2.3 scFoundation and Scaling Single-Cell Models\nscFoundation pushes the scale of single-cell foundation models further, training on over 50 million cells with an architecture designed for both representation learning and generation (Hao et al. 2024). The model explores how scaling laws observed in language models translate to cellular data, finding that larger models trained on more diverse data produce embeddings that transfer better across tasks and contexts. This scaling behavior mirrors patterns observed in DNA language models (see Chapter 10).\nThe pretraining corpus spans diverse tissues, developmental stages, and disease states, including both human and mouse data. This diversity proves essential: models trained on narrow datasets (a single tissue or condition) learn representations that capture that specific context but fail to generalize. Models trained on diverse corpora learn more abstract representations of cellular state that transfer across biological contexts.\nscFoundation emphasizes the importance of tokenization and normalization choices for downstream performance. The model systematically compared different approaches to handling zero-inflation, normalization across sequencing depth, and gene vocabulary selection. These preprocessing decisions, often treated as implementation details, significantly affect what biological signals the model can capture. The parallels to tokenization debates in DNA language models (see Chapter 5) are striking: representation choices made before training constrain what patterns can be learned.\n\n\n16.2.4 TranscriptFormer: Cross-Species Cellular Models\nTranscriptFormer extends single-cell foundation models across evolutionary time, training on over 112 million cells spanning 1.5 billion years of evolution across 12 species (Pearce et al. 2025). This cross-species approach tests whether regulatory principles learned from one organism generalize to others.\nThe model uses a novel autoregressive architecture that jointly predicts genes and their expression levels. Rather than treating gene identity and expression as separate prediction problems, TranscriptFormer generates them together, enabling it to produce synthetic cells conditioned on prompts specifying species, tissue, or cell type. Because the vocabulary spans multiple species with ortholog mappings, the model can transfer cell type annotations across evolutionary distances.\nIn zero-shot settings, TranscriptFormer demonstrates strong performance on both in-distribution and out-of-distribution cell type classification. Remarkably, models trained predominantly on mouse and human data can annotate cell types in zebrafish and other species separated by hundreds of millions of years of evolution. This cross-species transfer reveals that core principles of cellular regulation are deeply conserved, and that foundation models can capture these conserved principles when trained on evolutionarily diverse data. The success of cross-species transfer in cellular models parallels similar findings in protein language models, where evolutionary conservation provides a powerful inductive bias (see Chapter 12).",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch16-single-cell.html#sec-ch16-perturbation",
    "href": "part_4/p4-ch16-single-cell.html#sec-ch16-perturbation",
    "title": "16  Single-Cell Models",
    "section": "16.3 Perturbation Response Prediction",
    "text": "16.3 Perturbation Response Prediction\nThe ultimate test of whether cellular foundation models understand regulatory biology is prediction: can they anticipate how cells will respond to interventions they have never seen? Perturbation prediction moves beyond pattern recognition toward mechanistic understanding. If a model has learned the causal structure of gene regulatory networks, it should predict the downstream consequences of knocking out a transcription factor or activating a signaling pathway. This capability would transform drug discovery and target identification (Chapter 27), enabling in silico screening of perturbations before expensive wet-lab validation. The design-build-test-learn cycles that could exploit such predictions are examined in Section 28.6. Achieving this capability requires models to distinguish causation from correlation in observational data.\n\n16.3.1 In Silico Experiment Promise\nOne of the most compelling applications of cellular foundation models is predicting how cells will respond to perturbations. If a model has learned regulatory logic from expression data, it should be able to anticipate the transcriptional consequences of knocking out a gene, activating a pathway, or treating with a drug. Such predictions could accelerate drug discovery by prioritizing candidates before expensive wet-lab validation, identify synthetic lethal interactions for cancer therapy, and suggest targets for diseases without known interventions.\nThe perturbation prediction task requires more than memorizing co-expression patterns. The model must understand directional relationships: if gene A activates gene B, then knocking out A should reduce B’s expression. It must capture network effects: perturbations propagate through regulatory cascades, producing secondary and tertiary effects beyond direct targets. It must recognize context dependence: the same perturbation may have different effects in different cell types or states.\n\n\n16.3.2 Perturb-seq and Foundation Model Training\nPerturb-seq combines CRISPR-based genetic perturbations with single-cell RNA sequencing, measuring the transcriptional consequences of gene knockouts across thousands of cells (Dixit et al. 2016). These functional screens complement the deep mutational scanning approaches covered in Section 2.4.4, providing cellular rather than molecular readouts of perturbation effects. These datasets provide supervised signal for perturbation prediction: given the pre-perturbation state and the identity of the perturbed gene, predict the post-perturbation expression profile.\nFoundation models approach this task through transfer learning (see Chapter 9). A model pretrained on tens of millions of unperturbed cells learns general representations of cellular state and gene-gene relationships. Fine-tuning on Perturb-seq data teaches the model to map these representations to perturbation outcomes. The hope is that general biological knowledge from pretraining will enable accurate predictions for perturbations not seen during fine-tuning, including knockouts of genes never directly perturbed in training data.\nscGPT and Geneformer both demonstrate perturbation prediction capabilities, though performance varies across perturbation types and cellular contexts. Predictions are most accurate for well-characterized genes with many training examples and clear regulatory relationships. Performance degrades for poorly characterized genes, complex combinatorial perturbations, and cell types underrepresented in training data.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 16.4: [High] Perturbation response. Panel A (The Task): Unperturbed cell + perturbation identity → predicted post-perturbation profile → comparison to Perturb-seq. Panel B (Training Data): CRISPR knockout library + single-cell readout; thousands of genes × cells; “Supervised signal.” Panel C (What Models Must Learn): Directional relationships (A activates B → KO A reduces B); network cascades; context dependence. Panel D (Current Performance): Strong for well-characterized genes; weak for poorly characterized; “Predictions most accurate where we need them least.”\n\n\n\n\n\n16.3.3 Limitations of Current Approaches\nDespite promising results, current perturbation prediction models face fundamental limitations. Most training data come from immortalized cell lines that may not reflect primary tissue biology. Perturbations are typically single-gene knockouts; combinatorial perturbations involving multiple genes remain challenging. The models predict average responses across perturbed cells rather than the heterogeneity of individual responses.\nMore fundamentally, correlation-based learning from expression data cannot reliably distinguish correlation from causation. A gene that is always co-expressed with another may be co-regulated rather than directly regulating. Training on observational data (unperturbed cells) and interventional data (perturbed cells) provides complementary signals, but even Perturb-seq data have limited coverage of the regulatory network. Foundation models capture patterns in data; whether those patterns reflect causal regulatory relationships remains an empirical question that requires experimental validation.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch16-single-cell.html#sec-ch16-epigenomic",
    "href": "part_4/p4-ch16-single-cell.html#sec-ch16-epigenomic",
    "title": "16  Single-Cell Models",
    "section": "16.4 Epigenomic Foundation Models",
    "text": "16.4 Epigenomic Foundation Models\nGene expression profiles capture one layer of cellular state, but the regulatory machinery determining which genes can be expressed operates through epigenomic modifications. DNA methylation silences genes by blocking transcription factor binding; chromatin accessibility determines which regulatory elements are available for activation. These epigenomic layers sit upstream of expression, establishing the potential for transcription before any RNA is produced. Foundation models that learn from epigenomic data capture this regulatory potential, complementing expression-based models with a mechanistic view of how cellular identity is encoded and maintained.\n\n16.4.1 DNA Methylation and CpGPT\nDNA methylation occupies a privileged position in the regulatory hierarchy, sitting at a junction between genotype, environment, and phenotype. Methylation patterns integrate genetic influences, since sequence context affects which CpG sites can be methylated and polymorphisms can create or destroy CpG dinucleotides. They also integrate developmental programs, since methylation landscapes are extensively remodeled during differentiation and establish cell-type-specific regulatory states. Environmental exposures including diet, smoking, and stress leave lasting methylation signatures that persist long after the exposure ends [Citation Needed].\nBeyond serving as an integrative readout, methylation encodes rich information about cellular identity and state. Epigenetic clocks built from methylation data predict chronological age with striking accuracy, and deviations from predicted age (epigenetic age acceleration) correlate with mortality risk and disease burden [Citation Needed]. Cell types can be distinguished by their methylation profiles, and disease states often manifest as characteristic methylation changes.\nCpGPT (Cytosine-phosphate-Guanine Pretrained Transformer) treats methylation as a sequence-like object amenable to transformer-based pretraining (Camillo et al. 2024). The model was pretrained on over 1,500 DNA methylation datasets encompassing more than 100,000 samples from diverse tissues and conditions. Each sample is tokenized as a sequence of CpG sites with their methylation values (beta values ranging from 0 to 1) and genomic positions. The model learns to predict masked methylation values from surrounding context, capturing both local correlations between neighboring CpG sites and global patterns that distinguish different tissues or conditions.\nAfter pretraining, CpGPT supports several capabilities with minimal additional supervision. The model can impute methylation levels at CpG sites not directly measured on a given array platform, effectively enabling conversion between different array technologies such as EPIC and 450K. For biological age prediction, fine-tuned CpGPT models match or exceed purpose-built epigenetic clocks while using a more general architecture. The learned embeddings cluster by tissue type without explicit supervision during pretraining, suggesting that the model captures biologically meaningful variation. For disease-associated methylation patterns, CpGPT can be adapted to distinguish cases from controls across multiple disease contexts through transfer learning.\n\n\n16.4.2 Chromatin Accessibility Models\nChromatin accessibility, measured by assay for transposase-accessible chromatin sequencing (ATAC-seq) and related assays, provides a complementary view of regulatory state. Accessible chromatin regions mark active regulatory elements: promoters, enhancers, and insulators where transcription factors can bind. The accessibility landscape varies across cell types and conditions, reflecting the regulatory programs that define cellular identity.\nFoundation models for chromatin accessibility face the challenge of representing accessibility peaks, which are genomic intervals of variable width rather than single values at fixed positions. Different approaches tokenize this data differently: some treat peaks as binary features (accessible or not), others use continuous accessibility scores, and some operate directly on the underlying sequence to predict accessibility.\nModels that predict chromatin accessibility from DNA sequence, such as those built on Enformer-style architectures (see Chapter 13), learn how sequence motifs and their arrangements determine accessibility. These models complement single-cell accessibility measurements by providing a mechanistic link between genotype and epigenetic state. Variants that alter predicted accessibility become candidates for regulatory function even when they fall outside coding regions.\nSingle-cell ATAC-seq (scATAC-seq) provides cell-type-resolved accessibility profiles, revealing which regulatory elements are active in which cells. Foundation models for scATAC-seq face similar challenges to scRNA-seq models (sparsity, dropout, batch effects) with the additional complexity that the feature space (accessibility peaks) varies across datasets depending on peak calling procedures. Models that operate on fixed genomic coordinates can integrate across datasets more readily than those that rely on dataset-specific peak sets.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch16-single-cell.html#sec-ch16-integration",
    "href": "part_4/p4-ch16-single-cell.html#sec-ch16-integration",
    "title": "16  Single-Cell Models",
    "section": "16.5 Cross-Modality Integration",
    "text": "16.5 Cross-Modality Integration\nSingle-cell technologies have expanded beyond transcriptomics to profile chromatin accessibility, DNA methylation, protein levels, and spatial position. Each modality captures a different aspect of cellular state: expression reflects current activity, accessibility reflects regulatory potential, methylation reflects developmental history. Integrating these perspectives into unified representations requires solving a fundamental challenge: aligning cells profiled with different assays when the feature spaces share no direct correspondence. Foundation model approaches to this integration problem combine learned embeddings with biological prior knowledge, producing unified atlases that leverage all available modalities. These integration challenges anticipate the broader multi-omics approaches examined in Chapter 19, where the principles of intermediate fusion and shared latent spaces extend beyond single-cell to patient-level integration of genomics, transcriptomics, proteomics, and clinical data.\n\n16.5.1 Unpaired Integration Challenge\nSingle-cell experiments often profile different modalities in different cells. A study might include scRNA-seq data from one set of cells, scATAC-seq data from another set, and perhaps a small subset with both modalities measured simultaneously through multiome protocols. Integrating these data into a unified atlas requires aligning cells across modalities when the feature spaces are entirely different.\nThis problem is harder than standard batch correction because there is no direct correspondence between features. RNA-seq measures expression across roughly 20,000 genes. ATAC-seq measures accessibility across hundreds of thousands of peaks. A gene is not the same object as a peak. Simple approaches assign peaks to nearby genes and use gene-level summaries for alignment, but this conversion loses information about the detailed structure of accessibility within regulatory regions and introduces arbitrary choices about assignment rules.\n\n\n16.5.2 GLUE: Graph-Linked Unified Embedding\nGLUE (Graph-Linked Unified Embedding) addresses unpaired integration by combining modality-specific encoders with a graph of biological prior knowledge linking features across omics (Cao and Gao 2022). Rather than converting features between modalities, GLUE explicitly encodes regulatory relationships into a guidance graph and learns cell embeddings that are consistent with this graph.\nThe architecture has three key components. Modality-specific variational autoencoders provide encoders that map cells to a shared low-dimensional latent space and decoders that reconstruct modality-specific features. Generative distributions are tailored to each modality: negative binomial for count data, appropriate alternatives for accessibility.\nThe feature graph encodes biological prior knowledge about relationships between features across modalities. Nodes represent genes, peaks, and other genomic features. Edges connect ATAC peaks to genes they might regulate based on genomic proximity or chromatin conformation data. Edges connect genes to transcription factors that bind their promoters. This graph is provided as input rather than learned, allowing incorporation of external knowledge from databases and literature.\nA graph variational autoencoder learns feature embeddings from the guidance graph. These embeddings are used in the decoders, tying different modalities to a common regulatory backbone. Biologically related features (a gene and its putative enhancer) have similar representations, helping align the latent spaces.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 16.5: [High] Multi-omics integration. Panel A (Unpaired Integration Problem): RNA-seq in cells A,B,C; ATAC-seq in D,E,F; no direct correspondence; goal: unified embedding. Panel B (GLUE Architecture): Modality-specific VAE encoders; feature graph linking genes ↔︎ peaks; shared latent space; adversarial alignment. Panel C (Feature Graph as Prior): Nodes (genes, peaks, transcription factor (TF) sites); edges (proximity, regulatory); graph VAE learns embeddings; “Biological knowledge constrains alignment.” Panel D (Applications): Cross-modal prediction, regulatory inference, triple-omics integration.\n\n\n\nAdversarial alignment ensures that cell embeddings from different modalities are truly integrated. A discriminator tries to distinguish which modality produced each embedding, and encoders are trained to fool the discriminator. This forces the encoders to produce modality-invariant embeddings where cells from different assays occupy a shared manifold reflecting biological rather than technical variation.\n\n\n16.5.3 Applications of Cross-Modal Integration\nGLUE enables several applications beyond basic integration. Triple-omics integration combines gene expression, chromatin accessibility, and DNA methylation measured in different cells, producing unified cell type annotations that leverage all data types. Regulatory inference uses learned feature embeddings to identify candidate enhancer-gene links, providing a principled alternative to simple distance-based assignment.\nCross-modal prediction becomes possible once cells are aligned. The model can predict chromatin accessibility from expression or vice versa, enabling imputation of missing modalities. If a new dataset contains only scRNA-seq, the integrated model can predict which accessibility peaks would likely be active in each cell type based on expression patterns.\nSCGLUE extends the framework with optimizations for single-cell scale and sparsity (Cao and Gao 2022). The adversarial alignment handles batch effects common in single-cell experiments, and the graph structure incorporates tissue-specific regulatory relationships. The model scales to millions of cells while maintaining biological grounding from the guidance graph.\nThe success of graph-guided integration demonstrates that biological prior knowledge can regularize learning and improve alignment. The feature graph constrains what the model learns, ensuring consistency with known regulatory relationships while allowing discovery of new patterns. This combination of learned representations with structured biological knowledge provides a template for integrating foundation model embeddings with domain expertise (see Chapter 18 for further discussion of graph-based approaches).",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch16-single-cell.html#sec-ch16-limitations",
    "href": "part_4/p4-ch16-single-cell.html#sec-ch16-limitations",
    "title": "16  Single-Cell Models",
    "section": "16.6 Practical Challenges and Limitations",
    "text": "16.6 Practical Challenges and Limitations\nThe promise of single-cell foundation models comes with significant caveats. Evaluation remains difficult when ground truth is uncertain, training corpora reflect biases in what tissues and populations have been studied, and the distinction between learning biology and memorizing artifacts is not always clear. These challenges do not invalidate the approach, but they constrain what claims can be made and what applications are appropriate. Understanding these limitations is essential for responsible deployment of single-cell foundation models in research and clinical settings.\n\n16.6.1 Batch Effects and Technical Artifacts\nBatch effects remain the dominant challenge in single-cell analysis. Technical variation between experiments, protocols, and platforms can exceed biological variation, causing cells to cluster by batch rather than by type. Foundation models pretrained on diverse data may be more robust to batch effects than models trained on narrow datasets, but robustness is not guaranteed.\nThe problem is particularly acute when applying pretrained models to new data from platforms or protocols not represented in pretraining. A model trained predominantly on 10x Genomics data may perform poorly on Smart-seq2 data, not because of biological differences but because of systematic technical differences in capture efficiency, amplification bias, and gene detection. Evaluation must carefully distinguish genuine biological generalization from memorization of technical signatures. These evaluation challenges parallel the broader methodological concerns discussed in Chapter 21, while specific strategies for detecting and mitigating batch-driven confounding appear in Section 22.2.2.\n\n\n16.6.2 Cell Type Imbalance\nTraining corpora overrepresent common cell types while rare populations are poorly captured. Immune cells, particularly from blood, dominate many datasets. Rare cell types that may be disease-relevant, such as specific neuronal subtypes or tissue-resident stem cells, appear infrequently. Models may excel at distinguishing well-represented types while struggling with rare or novel populations.\nThis imbalance has equity implications when certain tissues or conditions are systematically undersampled. Neurological and psychiatric diseases involve cell types less represented in current atlases than blood or epithelial cells. Diseases affecting underrepresented populations may be modeled less accurately if training data come predominantly from European ancestry cohorts. These equity concerns mirror the population stratification issues examined in Chapter 22.\n\n\n16.6.3 Evaluation Complexity\nEvaluating single-cell foundation models is complicated by uncertain ground truth. Cell type labels in training data reflect current annotations that may be incomplete or inconsistent. Different studies use different annotation schemes, different levels of granularity, and different evidence standards. Performance metrics conflate model quality with annotation quality.\nPerturbation predictions face similar challenges. The “correct” transcriptional response to a perturbation depends on cell type, context, and measurement technology. Even well-characterized perturbations produce variable responses across replicates. Evaluation protocols must acknowledge these uncertainties rather than treating benchmarks as definitive ground truth. The broader principles of rigorous evaluation methodology from Chapter 21 apply here, while benchmark construction considerations specific to cellular models are addressed in Section 20.5. Single-cell data introduce domain-specific complications that require careful attention to leakage and distribution shift.\n\n\n16.6.4 Causality and Mechanism\nThe most fundamental limitation is that correlation-based learning cannot establish causation. Foundation models learn patterns of co-occurrence: which genes appear together, which accessibility peaks associate with which expression changes. These patterns may reflect regulatory relationships, but they may also reflect confounding factors, indirect associations, or artifacts of data processing.\nThe perturbation prediction task illustrates this limitation. A model that accurately predicts perturbation outcomes for well-characterized genes may be learning genuine regulatory logic, or it may be exploiting superficial correlations that happen to work for genes with abundant training data. Distinguishing these possibilities requires experimental validation and careful analysis of model behavior on held-out perturbations.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch16-single-cell.html#sec-ch16-conclusion",
    "href": "part_4/p4-ch16-single-cell.html#sec-ch16-conclusion",
    "title": "16  Single-Cell Models",
    "section": "16.7 From Sequence to State",
    "text": "16.7 From Sequence to State\nSingle-cell and epigenomic foundation models learn what states cells occupy, complementing the sequence-based models that learn what sequences encode. DNA and protein language models capture the information content of genomic and protein sequence (see Chapter 11, Chapter 12); cellular models capture the configurations that cells assume in development, homeostasis, and disease. These perspectives address different biological questions: sequence determines the possible states a cell can achieve, while cellular state reflects which possibilities are realized in a given context. A complete understanding of gene regulation requires both.\nThe representations learned by cellular foundation models enable integration across scales and modalities. Cell embeddings serve as node features in graph-based reasoning systems (Chapter 18), connecting expression profiles to protein interaction networks and regulatory pathways. Three-dimensional genome organization (Chapter 17) provides spatial context that constrains which regulatory relationships can operate. Multi-omics integration (Chapter 19) extends beyond expression to proteomics, epigenomics, and clinical measurements. In each case, foundation model embeddings provide the representational substrate that downstream methods refine.\nThe ultimate goal extends beyond prediction to explanation: models that identify the regulatory mechanisms underlying cellular state, the variants that perturb those mechanisms, and the interventions that might restore normal function. Current foundation models capture patterns in cellular data with high fidelity, enabling accurate cell type classification, perturbation response prediction, and cross-dataset integration. Whether those patterns reflect the causal structure of biological regulation, or merely correlations useful for prediction, remains open. Resolving this question requires continued integration of computational modeling with experimental validation, connecting the patterns that models learn to the mechanisms that biology employs.\n\n\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T. Higgins-Chen, Steve Horvath, and Bo Wang. 2024. “CpGPT: A Foundation Model for DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. 2024. “scGPT: Toward Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI.” Nature Methods 21 (8): 1470–80. https://doi.org/10.1038/s41592-024-02201-0.\n\n\nDixit, Atray, Oren Parnas, Biyu Li, Jenny Chen, Charles P. Fulco, Livnat Jerby-Arnon, Nemanja D. Marjanovic, et al. 2016. “Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens.” Cell 167 (7): 1853–1866.e17. https://doi.org/10.1016/j.cell.2016.11.038.\n\n\nHao, Minsheng, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Xuegong Zhang, and Le Song. 2024. “Large-Scale Foundation Model on Single-Cell Transcriptomics.” Nature Methods 21 (8): 1481–91. https://doi.org/10.1038/s41592-024-02305-7.\n\n\nPearce, James D., Sara E. Simmonds, Gita Mahmoudabadi, Lakshmi Krishnan, Giovanni Palla, Ana-Maria Istrate, Alexander Tarashansky, et al. 2025. “[TranscriptFormer] Cross-Species Generative Cell Atlas Across 1.5 Billion Years of Evolution: The TranscriptFormer Single-Cell Model.” bioRxiv. https://doi.org/10.1101/2025.04.25.650731.\n\n\nTheodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023. “[Geneformer] Transfer Learning Enables Predictions in Network Biology.” Nature 618 (7965): 616–24. https://doi.org/10.1038/s41586-023-06139-9.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch17-3d-genome.html",
    "href": "part_4/p4-ch17-3d-genome.html",
    "title": "17  3D Genome Organization",
    "section": "",
    "text": "17.1 Chromatin Organization Hierarchy\nThe human genome spans approximately two meters of linear DNA, yet it must fit within a nucleus roughly ten micrometers in diameter: a compaction ratio of nearly 200,000 to one. This folding is not random. Specific sequences contact each other across vast genomic distances while others remain isolated, and these contact patterns determine which enhancers can activate which genes. An enhancer 500 kilobases from its target can drive transcription only because intervening chromatin folds to bring them into physical proximity. The regulatory models covered in Chapter 13 predict expression from sequence within a fixed window, building on the convolutional architectures introduced in Chapter 6 and treating the genome as a one-dimensional string. They cannot explain why an enhancer activates one gene and not another when multiple promoters lie within range.\nDisruptions to 3D genome architecture cause disease through mechanisms that sequence alone cannot predict. When structural variants delete a boundary between chromatin domains, enhancers can contact genes they normally never reach, a phenomenon called enhancer hijacking that underlies developmental disorders and cancer. The clinical consequences depend entirely on which contacts are disrupted. A deletion that removes a domain boundary may be pathogenic; an identical-sized deletion preserving boundaries may be benign. Current variant effect prediction tools (Chapter 14) largely ignore this spatial dimension, creating systematic blind spots for structural variant interpretation.\nThe genome folds through multiple organizational levels, each with distinct functional consequences and arising from different molecular mechanisms. Understanding this hierarchy is essential for interpreting both normal gene regulation and how structural variants cause disease. The levels are not independent; they interact in complex ways that computational models must capture to predict 3D structure accurately.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch17-3d-genome.html#sec-ch17-chromatin-hierarchy",
    "href": "part_4/p4-ch17-3d-genome.html#sec-ch17-chromatin-hierarchy",
    "title": "17  3D Genome Organization",
    "section": "",
    "text": "17.1.1 Chromosome Territories and Compartments\nAt the largest scale, chromosomes occupy distinct nuclear volumes called chromosome territories. Gene-rich chromosomes tend toward the nuclear interior while gene-poor chromosomes associate with the nuclear periphery. This territorial organization limits which chromosomes can exchange material during translocations: recurrent cancer-associated translocations occur preferentially between chromosomes that occupy neighboring territories [Citation Needed]. While chromosome territory organization has clear functional implications, most computational models focus on finer-scale structures where sequence determinants are more tractable.\nWithin chromosome territories, chromatin partitions into two major compartment types distinguished by their transcriptional activity and chromatin state. A compartments contain gene-rich, transcriptionally active chromatin with open, accessible structure. B compartments contain gene-poor, transcriptionally silent regions often associated with the nuclear lamina at the nuclear periphery. This compartmentalization is visible in Hi-C contact maps as a characteristic checkerboard pattern: A compartment regions preferentially contact other A regions even when separated by megabases, while B regions contact other B regions [Citation Needed]. Compartment identity correlates strongly with histone modifications (H3K27ac marks active A compartments; H3K9me3 marks repressive B compartments) and changes during cellular differentiation as lineage-specific genes shift between active and inactive states. The molecular mechanism underlying compartmentalization appears to involve phase separation: regions with similar chromatin states aggregate through weak multivalent interactions, creating nuclear microenvironments with distinct biochemical properties [Citation Needed].\n\n\n17.1.2 Topologically Associating Domains\nBelow the megabase scale of compartments, the genome organizes into topologically associating domains (TADs): sub-megabase regions (median approximately 800 kilobases in mammals) within which sequences contact each other more frequently than with sequences outside the domain. TAD boundaries appear as sharp transitions in contact frequency, visible in Hi-C maps as triangular domains along the matrix diagonal. These boundaries show strong conservation across mammalian species and across cell types within a species, suggesting strong selective pressure to maintain domain organization [Citation Needed]. The prevailing model holds that TADs constrain enhancer-promoter interactions: regulatory elements within a TAD can contact genes in the same domain, but boundaries prevent crosstalk with genes in adjacent domains. This insulation function has clear clinical relevance. Deletions that remove TAD boundaries allow enhancers to contact genes they normally cannot reach. In a well-characterized example, deletions removing the boundary between the EPHA4 locus and the WNT6/PAX3 region allow limb enhancers to ectopically activate WNT6, causing brachydactyly and other limb malformations (Lupiáñez et al. 2015).\n\n\n17.1.3 Loop Extrusion Mechanism\nThe molecular basis of TAD formation is now well understood through the loop extrusion model. The cohesin protein complex loads onto chromatin and extrudes DNA bidirectionally, progressively enlarging the extruded loop until it encounters an obstacle. The key obstacle is CTCF protein bound to DNA in a specific orientation. When cohesin encounters CTCF sites oriented toward each other (convergent orientation), extrusion halts and a stable loop forms with the convergent CTCF sites at the loop anchors [Citation Needed]. This model explains several key observations: TAD boundaries are enriched for CTCF binding sites; CTCF motif orientation predicts which sites will anchor loops (convergent pairs form loops while divergent pairs do not); and acute degradation of cohesin eliminates TADs within hours while leaving compartments intact [Citation Needed]. The distinction between compartment and TAD formation mechanisms has important implications for prediction. Models that capture CTCF binding and orientation can predict TAD boundaries; predicting compartments requires learning different sequence features associated with chromatin state.\n\n\n17.1.4 Fine-Scale Chromatin Loops\nAt the finest scale, chromatin forms specific loops between defined loci. Enhancer-promoter loops bring distal regulatory elements into physical proximity with their target genes, while structural loops between convergent CTCF sites establish the TAD framework. Most enhancer-promoter contacts span less than 200 kilobases, but some extend over a megabase [Citation Needed]. Detecting these fine-scale contacts requires high-resolution data; the Micro-C method uses micrococcal nuclease digestion to achieve nucleosome-level resolution, revealing contact patterns invisible in standard Hi-C [Citation Needed]. The functional significance of individual loops remains debated. Some loops appear essential for gene activation; others may be structural features without direct regulatory consequences.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 17.2: [Essential] Four-level visualization. Panel A (Chromosome Territories, Nuclear Scale): Nucleus with distinct territories; gene-rich interior, gene-poor periphery. Panel B (A/B Compartments, Megabase Scale): Hi-C checkerboard pattern; A active interior, B inactive lamina-associated. Panel C (TADs, Sub-Megabase Scale): Triangular domains; sharp boundaries; median ~800 kb; CTCF at boundaries. Panel D (Loops, Kilobase Scale): Focal Hi-C enrichments; enhancer-promoter contacts; CTCF-CTCF structural loops.\n\n\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 17.3: [Essential] Loop extrusion mechanism. Panel A (Cohesin Loading): Ring loading onto chromatin; initial state no loop. Panel B (Bidirectional Extrusion): Cohesin extruding DNA; loop growing progressively; intermediate states. Panel C (CTCF Arrest): Cohesin encountering convergent CTCF; extrusion halted; stable loop. Panel D (The Orientation Rule): Convergent → ← (loop anchors ✓); divergent ← → (no stable loop ✗); tandem → → (extrusion continues ~). Inset: Acute cohesin degradation eliminates TADs but compartments remain.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch17-3d-genome.html#sec-ch17-3d-measurement",
    "href": "part_4/p4-ch17-3d-genome.html#sec-ch17-3d-measurement",
    "title": "17  3D Genome Organization",
    "section": "17.2 Measuring the 3D Genome",
    "text": "17.2 Measuring the 3D Genome\nPredicting 3D genome structure requires training data: measurements of which sequences contact which other sequences in real cells. Chromosome conformation capture methods provide these measurements through a common biochemical principle, though the technologies vary in resolution, throughput, and the aspects of 3D organization they reveal.\n\n17.2.1 Hi-C and Contact Matrices\nCells are crosslinked with formaldehyde to freeze chromatin contacts in place; DNA is digested with restriction enzymes; free DNA ends are ligated, preferentially joining fragments that were spatially proximate; and the ligated junctions are identified through sequencing. The frequency of junction reads between two genomic regions reflects how often those regions were in physical contact across the cell population.\nHi-C extends this principle genome-wide by incorporating biotinylated nucleotides at ligation junctions, enabling purification of chimeric fragments from the entire genome [Citation Needed]. The output is a contact matrix where rows and columns represent genomic bins (typically 1 to 50 kilobases depending on sequencing depth) and values represent contact frequencies between bin pairs. Resolution depends directly on sequencing depth: achieving 1 kilobase resolution requires billions of reads, while 10 kilobase resolution requires hundreds of millions. Raw contact frequencies require extensive normalization to correct for biases from GC content, restriction site density, and mappability. The ICE (iterative correction and eigenvector decomposition) method and related approaches remove these technical artifacts while preserving biological signal [Citation Needed]. The training strategies that enable models to learn from these normalized contact matrices follow the multi-task principles introduced in Section 8.6.\nThe contact matrix encodes all levels of chromatin organization. Compartments appear as the checkerboard pattern when viewing megabase-scale interactions; TADs appear as triangular domains of enriched contacts along the diagonal; and loops appear as focal enrichments at specific off-diagonal positions. The matrix is dominated by the polymer effect: sequences that are close in linear distance contact each other frequently regardless of specific 3D structure, creating strong signal along the diagonal that can obscure biologically meaningful contacts at greater distances.\n\n\n17.2.2 Resolution and Data Resources\nBeyond standard Hi-C, several technologies address specific limitations. Micro-C achieves nucleosome-level resolution by using micrococcal nuclease instead of restriction enzymes, revealing fine-scale contact patterns invisible at standard Hi-C resolution [Citation Needed]. Single-cell Hi-C measures contacts in individual cells, revealing that any two loci contact each other in only 5 to 15 percent of cells, but the resulting matrices are extremely sparse (most possible contacts are unmeasured in any single cell) [Citation Needed]. Imaging methods such as DNA FISH directly visualize genomic loci in the nucleus, providing ground truth for computational predictions but at much lower throughput than sequencing-based approaches.\nTraining data for 3D prediction models comes primarily from a small number of well-characterized cell lines. The lymphoblastoid cell line GM12878 and the leukemia cell line K562 have deep Hi-C coverage across multiple laboratories, making them the default training sets for most models. Primary tissues and rare cell types have sparse coverage, creating a significant gap between where models are trained and where clinical applications require predictions. The 4D Nucleome Data Portal and ENCODE provide the most comprehensive repositories of 3D genome data, though coverage remains heavily biased toward common cell lines and human samples. This data landscape parallels the challenges discussed for functional genomics data more broadly (Chapter 2).",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch17-3d-genome.html#sec-ch17-3d-prediction",
    "href": "part_4/p4-ch17-3d-genome.html#sec-ch17-3d-prediction",
    "title": "17  3D Genome Organization",
    "section": "17.3 Predicting 3D Structure from Sequence",
    "text": "17.3 Predicting 3D Structure from Sequence\nSequence-based prediction of 3D genome structure asks whether DNA sequence alone contains sufficient information to predict chromatin contacts. The success of models like Akita, Orca, and C.Origami demonstrates that sequence encodes substantial 3D information, particularly for TAD boundaries and CTCF-anchored loops. These models share a common challenge: predicting a two-dimensional contact matrix from a one-dimensional sequence input.\n\n17.3.1 Akita and Dilated Convolutions\nAkita, introduced by Fudenberg et al. in 2020, established the paradigm for sequence-to-contact prediction (Fudenberg, Kelley, and Pollard 2020). The model takes approximately one megabase of DNA sequence as input and predicts Hi-C contact frequencies at 2 kilobase resolution. The architecture uses dilated convolutions to expand the receptive field without proportionally increasing parameters (an approach discussed in detail in Section 6.5.1), enabling the model to integrate information across the full input window. The output is symmetric (contacts between positions i and j equal contacts between j and i), which the architecture enforces through appropriate pooling operations. Akita achieves correlation coefficients of 0.6 to 0.8 between predicted and observed contact maps in held-out genomic regions, successfully identifying TAD boundaries and major loop anchors.\n\n\n17.3.2 Orca and Multiscale Prediction\nOrca extends sequence-based prediction to multiple resolutions simultaneously (Zhou 2022). Rather than predicting a single-resolution contact map, Orca generates predictions at 4, 8, 16, 32, 64, 128, and 256 kilobase resolutions, capturing both fine-scale loops and large-scale compartment structure. The multiscale approach addresses a fundamental challenge: compartments span megabases while loops span kilobases, and no single resolution optimally captures both. Orca’s architecture processes sequence through parallel pathways tuned to different scales, then combines predictions into a coherent multiscale representation. This design enables prediction of structural variants’ effects across organizational levels, from disrupted loops to altered compartment boundaries.\n\n\n17.3.3 C.Origami and Cross-Cell-Type Transfer\nC.Origami addresses the cell-type specificity problem (Tan et al. 2023). While TAD boundaries are largely conserved across cell types, finer-scale contacts vary substantially. C.Origami incorporates CTCF ChIP-seq data alongside sequence, enabling the model to learn how cell-type-specific CTCF binding patterns shape cell-type-specific contact maps. This design enables transfer learning: train on cell types with both Hi-C and CTCF data, then predict contacts in new cell types using only CTCF ChIP-seq. The approach substantially expands the range of cell types where 3D predictions are possible, since CTCF ChIP-seq is available for many more cell types than deep Hi-C. This transfer strategy echoes the broader transfer learning principles discussed in Chapter 9.\n\n\n17.3.4 Learned Sequence Determinants\nInterpretability analysis reveals what these models learn about sequence determinants of 3D structure. Attribution methods (discussed more fully in Chapter 24) consistently identify CTCF motifs as the strongest predictors of contact patterns, with convergent CTCF pairs (motifs oriented toward each other) most strongly associated with loop anchors. Transcription start sites contribute to boundary predictions, consistent with the observation that active promoters often coincide with domain edges. GC content correlates with compartment identity (GC-rich regions tend toward A compartment), and repetitive element composition shows systematic associations (LINE elements with B compartment; Alu elements with A compartment) [Citation Needed]. The orientation rule for CTCF emerges naturally from training: models learn that CTCF motif orientation, not just presence, predicts which sites will anchor loops. This learned relationship matches the mechanistic understanding from the loop extrusion model, providing validation that models capture biologically meaningful features.\nDespite these advances, significant limitations remain. Resolution is constrained by training data; predicting nucleosome-level contacts requires Micro-C training data that exists for few cell types. The single-cell variation problem is fundamental: models trained on bulk Hi-C predict population averages, but gene regulation may depend on the stochastic 3D configurations in individual cells. Causality cannot be established from prediction alone; a model may correctly predict that two regions contact each other without revealing whether that contact causes any functional consequence. Generalization to cell types distant from training data remains uncertain, and the computational cost of processing megabase sequences limits practical applications for genome-wide analysis.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 17.4: [High] Model comparison. Panel A (Akita Architecture): 1 Mb DNA input → dilated convolutions → symmetric output → 2 kb Hi-C. Panel B (Orca Multi-Scale): Coarse Mb prediction → progressive refinement → multi-resolution loss. Panel C (C.Origami with CTCF): Sequence + CTCF ChIP-seq → cell-type-specific predictions; transfer from Hi-C-rich to CTCF-only cells. Panel D (Prediction vs. Ground Truth): Example region; TAD boundaries correctly positioned; loop anchors identified; “Sequence contains substantial 3D information.”",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch17-3d-genome.html#sec-ch17-3d-regulation",
    "href": "part_4/p4-ch17-3d-genome.html#sec-ch17-3d-regulation",
    "title": "17  3D Genome Organization",
    "section": "17.4 3D Structure and Gene Regulation",
    "text": "17.4 3D Structure and Gene Regulation\nThe ultimate purpose of 3D genome prediction is understanding gene regulation. Contact maps matter because they reveal which enhancers can reach which genes. Integrating 3D structure with expression prediction addresses limitations that purely one-dimensional models cannot overcome.\n\n17.4.1 Beyond One-Dimensional Models\nEnformer (Section 13.2) predicts gene expression from sequence within a 200 kilobase window, sufficient to capture many enhancer-promoter relationships but fundamentally limited by its treatment of the genome as a one-dimensional string. This representation cannot distinguish an enhancer that loops to a distant gene from one blocked by a TAD boundary, nor can it explain cell-type-specific contacts that activate different genes from the same enhancer in different contexts. The 3D genome provides this missing context: physical proximity through chromatin loops determines which regulatory elements can communicate.\nConsider an enhancer located 300 kilobases from two genes, one upstream and one downstream. Linear models would predict similar regulatory influence on both genes based on comparable distances. But if a TAD boundary lies between the enhancer and the upstream gene, 3D structure predicts that only the downstream gene receives regulatory input. The boundary insulates the upstream gene from enhancer activity regardless of linear proximity. This insulation function explains why TAD boundaries show such strong evolutionary conservation: disrupting boundaries allows regulatory crosstalk that can dysregulate gene expression with pathogenic consequences.\n\n\n17.4.2 Structural Variant Interpretation\nThe clinical significance is clearest in structural variant interpretation. Deletions that remove TAD boundaries cause enhancer hijacking, where regulatory elements gain access to genes in adjacent domains. The EPHA4 locus provides the canonical example: limb enhancers normally activate EPHA4 expression in developing limbs. When deletions remove the TAD boundary separating EPHA4 from the adjacent WNT6/PAX3 domain, these enhancers ectopically activate WNT6, causing limb malformations including brachydactyly and polydactyly (Lupiáñez et al. 2015). Different deletion sizes produce different phenotypes depending on which boundaries are removed and which new enhancer-gene contacts form. Similar mechanisms operate in cancer, where structural variants create novel enhancer-oncogene contacts that drive tumor growth [Citation Needed]. The diagnostic challenge is substantial: predicting pathogenicity of structural variants requires understanding which 3D contacts will be disrupted and what new contacts will form, predictions that sequence-only models cannot provide. This challenge intersects with the variant prioritization pipelines discussed in Chapter 26, where 3D genome effects represent a systematic blind spot in current foundation model approaches to variant effect prediction (Chapter 14).\nIntegrating 3D predictions with expression models remains technically challenging. Hybrid approaches use predicted contacts to weight enhancer contributions: rather than treating all enhancers within a window equally, weights reflect predicted contact frequency with the target promoter. This activity-by-contact framework (expression proportional to the sum of enhancer activities weighted by contact frequencies) captures some of the regulatory logic that 1D models miss [Citation Needed]. Graph-based representations (Chapter 18) can encode genes and enhancers as nodes with contacts as edges, enabling graph neural networks to reason about regulatory relationships in 3D space. Attribution methods for understanding which contacts drive expression predictions are examined in Section 24.1. End-to-end training of combined 3D and expression models remains difficult; most current approaches train the components separately and combine predictions post hoc.\n\n\n17.4.3 Causality and Permissive Architecture\nThe causality question complicates interpretation. Do enhancer-promoter contacts cause gene activation, or does gene activation cause contacts? Transcription itself can influence chromatin organization: active transcription may stabilize enhancer-promoter contacts that would otherwise be transient [Citation Needed]. Perturbation experiments provide cleaner causal tests than correlational analysis. Acute degradation of cohesin eliminates TADs within hours, yet most genes show minimal expression changes, suggesting that many TAD structures are permissive rather than deterministic for gene regulation [Citation Needed]. CRISPR-based deletion of specific TAD boundaries similarly produces more modest effects than the structural disruption would suggest [Citation Needed]. The emerging view is nuanced: 3D structure constrains which enhancer-promoter interactions are possible, but whether those interactions occur depends on additional factors including transcription factor availability and chromatin state. This distinction between correlation and causation echoes the confounding challenges discussed in Chapter 22.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch17-3d-genome.html#sec-ch17-spatial-transcriptomics",
    "href": "part_4/p4-ch17-3d-genome.html#sec-ch17-spatial-transcriptomics",
    "title": "17  3D Genome Organization",
    "section": "17.5 Spatial Transcriptomics",
    "text": "17.5 Spatial Transcriptomics\nSingle-cell RNA sequencing (Chapter 16) reveals cellular heterogeneity but discards spatial information: we learn which genes each cell expresses but not where that cell sits within the tissue. For understanding tumor microenvironments, developmental gradients, or tissue architecture, spatial context is essential. A T cell adjacent to a tumor cell experiences a different microenvironment than one in the surrounding stroma, and this spatial context shapes gene expression programs in ways that dissociated single-cell data cannot capture.\n\n17.5.1 Measurement Technologies\nSpatial transcriptomics technologies fall into two broad categories with complementary strengths. Spot-based methods like Visium (10x Genomics) capture polyadenylated RNA at arrayed positions on a slide, providing transcriptome-wide measurement at approximately 55 micrometer resolution (typically 1 to 10 cells per spot). These methods offer comprehensive gene coverage but limited spatial resolution. Imaging-based methods like MERFISH use sequential rounds of fluorescent hybridization to identify RNA molecules in situ, achieving subcellular resolution but limited to pre-selected gene panels (hundreds to thousands of genes rather than transcriptome-wide) [Citation Needed]. Newer technologies like Stereo-seq achieve near-cellular resolution with transcriptome-wide coverage through spatial barcoding, though they remain less validated than established methods [Citation Needed].\n\n\n17.5.2 Computational Challenges\nComputational challenges in spatial transcriptomics mirror and extend those in single-cell analysis (Section 16.1.2). Spot deconvolution addresses the multiple-cells-per-spot problem in Visium data: inferring the cell type composition within each spot by comparing spot expression profiles to reference single-cell atlases. Imputation methods predict expression of genes not measured in imaging-based assays, leveraging correlations learned from reference datasets. Integration aligns spatial data with single-cell references, mapping reference cell types onto spatial coordinates. Domain correction handles batch effects that manifest in spatial patterns as well as expression levels. The sparsity problem is even more severe than in standard single-cell RNA sequencing; gene detection rates in spatial methods often fall below 10 percent [Citation Needed]. The missing modality strategies developed for multi-omics integration (Section 19.6) become essential when spatial methods fail to detect genes that single-cell RNA-seq measures reliably.\n\n\n17.5.3 Spatial Foundation Models\nSpatial foundation models remain much less mature than sequence-based models (Chapter 11, Chapter 12). The fundamental challenge is the lack of an equivalent to evolutionary pretraining: DNA and protein models learn from billions of years of evolutionary experiments encoded in sequence databases, but no comparable natural augmentation exists for spatial organization. Current approaches include graph neural networks that encode spatial relationships as edges between neighboring cells or spots, transformer architectures that treat spatial positions as tokens with positional encodings derived from coordinates, and generative models that learn spatial patterns from atlases of reference tissues. Models like Nicheformer apply transformer architectures to spatial niches (local cellular neighborhoods), learning representations that capture cell-cell communication patterns and tissue microenvironment signatures [Citation Needed]. SpaGCN uses graph convolutional networks with spatial graphs, propagating information between spatially adjacent regions to identify spatial domains with coherent expression patterns [Citation Needed].\nOther approaches address different aspects of the spatial modeling problem. CellPLM pretrains on millions of spatial transcriptomics cells, learning representations that transfer across tissue types and experimental platforms [Citation Needed]. STACI combines spatial coordinates with morphological features from histology images, enabling joint reasoning about molecular and visual tissue properties [Citation Needed]. GraphST uses graph attention networks to propagate expression signals across spatial neighborhoods while preserving local heterogeneity [Citation Needed]. These methods remain early in development compared to sequence foundation models; no spatial equivalent of DNABERT or ESM-2 has achieved broad adoption, and benchmark comparisons across methods remain limited by the diversity of spatial platforms and tissue types.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 17.5: [Enhancing] Spatial data and models. Panel A (Visium Spot-Based): Tissue section with spot overlay; 1-10 cells per spot; ~55 μm resolution. Panel B (Imaging-Based MERFISH/Xenium): Single-cell resolution; limited gene panel; subcellular localization. Panel C (Deconvolution Challenge): Multi-cell spot → infer composition; reference atlas required. Panel D (Spatial Foundation Models): Cell as node, proximity as edge; GNNs over spatial graphs; cell-cell communication; examples (Nicheformer, SpaGCN, GraphST).\n\n\n\nThe clinical applications motivating spatial foundation model development center on tumor microenvironment characterization. The spatial organization of immune cells relative to tumor cells predicts treatment response: tumors with immune cells infiltrating the tumor core respond better to immunotherapy than those with immune exclusion at the tumor periphery [Citation Needed]. Spatial models aim to learn these prognostic patterns from training data, enabling prediction of treatment response from spatial organization alone. Similar applications exist in developmental biology (understanding morphogen gradients and cell fate decisions), neuroscience (mapping brain region organization), and pathology (characterizing disease architecture in tissue sections).",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch17-3d-genome.html#sec-ch17-3d-limitations",
    "href": "part_4/p4-ch17-3d-genome.html#sec-ch17-3d-limitations",
    "title": "17  3D Genome Organization",
    "section": "17.6 Limitations and Open Questions",
    "text": "17.6 Limitations and Open Questions\nCurrent 3D genome and spatial models face limitations that constrain their utility for clinical and research applications. Resolution remains a fundamental constraint: most Hi-C prediction models operate at 2 to 10 kilobase resolution, while functionally relevant enhancer-promoter contacts involve specific sequences within those bins. Predicting which specific kilobases within a TAD contact each other requires resolution that exceeds current training data in most cell types. The resolution needed for accurate prediction may exceed the resolution achievable from bulk Hi-C, creating a data ceiling that computational methods cannot overcome.\nThe population averaging problem is more fundamental than a mere technical limitation. Bulk Hi-C measurements average over millions of cells, each with a different 3D configuration. Any two loci contact each other in only a minority of cells at any given time, yet the averaged contact frequency appears as a single value in the training data. Single-cell Hi-C reveals this heterogeneity but produces extremely sparse data (most possible contacts unmeasured in each cell). Models trained on population averages cannot predict single-cell behavior, yet gene regulation may depend on the stochastic dynamics of contact formation in individual cells. Whether the population average or the single-cell distribution matters more for predicting gene expression remains unclear.\nCausality represents the deepest conceptual challenge. Predicting that two regions contact each other does not establish that the contact causes any biological consequence. Many TAD disruptions produce minimal expression changes; many enhancer-promoter contacts may be bystanders rather than drivers of transcription. The loop extrusion machinery that creates TADs operates continuously, but the transcriptional machinery that reads out enhancer-promoter communication operates on different timescales and with different requirements. Computational predictions of 3D structure are correlational; establishing which predicted contacts matter functionally requires experimental validation that computational methods cannot replace.\nFor clinical applications, the sparse training data creates systematic blind spots. Models trained on GM12878 and K562 may not transfer to the primary cells, developmental stages, or disease states where predictions matter most. A structural variant affecting 3D organization in neural progenitor cells cannot be reliably interpreted using models trained only on lymphoblastoid cells. The cell types most relevant for clinical interpretation are often those with the least 3D characterization data available. This challenge parallels the transferability concerns discussed throughout Chapter 21 and Chapter 22.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch17-3d-genome.html#sec-ch17-structure-context",
    "href": "part_4/p4-ch17-3d-genome.html#sec-ch17-structure-context",
    "title": "17  3D Genome Organization",
    "section": "17.7 Structure as Context, Not Cause",
    "text": "17.7 Structure as Context, Not Cause\nThe genome’s three-dimensional organization provides context that one-dimensional sequence models cannot capture. Enhancer-promoter contacts explain regulatory relationships spanning hundreds of kilobases; TAD boundaries constrain which elements can interact; tissue architecture determines the cellular neighborhoods where gene expression programs execute. Models like Akita, Orca, and C.Origami demonstrate that sequence contains substantial information about chromatin folding, predicting contact maps from DNA sequence with accuracy sufficient to identify structural variants and disease-associated changes.\nYet the functional role of 3D structure remains more modest than early enthusiasm implied. Experimental perturbation studies show that TAD boundary disruption often has limited expression consequences [Citation Needed]. Many chromatin contacts appear permissive rather than instructive: they establish the possibility of regulatory communication without determining whether that communication occurs. A predicted enhancer-promoter contact indicates that interaction could happen, not that it does happen or that it matters when it does. The 3D genome may constrain the regulatory landscape without specifying regulatory outcomes.\nThis distinction shapes how 3D structure should be integrated with other modalities. Chromatin contacts become edges in gene regulatory networks (Chapter 18), providing structural priors for graph-based reasoning. Spatial expression patterns integrate with multi-omics approaches (Chapter 19), adding tissue architecture alongside genomics and transcriptomics. For interpretability (Chapter 24), 3D structure offers mechanistic hypotheses that require experimental validation. Whether a predicted regulatory effect operates through chromatin proximity, or whether proximity merely correlates with regulation through shared causes, remains a question that computational models can motivate but not answer. The integration of 3D information into genomic AI proceeds with appropriate uncertainty about what that information contributes.\n\n\n\n\nFudenberg, Geoff, David R. Kelley, and Katherine S. Pollard. 2020. “[Akita] Predicting 3D Genome Folding from DNA Sequence with Akita.” Nature Methods 17 (11): 1111–17. https://doi.org/10.1038/s41592-020-0958-x.\n\n\nLupiáñez, Darío G., Katerina Kraft, Verena Heinrich, Peter Krawitz, Francesco Brancati, Eva Klopocki, Denise Horn, et al. 2015. “Disruptions of Topological Chromatin Domains Cause Pathogenic Rewiring of Gene-Enhancer Interactions.” Cell 161 (5): 1012–25. https://doi.org/10.1016/j.cell.2015.04.004.\n\n\nTan, Jimin, Nina Shenker-Tauris, Javier Rodriguez-Hernaez, Eric Wang, Theodore Sakellaropoulos, Francesco Boccalatte, Palaniraja Thandapani, et al. 2023. “Cell-Type-Specific Prediction of 3D Chromatin Organization Enables High-Throughput in Silico Genetic Screening.” Nature Biotechnology 41 (8): 1140–50. https://doi.org/10.1038/s41587-022-01612-8.\n\n\nZhou, Jian. 2022. “Sequence-Based Modeling of Three-Dimensional Genome Architecture from Kilobase to Chromosome Scale.” Nature Genetics 54 (5): 725–34. https://doi.org/10.1038/s41588-022-01065-4.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-networks.html",
    "href": "part_4/p4-ch18-networks.html",
    "title": "18  Graph and Network Models",
    "section": "",
    "text": "18.1 Biological Networks and Data Resources\nGraph neural networks are not alternatives to foundation models; they are consumers of them. Foundation models produce rich representations of individual biological entities: protein language models encode evolutionary constraint and structural propensity (Chapter 12), DNA models capture regulatory grammar (Chapter 13), RNA models represent transcript-level features (Chapter 15). These representations are powerful but operate on isolated sequences. A protein embedding captures what ESM learned about that protein’s sequence; it says nothing about which other proteins it binds, which pathways it participates in, or which disease phenotypes result from its disruption. Graph neural networks operate at a higher level of abstraction, taking foundation model representations as node features and learning to propagate information across relational structure. The combination yields capabilities that neither approach achieves alone.\nThis architectural relationship reflects a biological reality: organisms are not collections of independent molecules but systems of interacting components. A transcription factor affects its target genes through regulatory edges. Proteins assemble into functional complexes through physical binding. Signaling cascades propagate perturbations across cellular networks. These relationships exist at a level of abstraction above sequence, requiring a different mathematical framework to represent. Graphs provide precisely this framework. In a protein-protein interaction network, proteins become nodes and physical binding creates edges. In a gene regulatory network, directed edges connect transcription factors to their targets. In spatial transcriptomics data, cells become nodes with edges capturing physical proximity. Each graph encodes relational structure that sequence models cannot directly capture.\nThe practical implications are substantial. Disease gene prioritization leverages the observation that genes causing similar diseases cluster in network neighborhoods. A GNN can learn to propagate disease signals across protein interaction networks, but effectiveness depends critically on node feature quality. When those features come from protein language models encoding evolutionary constraint and structural propensity, the GNN gains access to sequence-level biological knowledge unavailable from simpler features like expression levels alone. Drug-target interaction prediction similarly benefits: ESM embeddings capture what makes a protein druggable, while network context reveals which targets sit in therapeutically relevant pathways.\nGraph neural networks can only learn from relationships encoded in their input graphs. The choice of network, its source, and its inherent biases determine what a model can discover and what it will miss. Understanding the landscape of available biological networks, their construction methods, and their systematic limitations is therefore prerequisite to effective graph-based modeling.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-networks.html#sec-ch18-biological-networks",
    "href": "part_4/p4-ch18-networks.html#sec-ch18-biological-networks",
    "title": "18  Graph and Network Models",
    "section": "",
    "text": "18.1.1 Landscape of Biological Graphs\nBefore examining graph neural network architectures, it is essential to understand what biological networks exist and where they come from. The choice of network fundamentally shapes what a model can learn, and the biases inherent in network construction propagate through all downstream analyses.\nPhysical associations between proteins constitute perhaps the most widely used network type for GNN applications. Major databases include STRING, which integrates experimental data with computational predictions and text mining to assign confidence scores to interactions; BioGRID, which focuses on curated experimental interactions; and IntAct, which provides detailed interaction metadata from direct molecular experiments. These protein-protein interaction networks are incomplete (current estimates suggest only 20-30% of human PPIs are catalogued) and biased toward well-studied proteins in well-characterized pathways (szklarczyk2023string?; oughtred2021biogrid?; orchard2014intact?; venkatesan2009protein?; hart2006completeness?). A gene involved in cancer or a common disease may have hundreds of documented interactions, while an uncharacterized protein in a specialized tissue may have none, not because it lacks interactions but because no one has looked.\nTranscriptional control relationships require a different network structure. Unlike PPIs, gene regulatory networks are inherently directed: a transcription factor activates or represses its targets, not vice versa. Sources include chromatin immunoprecipitation sequencing (ChIP-seq) experiments that identify transcription factor binding sites, chromatin accessibility data (assay for transposase-accessible chromatin sequencing (ATAC-seq), DNase-seq) that reveals active regulatory regions, and chromosome conformation capture (Hi-C) that maps enhancer-promoter contacts (Chapter 17). Databases like ENCODE and the Roadmap Epigenomics Project provide regulatory annotations across cell types, though coverage varies dramatically by tissue (Chapter 2). Computational methods infer regulatory edges from expression correlations or sequence motifs, but such predictions contain substantial false positives and miss context-specific interactions.\nOrganized biochemical knowledge takes yet another form. KEGG, Reactome, and WikiPathways curate reactions, enzymatic steps, and signaling cascades into hierarchical pathway and metabolic networks where nodes can represent genes, proteins, metabolites, or abstract pathway concepts. These networks encode decades of molecular biology knowledge but reflect historical research priorities: metabolism and signal transduction are well-characterized, while more recently discovered processes like autophagy or RNA modification have sparser coverage.\nBeyond molecular interactions, relationships among genes, diseases, drugs, phenotypes, and other biomedical entities require heterogeneous representations. Unlike protein interaction networks, which contain a single node type and edge type, knowledge graphs are inherently heterogeneous: nodes represent diverse entity classes, and edges capture semantically distinct relationship types. This heterogeneity enables richer reasoning but demands architectures capable of handling multiple node and edge embeddings.\nSeveral large-scale biomedical knowledge graphs have become standard resources. Hetionet integrates 47,031 nodes across 11 types (genes, diseases, compounds, anatomies, and others) with 2.25 million edges spanning 24 relationship types, providing a comprehensive substrate for computational drug repurposing (himmelstein2017systematic?). The Unified Medical Language System (UMLS) aggregates over 200 biomedical vocabularies into a metathesaurus linking millions of concepts through hierarchical and associative relationships. PrimeKG consolidates 17 biological databases into a precision medicine knowledge graph with over 4 million relationships connecting diseases, drugs, genes, pathways, and phenotypes, explicitly designed for machine learning applications (chandak2023primekg?).\nDisease-gene association databases provide critical edges for clinical applications. DisGeNET curates over one million gene-disease associations from expert-reviewed sources, GWAS catalogs (Chapter 3), and text mining, assigning evidence scores that enable confidence-based filtering (pinero2020disgenet?). OMIM (Online Mendelian Inheritance in Man) provides authoritative curation of Mendelian disease genes, while OrphaNet focuses on rare diseases with detailed phenotypic annotations (Chapter 26). The Clinical Genome Resource (ClinGen) adds expert-reviewed gene-disease validity assessments using standardized evidence frameworks.\nDrug-centric resources complete the translational picture. DrugBank provides comprehensive drug-target annotations with mechanism and pharmacology details. ChEMBL aggregates bioactivity data from medicinal chemistry literature, linking compounds to protein targets through binding affinity measurements. The Drug Gene Interaction Database (DGIdb) consolidates druggable gene categories and known interactions to support target prioritization (Chapter 27).\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 18.1: [Essential] Network landscape. Panel A (PPI Networks): Undirected edges (physical binding); STRING, BioGRID, IntAct; “20-30% catalogued.” Panel B (Gene Regulatory Networks): Directed transcription factor (TF) → target edges; ChIP-seq, ATAC-seq, motifs; cell-type specific. Panel C (Knowledge Graphs): Multiple node types (genes, diseases, drugs, pathways); multiple edge types; multi-hop reasoning; Hetionet example. Panel D (Spatial Graphs): Nodes as cells/spots; edges as proximity/communication; from spatial transcriptomics; emerging and sparse.\n\n\n\nThe power of knowledge graphs lies in their support for multi-hop reasoning. A query asking whether a drug might treat a disease can traverse multiple edge types: drug inhibits protein A, protein A interacts with protein B, protein B is implicated in disease. Each hop contributes evidence, and the combination of paths provides signal that no single edge contains. Graph neural networks learn to aggregate across such paths, weighting different relationship types and path lengths according to their predictive value for specific tasks.\nSpatially resolved transcriptomics and imaging data give rise to graphs capturing tissue organization invisible to bulk or even single-cell measurements (Chapter 16). In these spatial and cell-cell interaction graphs, nodes represent cells or spatial locations, while edges encode physical proximity or inferred ligand-receptor communication. Such graphs enable questions about how spatial context influences cell behavior.\n\n\n18.1.2 Biases and Limitations\nAll biological networks share systematic biases that affect downstream modeling. Well-studied genes appear as highly connected hubs not necessarily because they have more interactions but because researchers have investigated them more thoroughly. This ascertainment bias means that GNNs trained on network structure may primarily learn to propagate signals toward well-characterized genes, potentially missing novel biology in peripheral network regions.\nNetwork incompleteness creates particular challenges for message passing algorithms. If a critical interaction is missing, information cannot flow across that gap. If a spurious interaction is present, noise propagates where it should not. These issues are especially acute for less-studied organisms, tissues, or disease contexts where network coverage is sparse.\nThe distinction between physical and functional associations matters for interpretation. A protein-protein interaction might represent stable complex membership, transient signaling, or indirect association through shared binding partners. Different edge types may warrant different treatment by graph models, but many databases conflate these categories or provide insufficient metadata to distinguish them.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-networks.html#sec-ch18-gnn-fundamentals",
    "href": "part_4/p4-ch18-networks.html#sec-ch18-gnn-fundamentals",
    "title": "18  Graph and Network Models",
    "section": "18.2 Graph Neural Network Fundamentals",
    "text": "18.2 Graph Neural Network Fundamentals\nThe mathematical machinery underlying graph neural networks differs fundamentally from the architectures examined in previous chapters. Where convolutional and transformer models operate on regular structures (sequences, grids), GNNs must handle irregular topology with variable-degree nodes, no inherent ordering, and arbitrary connectivity (Chapter 6, Chapter 7). This section develops the message passing framework that addresses these challenges, then surveys the canonical architectures that have become standard tools for biological applications.\n\n18.2.1 Message Passing Principles\nThe challenge of learning from graph-structured data lies in the irregular topology: unlike images (regular grids) or sequences (linear chains), graphs have variable-degree nodes, no inherent ordering, and complex connectivity patterns. Classical approaches computed hand-crafted features such as degree centrality, clustering coefficients, or shortest path statistics, then fed these to standard machine learning models. Such features capture useful properties but cannot adapt to task-specific patterns.\nMessage passing provides a learnable alternative. The core intuition is local information exchange: each node should update its representation based on what its neighbors know. By iterating this process across multiple layers, information propagates across the graph, allowing nodes to incorporate signals from increasingly distant parts of the network.\nFormally, at layer \\(\\ell\\), each node \\(i\\) maintains a hidden state \\(\\mathbf{h}_i^{(\\ell)}\\). A message passing layer computes, for each edge from neighbor \\(j\\) to node \\(i\\), a message:\n\\[\n\\mathbf{m}_{ij}^{(\\ell)} = \\phi_m\\left(\\mathbf{h}_i^{(\\ell)}, \\mathbf{h}_j^{(\\ell)}, \\mathbf{e}_{ij}\\right)\n\\]\nwhere \\(\\phi_m\\) is a learned function and \\(\\mathbf{e}_{ij}\\) represents edge features. The node then aggregates messages from all neighbors and updates its state:\n\\[\n\\mathbf{h}_i^{(\\ell+1)} = \\phi_h\\left(\\mathbf{h}_i^{(\\ell)}, \\bigoplus_{j \\in \\mathcal{N}(i)} \\mathbf{m}_{ij}^{(\\ell)}\\right)\n\\]\nwhere \\(\\mathcal{N}(i)\\) denotes neighbors of node \\(i\\) and \\(\\bigoplus\\) is a permutation-invariant aggregation (sum, mean, max, or attention-weighted combination). The aggregation must be permutation-invariant because neighbors have no inherent ordering.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 18.2: [High] Step-by-step message passing. Panel A (Initial State): 5-node graph; each with initial features from FM. Panel B (Message Computation): For each edge, compute message \\(m_{ij} = \\phi(h_i, h_j, e_{ij})\\); visualize as arrows. Panel C (Aggregation): Each node aggregates incoming (sum, mean, max, attention); new representation. Panel D (After \\(L\\) Layers): Each embedding incorporates \\(L\\)-hop neighborhood; “Gene embedding now reflects pathway context.” Mathematical summary at bottom.\n\n\n\nAfter \\(L\\) layers, a node’s representation incorporates information from all nodes within \\(L\\) hops. For biological networks, this means a gene’s learned embedding can reflect not only its own features but signals from interaction partners, their partners, and so on, capturing pathway-level and module-level context.\n\n\n18.2.2 Canonical Architectures\nSeveral GNN architectures have become standard tools for biological applications, each with distinct design choices that reflect different tradeoffs between computational efficiency, expressive power, and scalability.\nThe simplest approach performs normalized neighborhood averaging followed by linear transformation and nonlinearity. Graph convolutional networks (GCN) are computationally efficient and conceptually straightforward but suffer from over-smoothing when stacked deeply: repeated averaging causes node representations to converge, losing the discriminative signal that distinguishes different network positions (li2018deeper?; oono2019graph?).\nScalability to large graphs requires a different strategy. GraphSAGE learns aggregation functions that operate on sampled neighborhoods rather than the full neighbor set (hamilton2017inductive?). This enables mini-batch training on large graphs and provides inductive capability: the model can generate embeddings for nodes not seen during training by applying learned aggregators to their neighborhoods. For biological networks that grow as new genes are characterized, this generalization is valuable.\nWhen some neighbors matter more than others, attention-weighted aggregation provides a learnable solution. Graph attention networks (GAT) compute attention scores between each node and its neighbors, allowing the model to focus on the most informative interactions (velickovic2018graph?). This is analogous to attention in transformers but operates over graph neighborhoods rather than sequence positions (Chapter 7).\nFinally, the boundary between sequence and graph models blurs when transformer architectures extend to graphs. Graph transformers replace local message passing with structured or global attention. Some variants attend over all node pairs with positional encodings derived from graph structure (shortest paths, Laplacian eigenvectors); others restrict attention to k-hop neighborhoods (ying2021graphormer?; dwivedi2021graph?). These architectures potentially capture long-range dependencies that multi-layer message passing struggles to propagate.\nThe expressiveness of GNNs is bounded by their ability to distinguish different graph structures. Theoretical analysis connects standard message passing to the Weisfeiler-Lehman graph isomorphism test, revealing that certain graph structures remain indistinguishable regardless of the number of layers (xu2019how?; morris2019weisfeiler?). For most biological applications, this theoretical limitation is less constraining than practical issues of data quality, training efficiency, and interpretability (Chapter 24).",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-networks.html#sec-ch18-fm-embeddings",
    "href": "part_4/p4-ch18-networks.html#sec-ch18-fm-embeddings",
    "title": "18  Graph and Network Models",
    "section": "18.3 Foundation Model Embeddings as Node Features",
    "text": "18.3 Foundation Model Embeddings as Node Features\nThe power of combining foundation models with graph neural networks lies in their complementary strengths. Foundation models extract rich biological information from sequence, but they operate on isolated entities without relational context. Graph neural networks reason over relationships, but they require informative node features to propagate meaningful signal. This section examines how to integrate these approaches effectively, from the architectural principle underlying the combination to practical patterns for implementation.\n\n18.3.1 Integration Principle\nThe central architectural insight for genomic graph learning is that foundation models and graph neural networks operate at complementary levels of abstraction. Sequence-based foundation models excel at extracting biological information from linear sequences: ESM-2 learns evolutionary constraints and structural propensities from protein sequences (Chapter 12); DNABERT and its successors capture regulatory motifs and sequence grammar (Chapter 11); single-cell foundation models like scGPT learn cell state representations from expression profiles (Chapter 16). These representations encode rich biological knowledge but operate on individual entities without explicit relational information. The principles of feature extraction from pretrained models, which underpin this integration pattern, are developed in Section 9.3.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 18.3: [Essential] Central concept. Panel A (Foundation Models Produce Representations): ESM-2 → protein embeddings; DNABERT → DNA embeddings; scGPT → cell embeddings; “Rich representations of individual entities.” Panel B (Biological Networks Encode Relationships): PPI, regulatory, cell-cell communication; “Relational structure not in sequence.” Panel C (GNNs Integrate Both): Node features from FMs; edges from networks; message passing refines representations; context-aware outputs. Panel D (Capabilities Neither Achieves Alone): FM can not model interactions; networks alone limited features; combined enables disease gene prioritization, drug-target prediction.\n\n\n\nGraph neural networks excel at learning from relational structure but require informative node features to propagate. When node features are uninformative (simple one-hot encodings or scalar expression values), message passing can only learn from network topology. When node features carry substantial biological signal, message passing can refine and contextualize that information based on network position.\nCombining these approaches follows a natural two-stage pattern. First, apply a foundation model to each entity in the graph to generate initial node embeddings. For a protein-protein interaction network, run ESM-2 on each protein sequence; for a gene regulatory network, use DNA embeddings for regulatory elements and protein embeddings for transcription factors; for a cell graph, apply scGPT to generate cell state representations. Second, train a GNN on these embeddings using the biological graph structure, allowing message passing to integrate entity-level representations with relational context.\nThis combination yields capabilities that neither component achieves alone. The foundation model provides rich, transferable features that would require massive labeled datasets to learn from scratch. The GNN provides relational reasoning that sequence models cannot perform. A protein’s druggability depends both on intrinsic properties (binding pocket geometry, expression pattern) that ESM captures and on network context (pathway position, interaction partners) that the GNN integrates.\n\n\n18.3.2 Practical Integration Patterns\nSeveral integration patterns have emerged in practice, each suited to different constraints and objectives. The simplest approach freezes foundation model weights and treats embeddings as fixed features, training only the GNN layers. This is computationally efficient and prevents catastrophic forgetting of pretrained knowledge but limits the model’s ability to adapt representations to the specific task (Chapter 9).\nWhen sufficient task-specific data is available, allowing gradients to flow through both the GNN and (parts of) the foundation model enables end-to-end optimization. This joint fine-tuning typically requires careful learning rate scheduling, with smaller updates to foundation model parameters and larger updates to GNN layers. The approach can improve performance but risks overfitting and requires substantially more computation.\nA middle ground inserts small trainable modules between foundation model layers or at the interface between foundation model outputs and GNN inputs. Adapter-based integration provides task adaptation with modest parameter overhead, avoiding full fine-tuning costs while retaining flexibility (Chapter 9).\nThe granularity of representations also offers flexibility. For proteins, one might extract both per-residue embeddings (capturing local structure) and sequence-level embeddings (capturing global properties), concatenating these as node features. For regulatory networks, one might combine nucleotide-level DNA embeddings with region-level chromatin accessibility predictions. This multi-scale integration uses foundation model representations at multiple granularities to capture different aspects of biological function.\nThe choice of integration pattern depends on data availability, computational resources, and the degree of distribution shift between foundation model pretraining and the target application (Chapter 22). For well-characterized systems with substantial labeled data, joint fine-tuning may be warranted. For novel organisms or rare diseases with limited labels, frozen embeddings with simple GNN layers often generalize better.\n\n\n18.3.3 Evidence for the Integration Benefit\nEmpirical studies consistently demonstrate that foundation model embeddings improve GNN performance on biological tasks. In protein function prediction, ESM embeddings combined with PPI network GNNs substantially outperform either sequence-only or network-only baselines (lin2025gobeacon?). The improvement is particularly pronounced for proteins with few characterized interaction partners, where network structure alone provides limited signal but sequence features carry evolutionary information.\nFor disease gene prioritization, combining DNA and protein foundation model embeddings with multi-relational GNNs over heterogeneous biological networks improves ranking of causal genes from GWAS loci (saadat2024dna?; decarlo2023xgdag?). The foundation model features help distinguish genes with similar network positions based on sequence-level functional signals.\nIn single-cell analysis, scGPT embeddings combined with cell-cell communication graphs enable more accurate prediction of perturbation effects than either component alone (cui2023scgpt?; segceco2024?; cgcom2023?). The cell embeddings capture transcriptional state, while the graph structure encodes spatial and molecular interaction context.\nThese results suggest that the integration principle generalizes across biological domains. The specific foundation models and graph types vary, but the architectural pattern (rich entity embeddings + relational structure + message passing) consistently outperforms simpler alternatives.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-networks.html#sec-ch18-applications",
    "href": "part_4/p4-ch18-networks.html#sec-ch18-applications",
    "title": "18  Graph and Network Models",
    "section": "18.4 Applications",
    "text": "18.4 Applications\nThe integration of foundation model embeddings with graph neural networks enables applications across the translational pipeline. Disease gene prioritization leverages network context to identify causal genes from GWAS loci. Drug-target prediction exploits both sequence-derived druggability features and pathway positioning. Knowledge graph reasoning supports multi-hop inference for drug repurposing. Pathway analysis identifies dysregulated modules in patient-specific contexts. Each application follows the same architectural pattern (rich node features from foundation models, relational structure from biological networks, refinement through message passing) while addressing distinct biological questions.\n\n18.4.1 Disease Gene Prioritization\nGenome-wide association studies identify genomic loci associated with disease risk but rarely pinpoint causal genes (Chapter 3). A typical GWAS locus contains dozens of genes, most of which are passengers linked to the true causal variant through linkage disequilibrium. Identifying which gene(s) mediate the association requires integrating functional evidence with genetic signal.\nNetwork-based prioritization leverages the observation that disease genes cluster in biological networks. If a GWAS locus contains genes A, B, and C, and gene B interacts with five known disease genes while A and C interact with none, gene B becomes a stronger causal candidate. Graph neural networks formalize and extend this intuition, learning to propagate disease labels through networks and score candidate genes based on their network context.\nThe integration with foundation models strengthens this approach. Rather than relying solely on network topology, which favors well-studied hub genes, the model can assess each candidate’s intrinsic functional properties through sequence embeddings. A gene with protein features characteristic of disease-relevant functions (membrane localization, DNA binding, signaling domains) receives higher scores even if its network position is peripheral. This helps mitigate the ascertainment bias toward well-characterized genes that plagues purely topological methods.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 18.4: [High] GWAS follow-up. Panel A (The GWAS Challenge): Locus with genes A,B,C,D; lead SNP in LD with all; which is causal? Panel B (Network Context): Same genes in PPI; gene B connected to 5 known disease genes; A,C,D peripheral. Panel C (GNN Scoring): FM embeddings as node features; disease labels propagate; B receives higher network score. Panel D (Integration with Sequence Features): Gene C strong protein features; gene B network + moderate protein; combined identifies both candidates.\n\n\n\nClinical applications include rare disease diagnosis, where patient exome sequencing identifies hundreds of candidate variants and network-based scoring helps prioritize which genes to investigate further (Chapter 26). The approach also supports drug target identification by highlighting genes whose network position and functional properties make them amenable to therapeutic modulation (Chapter 27). For rare disease diagnosis, network-based prioritization integrates with the variant filtering pipelines in Section 26.1, where foundation model embeddings and network context jointly inform gene ranking.\n\n\n18.4.2 Drug-Target Interaction Prediction\nIdentifying which proteins a drug binds is fundamental to understanding mechanism and predicting side effects. Experimental screening of drug-target pairs is expensive and incomplete; computational prediction can prioritize candidates for validation.\nDrug-target interaction prediction naturally fits a graph framework. Construct a heterogeneous graph with drug nodes, protein nodes, and edges representing known interactions. Node features for proteins come from sequence foundation models; node features for drugs come from molecular encodings (fingerprints, learned representations from molecular graphs). Train a GNN to predict missing edges, learning which drug and protein features, combined with network context, indicate likely binding.\nThe foundation model integration is critical here. Protein embeddings from ESM capture binding pocket characteristics, domain structure, and evolutionary constraint that influence druggability. The graph structure provides context: if a drug binds protein A, and protein A participates in complex with protein B, then the drug may also affect protein B’s function. Multi-relational GNNs can learn different propagation patterns for different edge types (physical binding versus pathway membership versus sequence similarity), improving prediction accuracy.\nThis application connects to broader drug discovery workflows (Chapter 27), where target identification is one component of a multi-stage pipeline. GNN-based predictions provide hypotheses for experimental validation, accelerating the search for novel therapeutic targets.\n\n\n18.4.3 Knowledge Graph Reasoning and Drug Repurposing\nDrug repurposing seeks new therapeutic applications for existing compounds, exploiting the observation that drugs often affect multiple targets and pathways beyond their original indication. Knowledge graphs provide a natural framework for repurposing by encoding the relationships through which a drug’s effects might propagate to new disease contexts.\nThe repurposing problem can be framed as link prediction in a heterogeneous graph: given a knowledge graph with drugs, diseases, genes, and pathways as nodes, predict missing drug-treats-disease edges. Unlike direct drug-target prediction, this task requires reasoning across multiple relationship types. A candidate repurposing hypothesis might involve a chain such as: drug D binds protein P1, P1 regulates pathway W, pathway W is dysregulated in disease X, therefore D may treat X. Graph neural networks designed for heterogeneous graphs learn to aggregate evidence across such chains, weighting different metapaths (sequences of edge types) according to their predictive reliability. The drug repurposing applications that exploit this reasoning are detailed in Section 27.2.2.\nFoundation model embeddings strengthen knowledge graph reasoning in several ways. For gene and protein nodes, ESM embeddings encode functional properties that influence druggability and pathway membership. For disease nodes, embeddings derived from clinical text or phenotype ontologies capture symptom patterns and comorbidity relationships. For drug nodes, molecular representations from chemical language models or graph neural networks over molecular structure encode binding properties and pharmacokinetics. These rich node features allow the GNN to assess not just whether a path exists but whether the entities along that path have compatible functional characteristics.\nEmpirical results demonstrate the value of this integration. Models combining knowledge graph structure with foundation model embeddings outperform both topology-only approaches (which ignore node semantics) and embedding-only approaches (which ignore relational structure) on standard drug repurposing benchmarks (biomedkg2025?; dreamgnn2025?). The improvement is particularly pronounced for drugs and diseases with sparse direct evidence, where multi-hop reasoning through well-characterized intermediate entities provides the primary signal.\nClinical translation of knowledge graph predictions requires careful interpretation. A high-scoring drug-disease prediction indicates that multiple lines of computational evidence converge, not that efficacy has been established. The paths contributing to predictions provide mechanistic hypotheses that can guide experimental validation: if the model relies heavily on a drug-protein-pathway-disease chain, that pathway becomes a candidate biomarker for patient selection or treatment response monitoring. Several repurposing candidates identified through knowledge graph methods have entered clinical trials, though the approach remains most valuable for hypothesis generation rather than definitive target validation (stebbing2020mechanism?; richardson2020baricitinib?).\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 18.5: [High] Knowledge graph reasoning. Panel A (KG Structure): Heterogeneous graph with drugs, proteins, diseases, pathways; multiple edge types. Panel B (Multi-Hop Reasoning Path): Drug D → binds → Protein P1 → participates_in → Pathway W → disrupted_in → Disease X. Panel C (Link Prediction): Learn embeddings; predict missing edges; score drug-disease pairs; new indication discovery. Panel D (FM Enhancement): Replace node features with FM embeddings; capture functional similarity beyond database annotations.\n\n\n\n\n\n18.4.4 Pathway and Module Analysis\nIndividual genes rarely act alone; biological function emerges from coordinated activity of gene sets organized into pathways and functional modules. Patient-specific pathway analysis identifies which modules show coordinated dysregulation, providing mechanistic insight beyond single-gene associations.\nGNNs enable pathway analysis that respects network structure rather than treating gene sets as independent members. By propagating patient-specific expression or mutation signals through pathway graphs, models can identify which subnetworks show coherent perturbation. This differs from classical gene set enrichment, which tests for overrepresentation without considering internal pathway topology.\nFoundation model features strengthen pathway analysis by providing functional context for each gene. A gene with features indicating chromatin regulation may contribute to pathway dysfunction through different mechanisms than one with features indicating membrane signaling. The GNN learns to weight these contributions based on network position and functional annotation, identifying pathway perturbations that purely topological or purely gene-set methods miss.\n\n\n18.4.5 Cell Type and State Annotation\nSingle-cell foundation models generate rich representations of individual cells (Chapter 16), but many biological questions involve relationships between cells: which cells communicate, how spatial neighborhoods influence behavior, which cell types co-occur in disease states.\nGraph neural networks over cell-cell interaction graphs enable several applications. Cell type annotation propagates labels from well-characterized cells to ambiguous ones based on expression similarity and spatial proximity. Perturbation response prediction models how signals from perturbed cells propagate to neighbors. Tissue region classification identifies coherent spatial domains (tumor, stroma, immune infiltrate) based on local cell compositions.\nThe foundation model integration follows the standard pattern: scGPT or similar models generate cell embeddings, spatial proximity or inferred ligand-receptor interactions define edges, and GNN message passing refines cell representations based on neighborhood context. The resulting embeddings capture both intrinsic cell state and extrinsic spatial/communicative context, enabling predictions that purely expression-based or purely spatial models cannot make.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-networks.html#sec-ch18-practical",
    "href": "part_4/p4-ch18-networks.html#sec-ch18-practical",
    "title": "18  Graph and Network Models",
    "section": "18.5 Practical Considerations",
    "text": "18.5 Practical Considerations\nDeploying graph neural networks on biological data requires navigating choices that profoundly affect model behavior. Graph construction determines what relationships the model can exploit. Scalability strategies determine whether training is feasible on large networks. Robustness techniques determine whether predictions generalize beyond well-characterized network regions. Interpretation methods determine whether outputs provide actionable biological insight. The following subsections address each consideration in turn.\n\n18.5.1 Graph Construction Quality\nThe impact of graph construction choices cannot be overstated. A GNN can only learn from relationships encoded in its input graph; missing edges prevent information flow, spurious edges introduce noise, and biased edge sets propagate ascertainment artifacts.\nSource selection involves tradeoffs between precision and completeness. Curated databases like BioGRID provide high-confidence interactions but miss most true relationships. Computational predictions from STRING or co-expression analysis are more comprehensive but noisier. The appropriate choice depends on the downstream task: high-precision networks may be preferable when false positives are costly, while high-recall networks enable discovery of novel biology at the risk of chasing artifacts.\nThresholding decisions determine network density. Confidence scores or distance metrics allow continuous edge weights, but many GNN implementations require discrete edges or work better with relatively sparse graphs. Cross-validation over threshold values or principled selection criteria (target edge density, ensure graph connectivity) help navigate this choice.\nFor heterogeneous graphs, schema design (which node types exist, which edge types connect them) encodes strong assumptions about relevant biology. A knowledge graph that separates genes, transcripts, and proteins as distinct node types enables fine-grained reasoning but requires more training data than a simpler gene-only representation.\n\n\n18.5.2 Scalability and Mini-Batching\nBiological graphs range from thousands of nodes (a single-patient cell graph) to millions (a comprehensive knowledge graph or large spatial transcriptomics dataset). Full-batch training, where the entire graph is processed simultaneously, becomes infeasible at scale due to memory constraints.\nMini-batching strategies partition computation into manageable pieces. Neighborhood sampling (GraphSAGE-style) restricts message passing to a fixed sample of neighbors per node, enabling node-level mini-batches. Subgraph sampling trains on induced subgraphs corresponding to meaningful units (individual pathways, tissue regions, patient subsets). Cluster-based training partitions the graph into communities, processes each independently, and handles cross-cluster edges in a second pass.\nFor foundation model integration, computational cost compounds: generating embeddings for millions of proteins or cells may itself be expensive. Pre-computing and caching embeddings is often practical, decoupling the foundation model forward pass from GNN training. When embeddings must be computed on-the-fly (for dynamic features or joint fine-tuning), careful batching and gradient checkpointing become essential.\n\n\n18.5.3 Robustness to Noise and Missingness\nAll biological networks contain errors. Experimental methods for detecting interactions have false positive and false negative rates; computational predictions rely on imperfect proxies; even curated databases contain mistakes. GNNs must tolerate this noise to be practically useful.\nRandomly masking edges during training forces the model to avoid relying on any single interaction. This edge dropout improves robustness to missing or incorrect edges and serves as a form of regularization. Similarly, masking node features or entire nodes through node dropout prevents overfitting to well-connected hubs.\nEnsemble methods train multiple GNNs on different network subsamples or with different random initializations, aggregating predictions to reduce variance from network noise. Bayesian GNNs provide uncertainty estimates that flag low-confidence predictions for manual review (Chapter 23).\nEvaluation should explicitly assess robustness by testing on held-out edges, nodes from poorly characterized network regions, or networks constructed from different data sources than training. A model that performs well only on hub genes or well-characterized interactions may fail in precisely the scenarios where computational prediction is most needed (Chapter 21).\n\n\n18.5.4 Interpretation and Validation\nA key advantage of graph models is interpretability: the graph structure itself provides a scaffold for understanding predictions (Chapter 24). Several techniques extract biological insight from trained GNNs.\nAttention weights in GAT and graph transformer models indicate which neighbors most influenced each node’s prediction. Aggregating attention across predictions can highlight critical edges or subgraphs, suggesting which interactions drive model behavior. For cases where some neighbors matter more than others, this attention weight analysis reveals learned priorities.\nComputing how predictions change with respect to node or edge features identifies which parts of the input most affect outputs. Gradient-based attribution methods such as integrated gradients provide smoother, more reliable attributions than raw gradients.\nSystematically removing edges, masking nodes, or perturbing features and observing prediction changes reveals which graph elements are necessary for specific predictions. This counterfactual analysis can identify model vulnerabilities and generate testable hypotheses about which interactions are essential.\nProjecting learned node representations into two dimensions using Uniform Manifold Approximation and Projection (UMAP) or t-distributed stochastic neighbor embedding (t-SNE) reveals clusters that may correspond to functional categories, cell types, or disease subtypes. Comparing embedding visualizations across conditions identifies network regions that show context-specific changes.\nInterpretation is not an afterthought but a central goal. The most impactful applications are those where GNN predictions generate testable hypotheses about biological mechanism, ultimately validated by experiment. Attention weights highlighting a regulatory edge or gradient attribution implicating a signaling pathway should prompt follow-up experiments, not immediate clinical action.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-networks.html#sec-ch18-limitations",
    "href": "part_4/p4-ch18-networks.html#sec-ch18-limitations",
    "title": "18  Graph and Network Models",
    "section": "18.6 Limitations and Open Challenges",
    "text": "18.6 Limitations and Open Challenges\nGraph neural networks inherit the biases and limitations of their input networks. Network incompleteness means critical relationships may be absent. Ascertainment bias means well-studied genes dominate predictions. Correlational structure may not reflect causal mechanisms. These limitations do not invalidate the approach but constrain its appropriate use and interpretation.\n\n18.6.1 Study Bias Problem\nNetwork-based methods inherit the biases of their input networks. Well-studied genes appear as hubs; poorly characterized genes are peripheral or disconnected. GNNs trained on such networks learn to propagate signals toward well-characterized genes, effectively recapitulating rather than extending existing knowledge.\nThis creates particular problems for disease gene discovery, where the goal is often to identify previously unrecognized genes. A model that consistently ranks known disease genes highly may simply be exploiting their network prominence rather than learning generalizable disease biology. Careful evaluation on temporal holdouts (genes characterized after training data was assembled) or stratified by network degree can reveal whether models truly generalize (Chapter 21). The systematic approaches for detecting and quantifying such confounding patterns are detailed in Section 22.8.\nMitigation strategies include degree-corrected training objectives, explicit modeling of ascertainment bias, or alternative network constructions that reduce dependence on historical research focus. None fully solves the problem, which reflects fundamental data limitations rather than algorithmic shortcomings.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 18.6: [Enhancing] Study bias visualization. Node degree vs. publication count showing strong correlation; well-studied genes (TP53, BRCA1) highly connected; understudied genes peripheral; risk: GNN prioritizes well-studied genes; mitigation strategies (degree normalization, attention to edge confidence).\n\n\n\nfigs/part_4/ch19/05-A-fig-information-cascade.png ### Causality Versus Association {#sec-ch18-causality}\nNetwork edges typically represent associations (two proteins bind, two genes correlate) rather than causal relationships (perturbing gene A changes gene B). GNNs learn to exploit correlational patterns, which may not correspond to causal mechanisms.\nFor applications like drug target identification, this distinction matters enormously. A gene that correlates with disease through confounding may be a poor target despite high network-based prioritization scores. Integrating causal inference methods with graph learning is an active research area, but current GNN applications should be interpreted as identifying associations worthy of experimental follow-up rather than establishing causal relationships.\n\n\n18.6.2 Negative Data and Class Imbalance\nMost biological network datasets encode only positive relationships: known interactions, confirmed regulatory edges, documented associations. The absence of an edge may indicate true non-interaction or simply lack of evidence. This creates severe class imbalance for edge prediction tasks and makes negative sampling strategies critical (Chapter 22).\nRandom negative sampling (assuming absent edges represent non-interactions) is common but biologically unrealistic. More sophisticated approaches sample negatives with matched properties (same degree distribution, similar node features) to create harder and more meaningful contrasts. Evaluation should report performance separately on different negative sampling schemes to assess whether models generalize beyond easily discriminated negatives (Chapter 20).\n\n\n18.6.3 Distribution Shift\nA GNN trained on one biological network (human PPI from STRING) may not transfer to another (mouse regulatory network, patient-specific spatial graph). Foundation model embeddings help by providing transferable features, but network structure differences can still break performance.\nApplying models across species is particularly challenging: network topology, edge type distributions, and gene function may all differ between organisms. Cross-tissue or cross-disease transfer poses similar challenges. Explicit domain adaptation methods, multi-task training across related networks, or foundation model fine-tuning on target domains can help but add complexity (Chapter 9).",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-networks.html#sec-ch18-conclusion",
    "href": "part_4/p4-ch18-networks.html#sec-ch18-conclusion",
    "title": "18  Graph and Network Models",
    "section": "18.7 Sequence Encodes, Structure Connects",
    "text": "18.7 Sequence Encodes, Structure Connects\nGraph neural networks operate at a complementary level of abstraction to sequence-based foundation models. Foundation models learn rich representations of biological entities from sequence data; graph neural networks learn to reason about relationships between those entities. Combining them follows a natural pattern: generate embeddings with foundation models, then refine them through message passing over graph structure. This integration yields capabilities that neither component achieves alone, propagating information across protein interaction networks, regulatory pathways, and spatial neighborhoods in ways that sequence models cannot represent.\nThe central insight is that biological knowledge exists at multiple scales. Sequence encodes what individual genes and proteins can do; networks encode how they interact to produce cellular function. GNNs translate the relational structure of biological networks into learnable inductive biases, enabling disease gene prioritization through network propagation, drug target prediction through pathway context, and spatial analysis through tissue graphs. The improvements over sequence-only approaches are consistent across applications, demonstrating that relational context adds genuine information beyond what sequence representations capture.\nYet network structure carries its own biases. Protein interaction databases are enriched for well-studied genes and disease-relevant pathways; less-characterized genes have fewer annotated interactions regardless of their biological importance. Correlation between genes does not imply regulatory relationship. Class imbalance between known disease genes and the genome-wide background reflects research history as much as biology. These biases propagate through GNN predictions, creating systematic patterns in what the models emphasize and what they miss. The multi-omics integration examined in Chapter 19 extends graph-based reasoning to additional modalities. Clinical applications in Chapter 25 leverage network-derived features for risk stratification, while Chapter 26 applies network propagation to gene prioritization workflows. Both depend on understanding where network-derived predictions are trustworthy and where they inherit the limitations of their inputs, challenges that the uncertainty quantification methods in Section 23.4 help address.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-multi-omics.html",
    "href": "part_4/p4-ch19-multi-omics.html",
    "title": "19  Multi-Omics Integration",
    "section": "",
    "text": "19.1 Limits of Single-Modality Models\nCombining data types should improve prediction. If genomic variants provide one signal and transcriptomic measurements provide another, their combination ought to be more informative than either alone. This intuition, while reasonable, proves frequently wrong in practice. Naive concatenation of multi-omics features often degrades performance relative to single-modality models. Noise from uninformative features overwhelms signal from informative ones. Batch effects between modalities create spurious correlations that models exploit. The curse of dimensionality intensifies when features from multiple assays are stacked without principled integration. The paradox is real: more data can mean worse predictions, and understanding why is prerequisite to making multi-omics integration work.\nEach molecular layer captures part of the biological story but not all of it. Genomic variants identify predisposition; transcriptomics reveals which genes respond; proteomics shows which proteins change; metabolomics measures downstream biochemical consequences. A patient with a BRCA1 variant may show altered DNA repair gene expression, deficient homologous recombination protein activity, and characteristic metabolic signatures. No single layer provides the complete picture. Effective integration traces this causal cascade from genetic variation through molecular intermediates to clinical phenotype, distinguishing primary effects from downstream consequences and noise from signal.\nThe integration strategy matters as much as the data itself. Early fusion concatenates features before modeling, intermediate fusion learns joint representations across modalities, and late fusion combines predictions from modality-specific models. Each approach carries distinct tradeoffs for different applications and data characteristics. Multi-omics foundation models attempt to learn unified representations across genomics, transcriptomics, proteomics, and other modalities simultaneously, while clinical integration extends further still, combining electronic health records, imaging data, and molecular measurements for patient-level prediction. The practical challenges are substantial: missing modalities when not every patient has every assay, batch effects from technical variation between measurement platforms, and a persistent gap between multi-omics potential and deployment reality.\nEach molecular layer tells an incomplete story. DNA sequence is static; it encodes potential but not state. A variant’s presence says nothing about whether the gene is expressed, whether the protein is active, or whether the pathway is perturbed. Transcriptomic data captures expression state but misses post-transcriptional regulation, protein modifications, and metabolic flux. Proteomic measurements reveal protein abundance but not necessarily activity or localization. Methylation profiles indicate epigenetic state but require expression data to understand functional consequences.\nThe incompleteness becomes concrete when modeling complex traits. Genome-wide association studies explain perhaps 10-20% of heritability for most common diseases through identified variants [Citation Needed]. Adding expression quantitative trait loci (eQTLs) improves fine-mapping by suggesting which variants affect gene expression (see Section 3.4), but many causal mechanisms operate through splicing, translation, or post-translational modification rather than expression level. Single-cell RNA sequencing reveals cellular heterogeneity invisible to bulk measurements, but the same cell cannot simultaneously undergo RNA-seq and assay for transposase-accessible chromatin sequencing (ATAC-seq), forcing computational integration across modalities measured in different cells (see Chapter 16 for approaches to this challenge).\nConsider the challenge of predicting drug response. Germline variants in drug-metabolizing enzymes explain some inter-individual variation (see Section 2.8.4), but tumor-specific somatic mutations, expression programs, and microenvironment all influence therapeutic efficacy. A genomics-only model sees the inherited component; a transcriptomics-only model sees the current expression state; neither captures the full picture. Multi-omics integration promises to bridge these gaps by learning representations that span molecular layers.\nFoundation models address each molecular layer individually: sequence models predict regulatory effects from DNA (see Chapter 13), expression models capture transcriptional programs (see Chapter 15), and protein language models predict structure and function from amino acid sequence (see Chapter 12). Multi-omics integration asks how these modality-specific representations can be combined into unified patient or cell representations.\nThe promise comes with caveats. Adding modalities increases the number of parameters that must be estimated, potentially worsening overfitting when sample sizes are limited. Different modalities have different noise characteristics, batch structures, and missingness patterns. The same patient’s measurements across platforms may not align perfectly due to sample handling, timing, or technical variation. Naive concatenation of features often performs worse than single-modality models because the signal-to-noise ratio degrades when noisy features outnumber informative ones.\nThese challenges motivate careful consideration of integration strategy. The question is not whether to integrate, but how.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-multi-omics.html#sec-ch19-strategies",
    "href": "part_4/p4-ch19-multi-omics.html#sec-ch19-strategies",
    "title": "19  Multi-Omics Integration",
    "section": "19.2 Integration Strategies and Their Tradeoffs",
    "text": "19.2 Integration Strategies and Their Tradeoffs\nThree broad strategies have emerged for combining multi-omics data, each with distinct strengths and limitations.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 19.2: [Essential] Strategy comparison. Panel A (Early Fusion): All features concatenated → single model; pros: can learn cross-modal interactions; cons: curse of dimensionality, requires complete data. Panel B (Late Fusion): Separate models → predictions combined; pros: handles missing, modality-specific architectures; cons: cannot learn feature interactions. Panel C (Intermediate Fusion): Modality-specific encoders → shared latent space → task head; pros: flexibility + robustness; cons: alignment complexity. Summary table comparing cross-modal interactions, missing data handling, compute.\n\n\n\n\n19.2.1 Early Fusion\nEarly fusion concatenates features from multiple modalities before any modeling, creating a single high-dimensional input vector that contains genomic variants, expression values, methylation levels, and any other available measurements. A classifier or regressor then learns directly from this concatenated representation.\nThe appeal of early fusion lies in its simplicity and flexibility. Any downstream model architecture can operate on concatenated features, from linear regression to deep neural networks. The model can learn arbitrary interactions between features from different modalities, since all information is present in the input. Implementation requires only normalization and alignment of features across samples.\nThe limitations become apparent at scale. Dimensionality explodes when combining genome-wide variants (millions of features), gene expression (tens of thousands of genes), methylation (hundreds of thousands of CpG sites), and protein abundance (thousands of proteins). Most samples have far fewer observations than features, creating severe overfitting risk. Regularization helps but cannot fully compensate when the ratio of features to samples exceeds practical bounds.\nMissing data creates additional complications. If any modality is missing for a sample, early fusion requires either excluding that sample (reducing effective sample size) or imputing the missing modality (introducing noise and potential bias). Since multi-omics studies often have incomplete overlap between modalities, with some patients having genomics and transcriptomics but not proteomics, early fusion frequently operates on substantially reduced cohorts.\nScale differences between modalities pose another challenge. Expression values span orders of magnitude; methylation beta values range from zero to one; variant encodings are typically binary. Without careful normalization, modalities with larger variance can dominate the learned representation regardless of biological relevance. Batch effects within each modality add further complexity, since batch correction must precede concatenation but may interact with cross-modal relationships.\nDespite these limitations, early fusion remains appropriate when sample sizes are large relative to feature counts, when all modalities are available for all samples, and when the downstream task is well-defined enough to guide feature selection. Biobank-scale studies with thousands of participants and focused feature sets can succeed with early fusion approaches.\n\n\n19.2.2 Late Fusion\nLate fusion trains separate models for each modality and combines their predictions at the output level. A genomics model produces a risk score; a transcriptomics model produces another risk score; an ensemble method or meta-learner combines these modality-specific predictions into a final output.\nThis approach handles missing modalities gracefully. If a patient lacks proteomic data, the proteomics model simply does not contribute to the ensemble. Sample sizes for each modality-specific model can differ, since training requires only samples with that modality rather than complete multi-omics profiles. Each modality can use whatever architecture works best for its data type: deep networks for imaging, gradient boosting for tabular omics, convolutional architectures for sequence.\nLate fusion cannot capture cross-modal interactions at the feature level. If a variant’s effect on disease depends on expression level of a regulatory gene, neither the genomics model nor the transcriptomics model alone can detect this interaction. The ensemble sees only the modality-specific predictions, not the underlying features. This limitation is fundamental: late fusion assumes that each modality provides independent signal that can be additively combined.\nThe assumption of independence often fails in biological systems. Gene expression depends on genetic variants through eQTLs. Protein levels depend on both transcription and post-transcriptional regulation. Methylation states influence and are influenced by transcription. The molecular layers are not independent information sources but coupled components of a dynamic system. Late fusion ignores this coupling.\nCalibration presents a practical challenge. For ensemble predictions to be meaningful, the modality-specific models must produce well-calibrated probability estimates (see Section 23.2 for calibration methods). If the genomics model is overconfident and the transcriptomics model is underconfident, naive averaging produces biased predictions. Calibration techniques help but add complexity to the modeling pipeline.\nLate fusion works well when modalities genuinely provide independent signals, when sample sizes for each modality differ substantially, or when interpretability requires understanding each modality’s contribution separately. Clinical deployment often favors late fusion because it gracefully handles the reality that not all patients will have all measurements.\n\n\n19.2.3 Intermediate Fusion\nIntermediate fusion learns modality-specific encoders that map each data type into a shared latent space, then operates on the aligned representations for downstream tasks. This approach combines the flexibility of early fusion with the robustness of late fusion.\nEach modality has its own encoder architecture tailored to its characteristics. A variational autoencoder might encode single-cell expression data, handling sparsity and dropout noise. A convolutional network might process methylation profiles along chromosomal coordinates. A graph neural network might encode protein interaction data (see Section 18.2). These diverse architectures share nothing except their output dimensionality: all encoders produce embeddings in a common latent space.\nAlignment between modalities is encouraged through multiple mechanisms. Reconstruction losses require each encoder’s latent representation to support decoding back to the original features, ensuring that the embeddings retain modality-specific information. Contrastive terms pull together representations of the same biological entity across modalities: the expression embedding for a cell should be similar to the ATAC-seq embedding for the same cell. Graph constraints enforce consistency with known biological relationships: genes connected in interaction networks should have similar embeddings.\nThe shared latent space enables cross-modal reasoning. A classifier operating on the shared space can learn interactions between genomic and transcriptomic features, since both are present in the same representation. Transfer becomes possible: a model trained on expression data can be applied to samples with only ATAC-seq by encoding through the ATAC-seq encoder into the shared space.\nMissing modalities no longer require imputation or exclusion. If a sample lacks proteomics, only the available encoders fire, producing a partial representation in the shared space. The downstream model operates on whatever representation is available, degrading gracefully as modalities are missing rather than failing entirely.\nGLUE, introduced in Section 16.5.2 for single-cell multi-omics integration, exemplifies this approach. Separate variational autoencoders encode RNA-seq and ATAC-seq data into a shared cell embedding space. A feature graph links ATAC-seq peaks to genes based on genomic proximity and transcription factor binding, providing biological constraints on the alignment. The result enables integration of measurements from different cells, not just different modalities in the same cell.\nIntermediate fusion dominates modern multi-omics deep learning because it balances flexibility with robustness. The modality-specific encoders can be pretrained on large single-modality datasets, then fine-tuned for alignment (see Chapter 9 for transfer learning strategies). New modalities can be added by training new encoders without retraining existing components. The shared space provides a natural target for interpretation and visualization.\nThe approach is not without limitations. The quality of alignment depends heavily on the training objective and the availability of paired samples where multiple modalities are measured in the same biological entity. Without sufficient anchoring, the shared space may fail to capture true biological correspondence. Hyperparameter choices for balancing reconstruction against alignment losses require careful tuning.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 19.3: [High] Detailed architecture. Panel A (Modality-Specific Encoders): Expression VAE, methylation CNN, proteomics MLP; each tailored to modality. Panel B (Shared Latent Space): All encoders output same dimensionality; alignment via reconstruction loss, contrastive loss, graph constraints. Panel C (Downstream Task Head): Classifier/regressor on latent representation; can use partial representations. Panel D (Missing Modality Handling): Complete sample all encoders fire; partial sample available encoders only; graceful degradation.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-multi-omics.html#sec-ch19-foundation-models",
    "href": "part_4/p4-ch19-multi-omics.html#sec-ch19-foundation-models",
    "title": "19  Multi-Omics Integration",
    "section": "19.3 Multi-Omics Foundation Models",
    "text": "19.3 Multi-Omics Foundation Models\nThe foundation model paradigm, introduced in Chapter 10, extends naturally to multi-omics settings. Rather than training task-specific models that integrate modalities for a single downstream application, multi-omics foundation models learn general-purpose representations that transfer across tasks.\n\n19.3.1 Factor-Based Integration\nMulti-Omics Factor Analysis (MOFA and its successor MOFA+) provides a probabilistic framework for learning shared and modality-specific factors from multi-omics data [Citation Needed]. The approach assumes that observed measurements across modalities can be explained by a small number of latent factors, some shared across modalities and others specific to individual data types.\nMOFA+ extends this framework to handle multiple sample groups (such as different tissues or conditions), non-Gaussian likelihoods appropriate for count data, and scalable inference for large datasets [Citation Needed]. The factors learned by MOFA+ capture sources of variation that span modalities, enabling biological interpretation: a factor that loads heavily on inflammatory genes in expression data and on hypomethylation at immune loci in methylation data suggests coordinated epigenetic-transcriptional regulation of inflammation.\nWhile MOFA+ is not a deep learning method in the strict sense, its factor-based decomposition provides a foundation for understanding what multi-omics integration should capture. The shared factors correspond to biological processes that manifest across molecular layers; the modality-specific factors capture technical variation or layer-specific biology.\n\n\n19.3.2 Deep Generative Multi-Omics Models\ntotalVI (Total Variational Inference) integrates protein abundance from CITE-seq with gene expression in single-cell data through a hierarchical Bayesian model [Citation Needed]. The approach learns a joint latent space that captures cell state while properly modeling the distinct noise characteristics of RNA and protein measurements. Protein abundance follows a negative binomial distribution with technical factors including background binding; RNA counts follow a zero-inflated negative binomial accounting for dropout.\nThe generative model structure enables imputation of missing modalities. Given RNA expression alone, totalVI can predict expected protein abundance by sampling from the learned joint distribution. This imputation is not mere correlation-based prediction but reflects the full posterior distribution over protein levels given expression.\nMultiVI extends this framework to integrate gene expression with chromatin accessibility [Citation Needed]. The model learns to align measurements from different cells, enabling construction of unified cell atlases from studies that measured different modalities. The alignment relies on the biological assumption that gene expression and chromatin state reflect the same underlying cell state, even when measured in different cells.\nThese Bayesian deep generative models exemplify intermediate fusion with principled uncertainty quantification. The posterior distributions over latent variables capture not just point estimates but confidence in the learned representations (see Chapter 23 for uncertainty quantification methods). This property becomes important for clinical applications where prediction uncertainty must inform decision-making.\n\n\n19.3.3 Contrastive Multi-Modal Learning\nContrastive learning provides another path to multi-omics integration. The CLIP model for vision-language demonstrated that contrastive objectives can align embeddings from fundamentally different data types (images and text) into a shared space [Citation Needed]. Similar approaches apply to biological modalities.\nThe contrastive objective is straightforward: embeddings of the same biological entity across modalities should be similar, while embeddings of different entities should be dissimilar. A cell’s expression embedding should be close to its methylation embedding and far from other cells’ methylation embeddings. A patient’s genomic embedding should be close to their transcriptomic embedding across the cohort.\nThis objective requires paired samples for training: the same cells or patients measured across modalities. Anchor pairs define the positive examples; negative examples come from non-matching pairs within a batch. The encoders learn to produce embeddings where cross-modal correspondence emerges from training dynamics rather than explicit feature engineering.\nContrastive approaches scale well and can incorporate foundation model encoders pretrained on single modalities. An expression encoder pretrained on millions of cells via masked gene prediction can be fine-tuned with contrastive objectives to align with an ATAC-seq encoder. The pretraining provides rich initial representations; the contrastive fine-tuning establishes cross-modal correspondence (see Section 8.5 for contrastive pretraining strategies).",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-multi-omics.html#sec-ch19-clinical-integration",
    "href": "part_4/p4-ch19-multi-omics.html#sec-ch19-clinical-integration",
    "title": "19  Multi-Omics Integration",
    "section": "19.4 Clinical Integration: EHR, Imaging, and Molecular Data",
    "text": "19.4 Clinical Integration: EHR, Imaging, and Molecular Data\nThe ultimate goal of multi-omics modeling for many applications is patient-level prediction: disease risk, treatment response, prognosis. Achieving this goal requires integrating molecular measurements with clinical data that directly captures patient state and outcomes.\n\n19.4.1 Electronic Health Records as a Modality\nElectronic health records contain decades of longitudinal observations for millions of patients: diagnoses, procedures, medications, laboratory values, vital signs, clinical notes. This wealth of phenotypic information complements molecular data by capturing disease manifestation rather than molecular mechanism.\nIntegrating EHR with genomics poses distinct challenges. The data types differ fundamentally: structured codes, continuous lab values, free-text notes, and time-stamped events versus static or slowly-changing molecular measurements. Temporal structure matters: the sequence of diagnoses and treatments contains prognostic information that static snapshots miss. Missingness is informative: the absence of a laboratory test may indicate that a clinician deemed it unnecessary, which itself conveys information about patient state (Section 2.7.2). The phenotype quality challenges introduced there cascade through multi-omics integration, where EHR-derived labels may introduce systematic biases that Section 22.2.4 examines in detail.\nFoundation models for EHR data learn representations from the longitudinal event sequences. These models, often based on transformer architectures that process sequences of medical codes (see Chapter 7), capture temporal dependencies and co-occurrence patterns in clinical trajectories. The resulting patient embeddings encode disease state and prognosis in a form amenable to integration with molecular data.\nCombining EHR embeddings with genomic features requires handling the different temporal scales. Genetic variants are constant throughout life; EHR observations accumulate over years. The integration must determine which clinical observations are relevant to a given molecular measurement, accounting for the time between sample collection and clinical events.\n\n\n19.4.2 Imaging Integration\nMedical imaging provides spatial information that molecular assays lack. A CT scan reveals tumor location, size, and heterogeneity; histopathology slides show cellular morphology and tissue architecture; MRI captures organ structure and function. These spatial data complement molecular measurements that aggregate over dissected tissue regions.\nRadiogenomics links imaging features to genetic and molecular characteristics. Glioblastoma tumors with specific imaging signatures have distinct methylation patterns and expression programs [Citation Needed]. Radiomic features extracted from CT scans correlate with mutational burden and immune infiltration in lung cancer [Citation Needed]. These associations enable prediction of molecular state from non-invasive imaging, potentially guiding treatment decisions when biopsy is impractical.\nFoundation models for medical imaging, pretrained on millions of scans through self-supervised objectives, provide rich representations for downstream tasks [Citation Needed]. Integrating these imaging embeddings with molecular data follows the intermediate fusion paradigm: modality-specific encoders produce representations in a shared latent space where multi-modal classifiers operate.\nThe integration must account for correspondence between imaging regions and molecular samples. A tumor may be molecularly heterogeneous, with different subclones in different spatial locations. A biopsy samples one location; imaging captures the entire lesion. Alignment requires either spatial registration of biopsy location to imaging coordinates or acceptance that the correspondence is imperfect.\n\n\n19.4.3 Multi-Modal Clinical Prediction Models\nCombining EHR, imaging, and molecular data for clinical prediction follows the intermediate fusion pattern. Each data type has a specialized encoder: a transformer for longitudinal EHR events, a vision encoder for imaging, domain-specific encoders for expression, methylation, and other molecular modalities. All encoders produce embeddings in a common patient representation space.\nThe training objective typically combines modality-specific reconstruction losses with alignment terms that encourage consistency across data types. A patient’s EHR embedding should be predictive of their molecular state; their imaging embedding should be consistent with their clinical trajectory. Downstream classifiers for outcomes like survival, treatment response, or disease progression operate on the combined representation.\nMissing modalities are common in clinical settings. Not all patients have genomic data; imaging may be unavailable for some conditions; the depth of EHR history varies by healthcare system and patient engagement. Multi-modal clinical models must handle this missingness gracefully, producing useful predictions from whatever data are available while leveraging cross-modal information when present.\nThe clinical deployment path for such models requires validation on external cohorts, prospective evaluation, and regulatory clearance. These practical considerations, addressed in Chapter 25, shape model development from the outset. A model that performs well on a research cohort but requires modalities unavailable in clinical workflows provides little value. The practical deployment considerations, including feature availability and model calibration requirements, are examined in Section 25.3.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 19.4: [High] Patient-level integration. Panel A (Data Modalities): EHR (diagnoses, procedures, medications, labs: longitudinal), imaging (CT, MRI, histopathology: spatial), molecular (genomics, expression, proteomics: static). Panel B (Modality-Specific Encoders): EHR transformer over event sequence; imaging vision encoder; molecular FM embeddings. Panel C (Patient Representation Space): All → shared patient embedding; EHR predicts molecular; imaging consistent with trajectory. Panel D (Clinical Prediction Tasks): Risk stratification, treatment response, prognosis; missing modality handling (not all patients have all data). Practical challenges callout: batch effects, temporal alignment, cost constraints.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-multi-omics.html#sec-ch19-systems-view",
    "href": "part_4/p4-ch19-multi-omics.html#sec-ch19-systems-view",
    "title": "19  Multi-Omics Integration",
    "section": "19.5 Systems View: From Variant to Phenotype",
    "text": "19.5 Systems View: From Variant to Phenotype\nMulti-omics integration gains conceptual clarity from a systems biology perspective that traces information flow from genetic variation through molecular intermediates to clinical phenotypes. This cascade view organizes the molecular layers into a causal hierarchy and identifies where integration should occur.\n\n19.5.1 Information Cascade\nGenetic variants are the starting point: heritable differences in DNA sequence that perturb downstream molecular processes. Some variants directly alter protein structure through missense or nonsense mutations. Others affect regulation: promoter variants change expression level, splice site variants alter transcript isoforms, enhancer variants modulate tissue-specific expression.\nThese primary effects propagate through molecular layers. Expression changes alter the cellular protein complement. Protein level changes affect enzyme activity, signaling cascades, and transcriptional feedback. Metabolic flux shifts in response to enzyme availability. Cell behavior changes as the integrated molecular state crosses thresholds for proliferation, differentiation, or death.\nTissue-level phenotypes emerge from cellular behavior aggregated across the organ. Tumor growth reflects altered cell proliferation; fibrosis reflects aberrant extracellular matrix deposition; inflammation reflects immune cell recruitment and activation. These tissue phenotypes manifest as clinical symptoms, laboratory abnormalities, and imaging findings.\nThe cascade view suggests where different modalities provide information. Genomics captures the inherited potential and somatic alterations. Transcriptomics and epigenomics capture the current regulatory state. Proteomics and metabolomics capture the functional molecular complement. Clinical data captures the phenotypic consequences.\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\nFigure 19.5: [High] Systems biology view. Panel A (The Causal Cascade): Genetic variant → (cis-regulation) → Expression change → (translation) → Protein change → (metabolism) → Metabolite change → (cellular) → Cell behavior → (tissue) → Clinical phenotype. Panel B (Where Each Modality Provides Information): Genomics (starting point), transcriptomics (current state), proteomics (functional complement), metabolomics (downstream), clinical (manifestation). Panel C (Bottleneck Modalities): Coding variants bottleneck at protein; regulatory at expression; some phenotypes bottleneck downstream. Panel D (Integration Implications): Model should trace causal chain; not all modalities equally informative for all questions.\n\n\n\n\n\n19.5.2 Bottleneck Modalities\nNot all modalities are equally informative for all questions. The concept of bottleneck modalities identifies which molecular layers most directly mediate the relationship between genetic variation and phenotype.\nFor many coding variants, protein structure is the bottleneck. A missense variant’s effect on disease depends primarily on how it alters protein function, which depends on how the amino acid substitution affects folding, stability, and activity. Expression level matters less than structural consequence. Protein language models that predict structural effects from sequence directly address this bottleneck (see Chapter 12).\nFor regulatory variants, expression is closer to the bottleneck. An enhancer variant affects disease through its effect on target gene expression, which affects downstream processes. Chromatin accessibility and transcription factor binding are intermediate steps; expression level is the more proximal readout. Models that predict expression effects from sequence address this bottleneck (see Chapter 13).\nFor some phenotypes, the bottleneck may lie downstream of molecular measurements entirely. Behavioral traits depend on neural circuit function that emerges from complex cellular and network dynamics. Metabolic traits depend on flux through interconnected pathways that may not be apparent from enzyme abundance alone. These cases suggest that molecular measurements provide incomplete information regardless of integration sophistication.\n\n\n19.5.3 Causal vs. Correlational Integration\nMulti-omics data are pervasively correlated. Genes in the same pathway have correlated expression. Methylation and expression are anti-correlated at many promoters. Clinical variables cluster by disease category. These correlations can improve prediction even without causal understanding.\nCausal integration seeks to identify the mechanistic relationships between molecular layers. If a variant causes reduced expression, which causes protein deficiency, which causes metabolic dysfunction, this causal chain suggests intervention targets: expression restoration or enzyme supplementation might address the downstream effects. Correlational integration might achieve the same predictive performance without identifying this chain, since all layers correlate with the phenotype.\nDistinguishing causal from correlational relationships requires experimental perturbation or careful causal inference from observational data. Mendelian randomization uses genetic variants as instruments to infer causal effects of expression on outcomes (see Section 3.9 for integration of GWAS with mechanism). CRISPR screens directly perturb gene function and measure consequences. Multi-omics integration methods increasingly incorporate causal assumptions or validation against perturbation data.\nThe distinction matters for interpretation and intervention. A predictive model based on correlations may fail when the data distribution shifts (see Chapter 22) or when interventions alter the causal structure. A causally informed model captures mechanism that persists across contexts.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-multi-omics.html#sec-ch19-missing-modalities",
    "href": "part_4/p4-ch19-multi-omics.html#sec-ch19-missing-modalities",
    "title": "19  Multi-Omics Integration",
    "section": "19.6 Handling Missing Modalities",
    "text": "19.6 Handling Missing Modalities\nReal-world multi-omics data are incomplete. Different studies measure different modalities. Within studies, technical failures, sample limitations, and cost constraints create missing data. Clinical deployment must handle patients with incomplete molecular profiles. Robust multi-omics methods must address missingness directly.\n\n19.6.1 Training with Incomplete Data\nIntermediate fusion architectures handle missing modalities naturally during inference: only the available encoders contribute to the shared representation. Training is more complex because alignment terms require paired measurements across modalities.\nOne approach trains on the subset of samples with complete data, then applies the trained encoders to samples with partial data during inference. This wastes information from the samples with incomplete profiles and may learn representations that fail to generalize to the missing-modality setting.\nA better approach incorporates missingness into training. Modality dropout randomly masks modalities during training, forcing the model to learn representations robust to missing inputs. The reconstruction and alignment losses are computed only for available modalities, so samples with partial data can still contribute to training.\nCurriculum learning strategies may first train with complete data to establish alignment, then gradually increase modality dropout to improve robustness. The balance between alignment quality (which benefits from complete data) and robustness (which requires training on partial data) requires empirical tuning.\n\n\n19.6.2 Cross-Modal Imputation\nIntermediate fusion enables principled imputation of missing modalities. Given a sample’s available modalities encoded into the shared latent space, decoders for missing modalities can predict expected values. If a patient has expression data but not methylation, the expression encoder produces a latent embedding, and the methylation decoder generates predicted methylation values from that embedding.\nThe imputation quality depends on how well the shared space captures the biological factors underlying both modalities. If expression and methylation reflect the same cell state, the imputation may be accurate. If they capture distinct aspects of biology, imputation will smooth over true variation.\nUncertainty in imputation matters for downstream use. Point estimates of missing values provide no indication of confidence. Generative models that produce distributions over missing values enable propagation of uncertainty through downstream analyses (see Section 23.4). A risk prediction that depends heavily on imputed values should have wider confidence intervals than one based entirely on measured data. The selective prediction and uncertainty communication strategies that could implement this appropriate caution are developed in Section 23.7.\n\n\n19.6.3 Zero-Shot Cross-Modal Transfer\nThe most ambitious application of multi-omics integration is zero-shot prediction across modalities: using a model trained on one set of modalities to make predictions for samples measured with entirely different modalities.\nThis transfer relies on the shared latent space capturing biological state independently of measurement modality. If the space truly represents cell state, then a classifier trained on expression-derived embeddings should work on ATAC-seq-derived embeddings, since both encoders map to the same biological meaning. The alignment training enables this transfer by ensuring that the same biological entity maps to the same latent location regardless of which modality was measured.\nZero-shot transfer is rarely perfect. The modalities may capture somewhat different aspects of biology, and the alignment may be imprecise. But partial transfer can still be valuable: a model achieving 80% of supervised performance without any labeled examples in the new modality saves substantial annotation effort (see Section 9.9.2 for zero-shot transfer in other contexts).",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-multi-omics.html#sec-ch19-practical-challenges",
    "href": "part_4/p4-ch19-multi-omics.html#sec-ch19-practical-challenges",
    "title": "19  Multi-Omics Integration",
    "section": "19.7 Practical Challenges",
    "text": "19.7 Practical Challenges\nThe gap between multi-omics potential and deployed reality reflects obstacles that compound across modalities. Technical variation that is manageable within a single assay type becomes intractable when batch structures differ across genomics, transcriptomics, and proteomics. Sample sizes that support single-modality analysis may be insufficient when the effective dimensionality grows with each added data type. Interpretability, already challenging for deep learning on individual modalities, becomes harder still when attributions must be compared across features with different scales and semantics. These practical challenges determine whether integration improves predictions or merely adds complexity.\n\n19.7.1 Batch Effects Across Modalities\nBatch effects, systematic technical variation between experimental batches, are endemic in high-throughput biology. Multi-omics integration faces compounded batch effects: each modality may have its own batch structure, batches may be correlated or anti-correlated across modalities, and batch correction methods designed for single modalities may not extend to multi-modal settings.\nConsider a study where expression data were generated at three sequencing centers and proteomics data were generated at two mass spectrometry facilities. The batch effects in each modality are independent. Samples from expression batch 1 are spread across proteomics batches. Correcting expression batch effects does not address proteomics batch effects, and vice versa.\nIntegration must either correct batch effects within each modality before combining (risking removal of real biology that correlates with batch) or incorporate batch as a covariate in the integrated model (requiring that batch structure be known and modeled correctly). Domain adaptation techniques treat batches as domains and learn representations invariant to domain while retaining biological signal. The systematic strategies for detecting batch-driven confounding appear in Section 22.8, while mitigation approaches including adversarial domain adaptation are detailed in Section 22.9.\n\n\n19.7.2 Sample Size and Power\nMulti-omics studies typically have smaller sample sizes than single-modality studies due to cost constraints. Each additional modality increases per-sample cost, trading breadth for depth. This tradeoff has implications for statistical power and model complexity.\nThe effective sample size for multi-omics integration may be smaller than for any single modality. If 1000 patients have expression data and 800 have methylation data but only 600 have both, intermediate fusion sees 600 fully informative samples. Late fusion can use all 1000 expression samples and all 800 methylation samples, avoiding the intersection penalty.\nPower analyses for multi-omics studies must account for the specific integration strategy and the expected missingness pattern. A study designed for early fusion needs larger sample sizes (relative to feature count) than one designed for late fusion. Grant applications and study planning should explicitly consider how integration choices affect required sample sizes.\n\n\n19.7.3 Interpretability Across Modalities\nMulti-omics models compound the interpretability challenges inherent in deep learning. When a model predicts disease risk from integrated genomic, transcriptomic, and proteomic features, clinicians need to understand which modalities and which features drive the prediction. A black-box risk score, however accurate, provides little guidance for understanding mechanism or identifying intervention targets.\nAttribution methods that work for single-modality models do not automatically extend to multi-modal settings (see Chapter 24 for attribution methods). Gradient-based attribution can identify important features within each modality, but comparing importance across modalities requires careful normalization. A genomic variant and an expression value operate on different scales with different effect size distributions; raw attribution scores are not directly comparable.\nThe intermediate fusion architecture offers some interpretability advantages. The shared latent space can be visualized to understand how samples cluster and which modalities contribute to separation. Attention weights in cross-modal transformers indicate which features from each modality the model considers when making predictions. Modality ablation studies quantify each data type’s contribution to overall performance.\nBiological interpretability requires connecting learned representations to known biology. Do the latent dimensions correspond to pathways, cell types, or disease processes? Are cross-modal attention patterns consistent with known regulatory relationships? These questions demand validation against external biological knowledge, not just introspection of model parameters.\n\n\n19.7.4 Evaluation Complexity\nEvaluating multi-omics models is more complex than evaluating single-modality models. Multiple dimensions of performance matter: prediction accuracy, calibration, cross-modality transfer, robustness to missing modalities, biological plausibility of learned representations, and clinical utility.\nA model might achieve high prediction accuracy by memorizing batch effects or leveraging shortcuts in the data. Evaluation should include cross-batch and cross-cohort validation to assess generalization (Chapter 21), with particular attention to homology-aware splitting strategies (Section 21.2) that prevent information leakage across data partitions. Ablation studies that remove each modality quantify the contribution of each data type and identify whether the model genuinely integrates information or relies predominantly on one modality.\nBiological validation through comparison to known biology provides another evaluation axis. Do the learned factors correspond to known pathways? Are attention patterns consistent with regulatory relationships? Do imputed values match held-out measurements? These checks assess whether the model captures biological signal rather than technical artifacts.\nClinical evaluation, addressed in Chapter 25, requires prospective validation in real deployment settings. A model that improves prediction in research cohorts may not improve clinical decisions if the predictions do not change management or if the required modalities are unavailable in clinical workflows.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-multi-omics.html#sec-ch19-conclusion",
    "href": "part_4/p4-ch19-multi-omics.html#sec-ch19-conclusion",
    "title": "19  Multi-Omics Integration",
    "section": "19.8 Integration as Means, Not End",
    "text": "19.8 Integration as Means, Not End\nMulti-omics integration is not an end in itself but a means to improved prediction, understanding, and intervention. The integration strategies and foundation models surveyed here produce representations; downstream applications convert those representations to actionable outputs. Risk prediction combines multi-omic embeddings with clinical variables for individualized prognosis. Treatment response models predict which patients will benefit from specific therapies based on their integrated molecular profiles. Drug discovery uses multi-omics to inform target identification and patient stratification for clinical trials (see Chapter 27). In each case, integration provides the substrate; clinical or scientific goals provide the purpose.\nThe systems view that multi-omics enables shapes how predictions should be interpreted. A risk prediction based on integrated features inherits explanatory power from the causal relationships linking molecular layers to phenotype. Understanding which modalities drive predictions, and how those modalities relate to underlying biology, supports clinical reasoning about mechanism and intervention. This explanatory capacity distinguishes multi-omics from single-modality approaches that may predict equally well but provide less insight into why predictions succeed or fail.\nThe path from research models to clinical deployment requires addressing practical challenges that intensify with integration: batch effects across modalities and institutions, missing measurements that differ systematically across patients, sample size limitations that grow with feature dimensionality, and evaluation complexity when outcomes depend on multiple data types. The clinical applications examined in Chapter 25 and Chapter 26 confront these realities. As the field advances toward whole-patient foundation models that jointly encode genomics, transcriptomics, proteomics, imaging, and clinical data, the integration principles established here provide the foundation. The trade-offs between fusion strategies, the importance of shared latent spaces, the challenge of missing modalities, and the systems biology perspective on information flow will remain relevant as scale and scope expand. The interpretability challenges that compound across modalities (Chapter 24) and the calibration requirements for clinical deployment (Section 23.2) add further dimensions that shape how multi-omics models should be developed and evaluated.",
    "crumbs": [
      "Part IV: Systems and Scale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_5/p5--eval-interp.html",
    "href": "part_5/p5--eval-interp.html",
    "title": "Part V: Evaluation and Trust",
    "section": "",
    "text": "Evaluating genomic models presents challenges that distinguish this domain from natural language processing or computer vision. Biological sequences contain evolutionary history: a model tested on homologous sequences may appear to generalize when it has merely memorized. Population structure creates spurious associations: a variant predictor may learn ancestry rather than pathogenicity. Nested functional hierarchies obscure what models actually capture: strong performance on common variants provides no guarantee of accuracy on the rare variants that drive most clinical decisions. Standard machine learning evaluation practices, developed for domains where training and test examples are approximately independent and identically distributed, become actively misleading when applied to genomic data without careful adaptation.\nRigorous evaluation determines whether genomic foundation models deliver on their promises. Established benchmarks (20  Benchmarks) across protein, DNA, regulatory, and clinical domains require careful examination of their construction and limitations, distinguishing meaningful signal from benchmark-specific artifacts. Principles for using benchmarks appropriately (21  Evaluation Principles), including data splitting, metric selection, and statistical testing, produce trustworthy conclusions rather than inflated claims.\nSystematic biases pervade genomic datasets (22  Confounders and Leakage): population stratification confounds genotype with ancestry, batch effects encode technical rather than biological variation, and hidden correlations tempt models to exploit patterns without learning genuine biology. Calibration and uncertainty quantification (23  Uncertainty Quantification) determine whether model outputs can inform decisions or require careful reinterpretation. Moving beyond black-box prediction toward mechanistic understanding (24  Interpretability) requires distinguishing faithful explanations that accurately reflect model computation from plausible explanations that merely satisfy human intuition. This critical toolkit enables rigorous evaluation of genomic AI claims and responsible deployment in research and clinical settings.",
    "crumbs": [
      "Part V: Evaluation and Trust"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html",
    "href": "part_5/p5-ch20-benchmarks.html",
    "title": "20  Benchmarks",
    "section": "",
    "text": "20.1 Protein Language Model Benchmarks\nEvery benchmark measures a proxy. ClinVar pathogenicity labels proxy clinical impact. Area under the receiver operating characteristic curve (auROC) on held-out variants proxies discrimination ability in deployment. Chromatin accessibility predictions proxy regulatory function. The gap between proxy and target varies across benchmarks, across variant types, and across populations. A model achieving state-of-the-art performance on ClinVar may systematically miscalibrate predictions for the rare variants that matter most clinically, because ClinVar’s composition does not reflect the distribution of variants clinicians actually encounter. A DNA language model excelling at enhancer classification may have learned GC content rather than regulatory grammar, because the benchmark’s negative examples differ from positives in ways that have nothing to do with enhancer function.\nUnderstanding what benchmarks actually measure, and how that differs from what we need to know, is prerequisite to interpreting any leaderboard result. The genomic AI field has accumulated substantial evaluation infrastructure. Dozens of benchmark suites target different modalities: protein structure and function, DNA regulatory elements, variant pathogenicity, gene expression prediction, and more. Hundreds of individual tasks probe specific capabilities. Thousands of models have reported results, creating leaderboards that rank approaches and track progress over time. This infrastructure enables comparison and drives methodological improvement. Yet the relationship between benchmark success and deployment value remains poorly characterized. A foundation model that dominates protein benchmarks may fail on the specific protein family relevant to a drug discovery campaign. A variant effect predictor that leads regulatory benchmarks may provide no clinical utility for the variant classes that lack representation in evaluation data.\nProtein language models (Chapter 12) benefit from the longest-established and most systematic evaluation ecosystem in genomic AI, reflecting both the longer history of computational protein science and the relative tractability of protein structure and function prediction compared to the regulatory genomics tasks discussed in Chapter 13. The maturity of protein benchmarks reflects both the longer history of computational protein science and the relative tractability of protein structure and function prediction compared to regulatory genomics.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html#sec-ch20-protein-benchmarks",
    "href": "part_5/p5-ch20-benchmarks.html#sec-ch20-protein-benchmarks",
    "title": "20  Benchmarks",
    "section": "",
    "text": "20.1.1 TAPE: Tasks Assessing Protein Embeddings\nThe Tasks Assessing Protein Embeddings (TAPE) benchmark, introduced in 2019, established the template for systematic protein representation evaluation (Rao et al. 2019). TAPE frames protein language model assessment as transfer learning evaluation (Chapter 9): pretrained models generate embeddings (Section 5.6), which are then used as features for supervised prediction on downstream tasks. This framework decouples representation quality from task-specific modeling, enabling comparison across architectures that may have very different inductive biases.\nTAPE comprises five tasks spanning different aspects of protein biology. Secondary structure prediction requires classifying each residue as helix, sheet, or coil, testing whether embeddings capture local structural preferences. Contact prediction asks whether residue pairs are spatially proximate in the folded structure, probing the representation’s ability to encode tertiary structure information from sequence alone. Remote homology detection requires classifying proteins into structural superfamilies, testing whether embeddings capture evolutionary relationships that transcend sequence similarity. Fluorescence prediction and stability prediction use data from deep mutational scanning experiments to assess whether embeddings encode fitness landscapes.\nThe benchmark’s design reflects deliberate methodological choices. Train, validation, and test splits enforce sequence identity thresholds to prevent homology-based leakage (Section 21.4). Evaluation uses simple linear or shallow neural network heads rather than complex task-specific architectures, isolating representation quality from modeling capacity. Standardized preprocessing and data loading eliminate confounds from inconsistent implementation.\nTAPE’s influence extended beyond its specific tasks. The benchmark established norms for protein representation evaluation: systematic coverage of diverse prediction targets, controlled transfer learning protocols, and explicit attention to data splitting. Subsequent benchmarks adopted and extended this framework.\n\n\n20.1.2 FLIP: Function-Linked Protein Benchmark\nThe FLIP (Function-Linked Integrated Protein) benchmark addresses gaps in TAPE’s coverage by focusing on experimentally measured functional properties (Dallago et al. 2022). Where TAPE includes structurally derived labels and computational annotations, FLIP emphasizes high-throughput experimental assays that directly measure protein fitness.\nFLIP aggregates deep mutational scanning datasets across diverse proteins and functional readouts. The benchmark includes assays measuring enzymatic activity, binding affinity, thermostability, and expression level. Each dataset provides quantitative measurements for thousands of single-point mutations, enabling evaluation of fine-grained variant effect prediction.\nThe benchmark’s value lies in its experimental grounding. Computational structure predictions and evolutionary conservation scores, while useful, are indirect proxies for function. Deep mutational scanning provides direct measurements of how sequence changes affect the property of interest. Models that perform well on FLIP demonstrate the ability to predict experimentally validated functional consequences rather than computationally inferred annotations.\nFLIP also introduced systematic evaluation of different splitting strategies. Random splits, where training and test variants are sampled uniformly from the same protein, represent the easiest setting. Contiguous splits, where training and test variants occupy different sequence regions, test spatial generalization. Modulo splits, which interleave training and test positions along the sequence, provide intermediate difficulty. Performance typically degrades from random to contiguous splits, revealing how much models rely on local sequence context versus genuine functional understanding.\n\n\n20.1.3 ProteinGym: Comprehensive Variant Effect Evaluation\nProteinGym has emerged as the most comprehensive benchmark for protein variant effect prediction, compiling 217 deep mutational scanning assays across diverse protein families (Notin et al. 2023). The benchmark’s scale enables statistically robust comparison across modeling approaches while its diversity reveals where different methods excel or struggle.\nThe primary evaluation metric is Spearman correlation between predicted and experimentally measured fitness effects. This rank-based metric is appropriate for deep mutational scanning data, where absolute fitness values depend on assay-specific calibration but relative rankings are more comparable across experiments. ProteinGym reports correlations for each assay individually and aggregated across the full benchmark, enabling both global comparison and identification of task-specific strengths.\nProteinGym distinguishes between zero-shot and supervised evaluation regimes. In zero-shot evaluation, models predict variant effects without any task-specific training, relying entirely on representations learned during pretraining. Models like ESM-1v (Section 12.1) compute effects as log-likelihood ratios under the pretrained language model, while structure-based methods like AlphaMissense (Section 14.2.3) incorporate predicted structural consequences. In supervised evaluation, models are fine-tuned on a subset of measured variants before predicting held-out effects. The gap between zero-shot and supervised performance indicates how much task-specific information improves over general-purpose representations.\nThe benchmark reveals systematic patterns in model performance. Protein language models generally outperform conservation-based methods, particularly for variants in regions with sparse evolutionary sampling. Structure-aware models show advantages for variants affecting protein stability or buried residues. Ensemble methods that combine multiple predictors often achieve the highest correlations, suggesting that different approaches capture complementary information.\nProteinGym’s limitations mirror those of its constituent datasets. Deep mutational scanning experiments are biased toward well-studied proteins amenable to high-throughput screening. Assay-specific selection pressures affect which variants appear deleterious: a variant may strongly affect enzymatic activity while leaving thermostability unchanged, or vice versa. The benchmark measures correlation with specific experimental readouts rather than clinical pathogenicity, which integrates multiple functional consequences in complex ways.\n\n\n20.1.4 Structure Prediction Benchmarks\nProtein structure prediction benchmarks derive from the Critical Assessment of protein Structure Prediction (CASP) tradition, which has evaluated computational methods against experimentally determined structures since 1994 (kryshtafovych_critical_2021?). The dramatic success of AlphaFold2 at CASP14 in 2020 transformed the field, but structure prediction benchmarks remain relevant for evaluating single-sequence methods and assessing whether language model pretraining improves structural accuracy.\nStructure prediction quality is typically assessed using the Global Distance Test (GDT-TS) and Template Modeling score (TM-score). GDT-TS measures the percentage of residues that can be superimposed within various distance thresholds, providing a single number between 0 and 100 that correlates well with visual assessment of structural similarity. TM-score normalizes by protein length, enabling comparison across proteins of different sizes.\nFor protein language models, the relevant evaluation setting is single-sequence structure prediction, where the model receives only the target sequence without multiple sequence alignments. This tests whether pretraining on evolutionary sequence databases enables structure prediction without explicit evolutionary analysis at inference time. ESMFold (Section 12.4) demonstrated that single-sequence prediction can approach MSA-based methods for many proteins, though performance gaps remain for sequences with sparse evolutionary coverage.\nStructure prediction benchmarks complement sequence-based evaluations by testing whether learned representations encode biophysical constraints. A model that achieves high accuracy on contact prediction or secondary structure classification may still fail to integrate these local predictions into globally consistent structures. The emergence of accurate single-sequence structure prediction from language model embeddings suggests that pretraining captures substantial structural information, even without explicit structural supervision.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html#sec-ch20-dna-benchmarks",
    "href": "part_5/p5-ch20-benchmarks.html#sec-ch20-dna-benchmarks",
    "title": "20  Benchmarks",
    "section": "20.2 DNA and Regulatory Benchmarks",
    "text": "20.2 DNA and Regulatory Benchmarks\nDNA foundation models (Chapter 11) and regulatory models (Chapter 13) face a less mature but rapidly developing benchmark landscape compared to the protein ecosystem. Early deep learning work in genomics focused on individual tasks derived from ENCODE-style assays (Section 2.4.1), establishing evaluation paradigms that later benchmark suites would systematize. Recent efforts have introduced benchmark suites that attempt to standardize evaluation across multiple tasks, tissues, and species.\n\n20.2.1 Classical Regulatory Prediction Tasks\nThe earliest deep learning benchmarks for genomics framed regulatory prediction as classification over short sequence windows. Transcription factor binding prediction asks whether a specific TF ChIP-seq peak overlaps a given sequence window, typically around 1 kilobase centered on the binding site. Open chromatin prediction requires classifying regions as accessible or inaccessible based on DNase-seq or ATAC-seq signal. Histone mark prediction asks whether a chromatin modification peak (H3K27ac, H3K4me3, etc.) is present at each position.\nThese tasks derive from consortia like ENCODE and Roadmap Epigenomics (Section 2.4.1), which systematically profiled chromatin states across cell types. Benchmark construction typically involves defining positive regions from called peaks and sampling negative regions from elsewhere in the genome, extracting fixed-length sequences centered on each region, and evaluating binary classification using auROC or average precision.\nModels such as DeepSEA, Basset, and DanQ established baseline performance on these tasks (Chapter 6 for architectural details). Their success demonstrated that convolutional networks could learn sequence features predictive of regulatory state without hand-crafted motifs. Modern foundation models still report performance on similar tasks as sanity checks, though these classical benchmarks have significant limitations.\nThe primary limitation is that binary classification over short windows fails to capture the quantitative, cell-type-specific, and long-range nature of transcriptional regulation. A region may be weakly accessible in some cell types and strongly accessible in others; binary labels collapse this continuous variation. Short windows cannot assess whether models capture distal regulatory interactions that span tens to hundreds of kilobases. Evaluation on curated peak regions may overestimate performance relative to genome-wide prediction, where the vast majority of positions are regulatory “background.”\n\n\n20.2.2 Quantitative Regulatory Prediction\nBeyond binary classification, benchmarks increasingly require prediction of quantitative regulatory readouts. Signal regression asks models to predict per-base or per-bin signal intensity from ChIP-seq, ATAC-seq, or related assays. Gene expression prediction requires predicting transcript abundance (TPM, counts) from promoter sequences or larger genomic contexts. Massively parallel reporter assays (MPRAs) provide systematic measurements of enhancer or promoter activity for thousands of sequences, enabling evaluation of quantitative activity prediction.\nHybrid architectures like Enformer (Section 13.2) popularized benchmarks combining large receptive fields with dense quantitative targets across many assays and cell types. Evaluation metrics shift from auROC to Pearson or Spearman correlation between predicted and observed profiles. Some benchmarks report correlation relative to replicate concordance, establishing an upper bound set by experimental reproducibility.\nQuantitative benchmarks better reflect the continuous nature of regulatory activity but introduce new challenges. Heterogeneous noise across assays and laboratories complicates aggregation: should a model be penalized equally for poor performance on a low-quality assay versus a high-quality one? Cell-type diversity raises questions about how to weight performance across tissues: is accurate prediction in a rare cell type more or less important than in a common one? The relationship between predicted and observed signal depends on assay-specific calibration that may not transfer across experimental batches.\n\n\n20.2.3 Genomic Benchmarks\nThe Genomic Benchmarks resource provides standardized classification datasets for DNA sequence models (Grešová et al. 2023). The benchmark compiles tasks including enhancer identification, promoter recognition, splice site detection, and coding sequence classification across multiple species. Standardized train, validation, and test splits enable direct comparison of different architectures without confounds from inconsistent data processing.\nGenomic Benchmarks emphasizes accessibility and reproducibility. Datasets are available in a unified format with documented preprocessing. Baseline results for multiple architectures provide reference points for new models. The benchmark includes tasks of varying difficulty, from relatively easy (distinguishing coding from non-coding sequence) to challenging (identifying tissue-specific enhancers).\nThe benchmark’s limitations reflect its design priorities. Focus on classification rather than regression excludes quantitative prediction tasks. Task difficulty varies substantially, with some tasks approaching saturation where gains become difficult to measure. Species coverage, while broader than many benchmarks, remains biased toward well-studied model organisms.\n\n\n20.2.4 BEND: Benchmark for DNA Language Models\nBEND (Benchmark for Evaluating DNA Models) provides a unified framework for evaluating genomic foundation models across diverse tasks (Marin et al. 2024). The benchmark includes regulatory element classification, chromatin accessibility prediction, variant effect scoring, and gene expression prediction. Standardized splits and evaluation protocols enable fair comparison across model families.\nBEND’s design reflects lessons learned from earlier benchmarks. Tasks span multiple biological scales, from nucleotide-level variant effects to kilobase-scale regulatory elements. Evaluation includes both zero-shot settings (using pretrained representations directly) and fine-tuned settings (adapting models to specific tasks). Performance is reported separately for each task rather than aggregated into a single score, acknowledging that different models may excel at different aspects of genomic prediction.\nComparative evaluations using BEND reveal that no single model dominates across all tasks. Architecture choices (CNN versus transformer versus state space model), tokenization schemes (single nucleotide versus k-mer versus BPE), and pretraining corpora all influence task-specific performance (Chapter 5). These patterns inform model selection for specific applications while highlighting the limitations of aggregate benchmarks that obscure such variation.\n\n\n20.2.5 Long-Range Benchmarks\nLong-range regulatory interactions, where enhancers tens to hundreds of kilobases from their target genes influence expression, require benchmarks that specifically test extended context modeling. The Long Range Benchmark (LRB) evaluates models’ ability to integrate information across large genomic distances, with tasks including predicting distal enhancer-promoter interactions, modeling TAD boundary effects, and identifying long-range regulatory dependencies [Citation Needed].\nDNALongBench extends evaluation to ultra-long contexts spanning up to millions of base pairs [Citation Needed]. Tasks at this scale test whether models can leverage chromosome-level context for regulatory prediction, potentially capturing effects from 3D chromatin organization and large-scale chromatin domains.\nThese benchmarks are particularly relevant for evaluating efficient attention mechanisms, state space models, and other architectures designed to extend effective context length (Section 7.4). Performance on long-range benchmarks does not necessarily correlate with short-range task performance, indicating that different architectural choices optimize for different aspects of sequence modeling.\n\n\n20.2.6 Cross-Species Evaluation\nGenBench and related resources test whether models trained on one organism generalize to related species [Citation Needed]. Cross-species evaluation is important for several reasons. Many applications require predictions in non-human organisms (agricultural genomics, model organism research, comparative genomics). Multi-species training may improve within-species performance by providing additional evolutionary signal (Section 8.8.3). The ability to transfer across species indicates that models have learned general principles of genome organization rather than species-specific artifacts.\nCross-species benchmarks typically evaluate models on held-out species not seen during training. Performance degradation from training to held-out species indicates the degree to which learned representations depend on species-specific features. Some architectures show better cross-species transfer than others, suggesting differences in how well they capture conserved regulatory principles.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html#sec-ch20-vep-benchmarks",
    "href": "part_5/p5-ch20-benchmarks.html#sec-ch20-vep-benchmarks",
    "title": "20  Benchmarks",
    "section": "20.3 Variant Effect Prediction Benchmarks",
    "text": "20.3 Variant Effect Prediction Benchmarks\nVariant effect prediction (VEP) benchmarks connect sequence changes to molecular or phenotypic consequences, addressing the clinically central question of which variants matter. These benchmarks span multiple biological levels, from molecular function to clinical pathogenicity.\n\n20.3.1 Clinical Variant Databases\nClinVar provides the most widely used labels for clinical variant effect prediction, aggregating pathogenicity assertions from clinical laboratories and researchers worldwide (Section 2.8.1). Benchmarks derived from ClinVar frame variant interpretation as classification: given a variant, predict whether it is pathogenic, likely pathogenic, benign, or likely benign.\nClinVar’s value as a benchmark stems from its clinical relevance. Variants classified in ClinVar represent the actual population of variants encountered in clinical testing. Performance on ClinVar directly addresses whether a model can assist variant interpretation workflows. The database’s scale (over 2 million variant submissions as of 2024) enables statistically robust evaluation (Landrum et al. 2018).\nClinVar’s limitations as a benchmark are equally important. Submission heterogeneity means that label quality varies dramatically: expert-curated panels provide high-confidence classifications while single-laboratory submissions may reflect limited evidence. Version sensitivity means that benchmark composition changes over time as new submissions arrive and old classifications are updated. Most consequentially, circularity with computational predictors creates feedback loops: variants may have been classified using the very tools being evaluated, inflating apparent performance. This circularity problem, examined in detail for classical predictors in Section 4.5 and for its broader confounding implications in Section 22.2.4, represents one of the most insidious forms of benchmark contamination.\nAncestry and gene coverage biases profoundly shape what ClinVar benchmarks measure. Variants from European ancestry individuals and well-studied disease genes are heavily overrepresented. High performance on ClinVar demonstrates accuracy for this specific population rather than robust generalization across human genetic diversity (Section 3.7). Benchmarks stratified by ancestry reveal substantial performance gaps, with models typically performing worse on variants from underrepresented populations.\nBest practices for using ClinVar as a benchmark include specifying the exact database version and download date, excluding variants with conflicting assertions, stratifying performance by evidence level and ancestry, and comparing to baselines using only allele frequency to detect circularity. These practices are detailed in Section 21.4, with specific guidance on detecting label leakage in Section 21.4.1.\n\n\n20.3.2 CAGI: Critical Assessment of Genome Interpretation\nThe Critical Assessment of Genome Interpretation (CAGI) challenges provide prospective evaluation of variant effect predictors on unpublished datasets (hoskins_cagi6_2023?). Unlike retrospective benchmarks that evaluate models on historical data, CAGI distributes prediction targets before ground truth is available, preventing any possibility of overfitting to known labels.\nCAGI challenges cover diverse prediction targets. Some challenges focus on molecular phenotypes: predicting the effect of variants on protein stability, binding affinity, or enzymatic activity. Others target clinical phenotypes: predicting disease risk, drug response, or clinical severity from individual genomes. The diversity of challenges tests whether models generalize across different types of variant effects.\nThe prospective design provides several advantages over retrospective benchmarks. Predictions must be made before labels are known, eliminating leakage from any source. The timeline forces models to commit to predictions rather than post-hoc optimization. Community participation enables comparison across many approaches under identical conditions.\nCAGI’s limitation is scale: challenges include hundreds to thousands of variants rather than the millions available in databases like ClinVar. Statistical power to detect small performance differences is correspondingly limited. The challenges also depend on experimental collaborators willing to withhold data until after the prediction deadline, limiting the range of phenotypes that can be assessed.\n\n\n20.3.3 Deep Mutational Scanning Benchmarks\nDeep mutational scanning (DMS) provides systematic experimental measurement of variant effects across entire proteins or regulatory elements (Section 2.4.4). DMS benchmarks test whether models can predict these experimentally determined effects, providing direct validation against measured functional consequences rather than inferred clinical classifications.\nMaveDB aggregates DMS datasets in a standardized format, enabling systematic benchmarking across diverse proteins and assays (esposito_mavedb_2019?). ProteinGym’s DMS component (discussed above) represents the most comprehensive benchmark in this space. For non-coding variants, MPRA datasets provide analogous systematic measurements of regulatory activity.\nDMS benchmarks have distinct strengths and limitations compared to clinical databases. The experimental grounding means that labels reflect actual measured effects rather than clinical inference that may involve multiple assumptions. The relationship between DMS fitness and clinical pathogenicity is complex: a variant may substantially affect enzymatic activity without causing disease if the residual activity suffices for normal physiology. DMS benchmarks measure one component of the variant interpretation puzzle rather than the full clinical picture.\n\n\n20.3.4 Regulatory and Non-Coding Variant Benchmarks\nNon-coding variants require specialized benchmarks because their effects operate through different mechanisms than coding variants. The foundation model approaches to non-coding variant effect prediction are examined in Section 14.3, with the underlying regulatory models detailed in Chapter 13. MPRA-based benchmarks test whether models can predict the quantitative effect of variants on enhancer or promoter activity measured in reporter assays. Expression quantitative trait locus (eQTL)-based benchmarks use naturally occurring variants associated with expression changes, treating the statistical evidence for eQTL status as a proxy for regulatory impact.\nThe challenge for non-coding benchmarks is connecting molecular effects to phenotypic consequences. A variant may alter chromatin accessibility without affecting any gene’s expression. A variant may affect expression without influencing disease risk. This gap between molecular and clinical effects complicates interpretation: high performance on MPRA prediction does not necessarily translate to accurate regulatory disease variant interpretation.\nFine-mapped genome-wide association study (GWAS) variants provide another benchmark source for non-coding VEP. Statistical fine-mapping identifies putatively causal variants within associated loci (Section 3.4), and models can be evaluated on their ability to prioritize these variants over nearby non-causal variants. Performance on fine-mapping tasks more directly assesses clinical relevance than molecular phenotype prediction, though fine-mapping itself has substantial uncertainty.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html#sec-ch20-trait-benchmarks",
    "href": "part_5/p5-ch20-benchmarks.html#sec-ch20-trait-benchmarks",
    "title": "20  Benchmarks",
    "section": "20.4 Trait and Population-Level Benchmarks",
    "text": "20.4 Trait and Population-Level Benchmarks\nAt the individual and population level, benchmarks assess whether models improve prediction of complex traits and disease risk.\n\n20.4.1 Polygenic Score Evaluation\nPolygenic score (PGS) benchmarks evaluate how well genotype-derived scores predict disease risk or quantitative traits (Section 3.5). Common evaluation settings include within-biobank evaluation, where a single large cohort is partitioned into training and test sets, and cross-biobank evaluation, where models trained in one population are tested in another. The integration of foundation model features with PGS approaches represents an emerging research direction (Section 25.1).\nMetrics depend on the phenotype. For quantitative traits, benchmarks report the coefficient of determination (\\(R^2\\)) or incremental \\(R^2\\) over non-genetic covariates. For binary disease outcomes, auROC and area under the precision-recall curve (auPRC) quantify discrimination. Calibration metrics assess whether predicted risks match observed event rates (Section 23.2). The clinical utility of PGS, discussed in Chapter 25, depends on all these properties: a score may discriminate well (high auROC) while being poorly calibrated (predicted risks do not match actual event rates).\nCross-population evaluation is particularly important because PGS portability is a major limitation of current methods (Section 3.7). Benchmarks stratified by ancestry typically reveal substantial performance degradation from European ancestry (where most GWAS have been conducted) to other populations. This degradation stems from multiple sources: different linkage disequilibrium patterns mean that tag SNPs identify different causal variants, population-specific variants are absent from training data, and effect sizes may differ across populations due to gene-environment interactions.\n\n\n20.4.2 TraitGym\nTraitGym provides a framework specifically designed to assess complex trait prediction using genomic foundation models (yan_traitgym_2024?). The benchmark evaluates whether foundation model embeddings or variant scores improve prediction beyond traditional polygenic score methods.\nTraitGym’s design addresses several limitations of standard PGS benchmarks. Ancestry stratification is built into the evaluation protocol, requiring models to report performance separately for different population groups. Multiple phenotypes spanning different genetic architectures (highly polygenic versus more oligogenic) test generalization across trait types. Comparison to appropriate baselines (standard PGS methods, clinical covariates alone) isolates the contribution of foundation model features.\nThe benchmark is particularly relevant for assessing claims that genomic foundation models add predictive value beyond classical statistical genetics. Foundation models incur substantial computational costs compared to linear PGS models; TraitGym helps determine whether these costs are justified by improved prediction.\n\n\n20.4.3 EmbedGEM Framework\nThe EmbedGEM framework evaluates whether foundation model embeddings capture biologically meaningful genetic signal, as opposed to technical artifacts or confounders (Mukherjee et al. 2024). The framework assesses embeddings along two axes: heritability and disease relevance.\nThe heritability axis measures how much genetic signal an embedding captures. EmbedGEM counts the number of genome-wide significant loci associated with embedding components and quantifies the strength of association through mean chi-squared statistics. Higher values indicate that the embedding reflects heritable biology rather than noise.\nThe disease relevance axis measures whether embedding-associated variants predict clinically meaningful outcomes. Polygenic scores constructed from embedding GWAS hits are evaluated for their ability to predict disease in independent cohorts. Incremental predictive value over standard clinical models indicates that the embedding captures disease-relevant genetic information.\nThis two-axis evaluation addresses a critical question for foundation model deployment: do learned representations discover novel biology or merely recapitulate known associations with additional computational overhead? Embeddings that show high heritability but low disease relevance may capture biological signal that is not clinically actionable. Embeddings that show disease relevance without novel genetic discoveries may not add value beyond existing PGS methods.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html#sec-ch20-benchmark-construction",
    "href": "part_5/p5-ch20-benchmarks.html#sec-ch20-benchmark-construction",
    "title": "20  Benchmarks",
    "section": "20.5 Benchmark Construction and Hidden Assumptions",
    "text": "20.5 Benchmark Construction and Hidden Assumptions\nBeyond cataloging benchmark suites, understanding how benchmarks are constructed reveals assumptions that shape what they measure and what they miss.\n\n20.5.1 Data Sources and Label Provenance\nBenchmark labels derive from diverse sources with different properties. Experimental assays (ChIP-seq, DMS, MPRA) provide direct measurements but are limited by assay-specific artifacts and selection pressures. Computational annotations (gene calls, functional predictions, conservation scores) provide broader coverage but introduce circular dependencies if models are trained and evaluated on overlapping sources. Clinical classifications aggregate expert judgment but reflect the evidence available at classification time, which may include the very predictors being benchmarked.\nThe provenance of benchmark labels determines what success on that benchmark actually means. High performance on experimentally derived labels suggests the model captures the specific molecular process assayed. High performance on clinical labels may indicate genuine clinical utility or may reflect circularity with existing prediction tools. Understanding label provenance is prerequisite to interpreting benchmark results.\n\n\n20.5.2 Splitting Strategies and Leakage\nHow benchmarks partition data into training and test sets determines whether evaluation measures generalization or memorization (Chapter 21). Random splitting, where examples are assigned to splits uniformly at random, represents the weakest form of evaluation. In genomics, random splits often permit homology-based leakage: training and test sequences may share sufficient similarity that memorization suffices for good performance.\nHomology-aware splitting clusters sequences by similarity before assigning clusters to splits, ensuring that test sequences are evolutionarily distant from training sequences. This approach is standard for protein benchmarks (using tools like CD-HIT or MMseqs2) but less consistently applied for DNA benchmarks.\nChromosome-based splitting holds out entire chromosomes for testing, preventing any position-based leakage within chromosomes. This approach is common for regulatory benchmarks but does not account for homologous sequences on different chromosomes. Temporal splitting reserves recent data for testing, appropriate when benchmarks derive from databases with submission timestamps. Each splitting strategy tests different aspects of generalization; the choice should match the intended deployment scenario.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 20.2: [High] Flow diagram showing data flow with leakage highlighted. Central pipeline: Pretraining → FM → Fine-tuning → Benchmark. Leakage pathways (red): Direct overlap, homology leakage, label circularity, resource sharing, community iteration. Specific examples: ESM/UniRef→ProteinGym, ClinVar/CADD circularity, ENCODE in both. Visual encoding: legitimate flow blue, leakage red dashed.\n\n\n\n\n\n20.5.3 Metric Selection and Aggregation\nBenchmark metrics determine what aspects of model performance are measured. Discrimination metrics (auROC, auPRC, correlation) assess whether models rank predictions correctly. Calibration metrics (expected calibration error, reliability diagrams) assess whether predicted probabilities match observed frequencies (Section 23.2). Clinical utility metrics (net benefit, decision curves) assess whether predictions improve decisions compared to treating all patients the same (Chapter 25).\nDifferent metrics can yield different rankings of models. A model with superior discrimination may have poor calibration, predicting the right relative order but wrong absolute probabilities. Choosing which metric to optimize, and how to aggregate across multiple tasks or datasets, involves implicit decisions about what matters for downstream use.\nAggregation across tasks raises additional issues. Mean performance across many tasks weights each task equally, regardless of clinical importance or dataset quality. Median performance is robust to outliers but obscures variation. Reporting full distributions of task-level performance provides more information but complicates comparison. The choice of aggregation method can substantially affect which model appears best.\n\n\n20.5.4 Goodhart’s Law and Benchmark Gaming\nBenchmarks create incentive structures, and incentive structures invite optimization. Goodhart’s Law, that a measure ceases to be a good measure once it becomes a target, applies with particular force to machine learning evaluation. When model development prioritizes leaderboard position, the benchmark becomes the optimization target rather than a proxy for the underlying capability it was designed to measure.\nGaming takes multiple forms in genomic AI. Architectural choices may be tuned specifically to benchmark characteristics: receptive fields sized to match benchmark sequence lengths, output heads designed for benchmark label distributions, hyperparameters selected through extensive benchmark-specific search. Such tuning improves benchmark performance without necessarily improving generalization to deployment scenarios that differ from benchmark conditions.\nMore subtle gaming arises from selective reporting. Models may be evaluated on many benchmarks with only favorable results published. Benchmark versions may be chosen to maximize apparent performance. Evaluation protocols may deviate from published standards in ways that inflate metrics. The cumulative effect is a literature where reported performance systematically overestimates deployment capability.\nThe circularity between predictors and databases creates particularly insidious gaming dynamics. When ClinVar classifications incorporate computational predictions, and those predictions are then benchmarked against ClinVar, the benchmark rewards models that resemble their predecessors rather than models that provide independent information (Chapter 22). This circularity is rarely acknowledged in benchmark reporting, yet it fundamentally compromises the validity of performance claims.\nMitigating gaming requires structural changes to evaluation practice: prospective benchmarks like CAGI where predictions precede labels, held-out evaluation consortia that resist optimization pressure, and reporting standards that require disclosure of all benchmarks attempted rather than only those where performance was favorable. The field’s maturation depends on developing evaluation cultures that reward honest assessment over leaderboard position.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html#sec-ch20-saturation-staleness",
    "href": "part_5/p5-ch20-benchmarks.html#sec-ch20-saturation-staleness",
    "title": "20  Benchmarks",
    "section": "20.6 Benchmark Saturation and Staleness",
    "text": "20.6 Benchmark Saturation and Staleness\nBenchmarks have finite useful lifetimes. As models improve, benchmarks saturate; as data and methods evolve, benchmarks become stale.\n\n20.6.1 Saturation: When Benchmarks Stop Discriminating\nA benchmark saturates when the best models achieve performance that cannot be meaningfully improved. Saturation may reflect fundamental limits (the benchmark approaches the Bayes error rate), measurement noise (the benchmark’s labels are too noisy to support finer discrimination), or ceiling effects (the metric itself cannot distinguish between excellent and perfect performance).\nSaturation is problematic because it removes the benchmark’s value for model selection. When all reasonable models achieve 0.97 auROC, differences between 0.970 and 0.975 are unlikely to reflect meaningful capability differences. Yet benchmark reporting conventions often emphasize such decimal places, creating an illusion of progress.\nDetecting saturation requires estimating the irreducible error. For benchmarks with replicate measurements, comparing model performance to replicate concordance provides an upper bound: models cannot systematically outperform the reproducibility of the underlying assay. For benchmarks without replicates, saturation is harder to diagnose. One heuristic is tracking the rate of improvement: when new methods provide diminishing gains despite substantial architectural innovations, saturation is likely.\nThe response to saturation should be moving to harder benchmarks that still discriminate between methods, developing new benchmarks that capture aspects of performance that existing benchmarks miss, and retiring saturated benchmarks from active leaderboard competition while retaining them as sanity checks.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 20.3: [High] Two-panel figure. Panel A (Saturation curve): Time vs benchmark performance; multiple lines; ceiling line; saturation zone annotation. Panel B (Staleness timeline): Benchmark creation dates above; data collection/annotation dates; current year below; growing gap; examples (ENCODE 2012, ClinVar evolving).\n\n\n\n\n\n20.6.2 Staleness: When Benchmarks Diverge from Practice\nBenchmarks become stale when they no longer reflect current data, methods, or clinical practice. Assays evolve: a benchmark constructed from early ENCODE data may not represent current experimental protocols. Annotations improve: gene models, variant classifications, and functional element maps are continuously updated. Clinical practice shifts: treatment guidelines and diagnostic criteria change the meaning of historical labels.\nStaleness is insidious because it erodes benchmark validity gradually rather than abruptly. A benchmark that accurately represented regulatory prediction in 2015 may systematically misrepresent it in 2025, yet the benchmark’s continued use perpetuates optimization for an outdated target.\nAddressing staleness requires periodic benchmark refresh with updated data and annotations, version control that documents exactly what each benchmark version contains, and awareness that performance on historical benchmarks may not predict performance on current data.\n\n\n20.6.3 Leakage from Scale\nModern foundation models are pretrained on corpora that may include most publicly available genomic data. This creates novel leakage risks distinct from classical train-test overlap. A model pretrained on all ENCODE data may effectively have seen the exact experiments used in many regulatory benchmarks. A model pretrained on all UniRef may have seen sequences highly similar to protein benchmark test sets. This pretraining-benchmark overlap inflates performance in ways that are difficult to detect and even more difficult to correct.\nLeakage from scale is particularly problematic because it is often undocumented. Model papers rarely enumerate exactly which datasets were included in pretraining corpora, and benchmark papers rarely specify which datasets should be excluded. The result is ambiguity about whether benchmark success reflects genuine generalization or memorization from pretraining.\nMitigating leakage from scale requires explicit documentation of pretraining corpora, tools or hashes that help identify overlap between pretraining data and benchmark test sets, and held-out evaluation consortia that reserve data specifically for assessment without any use in pretraining.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html#sec-ch20-deployment-gap",
    "href": "part_5/p5-ch20-benchmarks.html#sec-ch20-deployment-gap",
    "title": "20  Benchmarks",
    "section": "20.7 Benchmark-Deployment Gap",
    "text": "20.7 Benchmark-Deployment Gap\nHigh benchmark performance does not guarantee deployment success. Understanding why requires examining the systematic differences between benchmark settings and real-world applications.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 20.4: [Essential] Conceptual diagram with layers. Left column (What we measure): ClinVar labels, held-out auROC, DMS correlation, expression prediction accuracy. Right column (What we want): True clinical impact, deployment discrimination, actual protein function, regulatory mechanism. Center: Arrows with gap indicators. Gap factors: label quality, distribution shift, metric mismatch, temporal drift. Width of arrows indicates strength of proxy. Key insight: gap not uniform.\n\n\n\n\n20.7.1 Distribution Shift\nBenchmark test sets sample from the same distribution as training sets. Deployment populations may differ systematically. For variant effect prediction, benchmark variants are typically common enough to appear in multiple databases, while deployment often targets rare variants seen in single individuals. For regulatory prediction, benchmarks derive from well-studied cell types and tissues, while deployment may require prediction in understudied contexts.\nDistribution shift manifests as degraded performance, but the pattern of degradation varies. The transfer learning framework in Section 9.8 examines how models handle distribution shift from a methodological perspective, while Section 20.7.1 addresses the confounding implications when shift correlates with protected attributes. Some models degrade gracefully, maintaining reasonable accuracy across the distribution shift. Others degrade catastrophically, with confident predictions that prove systematically wrong. Benchmarks that include held-out subpopulations or out-of-distribution test sets provide some information about robustness, but cannot anticipate every deployment scenario.\n\n\n20.7.2 Calibration Requirements\nClinical deployment requires not just accurate rankings but accurate probability estimates (Section 23.2). A variant classifier that achieves 0.95 auROC by assigning probability 0.9 to all pathogenic variants and 0.3 to all benign variants discriminates well but provides miscalibrated uncertainty. Clinical decisions that depend on thresholded predictions (reporting variants above a certain probability) will perform poorly if those probabilities do not reflect actual pathogenicity rates.\nMost benchmark metrics emphasize discrimination over calibration. auROC is invariant to monotonic transformations of predicted probabilities. Correlation measures rank preservation. As a result, models may be optimized for benchmark success through strategies that damage calibration. The benchmark-deployment gap for calibration can be large even when discrimination metrics are excellent.\n\n\n20.7.3 Metric Mismatch\nBenchmarks optimize specific metrics that may not align with deployment objectives. auROC weights errors equally regardless of where they occur on the score distribution, but clinical utility may depend primarily on performance at specific operating points. Correlation rewards getting the overall pattern right but may not penalize systematic errors in clinically important regions.\nThe gap between optimized metrics and deployment objectives creates misaligned incentives. Model developers optimize for benchmark success, which rewards specific metric improvements. Deployment success may require different tradeoffs: prioritizing calibration over discrimination, minimizing false negatives over false positives, or performing well on specific subpopulations rather than overall.\n\n\n20.7.4 Practical Constraints\nDeployment environments impose constraints that benchmarks typically ignore. Inference speed matters when predictions must be returned in clinical timescales. Model size matters when deployment hardware has limited memory. Interpretability matters when predictions must be explained to clinicians or patients (Chapter 24). Benchmarks that evaluate only accuracy miss these dimensions of deployment fitness.\nThe benchmark-deployment gap is not merely a technical inconvenience. It represents a fundamental tension between evaluation tractability and deployment validity. Benchmarks are valuable precisely because they are standardized, reproducible, and comparable across methods. Deployment is valuable precisely because it addresses the specific needs of real-world applications. Bridging this gap requires benchmark designs that better approximate deployment conditions and deployment evaluations that provide feedback to benchmark development.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html#sec-ch20-systematic-gaps",
    "href": "part_5/p5-ch20-benchmarks.html#sec-ch20-systematic-gaps",
    "title": "20  Benchmarks",
    "section": "20.8 Systematic Gaps in Current Benchmarks",
    "text": "20.8 Systematic Gaps in Current Benchmarks\nDespite the proliferation of benchmark suites, systematic gaps remain in the genomic evaluation landscape.\nVariant types remain inadequately covered: structural variants, inversions, copy number variants, and complex rearrangements are rarely evaluated despite accounting for substantial genomic variation and disease burden (Section 1.5.4). Repeat regions are often excluded or masked. Multi-variant effects and haplotype-specific phenomena receive minimal attention; the phasing challenges that underlie compound heterozygosity interpretation (Section 1.4.1) rarely appear in benchmark settings.\nPopulation representation shows profound disparities: non-European ancestry groups remain severely underrepresented (Section 3.7). The confounding implications of this underrepresentation extend beyond benchmark validity to fairness concerns examined in Section 22.10. Performance stratified by ancestry reveals gaps that aggregate metrics conceal. Environmental diversity (lifestyle, exposures, treatments) that shapes phenotypic expression is rarely incorporated.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 20.5: [High] Grouped bar chart or heatmap. Rows: Different tasks/models. Columns: Ancestry groups (European, African, East Asian, South Asian, Admixed). Values: Performance metrics. Specific comparisons: PGS 40-75% reduction non-European; ClinVar-trained models across ancestries. Annotations: sample sizes, significance, training data representation. Key insight: aggregate conceals systematic failures.\n\n\n\nModality coverage remains uneven: long-read sequencing data is scarce in benchmarks despite its advantages for structural variants and phasing (Section 1.2.4). Single-cell benchmarks are emerging but remain limited compared to bulk assay benchmarks; the evaluation challenges specific to single-cell models are examined in Section 16.6.3. Spatial transcriptomics and other emerging modalities have minimal coverage, though multi-omic integration approaches (Chapter 19) are beginning to address cross-modality assessment.\nClinical endpoints are underrepresented: most benchmarks use molecular surrogates rather than hard clinical endpoints. Disease incidence, progression, treatment response, and patient-reported outcomes are rarely the direct prediction target. The gap between molecular proxy accuracy and clinical utility remains poorly characterized.\nThese gaps mean that strong benchmark performance may not predict utility for underserved populations, understudied variant classes, or clinical applications that depend on endpoints the benchmarks do not measure.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch20-benchmarks.html#sec-ch20-proxy-problem",
    "href": "part_5/p5-ch20-benchmarks.html#sec-ch20-proxy-problem",
    "title": "20  Benchmarks",
    "section": "20.9 The Proxy Problem",
    "text": "20.9 The Proxy Problem\nBenchmarks structure the incentives of genomic AI development. The specific tasks, metrics, and leaderboards that the community adopts determine what models are optimized for, what claims of progress are evaluated against, and what capabilities receive attention versus neglect. A benchmark that emphasizes European-ancestry variants produces models tuned for European-ancestry performance. A benchmark that rewards discrimination (auROC) over calibration produces models that rank variants well but estimate probabilities poorly. A benchmark that reuses training data from widely available resources creates indirect leakage that inflates apparent performance. The benchmark landscape is not neutral infrastructure but an active force shaping what the field builds.\nThe landscape surveyed here spans protein benchmarks (TAPE, FLIP, ProteinGym), DNA and regulatory benchmarks (Genomic Benchmarks, BEND), variant effect benchmarks (ClinVar, CAGI, DMS), and trait-level benchmarks (TraitGym, EmbedGEM). Across all categories, persistent challenges emerge: saturation that reduces discriminative power as models approach ceiling performance, staleness that erodes validity as benchmarks age, leakage risks that inflate apparent capabilities, and systematic gaps in population diversity, variant type coverage, and clinical endpoint representation.\nThe benchmark-deployment gap represents perhaps the most consequential limitation. Strong performance on established benchmarks does not guarantee that models will behave reliably when deployed in clinical or research settings with different data distributions, patient populations, or outcome definitions. Proper benchmark use requires attention to experiment design, metric selection, and common pitfalls (Chapter 21). The confounding issues that plague both benchmark construction and model training receive dedicated treatment in Chapter 22, while uncertainty quantification methods (Chapter 23) provide tools for assessing when benchmark performance translates to deployment confidence. Interpretability approaches (Chapter 24) reveal whether benchmark success reflects genuine biological learning or exploitation of shortcuts. Together with this catalog of what benchmarks exist, these methodological principles provide the critical apparatus for evaluating genomic foundation model claims.\n\n\n\n\nDallago, Christian, Jody Mou, Kadina E. Johnston, Bruce J. Wittmann, Nicholas Bhattacharya, Samuel Goldman, Ali Madani, and Kevin K. Yang. 2022. “FLIP: Benchmark Tasks in Fitness Landscape Inference for Proteins.” bioRxiv. https://doi.org/10.1101/2021.11.09.467890.\n\n\nGrešová, Katarína, Vlastimil Martinek, David Čechák, Petr Šimeček, and Panagiotis Alexiou. 2023. “Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification.” BMC Genomic Data 24 (1): 25. https://doi.org/10.1186/s12863-023-01123-8.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. “BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks.” arXiv. https://doi.org/10.48550/arXiv.2311.12570.\n\n\nMukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom Soare, Raghav Tandon, David Amar, et al. 2024. “EmbedGEM: A Framework to Evaluate the Utility of Embeddings for Genetic Discovery.” Bioinformatics Advances 4 (1). https://doi.org/10.1093/bioadv/vbae135.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. “ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design.” Advances in Neural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nRao, Roshan, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, and Yun S. Song. 2019. “Evaluating Protein Transfer Learning with TAPE.” arXiv. https://doi.org/10.48550/arXiv.1906.08230.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Benchmarks</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html",
    "href": "part_5/p5-ch21-eval.html",
    "title": "21  Evaluation Principles",
    "section": "",
    "text": "21.1 Why Random Splits Fail\nGenomic data makes it exceptionally easy to fool yourself. Sequences share evolutionary history, so a model that memorizes training sequences may appear to generalize when tested on homologs. Variants cluster in families and populations, so ancestry-stratified performance can masquerade as genuine prediction. Experimental measurements carry batch effects invisible to the untrained eye, so a model can learn to distinguish sequencing centers rather than biological states. Training labels often derive from the very databases used for evaluation, creating circular validations that inflate performance without testing genuine predictive power. Every shortcut that simplifies evaluation in other machine learning domains becomes an opportunity for false confidence in genomics.\nRandom data splits that work perfectly well for natural images become actively misleading when applied to biological sequences. A protein held out for testing may share 90% sequence identity with a training protein, allowing the model to succeed through memorization rather than generalization. A variant classified as pathogenic in the test set may come from the same gene family as training variants, letting the model exploit gene-level signals rather than learning variant-specific effects. A cell line in the test set may have been processed at the same sequencing center as training samples, enabling the model to recognize batch signatures rather than biological patterns. These leakages are not hypothetical; they have inflated reported performance across the genomic machine learning literature.\nThe difference between valid and misleading evaluation often lies not in benchmark choice but in methodological details: data splitting strategies, metric selection, baseline comparisons, ablation designs, and statistical testing. Chapter 20 catalogs what benchmark tasks exist, how they are constructed, and what capabilities they probe. The complementary question: given a benchmark, how do we apply it to produce trustworthy results? The difference between valid and misleading evaluation often lies not in benchmark choice but in methodological details: data splitting strategies, metric selection, baseline comparisons, ablation designs, and statistical testing. These principles apply across all benchmark categories, from chromatin state prediction to clinical variant classification. By mastering evaluation methodology, practitioners can distinguish genuine advances from artifacts that will not survive deployment.\nThe standard machine learning recipe calls for randomly partitioning data into training, validation, and test sets. For image classification or sentiment analysis, this approach works well because individual examples are approximately independent. A photograph of a cat shares no special relationship with another photograph of a different cat beyond their common label. Random assignment ensures that training and test distributions match, and performance on the test set provides an unbiased estimate of performance on new examples from the same distribution.\nGenomic data violates these assumptions at every level. Consider a protein dataset where the goal is to predict stability from sequence. Proteins in the same family share evolutionary history and often similar structures. If a training set includes beta-lactamase variants from E. coli and the test set includes beta-lactamase variants from Klebsiella, the model may appear to generalize to “new” proteins while actually recognizing sequence patterns it saw during training. The test performance reflects memorization of family-specific features rather than general principles of protein stability.\nThe problem compounds when sequence identity is high. Two proteins sharing 80% sequence identity will typically have similar structures and functions. A model trained on one and tested on the other is not really being tested on a novel example; it is being asked to interpolate within a region of sequence space it has already explored. Even at 30% sequence identity, the so-called “twilight zone” of homology detection (rost_twilight_1999?), proteins often share structural and functional similarities that can be exploited by sufficiently powerful models.\nVariant-level data presents analogous challenges. Variants within the same gene share genomic context, and variants affecting the same protein domain share structural environment. Variants from the same individual share haplotype background. Variants from the same population share allele frequency distributions shaped by demographic history. Each of these relationships creates opportunities for models to learn shortcuts that generalize within the training distribution but fail on genuinely novel examples.\nThe consequence is that random splits systematically overestimate generalization. A model that achieves 0.90 area under the receiver operating characteristic curve (auROC) with random splitting might achieve only 0.75 auROC when evaluated on truly held-out examples, with the gap reflecting how much the model learned about biology versus how much it learned about the structure of the training data. Recognizing this problem is the first step toward solving it.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-random-splits-fail",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-random-splits-fail",
    "title": "21  Evaluation Principles",
    "section": "",
    "text": "FIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 21.1: [Essential] Three-panel comparison. Panel A (Image classification): Grid of images; random assignment works; independent. Panel B (Protein classification): Family tree; random split places related across train/test; 80% identity highlighted; “memorization pathway.” Panel C (Variant prediction): Gene diagram with variants; same gene in train/test; family pedigree spanning splits; population structure. Annotations: sequence identity, relatedness, pseudo-replication.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-homology-aware-splitting",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-homology-aware-splitting",
    "title": "21  Evaluation Principles",
    "section": "21.2 Homology-Aware Splitting",
    "text": "21.2 Homology-Aware Splitting\nThe solution to homology-driven leakage is to explicitly account for sequence similarity when constructing data splits. Rather than random assignment, examples are clustered by sequence identity, and entire clusters are assigned to training, validation, or test sets. This ensures that no test example is “too similar” to any training example, forcing the model to demonstrate genuine generalization.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 21.2: [Essential] Step-by-step workflow. Steps: (1) All sequences; (2) Clustering (CD-HIT/MMseqs2 at threshold); (3) Cluster visualization; (4) Split assignment (entire clusters to train/val/test); (5) Validation (no test &gt;X% identity to training). Code snippets. Comparison table (strategy, strictness, efficiency, use case). Key insight: threshold determines hardness.\n\n\n\n\n21.2.1 Clustering Tools and Workflows\nTwo tools dominate homology-aware splitting in practice. CD-HIT clusters sequences by greedy incremental clustering, assigning each sequence to an existing cluster if it exceeds a similarity threshold to the cluster representative, or creating a new cluster otherwise (li_cd-hit_2006?). The algorithm is fast and scales to millions of sequences. For proteins, a typical workflow clusters at 40% sequence identity for stringent splitting or 70% for moderate splitting. For nucleotide sequences, thresholds are typically higher (80-95%) due to different evolutionary rates.\nMMseqs2 offers faster clustering with similar sensitivity, becoming essential for large-scale analyses (steinegger_mmseqs2_2017?). The tool supports multiple clustering modes and can handle databases with hundreds of millions of sequences. For foundation model pretraining where deduplication affects billions of sequences, MMseqs2 is often the only practical option.\nThe choice of identity threshold involves trade-offs. Stricter thresholds (lower identity for proteins, higher for nucleotides) create harder generalization tests but may leave insufficient data for training if clusters are small. Permissive thresholds retain more training data but allow more leakage through homologous sequences. For protein function prediction, 30-40% identity thresholds are common; for variant effect prediction within genes (Chapter 14), even stricter gene-family-level splits may be necessary.\n\n\n21.2.2 Practical Considerations\nSeveral subtleties affect the quality of homology-aware splits. When one cluster contains half the data and is assigned to training, the remaining clusters may be too small or too biased to serve as representative test sets. This cluster size distribution problem can be mitigated through stratified sampling within clusters or careful balancing across splits, ensuring that test sets contain sufficient examples across the label distribution.\nPairwise clustering can miss hidden relationships that arise through transitive homology. Protein A may share 35% identity with protein B, and protein B may share 35% identity with protein C, yet A and C share only 20% identity. If A is in training and C is in testing, B serves as an indirect bridge that allows information to leak between splits despite no direct high-identity pair spanning them. Connected component analysis or multi-step clustering can address these transitive relationships, though at increased computational cost.\nMulti-domain proteins complicate whole-protein clustering because different domains may have different evolutionary histories. A protein may share one domain with training proteins and another domain with test proteins. Whether this represents leakage depends on the prediction task: if predicting whole-protein function, shared domains matter; if predicting domain-specific properties, they matter more acutely. Domain-aware splitting assigns domains rather than whole proteins to clusters, though this requires domain annotation that may not always be available.\nFor genomic (non-protein) sequences, repeat elements and transposable elements create analogous challenges. A model trained to predict chromatin state may learn features of LINE elements that recur throughout the genome. Excluding repetitive regions from evaluation or explicitly accounting for repeat content can clarify what the model has actually learned about regulatory sequences versus repetitive element patterns.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-splitting-biological-axis",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-splitting-biological-axis",
    "title": "21  Evaluation Principles",
    "section": "21.3 Splitting by Biological Axis",
    "text": "21.3 Splitting by Biological Axis\nBeyond sequence homology, genomic data admits multiple axes along which splits can be constructed. The choice of axis determines what kind of generalization is being tested.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 21.3: [High] Matrix: Splitting strategies vs. what they test. Rows: Random, individual-aware, family-aware, chromosome, gene/protein family, cohort/site, temporal, ancestry. Columns: Sequence generalization, individual, population, temporal robustness, cross-site. Cells: ✓, ✗, ~. Schematics for each strategy; performance drop annotations; combinable strategies note.\n\n\n\n\n21.3.1 Splitting by Individual\nFor tasks involving human genetic variation, ensuring that data from the same individual (or related individuals) does not appear in both training and test sets is essential. A variant effect predictor trained on variants from person A and tested on other variants from person A may learn individual-specific patterns, such as haplotype structure or ancestry-correlated allele frequencies, that do not generalize to new individuals.\nFamily structure creates subtler leakage. First-degree relatives share approximately 50% of their genomes identical by descent. Even distant relatives share genomic segments that can be exploited by sufficiently powerful models. Best practice involves computing kinship coefficients across all individuals and either excluding one member of each related pair or assigning entire family clusters to the same split. The UK Biobank provides pre-computed relatedness estimates; other cohorts may require explicit calculation using tools like KING or PLINK. [Citation Needed]\n\n\n21.3.2 Splitting by Genomic Region\nChromosome-based splits assign entire chromosomes to training or testing. This approach is common in regulatory genomics, where models trained on chromosomes 1-16 are tested on chromosomes 17-22 (or similar partitions). The advantage is simplicity and reproducibility; the disadvantage is that chromosomes are not independent. Chromosome 6 contains the HLA region with its unusual patterns of variation and selection; chromosome 21 is small and gene-poor; sex chromosomes have distinct biology. Results may vary substantially depending on which chromosomes are held out.\nRegion-based splits hold out contiguous segments (e.g., 1 Mb windows) distributed across the genome. This provides more uniform coverage than chromosome splits but requires careful attention to boundary effects. If a regulatory element spans the boundary between training and test regions, parts of its context may leak into training.\n\n\n21.3.3 Splitting by Gene or Protein Family\nFor variant effect prediction, holding out entire genes or protein families tests whether models learn general principles versus gene-specific patterns. A model that achieves high accuracy by memorizing that TP53 variants are often pathogenic has not demonstrated understanding of mutational mechanisms. Gene-level splits force models to generalize to genes they have never seen, providing stronger evidence of biological insight.\nFamily-level splits extend this logic to groups of related genes. Holding out all kinases or all GPCRs tests whether models can generalize across evolutionary families. This is particularly stringent for protein structure and function prediction, where family membership strongly predicts properties.\n\n\n21.3.4 Splitting by Experimental Context\nMulti-task models that predict chromatin marks across cell types can be split by cell type rather than genomic position. Training on liver, lung, and brain while testing on heart and kidney assesses whether learned regulatory logic transfers across tissues. This matters because cell-type-specific factors drive much of regulatory variation; a model that has simply learned which regions are accessible in the training cell types may fail on novel cell types even when sequence features should transfer.\nSimilarly, models can be split by assay type (e.g., training on ATAC-seq, testing on DNase-seq), laboratory (to assess batch effects), or time point (for longitudinal data). Each split tests a different axis of generalization.\n\n\n21.3.5 Splitting by Ancestry\nFor human genomic applications, ancestry-stratified evaluation has become essential. Models trained predominantly on European ancestry cohorts often show degraded performance in African, East Asian, South Asian, and admixed populations. This degradation reflects both differences in allele frequency spectra and differences in linkage disequilibrium patterns that affect which variants are informative.\nBest practice reports performance separately for each major ancestry group represented in the data. When held-out ancestry groups are available (e.g., training on Europeans and testing on Africans), this provides the strongest test of cross-population generalization. When only European data are available, this limitation should be explicitly acknowledged, and claims about generalization should be appropriately modest. The confounding effects of ancestry on genomic predictions are detailed in Chapter 22.\n\n\n21.3.6 Splitting by Time\nTemporal splits assign data to training and test sets based on when observations were collected, annotations were created, or variants were classified. This strategy tests whether models generalize forward in time, the actual deployment scenario for any predictive system.\nFor variant pathogenicity prediction, temporal splits are particularly revealing. ClinVar (Section 2.8.1) provides submission dates enabling clean temporal partitioning. Training on ClinVar annotations from 2018 and testing on variants first classified in 2022 asks whether the model can predict labels that did not yet exist during training. This avoids the circularity that arises when training and test labels were assigned by similar processes at similar times. Variants classified more recently may reflect updated curation standards, new functional evidence, or reclassifications of previously uncertain variants; a model that performs well on these genuinely new classifications demonstrates predictive validity rather than recapitulation of historical curation patterns.\nImplementing temporal splits requires metadata that many datasets lack. ClinVar provides submission dates, enabling clean temporal partitioning. UniProt tracks annotation dates for functional assignments. Clinical cohorts with longitudinal follow-up naturally admit temporal splits based on diagnosis dates. When temporal metadata is unavailable, publication dates of source literature can serve as proxies, though these may not perfectly reflect when information became available to model developers.\nThe key limitation of temporal splits is non-stationarity. The distribution of variants classified in 2022 may differ systematically from those classified in 2018, not because biology changed but because research priorities, sequencing technologies, and ascertainment patterns evolved. Performance degradation on temporally held-out data may reflect distribution shift rather than genuine failure to generalize. Combining temporal splits with stratified analysis (performance by variant type, gene category, or evidence strength) helps disentangle these factors.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-leakage-detection",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-leakage-detection",
    "title": "21  Evaluation Principles",
    "section": "21.4 Leakage Taxonomy and Detection",
    "text": "21.4 Leakage Taxonomy and Detection\nEven with careful splitting, leakage can enter evaluations through multiple pathways. A variant effect predictor that achieves 0.95 auROC on held-out test data may be exploiting information that would never exist for truly novel variants, rendering the performance estimate meaningless for clinical deployment. Understanding common leakage patterns helps practitioners design cleaner evaluations and critically assess published results.\nGenomic machine learning faces four distinct leakage types, each creating different pathways for inflated performance: label leakage, feature leakage, temporal leakage, and benchmark leakage. These categories are not mutually exclusive; a single evaluation may suffer from multiple forms simultaneously, with compounding effects on apparent performance.\n\n21.4.1 Label Leakage\nLabel leakage occurs when target labels are derived from information that the model can access through its features. The classic example is training pathogenicity predictors on ClinVar annotations while using sequence features that contributed to those annotations. If ClinVar curators used SIFT and PolyPhen scores when classifying variants, and the new model uses similar sequence features, high performance may reflect recapitulation of curation criteria rather than independent predictive power.\nThe ClinVar circularity problem represents a particularly insidious form of label leakage. When computational predictions contributed to the pathogenicity classifications that later become training labels, new models learn to replicate their predecessors rather than discover independent signal. This circularity propagates through generations of models, each inheriting and reinforcing the biases of earlier predictors. The circularity problem for classical variant effect predictors is examined in Section 4.5, with broader treatment of how such label contamination creates confounded evaluations in Section 22.2.4.\nExpression models face analogous challenges when trained on features derived from the same samples used to define expression labels. The information flow becomes circular: labels inform features, which predict labels, creating apparent performance that would not generalize to independent samples.\n\n\n21.4.2 Feature Leakage\nFeature leakage occurs when input features encode information about the target that would not be available at prediction time. In genomics, conservation scores are a common source. If a model uses PhyloP scores as features and the target is pathogenicity, the model may learn that conserved positions are more likely pathogenic without learning anything about variant-specific biology. This would be appropriate if conservation scores are intended to be part of the prediction pipeline, but problematic if the goal is to develop a model that predicts pathogenicity from sequence alone.\nSimilarly, population allele frequency encodes selection pressure. A model that learns “rare variants are more likely pathogenic” has discovered a useful heuristic but not necessarily mechanistic understanding. Whether this counts as leakage depends on the intended use case. For clinical variant interpretation where allele frequency is always available, exploiting this feature is appropriate. For understanding variant biology, it may mask whether the model has learned anything beyond frequency-based priors.\nFeature leakage also arises when features encode information about data partitions or batch structure rather than biology. Coverage patterns that differ systematically between cases and controls, quality metrics that correlate with sequencing center, or variant density profiles that reflect caller-specific behavior all constitute feature leakage of this form.\n\n\n21.4.3 Temporal Leakage\nTemporal leakage violates the causal structure of prediction by using future information to predict past events. A model trained on ClinVar annotations from 2023 and tested on annotations that were uncertain in 2020 may perform well because new annotations were informed by model-like predictions. The apparent validation is circular: the model predicts labels that were partially derived from model-like reasoning applied after the prediction timepoint.\nClinical outcome prediction faces similar risks when laboratory values, imaging results, or clinical notes recorded after the prediction timepoint enter the feature set. A model predicting 30-day mortality that includes vital signs from day 15 has trivial access to outcome-correlated information. Proper temporal splits must respect not only when samples were collected but when each feature became available.\nTraining on variants classified in 2023 to predict classifications that were uncertain in 2020 allows models to learn from reclassification patterns rather than intrinsic variant properties. The model exploits the trajectory of scientific knowledge rather than the underlying biology.\n\n\n21.4.4 Benchmark Leakage\nBenchmark leakage occurs when test set construction was influenced by methods similar to those being evaluated. If a protein function benchmark was created by selecting proteins with high-confidence annotations, and those annotations were partly derived from sequence similarity searches, sequence-based models may perform well by exploiting the same similarity that guided benchmark construction.\nFoundation models face particular challenges with benchmark leakage. If a DNA language model is pretrained on all publicly available genomic sequence including ENCODE data, and then evaluated on ENCODE-derived benchmarks, the pretraining has exposed the model to information about the test distribution even if specific test examples were held out. The model may have learned statistical patterns in ENCODE data that transfer to ENCODE benchmarks without reflecting genuine biological understanding.\nThis form of leakage is especially difficult to detect because it operates at the level of distributional overlap rather than specific example memorization. A model that has never seen a particular test sequence may still have learned the statistical regularities that make that sequence predictable within the benchmark distribution.\n\n\n21.4.5 Detecting Leakage\nSeveral strategies help detect leakage, though none provides definitive proof of its absence. These approaches complement each other; rigorous evaluation employs multiple strategies, recognizing that each catches different leakage pathways while remaining blind to others.\nSimple models that could not plausibly have learned biology provide an essential baseline analysis. If a linear model using only allele frequency achieves 0.80 auROC on a pathogenicity benchmark, and a sophisticated deep model achieves 0.82, the marginal improvement may not justify claims of biological insight. The deep model’s performance is bounded by what simple confounders already explain.\nSystematic feature ablation removes potentially leaky features and measures performance degradation. If removing conservation scores causes a 20-point drop in auROC, the model was heavily dependent on conservation rather than learning independent predictors. This approach identifies which features drive performance but cannot distinguish legitimate signal from leakage without domain knowledge about what information should be available at prediction time.\nExplicit confounder analysis models potential confounders and tests whether model predictions remain informative after conditioning. If a variant effect predictor’s scores become non-predictive after controlling for gene length and expression level, the model may have learned gene-level confounders rather than variant-level effects. Chapter 22 examines how leakage relates to these broader confounding structures.\nTemporal validation evaluates models on data collected after the training data was frozen. If performance degrades substantially on newer data, the model may have been fitted to temporal artifacts in the original dataset. This approach is particularly valuable for detecting temporal leakage but requires access to prospectively collected data.\nFinally, overlap auditing explicitly checks for sequence or sample overlap between pretraining corpora and evaluation benchmarks. For foundation models, this requires documenting pretraining data composition and comparing against benchmark construction procedures. The audit may reveal that apparent generalization is actually interpolation within seen distributions.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-metrics-genomic-tasks",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-metrics-genomic-tasks",
    "title": "21  Evaluation Principles",
    "section": "21.5 Metrics for Genomic Tasks",
    "text": "21.5 Metrics for Genomic Tasks\nMetrics quantify model performance but different metrics answer different questions. Choosing appropriate metrics requires clarity about what aspect of performance matters for the intended application.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 21.4: [High] Decision flowchart. Decision points: Binary vs continuous? Balanced? Ranking vs probability? Clinical decisions? Metric recommendations at each terminal. Metric descriptions: auROC, area under the precision-recall curve (auPRC), calibration, net benefit. Warning callouts: “auROC invariant to monotonic transforms”; “High correlation ≠ clinically meaningful.”\n\n\n\n\n21.5.1 Discrimination Metrics\nFor binary outcomes (pathogenic versus benign, bound versus unbound, accessible versus closed), discrimination metrics assess how well the model separates classes. The auROC measures the probability that a randomly selected positive example is ranked above a randomly selected negative example. auROC is threshold-independent and widely reported but can be misleading when classes are highly imbalanced.\nThe auPRC better reflects performance when positives are rare. For variant pathogenicity prediction, where perhaps 1% of variants are truly pathogenic, a model achieving 0.95 auROC might still have poor precision at useful recall levels. auPRC directly captures the precision-recall trade-off that matters for applications requiring both high sensitivity and manageable false positive rates.\nThe distinction between these metrics reflects a mathematical property with practical consequences. auROC is invariant to class imbalance: a model’s auROC remains identical whether 1% or 50% of examples are positive, because it measures pairwise ranking between one positive and one negative. This invariance makes auROC valuable for comparing models across datasets with different prevalence. A variant effect predictor can be compared against the same model on a different benchmark without performance differences being confounded by differing positive rates in each benchmark.\nThis same invariance becomes a liability when evaluating for deployment. A model with 0.95 auROC applied to a dataset where 0.1% of variants are pathogenic might flag 100 false positives for every true pathogenic variant at a threshold capturing 80% of positives. The auROC provides no warning of this behavior because it treats a positive-to-negative pair the same regardless of how many negatives exist. For any application where false positives carry real costs (manual curation, clinical follow-up, unnecessary patient anxiety), auROC presents an optimistic picture that collapses upon deployment.\nauPRC explicitly accounts for the negative class size. When positives are rare, achieving high precision requires a model that scores the vast majority of negatives lower than positives, not just a typical negative. This makes auPRC sensitive to class imbalance in exactly the way deployment is sensitive to class imbalance. A model moving from a balanced benchmark to a 1000:1 imbalanced application will show stable auROC but declining auPRC, mirroring the actual increase in false discovery rate users will experience. For this reason, auPRC (or equivalently, average precision) should be the primary metric when the deployment class distribution is known and imbalanced.\nThreshold-dependent metrics including sensitivity, specificity, positive predictive value, and negative predictive value require specifying a decision threshold. These metrics are more interpretable for specific use cases (e.g., “the model identifies 90% of pathogenic variants while flagging only 5% of benign variants as false positives”) but require choosing thresholds that may not generalize across settings.\n\n\n21.5.2 Regression and Correlation Metrics\nFor continuous predictions (expression levels, effect sizes, binding affinities), correlation metrics assess agreement between predicted and observed values. Pearson correlation measures linear association; Spearman correlation measures rank association and is robust to nonlinear relationships. The coefficient of determination (\\(R^2\\)) measures variance explained, though interpretation requires care when baseline performance is near zero.\nFor predictions at genomic scale (e.g., predicted versus observed expression across thousands of genes), these metrics may obscure important patterns. A model might achieve high genome-wide correlation by correctly predicting which genes are highly expressed while failing on the genes where predictions matter most. Task-specific stratification, such as correlation within expression quantiles or among disease-relevant genes, provides more actionable information.\n\n\n21.5.3 Ranking and Prioritization Metrics\nMany genomic workflows care about ranking rather than absolute prediction. Variant prioritization pipelines rank candidates for follow-up; gene prioritization ranks targets for experimental validation. Top-k recall measures the fraction of true positives captured in the top \\(k\\) predictions. Enrichment at k compares the true positive rate in the top \\(k\\) to the background rate. Normalized discounted cumulative gain (NDCG) weights ranking quality by position, penalizing relevant items placed lower in the list more than those placed near the top.\nThese metrics align with how predictions are actually used. If experimental capacity permits validating only 20 variants per locus, top-20 recall matters more than global auROC. Reporting both global metrics and rank-aware metrics at relevant cutoffs provides a complete picture.\n\n\n21.5.4 Clinical Utility Metrics\nFor clinical applications, discrimination and calibration are necessary but not sufficient. Decision curves plot net benefit across decision thresholds, where net benefit weighs the value of true positives against the cost of false positives at each threshold. A model may achieve high auROC but offer no net benefit at clinically relevant thresholds if it fails to discriminate in the region where decisions are actually made.\nNet reclassification improvement (NRI) measures how often adding genomic features to a clinical model changes risk classifications in the correct direction. This directly addresses whether genomics adds clinical value beyond existing predictors. Chapter 25 provides detailed treatment of clinical evaluation frameworks.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-baseline-selection",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-baseline-selection",
    "title": "21  Evaluation Principles",
    "section": "21.6 Baseline Selection",
    "text": "21.6 Baseline Selection\nBaseline comparisons determine the meaning of reported performance. A model achieving 0.85 auROC might represent a major advance if the best prior method achieved 0.70, or a trivial improvement if simple heuristics achieve 0.83. Choosing appropriate baselines is as important as choosing appropriate metrics.\n\n21.6.1 Strong Baselines, Not Straw Men\nThe temptation to compare against weak baselines inflates apparent contributions. A deep learning model compared against a naive prior or a deliberately crippled baseline will appear impressive regardless of whether it offers genuine value. Strong baselines force honest assessment of improvement.\nFor sequence-based predictions, position weight matrices (PWMs) and k-mer logistic regression provide classical baselines that capture sequence composition without deep learning. If a convolutional model barely outperforms logistic regression on k-mer counts, the convolutional architecture may not be contributing as much as claimed.\nFor variant effect prediction, simple features like allele frequency, conservation scores, and amino acid properties provide baselines that any sophisticated model should substantially exceed. CADD (Section 4.3) serves as a well-calibrated baseline that combines many hand-crafted features; outperforming CADD demonstrates that learning provides value beyond feature engineering.\nFor foundation models, comparisons should include both randomly initialized models of similar architecture (to isolate the value of pretraining) and simpler pretrained models (to isolate the value of scale or architectural innovations). Claiming that pretraining helps requires demonstrating improvement over training from scratch on the same downstream data.\n\n\n21.6.2 Historical Baselines and Progress Tracking\nComparing to methods from five years ago may demonstrate progress but overstates the contribution of any single method. Comparisons should include the best currently available alternatives, not just historically important ones. When prior work is not directly comparable (different data, different splits, different metrics), reimplementing baselines on common benchmarks provides fairer comparison.\nField-wide progress tracking benefits from persistent benchmarks with frozen test sets. Once test set results for a benchmark are published, that benchmark becomes less useful for future model development because the test set is no longer truly held out. Periodic benchmark refresh with new held-out data helps maintain evaluation integrity.\n\n\n21.6.3 Non-Deep-Learning Baselines\nDeep learning models should be compared against strong non-deep alternatives. Gradient-boosted trees, random forests, and regularized linear models often achieve competitive performance with far less computation. If a 100-million-parameter transformer barely outperforms XGBoost on tabular features, the complexity may not be justified.\nThis comparison is especially important for clinical deployment, where simpler models may be preferred for interpretability, computational efficiency, or regulatory approval. Demonstrating that deep learning provides substantial gains over strong non-deep baselines strengthens the case for adoption.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-ablation-studies",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-ablation-studies",
    "title": "21  Evaluation Principles",
    "section": "21.7 Ablation Studies",
    "text": "21.7 Ablation Studies\nAblation studies systematically remove or modify model components to understand their contributions. Where baselines compare across methods, ablations investigate within a method, revealing which design choices actually matter.\n\n21.7.1 Component Isolation\nStandard ablations remove individual components: attention layers, skip connections, normalization schemes, specific input features. If removing attention heads causes minimal performance degradation, the model may not be exploiting long-range dependencies as claimed. If removing a particular input modality has no effect, that modality may not be contributing useful information.\nAblations should be designed to test specific hypotheses. If the claim is that a foundation model learns biologically meaningful representations, ablating pretraining (comparing to random initialization) directly tests this claim. If the claim is that cross-attention between modalities enables integration, ablating cross-attention while retaining separate encoders tests whether integration provides value.\n\n\n21.7.2 Hyperparameter Sensitivity\nReporting performance across hyperparameter ranges reveals robustness. A model that achieves state-of-the-art performance only at a narrow learning rate range with specific regularization may be overfit to the evaluation setup. Consistent performance across reasonable hyperparameter variations provides stronger evidence of genuine capability.\n\n\n21.7.3 Architecture Search Confounds\nWhen model development involves extensive architecture search, reported performance conflates the value of the final architecture with the value of search on the validation set. The validation set is no longer truly held out; it has been used to select among architectures. Final evaluation on a completely untouched test set, with the architecture fixed before test set examination, provides cleaner assessment.\n\n\n21.7.4 Reporting Standards\nAblation tables should clearly indicate what was changed in each condition, the number of random seeds or runs, and measures of variance. Single-run ablations can produce misleading results due to training stochasticity. Reporting means and standard deviations across multiple runs reveals whether observed differences exceed random variation.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-statistical-rigor",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-statistical-rigor",
    "title": "21  Evaluation Principles",
    "section": "21.8 Statistical Rigor",
    "text": "21.8 Statistical Rigor\nPerformance differences between models may reflect genuine capability differences or random variation in training and evaluation. Statistical analysis distinguishes signal from noise.\n\n21.8.1 Significance Testing\nFor classification metrics, significance tests ask whether observed differences exceed what would be expected from sampling variation. Bootstrap confidence intervals resample the test set with replacement, recompute metrics on each resample, and report the distribution of metric values. Non-overlapping 95% confidence intervals suggest significant differences. Permutation tests shuffle predictions between models and measure how often shuffled differences exceed observed differences.\nFor comparing multiple models across multiple benchmarks, correction for multiple testing becomes important. Without correction, 20 pairwise comparisons will produce an expected one false positive at the 0.05 level even when all models perform equally. The Bonferroni correction divides the significance threshold by the number of tests; the Benjamini-Hochberg procedure controls false discovery rate with more power than Bonferroni. [Citation Needed]\n\n\n21.8.2 Effect Sizes\nStatistical significance does not imply practical significance. A difference of 0.001 auROC might be statistically significant with millions of test examples while being practically meaningless. Effect sizes quantify the magnitude of differences independent of sample size. Cohen’s d for continuous outcomes and odds ratios for binary outcomes provide standardized measures of effect magnitude.\nReporting both significance tests and effect sizes provides complete information. A result that is statistically significant with a tiny effect size warrants different interpretation than one that is significant with a large effect size.\n\n\n21.8.3 Confidence Intervals on Metrics\nPoint estimates of auROC or correlation should be accompanied by confidence intervals. DeLong’s method provides analytical confidence intervals for auROC (delong_comparing_1988?); bootstrap methods provide distribution-free intervals for any metric. Reporting “auROC = \\(0.85\\) (95% CI: \\(0.82\\)–\\(0.88\\))” is more informative than “auROC = \\(0.85\\)” alone.\n\n\n21.8.4 Variance Across Random Seeds\nDeep learning models are sensitive to initialization and optimization stochasticity. Training the same architecture with different random seeds can produce substantially different results. Best practice trains multiple runs and reports means and standard deviations. If the standard deviation across runs exceeds the difference between methods, claimed improvements may not be reproducible.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-evaluating-fm",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-evaluating-fm",
    "title": "21  Evaluation Principles",
    "section": "21.9 Evaluating Foundation Models",
    "text": "21.9 Evaluating Foundation Models\nGenomic foundation models (Chapter 10) admit multiple evaluation paradigms, each testing different aspects of learned representations.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 21.5: [Enhancing] Three-column comparison. Column 1 (Zero-shot): Frozen model → direct prediction; tests alignment; ESM-1v log-likelihood example; pros/cons. Column 2 (Linear Probing): Frozen → embeddings → linear classifier; tests linear accessibility; isolates representation quality; pros/cons. Column 3 (Fine-tuning): Gradients → adaptation; tests total potential; best performance but conflates representation with adaptation. Bottom: Data efficiency curve; “Pretraining value = gap at low data.”\n\n\n\n\n21.9.1 Zero-Shot Evaluation\nIn zero-shot evaluation, the pretrained model is applied without any task-specific training. For masked language models, this typically means using token probabilities to score variants or classify sequences. A variant that disrupts a position the model predicts with high confidence may indicate functional importance.\nZero-shot performance tests whether pretraining captures task-relevant structure without explicit supervision. Strong zero-shot performance suggests the pretraining objective aligned with the evaluation task; weak zero-shot performance suggests misalignment. Comparing zero-shot performance to simple baselines (e.g., conservation scores for variant effects) calibrates whether the foundation model provides value beyond what simpler approaches achieve.\n\n\n21.9.2 Linear Probing\nLinear probing freezes the foundation model and trains only a linear classifier on extracted embeddings. This isolates representation quality from fine-tuning capacity. If a linear probe on foundation model embeddings substantially outperforms a linear probe on random embeddings, the foundation model has learned useful features.\nLayer-wise probing reveals where information is encoded. Early layers may capture local sequence features while later layers capture more abstract patterns. If the information needed for a task is extractable from early layers, the model may not require the full depth of the architecture for that application.\n\n\n21.9.3 Fine-Tuning Evaluation\nFull fine-tuning adapts all model parameters to the downstream task. This provides the best performance but conflates representation quality with adaptation capacity. A foundation model might achieve high fine-tuned performance through the capacity of its architecture rather than the quality of its pretrained representations.\nComparing fine-tuned foundation models to equivalently architected models trained from scratch isolates the value of pretraining. If both approaches converge to similar performance given sufficient downstream data, pretraining provides label efficiency (less data needed to reach a given performance level) rather than improved final performance. Data efficiency curves, plotting performance against downstream training set size, reveal this trade-off.\n\n\n21.9.4 Transfer Across Tasks\nFoundation models justify their “foundation” designation by transferring to diverse downstream tasks. Evaluating on a single task, however well-designed, cannot assess breadth of transfer. Multi-task evaluation across regulatory prediction, variant effects, protein properties, and other applications reveals whether foundation models provide general-purpose representations or excel only on tasks similar to their pretraining objective.\nTransfer across species, tissues, and experimental modalities provides additional evidence of generalization. A DNA language model that transfers from human to mouse, or from blood cells to neurons, demonstrates that its representations capture biological principles rather than species-specific or tissue-specific patterns.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-calibration",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-calibration",
    "title": "21  Evaluation Principles",
    "section": "21.10 Calibration Essentials",
    "text": "21.10 Calibration Essentials\nStrong discrimination does not guarantee useful probability estimates. A model achieving 0.95 auROC might assign probability 0.99 to all positive examples and 0.98 to all negatives, ranking perfectly while providing meaningless confidence values. Clinical decision-making requires both: accurate ranking to identify high-risk variants and accurate probabilities to inform the weight of computational evidence. Calibration assesses whether predicted probabilities match observed frequencies, a property essential for rational integration of model outputs into diagnostic workflows.\n\n21.10.1 Assessing Calibration\nThe most intuitive assessment comes from reliability diagrams, which plot predicted probabilities against observed frequencies. The construction bins predictions into intervals (commonly ten bins spanning 0 to 0.1, 0.1 to 0.2, and so forth), computes the mean predicted probability within each bin, computes the fraction of positive examples within each bin, and plots these quantities against each other. Perfect calibration produces points along the diagonal; systematic deviations reveal overconfidence (points below the diagonal) or underconfidence (points above).\nA single summary statistic, the expected calibration error (ECE), captures miscalibration as the weighted average absolute difference between predicted and observed probabilities across bins. Lower ECE indicates better calibration. The metric depends on binning choices; equal-width bins may place most examples in a few bins for models with concentrated predictions, while equal-mass bins ensure each bin contains the same number of examples but may span wide probability ranges. ECE should be reported alongside reliability diagrams for interpretability.\nAggregate calibration metrics can mask important heterogeneity. A model might achieve low aggregate ECE while being systematically overconfident for rare variant classes and underconfident for common ones, with opposite errors canceling in the aggregate statistic. Stratified calibration analysis across ancestry groups, variant classes, and gene categories identifies these disparities. For genomic models intended for diverse populations, subgroup-stratified calibration is not optional; aggregate metrics can mask clinically significant differential performance.\n\n\n21.10.2 Recalibration Methods\nPost-hoc recalibration adjusts predicted probabilities without retraining the underlying model. Methods range from single-parameter approaches like temperature scaling (Guo et al. 2017), which divides logits by a learned constant to compress overconfident distributions, to non-parametric transformations like isotonic regression, which fits a monotonic function mapping raw scores to calibrated probabilities. Platt scaling (platt_probabilistic_1999?) fits a logistic regression from model outputs to true labels, providing intermediate flexibility. Each method makes different assumptions about the structure of miscalibration and requires different amounts of calibration data. The mathematical details, theoretical foundations, and guidance for method selection are developed in Section 23.3.\nAll recalibration methods require held-out calibration data distinct from both training and test sets. Calibrating on test data and then evaluating calibration on the same test data produces overoptimistic estimates. For foundation models, the calibration set should be drawn from the deployment distribution; calibrating on ClinVar expert-reviewed variants may not transfer to variants in less-studied genes or underrepresented populations.\n\n\n21.10.3 Calibration in Model Comparison\nWhen comparing models, calibration metrics complement discrimination metrics. Two models with identical auROC may have dramatically different calibration, and the better-calibrated model will produce more reliable clinical evidence even though its ranking performance is equivalent. Reporting both discrimination (auROC, auPRC) and calibration (ECE, reliability diagrams) provides a complete picture of model performance.\nCalibration can often be improved post-hoc without sacrificing discrimination. Temperature scaling preserves ranking while adjusting probability magnitudes, meaning a model can be recalibrated to improve ECE without changing auROC. This observation suggests that raw discrimination metrics may be more fundamental indicators of model quality, with calibration treated as an adjustable property. The comprehensive treatment of calibration theory is developed in Section 23.2, including its relationship to uncertainty quantification (Section 23.1) and methods for quantifying different sources of prediction uncertainty. Clinical deployment requires additional calibration considerations examined in Section 25.6.2.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-putting-together",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-putting-together",
    "title": "21  Evaluation Principles",
    "section": "21.11 Putting It All Together",
    "text": "21.11 Putting It All Together\nWhen designing or evaluating a genomic model assessment, working through a systematic checklist helps identify gaps and potential problems. The following questions organize this review, though the specific considerations will vary by application.\nFirst, consider the level of decision the model is intended to support. A model intended for molecular prediction faces different evaluation requirements than one designed for variant prioritization, patient risk stratification, or clinical action. Metrics should align with the actual decision context: enrichment metrics suit variant ranking, while net benefit matters for clinical decisions.\nSecond, examine whether data splits adequately prevent leakage. Are individuals, genomic regions, gene families, and ancestries appropriately separated? Has homology-aware clustering been applied with appropriate identity thresholds? Is there any plausible pathway for leakage or circularity through shared labels, features, or distributional overlap?\nThird, assess the baseline comparisons. Are comparisons made against the best available alternatives, not just historical or deliberately weak baselines? Do non-deep-learning baselines establish floors that justify architectural complexity? Does the improvement over baselines warrant the additional computational and interpretability costs?\nFourth, evaluate metric selection. Are multiple metrics reported to capture discrimination, calibration, and ranking quality? Are metrics computed with confidence intervals that convey uncertainty? Are subgroup-stratified metrics reported to assess whether performance varies across clinically relevant populations?\nFifth, examine whether ablation studies isolate component contributions. Have systematic ablations demonstrated which design choices drive performance? Is performance robust across hyperparameter ranges and random seeds, or does it depend on specific configurations?\nSixth, consider statistical rigor. Are significance tests applied with appropriate correction for multiple comparisons? Are effect sizes reported alongside p-values to distinguish statistical from practical significance? Are confidence intervals provided for key metrics?\nFor foundation models specifically, additional considerations apply. Is performance reported across zero-shot, probing, and fine-tuning regimes? Do data efficiency curves reveal where pretraining provides value? Has transfer been tested across diverse tasks to justify the “foundation” designation?\nFinally, assess robustness to deployment conditions. How does performance vary across cohorts, platforms, and ancestries? How does the model behave under distribution shift, missing data, or label noise? Would the evaluation translate to realistic deployment scenarios?\nThis checklist is not exhaustive but covers the most common evaluation pitfalls. Working through it systematically at the design stage can prevent problems that are difficult to fix retrospectively. Reviewers and readers can use the same checklist to critically assess published work.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch21-eval.html#sec-ch21-question-behind-metric",
    "href": "part_5/p5-ch21-eval.html#sec-ch21-question-behind-metric",
    "title": "21  Evaluation Principles",
    "section": "21.12 The Question Behind the Metric",
    "text": "21.12 The Question Behind the Metric\nThe question is never simply “what is the auROC?” but rather “what has been demonstrated, and how much should we trust it?” A reported metric summarizes one aspect of model behavior on one dataset under one evaluation protocol. Whether that metric predicts performance in deployment depends on details that standard reporting obscures: how data were split, whether leakage occurred, which subgroups were evaluated, what baselines were compared, and whether statistical conclusions account for multiple comparisons and estimation uncertainty.\nThe shortcuts that accelerate research in other machine learning domains produce misleading conclusions when applied to genomic data. Random train-test splits ignore homology that creates pseudo-replication. Single-metric comparisons miss failure modes in clinically relevant subgroups. Significance tests without effect sizes conflate statistical and practical importance. Benchmark evaluation without temporal awareness allows indirect leakage through shared community resources. Homology, population structure, batch effects, and label circularity create countless opportunities for self-deception, and genomic data exhibit all of these in abundance.\nRigorous evaluation requires sustained effort at every stage, from experimental design through statistical analysis. Confounding and leakage (Chapter 22) examines how population stratification, batch effects, and ascertainment bias produce results that evaporate under deployment, with specific attention to ancestry-stratified evaluation in Section 22.2.1 and batch effect detection in Section 22.2.2. Uncertainty quantification (Chapter 23) extends calibration assessment to epistemic versus aleatoric uncertainty (Section 23.1) and selective prediction (Section 23.7). Interpretability (Chapter 24) addresses whether models have learned genuine biology or exploited confounded patterns, with attribution methods in Section 24.1 providing specific diagnostic tools. For clinical applications specifically, risk prediction frameworks (Chapter 25) develop evaluation approaches tailored to decision-making, where net benefit and decision curves supplement discrimination metrics. Together, these perspectives provide the critical apparatus for engaging with genomic foundation model claims.\n\n\n\n\nGuo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. “On Calibration of Modern Neural Networks.” In Proceedings of the 34th International Conference on Machine Learning, 1321–30. PMLR. https://proceedings.mlr.press/v70/guo17a.html.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Evaluation Principles</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html",
    "href": "part_5/p5-ch22-confounding.html",
    "title": "22  Confounders and Leakage",
    "section": "",
    "text": "22.1 Confounding, Bias, and Leakage\nA variant effect predictor trained on ClinVar achieves 0.92 auROC on held-out variants from the same database, yet performance drops to 0.71 when evaluated on a prospectively collected clinical cohort. A polygenic risk score for coronary artery disease stratifies European-ancestry individuals with impressive discrimination, then fails almost completely when applied to individuals of African ancestry. A gene expression model trained on GTEx data predicts tissue-specific patterns with apparent precision, until deployment reveals it learned to distinguish sequencing centers rather than biological states. Each model worked brilliantly in evaluation and failed quietly in practice.\nThese failures share a common cause: the models learned shortcuts rather than biology. Genomic datasets encode hidden structure from ancestry and family relatedness to sequencing center, capture kit, and label curation protocol. These factors correlate with both features and labels. When such confounders remain uncontrolled, models exploit them. The central challenge is that confounded models can appear to work, sometimes spectacularly well, until they encounter data where the shortcuts no longer apply.\nThis problem is not unique to deep learning. Linear regression and logistic models suffer from the same biases when fit on confounded data. What makes confounding particularly dangerous in the foundation model era is scale: larger datasets and more expressive architectures make it easier to discover subtle shortcuts that remain invisible in standard diagnostics but cause dramatic failures when distributions shift at deployment. A shallow model might miss the correlation between sequencing center and disease status; a transformer with hundreds of millions of parameters will find it if that correlation helps optimize the training objective.\nThe terminology of confounding, bias, and leakage describes distinct phenomena that often co-occur and reinforce each other. Precision in language helps clarify what has gone wrong when a model fails.\nA confounder is a variable that influences both the input features and the label. Ancestry provides a canonical example: it affects allele frequencies across the genome (the features) and disease risk through environmental, socioeconomic, and healthcare pathways (the labels). If ancestry is not explicitly modeled or controlled, a model trained to predict disease may learn to identify ancestry rather than disease biology. The prediction appears accurate because ancestry correlates with outcome, but the model has captured correlation rather than mechanism.\nBias refers to systematic deviation from the quantity we intend to estimate or predict. Bias can result from confounding, but also arises from measurement error, label definitions, sampling procedures, or deployment differences. A case-control study with 50% disease prevalence will train models that systematically over-predict risk when deployed in populations where true prevalence is 5%. The model may be perfectly calibrated for the training distribution yet dangerously miscalibrated for clinical use.\nData leakage occurs when information about the test set inadvertently influences model training or selection. Leakage pathways include overlapping individuals or variants between training and evaluation, shared family members across splits, duplicated samples under different identifiers, and indirect channels such as pretraining on resources that later serve as benchmarks. The circularity between computational predictors and ClinVar annotations discussed in Section 4.5 exemplifies this last category: CADD-like scores influence which variants receive pathogenic annotations, and those annotations then become training labels for the next generation of predictors.\nDistribution shift describes mismatch between training and deployment data distributions. Shift can be driven by changes in ancestry composition, sequencing technology, clinical coding practices, or temporal trends in care. A model that learns hospital-specific coding patterns will fail when deployed at a different institution, not because the biology differs but because the label generation process does.\nFor foundation models, these risks are magnified. Genomes encode ancestry, relatedness, and assay conditions in thousands of subtle features, even when those labels are never explicitly provided. Large transformers find shortcuts that smaller models would miss if those shortcuts improve the training objective. Complex training regimes involving pretraining on biobank-scale data, fine-tuning on curated labels, and evaluation on community benchmarks create many opportunities for direct and indirect leakage.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-sources",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-sources",
    "title": "22  Confounders and Leakage",
    "section": "22.2 Sources of Confounding in Genomic Data",
    "text": "22.2 Sources of Confounding in Genomic Data\nConfounders in genomic modeling cluster into several categories, though the same underlying variable (such as recruitment site) may simultaneously induce ancestry differences, batch effects, and label bias. These categories are not mutually exclusive; batch effects in single-cell data (Section 16.6.1) and multi-omic integration (Section 19.7.1) represent domain-specific manifestations of the same underlying challenge.\n\n22.2.1 Population Structure and Relatedness\nAncestry creates perhaps the most pervasive confounders. Continental and sub-continental population structure affects both genomic features and many phenotypes of interest, creating classic confounding. The portability failures of polygenic scores across ancestry groups (Section 3.7) represent one clinically consequential manifestation of this confounding. Family relationships (siblings, parent-offspring pairs, cryptic relatedness detectable only through genotype similarity) and founder effects that create local haplotype structure compound these issues. Relatedness creates a more subtle problem than population stratification: when close relatives appear in both training and test sets, models can memorize shared haplotype segments rather than learning generalizable patterns, producing inflated performance estimates that collapse for unrelated individuals.\n\n\n22.2.2 Technical Batch Effects\nSequencing and analysis pipelines introduce their own systematic differences. Different instruments produce distinct error profiles. Library preparation protocols vary in GC bias, coverage uniformity, and adapter content. Capture kits determine which genomic regions receive adequate coverage. Alignment algorithms and variant callers make different decisions at ambiguous positions. When samples from a particular batch disproportionately represent a specific label class (cases sequenced at one center, controls at another), models learn to distinguish batches rather than biology.\n\n\n22.2.3 Institutional and Recruitment Confounding\nThe institutions where patients receive care introduce additional confounding layers. Hospital systems use distinct coding practices, diagnostic thresholds, and follow-up schedules. The phenotype quality issues that result are examined in Section 2.7, with implications for how models learn from systematically biased labels. Population-based biobanks differ from referral-center cohorts in disease severity, comorbidity patterns, and demographic composition. Individuals who receive genomic testing may be more severely affected, more affluent, or preferentially drawn from particular ancestry groups, introducing selection bias that distorts apparent variant-phenotype relationships.\nThese sources of confounding trace back to data collection and curation processes. Training data inherit the biases present in the databases from which they derive: ClinVar’s overrepresentation of European ancestry variants (Section 2.8.1), gnomAD’s population composition (Section 2.2.3), and the tissue coverage decisions of consortia like ENCODE and GTEx (Section 2.4.1). Understanding data provenance is prerequisite to anticipating which confounders a model may have learned.\n\n\n22.2.4 Label Generation Bias\nThe process of generating ground truth annotations itself creates biases. Clinical labels derived from billing codes or problem lists reflect documentation practices as much as underlying disease. Variant pathogenicity databases exhibit the systematic biases detailed in Section 2.8: ClinVar annotations over-represent European ancestry, well-studied genes, and variants submitted by high-volume clinical laboratories (Landrum et al. 2018). Expression, regulatory, or splicing labels derived from specific tissues or cell lines may not generalize to other biological contexts. The circularity problem identified in Section 4.5 persists into the foundation model era: when model predictions influence which variants receive expert review, and expert classifications become training labels, feedback loops amplify historical biases.\n\n\n22.2.5 Temporal Drift\nClinical practice, diagnostic criteria, and coding conventions evolve over time. Sequencing technologies and quality control pipelines also change. A model trained on 2015 data may fail on 2024 data not because biology changed but because documentation practices, coding standards, and available treatments all evolved. This temporal drift affects both the features models learn and the labels they predict.\n\n\n22.2.6 Resource Overlap and Indirect Leakage\nEven the resources used for training and evaluation create leakage pathways. When databases like gnomAD or UK Biobank appear in both model training and evaluation, indirect information flows compromise apparent generalization. A foundation model pretrained on gnomAD allele frequencies, then evaluated on a benchmark that uses gnomAD for population filtering, faces indirect leakage even if specific variants do not overlap. Community benchmarks that reuse widely available variant sets across multiple publications create additional leakage pathways that accumulate over time as the field iterates.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-population-shortcut",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-population-shortcut",
    "title": "22  Confounders and Leakage",
    "section": "22.3 Population Structure as a Shortcut",
    "text": "22.3 Population Structure as a Shortcut\nPopulation structure represents one of the most pervasive confounders in genomic modeling. The core issue is that ancestry simultaneously affects genomic features and many phenotypes through pathways that have nothing to do with direct genetic causation.\nHuman genetic variation is structured by ancestry: allele frequencies, haplotype blocks, and linkage disequilibrium patterns differ across populations in ways that reflect demographic history. Principal components computed from genome-wide genotypes provide a low-dimensional summary of this structure and have become standard in genome-wide association studies (GWAS) to correct for stratification (Patterson, Price, and Reich 2006; Price et al. 2006). Yet ancestry is not merely a statistical nuisance. It is intertwined with geography, environment, socioeconomic status, and access to healthcare, factors that directly impact disease risk, likelihood of receiving genetic testing, and the quality of phenotyping when testing occurs.\nThe statistical genetics community developed these corrections precisely because early genome-wide association studies produced spurious signals driven by ancestry differences between cases and controls rather than causal variant effects (see Section 3.1.4 for detailed treatment of population stratification in association testing). Foundation models face the same fundamental problem in a different guise: ancestry structure that confounded linear regression in GWAS now confounds neural network predictions, and the solutions require similar conceptual foundations even when the technical implementations differ.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 22.2: [Essential] Multi-panel figure. Panel A (PCA of genomic data): PC1 vs PC2; colored by ancestry; clear clustering. Panel B (Ancestry in k-mer frequencies): Heatmap across populations; even local composition differs. Panel C (The shortcut pathway): Flow diagram (Ancestry → Sequencing → Labels; Ancestry → Allele frequencies → Features; model learns via ancestry). Panel D (Cross-population performance): Bar chart showing 40-75% PGS reduction; “Shortcuts fail when relationship changes.”\n\n\n\nConsider a rare disease clinic serving primarily individuals of European ancestry. This clinic contributes most pathogenic variant submissions to ClinVar, while variants observed predominantly in other ancestries remain classified as variants of uncertain significance (Landrum et al. 2018).. A model trained on ClinVar may learn that European-enriched variants tend to have pathogenic labels and non-European-enriched variants tend to have uncertain or benign labels, not because of any biological difference in pathogenicity but because of differential clinical characterization. The model appears to predict pathogenicity while actually predicting ancestry-correlated ascertainment.\nFoundation models trained on nucleotide sequences see ancestry information directly: the distribution of k-mers and haplotypes differs by population. When such models are fine-tuned to predict disease risk or variant effects, they may leverage ancestry as a shortcut. Increasing model capacity does not solve this problem; it often makes it worse by enabling detection of increasingly subtle ancestry-linked features. The polygenic score portability literature provides stark evidence: risk scores derived from European ancestry cohorts show 40-75% reductions in prediction accuracy when applied to African ancestry individuals (Duncan et al. 2019). Similar patterns emerge for variant effect predictors and regulatory models, though they are often less thoroughly documented due to limited cross-ancestry evaluation.\nThis mismatch between the populations used for model development and the populations that would benefit from genomic medicine creates a fundamental tension between current practice and equitable healthcare. Models that work primarily for European ancestry individuals perpetuate existing health disparities, regardless of their benchmark performance. The fairness implications are examined further in Section 22.10.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-technical-artifacts",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-technical-artifacts",
    "title": "22  Confounders and Leakage",
    "section": "22.4 Technical Artifacts as Biological Signal",
    "text": "22.4 Technical Artifacts as Biological Signal\nTechnical pipelines are complex, and each step from sample collection through final variant calls can introduce systematic differences that models may learn.\nSequencing centers differ in instruments, reagents, and quality control thresholds. Library preparation protocols produce distinct coverage profiles and GC bias patterns. Capture kits determine which genomic regions are well-covered and which have systematic dropout. Read length affects the ability to span repetitive regions and call structural variants. Alignment and variant calling algorithms make different decisions at ambiguous genomic positions.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 22.3: [High] Three-panel figure. Panel A (Batch structure in embeddings): UMAP colored by sequencing center; samples cluster by batch not phenotype. Panel B (Coverage patterns by batch): Genome browser tracks; different centers show systematic differences. Panel C (Batch predicts phenotype): Contingency table (batch × case/control); imbalanced distribution. Warning: “Model predicting disease may actually predict sequencing center.”\n\n\n\nWhen samples from a particular batch or platform are disproportionately drawn from a specific phenotype class, models learn to distinguish batches. In high-dimensional feature spaces, even subtle batch-specific artifacts (coverage dips at particular loci, variant density patterns reflecting caller behavior, residual adapter sequences) can become predictive. Foundation models that process raw reads, coverage tracks, or variant streams are particularly vulnerable because batch signatures may be encoded in features that preprocessing would typically remove.\nCommon patterns suggesting batch confounding include embedding spaces where samples cluster by sequencing center rather than phenotype, strong predictive performance that collapses when evaluated on data from a new platform, and models that can accurately predict batch identity (sequencing center, capture kit, processing date) from inputs that should be batch-independent. When a model designed to predict disease can also predict which laboratory processed the sample, something has gone wrong.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-label-circularity",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-label-circularity",
    "title": "22  Confounders and Leakage",
    "section": "22.5 Label Bias and Circularity",
    "text": "22.5 Label Bias and Circularity\nLabels in genomic applications rarely represent ground truth in any absolute sense. They represent the outputs of complex processes involving clinical documentation, expert review, computational prediction, and database curation. These processes introduce biases that models absorb and may amplify.\nClinical phenotypes derived from electronic health records inherit the limitations of medical documentation. Billing codes capture what was reimbursable, not necessarily what was present. Problem lists reflect what clinicians chose to document, which varies by specialty, institution, and individual practice patterns. Diagnostic criteria change over time, creating apparent temporal trends in disease prevalence that reflect evolving definitions rather than changing biology.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 22.4: [High] Circular flow diagram. Steps: (1) Clinical lab submits to ClinVar using CADD/REVEL as evidence; (2) ClinVar aggregates (computational evidence influences labels); (3) New model trains on ClinVar (learns to replicate patterns); (4) New model used by labs (influences next submissions); (5) Return to step 1. Annotations: circularity, apparent validation reflects agreement not insight. Side panel: Breaking cycle (prospective, temporal, independent functional).\n\n\n\nVariant pathogenicity labels illustrate the problem of circularity. ClinVar aggregates submissions from clinical laboratories, research groups, and expert panels (Landrum et al. 2018). The evidence underlying these submissions often includes computational predictions: a laboratory may cite CADD, REVEL, or other predictors as supporting evidence for a pathogenic classification. When the next generation of predictors trains on ClinVar, it learns to replicate the computational predictions that contributed to those labels. Performance on ClinVar-derived benchmarks thus reflects, in part, agreement with previous predictors rather than independent biological insight.\nThis circularity extends across the ecosystem of genomic resources. gnomAD allele frequencies inform variant filtering in clinical pipelines. UK Biobank genotype-phenotype associations shape which variants receive functional follow-up. Structural annotations from ENCODE and Roadmap Epigenomics influence which regulatory regions are considered biologically important. Foundation models pretrained on these resources, then evaluated against benchmarks derived from the same resources, may achieve impressive scores while learning to reproduce the assumptions and biases of existing annotations rather than discovering new biology.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-data-splitting",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-data-splitting",
    "title": "22  Confounders and Leakage",
    "section": "22.6 Data Splitting",
    "text": "22.6 Data Splitting\nData splitting is among the primary tools for assessing generalization, yet naive splits can silently permit leakage that inflates apparent performance.\n\n22.6.1 Random Individual-Level Splits\nRandom individual-level splits assign samples randomly to training, validation, and test sets. This approach fails when samples are not independent: family members may appear on both sides of a split, allowing models to memorize shared haplotypes. Rare variant analysis is particularly vulnerable because disease-causing variants may be private to specific families, and memorizing which families have which variants is far easier than learning generalizable sequence-function relationships.\n\n\n22.6.2 Family-Aware Splits\nFamily-aware splits address relatedness by ensuring that all members of a family appear in the same split. This prevents direct memorization of family-specific variants but does not address population structure (ancestry groups may remain imbalanced across splits) or other confounders.\n\n\n22.6.3 Locus-Level Splits\nLocus-level splits hold out entire genomic positions, ensuring that no variant at a test position appears during training. This stringent approach prevents models from memorizing site-specific patterns and is essential for variant effect prediction where the goal is to score novel variants at positions the model has never seen. Many published benchmarks fail to implement locus-level splitting, allowing models to achieve high scores by recognizing familiar positions rather than learning generalizable effects. The evaluation considerations in Section 21.4 address these issues in detail.\n\n\n22.6.4 Region and Chromosome Splits\nRegion or chromosome splits hold out entire genomic regions, testing whether models learn biology that transfers across the genome rather than region-specific patterns. This is particularly relevant for regulatory prediction, where local chromatin context may differ between regions.\n\n\n22.6.5 Cohort and Site Splits\nCohort or site splits hold out entire institutions, sequencing centers, or biobanks, directly testing robustness to the batch and cohort effects discussed above. Models that perform well only within their training cohort but fail on held-out cohorts have learned institution-specific patterns.\n\n\n22.6.6 Temporal Splits\nTime-based splits use temporal ordering, training on earlier data and evaluating on later data. This approach simulates prospective deployment and tests robustness to temporal drift. A model trained on 2018 data and evaluated on 2023 data faces realistic distribution shift that random splits would obscure.\n\n\n22.6.7 Indirect Leakage Across Resources\nBeyond explicit split design, indirect leakage remains a concern. A variant that appears in ClinVar may also appear in gnomAD (with population frequency information), in functional assay datasets (with splicing or expression effects), and in literature-derived databases (with disease associations). Pretraining on any of these resources while evaluating on another creates indirect information flow that standard deduplication would miss.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-leakage-confounding",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-leakage-confounding",
    "title": "22  Confounders and Leakage",
    "section": "22.7 Data Leakage as Confounding",
    "text": "22.7 Data Leakage as Confounding\nData leakage can be understood as a special case of confounding where the confounder is information that should not exist at prediction time. This framing clarifies why leakage inflates performance estimates and why leaked models fail in deployment: they have learned associations with variables that are unavailable when predictions must actually be made.\nThe detailed taxonomy of leakage types (label, feature, temporal, and benchmark leakage) along with detection strategies is provided in Section 21.4. Here we examine how each leakage type creates confounding structures that distort model evaluation.\n\n22.7.1 Causal Structure of Leakage\nIn causal terms, leakage introduces a backdoor path between features and labels that does not represent the relationship we intend to model. Consider a pathogenicity predictor trained with conservation scores that were computed using alignments incorporating known pathogenic variants. The causal structure includes: (1) the intended path from sequence features through biological mechanism to pathogenicity, and (2) a leaked path from pathogenicity labels through their influence on conservation databases back to conservation features. The model cannot distinguish signal flowing through these two paths, and performance estimates reflect both.\nLabel leakage creates confounding when the process that generated labels also influenced feature construction. The confounder is the shared information source: ClinVar curators who used computational predictions created a dependency between those predictions and subsequent labels. Feature leakage creates confounding when features correlate with labels through non-causal pathways, such as batch effects that happen to align with case-control status. Temporal leakage creates confounding through time-dependent information flow: future knowledge that influenced either features or labels introduces associations that would not exist in prospective application.\n\n\n22.7.2 Compounding Effects\nThese leakage types interact and compound. A model suffering from multiple forms may achieve extraordinary benchmark performance while learning nothing transferable to prospective clinical use. The apparent signal is real within the leaked evaluation framework but spurious for the intended application.\nConsider a variant effect predictor that uses conservation scores (feature leakage), was trained on ClinVar labels influenced by earlier predictors (label leakage), and is evaluated on a benchmark constructed using similar computational methods (benchmark leakage). Each leakage type independently inflates performance; together, they create an evaluation that measures something entirely different from prospective predictive ability.\n\n\n22.7.3 Implications for Confounding Analysis\nThe confounding framework suggests that leakage detection methods (described in Section 21.4) can be understood as strategies for identifying and blocking backdoor paths. Feature ablation removes variables that may carry leaked signal. Temporal validation eliminates paths that depend on future information. Baseline analysis reveals when simple confounders explain most of the apparent performance.\nThis perspective also clarifies why some apparent leakage may be acceptable. If conservation scores will always be available at prediction time, the path through conservation represents legitimate signal rather than confounding. The distinction depends on the deployment context: what information will actually be available when the model must make predictions? Leakage is confounding by information that exists in evaluation but not in application.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-detection",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-detection",
    "title": "22  Confounders and Leakage",
    "section": "22.8 Detecting Confounding",
    "text": "22.8 Detecting Confounding\nConfounding is often subtle, requiring systematic diagnostics rather than reliance on aggregate performance metrics.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 22.5: [High] Diagnostic checklist with visualizations. Diagnostic 1 (Confounder-only baseline): Bar chart comparing full model vs ancestry PCs only vs batch only; if simple baseline approaches full → confounding. Diagnostic 2 (Subgroup stratification): Multiple reliability diagrams by ancestry. Diagnostic 3 (Prediction-confounder association): Scatter of predictions vs PC1; residual after controlling for label. Diagnostic 4 (Split sensitivity): Table showing performance across split strategies; large drop = memorization. Diagnostic 5 (Negative controls): Accuracy on outcomes unrelated to genetics; should be chance.\n\n\n\n\n22.8.1 Confounder-Only Baselines\nThe most direct diagnostic trains simple models using only potential confounders: ancestry principal components, batch indicators, sequencing center identifiers, recruitment site. If these confounder-only baselines approach the performance of complex genomic models, confounding likely drives a substantial portion of the signal. Reporting confounder-only baselines alongside genomic model results makes hidden shortcuts visible.\n\n\n22.8.2 Stratified Performance Analysis\nPerformance stratified by ancestry group, sequencing platform, institution, and time period reveals whether aggregate metrics mask heterogeneity. Both discrimination (auROC, area under the precision-recall curve (auPRC)) and calibration diagnostics should be computed for each subgroup. Models may achieve high overall auROC while being poorly calibrated or nearly useless for specific subpopulations. Performance that varies dramatically across subgroups suggests confounding or distribution shift even when overall metrics appear strong.\n\n\n22.8.3 Residual Confounder Associations\nAssociations between model outputs and potential confounders can reveal encoding of ancestry or batch information beyond what the label requires. Plotting predictions against ancestry principal components, adjusting for true label status, shows residual confounding. Comparing mean predicted risk across batches or time periods within the same true label class identifies systematic biases. Formal association tests (regression, mutual information) between predictions and confounders that show strong residual associations indicate the model has learned confounder-related features that go beyond predicting the label itself.\n\n\n22.8.4 Split Sensitivity Analysis\nVarying the splitting strategy probes for leakage. Re-evaluating performance under locus-level splits, cohort holdouts, or temporal splits reveals whether initial results depended on memorization. A model that achieves 0.90 auROC with random splits but only 0.75 auROC with locus-level splits has likely memorized site-specific patterns. Large drops in performance under stricter splitting indicate inflated initial results.\n\n\n22.8.5 Negative Control Outcomes\nUsing outcomes known to be unrelated to genomics as negative controls provides powerful confirmation of confounding. If a model trained to predict disease from genotypes can also predict administrative outcomes (insurance type, documentation completeness) with similar accuracy, it has learned confounders. Shuffling labels within batch or ancestry strata should eliminate predictive signal; if it does not, the model exploits structure that transcends any specific outcome.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-mitigation",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-mitigation",
    "title": "22  Confounders and Leakage",
    "section": "22.9 Mitigation Strategies",
    "text": "22.9 Mitigation Strategies\nNo mitigation strategy eliminates confounding entirely, and each involves trade-offs between bias, variance, and coverage. The approaches described here are complementary: design-based methods constrain confounding before modeling begins, statistical adjustments handle residual confounding, invariance learning provides protection when confounders are incompletely measured, and rigorous benchmark construction ensures that evaluation reflects generalization rather than shortcut learning.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 22.6: [Enhancing] Strategy comparison table. Strategies: Study design (match cases/controls, before collection, reduces sample), Covariate adjustment (include ancestry/batch as inputs, during training, may remove real signal), Domain adaptation (train invariant to confounders, complex), Robust optimization (minimize worst-group error, requires group labels), Benchmark design (locus-level splits, during evaluation, harder scores by design). Annotation: “Approaches complementary; use multiple.”\n\n\n\n\n22.9.1 Study Design and Cohort Construction\nDesign-based approaches provide the most robust protection against confounding because they prevent the problem rather than attempting to correct it statistically. When cases and controls are matched on potential confounders before data collection, those variables cannot drive spurious associations regardless of model complexity.\nMatching strategies balance cases and controls on age, sex, ancestry, recruitment site, and sequencing platform. For ancestry, matching can use self-reported categories, genetic principal components, or fine-scale population assignments depending on the granularity required. Exact matching (requiring identical values) provides the strongest protection but may be infeasible when confounders are continuous or when the pool of potential controls is limited. Propensity score matching or coarsened exact matching offer practical alternatives that achieve approximate balance across multiple confounders simultaneously.\nBalanced sampling during training prevents models from optimizing primarily for majority patterns. When one ancestry group comprises 80% of training data, gradient updates predominantly reflect that group’s patterns, and minority group performance suffers. Down-sampling the majority group or up-sampling minority groups within mini-batches ensures that all groups contribute meaningfully to parameter updates. The trade-off is reduced effective sample size: discarding majority group samples wastes information, while up-sampling minority groups risks overfitting to limited examples.\nProspective collection with diversity targets ensures that training data represent the populations where models will be deployed. Retrospective matching can balance existing cohorts but cannot address variants or patterns that are absent from the original collection. The All of Us Research Program, Million Veteran Program, and similar initiatives that prioritize ancestral diversity from inception provide data that enable genuinely generalizable models, though the genomic AI field has yet to fully leverage these resources.\nThe limitation of design-based approaches is that they must anticipate which variables will confound. Unknown or unmeasured confounders cannot be matched, and over-matching (matching on variables that are consequences of the exposure) can introduce bias rather than remove it. Design and analysis approaches work best in combination: match on known confounders, then adjust for residual imbalances that matching did not eliminate.\n\n\n22.9.2 Covariate Adjustment\nCovariate adjustment explicitly models confounders rather than ignoring them, allowing estimation of outcome effects that account for confounding variables. The approach is familiar from genome-wide association studies, where including ancestry principal components as covariates in regression models reduces spurious associations driven by population structure.\nFor foundation models, covariate adjustment takes several forms. The simplest approach includes confounder variables (ancestry PCs, batch indicators, sequencing platform) as additional input features alongside genomic data. The model learns to use confounder information when predicting outcomes, and the genomic feature coefficients or attention weights reflect associations that remain after accounting for confounders. This approach assumes the model can learn the appropriate adjustment; for complex confounding patterns, explicit modeling may be preferable to implicit learning.\nResidualization removes confounder-associated variance before training genomic models. Regressing features or phenotypes on confounders and retaining only the residuals ensures that subsequent models cannot exploit confounder-outcome associations. The risk is removing genuine biological signal when confounders correlate with causal variants. Ancestry principal components, for instance, capture population structure that includes both confounding (differential ascertainment) and biology (population-specific genetic architecture). Aggressive residualization may discard the latter along with the former.\nMixed models and hierarchical structures treat institution, batch, or ancestry group as random effects, estimating genomic associations while accounting for clustering within groups. This approach is standard in genetic epidemiology and translates naturally to deep learning through hierarchical Bayesian frameworks or explicit modeling of group-level parameters. The key advantage is borrowing strength across groups while allowing group-specific intercepts or slopes, though computational costs increase substantially for large datasets with many groups.\nThe fundamental limitation of covariate adjustment is that it requires measuring and correctly specifying confounders. Unmeasured confounders remain uncontrolled. Conditioning on colliders (variables caused by both exposure and outcome) introduces bias rather than removing it. Careful causal reasoning, often formalized through directed acyclic graphs, is essential for determining which variables should be adjusted and which should not.\n\n\n22.9.3 Domain Adaptation and Invariance Learning\nDomain adaptation methods aim to learn representations that do not encode confounders, achieving predictions that generalize across batches, institutions, or populations without explicitly modeling each source of variation. These approaches are particularly valuable when confounders are numerous, incompletely measured, or difficult to specify.\nAdversarial training adds a discriminator network that attempts to predict batch identity, ancestry, or other confounders from learned representations. The feature extractor is trained with two competing objectives: maximize prediction accuracy for the primary task while minimizing the discriminator’s ability to recover confounder labels. When successful, the learned representations retain information useful for prediction while discarding information that distinguishes confounded groups. Domain adversarial neural networks and gradient reversal layers implement this approach efficiently within standard deep learning frameworks.\nThe theoretical limitation is that perfect invariance and maximum accuracy cannot be achieved simultaneously when confounders correlate with the outcome through both causal and non-causal pathways. Enforcing strict invariance to ancestry, for instance, may remove genuine population-specific genetic effects along with confounding. Practitioners must balance the degree of invariance against task performance, typically through hyperparameters controlling the adversarial loss weight.\nGroup distributionally robust optimization (group DRO) targets worst-group performance rather than average performance, encouraging models that work for all subgroups rather than optimizing for the majority. The training objective minimizes the maximum loss across predefined groups (ancestry categories, sequencing platforms, institutions), ensuring that no group is systematically disadvantaged. This approach requires group labels during training and may sacrifice some average performance to improve worst-case outcomes.\nImportance weighting and distribution matching align feature distributions across domains without explicit adversarial training. Samples from underrepresented domains receive higher weights during training, or feature distributions are explicitly matched through maximum mean discrepancy or optimal transport objectives. These methods can be combined with other approaches and are particularly useful when the target deployment distribution is known but differs from training data.\n\n\n22.9.4 Data Curation and Benchmark Design\nThe signals available for learning depend entirely on how data are curated and how benchmarks are constructed. Careful attention to data provenance and evaluation design prevents many confounding problems that would otherwise require complex modeling solutions.\nDeduplication across training and evaluation sets prevents direct memorization. For genomic data, deduplication must operate at multiple levels: individual samples (the same person appearing under different identifiers), family groups (relatives sharing haplotype segments), and genomic loci (the same variant position appearing in both training and test sets). Variant effect prediction requires particularly stringent locus-level deduplication; a model that has seen any variant at position chr1:12345 during training cannot be fairly evaluated on novel variants at that position.\nSplitting strategies determine what generalization is actually tested. Random splits assess interpolation within the training distribution. Locus-level splits test generalization to novel genomic positions. Chromosome holdouts test transfer across genomic regions. Cohort splits test robustness to institutional and demographic differences. Temporal splits simulate prospective deployment. Each strategy answers a different question, and benchmark performance under one splitting regime does not guarantee performance under others. Reporting results across multiple splitting strategies reveals which aspects of generalization a model has achieved. The comprehensive treatment of benchmark design in Chapter 20 addresses these considerations in detail.\nBenchmark diversity ensures that evaluation reflects the full range of deployment contexts. Benchmarks constructed from a single ancestry group, institution, or sequencing platform test only narrow generalization. Explicitly including diverse ancestries, multiple institutions, and varied technical platforms in evaluation sets reveals performance heterogeneity that homogeneous benchmarks would hide. The ProteinGym and CASP benchmarks in protein modeling demonstrate how thoughtfully constructed evaluation resources can drive genuine progress; genomic variant interpretation would benefit from similar community efforts.\nDocumentation of overlaps between training resources and benchmarks enables readers to assess potential leakage. When a foundation model is pretrained on gnomAD, fine-tuned on ClinVar, and evaluated on a benchmark that filters variants using gnomAD frequencies, the information flow is complex and potentially circular. Explicit documentation of which resources contributed to which stages of model development allows appropriate skepticism about performance claims. Benchmark papers should catalog known overlaps with major training resources; model papers should acknowledge which benchmarks may be compromised by their pretraining choices.\n\n\n22.9.5 Causal Inference Approaches\nWhen observational confounding cannot be eliminated through design or statistical adjustment, causal inference frameworks offer principled alternatives that leverage the structure of genetic inheritance itself.\nThe random assortment of alleles at meiosis creates natural experiments that Mendelian randomization exploits (Davey Smith and Ebrahim 2003). Because genotypes are assigned before birth and cannot be influenced by most environmental confounders, genetic variants that affect an exposure (such as a biomarker level or gene expression) can serve as instrumental variables for estimating causal effects on downstream outcomes. A foundation model trained to predict expression levels can be evaluated for causal relevance by testing whether its predictions, instrumented through genetic variants, associate with disease outcomes in ways that survive Mendelian randomization assumptions. This approach has revealed that many observational biomarker-disease associations reflect confounding rather than causation, and similar logic applies to model-derived predictions.\nDirected acyclic graphs (DAGs) formalize assumptions about causal structure and clarify which variables should be adjusted, which should be left unadjusted, and which adjustments would introduce bias rather than remove it (Pearl 2009). Conditioning on a collider (a variable caused by both exposure and outcome) induces spurious associations; conditioning on a mediator blocks causal pathways of interest. Explicit DAG construction forces researchers to articulate their causal assumptions, making hidden confounding visible and enabling principled variable selection. For genomic models, DAGs clarify the relationships among ancestry, technical factors, biological mechanisms, and phenotypic outcomes, revealing which adjustment strategies address confounding versus which inadvertently condition on consequences of the outcome.\nOutcomes and exposures known to be unrelated to the prediction target provide empirical tests of residual confounding without requiring complete causal knowledge (Lipsitch, Tchetgen Tchetgen, and Cohen 2010). A negative control outcome is one that should not be causally affected by the exposure of interest; if the model predicts it anyway, confounding is present. A negative control exposure is one that should not causally affect the outcome; association with the outcome again indicates confounding. For a variant effect predictor, administrative outcomes (insurance status, documentation completeness) serve as negative control outcomes that genotypes should not predict. Synonymous variants in non-conserved regions can serve as negative control exposures that should not affect protein function. Strong predictions for negative controls reveal that the model has learned confounders rather than biology.\nThese causal approaches do not replace careful study design and rigorous splitting, but they provide additional tools for distinguishing genuine biological signal from confounded associations, particularly when the same observational data must serve both training and evaluation purposes.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-fairness",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-fairness",
    "title": "22  Confounders and Leakage",
    "section": "22.10 Fairness and External Validity",
    "text": "22.10 Fairness and External Validity\nConfounding connects directly to fairness and health equity. Models that achieve high average performance while failing for specific populations may appear successful while exacerbating existing disparities.\nPolygenic risk scores illustrate this tension. European ancestry-derived scores predict cardiovascular disease, diabetes, and breast cancer risk reasonably well within European ancestry populations. Applied to African or Asian ancestry individuals, the same scores show substantially worse discrimination and calibration (Duncan et al. 2019). Healthcare systems that deploy these scores without ancestry-specific validation risk providing inferior risk stratification to already underserved populations. The portability analysis framework in Section 3.7 quantifies these degradations, while clinical deployment frameworks (Section 25.8) address operational responses.\nVariant interpretation exhibits similar patterns. ClinVar contains many more pathogenic variant classifications for European ancestry individuals than for other populations (Landrum et al. 2018). The data composition issues underlying this imbalance are examined in Section 2.8.1. Predictors trained on ClinVar inherit this imbalance, performing better for variants common in European populations and worse for variants enriched in other ancestries. Clinical deployment of such predictors may reduce diagnostic yield for non-European patients.\nThe uncertainty quantification approaches discussed in Chapter 23 provide partial mitigation: models that report high uncertainty for under-represented populations at least flag predictions that should not be trusted. Out-of-distribution detection methods (Section 23.6) specifically address when inputs fall outside the training distribution. The interpretability methods in Chapter 24 can reveal when models rely on ancestry-correlated features, with attribution analysis (Section 24.1) identifying which input features drive ancestry-dependent predictions. Yet technical solutions alone are insufficient. Addressing fairness requires intentional data collection that prioritizes under-represented populations, evaluation protocols that mandate subgroup analysis, and deployment decisions that consider equity alongside aggregate accuracy.\nExternal validity asks whether a model’s performance in one setting predicts its performance in another. Confounding and distribution shift often cause dramatic external validity failures. A model that achieves excellent metrics in the development cohort may fail when deployed at a different institution, in a different healthcare system, or in a different country. The clinical risk prediction frameworks in Section 25.9 emphasize multi-site validation precisely because single-site performance frequently fails to generalize.\nThe fairness implications of confounding extend beyond technical model performance into questions of justice in healthcare resource allocation, diagnostic equity, and the distribution of benefits from genomic medicine. Governance frameworks for addressing these structural challenges are examined in Chapter 29.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-checklist",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-checklist",
    "title": "22  Confounders and Leakage",
    "section": "22.11 A Practical Checklist",
    "text": "22.11 A Practical Checklist\nThe following checklist synthesizes the diagnostics and mitigations discussed above. Systematic application during model development and evaluation surfaces confounding that would otherwise remain hidden.\nPopulation structure and relatedness: Quantify ancestry via principal components and relatedness via kinship coefficients. Decide explicitly whether to match, stratify, or adjust for these factors, and document the justification. Report performance stratified by ancestry group. When family structure exists in the data, verify that relatives do not appear across train-test boundaries.\nData splits and leakage: Ensure individuals, families, and genomic loci do not cross the train-validation-test boundaries for target tasks. Implement stricter splits (locus-level, chromosome-level, cohort-based, time-based) and report the performance differences. Check for overlap with external databases or benchmarks used in evaluation and document any shared resources.\nBatch, platform, and cohort effects: Catalog technical variables (sequencing center, instrument, protocol, assay) and cohort identifiers. Test whether these variables predict labels or align with subgroups of interest. Use embedding visualizations, principal components, or simple classifiers to detect batch signatures. Apply mitigation (design matching, covariate adjustment, domain adaptation) when batch effects are detected.\nLabel quality and curation bias: Document how labels were defined and what processes (billing codes, expert review, computational prediction, registry inclusion) produced them. Quantify label noise where possible. Consider robust training strategies when labels are noisy. Assess how curated resources like ClinVar reflect historical biases and whether those biases affect evaluation validity.\nCross-group performance and fairness: Report metrics for each major subgroup (ancestry, sex, age, cohort, platform) rather than only aggregate performance. Examine calibration across groups, not just discrimination. Discuss clinical implications of residual performance gaps and whether deployment might worsen existing disparities.\nReproducibility and transparency: Document dataset construction, inclusion criteria, and splitting strategies completely. Release preprocessing, training, and evaluation code when feasible. Describe which confounders were measured, how they were handled, and what limitations remain.\nModels that pass this checklist provide more reliable evidence of genuine biological learning. Models that fail at multiple points may achieve benchmark success while learning shortcuts that will not transfer to new settings.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch22-confounding.html#sec-ch22-rigor",
    "href": "part_5/p5-ch22-confounding.html#sec-ch22-rigor",
    "title": "22  Confounders and Leakage",
    "section": "22.12 Rigor as Response",
    "text": "22.12 Rigor as Response\nThese confounding and bias problems are not reasons for despair. They are reasons for rigor. The same expressive capacity that enables foundation models to discover subtle shortcuts also enables them to learn complex biological patterns when training data and evaluation protocols are designed appropriately. The goal is not to abandon powerful models but to create conditions under which their power serves biological discovery rather than benchmark gaming.\nSeveral trends support progress. Multi-ancestry biobanks and international collaborations expand the diversity of available training data. Benchmark developers implement stricter splitting protocols and require subgroup analyses. Pretraining strategies that explicitly promote invariance to technical factors are emerging. Uncertainty quantification methods (Chapter 23) provide mechanisms for models to express appropriate caution when inputs fall outside their training distribution. The problem of confounding is tractable with sustained attention to data provenance, evaluation design, and deployment monitoring. The benchmark catalog in Chapter 20 identifies which evaluation resources are most susceptible to particular confounders, while the evaluation methodology in Chapter 21 provides protocols for detecting leakage before it inflates reported performance.\nYet vigilance remains essential. New datasets bring new confounders. Novel architectures create new opportunities for shortcut learning. Community benchmarks accumulate indirect leakage as resources are reused across studies. Treating confounding as a first-order concern throughout model development, rather than an afterthought addressed only when reviewers complain, distinguishes models that actually work from models that merely perform well on convenient benchmarks. The interpretability methods in Chapter 24 provide tools for distinguishing genuine regulatory insight from sophisticated pattern matching, with mechanistic interpretability (Section 24.7) offering the strongest evidence about what models have actually learned. The uncertainty quantification approaches in Chapter 23 enable models to communicate when their predictions should not be trusted, with selective prediction (Section 23.7) providing operational frameworks for routing uncertain cases to human review. Together with rigorous evaluation, these capabilities move the field toward models that reveal genuine biology and behave reliably across the diverse clinical and scientific settings where they will be deployed.\n\n\n\n\nDavey Smith, George, and Shah Ebrahim. 2003. “‘Mendelian Randomization’: Can Genetic Epidemiology Contribute to Understanding Environmental Determinants of Disease?*.” International Journal of Epidemiology 32 (1): 1–22. https://doi.org/10.1093/ije/dyg070.\n\n\nDuncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R. Peterson, and B. Domingue. 2019. “Analysis of Polygenic Risk Score Usage and Performance in Diverse Human Populations.” Nature Communications 10 (1): 3328. https://doi.org/10.1038/s41467-019-11112-0.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLipsitch, Marc, Eric Tchetgen Tchetgen, and Ted Cohen. 2010. “Negative Controls: A Tool for Detecting Confounding and Bias in Observational Studies.” Epidemiology 21 (3): 383. https://doi.org/10.1097/EDE.0b013e3181d61eeb.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006. “Population Structure and Eigenanalysis.” PLOS Genetics 2 (12): e190. https://doi.org/10.1371/journal.pgen.0020190.\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\nPrice, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E. Weinblatt, Nancy A. Shadick, and David Reich. 2006. “Principal Components Analysis Corrects for Stratification in Genome-Wide Association Studies.” Nature Genetics 38 (8): 904–9. https://doi.org/10.1038/ng1847.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Confounders and Leakage</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html",
    "href": "part_5/p5-ch23-uncertainty.html",
    "title": "23  Uncertainty Quantification",
    "section": "",
    "text": "23.1 Types of Uncertainty in Genomic Prediction\nA pathogenicity score of 0.73 means nothing unless we know what 0.73 means. If the model is well-calibrated, approximately 73% of variants receiving this score are truly pathogenic, and a clinician can weigh this probability against the costs of further testing. If the model is miscalibrated, the true pathogenicity rate among variants scored at 0.73 could be 40% or 95%, and the nominal probability provides no reliable basis for decision-making. The distinction is not between accurate and inaccurate models but between models that know what they know and models that do not. A miscalibrated model with high average accuracy can be more dangerous than a calibrated model with lower accuracy, because the miscalibrated model provides false confidence that leads to systematically wrong decisions.\nFoundation models produce continuous scores, but clinical decisions require categorical actions: test or do not test, treat or do not treat, report to the family or continue monitoring. This translation from probability to action only works when probabilities are trustworthy. A model that systematically overstates confidence will trigger unnecessary interventions. A model that understates confidence will miss actionable findings. A model that reports high confidence on inputs it has never seen before fails at a fundamental level regardless of its average performance on familiar data. Uncertainty quantification provides the tools to assess when model predictions deserve trust.\nUncertainty in genomic prediction arises from two fundamentally different sources that demand different responses. One source reflects limitations in what the model has learned from available data; this uncertainty can, in principle, be reduced by gathering more examples or improving model architecture. The other source reflects genuine randomness in the biological system itself, where identical genotypes produce variable phenotypes through stochastic developmental processes, environmental interactions, or incomplete penetrance. Distinguishing between these sources determines whether additional data collection would help or whether we must accept irreducible limits on predictive confidence.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-types",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-types",
    "title": "23  Uncertainty Quantification",
    "section": "",
    "text": "23.1.1 Why Uncertainty Matters\nClinical genetics operates under fundamental uncertainty. When a laboratory reports a variant of uncertain significance (VUS), they acknowledge that current evidence cannot confidently classify the variant as pathogenic or benign. ClinVar contains approximately two million VUS compared to roughly 250,000 variants classified as pathogenic (Landrum et al. 2018), reflecting the reality that most genetic variation remains incompletely understood. Foundation models inherit and sometimes amplify this uncertainty: they may produce confident-seeming scores for variants where the underlying biology remains genuinely unknown. The challenges of VUS classification and current interpretation frameworks are examined in detail in Chapter 26.\nThe consequences of ignoring uncertainty extend beyond statistical abstraction. An overconfident pathogenic prediction may trigger unnecessary interventions, from prophylactic surgeries to reproductive decisions that alter family planning. An overconfident benign prediction may provide false reassurance, delaying diagnosis while a treatable condition progresses. In both cases, the harm stems not from prediction error per se but from the mismatch between stated confidence and actual reliability. A model that accurately conveys its uncertainty enables appropriate clinical reasoning even when the prediction itself is imperfect.\nDecision theory formalizes this intuition. The expected value of a clinical action depends on the probability of each outcome weighted by its utility. When a model reports 0.73 probability of pathogenicity, downstream decision-making implicitly assumes this probability is accurate. If the true probability is 0.50, actions optimized for 0.73 will systematically err. Uncertainty quantification ensures that the probabilities entering clinical decisions reflect genuine knowledge rather than artifacts of model architecture or training procedure.\n\n\n23.1.2 Epistemic Uncertainty\nA model trained exclusively on European-ancestry data encounters its first genome from an individual of African ancestry. The model’s predictions may be statistically valid within the distribution it has seen, yet unreliable for this new input due to limited exposure to ancestry-specific patterns of variation, linkage disequilibrium, and regulatory architecture. This uncertainty about what the model has learned, as distinct from noise inherent in the prediction task itself, constitutes epistemic uncertainty.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 23.1: [Essential] Two-panel conceptual diagram. Panel A (Epistemic, reducible): Novel protein fold not in training; embedding space with dense clusters and isolated test point; could reduce with more data; ensemble members disagree; examples (under-represented ancestry, rare gene, novel pathogen). Panel B (Aleatoric, irreducible): Incomplete penetrance in BRCA1; same variant, different outcomes; inherent randomness; examples (penetrance variation, deep mutational scanning noise, context-dependent regulation). Bottom: Decomposition formula; practical implication (high epistemic → more data; high aleatoric → accept limits).\n\n\n\nEpistemic uncertainty arises from limitations in training data that could, in principle, be reduced by gathering more examples. In genomic foundation models, epistemic uncertainty concentrates in predictable regions of biological space. Proteins from poorly characterized families, where training data contained few homologs, exhibit high epistemic uncertainty because the model has limited basis for inference. This manifests concretely in protein benchmarks: ProteinGym performance varies substantially across protein families (Section 20.1.3). Genes with few characterized variants in ClinVar or gnomAD provide sparse supervision, leaving the model uncertain about which sequence features distinguish pathogenic from benign variation (see Section 2.8.1 and Section 2.2.3 for data resource details). Rare variant classes, such as in-frame deletions in specific protein domains, appear infrequently in training data and consequently generate uncertain predictions. Populations under-represented in biobanks contribute fewer training examples, creating systematic epistemic uncertainty for individuals from these backgrounds, a challenge examined in Section 3.7 and with confounding implications discussed in Section 22.2.1.\nMathematically, epistemic uncertainty reflects uncertainty over model parameters or learned representations. A Bayesian perspective treats the trained model as one sample from a posterior distribution over possible models consistent with the training data. Different plausible models may disagree on predictions for inputs far from training examples while agreeing on well-represented inputs. This disagreement manifests as high variance in predictions across model variants, sensitivity to random initialization, or instability under small perturbations to training data.\nFoundation models exhibit epistemic uncertainty through several observable signatures. Embeddings for unfamiliar sequences cluster in sparse regions of representation space, distant from the dense clusters formed by well-represented sequence families. Ensemble members trained with different random seeds produce divergent predictions for novel inputs while converging for familiar ones. Fine-tuning on the same downstream task with different random seeds yields inconsistent results for edge cases. These signatures provide practical diagnostics for identifying when epistemic uncertainty is high.\n\n\n23.1.3 Aleatoric Uncertainty\nSome variants are genuinely ambiguous regardless of how much data we collect. The same pathogenic variant in BRCA1 causes breast cancer in one carrier but not another due to modifier genes, hormonal exposures, or stochastic developmental processes. Incomplete penetrance, the phenomenon where disease-associated variants do not always produce disease, creates irreducible uncertainty that no amount of training data can eliminate. This inherent randomness in the mapping from genotype to phenotype constitutes aleatoric uncertainty.\nAleatoric uncertainty reflects noise or stochasticity intrinsic to the prediction problem rather than limitations of the model. Variable expressivity means that even when a variant causes disease, the severity and specific manifestations vary across individuals. Measurement noise in functional assays introduces uncertainty into the labels used for training: deep mutational scanning experiments typically exhibit 10 to 20 percent technical variation between replicates (Fowler and Fields 2014; Rubin et al. 2017),, creating a floor below which prediction error cannot decrease regardless of model sophistication (see Section 2.4.4 for a discussion of DMS data characteristics). Stochastic gene expression means that two genetically identical cells may express a gene at different levels due to random fluctuations in transcription and translation. These sources of randomness set fundamental limits on predictive accuracy.\nAleatoric uncertainty often varies with the input, a property termed heteroscedasticity. Coding variants in essential genes may have relatively low aleatoric uncertainty because strong selection pressure produces consistent phenotypic effects. Regulatory variants exhibit higher aleatoric uncertainty because their effects depend on cellular context, developmental timing, and interactions with other genetic and environmental factors. A model that captures this heteroscedasticity can provide more informative uncertainty estimates by conveying that some predictions are inherently more reliable than others.\n\n\n23.1.4 Decomposing Total Uncertainty\nTotal predictive uncertainty combines epistemic and aleatoric components, and distinguishing between them has practical implications for decision-making. High epistemic uncertainty suggests that gathering more data, either through additional training examples or further investigation of the specific case, could reduce uncertainty and improve the prediction. High aleatoric uncertainty indicates that the prediction is as good as it can get given inherent noise in the problem; additional data will not help because the underlying biology is stochastic.\nThe law of total variance provides a mathematical framework for decomposition. Total variance in predictions equals the sum of variance due to model uncertainty (epistemic) and variance inherent in the data-generating process (aleatoric). In practice, ensemble methods approximate epistemic uncertainty through disagreement between members: if five independently trained models produce predictions of 0.65, 0.68, 0.70, 0.72, and 0.75, the spread reflects epistemic uncertainty, while the residual variance within each model’s predictions reflects aleatoric uncertainty. Heteroscedastic neural networks, which output both a predicted mean and a predicted variance, can estimate aleatoric uncertainty by learning input-dependent noise levels.\nThese decompositions depend on modeling assumptions and provide approximations rather than exact separations. Ensemble disagreement may underestimate epistemic uncertainty if all members share similar biases from common training data. Heteroscedastic models may confound aleatoric and epistemic uncertainty if the training data is too sparse to reliably estimate noise levels. Despite these limitations, approximate decomposition provides actionable information: variants flagged for high epistemic uncertainty warrant additional data collection or expert review, while variants with high aleatoric uncertainty may require acceptance of irreducible limits on predictive confidence.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-calibration",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-calibration",
    "title": "23  Uncertainty Quantification",
    "section": "23.2 Calibration and Confidence Interpretation",
    "text": "23.2 Calibration and Confidence Interpretation\nCalibration determines whether model outputs can be interpreted as probabilities. A score of 0.85 from a pathogenicity predictor should mean that 85% of variants receiving similar scores are truly pathogenic; only then can clinicians rationally weight computational evidence against other diagnostic criteria. This chapter provides the comprehensive treatment of calibration theory and methods. Readers encountering calibration in applied contexts, whether variant effect prediction (Section 14.5) or evaluation methodology (Chapter 21), are referred here for the formal foundations and complete methodological catalog.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 23.2: [Essential] Four-panel figure. Panel A (Perfect calibration): Reliability diagram with points on diagonal; annotation “Predictions match reality.” Panel B (Overconfident): Points below diagonal; model predicts 0.9, true frequency 0.6; annotation “Common for deep neural networks” and “Dangerous for clinical use.” Panel C (Underconfident): Points above diagonal; annotation “May miss actionable variants.” Panel D (Miscalibration by subgroup): Two reliability curves (European ancestry, African ancestry); one well-calibrated, one poorly calibrated; annotation “Aggregate metrics can mask disparities.” Include expected calibration error (ECE) values, prediction histograms showing score distributions, and note “Calibration ≠ discrimination.”\n\n\n\n\n23.2.1 The Calibration Problem\nAlphaMissense outputs a continuous score between 0 and 1 for each possible missense variant in the human proteome. When it reports 0.85 for a particular variant, what does this number mean? If the model is calibrated, collecting all variants scored near 0.85 and checking their true clinical status should reveal that approximately 85% are pathogenic. Perfect calibration means that predicted probabilities match observed frequencies across the entire range of model outputs: among variants scored at 0.30, roughly 30% should be pathogenic; among variants scored at 0.95, roughly 95% should be pathogenic. This alignment between stated confidence and empirical accuracy is calibration, and most foundation models fail to achieve it.\nFormally, a model \\(f\\) mapping inputs \\(X\\) to probability estimates \\(p = f(X)\\) is calibrated if \\(P(Y = 1 \\mid f(X) = p) = p\\) for all \\(p\\) in the interval from \\(0\\) to \\(1\\). The calibration condition requires that the model’s stated confidence equals the true probability of the positive class conditional on that stated confidence. Miscalibration occurs when this equality fails: overconfident models produce predicted probabilities that exceed true frequencies (a variant scored at \\(0.85\\) is pathogenic only \\(60\\%\\) of the time), while underconfident models produce predicted probabilities below true frequencies.\nModern deep neural networks are systematically miscalibrated despite achieving high accuracy. Guo and colleagues demonstrated that contemporary architectures exhibit worse calibration than older, less accurate models (Guo et al. 2017). The phenomenon arises because standard training objectives like cross-entropy loss optimize for discrimination (separating positive from negative examples) rather than calibration (matching predicted probabilities to frequencies). Over-parameterized models with capacity exceeding what the data requires can achieve near-perfect training loss while producing overconfident predictions on held-out data. The softmax temperature in transformer architectures affects the sharpness of probability distributions, and default settings often produce excessively peaked outputs.\nCalibration and discrimination are distinct properties. A model can achieve perfect area under the receiver operating characteristic curve (auROC), correctly ranking all positive examples above all negative examples, while being arbitrarily miscalibrated. If a classifier assigns probability 0.99 to all positive examples and 0.98 to all negative examples, it ranks perfectly but provides useless probability estimates. Conversely, a calibrated model that assigns 0.51 to positives and 0.49 to negatives would be calibrated but nearly useless for discrimination. Clinical applications typically require both: accurate ranking to identify high-risk variants and accurate probabilities to inform decision-making.\n\n\n23.2.2 Measuring Calibration\nReliability diagrams provide visual assessment of calibration by plotting predicted probabilities against observed frequencies. Construction involves binning predictions into intervals (commonly ten bins spanning 0 to 0.1, 0.1 to 0.2, and so forth), computing the mean predicted probability within each bin, computing the fraction of positive examples within each bin, and plotting these two quantities against each other. A perfectly calibrated model produces points along the diagonal where predicted probability equals observed frequency. Systematic deviations reveal calibration patterns: points below the diagonal indicate overconfidence (predictions exceed reality), points above indicate underconfidence, and S-shaped curves suggest nonlinear miscalibration requiring more flexible correction.\nExpected calibration error (ECE) provides a scalar summary of calibration quality. ECE computes the weighted average absolute difference between predicted probabilities and observed frequencies across bins:\n\\[\n\\text{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{n} \\left| \\text{acc}(B_m) - \\text{conf}(B_m) \\right|\n\\]\nwhere \\(B_m\\) denotes the set of examples in bin \\(m\\), \\(|B_m|\\) is the number of examples in that bin, \\(n\\) is the total number of examples, \\(\\text{acc}(B_m)\\) is the accuracy (fraction of positives) in bin \\(m\\), and \\(\\text{conf}(B_m)\\) is the mean predicted probability in bin \\(m\\). Lower ECE indicates better calibration, with zero representing perfect calibration. ECE depends on binning strategy; equal-width bins may place most examples in a few bins for models with concentrated predictions, while equal-mass bins ensure each bin contains the same number of examples but may span wide probability ranges.\nMaximum calibration error (MCE) captures worst-case miscalibration by reporting the largest absolute gap between predicted and observed frequencies across all bins. MCE is appropriate when any severe miscalibration is unacceptable, as in high-stakes clinical applications where even rare catastrophic errors carry significant consequences.\nBrier score decomposes into components measuring calibration and discrimination (refinement), providing a single proper scoring rule that rewards both properties. The Brier score equals the mean squared difference between predicted probabilities and binary outcomes, and its decomposition reveals whether poor scores stem from miscalibration, poor discrimination, or both.\n\n\n23.2.3 Why Foundation Models Are Often Miscalibrated\nFoundation models face calibration challenges beyond those affecting standard neural networks. Pretraining objectives like masked language modeling optimize for predicting held-out tokens, not for producing calibrated probability distributions over downstream tasks (see Chapter 8 for a detailed discussion of pretraining objectives). The representations learned during pretraining may encode useful information about sequence biology while providing no guarantee that fine-tuned classifiers will be well-calibrated.\nDistribution shift between pretraining and evaluation compounds miscalibration. A protein language model pretrained on UniRef sequences encounters a fine-tuning task using ClinVar variants. The pretraining distribution emphasizes common proteins with many homologs, while clinical variants concentrate in disease-associated genes with different sequence characteristics. Models may be well-calibrated on held-out pretraining data while miscalibrated on clinically relevant evaluation sets. The broader challenges of distribution shift are examined in Section 9.8.\nLabel noise in training data propagates to calibration errors. ClinVar annotations reflect the state of knowledge at submission time and may contain errors, particularly for older entries or variants from less-studied genes. Deep mutational scanning experiments provide functional labels but with measurement noise that varies across assays. Models trained on noisy labels may learn the noise distribution, producing predictions that match training labels but not underlying truth.\nZero-shot approaches present particular calibration challenges. ESM-1v log-likelihood ratios measure how surprising a mutation is to the language model, but these ratios are not probabilities and have no inherent calibration. Converting log-likelihood ratios to pathogenicity probabilities requires explicit calibration against external labels, and the resulting calibration depends on the reference dataset used for this conversion. The protein language model family and its variant effect scoring capabilities are discussed in Section 12.6.\n\n\n23.2.4 Calibration Across Subgroups\nAggregate calibration metrics can mask severe miscalibration in clinically important subgroups. A model might achieve low ECE overall while being dramatically overconfident for variants in African-ancestry individuals and underconfident for European-ancestry individuals, with opposite errors canceling in aggregate statistics. Subgroup-stratified calibration assessment is essential for any model intended for diverse populations.\nAncestry-stratified calibration reveals systematic patterns in current foundation models. Training data for protein language models and variant effect predictors derive predominantly from European-ancestry cohorts, creating differential epistemic uncertainty across populations. Calibration curves stratified by ancestry often show that models are better calibrated for populations well-represented in training data and overconfident or underconfident for underrepresented populations. This differential calibration has direct fairness implications: clinical decisions based on miscalibrated predictions will be systematically worse for patients from underrepresented backgrounds. The broader challenges of fairness and health equity are addressed in Section 3.7.2 and Chapter 29.\nCalibration may also vary by variant class, gene constraint level, protein family, or disease category. Missense variants in highly constrained genes may show different calibration patterns than those in tolerant genes. Variants in well-studied protein families with abundant training examples may be better calibrated than variants in orphan proteins. Stratified reliability diagrams across these categories reveal whether a single calibration correction suffices or whether subgroup-specific approaches are necessary.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-post-hoc-calibration",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-post-hoc-calibration",
    "title": "23  Uncertainty Quantification",
    "section": "23.3 Post-Hoc Calibration Methods",
    "text": "23.3 Post-Hoc Calibration Methods\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 23.3: [High] Three-column comparison with before/after. Column 1 (Temperature scaling): Formula p = softmax(z/T); single parameter T; before/after reliability; preserves ranking; can not fix complex patterns. Column 2 (Platt scaling): Formula p = σ(az + b); two parameters; handles bias and confidence; assumes logistic. Column 3 (Isotonic regression): Non-parametric monotonic fit; flexible, no assumptions; overfits with limited data. Bottom: Decision guide for method selection.\n\n\n\n\n23.3.1 Temperature Scaling\nThe simplest calibration fix is often the most effective. Temperature scaling applies a single learned parameter to adjust model confidence, dramatically improving calibration with minimal computational overhead and no change to model predictions’ ranking.\nThe method modifies the softmax function by dividing logits by a temperature parameter T before applying softmax:\n\\[\\hat{p}_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\\]\nwhere z_i are the logits (pre-softmax outputs) and *p̂_i* are the calibrated probabilities. When T &gt; 1, the distribution becomes softer (more uniform), reducing overconfidence. When T &lt; 1, the distribution becomes sharper, increasing confidence. The optimal temperature is learned by minimizing negative log-likelihood on a held-out calibration set, typically yielding T between 1.5 and 3 for overconfident deep networks.\nTemperature scaling preserves the model’s ranking because dividing all logits by the same constant does not change their relative ordering. A variant ranked as more likely pathogenic than another remains more likely after temperature scaling; only the magnitudes of probability estimates change. This preservation of discrimination while improving calibration makes temperature scaling particularly attractive: calibration improves without sacrificing the model’s hard-won ability to distinguish pathogenic from benign variants.\nThe method’s simplicity (one parameter) is both strength and limitation. A single global temperature cannot fix heterogeneous miscalibration where the model is overconfident in some regions of input space and underconfident in others. When reliability diagrams show complex nonlinear patterns, more flexible calibration methods are necessary.\n\n\n23.3.2 Platt Scaling\nPlatt scaling fits a logistic regression model on the original model’s outputs, learning both a slope and intercept to transform scores into calibrated probabilities. For binary classification:\n\\[\\hat{p} = \\sigma(a \\cdot f(x) + b)\\]\nwhere \\(f(x)\\) is the original model’s output, \\(\\sigma\\) is the sigmoid function, and parameters \\(a\\) and \\(b\\) are learned on calibration data. The two parameters provide more flexibility than temperature scaling’s single parameter, allowing correction of both the sharpness and the location of the probability distribution.\nPlatt scaling is appropriate when miscalibration involves systematic bias (predictions consistently too high or too low) in addition to over- or underconfidence. The method assumes that a monotonic logistic transformation suffices to correct miscalibration, which may not hold for models with complex, non-monotonic calibration curves.\n\n\n23.3.3 Isotonic Regression\nIsotonic regression provides a non-parametric approach that fits a monotonically increasing function mapping raw scores to calibrated probabilities. Unlike temperature or Platt scaling, isotonic regression makes no assumptions about the functional form of miscalibration, allowing it to correct arbitrary monotonic patterns.\nThe method works by pooling adjacent bins whose empirical frequencies violate monotonicity, then assigning each bin its pooled frequency. The resulting calibration function is a step function that increases with the original score. This flexibility comes at a cost: with limited calibration data, isotonic regression may overfit to noise in the calibration set, and the step-function output can appear discontinuous. Additionally, isotonic regression provides no uncertainty estimate on the calibration itself; we learn a point estimate of the calibration function without knowing how reliable that estimate is.\n\n\n23.3.4 Calibrating Foundation Model Outputs\nGenomic foundation models present specific calibration considerations beyond standard classification settings. The choice of calibration approach depends on whether the model produces logits, log-likelihood ratios, or continuous regression outputs, and on whether calibration targets are available for the deployment distribution.\nFor zero-shot variant effect scores like ESM-1v log-likelihood ratios, raw outputs have no inherent probabilistic interpretation. Calibration requires mapping these continuous scores to pathogenicity probabilities using external labels, typically from ClinVar or population frequency data. This mapping should occur on held-out genes or variants not used for any model development, and the resulting calibration reflects the specific label set used; calibration against ClinVar pathogenic/benign labels may not transfer to other clinical contexts. The principles of proper held-out evaluation are discussed in Chapter 21.\nMulti-output models that predict across many tasks (multiple cell types, tissues, or assays) may require separate calibration for each output. A regulatory model predicting expression across 200 cell types is unlikely to be uniformly calibrated across all outputs; cell types with more training data may show better calibration than rare cell types.\nTemporal stability of calibration deserves consideration. As ClinVar annotations evolve with new evidence, the ground truth against which models were calibrated changes. A model calibrated against 2020 ClinVar labels may become miscalibrated relative to 2025 labels as variant classifications are updated. Periodic recalibration against current labels helps maintain clinical relevance.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-uq-methods",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-uq-methods",
    "title": "23  Uncertainty Quantification",
    "section": "23.4 Uncertainty Quantification Methods for Foundation Models",
    "text": "23.4 Uncertainty Quantification Methods for Foundation Models\nCalibration ensures that stated probabilities match observed frequencies, but even well-calibrated models provide only point estimates. When a model reports 0.70 pathogenicity probability, is that uncertainty reducible with more data, or does it reflect genuine ambiguity in the biological signal? Distinguishing these sources of uncertainty enables more appropriate clinical responses: epistemic uncertainty (arising from limited data) suggests the prediction might change with additional evidence, while aleatoric uncertainty (inherent to the problem) indicates that even perfect models would remain uncertain.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 23.4: [High] Method comparison across four approaches. Deep ensembles: 5 models independently trained; variance/entropy computed across predictions; 5× training and inference cost; gold standard for epistemic uncertainty. MC Dropout: Single model with multiple stochastic forward passes; variance across dropout samples; 1× training cost, \\(N\\)× inference cost; approximates ensemble behavior. Last-layer ensembles: Frozen pretrained backbone with ensemble of prediction heads; head disagreement captures fine-tuning uncertainty; 1× pretraining cost plus 5× cheap head training; practical for foundation models. Heteroscedastic networks: Single model outputs mean plus variance; learned input-dependent uncertainty; 1× training and inference cost; captures aleatoric but not epistemic uncertainty. Comparison table showing which methods capture epistemic vs. aleatoric uncertainty, computational cost, and foundation model compatibility.\n\n\n\n\n23.4.1 Deep Ensembles\nIf one model expresses uncertainty about a prediction, querying multiple models reveals whether that uncertainty reflects genuine ambiguity in the data or an artifact of a particular training run. When five independently trained models agree on a prediction, confidence is warranted; when they disagree, the disagreement itself signals uncertainty. Ensemble disagreement provides one of the most reliable uncertainty estimates available in deep learning, at the cost of training and maintaining multiple models.\nDeep ensembles train M models (typically 5 to 10) with different random initializations, data orderings, or minor architectural variations. At inference time, all members produce predictions, and uncertainty is estimated from the variance or entropy of the ensemble distribution. For classification, epistemic uncertainty appears as disagreement in predicted class probabilities across members. For regression, epistemic uncertainty appears as variance in predicted values.\nThe theoretical basis for ensemble uncertainty estimation rests on the observation that disagreement between models reflects regions of input space where the training data provides insufficient constraint. Where training examples are dense, gradient descent from different initializations converges to similar solutions, producing agreement. Where training examples are sparse or conflicting, different initializations find different local optima, producing disagreement. This interpretation connects ensembles to Bayesian model averaging, where predictions are averaged over the posterior distribution of model parameters.\nFor foundation models with billions of parameters, training full ensembles becomes prohibitively expensive. Training five copies of ESM-2 requires approximately five times the compute of a single model, potentially millions of dollars in cloud computing costs. Several practical alternatives reduce this burden. Last-layer ensembles freeze the pretrained backbone and train only an ensemble of prediction heads, reducing cost by orders of magnitude while still capturing uncertainty from the fine-tuning process. Snapshot ensembles save model checkpoints at various points during optimization and use these snapshots as ensemble members, requiring only single-model training time. Multi-seed fine-tuning trains the same architecture from multiple random seeds on the fine-tuning task, which is far cheaper than multi-seed pretraining. The broader considerations of fine-tuning and adaptation strategies are discussed in Chapter 9.\n\n\n23.4.2 Monte Carlo Dropout\nMonte Carlo (MC) dropout provides uncertainty estimates from a single trained model by treating dropout regularization as approximate Bayesian inference. During standard training with dropout, random subsets of neurons are zeroed at each forward pass. MC dropout keeps dropout active at test time and performs multiple stochastic forward passes, treating the variation across passes as a measure of model uncertainty.\nGal and Ghahramani showed that this procedure approximates variational inference over the model’s weights (Gal and Ghahramani 2016). Each forward pass with dropout samples a different subnetwork, and the distribution of predictions across samples approximates the predictive distribution under a particular prior over weights. High variance across MC samples indicates epistemic uncertainty about the model’s parameters for that input.\nMC dropout offers the significant advantage of requiring only a single trained model, avoiding the computational overhead of ensembles. Implementation is straightforward: enable dropout during inference and average predictions over 10 to 50 stochastic forward passes. The variance or entropy of these predictions serves as the uncertainty estimate.\nLimitations temper the method’s appeal. Modern transformer architectures often do not use dropout in their standard configurations, or use dropout only in specific locations (attention dropout, residual dropout) where the approximation may be less accurate. The quality of uncertainty estimates depends on the dropout rate and architecture, with higher dropout rates providing better uncertainty estimates but potentially degrading mean predictions. Empirical comparisons often find that MC dropout underestimates uncertainty relative to deep ensembles, particularly in low-data regimes where epistemic uncertainty should be high.\n\n\n23.4.3 Heteroscedastic Models\nStandard regression models predict a single output value, implicitly assuming constant noise variance across all inputs. Heteroscedastic models instead predict both a mean and a variance for each input, capturing the intuition that prediction uncertainty varies depending on the input. For genomic applications, this approach naturally handles the observation that some prediction tasks are inherently noisier than others: coding variant effects may be more predictable than regulatory variant effects, constrained genes more predictable than tolerant genes.\nArchitecture modifications are minimal. Instead of outputting a single value, the model outputs two values interpreted as the mean \\(\\mu(x)\\) and variance \\(\\sigma^2(x)\\) of a Gaussian distribution over outputs. Training uses negative log-likelihood loss under this Gaussian, which penalizes both prediction errors and miscalibrated variance estimates:\n\\[\\mathcal{L} = \\frac{1}{2\\sigma^2(x)}(y - \\mu(x))^2 + \\frac{1}{2}\\log \\sigma^2(x)\\]\nThe first term penalizes prediction errors, weighted by inverse variance so that high-variance predictions are penalized less for the same absolute error. The second term prevents the model from simply predicting infinite variance to avoid all penalties. The result is a model that learns to predict larger variance for inputs where training labels are noisy or inconsistent, capturing aleatoric uncertainty in an input-dependent manner.\nHeteroscedastic models capture aleatoric uncertainty but not epistemic uncertainty. The predicted variance reflects noise inherent in the labels, not uncertainty about model parameters. Combining heteroscedastic outputs with ensemble methods provides estimates of both uncertainty types: ensemble disagreement captures epistemic uncertainty while the predicted variance captures aleatoric uncertainty.\n\n\n23.4.4 Evidential Deep Learning\nEvidential deep learning places a prior distribution over the class probabilities themselves rather than directly predicting probabilities. For classification, the model outputs parameters of a Dirichlet distribution, which serves as a prior over the simplex of class probabilities. The concentration parameters of this Dirichlet encode both the predicted class probabilities (via their relative magnitudes) and the model’s uncertainty (via their absolute magnitudes).\nLow total concentration indicates high uncertainty: the model is unsure which class is correct. High total concentration with one dominant class indicates confident prediction. This framework provides a principled way to separate epistemic uncertainty (low concentration) from confident predictions (high concentration), all from a single forward pass without ensembling or MC sampling.\nCritics have noted that evidential deep learning can produce unreliable uncertainty estimates when the distributional assumptions are violated or when training data is limited [Citation Needed]. Practical experience suggests that ensembles and MC dropout often provide more robust uncertainty estimates, though evidential methods continue to be refined.\n\n\n23.4.5 Selecting Uncertainty Quantification Methods\nThe choice among uncertainty quantification methods depends on computational constraints, the types of uncertainty relevant to the application, and the foundation model architecture.\nFor applications where distinguishing epistemic from aleatoric uncertainty matters, combining ensemble methods with heteroscedastic predictions provides both. Ensemble disagreement identifies variants where more training data might reduce uncertainty, while high predicted variance identifies variants where uncertainty is inherent to the prediction task.\nFor foundation model applications where full ensembles are impractical, last-layer ensembles offer the best trade-off between computational cost and uncertainty quality. The pretrained representations capture most of the model’s knowledge, and ensembling only the prediction heads captures uncertainty arising from the fine-tuning task.\nFor real-time applications requiring single forward passes, evidential deep learning or heteroscedastic models provide uncertainty estimates without inference-time overhead. These methods capture aleatoric uncertainty effectively but may underestimate epistemic uncertainty for out-of-distribution inputs.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-conformal",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-conformal",
    "title": "23  Uncertainty Quantification",
    "section": "23.5 Conformal Prediction: Distribution-Free Guarantees",
    "text": "23.5 Conformal Prediction: Distribution-Free Guarantees\nMost uncertainty quantification methods make assumptions about model behavior or data distributions that may not hold in practice. Temperature scaling assumes miscalibration follows a particular functional form. Ensembles assume that disagreement reflects epistemic uncertainty rather than artifacts of training. Bayesian methods assume specific priors over model parameters. When these assumptions fail, uncertainty estimates may be unreliable precisely when reliability matters most.\nConformal prediction offers something stronger: finite-sample coverage guarantees that hold under minimal assumptions. Instead of outputting a point prediction, conformal methods produce a prediction set guaranteed to contain the true label with probability at least \\(1 - \\alpha\\), where \\(\\alpha\\) is a user-specified error rate. If we request \\(90\\%\\) coverage (\\(\\alpha = 0.10\\)), the prediction set will contain the true label at least \\(90\\%\\) of the time, regardless of the model’s accuracy or calibration. This guarantee requires only that calibration and test examples are exchangeable (a condition weaker than independent and identically distributed), making conformal prediction robust to model misspecification.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 23.5: [High] Workflow diagram with variant classification examples. Step 1 (Calibration): Score variants on held-out calibration set; histogram of non-conformity scores. Step 2 (Threshold): Find threshold q for desired coverage (e.g., 90%); vertical line on histogram. Step 3 (Prediction sets): Include classes with scores below threshold. Example outputs showing set sizes conveying confidence: High confidence → {Pathogenic}, moderate confidence → {Pathogenic, VUS}, low confidence → {Pathogenic, VUS, Benign}, abstention → {}. Key properties annotated: “Coverage guarantee (≥90% marginal)”; “Set size conveys uncertainty”; “No probability interpretation needed.” Limitation note: “Marginal, not conditional coverage.”\n\n\n\n\n23.5.1 Split Conformal Prediction\nThe most practical conformal method, split conformal prediction, begins by partitioning labeled data into training and calibration subsets. After training the model exclusively on the training portion, non-conformity scores are computed for each calibration example, where higher scores indicate poorer agreement between prediction and true label. The threshold \\(q\\) is then set at the \\((1-\\alpha)(1+1/n)\\) quantile of these calibration scores. At test time, the prediction set includes all labels whose non-conformity score falls below this threshold.\nNon-conformity scores measure how “strange” a candidate label is given the model’s output. For classification, a common choice is \\(1 - \\hat{p}_y\\), where \\(\\hat{p}_y\\) is the predicted probability of the true class. High predicted probability means low non-conformity (the label conforms to the model’s expectations); low predicted probability means high non-conformity. For regression, absolute residuals \\(|y - \\hat{y}|\\) serve as non-conformity scores.\nThe construction ensures coverage because calibration scores are exchangeable with test scores under the exchangeability assumption. The quantile threshold is set so that a random calibration score exceeds the threshold with probability at most \\(\\aplha\\); by exchangeability, the same holds for test scores. This elegant argument yields exact coverage guarantees without requiring the model to be accurate or well-calibrated.\nThe coverage guarantee is finite-sample: it holds exactly for any sample size, not just asymptotically. For clinical genomics applications where individual predictions carry significant consequences, this finite-sample property provides assurance that cannot be obtained from asymptotic calibration arguments.\n\n\n23.5.2 Conformal Prediction for Variant Classification\nVariant effect prediction, examined in detail in Chapter 14, concentrates the challenges of uncertainty quantification. Instead of reporting a single pathogenicity score, a conformalized variant classifier outputs a prediction set from the possibilities: {pathogenic}, {benign}, {pathogenic, benign}, or the empty set. The set is guaranteed to contain the true label at the specified coverage rate.\nSet size conveys uncertainty without requiring probability interpretation. A singleton prediction set indicates high confidence: the model has enough information to narrow to a single class. A set containing multiple classes indicates uncertainty: the model cannot confidently distinguish between possibilities. The empty set indicates extreme uncertainty where even the most permissive threshold cannot be satisfied.\nThe trade-off between coverage and informativeness shapes practical deployment. At 99% coverage, prediction sets will frequently include multiple classes, providing reliable but uninformative predictions. At 80% coverage, prediction sets will more often be singletons, providing informative but less reliable predictions. Stakeholders must choose coverage levels that match their tolerance for error versus the cost of uninformative predictions.\n\n\n23.5.3 Limitations and Practical Considerations\nConformal prediction provides marginal coverage guarantees: averaged over all inputs, 90% of prediction sets will contain the true label. This does not guarantee conditional coverage for any particular subgroup. A model might achieve 90% coverage overall while providing only 70% coverage for rare variant classes or underrepresented populations. Subgroup-stratified coverage assessment reveals these disparities, though achieving conditional coverage guarantees requires stronger assumptions or larger calibration datasets.\nThe exchangeability assumption can fail in practice. If the calibration set derives from one population and the test set from another, coverage guarantees may not hold. Temporal shifts (calibration on historical data, testing on future data) similarly violate exchangeability. Methods for conformal prediction under distribution shift exist but require additional assumptions about the nature of the shift.\nPrediction set size trades off against informativeness. Larger sets provide more reliable coverage but less useful predictions. A model that produces {pathogenic, benign} for every variant achieves perfect coverage but provides no discrimination. Careful model development to improve underlying accuracy reduces average set size while maintaining coverage guarantees.\n\n\n23.5.4 Integration with Clinical Workflows\nConformal prediction sets integrate naturally with existing variant classification frameworks. The ACMG-AMP guidelines already accommodate uncertainty through categories like “variant of uncertain significance.” Conformal sets provide a principled basis for this categorization: variants receiving singleton sets ({pathogenic} or {benign}) have strong computational evidence, while variants receiving larger sets have uncertain computational evidence. The ACMG-AMP framework and its integration with computational evidence are discussed in Chapter 26.\nThe coverage guarantee provides a quantitative basis for laboratory policies. A laboratory might decide that computational evidence should achieve 95% coverage before contributing to variant classification, using conformal methods to verify this threshold is met. The guarantee holds regardless of which specific variants are encountered, providing assurance that the policy will perform as intended across the laboratory’s case mix.\nConformal methods also enable selective prediction, where the model abstains rather than producing uncertain predictions. By setting coverage requirements appropriately, laboratories can identify variants where computational methods provide reliable evidence and variants where human review is essential. This selective approach focuses expert attention where it is most needed while allowing automated processing of straightforward cases.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-ood-detection",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-ood-detection",
    "title": "23  Uncertainty Quantification",
    "section": "23.6 Out-of-Distribution Detection",
    "text": "23.6 Out-of-Distribution Detection\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 23.6: [High] Embedding space visualization. Main panel: UMAP/t-SNE; dense training clusters (blue); isolated OOD points (red); examples (novel archaeal sequence, synthetic protein, variant in poorly characterized gene). Detection methods (side panels): Mahalanobis distance (histogram ID vs OOD, threshold), nearest neighbor distance, ensemble disagreement. Key insight: Flag for manual review rather than automate.\n\n\n\n\n23.6.1 Out-of-Distribution Problem\nA DNA language model trained on mammalian genomes encounters a novel archaeal sequence. The model’s embedding places this sequence in an unfamiliar region of representation space, far from the clusters formed by training examples. Yet the model still produces a prediction, potentially with high confidence, because standard neural networks are not designed to recognize when inputs lie outside their training distribution. Detecting out-of-distribution (OOD) inputs is essential for safe deployment of foundation models in settings where novel sequences are inevitable.\nOOD detection identifies inputs that differ meaningfully from training data, allowing systems to flag uncertain predictions before they cause harm. Novel pathogens may share little sequence similarity with characterized viruses in training data. Synthetic proteins designed for therapeutic purposes may occupy regions of sequence space unsampled by evolution. Variants in poorly characterized genes may lack the contextual information that models rely on for accurate prediction. In each case, recognizing that the input is unusual enables appropriate caution.\nThe confidence problem compounds OOD challenges. Neural networks often produce high-confidence predictions on OOD inputs because nothing in standard training penalizes confidence on unfamiliar examples. A classifier trained to distinguish pathogenic from benign variants may confidently predict “pathogenic” for a completely random sequence, not because it has evidence for pathogenicity but because it lacks the capacity to say “I do not know.” This failure mode makes OOD detection essential rather than optional.\n\n\n23.6.2 Likelihood-Based Detection and Its Failures\nThe intuitive approach to OOD detection uses model likelihood: inputs the model finds improbable should be flagged as OOD. Language models assign likelihoods to sequences; surely OOD sequences should receive low likelihood?\nThis intuition fails for deep generative models. Complex models can assign high likelihood to OOD data for reasons unrelated to semantic similarity to training examples. In high-dimensional spaces, typical sets (regions where most probability mass concentrates) do not coincide with high-density regions. A sequence might land in a high-density region of the model’s distribution while being semantically distant from any training example.\nEmpirically, language models assign high likelihood to repetitive sequences, sequences with unusual but consistent patterns, and sequences from different domains that happen to share statistical properties with training data [Citation Needed]. For genomic models, this means likelihood alone cannot reliably distinguish novel biological sequences from sequences within the training distribution.\n\n\n23.6.3 Embedding-Based Detection\nLearned representations provide more reliable OOD detection than raw likelihood. The key insight is that embeddings encode semantic structure: similar sequences cluster together in embedding space, and OOD sequences land in sparse regions distant from training clusters.\nMahalanobis distance measures how far a test embedding lies from training data, accounting for the covariance structure of the embedding space. For each class, compute the mean embedding and covariance matrix from training examples. For a test input, compute its distance to each class centroid in units of standard deviations, accounting for correlations between embedding dimensions. Large Mahalanobis distance indicates OOD inputs.\nNearest-neighbor methods provide a non-parametric alternative. For a test embedding, find the \\(k\\) nearest neighbors among training embeddings and compute the average distance. Large average distance to neighbors indicates the test input lies in a sparse region of embedding space, suggesting it is OOD. This approach makes no distributional assumptions and scales well with modern approximate nearest-neighbor algorithms.\nFor genomic foundation models, embedding-based OOD detection enables practical deployment safeguards. ESM embeddings place novel protein folds in regions distant from characterized folds, allowing detection of sequences outside the model’s training experience. DNABERT embeddings reveal unusual sequence composition or repeat structures that may confound predictions. Flagging these cases for expert review prevents confident but unreliable predictions from reaching clinical decisions. The properties of DNA and protein language model embeddings are discussed in Chapter 11 and Chapter 12.\n\n\n23.6.4 Practical OOD Detection for Genomic Applications\nDefining what counts as OOD requires domain knowledge. Novel species or clades may share evolutionary history with training examples yet differ enough to warrant caution. Extreme GC content can indicate contamination, unusual biology, or simply under-represented genomic regions. Engineered sequences (designed proteins, synthetic regulatory elements) intentionally explore regions of sequence space not represented in natural sequences.\nCombining multiple OOD signals improves reliability. Embedding distance, likelihood, and prediction confidence each capture different aspects of distributional difference. An input flagged by multiple methods is more reliably OOD than one flagged by a single method. Threshold selection involves trade-offs between false positives (flagging in-distribution examples unnecessarily) and false negatives (missing true OOD examples).\nThe operational response to OOD detection depends on the application. For variant interpretation, OOD inputs might trigger automatic flagging for expert review rather than automated classification. For high-throughput screening, OOD inputs might receive tentative predictions with explicit uncertainty warnings. For safety-critical applications, OOD inputs might trigger rejection with a request for additional information.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-selective-prediction",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-selective-prediction",
    "title": "23  Uncertainty Quantification",
    "section": "23.7 Selective Prediction and Abstention",
    "text": "23.7 Selective Prediction and Abstention\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 23.7: [Enhancing] Accuracy-coverage tradeoff. Main plot: Coverage (x) vs Accuracy (y); curve rising as coverage decreases (more selective = more accurate). Operating point selection: Clinical applications need different tradeoffs. Reject option: High uncertainty → abstain. Example: 90% coverage with 85% accuracy vs 70% coverage with 95% accuracy. Connection to clinical workflow: Rejected variants → expert review.\n\n\n\n\n23.7.1 When to Abstain\nA variant effect predictor achieving 95% accuracy overall provides more clinical value if it can identify which predictions are reliable. Selective prediction allows models to abstain on difficult cases, concentrating predictions on inputs where confidence is warranted. The trade-off between coverage (fraction of inputs receiving predictions) and accuracy (correctness among predictions made) defines the selective prediction problem.\nThe coverage-accuracy trade-off reflects a fundamental tension. At 100% coverage, the model predicts on all inputs and achieves its baseline accuracy. As coverage decreases (more abstention), accuracy among predictions made typically increases because the model abstains on its most uncertain cases. The shape of this trade-off curve characterizes the model’s ability to identify reliable predictions.\nAbstention is appropriate when the cost of errors exceeds the cost of deferral. In clinical variant interpretation, a confident but incorrect pathogenic prediction may trigger unnecessary medical intervention, while abstention merely defers the decision to expert review. If expert review is available and affordable relative to error costs, abstaining on uncertain cases improves overall decision quality. Conversely, in high-throughput screening where expert review is infeasible, abstention may provide little benefit because all predictions eventually require automated handling.\n\n\n23.7.2 Selective Prediction Methods\nConfidence-based selection abstains when the model’s maximum predicted probability falls below a threshold. For a classifier producing probabilities over classes, if max_c *p̂_c* &lt; τ, the model abstains. This simple approach works well when model confidence correlates with correctness, but fails when models are confidently wrong.\nEnsemble-based selection abstains when ensemble members disagree beyond a threshold. High disagreement indicates epistemic uncertainty about the correct prediction, warranting abstention even if individual members express confidence. This approach captures uncertainty that confidence-based selection misses when models are overconfident.\nConformal selection abstains when prediction sets exceed a size threshold. If the conformal prediction set contains more than one class, the model lacks confidence to make a unique prediction. This approach connects selective prediction to the coverage guarantees of conformal methods: the model makes predictions with guaranteed coverage on the non-abstained cases.\nLearned selection trains a separate model to predict whether the primary model will be correct on each input. This “rejection model” learns to identify failure modes that simple confidence thresholds miss, potentially achieving better coverage-accuracy trade-offs than heuristic methods.\n\n\n23.7.3 Evaluating Selective Prediction\nRisk-coverage curves plot accuracy (or its complement, risk) as a function of coverage, revealing how performance improves as the model becomes more selective. The area under the risk-coverage curve summarizes overall selective prediction quality. Models with better uncertainty estimates produce steeper curves, achieving high accuracy at lower coverage.\nSelective accuracy at fixed coverage specifies a coverage level (e.g., 80%) and reports accuracy among predictions made at that coverage. This metric directly answers practical questions: “If we let the model predict on its 80% most confident cases, how accurate will it be?”\nComparison across methods requires matched coverage levels. A method that achieves 99% accuracy at 50% coverage and 95% accuracy at 90% coverage may be preferable to a method achieving 97% accuracy at both levels, depending on operational requirements. Reporting full risk-coverage curves enables stakeholders to select operating points appropriate to their cost structures.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-genomic-uq",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-genomic-uq",
    "title": "23  Uncertainty Quantification",
    "section": "23.8 Uncertainty for Specific Genomic Tasks",
    "text": "23.8 Uncertainty for Specific Genomic Tasks\nThe general principles of uncertainty quantification apply differently across genomic prediction tasks. Variant effect prediction, regulatory variant interpretation, and cross-population generalization each present distinct challenges for calibration, coverage, and out-of-distribution detection. The sources of uncertainty vary: coding variants benefit from stronger evolutionary constraint and clearer functional readouts, while regulatory variants operate through context-dependent mechanisms that introduce irreducible noise. Population-specific uncertainty reflects training data composition and has direct implications for equitable clinical deployment.\n\n23.8.1 Variant Effect Prediction Uncertainty\nVariant effect prediction concentrates the challenges of uncertainty quantification. Epistemic uncertainty arises from poorly characterized genes, novel protein folds, and under-represented populations in training data. Aleatoric uncertainty stems from incomplete penetrance, variable expressivity, and noise in functional assay labels. Both types of uncertainty must be estimated and communicated for variant predictions to inform clinical decisions appropriately. The technical details of variant effect prediction models are covered in Chapter 14.\nCalibration challenges for VEP include the evolving nature of ground truth labels. ClinVar annotations change as new evidence emerges; variants classified as VUS may be reclassified as pathogenic or benign, and even confident classifications occasionally reverse. A model calibrated against a historical version of ClinVar may appear miscalibrated against current annotations, not because the model changed but because the labels did. Periodic recalibration against current databases maintains alignment between model outputs and contemporary clinical understanding.\nPopulation-specific calibration addresses the reality that training data predominantly derive from European-ancestry cohorts. For patients from other ancestral backgrounds, both epistemic uncertainty (fewer training examples) and calibration (different baseline pathogenicity rates, different patterns of variation) may differ from the aggregate. Stratified reliability diagrams by ancestry reveal these differences; ancestry-conditional calibration may be necessary for equitable performance across populations. The governance and policy dimensions of ensuring equitable uncertainty communication are addressed in Chapter 29.\n\n\n23.8.2 Regulatory Variant Uncertainty\nRegulatory variants present distinct uncertainty challenges. Unlike coding variants where effects can be localized to specific amino acid changes, regulatory variants act through complex mechanisms involving transcription factor binding, chromatin accessibility, and three-dimensional genome organization. This mechanistic complexity translates to higher aleatoric uncertainty: even perfectly characterized regulatory variants may have context-dependent effects that vary across cell types, developmental stages, and genetic backgrounds. A variant that disrupts a transcription factor binding site may have dramatic effects in tissues where that factor is active and negligible effects elsewhere, yet the model must predict across all contexts simultaneously. The architecture and capabilities of regulatory prediction models are discussed in Chapter 13.\nThe context-dependence of regulatory effects creates a calibration challenge distinct from coding variants. A model may be well-calibrated for predicting expression changes in cell types abundant in training data (lymphoblastoid cell lines, common cancer lines) while poorly calibrated for clinically relevant primary tissues rarely profiled at scale. Stratified calibration assessment across tissue types reveals these disparities, but the sparsity of ground truth labels for many tissues limits the precision of tissue-specific calibration estimates.\nExpression prediction models like Enformer and Borzoi provide uncertainty estimates for predicted expression changes through several approaches. Ensemble methods quantify disagreement across model variants trained with different random seeds. Heteroscedastic architectures predict tissue-specific confidence alongside tissue-specific expression, learning that predictions for well-characterized tissues deserve higher confidence than those for rarely profiled contexts. These uncertainties propagate to downstream interpretations: a variant predicted to alter expression with high uncertainty warrants different treatment than one with narrow confidence bounds, and the tissue-specificity of uncertainty may itself be informative about which experimental follow-up would most reduce ambiguity.\n\n\n23.8.3 Uncertainty Across Populations\nDifferential uncertainty across populations has direct implications for health equity. Models trained predominantly on European-ancestry data exhibit higher epistemic uncertainty for other populations, manifesting in several observable ways: larger prediction sets from conformal methods, higher abstention rates from selective prediction, greater ensemble disagreement, and less reliable confidence estimates from calibration. These differences arise from multiple sources. Linkage disequilibrium patterns differ across populations, meaning that variant correlations learned from European data may not transfer. Population-specific variants absent from training data generate pure epistemic uncertainty. Even shared variants may have different effect sizes across populations due to gene-environment interactions or epistatic backgrounds that vary by ancestry.\nQuantifying population-specific uncertainty requires appropriate calibration and evaluation datasets. A model calibrated exclusively on European-ancestry ClinVar submissions may appear well-calibrated on aggregate metrics while being systematically miscalibrated for other populations. The scarcity of diverse calibration data creates a challenging circularity: we cannot assess population-specific calibration without diverse labeled datasets, yet diverse labeled datasets are precisely what current genomic databases lack. Initiatives like the All of Us Research Program and population-specific biobanks (Uganda Genome Resource, Taiwan Biobank, BioBank Japan) are beginning to address this gap, enabling population-stratified uncertainty assessment that was previously impossible. The broader context of biobank resources and their composition is discussed in Section 2.3.\nTransparent reporting of population-stratified uncertainty metrics enables informed decisions about model deployment. If a model abstains on 30% of variants in one population but only 10% in another, users can make informed choices about supplementary analyses for the higher-abstention population. Clinical laboratories might establish ancestry-specific thresholds for automated reporting versus expert review. Research applications might weight predictions by ancestry-specific confidence when aggregating across diverse cohorts. Ignoring these differences risks providing lower-quality predictions to already under-served populations while presenting a false appearance of uniform reliability, compounding existing disparities in genomic medicine.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-communication",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-communication",
    "title": "23  Uncertainty Quantification",
    "section": "23.9 Communicating Uncertainty to End Users",
    "text": "23.9 Communicating Uncertainty to End Users\nStatistical uncertainty estimates serve clinical decisions only when they reach end users in interpretable form. The translation from model outputs to actionable information involves choices about categorical versus continuous reporting, numerical versus visual presentation, and whether to frame results as probabilities or as expected outcomes under alternative decisions. Different stakeholders require different presentations: clinicians need actionable categories, researchers need distributional information, and patients need accessible risk communication.\n\n23.9.1 Communication Challenge\nA pathogenicity score of 0.73 ± 0.15 may be statistically accurate but nearly useless to a clinician deciding whether to order confirmatory testing. The gap between statistical uncertainty and decision-relevant communication presents a persistent challenge for genomic AI deployment. Different users reason differently about probability and risk; effective communication requires understanding these differences.\nCognitive biases complicate probability interpretation. Humans tend toward overconfidence in point estimates, treating 0.73 as more certain than warranted. Prediction intervals are frequently misunderstood: a 90% confidence interval does not mean the true value has a 90% chance of being in that specific interval (a Bayesian interpretation) but rather that 90% of such intervals would contain the true value (a frequentist interpretation). Base rate neglect leads users to interpret variant-level pathogenicity predictions without accounting for prior probability based on clinical presentation, family history, and phenotypic specificity.\nDifferent stakeholders have different needs. Clinicians require actionable categories that map to clinical decision points, not continuous scores requiring interpretation. Researchers may prefer full probability distributions enabling flexible downstream analysis. Patients and families need understandable risk communication that supports informed decision-making without inducing inappropriate anxiety or false reassurance.\n\n\n23.9.2 Categorical Reporting\nClinical genetics has established categorical frameworks for variant interpretation. The ACMG-AMP guidelines define five categories: pathogenic, likely pathogenic, variant of uncertain significance, likely benign, and benign. The complete ACMG-AMP framework, including how computational evidence integrates with other evidence types, is examined in Section 26.2. Mapping continuous model outputs to these categories requires threshold selection that balances sensitivity and specificity at clinically meaningful operating points, with guidance on calibrating model outputs to ACMG evidence strength provided in Section 14.5.3.\nUncertainty within categories can be conveyed through confidence qualifiers or numerical confidence scores attached to categorical calls. A “likely pathogenic” call with 95% confidence differs meaningfully from one with 60% confidence, even though both receive the same categorical label. Two-dimensional reporting combining category and confidence enables more nuanced interpretation without abandoning the categorical framework that clinicians expect.\nThreshold selection involves value judgments beyond pure statistics. The consequences of false positive and false negative pathogenic calls differ by clinical context. For a severe, treatable condition, false negatives carry higher cost, warranting lower thresholds for pathogenic classification. For untreatable conditions where pathogenic classification affects reproductive decisions, the calculus differs. Uncertainty quantification enables informed threshold selection by revealing the trade-offs at different operating points.\n\n\n23.9.3 Visual Communication\nProbability bars and confidence intervals provide visual representation of uncertainty, though their interpretation depends on user familiarity with statistical graphics. Icon arrays, which represent probabilities as proportions of colored icons in a grid (e.g., 73 red icons and 27 blue icons out of 100), improve comprehension for users without statistical training. The visual representation of proportion is more intuitive than numerical probability for many audiences.\nRisk ladders place the prediction in context by showing where it falls relative to other risks of varying magnitude. A variant with 0.73 probability of pathogenicity can be placed alongside risks from other genetic conditions, environmental exposures, or common medical procedures, enabling intuitive comparison.\nInteractive visualizations allow users to explore uncertainty in detail, examining how predictions change under different assumptions or how uncertainty varies across related variants. These approaches suit sophisticated users engaged in research or detailed clinical analysis but may overwhelm users seeking simple answers.\n\n\n23.9.4 Decision-Theoretic Framing\nRather than communicating probability alone, decision-theoretic framing presents expected outcomes under different actions. Instead of “this variant has 73% probability of being pathogenic,” the report might state “if we assume this variant is pathogenic and proceed with surveillance, the expected outcomes are X; if we assume it is benign and decline surveillance, the expected outcomes are Y.”\nThis framing integrates uncertainty with action, helping users understand how uncertainty affects what they should do rather than treating probability as an end in itself. The approach requires modeling clinical outcomes, which introduces additional assumptions, but makes explicit the decision-relevant implications of uncertainty rather than leaving users to integrate probability with consequences on their own.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-conclusion",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-conclusion",
    "title": "23  Uncertainty Quantification",
    "section": "23.10 Necessary but Insufficient",
    "text": "23.10 Necessary but Insufficient\nUncertainty quantification transforms foundation model outputs from opaque scores into components of rational decision processes. A well-calibrated pathogenicity prediction that honestly communicates its limitations enables appropriate clinical reasoning: high confidence warrants action, low confidence warrants additional testing or expert review. An overconfident score that claims false precision causes harm through both false positives (unnecessary interventions) and false negatives (missed diagnoses). Temperature scaling, conformal prediction, and out-of-distribution detection together provide the technical foundation for trustworthy genomic AI.\nThe path from uncertainty quantification to clinical impact requires integrating these methods into operational workflows. Selective prediction enables triage between automated handling and expert review based on model confidence. Conformal prediction sets provide coverage guarantees that support risk-aware decision-making. Out-of-distribution detection prevents confident predictions on inputs that fall outside the training distribution, a particularly important capability given the confounding issues examined in Chapter 22. Calibration ensures that numerical probabilities mean what they claim to mean. Together, these tools enable foundation models to participate in clinical decisions without overstating their reliability. Clinical risk prediction frameworks (Chapter 25) develop these tools further for deployment contexts, while rare disease workflows (Chapter 26) apply them to diagnostic interpretation.\nYet uncertainty quantification alone is insufficient. A perfectly calibrated black box remains a black box. The clinician who receives an uncertain prediction wants to understand why the model is uncertain: Is it because the variant falls in a poorly characterized gene? Because the model has never encountered this protein fold? Because the underlying biology is genuinely ambiguous? Interpretability, examined in Chapter 24, complements uncertainty by revealing the basis for predictions and their associated confidence. Attribution methods (Section 24.1) identify which input features drive predictions; probing classifiers (Section 24.4) reveal what information representations encode. The conjunction of calibrated uncertainty and mechanistic understanding approaches what trustworthy clinical AI requires. Neither alone suffices; together they provide the foundation for models that clinicians can reason with rather than merely defer to.\n\n\n\n\nFowler, Douglas M., and Stanley Fields. 2014. “Deep Mutational Scanning: A New Style of Protein Science.” Nature Methods 11 (8): 801–7. https://doi.org/10.1038/nmeth.3027.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In Proceedings of The 33rd International Conference on Machine Learning, 1050–59. PMLR. https://proceedings.mlr.press/v48/gal16.html.\n\n\nGuo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. “On Calibration of Modern Neural Networks.” In Proceedings of the 34th International Conference on Machine Learning, 1321–30. PMLR. https://proceedings.mlr.press/v70/guo17a.html.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nRubin, Alan F., Hannah Gelman, Nathan Lucas, Sandra M. Bajjalieh, Anthony T. Papenfuss, Terence P. Speed, and Douglas M. Fowler. 2017. “A Statistical Framework for Analyzing Deep Mutational Scanning Data.” Genome Biology 18 (1): 150. https://doi.org/10.1186/s13059-017-1272-5.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html",
    "href": "part_5/p5-ch24-interpretability.html",
    "title": "24  Interpretability",
    "section": "",
    "text": "24.1 Attribution Methods and Input Importance\nAn attribution method highlights a GATA motif when explaining why a model predicts enhancer activity. The explanation is biologically plausible: GATA transcription factors bind this motif and drive tissue-specific expression. But plausibility is not faithfulness. The model may have learned a completely different pattern (perhaps GC content correlating with enhancer labels in the training data) and the attribution method may be highlighting the GATA motif because human-interpretable explanations tend to find human-interpretable patterns. The explanation matches biological intuition without accurately reflecting model computation. This distinction between plausible and faithful interpretation structures the entire field of model interpretability, and failing to respect it produces explanations that provide false comfort rather than genuine insight.\nThe stakes extend beyond scientific curiosity. Variant interpretation guidelines from the American College of Medical Genetics require that computational evidence be weighed alongside functional assays, segregation data, and population frequency (see Chapter 26 for detailed discussion of the ACMG-AMP framework). A pathogenicity score alone satisfies only weak evidence criteria; knowing that a variant disrupts a specific CTCF binding site in a cardiac enhancer provides interpretable mechanistic evidence that can be combined with clinical presentation and family history. When models cannot explain their predictions faithfully, clinicians cannot integrate computational evidence with biological reasoning. The same limitation affects research: a model that predicts enhancer activity cannot generate testable hypotheses about regulatory grammar unless its internal computations can be translated into statements about motifs, spacing constraints, and combinatorial logic that can be experimentally validated.\nAttribution methods identify important input positions. Motif discovery algorithms translate attributions into regulatory vocabularies. Probing classifiers diagnose what representations encode. Mechanistic interpretability traces computational circuits within transformer architectures. Throughout, the plausible-versus-faithful distinction guides interpretation. We examine how to validate interpretability claims experimentally, distinguishing explanations that accurately reflect model computation from those that merely satisfy human intuition. Understanding when these diverge determines whether model explanations accelerate discovery or mislead researchers pursuing patterns the model never actually learned.\nWhen a model predicts that a 200-kilobase genomic region will show high chromatin accessibility in hepatocytes, a natural question arises: which bases within that region drive the prediction? Attribution methods answer this question by assigning importance scores to input positions, identifying where the model focuses its computational attention. These scores can reveal candidate regulatory elements, highlight the sequence features underlying variant effects, and provide the raw material for downstream motif discovery.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-attribution",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-attribution",
    "title": "24  Interpretability",
    "section": "",
    "text": "FIGURE PLACEHOLDER\n\n\n\n\nFigure 24.1: [Essential] Attribution methods on same input sequence. Methods: Gradient × Input (fast, noisy), Integrated Gradients (principled, slower), DeepLIFT (reference-based), Attention weights (inspect what model attends), In silico mutagenesis (exhaustive, expensive). Visualization: Heatmaps on same sequence; correlation between methods; areas of agreement/disagreement. Annotations: Compute cost, principled basis, what each reveals.\n\n\n\n\n24.1.1 In Silico Mutagenesis\nThe most direct approach to measuring input importance is simply to change each base and observe what happens to the prediction. In silico mutagenesis (ISM) systematically introduces mutations at every position, computing the difference between mutant and reference predictions. For a sequence of length L, ISM creates three mutant sequences at each position (substituting each non-reference nucleotide), yielding 3L forward passes through the model. The resulting mutation effect matrix captures how sensitive the prediction is to changes at each position and to each alternative base.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 24.2: [High] Four-panel figure. Panel A (ISM procedure): For each position, systematically mutate to all alternatives; compute prediction change; produces position \\(\\times\\) mutation matrix. Panel B (ISM profile): Heatmap aligned to sequence; functional regions show large effects; silent regions near zero. Panel C (Saturation mutagenesis comparison): ISM predictions vs experimental deep mutational scanning (DMS) data; correlation validation. Panel D (Mechanistic insights): ISM reveals: binding site boundaries, position-specific tolerance, allele-specific effects. Note: Computational cost \\(\\sim L \\times 4\\) forward passes.\n\n\n\nISM provides true counterfactual information rather than approximations. When ISM shows that mutating position 47 from A to G reduces the predicted accessibility by 0.3 log-fold, that is a direct observation about model behavior, not an estimate derived from gradients or attention weights. This directness makes ISM the gold standard for faithfulness: if ISM identifies a position as important, perturbing that position genuinely changes the output.\nThe limitation is computational cost. Scoring all single-nucleotide substitutions in a 200-kilobase input requires 600,000 forward passes, which becomes prohibitive for large models or genome-wide analysis. Practical applications often restrict ISM to targeted windows around variants of interest, using faster methods to identify candidate regions for detailed analysis. For variant effect prediction specifically, ISM reduces to comparing reference and alternative allele predictions, requiring only two forward passes per variant. This forms the computational basis for zero-shot variant scoring in foundation models (Section 14.1.1), where the difference between wild-type and mutant log-likelihoods directly measures predicted effect.\n\n\n24.1.2 Gradient-Based Attribution\nGradient-based methods approximate the counterfactual information from ISM using backpropagation. The gradient of the output with respect to each input position measures how much an infinitesimal change at that position would affect the prediction. With one-hot encoded sequence, the gradient at each base indicates the sensitivity to substituting that nucleotide.\nThe simplest approach, often called saliency mapping, computes raw gradients and visualizes their magnitudes across the sequence. A common variant multiplies gradients by inputs (gradient × input), focusing on positions where the current nucleotide is both important and present. These methods require only a single backward pass, making them orders of magnitude faster than ISM.\nGradient-based methods suffer from saturation in regions where the model is already confident. If a strong motif drives the prediction into a saturated region of the output nonlinearity, small perturbations produce near-zero gradients even though the motif is functionally critical. DeepLIFT addresses this limitation by comparing activations between an input sequence and a reference, propagating differences through the network using custom rules that avoid gradient saturation. The resulting attributions satisfy a completeness property: contributions sum to the difference between input and reference predictions (Shrikumar, Greenside, and Kundaje 2017).\nIntegrated gradients provide theoretical grounding through the path integral of gradients along a linear interpolation from reference to input (Sundararajan, Taly, and Yan 2017):\n\\[\\text{IG}_i(x) = (x_i - x'_i) \\int_{\\alpha=0}^{1} \\frac{\\partial f(x' + \\alpha(x - x'))}{\\partial x_i} \\, d\\alpha\\]\nThis integral, approximated by summing gradients at discrete interpolation steps, satisfies sensitivity (any input that affects the output receives nonzero attribution) and implementation invariance (functionally equivalent networks produce identical attributions). Integrated gradients have become a standard choice for genomic models, balancing computational efficiency with theoretical guarantees.\nAll gradient-based methods require choosing a reference sequence, which substantially affects the resulting attributions. Common choices include dinucleotide-shuffled versions of the input (preserving local composition while disrupting motifs), average non-functional sequence, or simply zeros. The reference defines what counts as informative: attributions highlight features that differ from the reference and contribute to the prediction difference. A shuffled reference emphasizes motif content; a zero reference treats any sequence information as potentially important.\n\n\n24.1.3 Reconciling Attribution Methods\nDifferent attribution methods can produce strikingly different importance maps for the same sequence and prediction. A position might show high importance under ISM but near-zero gradients due to saturation, or high gradient magnitude but minimal effect when actually mutated due to redundancy with nearby positions. This disagreement reflects genuine differences in what each method measures: gradients capture local sensitivity, ISM captures counterfactual effects, and DeepLIFT captures contribution relative to a reference.\nPractical workflows often combine multiple methods. Gradient-based approaches efficiently scan long sequences to identify candidate regions, ISM validates importance in targeted windows, and agreement across methods increases confidence that identified features genuinely drive predictions. Disagreement flags positions for closer investigation, potentially revealing saturation effects, redundancy, or artifacts in individual methods.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-cnn-filters",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-cnn-filters",
    "title": "24  Interpretability",
    "section": "24.2 Interpreting Convolutional Filters",
    "text": "24.2 Interpreting Convolutional Filters\nConvolutional neural networks remain central to genomic sequence modeling, as discussed in Chapter 6, and their first-layer filters offer a particularly tractable interpretability target. Each filter slides along the sequence computing dot products with local windows, and high activation indicates that the local sequence matches the filter’s learned pattern. This architecture creates a natural correspondence between filters and sequence motifs.\n\n24.2.1 From Filters to Position Weight Matrices\nConverting learned filters to interpretable motifs follows a standard workflow. The trained model processes a large sequence set, typically training data or genome-wide tiles, recording positions where each filter’s activation exceeds a threshold. The fixed-length windows around high-activation positions are extracted and aligned, and nucleotide frequencies at each position are computed to build a position weight matrix (PWM). This PWM can be visualized as a sequence logo and compared to databases like JASPAR or HOCOMOCO.\nWhen this procedure is applied to models trained on chromatin accessibility or transcription factor binding, first-layer filters frequently match known transcription factor motifs. DeepSEA filters include recognizable matches to CTCF, AP-1, and cell-type-specific factors [Citation Needed: Zhou & Troyanskaya, DeepSEA paper]. This correspondence validates that models discover biologically meaningful patterns rather than arbitrary correlations, and it provides a direct link between model weights and decades of experimental characterization of transcription factor binding preferences.\nSeveral complications affect filter interpretation. DNA is double-stranded, and models may learn forward and reverse-complement versions of the same motif as separate filters. Some filters capture general sequence composition (GC-rich regions, homopolymer runs) rather than specific binding sites. These patterns can be biologically meaningful in contexts like nucleosome positioning or purely artifactual depending on the training task. Distinguishing informative filters from compositional shortcuts requires cross-referencing with known biology and testing whether filter-derived motifs predict binding in held-out data.\n\n\n24.2.2 Deeper Layers and Combinatorial Patterns\nBeyond the first layer, convolutional filters combine lower-level patterns into complex representations. Deeper layers can encode motif pairs that co-occur at characteristic spacing, orientation preferences between binding sites, and contextual dependencies where a motif’s importance varies with surrounding sequence. These combinatorial patterns capture aspects of regulatory grammar that individual motifs cannot represent.\nDirect interpretation of deeper filters becomes increasingly difficult as receptive fields expand and nonlinearities accumulate. The activation of a layer-5 filter depends on intricate combinations of earlier patterns, resisting simple biological annotation. Indirect approaches prove more tractable: analyzing which input regions drive high activation at deeper layers, clustering high-activation sequences to find common themes, or probing whether deeper representations encode specific biological properties.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-motif-discovery",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-motif-discovery",
    "title": "24  Interpretability",
    "section": "24.3 Motif Discovery from Attributions",
    "text": "24.3 Motif Discovery from Attributions\nAttribution maps highlight important positions but do not directly reveal motifs. A DeepLIFT track might show scattered high-importance bases throughout a sequence without indicating that those bases collectively form instances of the same transcription factor binding site. TF-MoDISco (Transcription Factor Motif Discovery from Importance Scores) bridges this gap by discovering motifs from attribution scores rather than raw sequences (Shrikumar et al. 2018).\nThe insight underlying TF-MoDISco is that importance-weighted sequences focus motif discovery on positions the model actually uses. Traditional motif finders must contend with the fact that most positions in regulatory sequences do not participate in functional motifs. By extracting seqlets (short windows where total importance exceeds a threshold) and clustering them based on both sequence content and importance profiles, TF-MoDISco identifies patterns that drive model predictions.\nThe workflow proceeds through several stages. Base-level importance scores are computed for many sequences using DeepLIFT, ISM, or integrated gradients. Windows where total importance exceeds a threshold are extracted as seqlets, each representing a candidate motif instance. These seqlets are compared using metrics that consider both sequence content and importance profiles, then clustered into groups corresponding to putative motifs. Within each cluster, seqlets are aligned and consolidated into PWMs and importance-weighted logos. The resulting motifs can be matched to known transcription factors or flagged as novel patterns.\nBeyond individual motifs, TF-MoDISco enables grammar inference by analyzing motif co-occurrence. Mapping discovered motif instances back to genomic coordinates reveals characteristic spacing between motif pairs, orientation preferences, and cell-type-specific usage patterns. These grammatical rules can be validated through in silico experiments: inserting or removing motifs in synthetic sequences and checking whether predictions change as expected.\nApplications to models like BPNet trained on ChIP-seq data have recovered known transcription factor motifs, discovered novel sequence variants, and revealed spacing constraints validated through synthetic reporter assays [Citation Needed: BPNet paper]. The same workflow applies to foundation model analysis: use the model to produce base-level attributions for a downstream task, run TF-MoDISco to extract a task-specific motif vocabulary, and analyze how motif usage varies across conditions.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 24.3: [High] Motif discovery pipeline. Steps: (1) Attribution scores across many sequences; (2) Cluster high-attribution regions; (3) Align and aggregate into motifs; (4) Match to known databases (JASPAR). Examples: Discovered motif aligned to known TF (CTCF, GATA); novel motifs with unknown biology; composite motifs (TF combinations). Key insight: Bridge between black-box attributions and interpretable biology.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-probing",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-probing",
    "title": "24  Interpretability",
    "section": "24.4 Probing Learned Representations",
    "text": "24.4 Probing Learned Representations\nAttribution methods ask which input positions matter; probing asks what information the model’s internal representations encode. A probing classifier is a simple supervised model (typically linear) trained to predict some property of interest from the hidden representations of a pretrained model. If a linear probe can accurately predict a property, that property is encoded in an accessible form within the representation.\n\n24.4.1 Probing Methodology\nThe standard probing workflow extracts hidden states from a pretrained model for a set of inputs where the property of interest is known. These hidden states, without further transformation, serve as features for training a simple classifier to predict the property. The classifier’s accuracy indicates how well the representation encodes the probed property, while its simplicity (linearity, minimal parameters) ensures that the probe identifies information present in the representation rather than information the probe itself computes.\nFor protein language models like ESM-2, probing has revealed that representations encode secondary structure, solvent accessibility, contact maps, and even 3D coordinates to a surprising degree, as discussed in Chapter 12. These properties emerge despite training on sequence alone, demonstrating that masked language modeling on evolutionary sequences induces representations that capture structural information. For DNA language models (see Chapter 11), probing can assess whether representations encode chromatin state, gene boundaries, promoter versus enhancer identity, or species-specific regulatory signatures.\nProbing provides diagnostic information distinct from downstream task performance. A model might achieve high accuracy on a regulatory prediction task by learning shortcuts (correlations with GC content, distance to annotated genes) rather than encoding genuine regulatory grammar. Probing can detect such shortcuts: if representations strongly encode GC content but weakly encode transcription factor binding site presence, the model may be exploiting composition rather than sequence logic. This diagnostic function complements the confounder analysis discussed in Chapter 22.\n\n\n24.4.2 Limitations of Probing\nProbing results require careful interpretation. A probe’s failure to predict some property might indicate that the representation does not encode it, or might reflect limitations of the probe architecture, insufficient training data, or mismatch between the probe’s capacity and the complexity of the encoding. Linear probes may miss nonlinearly encoded information; more complex probes risk learning the property themselves rather than reading it from the representation.\nThe selectivity-accessibility tradeoff complicates interpretation. A representation might encode a property accessibly (recoverable by a linear probe) or selectively (encoded but requiring nonlinear decoding). Properties encoded selectively might be present but not easily extracted, while properties encoded accessibly might be incidentally correlated with the training objective rather than causally important. Combining probing with causal interventions (ablating representation components and measuring effects on downstream predictions) provides stronger evidence about which encoded properties actually matter.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-attention",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-attention",
    "title": "24  Interpretability",
    "section": "24.5 Attention Patterns in Transformer Models",
    "text": "24.5 Attention Patterns in Transformer Models\nTransformer-based genomic models use self-attention to aggregate information across long sequence contexts (see Chapter 7 for architectural details), potentially capturing distal regulatory interactions invisible to models with narrow receptive fields. Attention weights indicate which positions each position attends to, creating natural candidates for interpretability: perhaps high attention weights identify functionally related sequence elements.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 24.4: [High] Three-panel figure. Panel A (Attention heatmap): Position × position; patterns revealing (promoter-enhancer contacts, local structure). Panel B (Biological overlay): Attention on genome browser; peaks align with known regulatory elements. Panel C (Multi-head specialization): Different heads capturing different patterns; local context head, long-range head, motif-specific head. Caveats: Attention ≠ causation; sanity checks needed.\n\n\n\n\n24.5.1 What Attention Patterns Reveal\nWhen attention weights are analyzed in genomic language models, certain heads exhibit strikingly structured patterns. Some heads preferentially connect positions within the same predicted gene or operon, suggesting the model has learned gene boundaries from sequence alone. Other heads show long-range connections that align with known enhancer-promoter relationships or chromatin loop anchors. Still others cluster positions by functional annotation, connecting genes with similar Gene Ontology terms despite lacking explicit functional labels during training.\nIn models like Enformer that predict regulatory outputs from long genomic windows (see Section 13.2), attention can reveal which distal regions influence predictions at a target gene. Contribution scores aggregated across attention heads often peak at known enhancers, insulators, and chromatin domain boundaries. These patterns suggest that the model has learned aspects of regulatory architecture from the correlation between sequence and chromatin output labels.\n\n\n24.5.2 Why Attention Weights Mislead\nRaw attention weights require skeptical interpretation. High attention between two positions indicates information flow in the model’s computation but does not necessarily indicate causal influence on predictions. Attention serves multiple computational roles beyond identifying important features: routing information for intermediate computations, implementing positional reasoning, and satisfying architectural constraints. A position receiving high attention might be used for bookkeeping rather than contributing to the final output.\nSeveral specific issues undermine naive attention interpretation. Attention weights describe information movement before value vectors are applied; positions with high attention but small value vector magnitudes contribute little to the output. Multi-head attention averages across heads with different functions; examining average attention obscures specialized head behavior. Cross-layer effects mean that the importance of early-layer attention depends on what later layers do with the routed information.\nMore robust approaches combine attention analysis with perturbation experiments. If deleting a position that receives high attention changes the prediction substantially, the attention is functionally meaningful. If deletion has minimal effect, the attention may serve computational purposes unrelated to the target output. Attention rollout and attention flow methods propagate attention through layers to better capture information movement across the full network, though these too provide correlational rather than causal evidence.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-global",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-global",
    "title": "24  Interpretability",
    "section": "24.6 Regulatory Vocabularies and Global Interpretability",
    "text": "24.6 Regulatory Vocabularies and Global Interpretability\nLocal interpretability methods explain individual predictions; global interpretability characterizes what a model has learned across its entire training distribution. For genomic models trained to predict thousands of chromatin features, global interpretability asks whether the model has learned a coherent vocabulary of regulatory sequence classes and how those classes map to biological programs.\n\n24.6.1 Sequence Classes from Sei\nSei exemplifies the global interpretability approach by learning a vocabulary of regulatory sequence classes that summarize chromatin profile diversity across the genome (see Section 13.4 for architectural details). The model predicts tens of thousands of chromatin outputs (transcription factor binding, histone modifications, accessibility across cell types), then compresses this high-dimensional prediction space into approximately 40 sequence classes through dimensionality reduction and clustering.\nEach sequence class corresponds to a characteristic regulatory activity pattern. Some classes show promoter-like signatures (H3K4me3, TSS proximity, broad expression). Others exhibit enhancer patterns (H3K27ac, H3K4me1, cell-type-restricted activity). Repressive classes display H3K27me3 or H3K9me3 enrichment. Cell-type-specific classes capture lineage-restricted regulatory programs (neuronal, immune, hepatic). This vocabulary transforms thousands of raw chromatin predictions into a compact, interpretable representation.\nVariants can be characterized by their effects on sequence class scores, yielding functional descriptions more informative than raw pathogenicity predictions. A variant that shifts a region from enhancer-like to promoter-like class, or from active to repressive, provides mechanistic hypotheses about its functional consequences. Genome-wide association study (GWAS) enrichment analysis can identify which sequence classes are overrepresented among disease-associated variants, revealing the regulatory programs most relevant to specific phenotypes (see Chapter 3 for GWAS foundations).\n\n\n24.6.2 Embedding Geometry and Regulatory Programs\nBeyond discrete sequence classes, the continuous geometry of learned representations encodes regulatory relationships. Sequences with similar regulatory functions cluster in embedding space; directions in this space correspond to biological axes of variation. Dimensionality reduction techniques (UMAP, t-SNE, principal component analysis) visualize these relationships, revealing how the model organizes regulatory diversity.\nFor foundation models trained on diverse genomic tasks, embedding geometry can capture cross-task relationships. Sequences that function as enhancers in one cell type might cluster near sequences with enhancer function in related cell types, even if trained independently. Variants that disrupt shared regulatory logic should produce similar embedding perturbations. These geometric properties enable transfer of interpretability insights across tasks and provide compact summaries of model knowledge.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-mechanistic",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-mechanistic",
    "title": "24  Interpretability",
    "section": "24.7 Mechanistic Interpretability",
    "text": "24.7 Mechanistic Interpretability\nClassical interpretability methods treat models as input-output functions, probing what they compute without examining how they compute it. Mechanistic interpretability takes a different approach, attempting to reverse-engineer the algorithms implemented by neural network weights. This emerging field, most developed for language models, offers tools increasingly applicable to genomic foundation models.\n\n24.7.1 Circuits and Features\nThe central hypothesis of mechanistic interpretability is that neural networks implement interpretable computations through identifiable circuits: connected subnetworks that perform specific functions. A circuit might detect whether a motif is present, compute the distance between two motifs, or integrate evidence across regulatory elements. Identifying circuits requires tracing information flow through the network and characterizing what each component contributes.\nFeatures are the atomic units of this analysis: directions in activation space that correspond to interpretable concepts. In language models, features have been found that activate for specific topics, syntactic structures, or semantic properties. Analogous features in genomic models might activate for transcription factor binding sites, coding versus non-coding sequence, or regulatory element types. Sparse autoencoders trained on model activations can extract interpretable features by encouraging representations where most features are inactive for any given input.\nSuperposition complicates feature identification. Neural networks can represent more features than they have dimensions by using overlapping, nearly orthogonal directions. Features active for different inputs can share parameters, enabling high-capacity representations but complicating interpretation. Techniques from compressed sensing and dictionary learning help decompose superposed representations into constituent features.\n\n\n24.7.2 Applications to Genomic Models\nMechanistic interpretability remains nascent for genomic foundation models, but initial applications show promise. Attention head analysis in DNA language models has identified heads specialized for different genomic functions: some attend within genes, others across regulatory regions, still others implement positional computations [Citation Needed]. Probing activations at different layers reveals hierarchical feature construction, from local sequence patterns in early layers to long-range regulatory relationships in later layers.\nCircuit analysis can explain specific model behaviors. If a model predicts that a variant disrupts regulation, mechanistic analysis can trace which features activate differently for reference versus variant sequence, which attention heads route information about the variant to the prediction, and which intermediate computations change. This mechanistic trace provides far richer explanation than attribution scores alone, potentially identifying the regulatory logic the model has learned.\nThe challenge is scalability. Current mechanistic interpretability techniques require substantial manual analysis and work best for small models or specific behaviors. Foundation models with billions of parameters resist exhaustive circuit enumeration. Developing automated tools for circuit discovery and scaling mechanistic analysis to large genomic models represents an active research frontier.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-validation",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-validation",
    "title": "24  Interpretability",
    "section": "24.8 Validation: From Explanations to Experiments",
    "text": "24.8 Validation: From Explanations to Experiments\nInterpretability methods produce explanations, but explanations are only valuable if they accurately reflect model behavior and connect to biological reality. Validation closes the loop by testing whether interpretability-derived hypotheses hold when subjected to experimental scrutiny.\n\n24.8.1 Faithfulness Testing\nAn interpretation is faithful if it accurately describes what the model does. Testing faithfulness requires interventions: changing the features identified as important and verifying that predictions change accordingly. If an attribution method highlights certain positions as driving a prediction, deleting or scrambling those positions should reduce the prediction. If discovered motifs are claimed to be necessary for regulatory activity, removing them from sequences should impair predicted and measured function.\nSanity checks provide baseline validation. When model weights are randomized, attributions should degrade to uninformative noise. When training labels are scrambled, discovered motifs should disappear or lose predictive power. These checks identify methods that produce plausible-looking outputs regardless of model content, revealing explanations that reflect method biases rather than genuine model features.\nCounterfactual experiments go further by testing whether identified features are sufficient as well as necessary. Inserting discovered motifs into neutral sequences should increase predicted regulatory activity if the motifs genuinely encode functional elements. Constructing synthetic sequences that combine motifs according to discovered grammatical rules should produce predictions consistent with those rules. Discrepancies between expected and observed effects indicate gaps in the interpretation.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 24.5: [Essential] Two-path diagram. Scenario: Model predicts high enhancer activity. Path A (Plausible but unfaithful): Attribution highlights GATA motif (biologically reasonable); but model learned GC content correlate; validation fails (mutating GATA does not change prediction; inserting GATA does not increase); explanation matches intuition not computation. Path B (Faithful): Attribution highlights GATA; validation succeeds (mutating reduces prediction; inserting increases). Validation tests: Necessity (removing reduces?), Sufficiency (adding increases?), Sanity checks (random weights different?). Key distinction: Plausible matches intuition; faithful reflects computation; unfaithful provides false comfort.\n\n\n\n\n\n24.8.2 Experimental Validation\nThe ultimate test of interpretability connects model-derived hypotheses to biological experiments. Motifs discovered through TF-MoDISco can be tested through electrophoretic mobility shift assays, ChIP-qPCR, or reporter constructs. Predicted spacing constraints can be validated by varying distances between motifs in synthetic constructs and measuring activity. Hypothesized enhancer-promoter connections can be tested through CRISPR deletion of predicted enhancers and measurement of target gene expression.\nThis experimental validation distinguishes genuine mechanistic discovery from pattern matching that happens to produce plausible-looking results. A model might learn that certain k-mers correlate with regulatory activity for confounded reasons (batch effects, mappability artifacts) yet produce motif logos resembling real transcription factors. Only experimental testing can determine whether model-derived hypotheses reflect causal regulatory logic.\nHigh-throughput functional assays enable systematic validation at scale. Massively parallel reporter assays (MPRAs) can test thousands of model-predicted regulatory elements simultaneously. Perturb-seq combines CRISPR perturbations with single-cell RNA-seq to measure effects of knocking out predicted regulatory factors (see Section 16.3). These technologies create opportunities for iterative model improvement: interpretability generates hypotheses, experiments test them, and results refine both model architecture and training.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 24.6: [High] Circular workflow. Steps: (1) Model prediction; (2) Interpretability analysis (attribution, TF-MoDISco, attention patterns); (3) Hypothesis generation (“GATA motif drives activity”); (4) Experimental validation (EMSA for binding, reporter for activity, CRISPR for necessity, MPRA for systematic testing); (5) Model refinement (validated → improved training; failed → identify limitations; return to step 1). Example pathways: TF-MoDISco → EMSA ✓; attention enhancer → CRISPR ✓; GC attribution → MPRA no effect ✗ → confounder identified. Key insight: Interpretability advances biology only when closed with validation.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-clinical",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-clinical",
    "title": "24  Interpretability",
    "section": "24.9 Interpretability in Clinical Variant Assessment",
    "text": "24.9 Interpretability in Clinical Variant Assessment\nVariant interpretation guidelines require that computational predictions be weighed alongside experimental and clinical evidence, as discussed further in Chapter 26. Interpretability determines whether model predictions can contribute meaningful evidence beyond raw pathogenicity scores.\nCurrent ACMG-AMP criteria allow computational evidence as supporting (PP3) or opposing (BP4) pathogenicity, but the evidence strength depends on understanding what the prediction reflects (Richards et al. 2015). The full ACMG-AMP framework and its integration with computational evidence is examined in Section 26.2. A splice site disruption score from SpliceAI provides interpretable mechanistic evidence: the variant is predicted to alter splicing because it changes the consensus splice site sequence (Section 6.5) (Jaganathan et al. 2019). This prediction can be evaluated against splice site models, tested with minigene assays, and combined with observations of aberrant transcripts in patient samples. The interpretation enables evidence integration.\nFoundation model predictions are less immediately interpretable but potentially more informative. A pathogenicity score from ESM-1v (Section 12.1) reflects evolutionary constraint inferred from protein language modeling, but the specific sequence features driving the prediction require attribution analysis to identify. The protein VEP paradigm is examined in Section 14.2. An expression effect predicted by Enformer (Section 13.2) might result from disrupted transcription factor binding, altered chromatin accessibility, or changed 3D regulatory contacts; interpretability analysis distinguishes these mechanisms and guides experimental validation. The DNA-based VEP approaches are detailed in Section 14.3.\nFor clinical utility, interpretability must be communicated effectively. Genome browsers displaying attribution tracks alongside variant calls help clinicians identify mechanistic hypotheses. Reports that accompany pathogenicity scores with regulatory vocabulary classifications (this variant shifts an enhancer toward a repressive state) provide actionable context. These communication challenges extend interpretability beyond algorithm development to user interface design and clinical workflow integration.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 24.7: [Enhancing] Clinical workflow integration. ACMG evidence framework: PP3 (computational supports pathogenicity), BP4 (supports benign). Evidence strength depends on interpretability: Weak (score only, 0.85, no mechanism, limited ACMG weight); Moderate (score + attribution: disrupts splice site; SpliceAI supports; can evaluate against transcript data); Strong (score + validated mechanism: disrupts CTCF binding; ChIP confirms; 3D genome shows contact; minigene assay confirms). Clinical report elements: Annotation, score with uncertainty, mechanistic hypothesis, supporting/conflicting evidence, recommended follow-up.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-practical",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-practical",
    "title": "24  Interpretability",
    "section": "24.10 Practical Approaches for Foundation Model Analysis",
    "text": "24.10 Practical Approaches for Foundation Model Analysis\nWorking with genomic foundation models requires matching interpretability methods to specific questions. Several complementary strategies address different aspects of model behavior.\nFor understanding variant effects, the primary goal is explaining why a specific variant receives a particular prediction. Attribution methods (ISM for validation, integrated gradients for efficiency) identify which input positions drive the difference between reference and alternative predictions. If the variant falls within a discovered motif, the interpretation is straightforward. If attributions spread across the sequence, the effect may operate through long-range regulatory changes requiring attention analysis or contribution scores from models like Enformer.\nFor characterizing model representations, probing classifiers diagnose what information is encoded and at which layers. Probing for known regulatory features (promoter versus enhancer, tissue specificity, evolutionary conservation) establishes which biological properties the model captures. Probing for potential confounders (GC content, distance to annotated genes, technical artifacts) identifies shortcuts that might inflate benchmark performance without reflecting genuine regulatory understanding (see Section 20.8 for benchmark limitations and Section 22.8 for confounder detection methods).\nFor discovering regulatory logic, TF-MoDISco applied to high-confidence predictions extracts motif vocabularies specific to prediction tasks or cell types. Grammar analysis of motif co-occurrence reveals combinatorial rules. Sei-style sequence class analysis situates local motifs within global regulatory programs. Comparing discovered vocabularies across models or training conditions reveals shared versus idiosyncratic features.\nFor debugging and auditing, interpretability methods identify what features drive predictions in held-out distributions. If a model fails on a new cell type, attribution analysis can reveal whether it relies on cell-type-specific versus generalizable features. If performance degrades on specific genomic regions, local interpretability can identify confounding patterns or training data gaps.\nFor generating experimental hypotheses, interpretability produces testable predictions. Discovered motifs can be synthesized and tested. Predicted regulatory elements can be perturbed. Hypothesized transcription factor binding can be validated by ChIP. Model-derived predictions that survive experimental testing represent genuine mechanistic insights; predictions that fail point toward model limitations or confounding.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-conclusion",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-conclusion",
    "title": "24  Interpretability",
    "section": "24.11 Plausibility Is Not Faithfulness",
    "text": "24.11 Plausibility Is Not Faithfulness\nThe distinction between plausibility and faithfulness remains central to interpretability for genomic foundation models. Models can produce compelling motifs, structured attention patterns, and interpretable probing results while operating through mechanisms that do not correspond to biological reality. A model that correctly predicts splice site strength may do so by recognizing confounded sequence features rather than learning splice site grammar. A model that attributes importance to a transcription factor binding site may be exploiting correlation with GC content rather than modeling regulatory mechanism. Plausible explanations that match biological intuition are not the same as faithful explanations that accurately reflect model computation.\nOnly interventional experiments can distinguish genuine regulatory insight from sophisticated pattern matching. Computational interventions (deletion tests, counterfactual sequence generation, circuit analysis) probe whether identified features are necessary and sufficient for model predictions. Biological interventions (reporter assays, CRISPR perturbations, massively parallel experiments) test whether model-derived hypotheses hold in living systems. The sequence design applications in Chapter 28 operationalize this validation loop, using interpretability-derived hypotheses to guide experimental libraries. The conjunction of computational and experimental validation transforms interpretability from rationalization into discovery, generating testable hypotheses that advance biological understanding rather than merely explaining model behavior.\nAs foundation models grow in scale and capability, interpretability becomes simultaneously more important and more challenging. Larger models implement more complex computations, potentially capturing subtler regulatory logic but resisting simple interpretation. Mechanistic interpretability offers a path forward by characterizing model internals directly, though scaling these techniques to billion-parameter genomic models remains an open problem. The evaluation challenges this creates are examined in Section 21.9, while the confounding risks of scale are addressed in Chapter 22. The integration of interpretability with model development points toward a future where understanding and prediction advance together: motifs discovered through interpretation inform architecture design, experimentally validated hypotheses become supervision signals, and interpretability failures that reveal confounding drive improvements in training data and evaluation. In this vision, interpretability is not merely a tool for explaining existing models but a methodology for building models whose predictions we trust because we understand the mechanisms they have learned.\n\n\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie Gastier-Foster, Wayne W. Grody, et al. 2015. “Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology.” Genetics in Medicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nShrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. 2017. “Learning Important Features Through Propagating Activation Differences.” In Proceedings of the 34th International Conference on Machine Learning, 3145–53. PMLR. https://proceedings.mlr.press/v70/shrikumar17a.html.\n\n\nShrikumar, Avanti, Katherine Tian, Žiga Avsec, Anna Shcherbina, Abhimanyu Banerjee, Mahfuza Sharmin, Surag Nair, and Anshul Kundaje. 2018. “Technical Note on Transcription Factor Motif Discovery from Importance Scores (TF-MoDISco) Version 0.5.6.5.” arXiv. https://doi.org/10.48550/arXiv.1811.00416.\n\n\nSundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. “Axiomatic Attribution for Deep Networks.” In Proceedings of the 34th International Conference on Machine Learning, 3319–28. PMLR. https://proceedings.mlr.press/v70/sundararajan17a.html.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_6/p6--translation.html",
    "href": "part_6/p6--translation.html",
    "title": "Part VI: Clinical Translation",
    "section": "",
    "text": "The question shifts from how these models work to how they are used, and from what they can predict to what they enable us to do. This transition is not merely practical but conceptual: deploying a model in a clinical or industrial setting exposes assumptions that benchmarks leave implicit and reveals failure modes that curated evaluations obscure. A model achieving impressive metrics on held-out test sets may falter when deployed on populations underrepresented in training data, when integrated into workflows designed around different assumptions, or when its outputs must be communicated to clinicians and patients who lack the technical background to interpret confidence intervals. The gap between benchmark performance and real-world utility represents one of the most consequential challenges in genomic AI.\nDeployment transforms the requirements for genomic foundation models. Clinical risk prediction, rare disease diagnosis, drug discovery, and biological design each impose constraints absent from research settings: calibration requirements become stricter, fairness considerations become urgent, interpretability demands become concrete, and the consequences of failure become measured in patient outcomes rather than leaderboard rankings.\nClinical risk prediction (25  Clinical Risk Prediction) combines foundation model features with electronic health records to stratify patients for disease, progression, and treatment response. Variant interpretation in rare disease (26  Rare Disease Diagnosis) enters diagnostic pipelines alongside clinical geneticists and laboratory scientists. Drug discovery applications (27  Drug Discovery) contribute to target identification, genetic validation, and biomarker development. Reversing the direction of inference from prediction to generation (28  Sequence Design), foundation models guide protein engineering, regulatory element design, and programmable biology. Regulatory, ethical, and frontier challenges (29  Ethics and Frontiers) will shape how genomic AI moves from research to practice. The goal is not definitive protocols for each domain but a framework for reasoning about deployment: what questions to ask, what pitfalls to anticipate, and what principles should guide responsible development.",
    "crumbs": [
      "Part VI: Clinical Translation"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html",
    "href": "part_6/p6-ch25-clinical-risk.html",
    "title": "25  Clinical Risk Prediction",
    "section": "",
    "text": "25.1 From Polygenic Scores to Foundation Model Features\nA risk prediction has clinical value only if it changes what happens next. A cardiologist who receives a polygenic risk score for coronary artery disease faces a simple question: does this information alter the treatment recommendation? If a patient with a high score receives the same statin prescription, lifestyle counseling, and follow-up schedule as a patient without genetic testing, the score added nothing to care regardless of its statistical validity. The fundamental challenge is not generating genomic predictions but translating them into actions that improve outcomes. This translation requires more than discrimination between who will and will not develop disease; it requires that the prediction reach clinicians in a usable form, at a decision point where alternatives exist, for a patient population where the prediction performs equitably.\nTraditional polygenic scores, despite their scientific validity, often fail this translation test. They reduce entire genomes to single numbers that provide little mechanistic insight. They transfer poorly across ancestries because training data overrepresent European populations. They exist outside the electronic health records where clinical decisions actually happen, requiring manual lookup that busy clinicians rarely perform. Most fundamentally, the clinical actions available in response to a polygenic risk score (PRS), such as lifestyle modification, earlier screening, or preventive medication, are often the same actions recommended for patients with conventional risk factors, leaving unclear what the genetic information specifically enables.\nGenomic foundation models offer capabilities that may address some of these limitations. Rather than collapsing genetic information into scalar risk scores, foundation models produce embeddings that capture sequence context, regulatory grammar, and functional consequences. These representations can integrate with clinical data through fusion architectures (Chapter 19), adapt to diverse prediction tasks through transfer learning (Chapter 9), and provide feature attributions that connect predictions to biological mechanisms (Chapter 24). Whether these capabilities translate into tools that change practice remains the open question. The path from these capabilities to tools that change practice runs through electronic health record integration, evidence standards for clinical deployment, fairness considerations that determine whether genomic AI reduces or amplifies health disparities, and the practical realities of care delivery.\nThe limitations of classical polygenic risk scores define the opportunity for foundation model approaches. As discussed in Section 3.5, polygenic scores aggregate the effects of common variants into weighted sums, with weights derived from genome-wide association study effect sizes. This framework has demonstrated that common genetic variation contributes substantially to risk for conditions including coronary artery disease, type 2 diabetes, and breast cancer. A patient in the top percentile of polygenic risk for coronary disease faces roughly threefold higher lifetime risk than one in the bottom percentile, a gradient comparable to traditional risk factors like smoking or hyperlipidemia [Citation Needed].\nSeveral limitations constrain the clinical impact of this approach. The linear additive model cannot capture epistatic interactions where one variant’s effect depends on others, nor can it represent the nonlinear relationships between genetic variation and disease that emerge from regulatory networks and cellular pathways. Polygenic scores derived from European-ancestry genome-wide association studies substantially underperform in other populations, with effect sizes often attenuating by half or more in African or East Asian ancestries due to differences in linkage disequilibrium structure and allele frequencies (Section 22.2.1; Section 3.7). Beyond these technical constraints, a single scalar provides no mechanistic insight: a high polygenic score for diabetes does not indicate whether risk stems from impaired insulin secretion, insulin resistance, or altered satiety signaling, information that might guide intervention selection.\nFoundation models address these limitations through richer representations. Instead of treating variants as independent weighted features, models like Delphi and G2PT learn genome-wide embeddings that encode sequence context, regulatory annotations, and cross-variant interactions (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025). These approaches can capture nonlinear structure in genetic risk, leverage functional priors that transfer across ancestries, and provide attention-based attributions that highlight which genomic regions contribute most to predictions. Fine-mapping models like MIFM estimate posterior probabilities for causal variants within association loci, allowing risk models to weight variants by evidence for causality rather than treating all correlated variants equally (Rakowski and Lippert 2025).\nThe practical architecture of a foundation model-enabled risk system typically involves three components: pretrained encoders that transform genomic data into embeddings, aggregation modules that summarize variant-level or region-level representations into patient-level features, and prediction heads that map these features (combined with clinical covariates) to risk estimates. This modular design separates the computationally expensive foundation model inference from the task-specific prediction layer, enabling updates to either component while maintaining clear interfaces for validation.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-defining-risk",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-defining-risk",
    "title": "25  Clinical Risk Prediction",
    "section": "25.2 Defining Clinical Risk Prediction",
    "text": "25.2 Defining Clinical Risk Prediction\nA risk prediction model is only as useful as the decision it informs. Effective clinical risk prediction requires precise specification of four elements: the outcome being predicted, the time horizon over which prediction applies, the target population for whom the model is intended, and the clinical action the prediction will trigger.\nConsider a 55-year-old woman with moderately elevated cholesterol and a family history of early coronary disease. Her cardiologist must decide whether to initiate statin therapy, a decision traditionally guided by 10-year cardiovascular risk estimates from tools like the Pooled Cohort Equations. A genomic foundation model could augment this decision in several ways. It might refine her absolute risk estimate by incorporating polygenic information that the traditional calculator ignores. It might identify whether her genetic risk concentrates in pathways amenable to specific interventions (LDL metabolism favoring statins versus inflammatory pathways suggesting alternative approaches). It might flag pharmacogenomic variants affecting statin metabolism that influence dose selection or drug choice.\nEach of these applications represents a different prediction task with distinct requirements. The 10-year risk estimate for major adverse cardiovascular events is an individual-level incident risk problem where discrimination and calibration matter most. The pathway-level attribution is an interpretability challenge requiring mechanistic grounding. The pharmacogenomic prediction is a treatment selection problem where the relevant outcome is adverse drug reaction risk conditional on therapy initiation.\nClinical risk prediction tasks cluster into several archetypes. Incident risk concerns whether a currently disease-free individual will develop disease within a specified window, such as 10-year diabetes risk for prediabetic patients. Progression risk asks which patients with existing disease will develop complications, for instance nephropathy in diabetes or heart failure after myocardial infarction. Survival and prognosis involve time-from-diagnosis to events like death, recurrence, or transplant, often requiring survival models that handle censoring and competing risks. Treatment response and toxicity concerns whether a patient will benefit from one therapy versus another and their probability of experiencing serious adverse effects.\nFoundation models enter these problems as feature generators. They transform raw sequence data into structured representations that downstream prediction models combine with clinical covariates. The architectural choices for this combination, and the evidence required to trust the resulting predictions, constitute the core methodological challenges of clinical translation.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-feature-integration",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-feature-integration",
    "title": "25  Clinical Risk Prediction",
    "section": "25.3 Feature Integration Architectures",
    "text": "25.3 Feature Integration Architectures\nThe features available for clinical risk models draw on multiple foundation model families, each capturing different aspects of genetic and molecular risk.\nDNA-level foundation models provide variant effect predictions without requiring trait-specific training. Systems like Nucleotide Transformer, HyenaDNA, and GPN compute sequence-based deleteriousness scores that reflect how mutations disrupt regulatory grammar, splice sites, or protein-coding sequences (Dalla-Torre et al. 2023; Nguyen et al. 2023; Benegas, Batra, and Song 2023). These zero-shot predictions transfer across traits and ancestries because they derive from sequence properties rather than population-specific association statistics (Chapter 11). Fine-mapping models integrate these functional priors with association evidence to estimate which variants within a locus are likely causal, providing principled weights for aggregation (Section 3.4). Fine-mapping models like MIFM integrate such functional priors with association evidence to estimate which variants within a locus are likely causal, providing principled weights for aggregation (Rakowski and Lippert 2025).\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 25.1: [Essential] Three-column comparison. Column 1 (Early Fusion): All features concatenated → single model → risk. Column 2 (Intermediate Fusion): Separate encoders (FM as genomic encoder) → embeddings → fusion → risk. Column 3 (Late Fusion): Independent models → scores → ensemble → risk. For each: pros/cons, missing data handling, compute. Comparison table: Cross-modal interactions, missing data, modularity, best for. Key insight: Intermediate balances modularity with interaction learning.\n\n\n\nProtein language models add coding variant interpretation. AlphaMissense and related systems predict pathogenicity for missense mutations based on evolutionary conservation patterns learned from millions of protein sequences, as discussed in Chapter 12. For conditions with strong coding variant contributions (Mendelian cardiomyopathies, cancer predisposition syndromes), these predictions provide crucial signal beyond what noncoding regulatory models capture.\nMulti-omics foundation models extend beyond germline sequence. Cell-type-resolved representations from GLUE, scGLUE, and CpGPT capture regulatory state across chromatin accessibility, methylation, and expression (Chapter 16) (Cao and Gao 2022; Camillo et al. 2024). Rare variant burden scores from DeepRVAT aggregate predicted effects across genes into pathway-level impairment measures (Clarke et al. 2024). For oncology applications, tumor embedding models like SetQuence and graph neural network-based subtypers encode complex somatic mutation landscapes into patient-level representations (Jurenaite et al. 2024; Li et al. 2022).\nElectronic health record features provide the clinical context without which genomic predictions lack meaning. Demographics, vital signs, laboratory values, medication lists, problem codes, and procedure histories characterize the patient’s current state and trajectory. Time-varying biomarker trajectories (estimated glomerular filtration rate trends, hemoglobin A1c patterns, tumor marker dynamics) capture disease evolution that static snapshots miss.\nThe architectural question is how to combine these heterogeneous inputs. Three fusion strategies offer different tradeoffs.\nEarly fusion concatenates all features into a single input vector and trains a unified model (neural network, gradient boosting, survival regression) on the combined representation. This approach allows the model to learn arbitrary interactions between genomic and clinical features but requires all inputs to be present for every patient, handles scale differences between modalities poorly, and can be dominated by whichever input provides the most features or strongest signal.\nIntermediate fusion trains separate encoders for each modality, producing genomic embeddings, clinical embeddings, and multi-omic embeddings that a fusion module then combines. The fusion module might use attention mechanisms to weight modality contributions dynamically, cross-modal transformers that allow features from one modality to attend to features from another, or simpler concatenation with learned combination weights. This approach offers modularity (foundation model encoders can be swapped as new versions become available) while still enabling learned cross-modal interactions.\nLate fusion trains independent models for each modality and combines their predictions through ensemble methods or meta-learning. A polygenic score model, an electronic health record model, and a multi-omic model each produce risk estimates that a final layer integrates. This approach handles missing modalities gracefully (each submodel operates independently) and allows modality-specific architectures but may underutilize cross-modal structure since interactions can only be captured at the final combination stage.\nFor clinical deployment, intermediate fusion often provides the best balance. It enables modular updates as foundation models improve, allows graceful degradation when modalities are missing, and captures cross-modal interactions that late fusion misses. The specific fusion mechanism (attention, concatenation, cross-modal transformer) matters less than ensuring the architecture supports the operational requirements of clinical deployment: batch computation, uncertainty quantification, and interpretable feature attribution.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-ehr-integration",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-ehr-integration",
    "title": "25  Clinical Risk Prediction",
    "section": "25.4 EHR Integration and Phenotype Embeddings",
    "text": "25.4 EHR Integration and Phenotype Embeddings\nPolygenic risk scores condense genetic information into scalar predictions, but clinical decision-making occurs in the context of rich electronic health records that capture diagnoses, procedures, medications, laboratory values, and clinical narratives. A PRS for coronary artery disease exists as an isolated number until integrated with a patient’s history of hypertension, diabetes, smoking, and lipid measurements. The question is not merely whether to combine genetic and clinical information, but how to do so in ways that improve prediction, maintain interpretability, and avoid introducing new sources of bias.\nTraditional approaches treat EHR data as additional covariates in regression models that already include the PRS. Age, sex, smoking status, and existing diagnoses enter as predictors alongside the genetic score, with effect sizes learned from training data. This additive framework has clear interpretation but limited capacity: it assumes that genetic risk and clinical risk contribute independently, missing interactions where genetic predisposition matters more or less depending on clinical context. A patient with elevated LDL cholesterol and high coronary disease PRS may face multiplicative risk that additive models underestimate.\n\n25.4.1 EEPRS Framework\nThe EHR-embedding-enhanced PRS (EEPRS) framework addresses these limitations by integrating phenotype embeddings derived from EHR data with GWAS summary statistics to construct improved polygenic scores (Xu et al. 2025). Rather than using expert-defined phenotype covariates, EEPRS learns vector representations of clinical phenotypes from their patterns of co-occurrence in patient records. These embeddings capture relationships among diseases, symptoms, and risk factors that expert definitions may miss.\nThe framework proceeds in stages. Embedding models (Word2Vec trained on ICD-10 code sequences, or GPT-based embeddings of code descriptions) transform each patient’s diagnostic history into a low-dimensional vector representation. GWAS conducted on these embedding dimensions identify genetic variants associated with each dimension of clinical phenotype space. The resulting summary statistics enable construction of embedding-based polygenic scores that capture genetic predisposition to the phenotypic patterns encoded in each dimension. Integration with traditional disease-specific PGS through weighted combination yields final risk predictions.\nValidation in UK Biobank demonstrated consistent improvement over single-trait polygenic scores across 41 clinical traits. Cardiovascular conditions showed the largest gains: ischemic stroke improved by 66%, heart failure by 32%, and peripheral artery disease by 25% [Citation Needed]. These improvements concentrate in traits where related phenotypes share genetic architecture, allowing the embedding-based scores to leverage cross-phenotype genetic correlation. For isolated traits without strong embedding-dimension associations, improvements were modest or absent.\n\n\n25.4.2 Understanding When Embeddings Help\nThe pattern of improvement across traits reveals when EHR embeddings add value to polygenic prediction. Conditions that cluster together in clinical space, co-occurring in patients and sharing risk factors, benefit most. The cardiovascular cluster (coronary artery disease, ischemic stroke, peripheral artery disease, heart failure, angina, type 2 diabetes) forms a coherent group in both clinical practice and genetic architecture. Embeddings trained on EHR data capture this clustering, and GWAS on embedding dimensions identify variants associated with the shared liability across the cluster. These variants provide additional prediction signal beyond what single-trait GWAS can detect.\nConversely, conditions with distinct genetic architectures that do not cluster with other phenotypes show minimal improvement. Breast cancer and coronary artery disease, despite both being common conditions well-represented in biobanks, did not benefit from embedding integration in external validation. Their genetic architectures are largely distinct; embedding-based scores derived from cardiovascular-weighted dimensions provide no additional signal for cancer prediction.\nThis selectivity has important implications for clinical deployment. EEPRS offers greatest value for conditions where conventional polygenic scores remain underpowered despite adequate GWAS sample sizes. Heart failure, peripheral artery disease, and asthma showed substantial improvements precisely because their polygenic scores have historically underperformed relative to heritability estimates. Embedding integration effectively borrows strength across genetically correlated phenotypes, amplifying signal that single-trait analyses struggle to detect.\n\n\n25.4.3 PRS-PheWAS for Clinical Interpretation\nClinical deployment requires interpretability: why does this score predict disease risk, and what biological mechanisms does it capture? PRS-based phenome-wide association studies provide one answer by systematically testing association between the polygenic score and hundreds of clinical phenotypes (Section 3.8). For embedding-enhanced scores, PRS-PheWAS reveals which clinical manifestations the genetic risk predicts.\nThe EEPRS framework’s cardiovascular improvements became interpretable through PRS-PheWAS analysis. Embedding-based scores derived from ICA-transformed dimensions showed strong associations (adjusted \\(p &lt; 10^{-20}\\)) with hypertension, atrial fibrillation, and cardiac dysrhythmias [Citation Needed]. These associations explain the improvement: the embeddings capture genetic variation that influences multiple cardiovascular endpoints, and aggregating across these endpoints provides stronger risk stratification than targeting any single outcome.\nPRS-PheWAS also reveals unexpected associations that warrant clinical attention. Different embedding methods capture different aspects of phenotypic structure, with GPT-based embeddings uniquely identifying associations with infectious diseases and mental disorders that Word2Vec embeddings missed. These method-specific patterns may reflect differences in what the embedding approaches learn from clinical data, or they may indicate opportunities for method combination that leverages complementary signals.\n\n\n25.4.4 Implementation Considerations\nTranslating EEPRS from research demonstration to clinical deployment requires addressing several practical challenges. The embedding models must be trained on EHR data representative of the deployment population; embeddings learned from UK Biobank may not transfer to health systems with different patient populations, coding practices, or documentation patterns. The integration weights that combine embedding-based and single-trait scores require calibration in the target population, not just the discovery cohort.\nComputational requirements are modest once embeddings are pretrained. Scoring new patients requires computing their embedding from available ICD codes (a lookup operation), then calculating weighted sums across precomputed variant weights. The workflow integrates with existing PGS calculation pipelines, adding embedding score computation and integration as additional steps. Summary statistics for embedding-based GWAS can be distributed like conventional GWAS results, enabling score construction without sharing individual-level data.\nThe deeper challenge is population representativeness. EHR-based embeddings inherit the documentation patterns, coding practices, and healthcare access disparities of the health systems where they were trained. An embedding that positions diabetes near cardiovascular disease reflects the co-occurrence pattern in patients who access both cardiology and endocrinology care; patients who lack access to specialty care may show different patterns. Multi-ancestry validation revealed that EEPRS improvements varied across populations, with gains concentrated in conditions where the underlying genetic correlation structure held across ancestries.\n\n\n25.4.5 Integration with Foundation Model Features\nThe EEPRS framework operates on GWAS summary statistics and phenotype embeddings, both derived from classical statistical approaches. Foundation models offer an alternative integration strategy where learned sequence representations replace or augment summary statistics. Rather than weighting variants by GWAS effect sizes, foundation model approaches can score variants by their predicted functional impact, regulatory consequence, or embedding similarity to known pathogenic variants (Chapter 14).\nAttention-based integration, graph neural networks for pathway aggregation (Chapter 18), and transformer encoders for sequence context can all incorporate EHR embeddings as additional input features. A patient’s clinical embedding provides context that may modify interpretation of their genetic variants: a variant of uncertain significance (VUS) in a cardiovascular gene carries different implications for a patient whose clinical embedding places them in the cardiovascular risk cluster versus one with an unremarkable clinical profile. This contextualization moves beyond additive combination toward models that learn interactions between genetic and clinical risk.\nThe combination of phenotype embeddings and foundation model features remains largely unexplored. EEPRS demonstrated that phenotype embeddings capture heritable variation beyond single-trait GWAS; foundation models demonstrate that sequence context improves variant effect prediction beyond simple annotations (Section 14.7). Whether these approaches provide complementary signal, and whether their combination improves clinical prediction beyond either alone, represents an open research question with substantial clinical implications.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-temporal-modeling",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-temporal-modeling",
    "title": "25  Clinical Risk Prediction",
    "section": "25.5 Temporal Modeling Architectures",
    "text": "25.5 Temporal Modeling Architectures\nClinical risk prediction spans diverse temporal structures, and the choice of modeling framework must match the prediction task. A screening tool estimating whether a patient will develop diabetes within ten years faces different statistical challenges than a monitoring system tracking whether a patient’s kidney function trajectory signals imminent decline. Foundation model features can integrate into each framework, but the integration patterns differ.\nSurvival models address time-to-event outcomes where patients are followed until an event occurs or observation ends. The Cox proportional hazards model remains the workhorse of clinical risk prediction, estimating hazard ratios for features while making minimal assumptions about baseline hazard shape. Foundation model embeddings enter as covariates alongside clinical variables, with the proportional hazards assumption requiring that genomic risk effects remain constant over time. When this assumption fails (as when genetic effects on cancer recurrence differ between early and late periods), stratified or time-varying coefficient extensions accommodate the violation.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 25.2: [High] Three-panel comparison. Panel A (Survival Models): Timeline with events (X) or censoring (O); Cox PH diagram; FM embeddings as static covariates; “10-year CV risk at baseline.” Panel B (Longitudinal Models): Repeated measurements over time; time-varying features; FM re-encodes at each timepoint; “Updated prognosis as disease evolves.” Panel C (Hybrid/Joint): Baseline genomic risk + trajectory updating; static genomic embedding + dynamic clinical transformer; “Genetic predisposition modified by treatment response.”\n\n\n\nDeep survival models extend this framework through neural network architectures that learn nonlinear feature interactions. DeepSurv replaces the linear Cox predictor with a multilayer network while preserving the partial likelihood objective (Katzman et al. 2018). Deep Survival Machines model the survival distribution as a mixture of parametric components, enabling richer distributional assumptions than the semiparametric Cox approach (Nagpal, Li, and Dubrawski 2021). These architectures naturally accommodate the high-dimensional embeddings that foundation models produce, though the risk of overfitting increases and careful regularization becomes essential.\nLongitudinal models address a different challenge: patients observed repeatedly over time, with measurements that evolve and interact. A patient’s hemoglobin A1c trajectory over five years contains information that a single baseline measurement cannot capture. Whether values are stable, rising, or fluctuating conveys prognostic signal beyond their current level. Joint longitudinal-survival models connect these repeated measurements to event outcomes, modeling how biomarker trajectories associate with hazard while accounting for informative dropout when sicker patients are measured more frequently or die before later observations.\nFoundation model features integrate into longitudinal frameworks at multiple levels. Static genomic embeddings (computed once from germline sequence) serve as time-invariant covariates influencing both trajectory shape and event hazard. Time-varying molecular features (expression profiles, methylation states, circulating tumor DNA levels) can be encoded through foundation models at each measurement occasion, producing sequences of embeddings that recurrent or attention-based architectures process into trajectory representations. The computational cost of re-encoding molecular data at each timepoint is substantial, making efficient inference strategies essential for deployment.\nTransformer architectures designed for irregularly sampled time series offer a natural framework for clinical trajectories. Models like STraTS and similar clinical transformers handle the variable timing and missing measurements characteristic of real-world healthcare data (Tipirneni and Reddy 2022). Position encodings based on actual timestamps rather than sequence position accommodate irregular sampling. Attention mechanisms identify which historical measurements most inform current predictions. Foundation model embeddings at each timepoint provide richer input representations than raw laboratory values alone.\nThe choice between survival and longitudinal frameworks depends on the clinical question and available data. When the goal is baseline risk stratification (identifying high-risk patients at a single decision point), survival models with static genomic features often suffice. When the goal is dynamic monitoring (detecting deterioration as it develops), longitudinal models that update predictions as new measurements arrive become necessary. Hybrid approaches that initialize with genomic risk and update based on clinical trajectory combine the strengths of both paradigms.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-evaluation",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-evaluation",
    "title": "25  Clinical Risk Prediction",
    "section": "25.6 Evaluation for Clinical Deployment",
    "text": "25.6 Evaluation for Clinical Deployment\nHigh performance on held-out test sets is necessary but far from sufficient for clinical deployment. Risk models must satisfy multiple evidence standards that typical machine learning papers do not address, and teams planning translation must understand these requirements from the outset rather than discovering them after development is complete.\n\n25.6.1 Discrimination\nDiscrimination measures how well a model ranks patients by risk, distinguishing those who will experience outcomes from those who will not. For binary endpoints like disease occurrence within a fixed time window, the area under the receiver operating characteristic curve (auROC) summarizes discrimination across all classification thresholds (Section 21.5). When outcomes are rare (severe adverse drug reactions, specific disease subtypes), the area under the precision-recall curve (auPRC) better reflects how well the model identifies true positives among many negatives. For survival tasks with censoring, the concordance index and time-dependent auROC generalize these metrics to the time-to-event setting.\nStrong discrimination is necessary but not sufficient. A model that correctly ranks patients but systematically overestimates or underestimates absolute risk magnitudes will lead to inappropriate clinical decisions. If a model predicts 5% risk for patients who actually experience 15% event rates, physicians using those predictions will undertreat. Conversely, systematically inflated predictions lead to overtreatment with attendant harms and costs.\n\n\n25.6.2 Calibration\nCalibration asks whether predicted probabilities match observed frequencies. If a model assigns 20% risk to a group of patients, approximately 20% should experience the outcome. Well-calibrated predictions can be interpreted at face value and used directly for clinical decision-making; miscalibrated predictions mislead regardless of discrimination quality.\nAssessment involves calibration plots comparing predicted risk deciles to observed event rates, statistical tests like the Hosmer-Lemeshow test, and proper scoring rules like the Brier score that combine calibration and discrimination (Section 21.10). The methodological foundations for these assessments, including temperature scaling and isotonic regression approaches, are detailed in Section 23.3. These assessments must be stratified by clinically relevant subgroups (ancestry, sex, age, comorbidity burden) because a model well-calibrated overall may be systematically miscalibrated for specific populations.\nFor polygenic score-informed models, calibration requires particular attention. Raw polygenic scores are typically centered and scaled rather than calibrated to absolute risk. Mapping a score to an absolute event probability requires post-hoc models incorporating baseline incidence and clinical covariates. Foundation models can shift score distributions as architectures evolve, meaning recalibration may be necessary when updating encoders. The connection to Chapter 23 is direct: calibration is one form of uncertainty quantification, assessing whether model confidence aligns with actual outcome frequencies.\n\n\n25.6.3 Clinical Utility\nBeyond discrimination and calibration, clinical utility asks whether using the model will change decisions beneficially. Net reclassification improvement quantifies how many patients are appropriately moved across risk thresholds compared to a baseline model. Decision curve analysis estimates net benefit across threshold probabilities, accounting for the relative costs of false positives and false negatives in specific clinical contexts.\nFor foundation model-based tools, these analyses must demonstrate incremental value over existing alternatives. If a complex genomic foundation model provides only marginal improvement over a traditional polygenic score plus standard clinical calculator, the additional complexity, cost, and implementation burden may not be justified. The relevant comparison is not “better than nothing” but “better than what clinicians can already access.”\n\n\n25.6.4 Validation Hierarchy\nEvidence strength depends critically on validation design. Internal validation through cross-validation or temporal splits within development data is useful but insufficient due to potential overfitting and subtle data leakage issues discussed in Section 21.4.1. External validation across institutions and ancestries tests the same locked model in independent health systems and diverse populations. This step is essential for assessing whether performance reflects genuine biological signal versus idiosyncratic features of the development dataset.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 25.3: [Essential] Pyramid/staircase with evidence levels. Levels bottom to top: (1) Internal validation (CV, temporal splits; low evidence; proof of concept; overfitting risk); (2) External validation (same locked model in independent systems; moderate; transportability; multiple institutions/ancestries); (3) Prospective observational (model runs silently; moderate-high; real-time performance, drift detection; months-years); (4) Prospective interventional (randomized/quasi-experimental; high; clinical outcome improvement; trial registration, IRB). Annotations: Cost/time increasing; regulatory requirements; “Most FM tools stop here” at external validation.\n\n\n\nProspective observational validation runs the model silently alongside clinical care without influencing decisions, measuring real-time performance and drift in deployment conditions. Prospective interventional trials use randomized or quasi-experimental designs to assess whether model-guided care actually improves outcomes, equity, and cost-effectiveness compared to usual care.\nFor most foundation model-based tools, regulators and health systems expect robust external validation at minimum. High-stakes applications (cancer prognosis affecting treatment intensity, pharmacogenomic predictions affecting drug choice) may require prospective interventional evidence. The investment required increases at each level of the hierarchy, but so does the confidence that deployment will produce benefit rather than harm.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-uncertainty",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-uncertainty",
    "title": "25  Clinical Risk Prediction",
    "section": "25.7 Uncertainty Quantification",
    "text": "25.7 Uncertainty Quantification\nIn clinical settings, models must know when they do not know. A risk prediction offered with false confidence is more dangerous than one accompanied by appropriate uncertainty bounds, because the former invites unwarranted action while the latter prompts appropriate caution or additional evaluation.\nTwo sources of uncertainty require distinction. Aleatoric uncertainty reflects irreducible noise in the outcome: even with perfect input features, some patients with identical measured characteristics will experience different outcomes due to unmeasured variables, stochastic biology, or measurement error. Epistemic uncertainty reflects model limitations: insufficient training data, architectural constraints, or distributional shift between training and deployment conditions. Aleatoric uncertainty cannot be reduced by collecting more data or improving models; epistemic uncertainty can (Section 23.1).\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 25.4: [High] Uncertainty decomposition. Components: (1) Genomic Uncertainty (under-represented ancestry, rare variants, novel genes; detection: embedding distance; response: flag for review); (2) Clinical Uncertainty (extrapolation to new settings; detection: feature distribution monitoring; response: local calibration); (3) Outcome Uncertainty/Aleatoric (biological variability, incomplete penetrance; detection: heteroscedastic models; response: communicate irreducible). Visualization: Venn or stacked bar. Example patient profiles with different compositions. Decision support: When total exceeds threshold → abstain/flag.\n\n\n\nPractical uncertainty quantification methods include ensemble approaches, where multiple models trained with different random seeds provide prediction intervals based on their disagreement (Section 23.4.1). Monte Carlo dropout approximates Bayesian uncertainty by averaging predictions across stochastic forward passes (Section 23.4.2). Conformal prediction provides principled prediction intervals with guaranteed coverage under exchangeability assumptions, avoiding the distributional assumptions required by parametric methods (Section 23.5). Temperature scaling post-hoc adjusts model outputs to improve calibration without retraining (Section 23.3).\nFor foundation model-based systems, uncertainty decomposes into genomic and clinical components. Genomic uncertainty reflects confidence in variant effect predictions, fine-mapping probabilities, or embedding reliability; it increases for variants from underrepresented populations, rare variants with limited training examples, or sequences falling outside the distribution seen during pretraining. Clinical uncertainty reflects extrapolation to new care settings, practice patterns, or patient populations not represented in development data.\nSelective prediction allows models to abstain when uncertainty exceeds thresholds, flagging cases for human review rather than providing potentially misleading predictions (Section 23.7). This is particularly important for patients from rare ancestries underrepresented in training data or with unusual clinical presentations. The tension between coverage (providing predictions for all patients) and reliability (ensuring predictions are trustworthy) must be navigated thoughtfully, ideally with input from the clinicians who will use the system.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-fairness",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-fairness",
    "title": "25  Clinical Risk Prediction",
    "section": "25.8 Fairness and Health Equity",
    "text": "25.8 Fairness and Health Equity\nMany genomic and electronic health record datasets encode historical inequities in who gets genotyped, which populations are recruited into biobanks, and how healthcare is documented and delivered. Risk models trained on such data can amplify disparities if not carefully evaluated and designed.\nThe structural biases that genomic datasets inherit, from sequencing cohort recruitment to biobank composition to ClinVar submission patterns, create cascading effects on model performance. These biases manifest not as random noise but as systematic underperformance for populations historically excluded from genomic research (Section 22.2.1).\nThe ancestry bias in genome-wide association studies persists in foundation model applications. As discussed in Section 3.7, polygenic scores derived from European-ancestry data substantially underperform in other populations. Foundation models have the opportunity but not the guarantee to improve portability by leveraging functional priors that transfer across ancestries (sequence-based deleteriousness does not depend on population-specific linkage disequilibrium) and by incorporating multi-ancestry training data. Whether they succeed depends on training data composition, evaluation practices, and explicit attention to cross-ancestry performance throughout development.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\n\n\nFigure 25.5: [High] Equity dashboard. Panel A (Performance Disparity): Bar chart AUROC by ancestry; confidence intervals; reference line; highlight underperformance. Panel B (Calibration Disparity): Multiple reliability diagrams overlaid; ECE per group. Panel C (Clinical Utility Disparity): Decision curves by subgroup; net benefit at thresholds; model improves over treat-all/treat-none? Panel D (Access and Outcome Metrics): Who receives testing? Who benefits? Model reduces or amplifies disparities? Mitigation callout: Reweighting, group-wise calibration, expanding sequencing access.\n\n\n\nElectronic health record features introduce additional bias sources. Which patients receive genetic testing, which laboratory tests are ordered, how diagnoses are coded, and how thoroughly clinical notes are documented all differ systematically across patient populations, care settings, and health systems. A model trained on one institution’s data may encode those institutional patterns rather than underlying biology.\nHealth equity evaluation requires disparity metrics measuring performance differences in discrimination, calibration, and clinical utility across subgroups defined by ancestry, sex, socioeconomic proxies, and care site. Access metrics assess whether financial, geographic, or systemic barriers limit which patients can benefit from genomic risk tools. Outcome metrics evaluate whether clinical actions triggered by predictions differ across groups and whether benefits accrue equitably or concentrate among already-advantaged populations.\nTechnical mitigation strategies include reweighting training data to reduce representation disparities, group-wise calibration ensuring equitable performance across subgroups, and localized fine-tuning using deployment-site data. These approaches are discussed further in Section 22.9. Technical interventions alone cannot overcome structural inequities. Non-technical approaches including expanding sequencing access, subsidizing testing for underserved populations, and designing workflows that accommodate diverse care settings are equally essential.\nThe core principle is that equity cannot be an afterthought addressed during final evaluation. It must inform pretraining data selection, benchmark choice, validation study design, and deployment planning from the outset. A model that appears well-calibrated overall but is miscalibrated for specific populations will exacerbate rather than reduce health disparities.\nThe governance frameworks, regulatory considerations, and responsible development practices for ensuring equitable clinical AI are examined in Chapter 29.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-clinical-integration",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-clinical-integration",
    "title": "25  Clinical Risk Prediction",
    "section": "25.9 Clinical Integration",
    "text": "25.9 Clinical Integration\nEven a comprehensively validated model can fail in practice if it does not integrate into clinical workflows. Genomic risk predictions must reach clinicians at decision points, in formats that support rather than disrupt care delivery, with appropriate interpretability and uncertainty communication.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 25.6: [High] Workflow diagram. Clinical workflow: Patient presents → Assessment → Decision point → Action. Integration patterns: (1) Laboratory Interpretation Augmentation (FM scores in variant report; human geneticist reviews; variant classification); (2) Risk Embedded in electronic health record (EHR) (precomputed scores in structured fields; dashboard at point of care; clinical decision support); (3) Pharmacogenomic Alert (synchronous alert at prescription entry; drug-gene interaction flagged; medication ordering). For each: When computation happens, who sees output, what action triggered. System architecture: Model serving, adapters, logging, version control.\n\n\n\n\n25.9.1 Workflow Integration Patterns\nClinical genomics has established pathways for returning results through CLIA-certified laboratories, structured reports, and genetic counseling. Foundation model-based risk tools can augment these pathways in two primary ways. Laboratory interpretation augmentation uses foundation model predictions to prioritize variants for manual review, provide richer functional annotations, and suggest likely disease mechanisms supporting differential diagnosis. Direct risk embedding in electronic health records precomputes risk scores for patients with genomic data, surfaces them in structured fields or clinical dashboards, and triggers alerts when thresholds are crossed.\nDesign choices include batch versus on-demand computation (batch overnight processing is often preferable given foundation model computational costs and the relative stability of genomic data), synchronous alerts at order entry versus asynchronous reports in clinical inboxes, and whether high-impact predictions require human-in-the-loop review before reaching front-line clinicians.\nThe specifics vary by clinical context. Pharmacogenomic alerts might appear synchronously at prescription order entry, providing immediate guidance on drug selection or dosing. Cardiometabolic risk scores might appear in primary care dashboards updated weekly, informing prevention discussions at annual visits. Oncology prognosis estimates might be generated at diagnosis and reviewed in tumor board settings where multidisciplinary teams make treatment decisions.\n\n\n25.9.2 System Architecture\nFrom an engineering perspective, foundation model-based clinical tools typically require a secure model-serving endpoint handling inference requests, input adapters transforming laboratory and electronic health record data into model-ready formats, output adapters mapping predictions to structured clinical concepts or user-facing text, and logging infrastructure providing audit trails and enabling drift detection.\nRegulated settings impose additional requirements: versioning of models, data pipelines, and reference genomes with complete reproducibility; access controls and network segmentation protecting genomic data; and validation environments separated from production for safe testing of updates. Practical guidance on hardware requirements, deployment patterns, and cost estimation appears in Appendix B.\n\n\n25.9.3 Post-Deployment Monitoring\nClinical deployment begins rather than ends the model lifecycle. Practice patterns evolve as new treatments and guidelines emerge. Patient populations shift as screening programs expand or contract. Laboratory assays and sequencing pipelines change, introducing distributional shifts in input features.\nMonitoring systems should track input distributions (genotype frequencies, electronic health record feature patterns) to detect when current patients differ from training populations. Output distributions (risk score histograms, threshold-crossing rates) reveal whether model behavior is changing. Performance metrics computed via rolling windows or periodic audits detect calibration or discrimination degradation before clinical consequences accumulate.\nWhen drift is detected, responses range from recalibration (adjusting the score-to-probability mapping while preserving ranking behavior) through partial retraining (updating prediction heads while keeping foundation model weights fixed) to full model updates (retraining encoders, requiring renewed validation). The modular separation between foundation model backbones and clinical prediction heads facilitates this maintenance: encoders can be versioned and swapped with compatibility testing while prediction heads adapt to local deployment conditions.\nIncident response processes allow clinicians to report surprising or harmful predictions, triggering root-cause analysis and potential remediation. Governance structures including AI oversight committees review models periodically and establish clear criteria for deprecation when performance degrades below acceptable thresholds.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-regulatory",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-regulatory",
    "title": "25  Clinical Risk Prediction",
    "section": "25.10 Regulatory and Quality Frameworks",
    "text": "25.10 Regulatory and Quality Frameworks\nFoundation model-based clinical tools exist on a spectrum from research-only applications supporting hypothesis generation through clinical decision support tools informing diagnosis or management to regulated medical devices subject to formal oversight. The regulatory classification depends on intended use, risk level, and the claims made for the tool.\nJurisdictions differ in specifics, but common expectations include transparent descriptions of training data and known limitations, quantitative performance evidence across relevant subgroups, plans for post-market surveillance and incident reporting, and change management procedures for model updates. Beyond formal regulation, health systems typically require standard operating procedures for deployment and decommissioning, model cards describing training data and limitations, validation reports documenting evaluation evidence, and governance structures reviewing and approving new tools (Section 29.1).\nFoundation models introduce additional documentation requirements. Descriptions of pretraining corpora must specify which genomes, assays, and populations were included. Fine-tuning datasets and label definitions require detailed documentation. Procedures for updating to new genome builds, reference panels, or assay types must be established and tested. The modular separation between pretrained encoders and clinical prediction heads can ease regulatory management by allowing independent updates to each component, but this requires careful version control and compatibility testing to ensure that updating one component does not degrade performance of the combined system.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-case-studies",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-case-studies",
    "title": "25  Clinical Risk Prediction",
    "section": "25.11 Case Studies",
    "text": "25.11 Case Studies\nThree stylized case studies illustrate how foundation model features integrate into clinical risk prediction across different disease contexts, time horizons, and decision types.\n\n25.11.1 Cardiometabolic Risk Stratification\nA 52-year-old man presents to his primary care physician for an annual wellness visit. His LDL cholesterol is 145 mg/dL, blood pressure is 138/88 mmHg, and hemoglobin A1c is 5.9%, placing him in the prediabetic range. His father had a myocardial infarction at age 58. The standard Pooled Cohort Equations estimate his 10-year atherosclerotic cardiovascular disease risk at 8.2%, just below the threshold where guidelines recommend statin therapy.\nA foundation model-augmented risk system could refine this assessment. Variant effect scores from DNA foundation models annotate variants in cardiometabolic risk loci with predicted regulatory and coding impacts, combining sequence-based scores with fine-mapping probabilities to prioritize likely causal variants (Section 14.3; Section 14.4). A polygenic embedding model like Delphi or G2PT produces a genome-wide representation capturing nonlinear risk structure beyond simple effect size sums (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025). This genomic embedding combines with electronic health record features through an intermediate fusion architecture, producing an updated 10-year risk estimate of 11.4%, above the treatment threshold.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 25.7: [Enhancing] Patient journey diagram. Patient profile: 52yo man, LDL 145, BP 138/88, HbA1c 5.9%; family history (father MI at 58); traditional risk 8.2% (below statin threshold). FM integration: DNA FMs annotate variants; polygenic embedding (Delphi/G2PT) captures nonlinear risk; fusion with EHR; updated estimate 11.4% (above threshold). Clinical decision impact: Threshold crossed → statin indicated; pathway attribution → LDL metabolism (supports mechanism); attention attribution → regions for counseling. Validation requirements: External validation, equity analysis. Key insight: Value depends on whether refined estimate enables different action.\n\n\n\nThe clinical value depends on what this refined estimate enables. If genomic foundation model features merely replicate traditional polygenic score information with higher computational cost, the benefit is marginal. But if the embedding captures pathway-level structure that identifies this patient’s risk as concentrating in LDL metabolism pathways rather than inflammatory or thrombotic mechanisms, that information might strengthen the indication for statin therapy specifically. Attention-based attributions highlighting which genomic regions contribute most to the elevated risk could inform counseling about heritability and family screening.\nExternal validation across multiple health systems and ancestries would need to demonstrate that the foundation model approach provides calibrated predictions and meaningful reclassification improvement over traditional tools. Equity analysis would verify that performance holds across the diverse populations the health system serves rather than degrading for non-European ancestries underrepresented in training data.\n\n\n25.11.2 Oncology Prognosis\nA 64-year-old woman has undergone surgical resection for stage II colorectal cancer with microsatellite stable tumor characteristics. Her oncology team must decide whether adjuvant chemotherapy is warranted given the balance between recurrence risk reduction and treatment toxicity. Traditional staging provides prognostic information, but substantial heterogeneity exists within stage categories.\nFoundation models can enrich prognostic assessment through multiple channels. Tumor mutation profiles encoded through models like SetQuence or SetOmic produce embeddings capturing the specific constellation of somatic alterations beyond simple mutation counts (Jurenaite et al. 2024). Transcriptomic profiling integrated through GLUE-style latent spaces adds expression context reflecting tumor microenvironment and pathway activity (Cao and Gao 2022). Graph neural network-based subtyping assigns the tumor to a molecular subtype with characteristic prognosis and treatment response patterns (Li et al. 2022).\nThese tumor-level representations combine with germline pharmacogenomic features (variants affecting fluoropyrimidine metabolism that influence toxicity risk) and clinical features (performance status, comorbidities, patient preferences) in a survival model predicting two-year recurrence hazard. A high-risk prediction might favor more intensive adjuvant therapy, while low-risk predictions might support observation with close surveillance.\nThe validation requirements are stringent. Retrospective analysis of institutional cohorts establishes proof of concept, but prospective validation in cohorts receiving contemporary treatment regimens is necessary given the rapid evolution of oncology care. Interpretability connecting predictions to specific mutations, pathways, or molecular subtypes supports clinical adoption by providing rationale beyond a black-box hazard estimate.\n\n\n25.11.3 Pharmacogenomic Adverse Event Prediction\nA 45-year-old man with newly diagnosed epilepsy requires anticonvulsant therapy. Carbamazepine is a common first-line choice, but it carries risk of severe cutaneous adverse reactions including Stevens-Johnson syndrome and toxic epidermal necrolysis. The HLA-B15:02* allele is strongly associated with carbamazepine hypersensitivity in patients of Asian ancestry, and FDA labeling recommends genetic testing before initiating therapy in at-risk populations [Citation Needed].\nThis established pharmacogenomic association illustrates both the potential and limitations of current approaches. Single-variant associations with high effect sizes enable straightforward clinical implementation, but they cover a small fraction of drug-gene interactions. Many patients who do not carry HLA-B15:02* still experience adverse reactions, suggesting additional genetic (and non-genetic) risk factors that single-variant testing misses.\nFoundation models could extend pharmacogenomic prediction beyond established single-gene associations. Variant effect scores across HLA genes, drug metabolism enzymes, and immune-related loci provide features reflecting the patient’s overall pharmacogenetic landscape (Section 2.8.4). These features aggregate into a polygenic adverse event risk score that captures contributions from many variants rather than relying on individual high-effect alleles. Combined with clinical features (renal function affecting drug clearance, concomitant medications with interaction potential, prior adverse reaction history), the model predicts adverse event probability specific to the proposed drug.\nThe validation challenge is severe. Serious adverse drug reactions are rare, making endpoint ascertainment difficult and underpowered. Case-control designs enriched for adverse events may overestimate model performance compared to prospective deployment. Multi-site validation across healthcare systems with different prescribing patterns and population ancestry compositions is essential.\nClinical implementation requires integration at the point of prescribing, providing actionable information when drug selection decisions are being made. This argues for pre-computed pharmacogenomic profiles that alert at order entry rather than reactive testing after a prescription is written. The interpretability requirement is particularly acute: clinicians must understand why a model flags a patient as high-risk for a specific drug to make informed risk-benefit decisions.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch25-clinical-risk.html#sec-ch25-translation-test",
    "href": "part_6/p6-ch25-clinical-risk.html#sec-ch25-translation-test",
    "title": "25  Clinical Risk Prediction",
    "section": "25.12 Translation as the Test",
    "text": "25.12 Translation as the Test\nSuccess for genomic foundation models in clinical medicine will depend less on model scale and more on rigorous translation. Problem definition, evidence generation, equity evaluation, regulatory compliance, workflow integration, and post-deployment monitoring each introduce opportunities for failure. Models that clear all hurdles are rare; models that skip stages fail in deployment regardless of their technical sophistication.\nThe representational advances that foundation models provide become valuable only when they flow through validated, equitable, well-integrated clinical tools into decisions that improve patient outcomes. A pathogenicity score with state-of-the-art discrimination adds nothing to care if it reaches clinicians at the wrong moment, in the wrong format, without appropriate uncertainty communication. A risk prediction that performs excellently on average but fails systematically for underrepresented populations may widen health disparities rather than narrow them. Technical capability is necessary but not sufficient for clinical impact.\nRare disease diagnosis illustrates these translation principles in a particularly high-stakes context: where risk prediction addresses population-level stratification, variant interpretation addresses individual patients, with different evidence requirements, clinical workflows, and definitions of success (Chapter 26).\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “[GPN] DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T. Higgins-Chen, Steve Horvath, and Bo Wang. 2024. “CpGPT: A Foundation Model for DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024. “[DeepRVAT] Integration of Variant Annotations Using Deep Set Networks Boosts Rare Variant Association Testing.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. “Delphi: A Deep-Learning Method for Polygenic Risk Prediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nJurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge, and René Jäkel. 2024. “SetQuence & SetOmic: Deep Set Transformers for Whole Genome and Exome Tumour Analysis.” BioSystems 235 (January): 105095. https://doi.org/10.1016/j.biosystems.2023.105095.\n\n\nKatzman, Jared L., Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval Kluger. 2018. “DeepSurv: Personalized Treatment Recommender System Using a Cox Proportional Hazards Deep Neural Network.” BMC Medical Research Methodology 18 (1): 24. https://doi.org/10.1186/s12874-018-0482-1.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam, Amit R. Majithia, and Trey Ideker. 2025. “[G2PT] A Genotype-Phenotype Transformer to Assess and Explain Polygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and Yunping Zhu. 2022. “MoGCN: A Multi-Omics Integration Method Based on Graph Convolutional Network for Cancer Subtype Analysis.” Frontiers in Genetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nNagpal, Chirag, Xinyu Li, and Artur Dubrawski. 2021. “Deep Survival Machines: Fully Parametric Survival Regression and Representation Learning for Censored Data With Competing Risks.” IEEE Journal of Biomedical and Health Informatics 25 (8): 3163–75. https://doi.org/10.1109/JBHI.2021.3052441.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nTipirneni, Sindhu, and Chandan K. Reddy. 2022. “Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series.” ACM Trans. Knowl. Discov. Data 16 (6): 105:1–17. https://doi.org/10.1145/3516367.\n\n\nXu, Leqi, Wangjie Zheng, Jiaqi Hu, Yingxin Lin, Jia Zhao, Gefei Wang, Tianyu Liu, and Hongyu Zhao. 2025. “Improving Polygenic Risk Prediction Performance by Integrating Electronic Health Records Through Phenotype Embedding.” The American Journal of Human Genetics 112 (12): 3030–45. https://doi.org/10.1016/j.ajhg.2025.11.006.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch26-rare-disease.html",
    "href": "part_6/p6-ch26-rare-disease.html",
    "title": "26  Rare Disease Diagnosis",
    "section": "",
    "text": "26.1 Variant Prioritization Funnel\nA four-year-old presents with developmental delay, hypotonia, and seizures that began at eighteen months. Standard metabolic testing reveals nothing. A gene panel for epilepsy returns negative. The neurologist orders whole-exome sequencing, which identifies 23,847 single nucleotide variants and 1,203 small insertions or deletions compared to the reference genome. Somewhere in this list of approximately 25,000 variants may lie the molecular explanation for this child’s condition. The clinical team must reduce this number to a handful of candidates for expert review, ideally to a single variant or gene that explains the phenotype and guides management. This is the diagnostic odyssey: the gap between sequencing a genome and understanding what it means for a patient.\nThis scenario plays out thousands of times daily across clinical laboratories worldwide. Rare diseases collectively affect approximately 300 million people globally, yet individual conditions range from tens of thousands of patients to fewer than a dozen known cases worldwide (Nguengang Wakap et al. 2020). Over 7,000 rare diseases have been characterized, the majority following Mendelian inheritance patterns where single genes exert large effects (Amberger et al. 2015). For these patients, identifying the causal variant can end years of uncertainty, enable accurate genetic counseling for families, and increasingly guide targeted therapies. The technical capacity to sequence genomes has advanced enormously; the interpretive bottleneck has not kept pace. Variant interpretation remains largely manual, relying on clinical geneticists and laboratory directors who cannot scale to meet demand.\nFoundation models offer new tools for this interpretive challenge. As detailed in Chapter 14, models like AlphaMissense provide proteome-wide estimates of missense pathogenicity, while regulatory models like Enformer predict variant effects on gene expression across tissues. These computational predictions become one line of evidence within structured interpretation frameworks. Foundation model outputs integrate into clinical variant interpretation workflows: from initial prioritization that reduces 25,000 variants to dozens, through ACMG-AMP evidence classification that structures expert review, to family-based analysis that leverages inheritance patterns, and laboratory validation that confirms computational predictions. The goal is not prediction for its own sake but actionable clinical insight: which variant explains this patient’s disease, and what should we do about it?\nClinical variant interpretation operates through progressive filtering, narrowing tens of thousands of candidates to a manageable set for expert review. Each filtering step applies different types of evidence, and foundation models contribute at multiple stages.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch26-rare-disease.html#sec-ch26-prioritization-funnel",
    "href": "part_6/p6-ch26-rare-disease.html#sec-ch26-prioritization-funnel",
    "title": "26  Rare Disease Diagnosis",
    "section": "",
    "text": "FIGURE PLACEHOLDER\n\n\n\n\nFigure 26.1: [Essential] Inverted funnel with filtering stages. Stages with numbers: (1) Raw variants (~25,000); (2) Quality filtered (~22,000); (3) Population frequency filtered (~500-1,000); (4) Consequence filtered (~100-200); (5) Foundation model scored (~20-50); (6) Expert review candidates (~5-10). Annotations: Percentage removed, time/compute cost, where FMs contribute most. Key insight: FMs contribute at scoring stage, after basic filtering, before expert review.\n\n\n\n\n26.1.1 Quality and Technical Filters\nThe first filter removes variants that are likely technical artifacts rather than true biological variation. Sequencing depth below 20x, strand bias exceeding established thresholds, and clustering of variants in repetitive regions all raise suspicion of false positives. Variant calling pipelines like GATK and DeepVariant (see Section 1.8) produce quality scores that guide this initial triage. As discussed in Section 23.2, these confidence estimates require careful calibration; systematic miscalibration in specific genomic contexts propagates directly into interpretation, creating blind spots where uncertain calls masquerade as confident ones or vice versa. Variants failing quality thresholds are removed before any biological interpretation begins.\nFor trio analysis (proband plus both parents), Mendelian inheritance consistency provides an additional quality check. A variant called heterozygous in the child should appear in at least one parent unless it arose de novo. Widespread Mendelian inconsistencies indicate sample swaps, contamination, or systematic calling errors that must be resolved before interpretation proceeds.\n\n\n26.1.2 Population Frequency Filters\nVariants common in the general population are unlikely to cause rare, severe disease. If a variant appears in 1% of gnomAD individuals, it cannot plausibly explain a condition affecting one in 100,000 people under a dominant model. Frequency thresholds depend on inheritance mode and disease prevalence: dominant conditions with complete penetrance require extremely rare variants (often absent from population databases), while recessive conditions can tolerate higher carrier frequencies.\nThe Genome Aggregation Database (gnomAD) provides allele frequencies across over 800,000 individuals from diverse ancestries (see Section 2.2.3) (Karczewski et al. 2020). Applying a frequency threshold of 0.01% for dominant conditions and 1% for recessive carriers typically removes 95% or more of variants from consideration. Ancestry-matched frequencies matter: a variant rare in European populations may be common in African or East Asian populations, and global frequency alone can be misleading.\n\n\n26.1.3 Consequence and Gene Filters\nPredicted functional consequence shapes prioritization. Loss-of-function variants (frameshift, nonsense, canonical splice site) in genes intolerant to haploinsufficiency receive immediate attention. Missense variants require additional assessment, as most are benign. Intronic and intergenic variants have historically been deprioritized, though foundation models are beginning to identify functional noncoding variants with greater precision (see Section 13.2 for regulatory models and Section 14.3.2 for variant effect prediction in noncoding regions).\nGene-level filters incorporate prior knowledge. Curated gene panels for specific phenotypes (such as the PanelApp epilepsy panel or cardiomyopathy panel) restrict analysis to genes with established disease associations. For undiagnosed cases without clear phenotype match, broader approaches may include all OMIM disease genes or genes with high constraint (low observed/expected loss-of-function ratios in gnomAD).\n\n\n26.1.4 Foundation Model Scoring\nAfter quality, frequency, and consequence filters, foundation model predictions provide quantitative effect estimates for remaining candidates. For missense variants, AlphaMissense scores offer genome-wide pathogenicity estimates derived from protein structure and evolutionary conservation (Cheng et al. 2023). For splice-region variants, SpliceAI predictions quantify the probability and magnitude of splicing disruption (Jaganathan et al. 2019). For regulatory variants, Enformer and related models estimate effects on chromatin accessibility and gene expression in relevant tissues (Section 13.2; Section 14.3.2) (Avsec et al. 2021).\nThese scores do not directly translate to pathogenicity classifications. A high AlphaMissense score indicates that the protein change is likely functionally disruptive, not that it causes a specific disease. The clinical relevance of any functional disruption depends on the gene’s role in the patient’s phenotype, the inheritance pattern, and whether disruption of that gene produces the observed clinical features. Foundation model scores become one input to a structured evidence framework, not a standalone answer.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch26-rare-disease.html#sec-ch26-acmg-amp",
    "href": "part_6/p6-ch26-rare-disease.html#sec-ch26-acmg-amp",
    "title": "26  Rare Disease Diagnosis",
    "section": "26.2 ACMG-AMP Criteria and Computational Evidence",
    "text": "26.2 ACMG-AMP Criteria and Computational Evidence\nThe American College of Medical Genetics and Genomics and Association for Molecular Pathology (ACMG-AMP) framework provides the dominant structure for clinical variant classification (Richards et al. 2015). Published in 2015 and subsequently refined through ClinGen expert panels, this framework assigns variants to five categories: pathogenic, likely pathogenic, variant of uncertain significance (VUS), likely benign, and benign. Classification emerges from combining multiple evidence types, each assigned a strength level (very strong, strong, moderate, supporting) and direction (pathogenic or benign).\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 26.2: [Essential] Evidence combination diagram. Columns (evidence categories): Population data, Computational predictions, Functional data, Segregation, de novo, Clinical. Rows (evidence strength): Very Strong (PVS1), Strong, Moderate, Supporting. Combination rules: Pathogenic criteria, Likely Pathogenic, VUS, Likely Benign, Benign. FM contribution highlighted: PP3/BP4 box emphasized; “Traditionally supporting strength”; arrow showing potential upgrade for calibrated predictors. Classification distribution showing VUS dominates.\n\n\n\n\n26.2.1 Evidence Categories\nACMG-AMP evidence spans several domains. Population data includes allele frequency in controls (BA1, BS1, BS2 for benign; PM2 for pathogenic support when absent). Computational predictions include in silico tools predicting deleterious effects (PP3 for pathogenic support) or benign effects (BP4 for benign support). Functional data includes well-established functional assays demonstrating deleterious (PS3) or no (BS3) effect. Segregation data addresses co-segregation with disease in multiple affected family members (PP1) or lack of segregation (BS4). De novo status assigns strong (PS2) or moderate (PM6) evidence when parental samples are available and the variant is absent in both parents. Clinical information incorporates specific phenotype match (PP4) and prevalence considerations.\nThe framework combines these evidence types through defined rules. Pathogenic classification requires either one very strong criterion plus one strong, or two strong criteria, with additional supporting evidence. Likely pathogenic requires somewhat less evidence. Most variants end up as VUS because available evidence is insufficient for confident classification in either direction.\n\n\n26.2.2 PP3 and BP4: Computational Evidence\nComputational predictions enter the ACMG-AMP framework primarily through PP3 (pathogenic supporting evidence from computational predictions) and BP4 (benign supporting evidence). These criteria apply when multiple in silico tools agree that a variant is deleterious (PP3) or benign (BP4).\nThe original 2015 guidelines assigned these criteria only “supporting” strength, reflecting appropriate caution about computational predictions available at the time. Tools like SIFT, PolyPhen-2, and CADD had limited accuracy and concerning circularity issues (Section 4.5). The evaluation challenges these tools face, including benchmark contamination and label leakage, are examined in Section 22.5. ClinGen sequence variant interpretation working groups have subsequently refined how computational evidence is weighted, in some cases upgrading to moderate strength for well-calibrated predictors in specific genes.\nFoundation models raise new questions about computational evidence strength. AlphaMissense achieves substantially higher accuracy than traditional tools on held-out ClinVar variants and deep mutational scanning data. Should predictions from these models receive greater evidentiary weight? The answer is not straightforward. Higher accuracy on aggregate benchmarks does not guarantee reliability for any individual prediction. Gene-specific calibration matters: a model may perform well across all genes but poorly for genes with unusual structure or function. And the fundamental limitation remains that computational predictions estimate functional impact, not clinical pathogenicity.\nResponsible application of foundation model predictions in ACMG-AMP classification requires gene-specific and variant-type-specific calibration whenever possible, explicit acknowledgment that PP3/BP4 evidence is supporting unless upgraded by expert panel guidance, use of multiple orthogonal predictors rather than reliance on any single model, and clear documentation of which tools were applied and how predictions were interpreted.\n\n\n26.2.3 Calibrating Predictions to Evidence Strength\nThe calibration problem is central to using foundation model predictions clinically. A model outputs a continuous score; clinical classification requires discrete evidence categories. How should thresholds be set, and what strength should be assigned?\nThe ClinGen Sequence Variant Interpretation Recommendations address this through the concept of odds of pathogenicity (Tavtigian et al. 2018). Supporting evidence corresponds to an odds ratio of approximately 2 (twice as likely pathogenic as benign given this evidence). Moderate evidence corresponds to odds of approximately 4, and strong evidence to odds of approximately 18. For a computational predictor to warrant upgrading from supporting to moderate strength, its predictions should demonstrably achieve odds ratios meeting these thresholds in relevant validation datasets.\nFor AlphaMissense and similar foundation models, published validation shows that the highest-scoring variants (above 0.9) achieve odds ratios exceeding the strong evidence threshold in some gene contexts (Pejaver et al. 2022; Bergquist et al. 2025). The calibration methods underlying these threshold determinations, including isotonic regression and Platt scaling, are detailed in Section 23.3; their application to ACMG evidence mapping is discussed in Section 14.5.3. ClinGen expert panels have begun incorporating these calibrations for specific genes, allowing upgraded evidence strength when predictions meet defined criteria. Clinicians should follow gene-specific expert panel recommendations when available rather than applying uniform thresholds across all genes.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch26-rare-disease.html#sec-ch26-family-analysis",
    "href": "part_6/p6-ch26-rare-disease.html#sec-ch26-family-analysis",
    "title": "26  Rare Disease Diagnosis",
    "section": "26.3 Family-Based Analysis",
    "text": "26.3 Family-Based Analysis\nRare disease interpretation rarely relies on proband sequence alone. Family structure provides substantial additional information through inheritance pattern constraints, de novo status determination, and segregation analysis.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\n\nFigure 26.3: [High] Three-panel scenarios. Panel A (de novo Detection): Trio pedigree; variant present in proband, absent in parents; strong evidence (PS2); FM prioritizes among multiple de novos. Panel B (Compound Het Trans): Two variants in same gene from different parents; biallelic disruption; FM assesses severity of each. Panel C (Compound Het Cis): Both from same parent; one functional copy remains; recessive does not fit; FM identifies if either alone sufficient. Phasing methods sidebar: Long-read, trio, statistical.\n\n\n\n\n26.3.1 De Novo Variants\nDe novo variants arise newly in the proband and are absent in both parents. For severe, early-onset dominant conditions, de novo mutations are expected: affected individuals rarely reproduce, so the disease-causing allele must arise fresh each generation. Observing a damaging variant as de novo provides strong evidence for pathogenicity under ACMG-AMP (PS2), often sufficient to push a candidate toward likely pathogenic or pathogenic classification.\nThe informativeness of de novo status depends on the mutation rate at that site and the expected de novo rate for the variant class. The human germline mutation rate is approximately 1 to 1.5 new mutations per 100 million base pairs per generation (Kong et al. 2012). For protein-coding exons (approximately 30 million base pairs), each individual carries roughly one new coding variant on average. Finding a damaging de novo variant in a candidate gene is therefore much more suspicious than finding an inherited variant of similar predicted effect.\nFoundation models assist de novo interpretation by providing effect estimates that help prioritize among multiple de novo variants (typical trio sequencing identifies one to three de novo coding variants) and by identifying de novo variants in noncoding regions that might disrupt critical regulatory elements. A de novo variant in a brain-specific enhancer upstream of a known epilepsy gene, predicted by Enformer to substantially reduce gene expression (Section 13.2.1), warrants investigation even though traditional pipelines might overlook noncoding de novo events.\n\n\n26.3.2 Compound Heterozygosity and Phasing\nRecessive diseases require biallelic disruption: both copies of the gene must be affected for disease to manifest. When a proband carries two different heterozygous variants in the same gene, the critical question is whether these variants are in trans (on opposite chromosomes, leading to biallelic disruption) or in cis (on the same chromosome, leaving one copy functional).\nPhasing determines which configuration applies (Section 1.4.1 for clinical stakes; Section 1.4.3 for methodological details). Several approaches are available. Physical phasing through long-read sequencing directly observes which variants occur on the same DNA molecule, providing definitive phase information when reads span both variant positions. Trio phasing infers phase from parental genotypes: if one variant is inherited from the mother and one from the father, they must be in trans. Statistical phasing uses population haplotype patterns to estimate phase, though accuracy decreases for rare variants not well-represented in reference panels.\nFor clinical interpretation, trio phasing is often the most practical approach. If both variants are confirmed in trans and both are predicted damaging, this supports pathogenicity under a recessive model. If both variants were inherited from a single parent (in cis), the gene cannot explain a recessive phenotype unless a third variant exists.\nFoundation models contribute by estimating the functional severity of each variant. A missense variant with marginal AlphaMissense score might not warrant attention alone, but paired in trans with a clear loss-of-function variant, the compound heterozygous combination could produce sufficient functional disruption to cause disease.\n\n\n26.3.3 Segregation Analysis\nIn larger families with multiple affected and unaffected individuals, segregation analysis examines whether candidate variants track with disease status. Under a dominant model, all affected individuals should carry the variant, and penetrance assumptions constrain how many unaffected carriers are expected. Under a recessive model, affected individuals should be homozygous or compound heterozygous, carriers should be heterozygous, and unaffected non-carriers should lack the variant entirely.\nStrong segregation evidence (PP1, upgradable to strong evidence with sufficient meioses) can substantially support pathogenicity classification. Equally important, failure to segregate provides benign evidence (BS4): a variant present in unaffected family members at rates inconsistent with the proposed inheritance model is unlikely to be causal.\nSegregation analysis requires accurate pedigree information, confirmed sample identities, and careful consideration of age-dependent penetrance and phenocopies. A variant might be present in an unaffected young relative who will develop disease later, or an affected relative might have a different etiology (phenocopy). These complexities require clinical judgment that no computational model can replace.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch26-rare-disease.html#sec-ch26-somatic",
    "href": "part_6/p6-ch26-rare-disease.html#sec-ch26-somatic",
    "title": "26  Rare Disease Diagnosis",
    "section": "26.4 Somatic Variant Interpretation in Cancer",
    "text": "26.4 Somatic Variant Interpretation in Cancer\nCancer genomics presents distinct interpretive challenges. Tumor genomes accumulate mutations throughout malignant evolution, creating a mix of driver mutations (those conferring selective advantage and contributing to cancer development) and passenger mutations (bystanders with no functional consequence). The interpretive task shifts from identifying variants causing inherited disease to identifying variants driving tumor biology and predicting therapeutic response.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 26.4: [High] Two-column comparison. Column 1 (Germline): Present in all cells; question: explains inherited disease?; framework: ACMG-AMP pathogenicity; FM role: functional impact; clinical: genetic counseling, family screening, prevention. Column 2 (Somatic): Acquired in tumor; question: driver? therapy response?; framework: recurrence, functional impact, biomarkers; FM role: driver vs passenger; clinical: treatment selection, prognosis. Key distinctions: Same variant, different implications; BRCA1 example. Practical confusion: Tumor-only sequencing cannot distinguish.\n\n\n\n\n26.4.1 Germline versus Somatic Distinction\nCancer sequencing must distinguish germline variants (present in all cells, inherited or de novo) from somatic variants (acquired in the tumor lineage). Tumor-only sequencing cannot make this distinction reliably, as rare germline variants may be mistaken for somatic events. Paired tumor-normal sequencing, comparing tumor to a non-malignant sample from the same patient, enables confident somatic variant identification.\nThis distinction has direct clinical implications. A germline pathogenic variant in BRCA1 indicates hereditary cancer predisposition affecting the patient and potentially their family members, warranting genetic counseling and possibly risk-reducing interventions. A somatic BRCA1 mutation arose in the tumor and has no implications for inherited risk, though it may still predict response to PARP inhibitors.\n\n\n26.4.2 Driver Classification\nAmong somatic mutations, identifying drivers requires different evidence than germline pathogenicity assessment. Recurrence across independent tumors suggests selective advantage: if BRAF V600E appears in 50% of melanomas, this frequency far exceeds what chance would predict, implying functional importance. Databases like COSMIC catalog somatic mutation frequencies across cancer types, enabling recurrence-based prioritization (Tate et al. 2019).\nFunctional impact predictions from foundation models apply somewhat differently in the somatic context. A missense variant predicted highly damaging by AlphaMissense in a tumor suppressor gene suggests loss of function consistent with a driver role. The same prediction in an oncogene might indicate loss of normal regulation, potentially activating rather than inactivating the protein. Interpretation must consider the gene’s role (oncogene versus tumor suppressor) and the specific functional consequence of the variant.\nTumor mutational burden provides context for individual variant interpretation. Hypermutated tumors (from mismatch repair deficiency or POLE mutations) may carry thousands of coding mutations, making it difficult to identify drivers against this noisy background. In such cases, restricting attention to known hotspots, truncating mutations in tumor suppressors, and variants with strong functional predictions helps prioritize the likely relevant events.\n\n\n26.4.3 Therapeutic Biomarkers\nSomatic variant interpretation increasingly focuses on therapeutic implications. Specific variants predict response to targeted therapies: EGFR exon 19 deletions and L858R mutations predict erlotinib response in lung cancer; BRAF V600E predicts vemurafenib response in melanoma; PIK3CA mutations indicate alpelisib benefit in breast cancer (Lynch et al. 2004; Chapman et al. 2011; André et al. 2019). These associations derive from clinical trials demonstrating differential response by mutation status.\nFoundation models do not directly predict therapeutic response, as they lack the clinical outcome data that would be required. Their contribution is in characterizing novel variants in known therapeutic target genes. A patient whose tumor carries an unusual EGFR mutation not previously characterized might be evaluated using structural models and effect predictions to estimate whether the mutation likely preserves the drug-binding site and confers similar dependency as canonical sensitizing mutations. Such analyses are hypothesis-generating rather than definitive but can inform clinical decision-making when direct trial evidence is unavailable.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch26-rare-disease.html#sec-ch26-validation",
    "href": "part_6/p6-ch26-rare-disease.html#sec-ch26-validation",
    "title": "26  Rare Disease Diagnosis",
    "section": "26.5 Laboratory Validation",
    "text": "26.5 Laboratory Validation\nComputational predictions, however accurate, remain predictions. Functional assays provide direct experimental evidence of variant effects, and ACMG-AMP appropriately weights functional data (PS3 for damaging functional effect, BS3 for no functional effect) as strong evidence when assays are well-validated.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 26.5: [High] Circular feedback loop. Steps: (1) Computational prediction (FM scores variants); (2) Prioritization for testing (select VUS where functional data resolves); (3) Functional assay (protein function, splicing, regulatory); (4) Result integration (PS3/BS3 evidence; classification update); (5) Model refinement (functional data improves training). Example pathway: AlphaMissense high impact → enzymatic assay → loss confirmed → PS3 → Likely Pathogenic → training data. Key insight: FM predictions are hypotheses; validation definitive but can only test limited variants.\n\n\n\n\n26.5.1 Types of Functional Assays\nDifferent variant types require different assay approaches. For missense variants, protein function assays measure specific biochemical activities of the mutant protein: enzyme activity, DNA binding, protein-protein interactions, or cellular phenotypes in model systems. Deep mutational scanning systematically characterizes all possible amino acid substitutions at each position in a protein, creating comprehensive functional maps (Section 2.4.4 for data resources; Section 14.4.1 for how foundation models leverage this data). These maps enable immediate lookup of functional effects for any observed missense variant, though coverage remains incomplete across the proteome.\nFor splicing variants, minigene assays clone genomic regions containing the variant into expression vectors and measure splicing patterns in cultured cells. RNA sequencing from patient tissue (when accessible) directly observes whether aberrant splicing occurs in vivo. SpliceAI predictions can be validated by these direct measurements, establishing whether computational predictions match experimental reality for specific variants.\nFor regulatory variants, reporter assays measure whether variant-containing regulatory elements drive appropriate expression patterns. Massively parallel reporter assays (MPRAs) enable testing thousands of variants simultaneously, generating the training data that informs foundation model development while also providing direct validation for specific variants of clinical interest. CRISPR-based approaches can introduce variants into endogenous genomic contexts rather than artificial reporter constructs, providing more physiologically relevant readouts.\n\n\n26.5.2 Integrating Functional Evidence\nFunctional data enters ACMG-AMP classification through PS3 (strong pathogenic evidence from functional studies showing deleterious effect) and BS3 (strong benign evidence from functional studies showing no effect) (Brnich et al. 2019). The strength assignment depends on assay validation: well-established assays measuring physiologically relevant endpoints warrant strong evidence, while novel or less-validated assays may warrant only moderate or supporting strength.\nClinGen has developed detailed recommendations for functional evidence evaluation. The specific gene and disease mechanism should guide assay selection. Controls (known pathogenic and known benign variants) should be included to validate assay performance. The biological relevance of the assay endpoint to the disease mechanism must be justified. These requirements reflect appropriate caution: not all functional assays are equally informative, and inappropriate assays can mislead classification.\nFoundation model predictions can prioritize which variants most warrant functional follow-up. When resources limit testing to a subset of VUS, selecting those with discordant computational predictions (high predicted impact but uncertain clinical classification) maximizes the information gained. Variants where functional testing might resolve classification provide greater value than variants where classification is already clear or unlikely to change regardless of functional results.\n\n\n26.5.3 Closing the VUS Loop\nThe accumulation of variants of uncertain significance represents a major challenge in clinical genetics. Patients receive results that cannot be interpreted, creating anxiety and uncertainty. As more individuals undergo sequencing, VUS prevalence grows. Systematic efforts to resolve VUS through functional characterization could dramatically improve the clinical utility of genetic testing.\nHigh-throughput functional approaches offer a path forward. Saturation genome editing applies CRISPR to introduce every possible single-nucleotide variant at clinically important loci, then measures functional consequences through cellular phenotypes or growth selection (Findlay et al. 2018). These experiments generate comprehensive functional maps that can immediately classify any observed variant. The Brotman Baty Institute’s ongoing efforts for BRCA1, mismatch repair genes, and other clinically important loci exemplify this approach.\nFoundation models trained on these functional datasets can generalize beyond directly measured variants, predicting effects for positions or genes not yet characterized experimentally. This creates a productive cycle: functional data improves model training, improved models identify high-priority variants for follow-up, and targeted experiments fill gaps while further improving models.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch26-rare-disease.html#sec-ch26-workflow",
    "href": "part_6/p6-ch26-rare-disease.html#sec-ch26-workflow",
    "title": "26  Rare Disease Diagnosis",
    "section": "26.6 Practical Workflow Integration",
    "text": "26.6 Practical Workflow Integration\nTranslating foundation model capabilities into clinical practice requires integration with existing laboratory and clinical workflows. The technical and interpretive steps must fit within established regulatory frameworks, electronic health record systems, and clinical team structures.\n\n26.6.1 Laboratory Workflow\nClinical sequencing laboratories operate under regulatory oversight (CLIA certification, state licensure, and potentially CAP accreditation in the United States). Validated pipelines must produce consistent, reproducible results. Introducing new computational tools requires formal validation demonstrating that the tool performs as expected on representative sample types, that outputs are interpretable and actionable by clinical staff, and that results are documented and traceable.\nFor foundation model integration, validation studies should assess performance on variants with known clinical classifications, compare predictions to existing tools to understand concordance and discordance, evaluate performance across variant types (missense, splice, regulatory) and gene categories, and document threshold selection and evidence strength assignment.\nLaboratory information management systems must capture foundation model predictions alongside other variant annotations. Reports to clinicians should clearly indicate the role of computational evidence, the specific tools applied, and the evidence strength assigned. Overreliance on computational predictions, or failure to communicate their limitations, risks inappropriate clinical decisions.\n\n\n26.6.2 Clinical Decision-Making\nVariant interpretation reports ultimately inform clinical decisions: whether to pursue additional testing, what genetic counseling to provide, whether to adjust medical management, and what surveillance or prevention strategies to recommend. These decisions rest with clinicians and genetic counselors working with patients, not with computational algorithms.\nFoundation model predictions support this process by improving the efficiency and accuracy of variant prioritization, reducing the number of VUS through more informative computational evidence, identifying potentially actionable variants in previously overlooked genomic regions, and enabling rapid assessment of novel variants not previously observed.\nThe interpretive report should convey both what computational predictions indicate and the uncertainty that remains. Clinicians must understand that even highly accurate models make errors, that predictions may be less reliable for underrepresented populations or unusual variant types, and that computational evidence is one component of a comprehensive assessment. Shared decision-making with patients should acknowledge these limitations while conveying the best current understanding.\n\n\n26.6.3 Regulatory and Ethical Considerations\nClinical use of foundation model predictions raises regulatory questions addressed more fully in Section 29.1. In the United States, laboratory-developed tests using computational predictions fall under CLIA oversight, with additional FDA jurisdiction increasingly asserted for software as a medical device. European regulations under IVDR impose their own requirements. Laboratories must navigate this evolving landscape while ensuring that clinical utility keeps pace with regulatory compliance.\nEquity concerns deserve particular attention. Foundation models trained predominantly on data from individuals of European ancestry may perform less well for other populations (Section 22.2.1 for detailed discussion of ancestry-related performance disparities; Section 29.5.2 for equity implications). If computational predictions systematically provide less informative evidence for underrepresented groups, this could widen existing disparities in diagnostic yield and clinical care. Ongoing efforts to diversify training data and evaluate performance across ancestries are essential for equitable clinical deployment.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch26-rare-disease.html#sec-ch26-partnership",
    "href": "part_6/p6-ch26-rare-disease.html#sec-ch26-partnership",
    "title": "26  Rare Disease Diagnosis",
    "section": "26.7 Interpretive Partnership",
    "text": "26.7 Interpretive Partnership\nFoundation models transform variant interpretation by providing more accurate, comprehensive, and fine-grained predictions than previous computational approaches. Missense pathogenicity can be estimated proteome-wide with unprecedented accuracy. Regulatory variant effects can be predicted across tissues and cell types. Splicing disruption can be quantified with clinical-grade precision. These capabilities accelerate the diagnostic odyssey, enabling faster and more confident resolution for patients who have often waited years for answers.\nYet foundation models do not replace human judgment in clinical genetics. They do not understand phenotypes, family structures, or therapeutic implications. They do not weigh the psychological impact of uncertain results or navigate the ethical complexities of predictive testing in unaffected relatives. They provide evidence that must be integrated within clinical frameworks designed around human decision-making, alongside family history, physical examination, prior testing, and the accumulated wisdom of clinical experience.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 26.6: [Enhancing] Partnership diagram. FM Capabilities: Pattern recognition at scale, proteome-wide prediction, cross-gene generalization, consistent scoring, speed. Human Expert Capabilities: Clinical context, family reasoning, therapeutic implications, ethical navigation, communication, judgment under uncertainty. Partnership Zone: Efficient prioritization, evidence integration, quality assurance, continuous improvement. What remains human: Final classification, clinical communication, counseling, ethical judgment. Key insight: FMs accelerate diagnostic odyssey; human judgment essential for clinical decisions.\n\n\n\nThe productive framing positions foundation models as partners in interpretation: computational systems that handle pattern recognition at scales beyond human capacity, freeing clinical experts to focus on integration, communication, and the decisions where human judgment remains essential. This partnership model, rather than replacement or autonomy, defines the path forward for genomic foundation models in rare disease diagnosis.\n\n\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F. Scott, and Ada Hamosh. 2015. “OMIM.org: Online Mendelian Inheritance in Man (OMIM®), an Online Catalog of Human Genes and Genetic Disorders.” Nucleic Acids Research 43 (D1): D789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAndré, Fabrice, Eva Ciruelos, Gabor Rubovszky, Mario Campone, Sibylle Loibl, Hope S. Rugo, Hiroji Iwata, et al. 2019. “Alpelisib for PIK3CA-Mutated, Hormone Receptor–Positive Advanced Breast Cancer.” New England Journal of Medicine 380 (20): 1929–40. https://doi.org/10.1056/NEJMoa1813904.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nBergquist, Timothy, Sarah L. Stenton, Emily A. W. Nadeau, Alicia B. Byrne, Marc S. Greenblatt, Steven M. Harrison, Sean V. Tavtigian, et al. 2025. “Calibration of Additional Computational Tools Expands ClinGen Recommendation Options for Variant Classification with PP3/BP4 Criteria.” Genetics in Medicine 27 (6): 101402. https://doi.org/10.1016/j.gim.2025.101402.\n\n\nBrnich, Sarah E., Ahmad N. Abou Tayoun, Fergus J. Couch, Garry R. Cutting, Marc S. Greenblatt, Christopher D. Heinen, Dona M. Kanavy, et al. 2019. “Recommendations for Application of the Functional Evidence PS3/BS3 Criterion Using the ACMG/AMP Sequence Variant Interpretation Framework.” Genome Medicine 12 (1): 3. https://doi.org/10.1186/s13073-019-0690-2.\n\n\nChapman, Paul B., Axel Hauschild, Caroline Robert, John B. Haanen, Paolo Ascierto, James Larkin, Reinhard Dummer, et al. 2011. “Improved Survival with Vemurafenib in Melanoma with BRAF V600E Mutation.” New England Journal of Medicine 364 (26): 2507–16. https://doi.org/10.1056/NEJMoa1103782.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nFindlay, Gregory M., Riza M. Daza, Beth Martin, Melissa D. Zhang, Anh P. Leith, Molly Gasperini, Joseph D. Janizek, Xingfan Huang, Lea M. Starita, and Jay Shendure. 2018. “Accurate Classification of BRCA1 Variants with Saturation Genome Editing.” Nature 562 (7726): 217–22. https://doi.org/10.1038/s41586-018-0461-z.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKong, Augustine, Michael L. Frigge, Gisli Masson, Soren Besenbacher, Patrick Sulem, Gisli Magnusson, Sigurjon A. Gudjonsson, et al. 2012. “Rate of de Novo Mutations and the Importance of Father’s Age to Disease Risk.” Nature 488 (7412): 471–75. https://doi.org/10.1038/nature11396.\n\n\nLynch, Thomas J., Daphne W. Bell, Raffaella Sordella, Sarada Gurubhagavatula, Ross A. Okimoto, Brian W. Brannigan, Patricia L. Harris, et al. 2004. “Activating Mutations in the Epidermal Growth Factor Receptor Underlying Responsiveness of Non–Small-Cell Lung Cancer to Gefitinib.” New England Journal of Medicine 350 (21): 2129–39. https://doi.org/10.1056/NEJMoa040938.\n\n\nNguengang Wakap, Stéphanie, Deborah M. Lambert, Annie Olry, Charlotte Rodwell, Charlotte Gueydan, Valérie Lanneau, Daniel Murphy, Yann Le Cam, and Ana Rath. 2020. “Estimating Cumulative Point Prevalence of Rare Diseases: Analysis of the Orphanet Database.” European Journal of Human Genetics 28 (2): 165–73. https://doi.org/10.1038/s41431-019-0508-0.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel, Sean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022. “Calibration of Computational Tools for Missense Variant Pathogenicity Classification and ClinGen Recommendations for PP3/BP4 Criteria.” American Journal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie Gastier-Foster, Wayne W. Grody, et al. 2015. “Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology.” Genetics in Medicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nTate, John G, Sally Bamford, Harry C Jubb, Zbyslaw Sondka, David M Beare, Nidhi Bindal, Harry Boutselakis, et al. 2019. “COSMIC: The Catalogue Of Somatic Mutations In Cancer.” Nucleic Acids Research 47 (D1): D941–47. https://doi.org/10.1093/nar/gky1015.\n\n\nTavtigian, Sean V., Marc S. Greenblatt, Steven M. Harrison, Robert L. Nussbaum, Snehit A. Prabhu, Kenneth M. Boucher, and Leslie G. Biesecker. 2018. “Modeling the ACMG/AMP Variant Classification Guidelines as a Bayesian Classification Framework.” Genetics in Medicine 20 (9): 1054–60. https://doi.org/10.1038/gim.2017.210.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html",
    "href": "part_6/p6-ch27-drug-discovery.html",
    "title": "27  Drug Discovery",
    "section": "",
    "text": "27.1 Genetic Foundation of Target Selection\nMore than 90% of drug candidates that enter clinical trials fail. They fail because they targeted the wrong gene. They fail because the patient population was too heterogeneous for a single mechanism to succeed. They fail because preclinical models predicted efficacy that did not translate to humans. They fail because safety signals emerged only at scale. The pharmaceutical industry spends billions of dollars on programs that will not produce approved therapies, and the cost of this attrition propagates to the drugs that do succeed: higher prices, longer development timelines, and reduced investment in diseases with smaller markets. The fundamental bottleneck is not generating drug candidates but identifying, early in the process, which targets and which patients offer the highest probability of success.\nHuman genetics provides one of the strongest predictors of clinical success. Targets with genetic support succeed in trials at roughly twice the rate of targets without such support [Citation Needed]; human mutations that cause phenotypes resembling the therapeutic goal provide direct evidence that modulating the target affects the disease. Yet exploiting this signal is harder than it sounds. Genome-wide association studies have identified thousands of disease-associated loci, but most point to noncoding regions where the causal gene is unclear. Even when a gene is implicated, the direction of effect often remains ambiguous. Translating a statistical association into a validated therapeutic target typically requires a decade of work and hundreds of millions of dollars in follow-up studies.\nGenomic foundation models offer a path through this translational bottleneck. Rather than treating each target identification program as a de novo effort, foundation models encode biological knowledge learned across millions of sequences into reusable representations. A variant effect predictor can score any missense mutation’s functional impact without task-specific retraining. A regulatory model can predict expression consequences of noncoding variants across tissues. Network models can propagate genetic signals to identify pathway relationships invisible in single-gene analyses. These capabilities connect to drug discovery through target prioritization and genetic validation, network-aware approaches that identify modules and repurposing opportunities, foundation model-guided functional genomics screens, and biomarker development for patient stratification.\nHuman genetics provides uniquely causal evidence for target selection. Unlike expression correlations or pathway membership, genetic associations reflect the consequences of lifelong modulation of gene activity in human populations. Multiple analyses over the past decade have demonstrated that genetically supported targets succeed in clinical trials at roughly twice the rate of targets without genetic evidence [Citation Needed]. Targets implicated by Mendelian disease genetics, genome-wide association study (GWAS) hits, or functional variants show higher probabilities of success in phase II and III trials compared to targets selected through other means.\nThis empirical observation motivates building pipelines where genetic architecture serves as a first-class input to target identification. Genomic foundation models extend this logic by providing richer biological context and enabling transfer learning across diseases and modalities. Rather than simple “variants near gene X,” foundation models encode regulatory architecture, chromatin state, three-dimensional genome interactions, cell-type specificity, and perturbation responses. A single model trained on diverse genomic and multi-omic data can be reused for multiple diseases and therapeutic areas, analogous to how language models transfer across domains.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-genetic-foundation",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-genetic-foundation",
    "title": "27  Drug Discovery",
    "section": "",
    "text": "FIGURE PLACEHOLDER\n\n\n\n\nFigure 27.1: [Essential] Pipeline diagram. Steps: (1) GWAS summary statistics (Manhattan plot); (2) Fine-mapping (credible sets, posteriors); (3) FM scoring (coding: AlphaMissense, GPN-MSA; regulatory: Enformer, Borzoi); (4) Variant-to-gene mapping (coding direct; noncoding via Hi-C, eQTL, enhancer-gene); (5) Target ranking (genetic evidence, druggability, safety, mechanism clarity). Output: Ranked target list with evidence, effect sizes, fine-mapping probabilities, predicted mechanism, constraint/druggability. Key insight: FMs enrich genetic evidence with mechanistic context.\n\n\n\n\n\n27.1.1 From Variant-Level Predictions to Gene-Level Evidence\nDrug discovery teams rarely care about individual variants per se; they care about genes and pathways. The fundamental challenge in target identification is therefore aggregating variant-level information into gene-level evidence that can guide target selection.\nConsider a typical workflow. Starting from GWAS summary statistics (see Section 2.3.2), statistical fine-mapping methods identify credible sets of potentially causal variants at each locus (see Section 3.4). Sequence-based foundation models then score each candidate variant for regulatory or coding impact. Protein-centric variant effect predictors such as AlphaMissense, GPN-MSA, and the missense components of AlphaGenome combine protein language models, structural information, and evolutionary conservation to assess coding variants (Section 12.1; Section 14.2) (Cheng et al. 2023; Benegas et al. 2024; Avsec, Latysheva, and Cheng 2025; Brandes et al. 2023). Regulatory foundation models including Enformer, Borzoi, and long-context DNA language models predict the consequences of noncoding variants on chromatin accessibility, transcription factor binding, and gene expression (Section 13.2; Section 14.3.2).\nThe critical step is connecting variants to genes. For coding variants, this mapping is straightforward: the variant lies within a gene’s coding sequence, and protein-level scores directly inform that gene’s candidacy. For noncoding variants, the mapping requires integrating chromatin conformation data (Hi-C, promoter-capture Hi-C), enhancer-gene predictions from models like Enformer, and expression quantitative trait locus (eQTL) data that empirically links variants to gene expression changes (see Section 2.5). Fine-mapping approaches such as MIFM can help distinguish truly causal regulatory variants from correlated passengers, tightening the map from GWAS locus to variant to target gene (Section 3.4) (Wu et al. 2024; Rakowski and Lippert 2025).\nGene-level aggregation proceeds by summarizing variant effects across all variants linked to each gene. For a given gene, this summary might include the burden of predicted loss-of-function variants in cases versus controls, the strongest regulatory variant effect sizes predicted by foundation models, constraint metrics indicating the gene’s intolerance to damaging variation (see Section 2.2.3), and pleiotropy scores reflecting associations with other traits that might indicate safety liabilities or broader biological importance. From a foundation model perspective, the core idea is to treat gene-level evidence as an aggregation problem over high-dimensional variant embeddings. Rather than manually defining a handful of summary statistics, variant embeddings and predicted functional profiles can feed into downstream models that learn which patterns matter most for disease.\n\n\n27.1.2 Linking Genetics to Target Safety and Efficacy\nClassical human genetics has established several heuristics for target selection that foundation models can reinforce and extend. Human knockout individuals, people carrying biallelic loss-of-function variants, provide natural experiments on the consequences of gene inactivation. Protective variants that reduce disease risk suggest the directionality of therapeutic intervention: partial inhibition of a protein may be beneficial rather than harmful. Pleiotropy, meaning associations with many unrelated traits, may signal safety liabilities if modulating a target affects multiple physiological systems (see Section 3.8).\nFoundation models sharpen these assessments. Fine-mapping methods combined with regulatory foundation models can distinguish causal variants from those merely in linkage disequilibrium with causal variants (see Section 3.3). Variant effect scores from protein and regulatory models approximate effect sizes, helping differentiate subtle modulators from catastrophic loss-of-function mutations. Multi-task predictions across chromatin marks, transcription factor binding, expression, and splicing provide mechanistic hypotheses for how risk loci affect biology, moving beyond statistical association toward functional understanding.\nThe output of this workflow is a ranked list of candidate targets with structured evidence that can be compared across diseases and programs. Each target comes annotated with the strength of genetic evidence (effect sizes, fine-mapping probabilities), predicted mechanisms (coding versus regulatory, affected tissues), constraint information (tolerance to loss-of-function, essentiality), and druggability features (protein family, structural information, existing ligands).",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-network-discovery",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-network-discovery",
    "title": "27  Drug Discovery",
    "section": "27.2 Network-Aware Target Discovery and Repurposing",
    "text": "27.2 Network-Aware Target Discovery and Repurposing\nIndividual genes do not operate in isolation. Proteins interact in complexes, genes participate in pathways, and regulatory networks coordinate cellular responses. Even with excellent variant-to-gene mapping, the biological context of a target shapes its therapeutic potential. Network-aware approaches propagate genetic signals through these relational structures to identify modules, bottleneck nodes, and repurposing opportunities.\n\n27.2.1 Propagating Genetic Signals Through Networks\nThe basic intuition is that GWAS signals concentrated in a pathway or protein interaction module provide stronger evidence than isolated hits. A single gene with modest genetic support but tight functional connections to several strongly implicated genes may be a more attractive target than an isolated hit with stronger statistics but unclear biology.\nNetwork-based methods integrate noncoding GWAS loci, regulatory annotations, and protein-protein interactomes to identify disease genes and evaluate drug repurposing opportunities in complex diseases. Graph neural network architectures (Section 18.2.2) can learn to propagate genetic evidence through interaction networks, scoring each gene not just by its direct genetic association but by its network context. The key methodological insight is that genes can be embedded jointly with their network neighbors, allowing the model to capture how genetic perturbations in one gene might affect functionally related genes.\nFoundation model representations enhance these network approaches. Instead of representing each gene by a sparse vector of annotations, genes can be embedded using features derived from protein language models (Section 12.1.3), regulatory foundation models (Section 13.2), and expression-based cell state encoders (Section 16.2). These embeddings can then be used as node features in graph neural networks, enabling network-aware target prioritization (Section 18.3). These rich representations capture functional similarity beyond what interaction databases alone can provide. Two genes with similar protein language model embeddings likely share functional properties even if no direct interaction has been catalogued.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 27.2: [High] Network visualization. Nodes: Genes/proteins; colors: GWAS signal strength; sizes: FM embedding similarity to disease genes. Key concepts: (1) Direct GWAS hits (strong statistical evidence); (2) Network neighborhood (connected to hits, may be better targets); (3) Pathway enrichment (multiple signals converge); (4) Bottleneck nodes (druggable connecting disease modules). Repurposing application: Drug targets marked; drugs near disease genes in embedding space → repurposing candidates. Caution: Proximity ≠ causation; MR needed.\n\n\n\n\n\n27.2.2 Drug Repurposing Through Shared Representations\nThe same framework enables systematic drug repurposing. By representing drugs via their targets, gene expression signatures, and phenotypic effects, and representing diseases via their genetic architecture and molecular signatures, models can score drug-disease pairs based on representation similarity. If a drug’s target sits near genetically implicated genes in representation space, or if the drug’s expression signature opposes the disease signature, that drug becomes a repurposing candidate.\nNetwork proximity provides one concrete operationalization: drugs whose targets are enriched near disease-risk genes, as measured by network diffusion or embedding similarity, may have therapeutic potential for that disease [Citation Needed]. Several retrospective analyses have found that such proximity predicts reduced disease incidence among users of particular drugs, though prospective validation remains limited.\nThe caution here is fundamental: representation similarity is not causation. A drug that appears near disease genes in embedding space might act through that mechanism, or the association might reflect confounding by indication, survivorship bias, or other artifacts of observational data (see Chapter 22). Network-based repurposing generates hypotheses; Mendelian randomization, natural experiments, and prospective trials must test them.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-dti-prediction",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-dti-prediction",
    "title": "27  Drug Discovery",
    "section": "27.3 Drug-Target Interaction Prediction",
    "text": "27.3 Drug-Target Interaction Prediction\nBeyond identifying disease-relevant targets, foundation models can predict which molecules might modulate those targets. Drug-target interaction prediction sits at the interface between genomic and chemical foundation models, using biological representations to inform molecular design decisions.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 27.3: [High] Dual-encoder architecture. Components: (1) Target Encoder (protein sequence → ESM-2 → protein embedding capturing structure, function, binding); (2) Molecule Encoder (SMILES/graph → chemical FM → molecule embedding); (3) Interaction Predictor (combine embeddings → predict binding affinity). Applications: Novel target screening, off-target prediction, selectivity profiling. Proteome-scale visualization: 2D protein embeddings; drug binding profile heatmap overlay; intended target and off-targets. Safety integration: Off-targets with CV expression flagged. Key insight: FM embeddings enable binding prediction without experimental structures.\n\n\n\n\n27.3.1 Representing Targets for Binding Prediction\nTraditional drug-target interaction methods rely on sequence similarity, structural docking, or chemical fingerprint matching. Foundation model approaches replace these hand-crafted features with learned representations. Protein language model embeddings from ESM-2 or similar architectures capture evolutionary and structural information that correlates with binding site properties (Section 12.1.3; Section 12.1) (Lin et al. 2022). Ligand representations from chemical foundation models encode molecular properties relevant to binding affinity and selectivity.\nThe prediction task becomes: given a protein embedding (derived from a protein language model) and a molecule embedding (derived from a chemical language model or graph neural network), predict binding affinity or interaction probability. These models can be trained on large databases of known drug-target interactions and binding affinities, then applied to predict interactions for novel targets or molecules [Citation Needed].\nFor genomics-focused applications, the protein representation is the critical contribution. A target identified through genetic validation can be immediately embedded using protein foundation models, enabling binding prediction without waiting for experimental structures or extensive biochemical characterization. This acceleration is particularly valuable for understudied targets where structural data is sparse.\n\n\n27.3.2 Selectivity and Off-Target Prediction\nThe same framework extends to selectivity prediction. By comparing a drug’s predicted binding across all proteins in a proteome-scale embedding space, models can flag potential off-target interactions. A compound predicted to bind its intended target but also showing high affinity for kinases with cardiovascular expression, for example, might warrant additional safety characterization before advancement.\nFoundation model representations capture protein family relationships and binding site similarities that inform off-target predictions. Two proteins with similar embeddings likely share structural features that could bind similar molecules. This information, combined with tissue expression data (Section 2.5.1) and phenome-wide association data linking genes to thousands of traits (Section 3.8.2), enables preliminary safety profiling before expensive preclinical experiments.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-toxicity",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-toxicity",
    "title": "27  Drug Discovery",
    "section": "27.4 Toxicity Prediction from Genomic Context",
    "text": "27.4 Toxicity Prediction from Genomic Context\nSafety failures represent a major cause of drug attrition, particularly in late-stage development where failures are most expensive. Genomic information provides several routes to earlier toxicity prediction.\n\n27.4.1 Genetic Evidence of Target Liabilities\nHuman genetic data offers direct evidence of target-related toxicity. If loss-of-function variants in a target gene associate with adverse phenotypes in biobank populations, those phenotypes may emerge as on-target toxicities during therapeutic inhibition. Phenome-wide association studies across biobanks link genes to thousands of traits, from laboratory values to disease diagnoses to imaging features (see Section 3.8). A target strongly associated with QT prolongation, hepatotoxicity markers, or nephrotoxicity phenotypes warrants careful safety evaluation.\nFoundation models enhance this analysis by providing more accurate variant effect predictions (distinguishing true loss-of-function from benign variants) and by integrating across evidence types (see Chapter 14). A gene might show modest individual associations with several safety-relevant traits that, when aggregated using foundation model representations, reveal a concerning pattern.\n\n\n27.4.2 Expression-Based Toxicity Prediction\nTissue expression patterns inform toxicity risk. A target expressed highly in hepatocytes poses greater hepatotoxicity risk than one expressed primarily in the target tissue. Single-cell foundation models (Section 18.4.5; Section 19.6.2) provide cell-type-resolved expression information, enabling predictions about which cell types might be affected by target modulation.\nMore sophisticated approaches use perturbation-response models trained on CRISPR screens and drug treatment data. Given a target knockdown or drug treatment, these models predict transcriptomic responses across cell types. If the predicted response signature resembles known toxicity signatures (mitochondrial stress, DNA damage response, inflammatory activation), that prediction informs safety risk assessment [Citation Needed].\n\n\n27.4.3 Integrating Genomic Context with Chemical Properties\nUltimate toxicity prediction requires integrating target information with compound properties. The same molecule might be safe or toxic depending on its selectivity profile, metabolism, and tissue distribution. Foundation models provide the biological context (target properties, off-target predictions, expression patterns) that complements chemical property predictions (metabolism, reactivity, distribution) in integrated toxicity models.\nThe field remains early: prospective validation of foundation model toxicity predictions against clinical outcomes is limited. Current utility lies in prioritizing compounds for experimental toxicity testing and in generating hypotheses about liability mechanisms, rather than replacing traditional safety pharmacology.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-functional-screens",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-functional-screens",
    "title": "27  Drug Discovery",
    "section": "27.5 Functional Genomics Screens and Perturbation Models",
    "text": "27.5 Functional Genomics Screens and Perturbation Models\nWhile human genetics offers observational evidence, drug discovery relies heavily on perturbation experiments that directly test hypotheses. CRISPR knockout and knockdown screens, saturation mutagenesis of protein domains, massively parallel reporter assays (MPRAs) of regulatory elements, and Perturb-seq experiments linking genetic perturbations to single-cell transcriptomic responses all generate data that both validates targets and improves models.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 27.4: [High] Perturbation-response workflow. Components: (1) Perturb-seq experiment (pooled CRISPR + scRNA-seq; each cell has perturbation + transcriptome); (2) Perturbation signatures (knockdown → expression profile change; FM learns mapping); (3) Disease signature (patient samples characteristic changes); (4) Perturbation matching (find perturbations reversing disease signature; knockdown X reverses disease → X is candidate; drug Y similar to knockdown X → Y targets X mechanism). Visualization: Expression heatmap (genes × perturbations); disease signature overlay. Lab-in-the-loop: Screen → update model → design next screen; active learning.\n\n\n\n\n27.5.1 Designing Informative Perturbation Libraries\nTraditional pooled screens use simple design rules: one guide RNA per exon, or tiling a regulatory region at fixed spacing. Foundation models enable smarter library design by providing priors over which perturbations are likely to be informative.\nVariant effect scores from protein foundation models can prioritize which amino acid positions are most likely to reveal functional differences when mutated. Positions predicted to be highly constrained and structurally important warrant more thorough coverage than positions predicted to be mutationally tolerant (Section 14.2). Regulatory foundation models can highlight which enhancer or promoter regions are predicted to have the largest expression effects in the cell type of interest, focusing screening effort on high-impact regions (Section 13.2; Section 14.3.2).\nBeyond prioritization, foundation models can guide combinatorial design. Model uncertainty, the degree to which a model is confident in its predictions, identifies regions where experimental data would be most informative (Section 23.1.2; Section 23.7). Positions where the model makes uncertain predictions are precisely those where experimental measurement adds the most value. Active learning strategies that select perturbations to maximize expected information gain can dramatically improve the efficiency of screening campaigns.\n\n\n27.5.2 Perturb-seq and Transcriptomic Readouts\nPerturb-seq experiments combine pooled genetic screens with single-cell RNA sequencing, linking each perturbation to its transcriptomic consequences. These data are exceptionally rich: rather than a single phenotypic readout (viability, fluorescence), each cell provides a high-dimensional expression profile reflecting how the perturbation affected cellular state.\nFoundation models trained on Perturb-seq data learn to predict transcriptomic responses to genetic perturbations (Section 16.3 for architectural details). Given a gene knockdown, these models predict which other genes will be up- or down-regulated, providing a functional signature for each target. Similar signatures suggest similar biology; divergent signatures suggest distinct mechanisms even for targets in the same pathway.\nThe drug discovery application is perturbation matching. Given a disease state characterized by a transcriptomic signature (perhaps derived from patient samples or disease models), foundation models can identify perturbations whose predicted response signature would move the system toward a healthier state. If knocking down gene X reverses the disease signature, X becomes a candidate therapeutic target. If treating with drug Y produces a signature similar to knocking down gene X, Y becomes a candidate molecule for that mechanism [Citation Needed].\n\n\n27.5.3 Closing the Loop: Lab-in-the-Loop Refinement\nIterative refinement represents a particularly valuable application of foundation models in functional genomics. Screen outcomes provide labeled examples that can fine-tune sequence-to-function models for the specific biological context of interest (Section 9.6 for transfer learning strategies).\nConsider an MPRA that assays thousands of enhancer variants for their effects on reporter gene expression in a disease-relevant cell type. These sequence-activity pairs directly supervise expression-prediction foundation models, dramatically improving their accuracy for that locus and tissue. The refined model then makes better predictions for the next round of experiments, suggesting which additional variants would be most informative to test.\nThis lab-in-the-loop cycle accelerates discovery while improving model accuracy in disease-relevant regions of sequence space. Foundation models provide the prior (general knowledge about sequence-function relationships); experiments provide the likelihood (specific measurements in the system of interest); and the posterior (updated model) makes better predictions for subsequent experiments.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-biomarkers",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-biomarkers",
    "title": "27  Drug Discovery",
    "section": "27.6 Biomarker Development and Patient Stratification",
    "text": "27.6 Biomarker Development and Patient Stratification\nEven when a target is well validated, many programs fail in clinical trials because the right patients were not enrolled, the right endpoints were not measured, or the treatment effect was diluted across a heterogeneous population. Foundation model representations provide new tools for defining and validating biomarkers.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 27.5: [Enhancing] Pipeline from features to trial. Stages: (1) FM features (variant embeddings, regulatory predictions, pathway scores; multi-omic); (2) Signature development (supervised learning on retrospective cohorts; predict response, toxicity, progression); (3) Validation (external cohorts, cross-ancestry, calibration); (4) Trial enrichment (select likely responders; smaller trial, higher effect); (5) Companion diagnostic (regulatory pathway; clinical deployment alongside therapy). Example: Genomic + transcriptomic signature predicts PARP inhibitor response; 3× efficiency improvement.\n\n\n\n\n27.6.1 Foundation Model Features for Stratification\nClassical polygenic scores summarize additive effects of common variants on disease risk (see Section 3.5). These scores have proven useful for patient enrichment in cardiovascular and metabolic disease trials, selecting patients at highest genetic risk who might benefit most from intervention [Citation Needed]. Deep learning methods extend this approach by learning nonlinear genotype-phenotype mappings that capture interactions and nonadditive effects.\nFoundation models enhance polygenic prediction in several ways. Instead of using raw genotypes as inputs, models can use variant effect scores, regulatory predictions, or gene-level embeddings derived from foundation models. This captures biological context that simple genotypes miss. Models trained on variant embeddings rather than binary genotype calls can capture subtle differences between variants at the same position, distinguishing a mildly damaging missense from a severely damaging one even when both are heterozygous.\nTransfer across populations represents a particular strength (see Section 3.7). Foundation models trained on diverse genomes provide representations that may generalize more robustly across ancestries than models trained on individual cohorts. Fine-mapping-aware approaches that use foundation model features can reduce dependence on linkage disequilibrium patterns that vary across populations, potentially improving the portability of genetic risk predictors.\n\n\n27.6.2 Multi-Omic Biomarker Discovery\nBeyond germline genetics, drug development increasingly leverages somatic genomics, transcriptomics, proteomics, and other molecular readouts. Tumor sequencing combined with expression profiling characterizes the molecular landscape of each patient’s cancer. Single-cell multiome data (RNA plus ATAC) reveal cell-state heterogeneity that bulk assays miss (see Chapter 16).\nFoundation models trained on these data types provide embeddings that capture patient-level molecular profiles. Set-based architectures that treat each patient’s genomic features as a set (rather than assuming fixed feature positions) can handle the heterogeneity of tumor genomes, where different patients have different mutations (Section 19.2). Gene regulatory network inference models trained on atlas-scale single-cell data can extract pathway activity scores that serve as mechanistically interpretable biomarkers.\nThe key shift is that biomarkers are no longer limited to a handful of hand-picked variants or expression markers. They become functions over high-dimensional genomic and multi-omic embeddings, learned in a data-driven way yet grounded in biological priors from foundation models. A biomarker might be a region of embedding space corresponding to patients with particular molecular subtypes, defined by the model rather than by manual curation.\n\n\n27.6.3 Trial Design and Endpoint Selection\nFoundation model predictions inform trial design at multiple stages. Patient enrichment uses genetic risk scores or molecular subtypes to select patients most likely to respond, increasing statistical power and reducing required sample sizes. Adaptive designs use intermediate biomarker responses to modify randomization or dosing during the trial. Endpoint selection uses molecular signatures to define pharmacodynamic biomarkers that indicate target engagement, supporting dose selection and early efficacy signals.\nRegulatory agencies increasingly accept genomic biomarkers for patient selection in oncology and are developing frameworks for other therapeutic areas [Citation Needed]. The challenge is validation: demonstrating that foundation model predictions actually stratify patient outcomes requires prospective trials or well-designed retrospective analyses with appropriate controls for confounding (see Chapter 22).",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-industry-workflows",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-industry-workflows",
    "title": "27  Drug Discovery",
    "section": "27.7 Industry Workflows and Infrastructure",
    "text": "27.7 Industry Workflows and Infrastructure\nFor pharmaceutical and biotechnology organizations, the challenge is not whether they can access a foundation model but how to integrate these models into existing data platforms, governance structures, and decision-making processes.\n\n27.7.1 Building Model Infrastructure\nIn mature organizations, foundation models should be treated as shared infrastructure rather than ad hoc scripts developed by individual project teams. A well-organized model catalog contains DNA language models (Nucleotide Transformer, HyenaDNA, GENA-LM), sequence-to-function models (Enformer, Borzoi), and variant effect predictors (AlphaMissense, GPN-MSA, CADD) with documented capabilities, limitations, and appropriate use cases (see Appendix D for model reference).\nFeature services provide centralized APIs that accept variants, genomic intervals, or genes as input and return embeddings, predicted functional profiles, or risk features. Centralization enables consistency across programs and avoids redundant computation. Logging and versioning ensure that analyses can be reproduced even as models and data evolve.\nData governance maintains clear separation between models trained on public data versus sensitive internal cohorts. Internal data, including proprietary clinical trial data, patient samples, and collaborator contributions, requires careful handling. Guardrails define where internal data can be used for fine-tuning and how resulting models can be shared or published.\n\n\n27.7.2 Strategic Choices: Build, Buy, or Fine-Tune\nOrganizations face three strategic options when adopting foundation models. Using external models as-is offers low upfront cost and benefits from community benchmarking, but may not capture organization-specific populations, assays, or therapeutic areas. A model trained primarily on European ancestry populations may perform poorly on a company’s Asian-focused programs; a model trained on common cell lines may miss biology relevant to rare disease indications.\nFine-tuning open-source foundation models on internal data retains general representations while adapting to local data distributions (see Chapter 9 for fine-tuning approaches). This approach requires computational investment and careful privacy controls, but often provides the best balance of generality and specificity. A company with large internal biobank data can fine-tune a general variant effect predictor on that cohort, improving predictions for its patient populations without sacrificing the broad knowledge captured during pretraining.\nTraining bespoke internal models from scratch offers maximum control and allows alignment of pretraining objectives with specific use cases (see Chapter 8). A company focused on rare diseases might pretrain on sequences and phenotypes particularly relevant to that space. The cost is substantial: pretraining requires significant compute, data engineering, and machine learning expertise (see Appendix B). There is also risk of overfitting to narrow internal datasets if the pretraining corpus is not sufficiently diverse.\nIn practice, most organizations adopt hybrid strategies. They start with public foundation models for early exploration and non-sensitive applications, gradually fine-tune on internal data as value becomes clear, and reserve from-scratch training for cases where unique data assets justify the investment. Lightweight model-serving infrastructure handles latency-sensitive applications such as clinical decision support, while heavier offline systems support large-scale research workloads.\n\n\n27.7.3 Industry Context: Timelines and Decision Gates\nAcademic machine learning research optimizes benchmark performance; drug discovery optimizes probability of clinical and commercial success under time and resource constraints. Understanding industry context helps foundation model practitioners contribute effectively.\nDrug discovery programs progress through gates: target validation, candidate selection, investigational new drug filing, and clinical trial phases. Each gate requires specific evidence: biological rationale, efficacy data, safety data, manufacturing feasibility. Foundation model contributions must align with gate requirements. A beautiful embedding space is valueless if it cannot be translated into evidence that advances a program through its next gate.\nTimelines matter. A prediction that takes six months to validate experimentally may be worthless if the program decision must be made in three months. Foundation models that enable faster experiments (through better library design, prioritization, or interpretation) create more value than models that provide incrementally better predictions but require the same experimental timeline to validate.\nBiotechnology companies and pharmaceutical companies operate differently. Biotechs often focus on single programs with limited resources, prioritizing speed and risk-taking. Pharma companies manage portfolios across therapeutic areas, prioritizing consistency and scalability. Foundation model infrastructure that serves one context may not serve the other. A boutique biotech might prefer lightweight, single-purpose models; a large pharma might invest in comprehensive infrastructure serving dozens of programs.\n\n\n27.7.4 Intellectual Property and Data Considerations\nFoundation models raise new questions around intellectual property, data sharing, and regulatory expectations that organizations must navigate.\nModels trained on proprietary data can be valuable assets but are difficult to patent directly. The model architecture is typically based on published methods; the weights reflect training data that may include public and proprietary components. Downstream discoveries, including specific targets, biomarkers, and therapeutic hypotheses derived from foundation model analyses, are more clearly protectable but require careful documentation of inventive contribution.\nCollaborative model development across institutions may require federated learning or model-to-data paradigms, especially for patient-level data. Genomic data carries re-identification risk (see Section 29.3); sharing raw data, even for model training, requires appropriate consent and data use agreements. Federated approaches that train on local data without centralizing raw information can enable multi-institutional collaboration while respecting privacy constraints.\nFor regulatory submissions, foundation models used in drug development create documentation requirements. If a model informed target selection, patient stratification, or safety assessment, regulators may request information about model training, validation, and performance across subgroups (see Section 29.1). The confounding and interpretability challenges discussed in Chapter 22 and Chapter 24 become acute when models inform pivotal decisions in drug development. Clear documentation trails from model prediction to program decision support regulatory review.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-evaluation",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-evaluation",
    "title": "27  Drug Discovery",
    "section": "27.8 Evaluation and Validation",
    "text": "27.8 Evaluation and Validation\nEvaluating foundation model contributions to drug discovery requires carefully separating model performance from scientific and clinical validity. A model that achieves high benchmark scores may still fail to improve drug discovery outcomes; a model with modest benchmarks may provide actionable insights that advance programs.\n\n27.8.1 Benchmark Limitations\nMany published benchmarks draw targets and drugs from the same databases used to pretrain models, creating risk of leakage that inflates performance estimates (Section 21.4.4; Section 21.4). Repurposing success stories often rely on retrospective data mining with limited prospective validation. The ultimate test of a foundation model for drug discovery is whether it identifies targets that succeed in clinical trials, a test that takes years and confounds model contribution with countless other factors.\nConfounding pervades drug discovery data (see Chapter 22). Models trained on observational clinical and genomic data inherit confounders from those data. Drug-disease associations learned by foundation models may reflect treatment patterns rather than true causal relationships. Confounding by indication (sicker patients receive different treatments), survivorship bias (only patients who survived long enough enter certain analyses), and healthcare access patterns all threaten validity. Genetic instruments and careful epidemiologic designs remain essential for causal claims that foundation model predictions cannot provide alone.\n\n\n27.8.2 From Prediction to Validation\nFoundation model predictions are hypotheses, not conclusions. A target ranked highly by genetic evidence and foundation model scoring still requires experimental validation. The value of foundation models lies in prioritizing which hypotheses to test, not in replacing experimental testing.\nValidation strategies depend on the application. Target predictions can be validated through functional genomics screens that test whether predicted targets affect disease-relevant phenotypes. Biomarker predictions require retrospective validation on held-out cohorts or prospective validation in clinical trials. Repurposing predictions require real-world evidence analyses or prospective trials.\nThe timeline for validation matters. Some predictions can be tested in weeks (cell-based assays for target validation); others require years (clinical outcomes for biomarkers). Foundation model contributions should be assessed on timescales relevant to drug discovery decisions, not just immediate benchmark performance.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-molecular-design",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-molecular-design",
    "title": "27  Drug Discovery",
    "section": "27.9 Connections to Molecular Design",
    "text": "27.9 Connections to Molecular Design\nFoundation model representations connect target identification to downstream molecular design. The bridge between genomic and molecular foundation models typically involves using target context as conditioning signals for molecule generation. Gene-level embeddings from genomic foundation models, reflecting genetic evidence, tissue specificity, and network context, can condition chemistry models that propose molecules targeting that gene.\nMulti-modal foundation models jointly trained on DNA, RNA, proteins, structures, small molecules, and phenotypic readouts learn representations that span these modalities (see Chapter 19). Such models can predict not just whether a molecule binds a target, but how target modulation in a particular genetic context might affect cellular phenotypes. Closed-loop optimization uses genomic foundation models to predict target relevance and liability, chemistry and protein foundation models to propose molecules, and experimental feedback to update both model types in active learning cycles.\nThe detailed treatment of molecular design belongs to Chapter 28. From a target identification perspective, the key point is that genomic foundation models determine whether a target is worth pursuing; downstream models then optimize how to hit it. The investment in accurate target identification and validation pays dividends throughout the drug discovery pipeline by ensuring that optimization efforts focus on targets with the highest probability of clinical success.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-drug-discovery.html#sec-ch27-conclusion",
    "href": "part_6/p6-ch27-drug-discovery.html#sec-ch27-conclusion",
    "title": "27  Drug Discovery",
    "section": "27.10 Prioritization, Not Automation",
    "text": "27.10 Prioritization, Not Automation\nFoundation models connect to drug discovery not as replacements for experimental validation but as tools that reduce risk and focus resources. Target discovery workflows aggregate variant-level predictions into gene-level evidence, integrating fine-mapping, variant effect prediction, and regulatory modeling to prioritize candidates with strong genetic and mechanistic support. Network-aware approaches propagate genetic signals through protein and regulatory networks to identify druggable nodes and repurposing opportunities. Drug-target interaction prediction uses foundation model representations to assess binding, selectivity, and safety liabilities before synthesis. Functional genomics screens leverage foundation models for library design and iterative refinement. Biomarker development uses foundation model features for patient stratification and trial enrichment.\nThroughout these applications, the value proposition is acceleration and prioritization, not automation of discovery. Foundation models help identify the most promising targets, design the most informative experiments, and select the patients most likely to benefit. Programs that would have required years of hypothesis testing can reach the right target faster. Screens that would have required exhaustive enumeration can focus on high-priority candidates. The fundamental uncertainties of drug development remain: targets validated genetically may still fail in trials, predictions of binding may not translate to efficacy, and patients selected by biomarkers may still not respond. Foundation models reduce risk without eliminating it.\nSequence design (Chapter 28) extends these ideas from analysis to generation, where foundation models not only evaluate existing sequences but propose new ones optimized for therapeutic function. The transition from interpretation to design represents the frontier where foundation models become engines for creating biology, not just understanding it.\n\n\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray, Peter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping Improves Identification of Causal Variants.” Research Square, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html",
    "href": "part_6/p6-ch28-design.html",
    "title": "28  Sequence Design",
    "section": "",
    "text": "28.1 Design Formalism\nGenomic foundation models predict the consequences of sequence variation with increasing accuracy. A protein language model estimates whether a missense variant disrupts function. A regulatory model forecasts how a promoter mutation alters expression across cell types. These predictive capabilities represent genuine advances. Yet prediction alone cannot create a therapeutic protein that nature never evolved, design a promoter that drives expression only in diseased tissue, or engineer an mRNA vaccine against a novel pathogen. The gap between reading genomes and writing them defines one of the central challenges in translational biology: we can characterize biological sequences with unprecedented resolution, but translating that understanding into designed molecules remains largely empirical, expensive, and slow.\nThis asymmetry reflects a fundamental mismatch between what evolution produced and what therapeutics require. Evolution optimizes for reproductive fitness over geological timescales, producing sequences that satisfied survival constraints under ancestral conditions. Therapeutic applications demand sequences optimized for entirely different objectives: binding a specific epitope with high affinity, expressing at therapeutic levels in a particular tissue, or evading immune recognition while retaining function. The sequences we need often lie far from natural evolutionary trajectories, in regions of sequence space that foundation models have never observed during training. Navigating this terra incognita requires not just accurate oracles that score candidate sequences, but principled strategies for proposing, testing, and refining designs where model reliability is uncertain.\nFoundation models have begun to address this challenge by providing both generative priors over plausible sequences and differentiable oracles that guide optimization. Protein language models sample novel sequences respecting the statistical patterns of natural proteins. Structure-aware diffusion models generate backbones and sequences simultaneously, enabling design of proteins with specified geometries. Regulatory sequence models predict expression outcomes across thousands of candidate promoters, enabling gradient-based optimization toward desired activity profiles. When coupled with high-throughput experimental assays in closed-loop design cycles, these capabilities are transforming biological engineering.\nSequence design inverts the standard prediction problem. Where prediction maps from sequence to function (given sequence \\(x\\), estimate property \\(f(x)\\)), design maps from desired function to sequence (given target property \\(y^\\star\\), find sequence \\(x^\\star\\) such that \\(f(x^\\star) \\approx y^\\star\\)). This inversion is computationally challenging because biological sequence spaces are astronomically large. A 200-residue protein admits \\(20^{200}\\) possible sequences, vastly exceeding the number of atoms in the observable universe. Even a modest 500-base-pair regulatory element spans \\(4^{500}\\) possibilities. Exhaustive enumeration is impossible; intelligent search strategies are essential.\nThe design objective can take several mathematical forms depending on the application. Optimization problems seek sequences that maximize (or minimize) a scalar objective, such as finding \\(x^\\star = \\arg\\max_x f_\\theta(x)\\) where \\(f_\\theta\\) might represent predicted binding affinity, expression level, or stability. Conditional generation problems sample sequences from a distribution conditioned on desired properties, drawing \\(x \\sim p_\\theta(x \\mid y)\\) where \\(y\\) specifies structural constraints, functional requirements, or context. Constrained optimization problems combine objective maximization with explicit constraints, seeking \\(x^\\star = \\arg\\max_x f_\\theta(x)\\) subject to \\(c(x) \\leq 0\\), where constraints \\(c\\) might enforce GC content limits, avoid restriction sites, or maintain similarity to natural sequences.\nFoundation models contribute to design through multiple mechanisms. As generative priors, they assign higher probability to sequences resembling natural biology, regularizing optimization toward plausible regions of sequence space. As differentiable oracles, they enable gradient-based optimization where sequence modifications are guided by gradients of predicted properties. As embedding functions, they map discrete sequences into continuous spaces where interpolation and optimization become tractable (Section 5.6 for representation fundamentals; Section 8.1.2 for how pretraining shapes these spaces). The challenge lies in searching enormous combinatorial spaces while remaining within regimes where these model-based estimates remain reliable.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-formalism",
    "href": "part_6/p6-ch28-design.html#sec-ch28-formalism",
    "title": "28  Sequence Design",
    "section": "",
    "text": "FIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 28.1: [Essential] Two-direction diagram. Top (Prediction Problem): Input sequence \\(x \\rightarrow\\) model \\(f_\\theta \\rightarrow\\) property \\(f(x)\\); arrow left to right; “Given sequence, estimate function.” Bottom (Design Problem): Desired property \\(y^\\* \\rightarrow\\) search for \\(x^\\*\\) such that \\(f(x^\\*) \\approx y^\\* \\rightarrow\\) optimized sequence \\(x^\\*\\); arrow right to left; “Given function, find sequence.” Design formulations: Optimization \\(\\arg\\max_x f_\\theta(x)\\), conditional generation \\(x \\sim p_\\theta(x \\mid y)\\), constrained optimization. Scale callout: \\(20^{200}\\) protein possibilities, \\(4^{500}\\) regulatory; “Exhaustive impossible—intelligent search required.” FM roles: Generative prior, differentiable oracle, embedding function.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-protein-design",
    "href": "part_6/p6-ch28-design.html#sec-ch28-protein-design",
    "title": "28  Sequence Design",
    "section": "28.2 Protein Design with Language Models",
    "text": "28.2 Protein Design with Language Models\nProtein language models trained on evolutionary sequence databases (Chapter 12) have emerged as effective tools for protein design, providing both generative sampling capabilities and fitness estimation for candidate sequences. The masked language modeling objectives that enable fitness estimation are detailed in Section 8.1. The success of these approaches stems from a key insight: evolution has conducted billions of years of experiments on protein sequence space, and models trained on the surviving sequences implicitly encode constraints on what works.\n\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\n\nFigure 28.2: [Essential] Two-pathway comparison. Pathway A (Sequence-Based): Protein LM (ProGen, ESM-2) → generate by sampling/infilling → screen for properties → iterate with fitness-guided selection. Pathway B (Structure-Aware): RFdiffusion generates backbone from noise (conditioned on interface, topology, symmetry) → ProteinMPNN/ESM-IF inverse folds → multiple sequences, select by expression/stability. Comparison table: Prior knowledge, novel structures, compute, examples. Multi-objective: Binding, stability, expression, immunogenicity; Pareto frontier; FM oracles for each.\n\n\n\n\n28.2.1 Sequence Generation from Language Model Priors\nAutoregressive protein language models such as ProGen and ProtGPT2 generate novel protein sequences by sampling tokens sequentially from learned distributions (Madani et al. 2023; Ferruz, Schmidt, and Höcker 2022). Given a partial sequence, the model predicts probability distributions over the next amino acid, enabling iterative extension until a complete protein emerges. This generation process can be unconditional (sampling from the full learned distribution) or conditional on control signals such as protein family annotations, organism of origin, or functional keywords.\nThe quality of generated sequences depends critically on how closely the sampling distribution matches functional proteins. Sequences sampled at low temperature (more deterministic) tend to resemble common protein families but may lack novelty. Sequences sampled at high temperature (more stochastic) exhibit greater diversity but risk straying into nonfunctional regions. Practical design workflows often generate large libraries of candidates across temperature ranges, then filter using downstream oracles for structure, stability, or function.\nMasked language models like ESM-2 support design through a different mechanism. Rather than generating sequences de novo, these models estimate the probability of each amino acid at each position given the surrounding context. Design proceeds by iterative refinement: starting from an initial sequence, positions are masked and resampled according to model predictions, gradually shifting the sequence toward higher-likelihood regions. This Gibbs-sampling-like procedure can be biased toward specific objectives by combining model likelihoods with scores from downstream predictors.\nThe key advantage of protein language model-based design lies in data efficiency. Because models are pretrained on millions of natural sequences, they generalize to design tasks with minimal task-specific data. A model fine-tuned on a few hundred functional variants can propose candidates across sequence space, extrapolating far beyond the training examples. This contrasts with traditional directed evolution approaches that require extensive experimental screening to navigate sequence space.\n\n\n28.2.2 Structure-Aware Design with Diffusion Models\nStructure-aware design addresses a fundamental limitation of sequence-only approaches: proteins function through three-dimensional structures, and sequence optimization without structural guidance may produce sequences that fail to fold correctly. The advent of accurate structure prediction (AlphaFold2, ESMFold; Section 12.4; Section 12.4) enables new design paradigms that jointly consider sequence and structure.\nRFdiffusion exemplifies this approach by generating protein backbones through a diffusion process in three-dimensional coordinate space (Watson et al. 2023). Starting from random noise, the model iteratively denoises toward plausible backbone geometries, conditioned on design specifications such as target binding interfaces, desired topology, or symmetric assembly requirements. The resulting backbones represent novel structures not observed in nature but predicted to be physically realizable.\nConverting designed backbones to sequences requires inverse folding models that predict amino acid sequences likely to adopt a given structure. ProteinMPNN and ESM-IF operate on this principle, taking backbone coordinates as input and outputting probability distributions over sequences predicted to fold onto that backbone(Dauparas et al. 2022; Hsu et al. 2022). ESM-IF leverages the representations learned by ESM-2 to condition sequence generation on structural constraints, connecting the inverse folding task directly to the protein language model paradigm. The model can generate thousands of candidate sequences for a single backbone, enabling selection based on additional criteria such as expression likelihood or immunogenicity.\nThis two-stage pipeline (structure diffusion followed by inverse folding) has proven effective for creating novel proteins. Designed binders targeting challenging therapeutic targets, de novo enzymes with specified active site geometries, and symmetric protein assemblies with precise nanoscale dimensions have all been realized experimentally. [Citation Needed] The key insight is that structure provides a useful intermediate representation: rather than searching directly in the vast space of sequences, design proceeds through the more constrained space of physically realizable structures.\n\n\n28.2.3 Functional Conditioning and Multi-Objective Optimization\nReal therapeutic or industrial applications rarely optimize a single objective. A designed enzyme must not only be catalytically active but also stable at process temperatures, expressible in the production host, and resistant to proteolytic degradation. A therapeutic antibody must bind its target with high affinity while avoiding off-target interactions, maintaining solubility, and minimizing immunogenicity. These competing demands create multi-objective optimization problems where no single sequence optimizes all criteria simultaneously.\nMulti-objective design produces Pareto frontiers of solutions representing different trade-offs among objectives. A sequence might achieve exceptional binding affinity at the cost of reduced stability, while another balances moderate affinity with excellent developability properties. Practitioners must select among Pareto-optimal solutions based on application-specific priorities, and foundation models increasingly support this selection by providing diverse oracles across multiple property dimensions.\nFoundation models contribute to multi-objective design in three ways. Generative priors propose candidate sequences that satisfy basic plausibility constraints (foldability, expressibility) before optimization begins. Multiple differentiable oracles (for binding, stability, immunogenicity) enable gradient-based optimization toward Pareto frontiers. Embedding spaces support interpolation between sequences with different property profiles, enabling exploration of intermediate trade-offs. The combination of these capabilities makes foundation models central to modern protein design pipelines.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-regulatory-design",
    "href": "part_6/p6-ch28-design.html#sec-ch28-regulatory-design",
    "title": "28  Sequence Design",
    "section": "28.3 Regulatory Sequence Design",
    "text": "28.3 Regulatory Sequence Design\nGenomic foundation models trained on chromatin accessibility, transcription factor binding, and gene expression data enable design of synthetic regulatory elements with specified activity profiles. Unlike protein design where the sequence-to-function mapping operates through three-dimensional structure, regulatory design must account for the genomic and cellular context in which elements function. The functional genomics resources described in Section 2.4 provide training data for these models, while the interpretability methods from Section 24.1 inform design strategies by revealing which sequence features drive predictions.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 28.3: [High] Optimization workflow. Steps: (1) Initial sequence (natural promoter or random); (2) FM prediction (Enformer → activity across cell types); (3) Gradient computation (∂expression/∂position); (4) Sequence update (relaxed representation, gradient step, project back); (5) Multi-objective balancing (maximize in target tissue, minimize off-target, respect constraints). Output examples: Cell-type-specific enhancer, inducible promoter, compact element for gene therapy. Generative alternative sidebar: Diffusion or autoregressive; condition on cell type labels; diverse library for screening. Key insight: Same saliency for interpretation runs in reverse for design.\n\n\n\n\n28.3.1 Promoter and Enhancer Engineering\nMassively parallel reporter assays (MPRAs) have generated training data for models that predict expression levels from promoter and enhancer sequences (Section 2.4.4; Section 2.4.4) (Boer et al. 2020). These models learn sequence determinants of regulatory activity, including transcription factor binding sites, spacing constraints between elements, and context-dependent interactions. Once trained, the same models serve as oracles for design: by evaluating expression predictions across millions of candidate sequences, optimization algorithms can identify synthetic regulatory elements with desired properties.\nGradient-based design treats the sequence-to-expression model as a differentiable function. Starting from an initial sequence, gradients of predicted expression with respect to input positions indicate which mutations would increase (or decrease) activity. Because sequences are discrete while gradients are continuous, optimization requires relaxation strategies that operate on “soft” sequence representations before projecting back to discrete nucleotides. These approaches leverage the same saliency map computations used for model interpretation (Section 24.1.2), running the analysis in reverse to guide design rather than explain predictions.\nDesign objectives for regulatory elements extend beyond maximizing expression in a target context. Cell-type-specific enhancers should drive high expression in desired tissues while remaining inactive elsewhere. Inducible promoters should respond to specific signals while maintaining low basal activity. Compact regulatory elements are preferred for gene therapy applications where vector capacity is limited. These constraints transform simple optimization into multi-objective problems requiring careful balancing of competing requirements.\nGenerative models trained directly on regulatory sequences offer an alternative to optimization-based approaches. Autoregressive or diffusion models learn to sample novel enhancers and promoters that match the statistical properties of natural regulatory elements. Conditioning on cell type labels, chromatin state annotations, or other metadata enables generation of elements with targeted activity profiles. The advantage of generative approaches lies in their ability to produce diverse candidate libraries for experimental screening, rather than converging on a single optimized sequence that may exploit model artifacts rather than genuine biology.\n\n\n28.3.2 Splicing and RNA Processing Elements\nModels trained on splicing outcomes (SpliceAI and related architectures described in Chapter 6; see also Chapter 15 for RNA-specific foundation models) enable design of sequences that modulate RNA processing. Therapeutic applications include correcting pathogenic splice site mutations by strengthening weak splice sites or weakening aberrant ones, designing antisense oligonucleotides that redirect splicing to skip exons containing disease-causing mutations, and engineering alternative splicing outcomes to produce desired protein isoforms.\nThe design space for splicing elements encompasses splice site sequences themselves (the canonical GT-AG dinucleotides and surrounding intronic and exonic enhancers and silencers), branch point sequences, and auxiliary sequences that recruit splicing regulatory proteins. Foundation models that predict splicing patterns from local sequence context serve as oracles for evaluating candidate modifications, while gradient-based optimization identifies changes predicted to shift splicing toward therapeutic outcomes.\nDesign of splicing modulators requires particular attention to off-target effects. The splicing code is highly context-dependent, and sequence modifications intended to affect one splice site may inadvertently alter recognition of others. Genome-wide splicing models that predict effects across all splice sites provide essential off-target assessment, flagging candidate designs that would disrupt normal splicing at unintended locations.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-mrna-design",
    "href": "part_6/p6-ch28-design.html#sec-ch28-mrna-design",
    "title": "28  Sequence Design",
    "section": "28.4 mRNA Design and Optimization",
    "text": "28.4 mRNA Design and Optimization\nThe clinical success of mRNA vaccines has intensified interest in systematic approaches to mRNA sequence design. Unlike protein or regulatory element design where the primary challenge is achieving desired function, mRNA design must simultaneously optimize translation efficiency, molecular stability, immune evasion, and manufacturing tractability. Foundation models increasingly contribute to each of these objectives.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 28.4: [High] Multi-objective radar chart. Objectives (axes): Translation efficiency, mRNA stability, immunogenicity (minimize), manufacturing tractability, codon optimality. Design elements per objective: Translation (ribosome profiling models, avoid rare codons); Stability (UTR engineering, secondary structure); Immunogenicity (avoid TLR motifs, modified nucleosides); Manufacturing (GC content, complexity, yield). Trade-off visualization: Two candidate designs on same radar; Design A (high translation, moderate stability); Design B (balanced). UTR design callout: 5’ affects ribosome recruitment, 3’ affects stability/localization. Key insight: Codon synonymy creates vast space; FMs navigate tradeoffs.\n\n\n\n\n28.4.1 Codon Optimization Principles\nThe genetic code is degenerate: sixty-one sense codons encode twenty amino acids, meaning that any protein sequence can be encoded by many different mRNA sequences. These synonymous sequences differ in translation efficiency, mRNA stability, and immunogenicity despite producing identical proteins. Codon optimization exploits this redundancy to improve therapeutic mRNA performance.\nTraditional codon optimization relied on codon adaptation indices derived from highly expressed genes in target organisms. Codons frequently used in abundant proteins were assumed to be efficiently translated, leading to optimization strategies that maximize use of preferred codons. This approach oversimplifies the complex relationship between codon choice and expression. Translation elongation rate varies with codon-anticodon interactions, tRNA abundance, mRNA secondary structure, and ribosome queuing effects. Local codon context matters: rare codons following abundant ones may be translated efficiently, while runs of preferred codons can cause ribosome collisions.\nMachine learning models trained on ribosome profiling data and reporter assays have begun to capture these context-dependent effects. [Citation Needed] These models predict translation efficiency from sequence features including codon frequencies, local secondary structure, and amino acid properties. Using such models as oracles, optimization algorithms can search for mRNA sequences that maximize predicted translation while avoiding problematic sequence features. The resulting designs often differ substantially from simple codon-frequency optimization, incorporating rare codons at specific positions to optimize local translation dynamics.\n\n\n28.4.2 Stability Engineering and UTR Design\nmRNA stability in the cytoplasm determines the duration of protein production and thus the dose required for therapeutic effect. Stability is governed by multiple sequence features: the 5’ and 3’ untranslated regions (UTRs) that flank the coding sequence, the presence of destabilizing sequence motifs recognized by RNA-binding proteins, and secondary structures that protect against or expose the molecule to nucleases.\nUTR engineering represents a particularly active area of foundation model application. Natural UTRs contain binding sites for regulatory proteins and microRNAs, sequences that affect ribosome recruitment, and structures that influence mRNA localization and stability. Foundation models trained on expression data across diverse UTR sequences learn which features promote stability and efficient translation. [Citation Needed] Design algorithms then search for synthetic UTRs that maximize these properties while avoiding sequences that trigger immune recognition or rapid degradation.\nChemical modifications of mRNA (pseudouridine, N1-methylpseudouridine, and other nucleoside analogs) dramatically improve stability and reduce immunogenicity. These modifications alter the sequence-function relationship in ways that current foundation models, trained primarily on natural RNA, may not fully capture. Emerging models that incorporate modification information promise to enable joint optimization of sequence and modification patterns.\n\n\n28.4.3 Immunogenicity Considerations\nExogenous mRNA triggers innate immune responses through pattern recognition receptors including Toll-like receptors (TLR3, TLR7, TLR8) and cytosolic sensors (RIG-I, MDA5). While some immune activation may be beneficial for vaccine applications, excessive inflammation limits dosing and causes adverse effects. For protein replacement therapies where repeated dosing is required, minimizing immunogenicity is essential.\nThe immunostimulatory potential of mRNA depends on sequence features including GC content, specific sequence motifs recognized by pattern receptors, and secondary structures that resemble viral replication intermediates. Foundation models that predict immunogenicity from sequence enable design of mRNAs that evade innate immune detection. [Citation Needed] These predictions must be balanced against other objectives: modifications that reduce immunogenicity may also reduce translation efficiency, creating multi-objective trade-offs that characterize mRNA design more broadly.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-antibody-vaccine",
    "href": "part_6/p6-ch28-design.html#sec-ch28-antibody-vaccine",
    "title": "28  Sequence Design",
    "section": "28.5 Antibody and Vaccine Design",
    "text": "28.5 Antibody and Vaccine Design\nAntibody engineering represents one of the most commercially significant applications of computational protein design. The modular architecture of antibodies (framework regions that maintain structural integrity surrounding hypervariable complementarity-determining regions (CDRs) that mediate antigen recognition) creates a well-defined design problem: optimize CDR sequences to achieve desired binding properties while maintaining framework stability and developability.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 28.5: [High] Antibody-specific pipeline. Starting point: Initial antibody from immunization/display; known antigen target. Steps: (1) Initial characterization (affinity, specificity, stability, developability); (2) CDR optimization (FM-guided mutations; sample CDR variants; maintain binding, improve developability); (3) Framework engineering (humanization, stability, expression); (4) Lead optimization (multi-objective: affinity, specificity, half-life, immunogenicity, manufacturability). FM contributions: Embedding space identifies similar therapeutic antibodies; structure prediction without experimental structure; developability prediction (aggregation, viscosity); liability scanning (post-translational modification, deamidation).\n\n\n\n\n28.5.1 CDR Optimization and Humanization\nAntibodies discovered through animal immunization or phage display often require optimization before therapeutic use. Non-human framework sequences may trigger immune responses in patients, necessitating humanization that replaces framework residues with human equivalents while preserving antigen binding. CDR sequences may require affinity maturation to achieve therapeutic potency or specificity optimization to reduce off-target binding.\nFoundation models support antibody optimization through multiple mechanisms. Antibody-specific language models trained on paired heavy and light chain sequences learn the structural and functional constraints on CDR sequences. [Citation Needed] These models predict which mutations are compatible with the antibody fold and which are likely to disrupt structure. Given a parental antibody sequence, the models can propose libraries of variants enriched for functional candidates, reducing the experimental screening burden required to identify improved variants.\nStructure-aware approaches enable more targeted design. Given a structure of the antibody-antigen complex (determined experimentally or predicted computationally via methods discussed in Section 12.4), optimization focuses on residues at the binding interface. Computational saturation mutagenesis predicts the effect of every possible amino acid substitution at each interface position, identifying combinations expected to improve affinity. These predictions guide the construction of focused libraries that explore the most promising region of sequence space.\n\n\n28.5.2 Vaccine Antigen Design\nVaccine development increasingly employs computational design to create immunogens that elicit protective immune responses. The challenge differs from therapeutic protein design: rather than optimizing for direct biological activity, vaccine antigens must be recognized by the immune system and induce antibodies or T cells that protect against pathogen challenge.\nFoundation models contribute to vaccine design in several ways. Epitope prediction models identify regions of pathogen proteins most likely to be recognized by antibodies or T cells, guiding selection of vaccine targets. Structural models predict how mutations affect epitope conformation, enabling design of stabilized antigens that maintain native epitope structure during manufacturing and storage. Glycan shielding analysis predicts which epitopes will be accessible on the pathogen surface versus hidden by glycosylation, focusing vaccine design on exposed regions.\nThe rapid development of mRNA vaccines against SARS-CoV-2 demonstrated the potential of computational approaches to accelerate vaccine design. Structure-guided stabilization of the prefusion spike conformation, optimization of mRNA sequences for expression and stability, and prediction of variant effects on vaccine efficacy all benefited from computational modeling. [Citation Needed] Future vaccine development will increasingly integrate foundation model predictions throughout the design process.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-dbtl",
    "href": "part_6/p6-ch28-design.html#sec-ch28-dbtl",
    "title": "28  Sequence Design",
    "section": "28.6 Closed-Loop Design-Build-Test-Learn Cycles",
    "text": "28.6 Closed-Loop Design-Build-Test-Learn Cycles\nFoundation models achieve their full potential when integrated into iterative experimental workflows. The design-build-test-learn (DBTL) paradigm treats computational predictions as hypotheses to be tested experimentally, with results feeding back to improve both the designed molecules and the models that guide design. This closed-loop approach connects to the lab-in-the-loop concepts introduced in Section 27.5.3.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 28.6: [Essential] Circular workflow. Stages: (1) DESIGN (FM proposes candidates; generative sampling or optimization; active learning selects informative candidates); (2) BUILD (automated synthesis, library construction, QC); (3) TEST (high-throughput assays: binding, activity, stability, expression; multiplexed: DMS, MPRA, Perturb-seq); (4) LEARN (results update model; improve predictions; identify failure modes). Active learning integration: Acquisition function balances exploitation/exploration; maximize expected information gain. Safety and oversight callouts: Safety filters at design; human review before build; stopping criteria. Key insight: FMs achieve full potential in iterative cycles; each round improves molecules and predictions.\n\n\n\n\n28.6.1 Active Learning for Efficient Exploration\nExperimental validation remains the bottleneck in biological design. Even high-throughput assays can test at most thousands to millions of variants, a tiny fraction of possible sequences. Active learning strategies select which experiments to perform by balancing two competing objectives: exploiting current model predictions to test sequences likely to succeed, and exploring regions of uncertainty to gather data that will improve the model.\nBayesian optimization provides a principled framework for this trade-off. A surrogate model (typically a Gaussian process or ensemble neural network) approximates the sequence-to-fitness mapping. Acquisition functions such as expected improvement or upper confidence bound combine predicted function values with uncertainty estimates to select informative test sequences. After each experimental round, the surrogate model is updated with new data, and the process repeats.\nFoundation models enhance active learning by providing informative priors and features. Rather than learning sequence-to-function mappings from scratch, surrogate models can operate on protein language model embeddings that capture evolutionary relationships and structural constraints. These embeddings provide a meaningful notion of sequence similarity even before any task-specific data is available, accelerating the early rounds of optimization when labeled data is scarce.\n\n\n28.6.2 High-Throughput Experimentation Integration\nModern experimental platforms generate data at scales well-matched to foundation model training. Deep mutational scanning (DMS) systematically characterizes thousands of single-mutant variants of a protein, mapping the functional landscape around a parental sequence (see Section 2.4.4 for discussion of DMS data resources). Massively parallel reporter assays test tens of thousands of regulatory element variants in a single experiment. CRISPR screens introduce perturbations across the genome and measure phenotypic consequences.\nThese assays generate dense local maps of sequence-function relationships that complement the global patterns captured by foundation models. The integration is bidirectional: model predictions prioritize which variants to include in experimental libraries, and experimental results fine-tune models for improved accuracy in relevant sequence neighborhoods. After several DBTL cycles, the combined system (fine-tuned model plus accumulated experimental data) can often design sequences that substantially outperform the parental molecule.\nThe design of experiments themselves benefits from computational guidance. Rather than testing all possible single mutants, active learning identifies the most informative subset. Rather than random library construction, computational analysis identifies epistatic interactions that should be explored through combinatorial variants. The cost of DNA synthesis and high-throughput assays makes efficient experimental design increasingly important as design ambitions grow.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-validation",
    "href": "part_6/p6-ch28-design.html#sec-ch28-validation",
    "title": "28  Sequence Design",
    "section": "28.7 Validation Requirements and Failure Modes",
    "text": "28.7 Validation Requirements and Failure Modes\nComputational design generates hypotheses; experimental validation determines whether those hypotheses are correct. The gap between predicted and observed performance represents the ultimate test of design methods, and understanding where predictions fail is essential for improving both models and design strategies. The evaluation principles discussed in Chapter 20 and uncertainty quantification from Chapter 23 apply directly to design validation.\n\n28.7.1 Validation Hierarchy\nDesigned sequences must pass through multiple validation stages before achieving real-world impact. Computational validation confirms that designs satisfy specified constraints and achieve predicted scores, filtering obvious failures before synthesis. In vitro validation tests whether designed proteins express, fold, and exhibit predicted activities in simplified experimental systems. In vivo validation assesses function in cellular or animal contexts where additional complexity may reveal unanticipated problems. Clinical validation, for therapeutic applications, determines whether designs are safe and effective in human patients.\nSuccess rates decline at each stage of this hierarchy. Computationally promising designs often fail to express or fold correctly. Designs that succeed in vitro may lose activity in cellular contexts due to incorrect localization, unexpected degradation, or off-target interactions. Molecules that perform well in model organisms may fail in human clinical trials due to immunogenicity, toxicity, or pharmacokinetic limitations. The attrition from computational design to clinical success remains substantial, motivating continued improvement in predictive accuracy and earlier identification of failure modes.\n\n\n28.7.2 Characteristic Failure Patterns\nFoundation model-guided design exhibits systematic failure modes that practitioners must recognize and mitigate. Distribution shift occurs when optimization pushes sequences into regions where model predictions are unreliable (Section 20.7.1 for detailed discussion of distribution shift in genomic models; Section 23.6 for detection methods). A model trained on natural proteins may produce confident but incorrect predictions for designed sequences that lie far from training data. Regularization toward natural sequence statistics and uncertainty quantification help identify when designs have strayed beyond reliable prediction regimes.\nMode collapse in generative models produces designs that are variants of training sequences rather than genuinely novel molecules. When generated sequences can be matched to close homologs in training data, the design process has failed to create anything new. Novelty filters and diversity requirements during generation help ensure that computational design adds value beyond database retrieval.\nReward hacking occurs when optimization exploits model artifacts rather than genuine sequence-function relationships. A model might predict high expression for sequences containing spurious features that happen to correlate with expression in training data but have no causal effect. Ensemble methods, where designs must score highly across multiple independently trained models, provide some protection against hacking individual model weaknesses.\nThe most insidious failures involve properties that models cannot predict because they were absent from training data. A designed protein might aggregate under manufacturing conditions never encountered during model development. A regulatory element might be silenced by chromatin modifications specific to the therapeutic context. These failures can only be identified through experimental validation in relevant conditions, motivating the closed-loop DBTL approach that continuously tests designs in application-relevant settings.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-practical-constraints",
    "href": "part_6/p6-ch28-design.html#sec-ch28-practical-constraints",
    "title": "28  Sequence Design",
    "section": "28.8 Practical Design Constraints",
    "text": "28.8 Practical Design Constraints\nBeyond achieving desired function, practical design must satisfy numerous constraints arising from manufacturing, safety, and deployment requirements.\n\n28.8.1 Manufacturing and Developability\nDesigned proteins must be producible at scale in expression systems such as bacteria, yeast, or mammalian cells. Expression levels, solubility, and purification behavior determine manufacturing feasibility and cost. Foundation models trained on expression data can predict which sequences are likely to express well, enabling design pipelines that optimize not only for function but for manufacturability. [Citation Needed]\nFor therapeutic proteins, developability encompasses additional properties including stability during storage, compatibility with formulation requirements, and behavior during analytical characterization. Aggregation propensity, chemical degradation sites (oxidation, deamidation), and glycosylation patterns all affect developability. Computational tools increasingly predict these properties from sequence, enabling their incorporation as design constraints.\n\n\n28.8.2 Safety and Biosecurity Considerations\nThe same capabilities that enable beneficial design applications also raise biosecurity concerns. Generative models trained on pathogen sequences might in principle be used to design enhanced pathogens or reconstruct dangerous organisms. The dual-use potential of biological design technology requires ongoing attention to safety practices and governance frameworks.\nCurrent foundation models do not provide straightforward paths to bioweapon development; designing a functional pathogen requires capabilities far beyond predicting sequence properties. As models improve and integrate with automated synthesis and testing platforms, the barrier to misuse may decrease. Responsible development practices, including careful consideration of training data, model access policies, and monitoring for concerning use patterns, are essential components of the foundation model ecosystem. These considerations connect to the broader discussion of safety and ethics in Chapter 29.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-algorithms",
    "href": "part_6/p6-ch28-design.html#sec-ch28-algorithms",
    "title": "28  Sequence Design",
    "section": "28.9 Algorithmic Search and Optimization",
    "text": "28.9 Algorithmic Search and Optimization\nDesign algorithms must navigate vast sequence spaces to identify candidates with desired properties. Several algorithmic paradigms have proven effective, each with characteristic strengths and limitations.\nGradient-based optimization treats foundation models as differentiable functions and computes gradients of objectives with respect to input sequence representations. Because sequences are discrete while gradients are continuous, optimization operates on relaxed representations (probability distributions over nucleotides or amino acids) that are projected back to discrete sequences for evaluation. This approach efficiently navigates high-dimensional spaces but can produce adversarial sequences that exploit model weaknesses rather than achieving genuine biological function.\nEvolutionary algorithms maintain populations of candidate sequences that undergo mutation, recombination, and selection based on fitness scores from foundation model oracles or experimental assays. This approach naturally handles discrete sequence spaces and can maintain diversity to avoid local optima. Multi-objective evolutionary algorithms explicitly construct Pareto frontiers of solutions trading off competing objectives.\nBayesian optimization models the sequence-to-fitness mapping with a probabilistic surrogate (typically a Gaussian process or ensemble neural network) and uses acquisition functions to balance exploration of uncertain regions with exploitation of predicted optima. This approach is particularly effective when experimental evaluations are expensive and each design round must be carefully chosen.\nMonte Carlo methods sample sequences from distributions defined by foundation model likelihoods, optionally biased toward high-scoring regions through importance weighting or Markov chain Monte Carlo. These approaches naturally integrate foundation model priors with task-specific objectives and can generate diverse candidate sets for experimental screening.\nThe choice among algorithmic approaches depends on the specific design problem, available computational resources, and experimental constraints. Many practical pipelines combine multiple approaches: generative sampling to produce initial candidate pools, gradient-based refinement to optimize specific objectives, and active learning to select informative experimental tests.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-design.html#sec-ch28-conclusion",
    "href": "part_6/p6-ch28-design.html#sec-ch28-conclusion",
    "title": "28  Sequence Design",
    "section": "28.10 From Understanding to Creating",
    "text": "28.10 From Understanding to Creating\nSequence design represents the frontier where foundation models transition from tools for understanding biology to engines for creating it. The field has advanced from designing individual stable proteins to engineering complex molecular machines, from optimizing isolated regulatory elements to programming cellular behavior, from incremental improvement of existing sequences to de novo creation of functions not found in nature. The constraints of natural evolution no longer bound the sequences we can consider; the statistical patterns of existing biology provide priors that guide exploration of novel territory.\nThe validation bottleneck persists as perhaps the most fundamental limitation. Computational design can propose candidates faster than experiments can test them, creating pressure to improve both predictive accuracy (reducing false positives that waste experimental resources) and experimental throughput (enabling more designs to be evaluated). Automated laboratories, standardized assay platforms, and improved experimental design methods all contribute to accelerating the design-build-test-learn cycle, but the gap between computational proposal and experimental validation remains substantial.\nThe transition from prediction to design amplifies both the potential benefits and the risks of these technologies. A model that predicts protein function enables analysis; a model that designs protein function enables creation. Ensuring that designed biology serves human flourishing while minimizing potential harms requires not just technical advances but thoughtful governance, inclusive deliberation about applications, and ongoing attention to safety. These broader considerations connect sequence design to regulatory, ethical, and societal dimensions (Chapter 29), where the technical capabilities developed throughout genomic AI meet the human systems that will determine how they are used.\n\n\n\n\nBoer, Carl G. de, Eeshit Dhaval Vaishnav, Ronen Sadeh, Esteban Luis Abeyta, Nir Friedman, and Aviv Regev. 2020. “Deciphering Eukaryotic Gene-Regulatory Logic with 100 Million Random Promoters.” Nature Biotechnology 38 (1): 56–65. https://doi.org/10.1038/s41587-019-0315-8.\n\n\nDauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. “Robust Deep Learning–Based Protein Sequence Design Using ProteinMPNN.” Science 378 (6615): 49–56. https://doi.org/10.1126/science.add2187.\n\n\nFerruz, Noelia, Steffen Schmidt, and Birte Höcker. 2022. “ProtGPT2 Is a Deep Unsupervised Language Model for Protein Design.” Nature Communications 13 (1): 4348. https://doi.org/10.1038/s41467-022-32007-7.\n\n\nHsu, Chloe, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. 2022. “Learning Inverse Folding from Millions of Predicted Structures.” In Proceedings of the 39th International Conference on Machine Learning, 8946–70. PMLR. https://proceedings.mlr.press/v162/hsu22a.html.\n\n\nMadani, Ali, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P. Mohr, James M. Holton, Jose Luis Olmos, et al. 2023. “Large Language Models Generate Functional Protein Sequences Across Diverse Families.” Nature Biotechnology 41 (8): 1099–1106. https://doi.org/10.1038/s41587-022-01618-2.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. “De Novo Design of Protein Structure and Function with RFdiffusion.” Nature 620 (7976): 1089–1100. https://doi.org/10.1038/s41586-023-06415-8.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html",
    "href": "part_6/p6-ch29-future.html",
    "title": "29  Ethics and Frontiers",
    "section": "",
    "text": "29.1 Regulatory Frameworks for Genomic AI\nThe first genomic foundation model to receive U.S. Food and Drug Administration (FDA) clearance as a medical device will face a peculiar regulatory challenge: demonstrating that a system trained on millions of sequences from research biobanks, academic databases, and public repositories can safely inform clinical decisions for individual patients who never consented to such use. Clinical-grade variant interpretation tools already incorporate deep learning predictions, yet the regulatory frameworks governing their deployment were designed for deterministic software with traceable decision logic, not for neural networks whose internal representations resist simple explanation. As of 2024, more than 500 AI-enabled medical devices have received FDA authorization, but fewer than a dozen involve genomic interpretation, and none yet deploys a foundation model at scale (Health 2025). The gap between technical capability and regulatory readiness defines one of the central tensions facing the field.\nThis asymmetry between what models can do in silico and what they may do in clinical practice shapes every translational decision. A variant effect predictor achieving 0.95 area under the receiver operating characteristic curve (auROC) on a curated benchmark may fail unpredictably on the rare variants that matter most for diagnosis. A regulatory sequence model that accurately predicts chromatin accessibility in well-characterized cell lines may produce unreliable predictions in patient-derived tissues never seen during training. Technical achievements in genomic deep learning represent necessary but insufficient conditions for clinical impact. Realizing the benefits of genomic foundation models while managing their risks requires navigating regulatory pathways designed for different technologies, building governance structures for data that spans generations and continents, and confronting ethical questions that genomics and artificial intelligence raise independently but compound when combined.\nGenomic foundation models exist in regulatory limbo. They are clearly software, sometimes medical devices, occasionally laboratory-developed tests, and frequently components of larger systems that span multiple regulatory categories. The frameworks designed for deterministic algorithms struggle with neural networks that learn from data, evolve through fine-tuning, and produce outputs that even their developers cannot fully predict. Navigating this landscape requires understanding how different jurisdictions approach AI-based medical software, what evidence they require, and where genomic applications create novel challenges that existing pathways did not anticipate.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html#sec-ch29-regulatory",
    "href": "part_6/p6-ch29-future.html#sec-ch29-regulatory",
    "title": "29  Ethics and Frontiers",
    "section": "",
    "text": "29.1.1 Software as Medical Device Paradigm\nRegulatory agencies worldwide classify AI-based clinical tools as software as a medical device (SaMD), a category that applies when software itself constitutes the medical device rather than merely controlling hardware. The International Medical Device Regulators Forum defines SaMD risk tiers based on the seriousness of the health condition and the role software plays in clinical decision-making: software that provides information to drive clinical management of a serious condition receives higher scrutiny than software that merely informs decisions about non-serious conditions (“Software as a Medical Device: Possible Framework for Risk Categorization and Corresponding Considerations  International Medical Device Regulators Forum” 2014).\nGenomic foundation models typically fall into higher-risk categories. A tool that classifies variants as pathogenic or benign directly influences diagnostic conclusions for conditions ranging from hereditary cancer syndromes to rare developmental disorders. The consequences of misclassification can be severe: a false negative may delay life-saving interventions, while a false positive may trigger unnecessary prophylactic surgery or cascade into family-wide psychological harm. Regulators accordingly require substantial evidence across the ACCE framework: analytical validity (does the model measure what it claims to measure?), clinical validity (does measurement correlate with the clinical outcome?), and in some cases clinical utility (does using the model improve patient outcomes?) (“ACCE Model Process for Evaluating Genetic Tests  CDC” 2004).\nThe FDA’s approach to AI-enabled devices has evolved considerably since the first autonomous diagnostic AI received FDA clearance in 2018 (Abràmoff et al. 2018). The agency now distinguishes between “locked” algorithms whose behavior is fixed at approval and “adaptive” algorithms that continue learning from new data after deployment (Administration 2021). Most foundation models fall into neither category cleanly: their weights are frozen after pretraining, but their outputs depend on prompts, fine-tuning, or downstream heads that may change across applications. This architectural ambiguity creates regulatory uncertainty. A foundation model serving as the backbone for multiple clinical applications might require separate submissions for each use case, or a single submission might cover the shared backbone while individual fine-tuned heads receive separate clearances.\n\n\n29.1.2 European and Global Regulatory Landscapes\nThe European Union’s approach differs from the FDA’s in several respects relevant to genomic AI. The EU Medical Device Regulation (MDR), which fully replaced prior directives in 2021, classifies standalone software according to similar risk principles but places greater emphasis on conformity assessment by notified bodies rather than centralized agency review (“Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on Medical Devices, Amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and Repealing Council Directives 90/385/EEC and 93/42/EEC (Text with EEA Relevance. )” 2017). For high-risk software, manufacturers must demonstrate compliance with essential safety and performance requirements through technical documentation, quality management systems, and post-market surveillance plans. The AI Act, which entered force in 2024, adds another regulatory layer: high-risk AI systems (including those used in medical diagnosis) must meet transparency, robustness, and human oversight requirements that go beyond device-specific regulations (“Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 Laying down Harmonised Rules on Artificial Intelligence and Amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA Relevance)” 2024).\nRegulatory divergence across jurisdictions creates practical challenges for global deployment. A genomic foundation model cleared by the FDA may require separate CE marking for European markets, TGA approval in Australia, and PMDA review in Japan, each with distinct evidentiary standards and submission formats. Harmonization efforts through the International Medical Device Regulators Forum provide common frameworks for definitions and risk classification, but substantive requirements continue to differ (“Software as a Medical Device (SaMD): Clinical Evaluation  International Medical Device Regulators Forum” 2017). Companies developing clinical-grade genomic AI must either design validation programs that satisfy the most stringent jurisdiction or pursue market-by-market strategies that delay access in some regions.\nThe regulatory landscape for laboratory-developed tests (LDTs) further complicates matters in the United States. Clinical laboratories have historically been able to develop and offer tests under their own validation without FDA premarket review, relying instead on CLIA certification and state licensure. Many clinical genomics laboratories use in-house bioinformatics pipelines, variant callers, and annotation tools that incorporate machine learning components without seeking FDA clearance. Recent FDA guidance signals intent to assert greater oversight over LDTs, particularly those using complex algorithms, but the boundary between regulated devices and unregulated laboratory procedures remains contested (“Medical Devices; Laboratory Developed Tests” 2024).\n\n\n29.1.3 Validation Requirements for Clinical Genomic AI\nRegulatory submissions for genomic AI devices require validation evidence spanning multiple dimensions. Analytical validation typically involves demonstrating that the model performs consistently across different sequencing platforms, library preparation methods, and sample types. For a variant effect predictor, this might include showing that scores remain calibrated when inputs come from whole-genome sequencing versus targeted panels, from fresh blood versus archived FFPE tissue, or from healthy individuals versus cancer patients with complex somatic variation.\nClinical validation connects model outputs to clinical outcomes. For a variant classifier, clinical validation might assess concordance with expert panel classifications, correlation with functional assay results, or agreement with segregation patterns in affected families. The choice of reference standard is itself contentious: ClinVar classifications, which many models use as training labels, reflect historical expert consensus that may lag behind accumulating evidence, and circular validation using the same database for training and evaluation produces misleadingly optimistic results (see Chapter 22). The deployment realities discussed in Section 25.6.4 and Appendix B illustrate how these validation requirements interact with institutional workflows, reimbursement constraints, and clinician trust; regulatory clearance represents only one barrier among many.\nSome regulators also require evidence of clinical utility, demonstrating that model use improves patient outcomes compared to standard practice. This higher bar is difficult to meet for genomic AI tools that operate as components within larger clinical workflows. A variant effect predictor may improve prioritization efficiency without changing ultimate diagnoses, or may enable earlier diagnosis that only translates to better outcomes when appropriate treatments exist. Designing trials that isolate the model’s contribution from confounding workflow factors requires careful attention to study design and endpoint selection.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html#sec-ch29-governance",
    "href": "part_6/p6-ch29-future.html#sec-ch29-governance",
    "title": "29  Ethics and Frontiers",
    "section": "29.2 Data Governance and Consent",
    "text": "29.2 Data Governance and Consent\nFoundation models require training data at scales that strain every assumption underlying informed consent. A protein language model draws on sequences from millions of organisms. A human genomic model aggregates variants from biobanks across continents, each governed by different consent frameworks, legal regimes, and cultural expectations about data use. The participants who contributed samples a decade ago could not have anticipated that their sequences might train generative AI systems capable of designing novel proteins or predicting sensitive traits. Governing this data requires frameworks that balance scientific utility against individual rights, present uses against unknown future applications, and open science norms against community concerns about exploitation.\n\n29.2.1 Consent Problem at Scale\nFoundation model training requires data at scales that challenge traditional consent paradigms. A protein language model trained on UniRef encompasses sequences from millions of organisms, including many species for which consent concepts do not apply and human sequences contributed under varied research protocols. A model trained on human genomic data from multiple biobanks aggregates information collected under different consent frameworks, some permitting broad secondary research use and others restricting use to specific studies.\nThe legal and ethical status of such aggregated training depends on how consent documents were written, how thoroughly participants understood the scope of future use, and how jurisdictions interpret secondary use provisions. GDPR provisions treat genetic data as a special category requiring explicit consent, but may permit research use under legitimate interest or public interest provisions with appropriate safeguards (“Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA Relevance)” 2016). United States regulations under the Common Rule permit secondary research on properly deidentified data, but genomic data resist complete deidentification given the uniqueness of individual genomes (“Federal Policy for the Protection of Human Subjects (’Common Rule” 2009).\nEven when consent technically permits model training, broader ethical questions remain. Participants who consented to genomic research in 2005 could not have anticipated that their data might train AI systems capable of generating novel sequences or predicting sensitive traits. The temporal gap between data collection and model development strains the fiction of informed consent. Dynamic consent systems that allow ongoing engagement and preference updates address some concerns but are difficult to retrofit onto legacy collections and impose burdens on participants and institutions alike (Kaye et al. 2015).\n\n\n29.2.2 Biobank Governance Models\nLarge biobanks have developed varied governance approaches that shape how their data can be used for foundation model development. UK Biobank, which combines genomic data with extensive phenotypic information on approximately 500,000 participants, permits registered researchers to use data for health-related research under terms that explicitly anticipate computational and AI applications (Sudlow et al. 2015; Bycroft et al. 2018). Access requires application review, data security commitments, and agreement to publish results. The model has enabled substantial foundation model research while maintaining participant trust through transparent policies and active communication.\nOther biobanks operate under more restrictive frameworks. Some disease-specific registries limit use to research on particular conditions. Some indigenous and community biobanks require tribal or community approval for research access, reflecting concerns about historical exploitation and the importance of data sovereignty. The tension between open science norms that favor broad data sharing and community governance norms that prioritize local control creates friction for foundation model developers seeking diverse training data.\nFederated learning and other privacy-preserving techniques offer partial solutions by enabling model training without centralizing raw data (Rieke et al. 2020). Under federated approaches, each data custodian trains local models that share only gradients or model updates with a central coordinator. The approach protects against centralization risks but introduces technical complexity, may reduce model quality compared to centralized training, and does not eliminate all privacy risks. Zhu et al. demonstrated that gradient updates can sometimes reveal individual-level information through reconstruction attacks (Zhu, Liu, and Han 2019). Practical federated training for genomic foundation models remains an active research area with limited deployment experience.\n\n\n29.2.3 Secondary Use and Data Futures\nThe genomic data collected today may be used for applications not yet imagined. A variant database assembled for pharmacogenomic research might later inform ancestry inference tools with implications for immigration enforcement. Chromatin accessibility data generated for cancer biology might reveal aging signatures relevant to insurance underwriting. Foundation models trained on diverse genomic data acquire emergent capabilities that their creators did not anticipate and may not recognize.\nGovernance structures must therefore address not just present uses but future possibilities. Some institutions adopt broad consent models that authorize essentially unlimited research use, relying on institutional review and public benefit assessments rather than individual authorization for each application. Others implement tiered consent allowing participants to authorize some uses while restricting others. Still others propose data trusts or cooperatives that hold data on participants’ behalf and negotiate access terms collectively.\nNo consensus has emerged on optimal governance structures for genomic foundation model development. The field operates within a patchwork of institutional policies, national regulations, and community norms that permit some training configurations while prohibiting others. Researchers building foundation models must navigate this landscape carefully, documenting data provenance, respecting access restrictions, and anticipating how governance norms may evolve as AI capabilities advance.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html#sec-ch29-privacy",
    "href": "part_6/p6-ch29-future.html#sec-ch29-privacy",
    "title": "29  Ethics and Frontiers",
    "section": "29.3 Privacy and Genomic Data",
    "text": "29.3 Privacy and Genomic Data\nGenomes are simultaneously the most personal data and the most shareable. A genome uniquely identifies its owner, reveals information about disease risk and ancestry, and exposes details about biological relatives who never consented to any disclosure. Standard anonymization techniques fail because the genome itself is an identifier. Foundation models compound these challenges by potentially memorizing and recombining information in ways that defeat conventional privacy protections. The technical solutions, from differential privacy to federated learning, each involve tradeoffs between utility and protection that genomic applications make particularly acute.\n\n\n\n\n\n\nFIGURE PLACEHOLDER A\n\n\n\n\n\nFIGURE PLACEHOLDER B\n\n\n\n\n\nFIGURE PLACEHOLDER C\n\n\n\n\n\nFIGURE PLACEHOLDER D\n\n\n\n\nFigure 29.1: [High] Multi-panel governance challenges. Panel A (Privacy vs. Utility): Anonymized data (limited utility) vs identified data (utility but privacy risk); differential privacy tradeoff curve. Panel B (Consent Complexity): Broad consent, specific consent, dynamic consent, tiered consent; each with implications. Panel C (Federated Learning): Data stays local; model travels; gradient aggregation; privacy-preserving computation. Panel D (Cross-Border Issues): Different jurisdictions with different rules; GDPR, HIPAA, emerging regulations; data localization requirements. Key insight: No perfect solution; governance requires ongoing negotiation between stakeholder interests.\n\n\n\n\n29.3.1 Re-identification Challenge\nGenomic data pose fundamental privacy challenges because genomes are simultaneously unique identifiers and richly informative biological records. A person’s genome can be matched against public genealogy databases, research repositories, or forensic databases to establish identity with high confidence. Once identified, the genomic record reveals information about disease predisposition, ancestry, family relationships, and other sensitive attributes that the person may not wish to disclose.\nConventional anonymization techniques that remove names and obvious identifiers provide limited protection. Re-identification of individuals from genomic data combined with surname inference through Y-chromosome analysis has been demonstrated (Gymrek et al. 2013) Subsequent work has shown re-identification from aggregate genomic statistics under certain conditions and through membership inference attacks on genomic databases (Homer et al. 2008; Erlich and Narayanan 2014). Foundation models compound these concerns by potentially extracting and recombining information in ways that defeat simple deidentification. A model trained on sequences from many individuals might, under adversarial prompting, generate outputs that reveal information about specific training examples.\nTechnical safeguards include differential privacy (which adds calibrated noise to training procedures to bound individual-level information leakage), secure multi-party computation (which enables joint computation over distributed data without revealing inputs), and synthetic data generation (which produces training data that preserves statistical properties without corresponding to real individuals). Each approach involves tradeoffs between privacy protection and model utility. Differential privacy with strong guarantees may degrade model performance substantially. Secure computation adds computational overhead and complexity. Synthetic data may fail to capture rare variants or unusual correlations essential for clinical applications.\n\n\n29.3.2 Family and Relational Privacy\nGenomic privacy extends beyond individuals to families and communities. A person’s genome reveals information about biological relatives who may not have consented to any data collection. Identifying a carrier of a hereditary cancer mutation implies elevated risk for parents, siblings, and children. Revealing ancestry information for one family member constrains inferences about relatives. These relational dimensions mean that individual consent cannot fully protect the interests of those affected by genomic disclosure.\nFoundation models trained on family data, or capable of inferring family relationships from population-level patterns, create new relational privacy risks. A model that accurately predicts recessive disease carrier status from sequence alone could identify at-risk couples without explicit testing. A model that infers extended pedigree structure from population genetics signals could reveal family secrets or create legal complications. Governance frameworks must consider not just the rights of data subjects but the interests of biological relatives who cannot meaningfully consent.\nSome jurisdictions have begun addressing relational genomic privacy through legislation. The Genetic Information Nondiscrimination Act (GINA) in the United States prohibits health insurers and employers from using genetic information discriminatorily, providing partial protection for individuals whose relatives have been tested (“Genetic Information Nondiscrimination Act of 2008” n.d.). GDPR provisions on special category data extend some protections to inferred genetic information (“Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA Relevance)” 2016). But legal frameworks lag behind technical capabilities, and enforcement mechanisms remain limited.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html#sec-ch29-ip",
    "href": "part_6/p6-ch29-future.html#sec-ch29-ip",
    "title": "29  Ethics and Frontiers",
    "section": "29.4 Intellectual Property and Ownership",
    "text": "29.4 Intellectual Property and Ownership\nWho owns a genome sequence? Who owns a prediction derived from it? Who owns the model weights that encode patterns learned from millions of sequences? These questions lack clear answers, and the uncertainty shapes every decision about data sharing, model release, and commercial deployment. Legal frameworks designed for physical inventions and traditional software fit poorly with foundation models that blur boundaries between data, algorithm, and output. The genomics community’s historical commitment to open science confronts new tensions when model weights represent millions of dollars in compute investment and potentially enable misuse.\n\n29.4.1 Genomic Data Ownership\nLegal frameworks for sequence data ownership vary across jurisdictions and remain contested. In the United States, the Supreme Court’s 2013 Myriad decision held that naturally occurring DNA sequences cannot be patented, eliminating one barrier to data sharing but leaving property rights in datasets unclear (“Assoc. For Molecular Pathology v. Myriad Genetics, Inc., 569 U.S. 576 (2013)” n.d.). Databases may receive limited copyright protection for their selection and arrangement, but individual sequences typically cannot be copyrighted as facts or natural phenomena. Contractual restrictions, such as data use agreements attached to biobank access, provide the primary mechanism for controlling sequence data use.\nThe situation differs for synthetic or engineered sequences, which may qualify for patent protection if they meet novelty, utility, and non-obviousness requirements. Foundation models that generate novel sequences thus operate in complex IP territory: sequences generated by the model may be patentable if sufficiently innovative, but determining inventorship (human researcher versus AI system) raises unresolved legal questions (LORD JUSTICE ARNOLD&lt;br&gt;LADY JUSTICE ELISABETH LAING&lt;br&gt;and&lt;br&gt;LORD JUSTICE BIRSS 2021). Courts and patent offices are only beginning to address AI-generated inventions, with varying approaches across jurisdictions.\nFor foundation model developers, the key practical questions concern what restrictions apply to training data and what rights attach to model outputs. Training on publicly available sequences may be permissible under database terms of use, research exemptions, or fair use principles depending on jurisdiction and use context. Commercial deployment of models trained on restricted-access data may require additional authorization. Outputs generated by models may be freely usable by the model operator, or may carry through restrictions from training data, depending on legal interpretation and contractual provisions.\n\n\n29.4.2 Model Weights as Assets\nFoundation model weights represent substantial investments of compute, data, and expertise, creating obvious commercial value. Companies training large genomic models face decisions about whether to release weights openly, provide API access without weight release, or restrict access entirely. Each approach carries different implications for scientific progress, commercial competition, and safety management.\nOpen release of weights enables independent research, reproduction, and adaptation but forfeits commercial control and complicates responsibility for misuse. API access maintains control while enabling broad use but creates dependencies and may restrict scientific scrutiny. Restricted access protects competitive advantage and may enhance safety oversight but limits beneficial applications and concentrates power.\nThe genomics community has historically favored open data sharing, with major databases and biobanks making data freely available under permissive terms. Whether this norm extends to foundation model weights is contested. Arguments for openness emphasize scientific reproducibility, broad access benefits, and the difficulty of maintaining meaningful restrictions given technical capabilities for weight reconstruction or distillation. Arguments for restriction emphasize dual-use risks from highly capable generative models, commercial incentives necessary to sustain development investment, and the potential for open models to be fine-tuned for harmful purposes.\n\n\n29.4.3 Prediction Ownership and Liability\nWhen a foundation model generates a clinically relevant prediction (this variant is likely pathogenic, this regulatory sequence will increase expression), questions arise about who owns that prediction and who bears responsibility if it proves wrong. The model developer, the clinical laboratory using the model, the health system employing the laboratory, and the clinician acting on results all have potential roles and potential liability.\nCurrent legal frameworks generally hold clinicians responsible for clinical decisions, with laboratories liable for test quality and medical device manufacturers liable for product defects. How these responsibilities apply when decisions incorporate foundation model outputs remains uncertain. If a model developer provides a variant classifier as SaMD, the developer likely bears some responsibility for the classifier’s performance. If a laboratory integrates foundation model embeddings into a proprietary pipeline, the laboratory may assume primary responsibility for overall system performance. If a clinician overrides a model recommendation based on clinical judgment, liability may shift toward the clinician’s decision-making.\nThese liability questions have practical implications for foundation model deployment. Developers may structure their offerings to minimize liability exposure, for instance by providing research-use-only tools that shift responsibility to users, or by limiting outputs to information that falls short of clinical recommendations. Such structuring may impede beneficial clinical applications if it creates uncertainty about appropriate use or fragments responsibility in ways that leave harms uncompensated.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html#sec-ch29-responsible",
    "href": "part_6/p6-ch29-future.html#sec-ch29-responsible",
    "title": "29  Ethics and Frontiers",
    "section": "29.5 Responsible Development Practices",
    "text": "29.5 Responsible Development Practices\nTechnical capability without responsible deployment causes harm. A foundation model that achieves excellent benchmark performance but fails silently on underrepresented populations widens health disparities. A tool that provides confident predictions without communicating uncertainty misleads clinicians. A system deployed without documentation leaves users unable to assess whether its outputs apply to their context. Responsible development encompasses the entire lifecycle from training data selection through deployment and monitoring, requiring attention to transparency, fairness, and human oversight at each stage.\n\n29.5.1 Transparency and Documentation\nResponsible foundation model development requires transparency about training data, model capabilities, limitations, and intended use. Model cards and datasheets provide structured approaches to capturing this information (Mitchell et al. 2019; Gebru et al. 2021). For genomic foundation models, relevant documentation includes:\nTraining data composition encompasses which species are represented, what genomic regions are covered, which populations contribute human data, what functional annotations are included, and how data were filtered or preprocessed. Data provenance documentation traces sources, access conditions, and any restrictions on use or redistribution. Evaluation results cover performance across relevant benchmarks, disaggregated by ancestry, variant type, gene family, and other relevant strata. Limitation disclosure identifies known failure modes, out-of-distribution detection capabilities, and contexts where model outputs should not be trusted.\nThe challenge is ensuring that documentation reaches users who need it and influences their decisions. A detailed model card published alongside model weights may be ignored by users seeking quick results. Clinical deployments may strip away documentation as models are integrated into larger systems. Effective transparency requires not just producing documentation but designing workflows that surface relevant information at decision points and verifying that users understand limitations.\n\n\n29.5.2 Fairness and Performance Equity\nAncestry bias manifests technically at every stage of the genomic AI pipeline. Polygenic scores derived from European-ancestry GWAS show 40 to 75 percent reductions in prediction accuracy for African-ancestry individuals (Section 3.7). Variant effect predictors trained on ClinVar inherit that database’s overrepresentation of European-ancestry variants, performing better for populations already well-served by genomic medicine (Section 14.2; Section 2.8.1). Models can exploit ancestry as a confounding shortcut, achieving high benchmark performance while systematically underperforming for underrepresented groups (Section 22.2.1). Clinical risk models calibrated on single-institution data may provide inferior risk stratification to patients from populations or care settings not represented in development (Section 25.8). These are not independent problems but manifestations of a single structural issue: genomic datasets encode historical inequities in who gets sequenced, which populations are recruited into biobanks, and whose variants receive clinical characterization.\nTechnical solutions exist but require deliberate implementation. Ancestry-stratified evaluation mandates reporting performance separately for major population groups, not just aggregate metrics that obscure disparities. Calibration assessment by subgroup reveals when models systematically over- or under-estimate risk for specific populations. Uncertainty quantification can flag predictions for patients from underrepresented ancestries as less reliable, enabling appropriate clinical caution (see Chapter 23). Reweighting training data or applying group-wise calibration adjustments can partially mitigate disparities, though these post-hoc corrections cannot fully compensate for fundamental data gaps.\nYet technical fixes alone are insufficient. Addressing fairness in genomic AI ultimately requires expanding who participates in genomic research, which populations are prioritized for biobank recruitment, and how resources flow to sequencing initiatives in underrepresented communities. Clinical use of polygenic risk scores derived from European-ancestry GWAS may exacerbate rather than reduce health disparities (Martin et al. 2019). A model trained on biased data and corrected post-hoc will always underperform compared to a model trained on representative data. The field’s trajectory depends on whether current disparities are treated as inconvenient technical limitations or as structural problems demanding structural solutions.\nGenomic foundation models inherit biases from their training data. If training corpora over-represent European ancestry populations, models may perform worse on variants common in other populations, on regulatory elements active in non-European tissues, or on genes under different selective pressures across populations. If functional annotations derive primarily from well-funded research programs focused on common diseases, models may underperform on rare diseases or conditions affecting underserved populations.\nFairness assessment requires disaggregated evaluation across relevant population strata, not just aggregate performance metrics. A variant effect predictor achieving 0.92 auROC overall might achieve 0.95 in European populations and 0.82 in African populations, a disparity masked by aggregate reporting. A regulatory model might perform well on cell types common in training data (lymphocytes, hepatocytes) while failing on less-studied cell types (specialized neurons, rare immune subsets) that matter for particular diseases.\nMitigation approaches include diversifying training data, applying reweighting or resampling strategies during training, and developing adaptation techniques that improve performance on underrepresented groups. But data diversification has limits when underlying resources remain skewed, and post-hoc corrections may trade off overall performance for equity gains. The deeper solution involves changing incentive structures to prioritize diverse data collection and equitable performance from the outset.\n\n\n29.5.3 Human Oversight and Decision Support\nEven highly capable foundation models should operate as decision support tools rather than autonomous decision-makers in clinical contexts. Human oversight serves multiple functions: catching model errors that fall outside training distribution, integrating clinical context that models cannot access, navigating value trade-offs where technical optimization is insufficient, and maintaining accountability structures that enable error correction and redress.\nEffective oversight requires that model outputs be interpretable enough for humans to exercise meaningful judgment. If a variant classifier provides only a pathogenic/benign label without supporting evidence, the overseeing clinician has no basis for assessing whether the model’s reasoning applies to the case at hand. If a regulatory effect predictor reports a large effect without indicating uncertainty, the user may not know when skepticism is warranted. Interpretability tools discussed in Section 24.1 and Section 24.7 support oversight by revealing internal model reasoning, but interpreting such explanations requires expertise and time that may not be available in clinical workflows.\nSystem design must also prevent automation bias, the tendency for human operators to defer to automated recommendations even when independent judgment would lead to different conclusions (Parasuraman and Manzey 2010). Training clinicians to use AI tools effectively, designing interfaces that prompt critical evaluation rather than passive acceptance, and monitoring for over-reliance patterns are all components of responsible oversight architecture.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html#sec-ch29-biosecurity",
    "href": "part_6/p6-ch29-future.html#sec-ch29-biosecurity",
    "title": "29  Ethics and Frontiers",
    "section": "29.6 Dual Use and Biosecurity",
    "text": "29.6 Dual Use and Biosecurity\nThe same capabilities that enable therapeutic protein design could, in principle, enable pathogen enhancement. A model that generates functional regulatory sequences could optimize expression in beneficial or harmful contexts. These dual-use concerns are not unique to foundation models, but the combination of generative capability, broad accessibility, and rapid improvement creates genuinely novel considerations. The gap between computational generation and biological realization provides some natural barrier, yet that gap narrows as both computational and wetlab capabilities advance. Balancing open scientific exchange against biosecurity risks requires ongoing assessment as model capabilities evolve.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 29.2: [High] Risk assessment matrix. Axes: Capability (what the model can do) × Access (who can use it). Quadrants: Low capability/restricted (current academic); high capability/restricted (industrial deployment with oversight); low capability/open (open-source educational); high capability/open (highest concern). Risk factors: Pathogen design, antibiotic resistance, agricultural biosecurity, emergent capabilities. Governance mechanisms: Pre-release evaluation, staged release, monitoring, audit trails, benefit-risk assessment. Key insight: Balancing open science benefits against misuse risks.\n\n\n\n\n29.6.1 Generative Models and Pathogen Enhancement\nFoundation models capable of generating functional biological sequences raise biosecurity concerns distinct from those posed by predictive models. A protein language model trained to generate functional enzymes might, in principle, be prompted to design proteins with enhanced pathogenic properties. A regulatory sequence model might generate promoters optimized for expression in human tissues of concern. A generative DNA model might propose sequences that evade detection by standard diagnostics.\nThe severity of these risks depends on technical factors that remain uncertain. Current generative models often produce sequences that are theoretically functional but fail in experimental validation; the gap between computational generation and biological realization provides a natural barrier (Soice et al. 2023). Specialized knowledge required to translate generated sequences into actual biological threats remains substantial, though it may decrease as wetlab automation advances. Many dangerous sequences are already documented in public databases, making novel generation less marginal than it might appear. The generative architectures examined in Section 28.2 and Section 28.3, which demonstrate increasing capability for producing functional sequences, make these concerns more than hypothetical; the same capabilities that enable therapeutic protein design also lower barriers to misuse.\nNonetheless, responsible development requires attention to dual-use potential. Strategies include capability evaluation (probing models for ability to generate concerning sequences before release), staged deployment (limiting access to highly capable generative models while monitoring for misuse indicators), and output filtering (screening generated sequences against known hazard databases) (Shevlane 2022). The optimal balance between open scientific exchange and biosecurity restriction remains contested, with reasonable experts holding divergent views on where lines should be drawn.\n\n\n29.6.2 Access Controls and Responsible Release\nFoundation model developers must decide how to release models in ways that enable beneficial use while limiting potential for harm. Complete openness maximizes beneficial applications but foregoes control over misuse. Complete restriction limits misuse but also limits beneficial applications and may prove impossible to maintain as model capabilities become reproducible. Graduated access models attempt to balance these considerations by providing broader access to less capable models while restricting access to more capable systems.\nAccess controls can operate at multiple levels: restricting weight access while providing API availability, limiting API capabilities through output filtering, requiring applications and use agreements for access, or monitoring usage patterns for indicators of concerning applications. Each control imposes costs on legitimate users and may prove circumventable by determined malicious actors. The effectiveness of controls depends on the specific model, the capability of concern, and the technical sophistication of potential misusers.\nFor genomic foundation models specifically, the biosecurity risks are generally lower than for models capable of synthesizing pathogen sequences from scratch, but concerns about privacy violations, discriminatory applications, and scientific misconduct remain. A model capable of inferring sensitive traits from genomic data might be misused for unauthorized health prediction. A model capable of generating realistic synthetic genomic data might be used to fabricate research results. Responsible release strategies must consider these diverse risk profiles.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html#sec-ch29-technical",
    "href": "part_6/p6-ch29-future.html#sec-ch29-technical",
    "title": "29  Ethics and Frontiers",
    "section": "29.7 Open Technical Problems",
    "text": "29.7 Open Technical Problems\nThe technical challenges surveyed in preceding chapters remain only partially solved. Foundation models for genomics have demonstrated remarkable capabilities, but they operate far below theoretical limits and fail in ways that better architectures, training strategies, or data could address. Three challenges stand out as particularly important for the field’s trajectory: scaling models to capture biological complexity, integrating information across biological scales, and moving from correlation to causal and mechanistic understanding. Progress on any of these fronts would unlock applications currently beyond reach.\n\n29.7.1 Scaling and Efficiency\nThe largest foundation models in natural language processing now exceed a trillion parameters and were trained on trillions of tokens (Fedus, Zoph, and Shazeer 2022; Chowdhery et al. 2022). Genomic foundation models remain substantially smaller, with typical models ranging from hundreds of millions to low billions of parameters. Whether genomic applications require comparable scale remains uncertain. The human genome spans 3 billion base pairs and encompasses perhaps 20,000 protein-coding genes, a smaller and more constrained space than natural language. But capturing the full complexity of gene regulation, protein structure, and cellular context may require parameter counts that approach or exceed language model scale.\nScaling genomic foundation models faces several bottlenecks. Training data availability constrains scale when models exhaust unique sequences and must rely on data augmentation or repetition. Compute costs remain prohibitive for most academic groups and limit experimentation with truly large architectures. Long sequence lengths required for genomic context (regulatory elements can span hundreds of kilobases) create quadratic attention costs that limit practical context windows despite architectural innovations (see Chapter 7).\nEfficiency improvements that reduce compute requirements without sacrificing capability are thus particularly valuable for genomic applications. Approaches include sparse attention patterns that avoid full quadratic costs, state space models that process sequences in linear time (Gu and Dao 2024), knowledge distillation that transfers capability from large models to smaller ones, and quantization that reduces precision requirements for inference (see Appendix B). Each approach involves trade-offs between efficiency gains and capability preservation that must be evaluated empirically on genomic tasks.\n\n\n29.7.2 Context and Multi-Scale Integration\nBiological phenomena span scales from nucleotides to ecosystems. Foundation models must integrate information across these scales to capture biological reality: local sequence motifs, regulatory element architecture, chromosome-level organization, cellular context, tissue environment, organism-level physiology, and population-level variation all contribute to genotype-phenotype relationships.\nCurrent approaches typically focus on single scales or model multi-scale relationships implicitly through large training datasets rather than explicitly through architectural design. A DNA language model processes sequence tokens without explicit representation of chromatin structure. A single-cell model embeds cells without explicit representation of tissue organization. A regulatory model predicts expression without explicit representation of 3D genome contacts.\nArchitectures that explicitly integrate across scales remain a frontier. Hierarchical models that compose representations at different resolutions, graph neural networks that encode biological relationships across scales (Section 18.2.2), and hybrid systems that combine modality-specific encoders with cross-modal attention layers all represent active research directions. Success will require not just architectural innovation but appropriate training data that captures multi-scale relationships and evaluation protocols that probe multi-scale reasoning.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 29.3: [High] Scale hierarchy visualization. Levels: (1) Nucleotide (base pairs, modifications); (2) Sequence element (motifs, domains, genes); (3) Molecular complex (chromatin, ribonucleoprotein); (4) Cell (transcriptome, proteome, state); (5) Cell population (tissue, heterogeneity); (6) Tissue/Organism (intercellular communication, development). Current model coverage: DNA-LM (nucleotide → gene); Protein models (residue → protein); Single-cell models (cell state). Scale boundary challenges: Arrow indicating information loss at each boundary; hierarchical models emerging; graph networks for relational structure. Key insight: Biology operates across scales; future models must integrate.\n\n\n\n\n\n29.7.3 Causality and Mechanism\nThe distinction between correlation and causation pervades genomic analysis. A variant associated with disease in genome-wide association study (GWAS) may be causal, in linkage disequilibrium with a causal variant, or confounded by population structure or other factors (Section 3.3). A regulatory element predicted to affect expression may directly drive transcription or may merely co-occur with other causal elements. Foundation models, like other statistical learners, capture patterns in training data without distinguishing causal from correlational relationships.\nProgress toward causal and mechanistic reasoning in genomic AI likely requires integrating diverse evidence types. Perturbation experiments (CRISPR knockouts, drug treatments, environmental exposures) provide interventional data that can distinguish causal effects from correlations. Mendelian randomization approaches leverage genetic instruments to estimate causal effects from observational data (Davey Smith and Ebrahim 2003) Structural causal models provide formal frameworks for encoding and reasoning about causal relationships.\nIncorporating causal structure into foundation models is technically challenging. Causal relationships are often unknown, contested, or context-dependent. Training objectives that encourage causal reasoning must balance causal accuracy against predictive performance on tasks where correlation suffices. Evaluation of causal reasoning requires benchmarks with known causal ground truth, which are scarce for complex biological systems.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html#sec-ch29-emerging",
    "href": "part_6/p6-ch29-future.html#sec-ch29-emerging",
    "title": "29  Ethics and Frontiers",
    "section": "29.8 Emerging Directions",
    "text": "29.8 Emerging Directions\nBeyond incremental improvements to existing approaches, several emerging directions may reshape how genomic foundation models develop and deploy. Multimodal architectures that jointly model sequence, structure, expression, and phenotype could capture biological relationships invisible to single-modality models. Agentic systems that autonomously design experiments, interpret results, and iterate toward biological goals could accelerate discovery while raising new governance challenges. Clinical integration through learning health systems could enable models that improve continuously from deployment experience. Each direction carries both promise and risk; realizing benefits while managing harms will require technical innovation alongside thoughtful governance.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 29.4: [High] Autonomous design cycle with oversight. Cycle components: (1) Generative model (proposes candidates, optimizes toward objective); (2) Safety filter (screen against hazard databases, reject concerning, log for audit); (3) Automated synthesis (DNA/protein production, QC, physical realization); (4) High-throughput assay (functional measurement, multiplexed readouts, data generation); (5) Model update (results improve predictions, refine objective, guide next iteration). Human oversight points: Objective specification (before cycle); periodic review (during); stopping criteria (when to halt); anomaly investigation (if unexpected). Risk management: Containment, audit trails, escalation, kill switches. Key insight: Agentic systems require careful objective specification and monitoring; human oversight essential.\n\n\n\n\n29.8.1 Multimodal Integration\nCurrent genomic foundation models largely operate on single modalities: DNA sequence, protein sequence, gene expression counts, chromatin accessibility signals. Biological reality is irreducibly multimodal, with information flowing across modalities through transcription, translation, signaling, and metabolism. The next generation of genomic foundation models will need to integrate across modalities more deeply, building on the multi-omic approaches discussed in Chapter 19.\nEarly multimodal genomic models combine encoders trained separately on different modalities, using cross-attention or shared embedding spaces to enable cross-modal reasoning. More ambitious architectures train end-to-end on multimodal data, learning unified representations that capture relationships between sequence and structure, expression and chromatin state, genotype and phenotype. The data requirements for such training are substantial, requiring aligned measurements across modalities at scale.\nClinical applications particularly benefit from multimodal integration. A diagnostic model that combines genomic variants with electronic health record data, imaging findings, and laboratory values can capture patterns invisible to any single modality. A prognostic model that integrates germline genetics with tumor transcriptomics and treatment history can personalize predictions in ways that purely genetic models cannot. Building such systems requires not just technical capability but also data governance frameworks that permit multimodal combination while protecting privacy.\n\n\n29.8.2 Agentic and Closed-Loop Systems\nFoundation models have traditionally operated as passive tools: given an input, they produce an output, and humans decide what to do with it. Emerging agentic architectures allow models to take actions, observe outcomes, and adapt behavior based on feedback. In genomic contexts, agentic systems might design experiments, interpret results, revise hypotheses, and iterate toward biological goals with minimal human intervention.\nClosed-loop systems couple computational prediction with experimental validation in automated cycles. A design model proposes sequences optimized for a target function. An automated synthesis and screening platform tests proposed sequences. Results feed back to update the model or guide subsequent proposals. Such systems can explore sequence space far more efficiently than sequential human-directed experimentation, as discussed in the design-build-test-learn cycles of Section 28.6.\nThe promise of agentic and closed-loop approaches is accelerated discovery: identifying functional sequences, characterizing biological mechanisms, and optimizing therapeutic candidates faster than traditional workflows. The risks include models pursuing objectives that diverge from human intent, experimental systems generating safety hazards, and accountability gaps when autonomous systems make consequential errors. Realizing benefits while managing risks requires careful attention to objective specification, monitoring and oversight mechanisms, and safety boundaries that constrain autonomous action.\n\n\n29.8.3 Clinical Integration and Learning Health Systems\nThe ultimate test of genomic foundation models is whether they improve health outcomes. Moving from research demonstrations to clinical impact requires integration into care workflows, evidence of benefit from prospective studies, regulatory clearance, and sustainable business models that support ongoing development and maintenance.\nLearning health systems provide a framework for continuous improvement: clinical use generates data that feeds back into model refinement, creating virtuous cycles where models improve as they serve more patients. Such systems raise governance questions about who controls the learning process, how improvements are validated before deployment, and how benefits and risks are distributed across patients, providers, and technology developers.\nThe foundation model paradigm offers particular advantages for learning health systems. Pretrained models can be adapted to local populations and practices through fine-tuning on institutional data (Section 9.6; Section 22.9.3). Improvements demonstrated at one institution can potentially transfer to others through shared model updates. Common architectures enable comparison across sites and accumulation of evidence across diverse populations.\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 29.5: [Enhancing] Circular learning system. Components: (1) Clinical deployment (FM informs decisions, predictions reach patients); (2) Outcome observation (what happened? correct/incorrect?); (3) Data aggregation (outcomes linked to predictions, privacy-preserving); (4) Model refinement (updated training data, improved predictions, federated learning); (5) Validation and approval (updated model validated, regulatory approval, return to deployment). Governance at each stage: Who controls learning? How are improvements validated? How are benefits distributed? How are underrepresented populations protected? Multi-site collaboration: Common architecture enables comparison; improvements transfer; accumulate evidence across diverse populations. Key insight: Learning systems create virtuous cycles; governance must ensure benefits reach all equitably.\n\n\n\nRealizing this vision requires infrastructure for secure data sharing, governance frameworks that enable learning while protecting privacy, regulatory pathways that accommodate evolving systems, and clinical workflows that support appropriate use and oversight. Technical capabilities alone are necessary but not sufficient. Genomic foundation models will achieve their potential only through sustained collaboration among technologists, clinicians, patients, policymakers, and communities working together to build systems that are both capable and trustworthy.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-future.html#sec-ch29-conclusion",
    "href": "part_6/p6-ch29-future.html#sec-ch29-conclusion",
    "title": "29  Ethics and Frontiers",
    "section": "29.9 Work Ahead",
    "text": "29.9 Work Ahead\nThe ultimate test of genomic foundation models is whether they improve health outcomes. The technical capabilities surveyed in the preceding chapters, from sequence representations through foundation model architectures to clinical applications, are necessary but not sufficient for that goal. Between a model that predicts well on benchmarks and a patient whose diagnosis comes faster or whose treatment works better lies the full complexity of clinical translation: validation across populations, integration into workflows, regulatory approval, equitable access, and ongoing monitoring for drift and harm.\nLearning health systems provide a framework for bridging this gap: clinical use generates data that feeds back into model refinement, creating virtuous cycles where models improve as they serve more patients. Such systems raise governance questions as important as the technical ones. Who controls the learning process? How are improvements validated before deployment? How are benefits and risks distributed across patients, providers, and technology developers? How do we ensure that populations underrepresented in training data are not further disadvantaged by systems that learn primarily from others?\nGenomic foundation models will achieve their potential only through sustained collaboration among technologists, clinicians, patients, policymakers, and communities working together to build systems that are both capable and trustworthy. Capability without trustworthiness is dangerous: models that predict accurately but fail silently for certain populations cause harm even as they help others. Trustworthiness without capability is insufficient: systems that are transparent and fair but do not improve on existing practice offer nothing worth adopting. Technical achievements in genomic deep learning enable new capabilities; the human systems that govern their development and deployment will determine whether those capabilities translate into genuine benefit for the patients and populations that genomic medicine aims to serve.\n\n\n\n\nAbràmoff, Michael D., Philip T. Lavin, Michele Birch, Nilay Shah, and James C. Folk. 2018. “Pivotal Trial of an Autonomous AI-Based Diagnostic System for Detection of Diabetic Retinopathy in Primary Care Offices.” Npj Digital Medicine 1 (1): 39. https://doi.org/10.1038/s41746-018-0040-6.\n\n\n“ACCE Model Process for Evaluating Genetic Tests  CDC.” 2004. https://archive.cdc.gov/www_cdc_gov/genomics/gtesting/acce/index.htm.\n\n\nAdministration, U. S. Food and Drug. 2021. “Artificial Intelligence/Machine Learning (‘AI/ML’)-Based Software as a Medical Device (‘SaMD’) Action Plan.” https://www.fda.gov/media/145022/download.\n\n\n“Assoc. For Molecular Pathology v. Myriad Genetics, Inc., 569 U.S. 576 (2013).” n.d. Justia Law. Accessed December 26, 2025. https://supreme.justia.com/cases/federal/us/569/576/.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2022. “PaLM: Scaling Language Modeling with Pathways.” arXiv. https://doi.org/10.48550/arXiv.2204.02311.\n\n\nDavey Smith, George, and Shah Ebrahim. 2003. “‘Mendelian Randomization’: Can Genetic Epidemiology Contribute to Understanding Environmental Determinants of Disease?*.” International Journal of Epidemiology 32 (1): 1–22. https://doi.org/10.1093/ije/dyg070.\n\n\nErlich, Yaniv, and Arvind Narayanan. 2014. “Routes for Breaching and Protecting Genetic Privacy.” Nature Reviews Genetics 15 (6): 409–21. https://doi.org/10.1038/nrg3723.\n\n\n“Federal Policy for the Protection of Human Subjects (’Common Rule.” 2009. Page. https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html.\n\n\nFedus, William, Barret Zoph, and Noam Shazeer. 2022. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” Journal of Machine Learning Research 23 (120): 1–39. http://jmlr.org/papers/v23/21-0998.html.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. “Datasheets for Datasets.” Commun. ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\n“Genetic Information Nondiscrimination Act of 2008.” n.d. U.S. Equal Employment Opportunity Commission. Accessed December 26, 2025. https://www.eeoc.gov/statutes/genetic-information-nondiscrimination-act-2008.\n\n\nGu, Albert, and Tri Dao. 2024. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” In. https://openreview.net/forum?id=tEYskw1VY2.\n\n\nGymrek, Melissa, Amy L. McGuire, David Golan, Eran Halperin, and Yaniv Erlich. 2013. “Identifying Personal Genomes by Surname Inference.” Science 339 (6117): 321–24. https://doi.org/10.1126/science.1229566.\n\n\nHealth, Center for Devices and Radiological. 2025. “Artificial Intelligence-Enabled Medical Devices.” FDA, December. https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-enabled-medical-devices.\n\n\nHomer, Nils, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V. Pearson, Dietrich A. Stephan, Stanley F. Nelson, and David W. Craig. 2008. “Resolving Individuals Contributing Trace Amounts of DNA to Highly Complex Mixtures Using High-Density SNP Genotyping Microarrays.” PLOS Genetics 4 (8): e1000167. https://doi.org/10.1371/journal.pgen.1000167.\n\n\nKaye, Jane, Edgar A. Whitley, David Lund, Michael Morrison, Harriet Teare, and Karen Melham. 2015. “Dynamic Consent: A Patient Interface for Twenty-First Century Research Networks.” European Journal of Human Genetics 23 (2): 141–46. https://doi.org/10.1038/ejhg.2014.71.\n\n\nLORD JUSTICE ARNOLD&lt;br&gt;LADY JUSTICE ELISABETH LAING&lt;br&gt;and&lt;br&gt;LORD JUSTICE BIRSS. 2021. “Thaler v Comptroller General of Patents Trade Marks And Designs [2021] EWCA Civ 1374.” https://www.bailii.org/ew/cases/EWCA/Civ/2021/1374.html.\n\n\nMartin, Alicia R., Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada, Benjamin M. Neale, and Mark J. Daly. 2019. “Clinical Use of Current Polygenic Risk Scores May Exacerbate Health Disparities.” Nature Genetics 51 (4): 584–91. https://doi.org/10.1038/s41588-019-0379-x.\n\n\n“Medical Devices; Laboratory Developed Tests.” 2024. Federal Register. https://www.federalregister.gov/documents/2024/05/06/2024-08935/medical-devices-laboratory-developed-tests.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. “Model Cards for Model Reporting.” In Proceedings of the Conference on Fairness, Accountability, and Transparency, 220–29. FAT* ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3287560.3287596.\n\n\nParasuraman, Raja, and Dietrich H. Manzey. 2010. “Complacency and Bias in Human Use of Automation: An Attentional Integration.” Human Factors 52 (3): 381–410. https://doi.org/10.1177/0018720810376055.\n\n\n“Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA Relevance).” 2016. http://data.europa.eu/eli/reg/2016/679/oj.\n\n\n“Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on Medical Devices, Amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and Repealing Council Directives 90/385/EEC and 93/42/EEC (Text with EEA Relevance. ).” 2017. http://data.europa.eu/eli/reg/2017/745/oj.\n\n\n“Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 Laying down Harmonised Rules on Artificial Intelligence and Amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA Relevance).” 2024. http://data.europa.eu/eli/reg/2024/1689/oj.\n\n\nRieke, Nicola, Jonny Hancox, Wenqi Li, Fausto Milletarì, Holger R. Roth, Shadi Albarqouni, Spyridon Bakas, et al. 2020. “The Future of Digital Health with Federated Learning.” Npj Digital Medicine 3 (1): 119. https://doi.org/10.1038/s41746-020-00323-1.\n\n\nShevlane, Toby. 2022. “Structured Access: An Emerging Paradigm for Safe AI Deployment.” arXiv. https://doi.org/10.48550/arXiv.2201.05159.\n\n\n“Software as a Medical Device (SaMD): Clinical Evaluation  International Medical Device Regulators Forum.” 2017. https://www.imdrf.org/documents/software-medical-device-samd-clinical-evaluation.\n\n\n“Software as a Medical Device: Possible Framework for Risk Categorization and Corresponding Considerations  International Medical Device Regulators Forum.” 2014. https://www.imdrf.org/documents/software-medical-device-possible-framework-risk-categorization-and-corresponding-considerations.\n\n\nSoice, Emily H., Rafael Rocha, Kimberlee Cordova, Michael Specter, and Kevin M. Esvelt. 2023. “Can Large Language Models Democratize Access to Dual-Use Biotechnology?” arXiv. https://doi.org/10.48550/arXiv.2306.03809.\n\n\nSudlow, Cathie, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey, et al. 2015. “UK Biobank: An Open Access Resource for Identifying the Causes of a Wide Range of Complex Diseases of Middle and Old Age.” PLOS Medicine 12 (3): e1001779. https://doi.org/10.1371/journal.pmed.1001779.\n\n\nZhu, Ligeng, Zhijian Liu, and Song Han. 2019. “Deep Leakage from Gradients.” arXiv. https://doi.org/10.48550/arXiv.1906.08935.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Ethics and Frontiers</span>"
    ]
  },
  {
    "objectID": "bib/references.html",
    "href": "bib/references.html",
    "title": "References",
    "section": "",
    "text": "Abràmoff, Michael D., Philip T. Lavin, Michele Birch, Nilay Shah, and\nJames C. Folk. 2018. “Pivotal Trial of an Autonomous\nAI-Based Diagnostic System for Detection of Diabetic\nRetinopathy in Primary Care Offices.” Npj Digital\nMedicine 1 (1): 39. https://doi.org/10.1038/s41746-018-0040-6.\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green,\nAlexander Pritzel, Olaf Ronneberger, et al. 2024.\n“[AlphaFold3] Accurate Structure\nPrediction of Biomolecular Interactions with AlphaFold\n3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\n“ACCE Model Process for\nEvaluating Genetic Tests\n CDC.” 2004. https://archive.cdc.gov/www_cdc_gov/genomics/gtesting/acce/index.htm.\n\n\nAdministration, U. S. Food and Drug. 2021. “Artificial\nIntelligence/Machine Learning\n(‘AI/ML’)-Based\nSoftware as a Medical Device\n(‘SaMD’) Action\nPlan.” https://www.fda.gov/media/145022/download.\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky,\nAnna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev.\n2010. “A Method and Server for Predicting Damaging Missense\nMutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nAgarwal, Vikram, and Jay Shendure. 2020. “Predicting mRNA Abundance Directly\nfrom Genomic Sequence Using\nDeep Convolutional Neural\nNetworks.” Cell Reports 31 (7): 107663. https://doi.org/10.1016/j.celrep.2020.107663.\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F.\nScott, and Ada Hamosh. 2015. “OMIM.org:\nOnline Mendelian Inheritance in\nMan (OMIM®), an Online Catalog of Human Genes\nand Genetic Disorders.” Nucleic Acids Research 43 (D1):\nD789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAndré, Fabrice, Eva Ciruelos, Gabor Rubovszky, Mario Campone, Sibylle\nLoibl, Hope S. Rugo, Hiroji Iwata, et al. 2019. “Alpelisib for\nPIK3CA-Mutated, Hormone\nReceptor–Positive Advanced\nBreast Cancer.” New England Journal\nof Medicine 380 (20): 1929–40. https://doi.org/10.1056/NEJMoa1813904.\n\n\nAngelopoulos, Anastasios N., and Stephen Bates. 2023. “Conformal\nPrediction: A Gentle\nIntroduction.” Foundations and Trends® in\nMachine Learning 16 (4): 494–591. https://doi.org/10.1561/2200000101.\n\n\n“Assoc. For Molecular Pathology v.\nMyriad Genetics, Inc., 569\nU.S. 576 (2013).” n.d. Justia\nLaw. Accessed December 26, 2025. https://supreme.justia.com/cases/federal/us/569/576/.\n\n\nAuton, Adam, Gonçalo R. Abecasis, David M. Altshuler, Richard M. Durbin,\nGonçalo R. Abecasis, David R. Bentley, Aravinda Chakravarti, et al.\n2015. “A Global Reference for Human Genetic Variation.”\nNature 526 (7571): 68–74. https://doi.org/10.1038/nature15393.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A.\nGrabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet\nKohli, and David R. Kelley. 2021. “[Enformer]\nEffective Gene Expression Prediction from Sequence by\nIntegrating Long-Range Interactions.” Nature Methods 18\n(October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025.\n“AlphaGenome: AI for Better\nUnderstanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBen-David, Shai, John Blitzer, Koby Crammer, Alex Kulesza, Fernando\nPereira, and Jennifer Wortman Vaughan. 2010. “A Theory of Learning\nfrom Different Domains.” Machine Learning 79 (1):\n151–75. https://doi.org/10.1007/s10994-009-5152-4.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S.\nSong. 2024. “GPN-MSA: An Alignment-Based\nDNA Language Model for Genome-Wide Variant Effect\nPrediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023.\n“[GPN] DNA Language Models Are Powerful\nPredictors of Genome-Wide Variant Effects.” Proceedings of\nthe National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025.\n“[TraitGym] Benchmarking\nDNA Sequence Models for\nCausal Regulatory Variant\nPrediction in Human\nGenetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBenner, Christian, Chris C. A. Spencer, Aki S. Havulinna, Veikko\nSalomaa, Samuli Ripatti, and Matti Pirinen. 2016.\n“FINEMAP: Efficient Variable Selection Using Summary\nData from Genome-Wide Association Studies.”\nBioinformatics 32 (10): 1493–1501. https://doi.org/10.1093/bioinformatics/btw018.\n\n\nBergquist, Timothy, Sarah L. Stenton, Emily A. W. Nadeau, Alicia B.\nByrne, Marc S. Greenblatt, Steven M. Harrison, Sean V. Tavtigian, et al.\n2025. “Calibration of Additional Computational Tools Expands\nClinGen Recommendation Options for Variant Classification\nwith PP3/BP4 Criteria.” Genetics in\nMedicine 27 (6): 101402. https://doi.org/10.1016/j.gim.2025.101402.\n\n\nBoer, Carl G. de, Eeshit Dhaval Vaishnav, Ronen Sadeh, Esteban Luis\nAbeyta, Nir Friedman, and Aviv Regev. 2020. “Deciphering\nEukaryotic Gene-Regulatory Logic with 100 Million Random\nPromoters.” Nature Biotechnology 38 (1): 56–65. https://doi.org/10.1038/s41587-019-0315-8.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and\nVasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant\nEffects with a Deep Protein Language Model.” Nature\nGenetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg\nBrockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025.\n“[Evo 2] Genome Modeling and Design\nAcross All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nBrnich, Sarah E., Ahmad N. Abou Tayoun, Fergus J. Couch, Garry R.\nCutting, Marc S. Greenblatt, Christopher D. Heinen, Dona M. Kanavy, et\nal. 2019. “Recommendations for Application of the Functional\nEvidence PS3/BS3 Criterion Using the\nACMG/AMP Sequence Variant Interpretation\nFramework.” Genome Medicine 12 (1): 3. https://doi.org/10.1186/s13073-019-0690-2.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language\nModels Are Few-Shot\nLearners.” Advances in Neural Information\nProcessing Systems 33: 1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=transaction&utm_medium=email&utm_campaign=linkedin_newsletter.\n\n\nBrowning, Brian L., Xiaowen Tian, Ying Zhou, and Sharon R. Browning.\n2021. “Fast Two-Stage Phasing of Large-Scale Sequence\nData.” American Journal of Human Genetics 108 (10):\n1880–90. https://doi.org/10.1016/j.ajhg.2021.08.005.\n\n\nBuniello, Annalisa, Daniel Suveges, Carlos Cruz-Castillo, Manuel Bernal\nLlinares, Helena Cornu, Irene Lopez, Kirill Tsukanov, et al. 2025.\n“Open Targets Platform: Facilitating\nTherapeutic Hypotheses Building in Drug Discovery.” Nucleic\nAcids Research 53 (D1): D1467–75. https://doi.org/10.1093/nar/gkae1128.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T.\nElliott, Kevin Sharp, Allan Motyer, et al. 2018. “The\nUK Biobank Resource with Deep Phenotyping and\nGenomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T.\nHiggins-Chen, Steve Horvath, and Bo Wang. 2024.\n“CpGPT: A Foundation Model\nfor DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE]\nMulti-Omics Single-Cell Data Integration and Regulatory\nInference with Graph-Linked Embedding.” Nature\nBiotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nChapman, Paul B., Axel Hauschild, Caroline Robert, John B. Haanen, Paolo\nAscierto, James Larkin, Reinhard Dummer, et al. 2011. “Improved\nSurvival with Vemurafenib in\nMelanoma with BRAF V600E\nMutation.” New England Journal of Medicine\n364 (26): 2507–16. https://doi.org/10.1056/NEJMoa1103782.\n\n\nChen, Jiayang, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze\nYu, Licheng Zong, et al. 2022. “[RNA-FM]\nInterpretable RNA Foundation\nModel from Unannotated Data for\nHighly Accurate RNA\nStructure and Function\nPredictions.” arXiv. https://doi.org/10.48550/arXiv.2204.00300.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou.\n2022b. “[DeepSEA Sei] A\nSequence-Based Global Map of Regulatory Activity for Deciphering Human\nGenetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\n———. 2022a. “[DeepSEA Sei]\nA Sequence-Based Global Map of Regulatory Activity for\nDeciphering Human Genetics.” Nature Genetics 54 (7):\n940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė,\nTaylor Applebaum, Alexander Pritzel, et al. 2023.\n“[AlphaMissense] Accurate Proteome-Wide\nMissense Variant Effect Prediction with\nAlphaMissense.” Science 381 (6664):\neadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nCheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu\nYang, Lei Li, and Jian Ma. 2024. “DNALONGBENCH:\nA Benchmark Suite\nFor Long-Range DNA\nPrediction Tasks,” October. https://openreview.net/forum?id=opv67PpqLS.\n\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua\nBengio. 2014. “On the Properties of\nNeural Machine Translation:\nEncoder-Decoder\nApproaches.” arXiv. https://doi.org/10.48550/arXiv.1409.1259.\n\n\nChoi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O’Reilly. 2020.\n“[PRS] Tutorial: A Guide to Performing\nPolygenic Risk Score Analyses.” Nature Protocols 15 (9):\n2759–72. https://doi.org/10.1038/s41596-020-0353-1.\n\n\nChoromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou\nSong, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2022.\n“Rethinking Attention with\nPerformers.” arXiv. https://doi.org/10.48550/arXiv.2009.14794.\n\n\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, et al. 2022.\n“PaLM: Scaling Language\nModeling with Pathways.” arXiv. https://doi.org/10.48550/arXiv.2204.02311.\n\n\nCirulli, Elizabeth T., Simon White, Robert W. Read, Gai Elhanan, William\nJ. Metcalf, Francisco Tanudjaja, Donna M. Fath, et al. 2020.\n“Genome-Wide Rare Variant Analysis for Thousands of Phenotypes in\nover 70,000 Exomes from Two Cohorts.” Nature\nCommunications 11 (1): 542. https://doi.org/10.1038/s41467-020-14288-y.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus\nWahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024.\n“[DeepRVAT] Integration of Variant\nAnnotations Using Deep Set Networks Boosts Rare Variant Association\nTesting.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan\nDuan, and Bo Wang. 2024. “scGPT:\nToward Building a Foundation Model for Single-Cell Multi-Omics Using\nGenerative AI.” Nature Methods 21 (8):\n1470–80. https://doi.org/10.1038/s41592-024-02201-0.\n\n\nDabernig-Heinz, Johanna, Mara Lohde, Martin Hölzer, Adriana Cabal, Rick\nConzemius, Christian Brandt, Matthias Kohl, et al. 2024. “A\nMulticenter Study on Accuracy and Reproducibility of Nanopore\nSequencing-Based Genotyping of Bacterial Pathogens.” Journal\nof Clinical Microbiology 62 (9): e00628–24. https://doi.org/10.1128/jcm.00628-24.\n\n\nDallago, Christian, Jody Mou, Kadina E. Johnston, Bruce J. Wittmann,\nNicholas Bhattacharya, Samuel Goldman, Ali Madani, and Kevin K. Yang.\n2022. “FLIP: Benchmark Tasks in Fitness\nLandscape Inference for Proteins.” bioRxiv. https://doi.org/10.1101/2021.11.09.467890.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez\nCarranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago,\net al. 2023. “Nucleotide Transformer: Building and\nEvaluating Robust Foundation Models for Human Genomics.”\nNature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDao, Tri, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022.\n“FlashAttention: Fast and\nMemory-Efficient Exact\nAttention with\nIO-Awareness.” Advances in Neural\nInformation Processing Systems 35 (December): 16344–59. https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.\n\n\nDauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F.\nMilles, B. I. M. Wicky, et al. 2022. “Robust Deep Learning–Based\nProtein Sequence Design Using ProteinMPNN.”\nScience 378 (6615): 49–56. https://doi.org/10.1126/science.add2187.\n\n\nDavey Smith, George, and Shah Ebrahim. 2003.\n“‘Mendelian Randomization’: Can Genetic\nEpidemiology Contribute to Understanding Environmental Determinants of\nDisease?*.” International Journal of Epidemiology 32\n(1): 1–22. https://doi.org/10.1093/ije/dyg070.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper,\nArend Sidow, and Serafim Batzoglou. 2010. “Identifying a\nHigh Fraction of the Human\nGenome to Be Under Selective\nConstraint Using GERP++.”\nPLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nDePristo, Mark A., Eric Banks, Ryan Poplin, Kiran V. Garimella, Jared R.\nMaguire, Christopher Hartl, Anthony A. Philippakis, et al. 2011.\n“A Framework for Variation Discovery and Genotyping Using\nNext-Generation DNA Sequencing Data.” Nature\nGenetics 43 (5): 491–98. https://doi.org/10.1038/ng.806.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“BERT: Pre-Training of Deep\nBidirectional Transformers for\nLanguage Understanding.” arXiv. https://doi.org/10.48550/arXiv.1810.04805.\n\n\nDixit, Atray, Oren Parnas, Biyu Li, Jenny Chen, Charles P. Fulco, Livnat\nJerby-Arnon, Nemanja D. Marjanovic, et al. 2016.\n“Perturb-Seq: Dissecting\nMolecular Circuits with Scalable\nSingle-Cell RNA\nProfiling of Pooled Genetic\nScreens.” Cell 167 (7): 1853–1866.e17. https://doi.org/10.1016/j.cell.2016.11.038.\n\n\nDuncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R.\nPeterson, and B. Domingue. 2019. “Analysis of Polygenic Risk Score\nUsage and Performance in Diverse Human Populations.” Nature\nCommunications 10 (1): 3328. https://doi.org/10.1038/s41467-019-11112-0.\n\n\nEdgar, Ron, Michael Domrachev, and Alex E. Lash. 2002. “Gene\nExpression Omnibus: NCBI Gene\nExpression and Hybridization Array Data Repository.” Nucleic\nAcids Research 30 (1): 207–10. https://doi.org/10.1093/nar/30.1.207.\n\n\nElks, Cathy E., Marcel Den Hoed, Jing Hua Zhao, Stephen J. Sharp,\nNicholas J. Wareham, Ruth J. F. Loos, and Ken K. Ong. 2012.\n“Variability in the Heritability of Body\nMass Index: A\nSystematic Review and\nMeta-Regression.” Frontiers in\nEndocrinology 3 (February). https://doi.org/10.3389/fendo.2012.00029.\n\n\nElnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi,\nYu Wang, Llion Jones, Tom Gibbs, et al. 2021.\n“ProtTrans: Towards\nCracking the Language of Life’s\nCode Through\nSelf-Supervised Deep\nLearning and High Performance\nComputing.” arXiv. https://doi.org/10.48550/arXiv.2007.06225.\n\n\nErlich, Yaniv, and Arvind Narayanan. 2014. “Routes for Breaching\nand Protecting Genetic Privacy.” Nature Reviews Genetics\n15 (6): 409–21. https://doi.org/10.1038/nrg3723.\n\n\n“Federal Policy for the Protection of\nHuman Subjects (’Common\nRule.” 2009. Page. https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html.\n\n\nFedus, William, Barret Zoph, and Noam Shazeer. 2022. “Switch\nTransformers: Scaling to Trillion\nParameter Models with Simple and\nEfficient Sparsity.” Journal of\nMachine Learning Research 23 (120): 1–39. http://jmlr.org/papers/v23/21-0998.html.\n\n\nFerruz, Noelia, Steffen Schmidt, and Birte Höcker. 2022.\n“ProtGPT2 Is a Deep Unsupervised Language Model for\nProtein Design.” Nature Communications 13 (1): 4348. https://doi.org/10.1038/s41467-022-32007-7.\n\n\nFindlay, Gregory M., Riza M. Daza, Beth Martin, Melissa D. Zhang, Anh P.\nLeith, Molly Gasperini, Joseph D. Janizek, Xingfan Huang, Lea M.\nStarita, and Jay Shendure. 2018. “Accurate Classification of\nBRCA1 Variants with Saturation Genome Editing.”\nNature 562 (7726): 217–22. https://doi.org/10.1038/s41586-018-0461-z.\n\n\nFinn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017.\n“Model-Agnostic\nMeta-Learning for Fast\nAdaptation of Deep\nNetworks.” In Proceedings of the 34th\nInternational Conference on\nMachine Learning, 1126–35. PMLR. https://proceedings.mlr.press/v70/finn17a.html.\n\n\nFokkema, Ivo F. A. C., Peter E. M. Taschner, Gerard C. P. Schaafsma, J.\nCelli, Jeroen F. J. Laros, and Johan T. den Dunnen. 2011.\n“LOVD v.2.0: The Next Generation in Gene Variant\nDatabases.” Human Mutation 32 (5): 557–63. https://doi.org/10.1002/humu.21438.\n\n\nFowler, Douglas M., and Stanley Fields. 2014. “Deep Mutational\nScanning: A New Style of Protein Science.” Nature\nMethods 11 (8): 801–7. https://doi.org/10.1038/nmeth.3027.\n\n\nFrankish, Adam, Mark Diekhans, Anne-Maud Ferreira, Rory Johnson, Irwin\nJungreis, Jane Loveland, Jonathan M Mudge, et al. 2019.\n“GENCODE Reference Annotation for the Human and Mouse\nGenomes.” Nucleic Acids Research 47 (D1): D766–73. https://doi.org/10.1093/nar/gky955.\n\n\nFrazer, Jonathan, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K.\nMin, Kelly Brock, Yarin Gal, and Debora S. Marks. 2021.\n“[EVE] Disease Variant Prediction with\nDeep Generative Models of Evolutionary Data.” Nature 599\n(7883): 91–95. https://doi.org/10.1038/s41586-021-04043-8.\n\n\nFudenberg, Geoff, David R. Kelley, and Katherine S. Pollard. 2020.\n“[Akita] Predicting 3D\nGenome Folding from DNA Sequence with\nAkita.” Nature Methods 17 (11): 1111–17. https://doi.org/10.1038/s41592-020-0958-x.\n\n\nGaedigk, Andrea, Magnus Ingelman-Sundberg, Neil A. Miller, J. Steven\nLeeder, Michelle Whirl-Carrillo, Teri E. Klein, and the PharmVar\nSteering Committee. 2018. “The Pharmacogene\nVariation (PharmVar) Consortium:\nIncorporation of the Human\nCytochrome P450 (CYP)\nAllele Nomenclature\nDatabase.” Clinical Pharmacology &\nTherapeutics 103 (3): 399–401. https://doi.org/10.1002/cpt.910.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a\nBayesian Approximation:\nRepresenting Model Uncertainty in\nDeep Learning.” In Proceedings of\nThe 33rd International Conference\non Machine Learning, 1050–59. PMLR. https://proceedings.mlr.press/v48/gal16.html.\n\n\nGamazon, Eric R., Heather E. Wheeler, Kaanan P. Shah, Sahar V.\nMozaffari, Keston Aquino-Michaels, Robert J. Carroll, Anne E. Eyler, et\nal. 2015. “A Gene-Based Association Method for Mapping Traits\nUsing Reference Transcriptome Data.” Nature Genetics 47\n(9): 1091–98. https://doi.org/10.1038/ng.3367.\n\n\nGanin, Yaroslav, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\nLarochelle, François Laviolette, Mario March, and Victor Lempitsky.\n2016. “Domain-Adversarial Training of\nNeural Networks.” Journal of\nMachine Learning Research 17 (59): 1–35. http://jmlr.org/papers/v17/15-239.html.\n\n\nGarrison, Erik, Jouni Sirén, Adam M. Novak, Glenn Hickey, Jordan M.\nEizenga, Eric T. Dawson, William Jones, et al. 2018. “Variation\nGraph Toolkit Improves Read Mapping by Representing Genetic Variation in\nthe Reference.” Nature Biotechnology 36 (9): 875–79. https://doi.org/10.1038/nbt.4227.\n\n\nGasperini, Molly, Andrew J. Hill, José L. McFaline-Figueroa, Beth\nMartin, Seungsoo Kim, Melissa D. Zhang, Dana Jackson, et al. 2019.\n“A Genome-Wide Framework for\nMapping Gene Regulation via\nCellular Genetic Screens.”\nCell 176 (1): 377–390.e19. https://doi.org/10.1016/j.cell.2018.11.029.\n\n\nGe, Tian, Chia-Yen Chen, Yang Ni, Yen-Chen Anne Feng, and Jordan W.\nSmoller. 2019. “Polygenic Prediction via Bayesian\nRegression and Continuous Shrinkage Priors.” Nature\nCommunications 10 (1): 1776. https://doi.org/10.1038/s41467-019-09718-5.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Commun. ACM 64 (12):\n86–92. https://doi.org/10.1145/3458723.\n\n\n“Genetic Information Nondiscrimination\nAct of 2008.” n.d. U.S. Equal Employment\nOpportunity Commission. Accessed December 26, 2025. https://www.eeoc.gov/statutes/genetic-information-nondiscrimination-act-2008.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024.\n“Delphi: A Deep-Learning\nMethod for Polygenic Risk\nPrediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nGong, Li, Clarissa J Klein, Kelly E Caudle, Ann M Moyer, Stuart A Scott,\nMichelle Whirl-Carrillo, Teri E Klein, ClinGen Pharmacogenomics Working\nGroup (PGxWG), and on behalf of the. 2025. “Integrating\nPharmacogenomics into the Broader\nConstruct of Genomic Medicine:\nEfforts by the ClinGen\nPharmacogenomics Working Group\n(PGxWG).” Clinical Chemistry 71 (1): 36–44.\nhttps://doi.org/10.1093/clinchem/hvae181.\n\n\nGoodwin, Sara, John D. McPherson, and W. Richard McCombie. 2016.\n“Coming of Age: Ten Years of Next-Generation Sequencing\nTechnologies.” Nature Reviews Genetics 17 (6): 333–51.\nhttps://doi.org/10.1038/nrg.2016.49.\n\n\nGrešová, Katarína, Vlastimil Martinek, David Čechák, Petr Šimeček, and\nPanagiotis Alexiou. 2023. “Genomic Benchmarks: A Collection of\nDatasets for Genomic Sequence Classification.” BMC Genomic\nData 24 (1): 25. https://doi.org/10.1186/s12863-023-01123-8.\n\n\nGu, Albert, and Tri Dao. 2024. “Mamba:\nLinear-Time Sequence\nModeling with Selective State\nSpaces.” In. https://openreview.net/forum?id=tEYskw1VY2.\n\n\nGu, Albert, Karan Goel, Ankit Gupta, and Christopher Ré. 2022. “On\nthe Parameterization and Initialization of\nDiagonal State Space\nModels.” Advances in Neural Information\nProcessing Systems 35 (December): 35971–83. https://proceedings.neurips.cc/paper_files/paper/2022/hash/e9a32fade47b906de908431991440f7c-Abstract-Conference.html.\n\n\nGudbjartsson, Daniel F., Patrick Sulem, Hannes Helgason, Arnaldur\nGylfason, Sigurjon A. Gudjonsson, Florian Zink, Asmundur Oddson, et al.\n2015. “Sequence Variants from Whole Genome Sequencing a Large\nGroup of Icelanders.” Scientific Data 2\n(1): 150011. https://doi.org/10.1038/sdata.2015.11.\n\n\nGuo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017.\n“On Calibration of Modern\nNeural Networks.” In Proceedings of\nthe 34th International Conference on\nMachine Learning, 1321–30. PMLR. https://proceedings.mlr.press/v70/guo17a.html.\n\n\nGusev, Alexander, Arthur Ko, Huwenbo Shi, Gaurav Bhatia, Wonil Chung,\nBrenda W. J. H. Penninx, Rick Jansen, et al. 2016. “Integrative\nApproaches for Large-Scale Transcriptome-Wide Association\nStudies.” Nature Genetics 48 (3): 245–52. https://doi.org/10.1038/ng.3506.\n\n\nGymrek, Melissa, Amy L. McGuire, David Golan, Eran Halperin, and Yaniv\nErlich. 2013. “Identifying Personal\nGenomes by Surname\nInference.” Science 339 (6117): 321–24. https://doi.org/10.1126/science.1229566.\n\n\nHao, Minsheng, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi\nCheng, Taifeng Wang, Jianzhu Ma, Xuegong Zhang, and Le Song. 2024.\n“Large-Scale Foundation Model on Single-Cell\nTranscriptomics.” Nature Methods 21 (8): 1481–91. https://doi.org/10.1038/s41592-024-02305-7.\n\n\nHealth, Center for Devices and Radiological. 2025. “Artificial\nIntelligence-Enabled Medical\nDevices.” FDA, December. https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-enabled-medical-devices.\n\n\nHenikoff, S, and J G Henikoff. 1992. “Amino Acid Substitution\nMatrices from Protein Blocks.” Proceedings of the National\nAcademy of Sciences 89 (22): 10915–19. https://doi.org/10.1073/pnas.89.22.10915.\n\n\nHilker, Rikke, Dorte Helenius, Birgitte Fagerlund, Axel Skytthe, Kaare\nChristensen, Thomas M. Werge, Merete Nordentoft, and Birte Glenthøj.\n2018. “Heritability of Schizophrenia and\nSchizophrenia Spectrum Based on\nthe Nationwide Danish Twin\nRegister.” Biological Psychiatry, Novel\nMechanisms in Schizophrenia\nPathophysiology, 83 (6): 492–98. https://doi.org/10.1016/j.biopsych.2017.08.017.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long\nShort-Term Memory.”\nNeural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022b.\n“Training Compute-Optimal\nLarge Language Models.”\narXiv. https://doi.org/10.48550/arXiv.2203.15556.\n\n\n———, et al. 2022a. “Training\nCompute-Optimal Large\nLanguage Models.” arXiv. https://doi.org/10.48550/arXiv.2203.15556.\n\n\nHomer, Nils, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav\nTembe, Jill Muehling, John V. Pearson, Dietrich A. Stephan, Stanley F.\nNelson, and David W. Craig. 2008. “Resolving\nIndividuals Contributing Trace\nAmounts of DNA to Highly\nComplex Mixtures Using\nHigh-Density SNP\nGenotyping Microarrays.” PLOS\nGenetics 4 (8): e1000167. https://doi.org/10.1371/journal.pgen.1000167.\n\n\nHormozdiari, Farhad, Emrah Kostem, Eun Yong kang, Bogdan Pasaniuc, and\nEleazar Eskin. 2014. “Identifying Causal Variants at Loci with\nMultiple Signals of Association.” In Proceedings of the 5th\nACM Conference on Bioinformatics,\nComputational Biology, and Health\nInformatics, 610–11. BCB ’14. New York,\nNY, USA: Association for Computing Machinery. https://doi.org/10.1145/2649387.2660800.\n\n\nHoulsby, Neil, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,\nQuentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain\nGelly. 2019. “Parameter-Efficient\nTransfer Learning for\nNLP.” In Proceedings of the 36th\nInternational Conference on\nMachine Learning, 2790–99. PMLR. https://proceedings.mlr.press/v97/houlsby19a.html.\n\n\nHoward, Jeremy, and Sebastian Ruder. 2018. “Universal\nLanguage Model Fine-Tuning for\nText Classification.” arXiv. https://doi.org/10.48550/arXiv.1801.06146.\n\n\nHsu, Chloe, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu,\nAdam Lerer, and Alexander Rives. 2022. “Learning Inverse Folding\nfrom Millions of Predicted Structures.” In Proceedings of the\n39th International Conference on\nMachine Learning, 8946–70. PMLR. https://proceedings.mlr.press/v162/hsu22a.html.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, and Weizhu Chen. 2021. “LoRA:\nLow-Rank Adaptation of\nLarge Language Models.”\narXiv. https://doi.org/10.48550/arXiv.2106.09685.\n\n\nIoannidis, Nilah M., Joseph H. Rothstein, Vikas Pejaver, Sumit Middha,\nShannon K. McDonnell, Saurabh Baheti, Anthony Musolf, et al. 2016.\n“REVEL: An Ensemble\nMethod for Predicting the\nPathogenicity of Rare Missense\nVariants.” The American Journal of Human\nGenetics 99 (4): 877–85. https://doi.org/10.1016/j.ajhg.2016.08.016.\n\n\nIonita-Laza, Iuliana, Kenneth McCallum, Bin Xu, and Joseph D. Buxbaum.\n2016. “A Spectral Approach Integrating Functional Genomic\nAnnotations for Coding and Noncoding Variants.” Nature\nGenetics 48 (2): 214–20. https://doi.org/10.1038/ng.3477.\n\n\nJagadeesh, Karthik A., Aaron M. Wenger, Mark J. Berger, Harendra Guturu,\nPeter D. Stenson, David N. Cooper, Jonathan A. Bernstein, and Gill\nBejerano. 2016. “M-CAP Eliminates a Majority of\nVariants of Uncertain Significance in Clinical Exomes at High\nSensitivity.” Nature Genetics 48 (12): 1581–86. https://doi.org/10.1038/ng.3703.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F.\nMcRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A.\nKosmicki, et al. 2019. “[SpliceAI]\nPredicting Splicing from Primary\nSequence with Deep\nLearning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJawahar, Ganesh, Benoît Sagot, and Djamé Seddah. 2019. “What Does\nBERT Learn about the Structure of Language?” In\nACL 2019 - 57th Annual\nMeeting of the Association for\nComputational Linguistics. Florence,\nItaly. https://inria.hal.science/hal-02131630.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021.\n“DNABERT: Pre-Trained Bidirectional\nEncoder Representations from\nTransformers Model for DNA-Language in\nGenome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nJiang, Tao, Yongzhuang Liu, Yue Jiang, Junyi Li, Yan Gao, Zhe Cui,\nYadong Liu, Bo Liu, and Yadong Wang. 2020. “Long-Read-Based Human\nGenomic Structural Variation Detection with cuteSV.” Genome Biology 21 (1):\n189. https://doi.org/10.1186/s13059-020-02107-y.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael\nFigurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.\n“[AlphaFold2] Highly Accurate Protein\nStructure Prediction with AlphaFold.”\nNature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nJurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge,\nand René Jäkel. 2024. “SetQuence &\nSetOmic: Deep Set Transformers for Whole\nGenome and Exome Tumour Analysis.” BioSystems 235\n(January): 105095. https://doi.org/10.1016/j.biosystems.2023.105095.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A.\nSloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data\nNavigation on the ENCODE Portal.” Nature\nCommunications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. “Scaling Laws for Neural\nLanguage Models.” arXiv. https://doi.org/10.48550/arXiv.2001.08361.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B.\nCummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020.\n“The Mutational Constraint Spectrum Quantified from Variation in\n141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKatzman, Jared L., Uri Shaham, Alexander Cloninger, Jonathan Bates,\nTingting Jiang, and Yuval Kluger. 2018. “DeepSurv:\nPersonalized Treatment Recommender System Using a Cox\nProportional Hazards Deep Neural Network.” BMC Medical\nResearch Methodology 18 (1): 24. https://doi.org/10.1186/s12874-018-0482-1.\n\n\nKaye, Jane, Edgar A. Whitley, David Lund, Michael Morrison, Harriet\nTeare, and Karen Melham. 2015. “Dynamic Consent: A Patient\nInterface for Twenty-First Century Research Networks.”\nEuropean Journal of Human Genetics 23 (2): 141–46. https://doi.org/10.1038/ejhg.2014.71.\n\n\nKelley, David R. 2020. “[Basenji2]\nCross-Species Regulatory Sequence Activity\nPrediction.” PLOS Computational Biology 16 (7):\ne1008050. https://doi.org/10.1371/journal.pcbi.1008050.\n\n\nKelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger,\nCory Y. McLean, and Jasper Snoek. 2018. “[Basenji2]\nSequential Regulatory Activity Prediction Across\nChromosomes with Convolutional Neural Networks.” Genome\nResearch 28 (5): 739–50. https://doi.org/10.1101/gr.227819.117.\n\n\nKelley, David R., Jasper Snoek, and John L. Rinn. 2016. “Basset:\nLearning the Regulatory Code of the Accessible Genome with Deep\nConvolutional Neural Networks.” Genome Research 26 (7):\n990–99. https://doi.org/10.1101/gr.200535.115.\n\n\nKhera, Amit V., and Sekar Kathiresan. 2017. “Genetics of Coronary\nArtery Disease: Discovery, Biology and Clinical Translation.”\nNature Reviews Genetics 18 (6): 331–44. https://doi.org/10.1038/nrg.2016.160.\n\n\nKichaev, Gleb, Megan Roytman, Ruth Johnson, Eleazar Eskin, Sara\nLindström, Peter Kraft, and Bogdan Pasaniuc. 2017. “Improved\nMethods for Multi-Trait Fine Mapping of Pleiotropic Risk Loci.”\nBioinformatics 33 (2): 248–55. https://doi.org/10.1093/bioinformatics/btw615.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory\nM. Cooper, and Jay Shendure. 2014. “A General Framework for\nEstimating the Relative Pathogenicity of Human Genetic Variants.”\nNature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nKong, Augustine, Michael L. Frigge, Gisli Masson, Soren Besenbacher,\nPatrick Sulem, Gisli Magnusson, Sigurjon A. Gudjonsson, et al. 2012.\n“Rate of de Novo Mutations and the Importance of Father’s Age to\nDisease Risk.” Nature 488 (7412): 471–75. https://doi.org/10.1038/nature11396.\n\n\nKrusche, Peter, Len Trigg, Paul C. Boutros, Christopher E. Mason,\nFrancisco M. De La Vega, Benjamin L. Moore, Mar Gonzalez-Porta, et al.\n2019. “Best Practices for Benchmarking\nGermline Small Variant\nCalls in Human Genomes.”\nNature Biotechnology 37 (5): 555–60. https://doi.org/10.1038/s41587-019-0054-x.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela\nYen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015.\n“Integrative Analysis of 111 Reference Human Epigenomes.”\nNature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nKurki, Mitja I., Juha Karjalainen, Priit Palta, Timo P. Sipilä, Kati\nKristiansson, Kati M. Donner, Mary P. Reeve, et al. 2023.\n“FinnGen Provides Genetic Insights from a\nWell-Phenotyped Isolated Population.” Nature 613 (7944):\n508–18. https://doi.org/10.1038/s41586-022-05473-8.\n\n\nLambert, Samuel A, Gad Abraham, and Michael Inouye. 2019. “Towards\nClinical Utility of Polygenic Risk Scores.” Human Molecular\nGenetics 28 (R2): R133–42. https://doi.org/10.1093/hmg/ddz187.\n\n\nLambert, Samuel A., Laurent Gil, Simon Jupp, Scott C. Ritchie, Yu Xu,\nAnnalisa Buniello, Aoife McMahon, et al. 2021. “The\nPolygenic Score Catalog as an\nOpen Database for Reproducibility and Systematic Evaluation.”\nNature Genetics 53 (4): 420–25. https://doi.org/10.1038/s41588-021-00783-5.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen\nChao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018.\n“ClinVar: Improving Access to Variant Interpretations\nand Supporting Evidence.” Nucleic Acids Research 46\n(D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam,\nAmit R. Majithia, and Trey Ideker. 2025. “[G2PT]\nA Genotype-Phenotype Transformer to Assess and Explain\nPolygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Hao, Zebei Han, Yu Sun, Fu Wang, Pengzhen Hu, Yuang Gao, Xuemei Bai,\net al. 2024. “CGMega: Explainable Graph Neural\nNetwork Framework with Attention Mechanisms for Cancer Gene Module\nDissection.” Nature Communications 15 (1): 5997. https://doi.org/10.1038/s41467-024-50426-6.\n\n\nLi, Heng. 2013. “Aligning Sequence Reads, Clone Sequences and\nAssembly Contigs with BWA-MEM.” arXiv.\nhttps://doi.org/10.48550/arXiv.1303.3997.\n\n\n———. 2014. “Towards Better Understanding\nof Artifacts in Variant Calling\nfrom High-Coverage\nSamples.” Bioinformatics 30 (20): 2843–51.\nhttps://doi.org/10.1093/bioinformatics/btu356.\n\n\n———. 2018. “Minimap2: Pairwise Alignment for Nucleotide\nSequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLi, Sizhen, Saeed Moayedpour, Ruijiang Li, Michael Bailey, Saleh Riahi,\nMilad Miladi, Jacob Miner, et al. 2023. “CodonBERT:\nLarge Language Models for mRNA Design and Optimization.” bioRxiv. https://doi.org/10.1101/2023.09.09.556981.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and\nYunping Zhu. 2022. “MoGCN: A\nMulti-Omics Integration\nMethod Based on Graph\nConvolutional Network for Cancer\nSubtype Analysis.” Frontiers in\nGenetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nLi, Zehui, Vallijah Subasri, Yifei Shen, Dongsheng Li, Yiren Zhao,\nGuy-Bart Stan, and Caihua Shan. 2025. “Omni-DNA:\nA Unified Genomic\nFoundation Model for\nCross-Modal and\nMulti-Task Learning.”\narXiv. https://doi.org/10.48550/arXiv.2502.03499.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness,\nGlenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome\nReference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting\nLu, Allan dos Santos Costa, et al. 2022. “[ESM-2]\nLanguage Models of Protein Sequences at the Scale of\nEvolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and\nDavid R. Kelley. 2025. “[Borzoi]\nPredicting RNA-Seq Coverage from\nDNA Sequence as a Unifying Model of Gene\nRegulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nLipsitch, Marc, Eric Tchetgen Tchetgen, and Ted Cohen. 2010.\n“Negative Controls: A Tool\nfor Detecting Confounding and\nBias in Observational\nStudies.” Epidemiology 21 (3): 383. https://doi.org/10.1097/EDE.0b013e3181d61eeb.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang,\nYucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025.\n“Life-Code: Central Dogma\nModeling with Multi-Omics\nSequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nLoh, Po-Ru, Petr Danecek, Pier Francesco Palamara, Christian\nFuchsberger, Yakir A Reshef, Hilary K Finucane, Sebastian Schoenherr, et\nal. 2016. “Reference-Based Phasing Using the\nHaplotype Reference Consortium\nPanel.” Nature Genetics 48 (11): 1443–48. https://doi.org/10.1038/ng.3679.\n\n\nLORD JUSTICE ARNOLD&lt;br&gt;LADY JUSTICE ELISABETH\nLAING&lt;br&gt;and&lt;br&gt;LORD JUSTICE BIRSS. 2021. “Thaler v\nComptroller General of Patents\nTrade Marks And\nDesigns [2021] EWCA Civ\n1374.” https://www.bailii.org/ew/cases/EWCA/Civ/2021/1374.html.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2019. “Decoupled\nWeight Decay\nRegularization.” arXiv. https://doi.org/10.48550/arXiv.1711.05101.\n\n\nLupiáñez, Darío G., Katerina Kraft, Verena Heinrich, Peter Krawitz,\nFrancesco Brancati, Eva Klopocki, Denise Horn, et al. 2015.\n“Disruptions of Topological Chromatin\nDomains Cause Pathogenic\nRewiring of Gene-Enhancer\nInteractions.” Cell 161 (5): 1012–25. https://doi.org/10.1016/j.cell.2015.04.004.\n\n\nLynch, Thomas J., Daphne W. Bell, Raffaella Sordella, Sarada\nGurubhagavatula, Ross A. Okimoto, Brian W. Brannigan, Patricia L.\nHarris, et al. 2004. “Activating Mutations in the\nEpidermal Growth Factor\nReceptor Underlying\nResponsiveness of\nNon–Small-Cell Lung\nCancer to Gefitinib.” New England\nJournal of Medicine 350 (21): 2129–39. https://doi.org/10.1056/NEJMoa040938.\n\n\nMadani, Ali, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P.\nMohr, James M. Holton, Jose Luis Olmos, et al. 2023. “Large\nLanguage Models Generate Functional Protein Sequences Across Diverse\nFamilies.” Nature Biotechnology 41 (8): 1099–1106. https://doi.org/10.1038/s41587-022-01618-2.\n\n\nMallal, Simon, Elizabeth Phillips, Giampiero Carosi, Jean-Michel Molina,\nCassy Workman, Janez Tomažič, Eva Jägel-Guedes, et al. 2008.\n“HLA-B*5701 Screening for\nHypersensitivity to Abacavir.” New\nEngland Journal of Medicine 358 (6): 568–79. https://doi.org/10.1056/NEJMoa0706135.\n\n\nMaller, Julian B., Gilean McVean, Jake Byrnes, Damjan Vukcevic, Kimmo\nPalin, Zhan Su, Joanna M. M. Howson, et al. 2012. “Bayesian\nRefinement of Association Signals for 14 Loci in 3 Common\nDiseases.” Nature Genetics 44 (12): 1294–1301. https://doi.org/10.1038/ng.2435.\n\n\nManolio, Teri A., Francis S. Collins, Nancy J. Cox, David B. Goldstein,\nLucia A. Hindorff, David J. Hunter, Mark I. McCarthy, et al. 2009.\n“Finding the Missing Heritability of Complex Diseases.”\nNature 461 (7265): 747–53. https://doi.org/10.1038/nature08494.\n\n\nManzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025.\n“Comparative Analysis of Deep\nLearning Models for Predicting\nCausative Regulatory\nVariants.” bioRxiv: The Preprint Server for\nBiology, June, 2025.05.19.654920. https://doi.org/10.1101/2025.05.19.654920.\n\n\nMarees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan,\nEmmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018.\n“[GWAS] A Tutorial on Conducting\nGenome-Wide Association Studies: Quality Control and\nStatistical Analysis.” International Journal of Methods in\nPsychiatric Research 27 (2): e1608. https://doi.org/10.1002/mpr.1608.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen,\nDennis Pultz, Ole Winther, and Wouter Boomsma. 2024.\n“BEND: Benchmarking DNA\nLanguage Models on Biologically Meaningful\nTasks.” arXiv. https://doi.org/10.48550/arXiv.2311.12570.\n\n\nMárquez-Luna, Carla, Po-Ru Loh, South Asian Type 2 Diabetes (SAT2D)\nConsortium, The SIGMA Type 2 Diabetes Consortium, and Alkes L. Price.\n2017. “Multiethnic Polygenic Risk Scores Improve Risk Prediction\nin Diverse Populations.” Genetic Epidemiology 41 (8):\n811–23. https://doi.org/10.1002/gepi.22083.\n\n\nMartin, Alicia R., Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada,\nBenjamin M. Neale, and Mark J. Daly. 2019. “Clinical Use of\nCurrent Polygenic Risk Scores May Exacerbate Health Disparities.”\nNature Genetics 51 (4): 584–91. https://doi.org/10.1038/s41588-019-0379-x.\n\n\nMavaddat, Nasim, Kyriaki Michailidou, Joe Dennis, Michael Lush, Laura\nFachal, Andrew Lee, Jonathan P. Tyrer, et al. 2019. “Polygenic\nRisk Scores for Prediction of\nBreast Cancer and Breast\nCancer Subtypes.” The American\nJournal of Human Genetics 104 (1): 21–34. https://doi.org/10.1016/j.ajhg.2018.11.002.\n\n\nMcCloskey, Michael, and Neal Cohen. 1989. “Catastrophic\nInterference in Connectionist\nNetworks: The Sequential\nLearning Problem.” In Psychology of\nLearning and Motivation, 24:109–65.\nAcademic Press. https://doi.org/10.1016/S0079-7421(08)60536-8.\n\n\n“Medical Devices; Laboratory\nDeveloped Tests.” 2024. Federal\nRegister. https://www.federalregister.gov/documents/2024/05/06/2024-08935/medical-devices-laboratory-developed-tests.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill\nVishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel,\nRonnie Rajan, and Shadab Khan. 2025. “BioToken and\nBioFM – Biologically-Informed\nTokenization Enables Accurate and\nEfficient Genomic Foundation\nModels.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and\nAlexander Rives. 2021. “[ESM-1v]\nLanguage Models Enable Zero-Shot Prediction of the Effects\nof Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy\nVasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and\nTimnit Gebru. 2019. “Model Cards for\nModel Reporting.” In Proceedings of\nthe Conference on Fairness,\nAccountability, and Transparency, 220–29.\nFAT* ’19. New York, NY, USA: Association for Computing\nMachinery. https://doi.org/10.1145/3287560.3287596.\n\n\nMorales, Joannella, Shashikant Pujar, Jane E. Loveland, Alex Astashyn,\nRuth Bennett, Andrew Berry, Eric Cox, et al. 2022. “A Joint\nNCBI and EMBL-EBI Transcript Set\nfor Clinical Genomics and Research.” Nature 604 (7905):\n310–15. https://doi.org/10.1038/s41586-022-04558-8.\n\n\nMorcos, Faruck, Andrea Pagnani, Bryan Lunt, Arianna Bertolino, Debora S.\nMarks, Chris Sander, Riccardo Zecchina, José N. Onuchic, Terence Hwa,\nand Martin Weigt. 2011. “Direct-Coupling Analysis of Residue\nCoevolution Captures Native Contacts Across Many Protein\nFamilies.” Proceedings of the National Academy of\nSciences 108 (49): E1293–1301. https://doi.org/10.1073/pnas.1111471108.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy\nSchwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021.\n“An Open Approach to Systematically Prioritize Causal Variants and\nGenes at All Published Human GWAS Trait-Associated\nLoci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nMukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom\nSoare, Raghav Tandon, David Amar, et al. 2024.\n“EmbedGEM: A Framework to Evaluate the Utility of\nEmbeddings for Genetic Discovery.” Bioinformatics\nAdvances 4 (1). https://doi.org/10.1093/bioadv/vbae135.\n\n\nNaghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B.\nMacdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow\nCoyote-Maestas, and Hani Goodarzi. 2024. “[cdsFM - EnCodon/DeCodon]\nA Suite of Foundation\nModels Captures the Contextual\nInterplay Between Codons.”\nbioRxiv. https://doi.org/10.1101/2024.10.10.617568.\n\n\nNagpal, Chirag, Xinyu Li, and Artur Dubrawski. 2021. “Deep\nSurvival Machines: Fully\nParametric Survival Regression\nand Representation Learning for\nCensored Data With\nCompeting Risks.” IEEE Journal of\nBiomedical and Health Informatics 25 (8): 3163–75. https://doi.org/10.1109/JBHI.2021.3052441.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT:\nPredicting Amino Acid Changes That Affect Protein\nFunction.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nNguengang Wakap, Stéphanie, Deborah M. Lambert, Annie Olry, Charlotte\nRodwell, Charlotte Gueydan, Valérie Lanneau, Daniel Murphy, Yann Le Cam,\nand Ana Rath. 2020. “Estimating Cumulative Point Prevalence of\nRare Diseases: Analysis of the Orphanet Database.”\nEuropean Journal of Human Genetics 28 (2): 165–73. https://doi.org/10.1038/s41431-019-0508-0.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva\nKatrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence\nModeling and Design from Molecular to Genome Scale with\nEvo.” Science 386 (6723): eado9336. https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum\nBirch-Sykes, Michael Wornow, Aman Patel, et al. 2023.\n“HyenaDNA: Long-Range\nGenomic Sequence Modeling at\nSingle Nucleotide\nResolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nNielsen, Rasmus, Joshua S. Paul, Anders Albrechtsen, and Yun S. Song.\n2011. “Genotype and SNP Calling from Next-Generation\nSequencing Data.” Nature Reviews. Genetics 12 (6):\n443–51. https://doi.org/10.1038/nrg2986.\n\n\nNofziger, Charity, Amy J. Turner, Katrin Sangkuhl, Michelle\nWhirl-Carrillo, José A. G. Agúndez, John L. Black, Henry M.\nDunnenberger, et al. 2019. “PharmVar\nGeneFocus: CYP2D6.” Clinical\nPharmacology & Therapeutics 107 (1): 154–70. https://doi.org/10.1002/cpt.1643.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk,\nSteffanie Paul, Han Spinner, Nathan Rollins, et al. 2023a.\n“ProteinGym: Large-Scale\nBenchmarks for Protein Fitness\nPrediction and Design.” Advances in\nNeural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\n———, et al. 2023b. “ProteinGym:\nLarge-Scale Benchmarks for\nProtein Fitness Prediction and\nDesign.” Advances in Neural Information\nProcessing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nnull, null. 2019. “The ‘All of\nUs’ Research\nProgram.” New England Journal of Medicine\n381 (7): 668–76. https://doi.org/10.1056/NEJMsr1809937.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V.\nBzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The\nComplete Sequence of a Human Genome.” Science 376\n(6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Connell, Jared, Deepti Gurdasani, Olivier Delaneau, Nicola Pirastu,\nSheila Ulivi, Massimiliano Cocca, Michela Traglia, et al. 2014. “A\nGeneral Approach for Haplotype\nPhasing Across the Full Spectrum\nof Relatedness.” PLOS Genetics 10 (4):\ne1004234. https://doi.org/10.1371/journal.pgen.1004234.\n\n\nO’Leary, Nuala A., Mathew W. Wright, J. Rodney Brister, Stacy Ciufo,\nDiana Haddad, Rich McVeigh, Bhanu Rajput, et al. 2016. “Reference\nSequence (RefSeq) Database at NCBI: Current\nStatus, Taxonomic Expansion, and Functional Annotation.”\nNucleic Acids Research 44 (D1): D733–45. https://doi.org/10.1093/nar/gkv1189.\n\n\nOord, Aaron van den, Yazhe Li, and Oriol Vinyals. 2019.\n“Representation Learning with\nContrastive Predictive\nCoding.” arXiv. https://doi.org/10.48550/arXiv.1807.03748.\n\n\nOrenbuch, Rose, Courtney A. Shearer, Aaron W. Kollasch, Aviv D. Spinner,\nThomas Hopf, Lood van Niekerk, Dinko Franceschi, Mafalda Dias, Jonathan\nFrazer, and Debora S. Marks. 2025. “[popEVE] Proteome-Wide Model for Human\nDisease Genetics.” Nature Genetics, November, 1–10. https://doi.org/10.1038/s41588-025-02400-1.\n\n\n“PacificBiosciences/Pbsv.” 2025. PacBio. https://github.com/PacificBiosciences/pbsv.\n\n\nParasuraman, Raja, and Dietrich H. Manzey. 2010. “Complacency and\nBias in Human Use of\nAutomation: An Attentional\nIntegration.” Human Factors 52 (3):\n381–410. https://doi.org/10.1177/0018720810376055.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006.\n“Population Structure and\nEigenanalysis.” PLOS Genetics 2 (12): e190.\nhttps://doi.org/10.1371/journal.pgen.0020190.\n\n\nPe’er, Itsik, Roman Yelensky, David Altshuler, and Mark J. Daly. 2008.\n“Estimation of the Multiple Testing Burden for Genomewide\nAssociation Studies of Nearly All Common Variants.” Genetic\nEpidemiology 32 (4): 381–85. https://doi.org/10.1002/gepi.20303.\n\n\nPearce, James D., Sara E. Simmonds, Gita Mahmoudabadi, Lakshmi Krishnan,\nGiovanni Palla, Ana-Maria Istrate, Alexander Tarashansky, et al. 2025.\n“[TranscriptFormer]\nCross-Species Generative\nCell Atlas Across 1.5\nBillion Years of Evolution:\nThe TranscriptFormer Single-Cell\nModel.” bioRxiv. https://doi.org/10.1101/2025.04.25.650731.\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel,\nSean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022.\n“Calibration of Computational Tools for Missense Variant\nPathogenicity Classification and ClinGen Recommendations\nfor PP3/BP4 Criteria.” American\nJournal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nPoli, Michael, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao,\nStephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. 2023.\n“Hyena Hierarchy: Towards\nLarger Convolutional Language\nModels.” In Proceedings of the 40th\nInternational Conference on\nMachine Learning, 28043–78. PMLR. https://proceedings.mlr.press/v202/poli23a.html.\n\n\nPoplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas\nColthurst, Alexander Ku, Dan Newburger, et al. 2018.\n“[DeepVariant] A Universal\nSNP and Small-Indel Variant Caller Using Deep Neural\nNetworks.” Nature Biotechnology 36 (10): 983–87. https://doi.org/10.1038/nbt.4235.\n\n\nPress, Ofir, Noah A. Smith, and Mike Lewis. 2022. “Train\nShort, Test Long:\nAttention with Linear Biases\nEnables Input Length\nExtrapolation.” arXiv. https://doi.org/10.48550/arXiv.2108.12409.\n\n\nPrice, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E.\nWeinblatt, Nancy A. Shadick, and David Reich. 2006. “Principal\nComponents Analysis Corrects for Stratification in Genome-Wide\nAssociation Studies.” Nature Genetics 38 (8): 904–9. https://doi.org/10.1038/ng1847.\n\n\nQuang, Daniel, Yifei Chen, and Xiaohui Xie. 2015.\n“DANN: A Deep Learning Approach for Annotating the\nPathogenicity of Genetic Variants.” Bioinformatics 31\n(5): 761–63. https://doi.org/10.1093/bioinformatics/btu703.\n\n\nQuang, Daniel, and Xiaohui Xie. 2016. “DanQ: A Hybrid\nConvolutional and Recurrent Deep Neural Network for Quantifying the\nFunction of DNA Sequences.” Nucleic Acids\nResearch 44 (11): e107. https://doi.org/10.1093/nar/gkw226.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023.\n“Exploring the Limits of Transfer\nLearning with a Unified\nText-to-Text Transformer.”\narXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025.\n“[MIFM] Multiple Instance Fine-Mapping:\nPredicting Causal Regulatory Variants with a Deep Sequence\nModel.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nRao, Roshan, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John\nCanny, Pieter Abbeel, and Yun S. Song. 2019. “Evaluating\nProtein Transfer Learning with\nTAPE.” arXiv. https://doi.org/10.48550/arXiv.1906.08230.\n\n\nRao, Roshan, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander\nRives. 2020. “Transformer Protein Language Models Are Unsupervised\nStructure Learners.” bioRxiv. https://doi.org/10.1101/2020.12.15.422761.\n\n\n“RealTimeGenomics/Rtg-Core.” 2025. Real Time\nGenomics. https://github.com/RealTimeGenomics/rtg-core.\n\n\nRegev, Aviv, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe\nBenoist, Ewan Birney, Bernd Bodenmiller, et al. 2017. “The\nHuman Cell Atlas.” Edited\nby Thomas R Gingeras. eLife 6 (December): e27041. https://doi.org/10.7554/eLife.27041.\n\n\n“Regulation (EU) 2016/679 of the\nEuropean Parliament and of the\nCouncil of 27 April 2016 on the Protection of\nNatural Persons with Regard to the Processing of Personal Data and on\nthe Free Movement of Such Data, and Repealing Directive\n95/46/EC (General Data\nProtection Regulation) (Text with\nEEA Relevance).” 2016. http://data.europa.eu/eli/reg/2016/679/oj.\n\n\n“Regulation (EU) 2017/745 of the\nEuropean Parliament and of the\nCouncil of 5 April 2017 on Medical Devices,\nAmending Directive 2001/83/EC,\nRegulation (EC) No 178/2002 and\nRegulation (EC) No 1223/2009 and\nRepealing Council Directives\n90/385/EEC and 93/42/EEC (Text\nwith EEA Relevance. ).” 2017. http://data.europa.eu/eli/reg/2017/745/oj.\n\n\n“Regulation (EU) 2024/1689 of the\nEuropean Parliament and of the\nCouncil of 13 June 2024 Laying down Harmonised\nRules on Artificial Intelligence and Amending Regulations\n(EC) No 300/2008, (EU)\nNo 167/2013, (EU) No 168/2013,\n(EU) 2018/858, (EU) 2018/1139 and\n(EU) 2019/2144 and Directives\n2014/90/EU, (EU) 2016/797 and\n(EU) 2020/1828 (Artificial\nIntelligence Act) (Text with\nEEA Relevance).” 2024. http://data.europa.eu/eli/reg/2024/1689/oj.\n\n\nRehm, Heidi L., Jonathan S. Berg, Lisa D. Brooks, Carlos D. Bustamante,\nJames P. Evans, Melissa J. Landrum, David H. Ledbetter, et al. 2015.\n“ClinGen — The Clinical\nGenome Resource.” New England\nJournal of Medicine 372 (23): 2235–42. https://doi.org/10.1056/NEJMsr1406261.\n\n\nRelling, Mary V., Teri E. Klein, Roseann S. Gammal, Michelle\nWhirl-Carrillo, James M. Hoffman, and Kelly E. Caudle. 2019. “The\nClinical Pharmacogenetics\nImplementation Consortium: 10\nYears Later.” Clinical Pharmacology\n& Therapeutics 107 (1): 171–75. https://doi.org/10.1002/cpt.1651.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and\nMartin Kircher. 2019. “CADD: Predicting the\nDeleteriousness of Variants Throughout the Human Genome.”\nNucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie\nGastier-Foster, Wayne W. Grody, et al. 2015. “Standards and\nGuidelines for the Interpretation of Sequence Variants: A Joint\nConsensus Recommendation of the American\nCollege of Medical Genetics and\nGenomics and the Association for\nMolecular Pathology.” Genetics in\nMedicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nRieke, Nicola, Jonny Hancox, Wenqi Li, Fausto Milletarì, Holger R. Roth,\nShadi Albarqouni, Spyridon Bakas, et al. 2020. “The Future of\nDigital Health with Federated Learning.” Npj Digital\nMedicine 3 (1): 119. https://doi.org/10.1038/s41746-020-00323-1.\n\n\nRisch, Neil, and Kathleen Merikangas. 1996. “The\nFuture of Genetic Studies of\nComplex Human Diseases.”\nScience 273 (5281): 1516–17. https://doi.org/10.1126/science.273.5281.1516.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin,\nJason Liu, Demi Guo, et al. 2021a. “[ESM-1b]\nBiological Structure and Function Emerge from Scaling\nUnsupervised Learning to 250 Million Protein Sequences.”\nProceedings of the National Academy of Sciences of the United States\nof America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\n———, et al. 2021b. “[ESM-1b] Biological\nStructure and Function Emerge from Scaling Unsupervised Learning to 250\nMillion Protein Sequences.” Proceedings of the National\nAcademy of Sciences of the United States of America 118 (15):\ne2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nRobinson, James, Dominic J Barker, Xenia Georgiou, Michael A Cooper,\nPaul Flicek, and Steven G E Marsh. 2020.\n“IPD-IMGT/HLA\nDatabase.” Nucleic Acids Research 48 (D1):\nD948–55. https://doi.org/10.1093/nar/gkz950.\n\n\nRuan, Yunfeng, Yen-Feng Lin, Yen-Chen Anne Feng, Chia-Yen Chen, Max Lam,\nZhenglin Guo, Lin He, et al. 2022. “Improving Polygenic Prediction\nin Ancestrally Diverse Populations.” Nature Genetics 54\n(5): 573–80. https://doi.org/10.1038/s41588-022-01054-7.\n\n\nRubin, Alan F., Hannah Gelman, Nathan Lucas, Sandra M. Bajjalieh,\nAnthony T. Papenfuss, Terence P. Speed, and Douglas M. Fowler. 2017.\n“A Statistical Framework for Analyzing Deep Mutational Scanning\nData.” Genome Biology 18 (1): 150. https://doi.org/10.1186/s13059-017-1272-5.\n\n\nRubinacci, Simone, Diogo M. Ribeiro, Robin J. Hofmeister, and Olivier\nDelaneau. 2021. “Efficient Phasing and Imputation of Low-Coverage\nSequencing Data Using Large Reference Panels.” Nature\nGenetics 53 (1): 120–26. https://doi.org/10.1038/s41588-020-00756-0.\n\n\nSainz, Oscar, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez\nde Lacalle, and Eneko Agirre. 2023. “NLP\nEvaluation in Trouble: On the\nNeed to Measure LLM\nData Contamination for Each\nBenchmark.” In Findings of the\nAssociation for Computational\nLinguistics: EMNLP 2023, edited by Houda\nBouamor, Juan Pino, and Kalika Bali, 10776–87. Singapore: Association\nfor Computational Linguistics. https://doi.org/10.18653/v1/2023.findings-emnlp.722.\n\n\nSakaue, Saori, Saisriram Gurajala, Michelle Curtis, Yang Luo, Wanson\nChoi, Kazuyoshi Ishigaki, Joyce B. Kang, et al. 2023. “Tutorial: A\nStatistical Genetics Guide to Identifying HLA Alleles\nDriving Complex Disease.” Nature Protocols 18 (9):\n2625–41. https://doi.org/10.1038/s41596-023-00853-4.\n\n\nSample, Paul J., Ban Wang, David W. Reid, Vlad Presnyak, Iain J.\nMcFadyen, David R. Morris, and Georg Seelig. 2019. “Human 5′\nUTR Design and Variant Effect Prediction from a Massively\nParallel Translation Assay.” Nature Biotechnology 37\n(7): 803–9. https://doi.org/10.1038/s41587-019-0164-5.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch.\n2024. “[GROVER] DNA Language Model\nGROVER Learns Sequence Context in the Human Genome.”\nNature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSangkuhl, Katrin, Michelle Whirl-Carrillo, Ryan M. Whaley, Mark Woon,\nAdam Lavertu, Russ B. Altman, Lester Carter, Anurag Verma, Marylyn D.\nRitchie, and Teri E. Klein. 2019. “Pharmacogenomics\nClinical Annotation Tool\n(PharmCAT).” Clinical Pharmacology &\nTherapeutics 107 (1): 203–10. https://doi.org/10.1002/cpt.1568.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and\nVolodymyr Kuleshov. 2024. “Caduceus:\nBi-Directional Equivariant\nLong-Range DNA\nSequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and\nMartin Kircher. 2024. “CADD V1.7: Using Protein\nLanguage Models, Regulatory CNNs and Other Nucleotide-Level\nScores to Improve Genome-Wide Variant Predictions.” Nucleic\nAcids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nShafin, Kishwar, Trevor Pesout, Pi-Chuan Chang, Maria Nattestad, Alexey\nKolesnikov, Sidharth Goel, Gunjan Baid, et al. 2021.\n“Haplotype-Aware Variant Calling with\nPEPPER-Margin-DeepVariant Enables\nHigh Accuracy in Nanopore Long-Reads.” Nature Methods 18\n(11): 1322–32. https://doi.org/10.1038/s41592-021-01299-w.\n\n\nSherry, S. T., M.-H. Ward, M. Kholodov, J. Baker, L. Phan, E. M.\nSmigielski, and K. Sirotkin. 2001. “dbSNP: The NCBI Database of Genetic\nVariation.” Nucleic Acids Research 29 (1): 308–11. https://doi.org/10.1093/nar/29.1.308.\n\n\nShevlane, Toby. 2022. “Structured Access: An Emerging Paradigm for\nSafe AI Deployment.” arXiv. https://doi.org/10.48550/arXiv.2201.05159.\n\n\nShrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. 2017.\n“Learning Important Features\nThrough Propagating Activation\nDifferences.” In Proceedings of the 34th\nInternational Conference on\nMachine Learning, 3145–53. PMLR. https://proceedings.mlr.press/v70/shrikumar17a.html.\n\n\nShrikumar, Avanti, Katherine Tian, Žiga Avsec, Anna Shcherbina,\nAbhimanyu Banerjee, Mahfuza Sharmin, Surag Nair, and Anshul Kundaje.\n2018. “Technical Note on Transcription\nFactor Motif Discovery from\nImportance Scores\n(TF-MoDISco) Version 0.5.6.5.” arXiv.\nhttps://doi.org/10.48550/arXiv.1811.00416.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs,\nMinmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005.\n“[PhastCons] Evolutionarily Conserved\nElements in Vertebrate, Insect, Worm, and Yeast Genomes.”\nGenome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.\n\n\nSirugo, Giorgio, Scott M. Williams, and Sarah A. Tishkoff. 2019.\n“The Missing Diversity in\nHuman Genetic Studies.”\nCell 177 (1): 26–31. https://doi.org/10.1016/j.cell.2019.02.048.\n\n\nSmolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W.\nHorner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024.\n“Detection of Mosaic and Population-Level Structural Variants with\nSniffles2.” Nature Biotechnology 42 (10):\n1571–80. https://doi.org/10.1038/s41587-023-02024-y.\n\n\nSnell, Jake, Kevin Swersky, and Richard Zemel. 2017. “Prototypical\nNetworks for Few-Shot\nLearning.” In Advances in Neural\nInformation Processing\nSystems. Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html.\n\n\n“Software as a Medical Device\n(SaMD): Clinical Evaluation\n International Medical\nDevice Regulators Forum.”\n2017. https://www.imdrf.org/documents/software-medical-device-samd-clinical-evaluation.\n\n\n“Software as a Medical Device:\nPossible Framework for Risk\nCategorization and Corresponding\nConsiderations  International\nMedical Device Regulators\nForum.” 2014. https://www.imdrf.org/documents/software-medical-device-possible-framework-risk-categorization-and-corresponding-considerations.\n\n\nSoice, Emily H., Rafael Rocha, Kimberlee Cordova, Michael Specter, and\nKevin M. Esvelt. 2023. “Can Large Language Models Democratize\nAccess to Dual-Use Biotechnology?” arXiv. https://doi.org/10.48550/arXiv.2306.03809.\n\n\nSollis, Elliot, Abayomi Mosaku, Ala Abid, Annalisa Buniello, Maria\nCerezo, Laurent Gil, Tudor Groza, et al. 2023. “The\nNHGRI-EBI GWAS\nCatalog: Knowledgebase and Deposition Resource.”\nNucleic Acids Research 51 (D1): D977–85. https://doi.org/10.1093/nar/gkac1010.\n\n\nSong, Li, Gali Bai, X. Shirley Liu, Bo Li, and Heng Li. 2022.\n“T1K: Efficient and Accurate KIR and\nHLA Genotyping with Next-Generation Sequencing\nData.” bioRxiv. https://doi.org/10.1101/2022.10.26.513955.\n\n\nStenson, Peter D., Matthew Mort, Edward V. Ball, Katy Evans, Matthew\nHayden, Sally Heywood, Michelle Hussain, Andrew D. Phillips, and David\nN. Cooper. 2017. “The Human Gene\nMutation Database: Towards a Comprehensive\nRepository of Inherited Mutation Data for Medical Research, Genetic\nDiagnosis and Next-Generation Sequencing Studies.” Human\nGenetics 136 (6): 665–77. https://doi.org/10.1007/s00439-017-1779-6.\n\n\nSu, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng\nLiu. 2024. “RoFormer: Enhanced\nTransformer with Rotary Position\nEmbedding.” Neurocomputing 568 (February):\n127063. https://doi.org/10.1016/j.neucom.2023.127063.\n\n\nSudlow, Cathie, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton,\nJohn Danesh, Paul Downey, et al. 2015. “UK\nBiobank: An Open\nAccess Resource for Identifying\nthe Causes of a Wide Range of\nComplex Diseases of Middle and\nOld Age.” PLOS Medicine 12\n(3): e1001779. https://doi.org/10.1371/journal.pmed.1001779.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N.\nPhan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023.\n“Leveraging Base-Pair Mammalian Constraint to Understand Genetic\nVariation and Human Disease.” Science 380 (6643):\neabn2937. https://doi.org/10.1126/science.abn2937.\n\n\nSundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. “Axiomatic\nAttribution for Deep\nNetworks.” In Proceedings of the 34th\nInternational Conference on\nMachine Learning, 3319–28. PMLR. https://proceedings.mlr.press/v70/sundararajan17a.html.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and\nCathy H. Wu. 2007. “UniRef: Comprehensive and\nNon-Redundant UniProt Reference Clusters.”\nBioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nTan, Jimin, Nina Shenker-Tauris, Javier Rodriguez-Hernaez, Eric Wang,\nTheodore Sakellaropoulos, Francesco Boccalatte, Palaniraja Thandapani,\net al. 2023. “Cell-Type-Specific Prediction of 3D\nChromatin Organization Enables High-Throughput in Silico Genetic\nScreening.” Nature Biotechnology 41 (8): 1140–50. https://doi.org/10.1038/s41587-022-01612-8.\n\n\nTate, John G, Sally Bamford, Harry C Jubb, Zbyslaw Sondka, David M\nBeare, Nidhi Bindal, Harry Boutselakis, et al. 2019.\n“COSMIC: The Catalogue Of\nSomatic Mutations In\nCancer.” Nucleic Acids Research 47 (D1):\nD941–47. https://doi.org/10.1093/nar/gky1015.\n\n\nTavtigian, Sean V., Marc S. Greenblatt, Steven M. Harrison, Robert L.\nNussbaum, Snehit A. Prabhu, Kenneth M. Boucher, and Leslie G. Biesecker.\n2018. “Modeling the ACMG/AMP Variant\nClassification Guidelines as a Bayesian Classification\nFramework.” Genetics in Medicine 20 (9): 1054–60. https://doi.org/10.1038/gim.2017.210.\n\n\nTHE GTEX CONSORTIUM. 2020. “The GTEx\nConsortium Atlas of Genetic Regulatory Effects Across Human\nTissues.” Science 369 (6509): 1318–30. https://doi.org/10.1126/science.aaz1776.\n\n\nTHE TABULA SAPIENS CONSORTIUM. 2022. “The Tabula\nSapiens: A Multiple-Organ, Single-Cell\nTranscriptomic Atlas of Humans.” Science 376 (6594):\neabl4896. https://doi.org/10.1126/science.abl4896.\n\n\nTheodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina\nR. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023.\n“[Geneformer] Transfer Learning Enables\nPredictions in Network Biology.” Nature 618 (7965):\n616–24. https://doi.org/10.1038/s41586-023-06139-9.\n\n\nTipirneni, Sindhu, and Chandan K. Reddy. 2022.\n“Self-Supervised Transformer for\nSparse and Irregularly Sampled\nMultivariate Clinical\nTime-Series.” ACM Trans. Knowl.\nDiscov. Data 16 (6): 105:1–17. https://doi.org/10.1145/3516367.\n\n\nTorkamani, Ali, Nathan E. Wineinger, and Eric J. Topol. 2018. “The\nPersonal and Clinical Utility of Polygenic Risk Scores.”\nNature Reviews Genetics 19 (9): 581–90. https://doi.org/10.1038/s41576-018-0018-x.\n\n\nVan der Auwera, Geraldine A., Mauricio O. Carneiro, Christopher Hartl,\nRyan Poplin, Guillermo del Angel, Ami Levy-Moonshine, Tadeusz Jordan, et\nal. 2018. “From FastQ Data to\nHigh-Confidence Variant\nCalls: The Genome\nAnalysis Toolkit Best\nPractices Pipeline.” Current\nProtocols in Bioinformatics 43 (1): 11.10.1–33. https://doi.org/10.1002/0471250953.bi1110s43.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.\n“Attention Is All You\nNeed.” arXiv. https://doi.org/10.48550/arXiv.1706.03762.\n\n\nVilhjálmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev,\nSara Lindström, Stephan Ripke, Giulio Genovese, et al. 2015.\n“Modeling Linkage Disequilibrium\nIncreases Accuracy of Polygenic\nRisk Scores.” American Journal of\nHuman Genetics 97 (4): 576–92. https://doi.org/10.1016/j.ajhg.2015.09.001.\n\n\nVisscher, Peter M., William G. Hill, and Naomi R. Wray. 2008.\n“Heritability in the Genomics Era — Concepts and\nMisconceptions.” Nature Reviews Genetics 9 (4): 255–66.\nhttps://doi.org/10.1038/nrg2322.\n\n\nVõsa, Urmo, Annique Claringbould, Harm-Jan Westra, Marc Jan Bonder,\nPatrick Deelen, Biao Zeng, Holger Kirsten, et al. 2021.\n“Large-Scale Cis- and Trans-eQTL\nAnalyses Identify Thousands of Genetic Loci and Polygenic Scores That\nRegulate Blood Gene Expression.” Nature Genetics 53 (9):\n1300–1310. https://doi.org/10.1038/s41588-021-00913-z.\n\n\nWang, Dequan, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor\nDarrell. 2021. “Tent: Fully Test-Time\nAdaptation by Entropy\nMinimization.” arXiv. https://doi.org/10.48550/arXiv.2006.10726.\n\n\nWang, Gao, Abhishek Sarkar, Peter Carbonetto, and Matthew Stephens.\n2020. “A Simple New\nApproach to Variable Selection in\nRegression, with Application to\nGenetic Fine Mapping.”\nJournal of the Royal Statistical Society Series B: Statistical\nMethodology 82 (5): 1273–1300. https://doi.org/10.1111/rssb.12388.\n\n\nWang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020.\n“Linformer: Self-Attention with\nLinear Complexity.” arXiv. https://doi.org/10.48550/arXiv.2006.04768.\n\n\nWang, Zirui, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. 2019.\n“Characterizing and Avoiding Negative\nTransfer.” In, 11293–302. https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Characterizing_and_Avoiding_Negative_Transfer_CVPR_2019_paper.html.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L.\nTrippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023.\n“De Novo Design of Protein Structure and Function with\nRFdiffusion.” Nature 620 (7976): 1089–1100.\nhttps://doi.org/10.1038/s41586-023-06415-8.\n\n\nWenger, Aaron M., Paul Peluso, William J. Rowell, Pi-Chuan Chang,\nRichard J. Hall, Gregory T. Concepcion, Jana Ebler, et al. 2019.\n“Accurate Circular Consensus Long-Read Sequencing Improves Variant\nDetection and Assembly of a Human Genome.” Nature\nBiotechnology 37 (10): 1155–62. https://doi.org/10.1038/s41587-019-0217-9.\n\n\nWhirl-Carrillo, M, E M McDonagh, J M Hebert, L Gong, K Sangkuhl, C F\nThorn, R B Altman, and T E Klein. 2012. “Pharmacogenomics\nKnowledge for Personalized\nMedicine.” Clinical Pharmacology &\nTherapeutics 92 (4): 414–17. https://doi.org/10.1038/clpt.2012.96.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray,\nPeter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping\nImproves Identification of Causal Variants.” Research\nSquare, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.\n\n\nXiong, Ruibin, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,\nHuishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. 2020. “On\nLayer Normalization in the\nTransformer Architecture.” In\nProceedings of the 37th International\nConference on Machine\nLearning, 10524–33. PMLR. https://proceedings.mlr.press/v119/xiong20b.html.\n\n\nXu, Leqi, Wangjie Zheng, Jiaqi Hu, Yingxin Lin, Jia Zhao, Gefei Wang,\nTianyu Liu, and Hongyu Zhao. 2025. “Improving Polygenic Risk\nPrediction Performance by Integrating Electronic Health Records Through\nPhenotype Embedding.” The American Journal of Human\nGenetics 112 (12): 3030–45. https://doi.org/10.1016/j.ajhg.2025.11.006.\n\n\nYang, Jian, Beben Benyamin, Brian P. McEvoy, Scott Gordon, Anjali K.\nHenders, Dale R. Nyholt, Pamela A. Madden, et al. 2010. “Common\nSNPs Explain a Large Proportion of the Heritability for\nHuman Height.” Nature Genetics 42 (7): 565–69. https://doi.org/10.1038/ng.608.\n\n\nYang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan\nSalakhutdinov, and Quoc V. Le. 2020. “XLNet:\nGeneralized Autoregressive\nPretraining for Language\nUnderstanding.” arXiv. https://doi.org/10.48550/arXiv.1906.08237.\n\n\nYengo, Loïc, Sailaja Vedantam, Eirini Marouli, Julia Sidorenko, Eric\nBartell, Saori Sakaue, Marielisa Graff, et al. 2022. “A Saturated\nMap of Common Genetic Variants Associated with Human Height.”\nNature 610 (7933): 704–12. https://doi.org/10.1038/s41586-022-05275-y.\n\n\nYeo, Gene, and Christopher B. Burge. 2004. “Maximum\nEntropy Modeling of Short\nSequence Motifs with Applications\nto RNA Splicing Signals.”\nJournal of Computational Biology 11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll,\nand Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls\nUsing DeepVariant and GLnexus.”\nBioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei\nSun, Chen-Hao Chen, et al. 2019. “Cistrome Data\nBrowser: Expanded Datasets and New Tools for Gene\nRegulatory Analysis.” Nucleic Acids Research 47 (D1):\nD729–35. https://doi.org/10.1093/nar/gky1094.\n\n\nZheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam,\nand Ruibang Luo. 2022. “Symphonizing Pileup and Full-Alignment for\nDeep Learning-Based Long-Read Variant Calling.” Nature\nComputational Science 2 (12): 797–803. https://doi.org/10.1038/s43588-022-00387-x.\n\n\nZhou, Jian. 2022. “Sequence-Based Modeling of Three-Dimensional\nGenome Architecture from Kilobase to Chromosome Scale.”\nNature Genetics 54 (5): 725–34. https://doi.org/10.1038/s41588-022-01065-4.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K.\nWong, and Olga G. Troyanskaya. 2018. “[Expecto]\nDeep Learning Sequence-Based Ab Initio Prediction of\nVariant Effects on Expression and Disease Risk.” Nature\nGenetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA]\nPredicting Effects of Noncoding Variants with Deep\nLearning–Based Sequence Model.” Nature Methods 12 (10):\n931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and\nHan Liu. 2024. “DNABERT-2: Efficient\nFoundation Model and Benchmark\nFor Multi-Species\nGenome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.\n\n\nZhu, Ligeng, Zhijian Liu, and Song Han. 2019. “Deep\nLeakage from Gradients.” arXiv. https://doi.org/10.48550/arXiv.1906.08935.\n\n\nZook, Justin M., Jennifer McDaniel, Nathan D. Olson, Justin Wagner,\nHemang Parikh, Haynes Heaton, Sean A. Irvine, et al. 2019. “An\nOpen Resource for Accurately Benchmarking Small Variant and Reference\nCalls.” Nature Biotechnology 37 (5): 561–66. https://doi.org/10.1038/s41587-019-0074-6.\n\n\nZvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang,\nCindy Orozco Bohorquez, Austin Clyde, et al. 2022.\n“GenSLMs: Genome-Scale Language Models\nReveal SARS-CoV-2 Evolutionary\nDynamics.” bioRxiv. https://doi.org/10.1101/2022.10.10.511571.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html",
    "href": "appendix/app-a-dl.html",
    "title": "Appendix A — Deep Learning Primer",
    "section": "",
    "text": "…",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html",
    "href": "appendix/app-b-compute.html",
    "title": "Appendix B — Deployment and Compute",
    "section": "",
    "text": "…",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html",
    "href": "appendix/app-c-data-curation.html",
    "title": "Appendix C — Data Curation",
    "section": "",
    "text": "Label: @sec-apx-data-curation\nOld source: NEW\nPurpose: Guide to constructing training sets for genomic FMs\nContent: - Data sources and access - Quality filtering strategies - Deduplication and redundancy reduction - Contamination detection - Data provenance and versioning - Bias assessment",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html",
    "href": "appendix/app-d-models.html",
    "title": "Appendix D — Model Reference",
    "section": "",
    "text": "D.1 Category Definitions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#category-definitions",
    "href": "appendix/app-d-models.html#category-definitions",
    "title": "Appendix D — Model Reference",
    "section": "",
    "text": "DNA LM: DNA language models using self-supervised pretraining on genomic sequences\nPLM: Protein language models trained on protein sequences\nSeq→Func: Supervised sequence-to-function models predicting chromatin/expression from DNA\nSplice: Specialized splice site prediction models\nVEP: Variant effect predictors (various paradigms)\nGFM: Genomic foundation model (broad, reusable representations)\nPGS: Polygenic score or risk prediction models\nGNN: Graph neural network for gene/pathway analysis\n\n\n\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. “[AlphaFold3] Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky, Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev. 2010. “A Method and Server for Predicting Damaging Missense Mutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024. “[DeepRVAT] Integration of Variant Annotations Using Deep Set Networks Boosts Rare Variant Association Testing.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper, Arend Sidow, and Serafim Batzoglou. 2010. “Identifying a High Fraction of the Human Genome to Be Under Selective Constraint Using GERP++.” PLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. “Delphi: A Deep-Learning Method for Polygenic Risk Prediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “[AlphaFold2] Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nKelley, David R. 2020. “[Basenji2] Cross-Species Regulatory Sequence Activity Prediction.” PLOS Computational Biology 16 (7): e1008050. https://doi.org/10.1371/journal.pcbi.1008050.\n\n\nKelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y. McLean, and Jasper Snoek. 2018. “[Basenji2] Sequential Regulatory Activity Prediction Across Chromosomes with Convolutional Neural Networks.” Genome Research 28 (5): 739–50. https://doi.org/10.1101/gr.227819.117.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam, Amit R. Majithia, and Trey Ideker. 2025. “[G2PT] A Genotype-Phenotype Transformer to Assess and Explain Polygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Hao, Zebei Han, Yu Sun, Fu Wang, Pengzhen Hu, Yuang Gao, Xuemei Bai, et al. 2024. “CGMega: Explainable Graph Neural Network Framework with Attention Mechanisms for Cancer Gene Module Dissection.” Nature Communications 15 (1): 5997. https://doi.org/10.1038/s41467-024-50426-6.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and Yunping Zhu. 2022. “MoGCN: A Multi-Omics Integration Method Based on Graph Convolutional Network for Cancer Subtype Analysis.” Frontiers in Genetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT: Predicting Amino Acid Changes That Affect Protein Function.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs, Minmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005. “[PhastCons] Evolutionarily Conserved Elements in Vertebrate, Insect, Worm, and Yeast Genomes.” Genome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.\n\n\nYeo, Gene, and Christopher B. Burge. 2004. “Maximum Entropy Modeling of Short Sequence Motifs with Applications to RNA Splicing Signals.” Journal of Computational Biology 11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html",
    "href": "appendix/app-e-resources.html",
    "title": "Appendix E — Resources",
    "section": "",
    "text": "E.1 Genomics & Human Genetics",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#genomics-human-genetics",
    "href": "appendix/app-e-resources.html#genomics-human-genetics",
    "title": "Appendix E — Resources",
    "section": "",
    "text": "Thompson & Thompson Genetics and Genomics in Medicine (9th ed.)\nRonald Cohn, Stephen Scherer, Ada Hamosh. Clinical-focused overview of human genetics and genomics for medicine, great for grounding in clinical genomics.\nHuman Molecular Genetics (5th ed.)\nTom Strachan, Andrew Read. Higher-level molecular genetics/genomics text with strong coverage of mechanisms, technologies, and disease applications.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#immunology",
    "href": "appendix/app-e-resources.html#immunology",
    "title": "Appendix E — Resources",
    "section": "E.2 Immunology",
    "text": "E.2 Immunology\n\nJaneway’s Immunobiology (10th ed.)\nKenneth M. Murphy, Casey Weaver, Leslie J. Berg. Standard comprehensive immunology textbook, excellent for understanding immune system biology relevant to genomics and disease.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#machine-learning-deep-learning",
    "href": "appendix/app-e-resources.html#machine-learning-deep-learning",
    "title": "Appendix E — Resources",
    "section": "E.3 Machine Learning & Deep Learning",
    "text": "E.3 Machine Learning & Deep Learning\n\nDeep Learning\nIan Goodfellow, Yoshua Bengio, Aaron Courville. Comprehensive deep learning textbook; free online: https://www.deeplearningbook.org/\nDive into Deep Learning (D2L)\nAston Zhang et al. Interactive deep learning book with Jupyter notebooks and multi-framework code; free online: https://d2l.ai/\nAn Introduction to Statistical Learning (ISLR, 2nd ed.)\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Gentle introduction to statistical learning methods used in ML, available free online: https://www.statlearning.com/\nThe Elements of Statistical Learning (ESL)\nTrevor Hastie, Robert Tibshirani, Jerome Friedman. More advanced, theory-heavy companion to ISLR; free PDF: https://hastie.su.domains/ElemStatLearn/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-f-glossary.html",
    "href": "appendix/app-f-glossary.html",
    "title": "Appendix F — Glossary",
    "section": "",
    "text": "F.1 Terms (A–Z)\nThis curated glossary is intended for an interdisciplinary audience with advanced degrees. It keeps terms that are: - used repeatedly or central to the book’s arguments, - easy to misinterpret across domains, or - required for clinical translation (calibration, uncertainty, evaluation, reporting).\nDomain tags:\nA/B compartments [Genomics]: Broad chromatin domains seen in Hi-C contact maps that separate active (A) from inactive (B) genome regions. They summarize large-scale 3D organization and often correlate with gene density and epigenomic state.\nACMG/AMP guidelines (American College of Medical Genetics and Genomics / Association for Molecular Pathology) [Clinical]: A widely used framework for interpreting genetic variants using standardized evidence categories and rules. In this book, model scores are often mapped to ACMG-style categories such as benign, VUS, or pathogenic to support clinical reporting.\nActionability (clinical) [Clinical]: The degree to which a result can change patient management, such as guiding surveillance, treatment, or family testing. Actionability is a key lens for deciding which model outputs to report.\nAdapter [ML]: A small set of additional layers inserted into a pre-trained model so it can be specialized to a new task without updating all parameters. Useful for adapting large genomic foundation models while keeping compute and storage manageable. See also: fine-tuning, LoRA.\nADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) [Clinical]: A set of properties describing how a drug behaves in the body and whether it is safe. Early ADMET prediction helps reduce late-stage failures in drug discovery.\nAdverse drug reaction (ADR) [Clinical]: An unintended and harmful response to a medication at normal doses. Predicting ADR risk can involve genetics, comedications, and clinical context.\nAnalytical validity [Clinical]: How accurately and reliably a test measures what it claims to measure (e.g., variant detection accuracy). Required before clinical validity and utility can be meaningfully assessed.\nATAC-seq (Assay for Transposase-Accessible Chromatin using sequencing) [Genomics]: A sequencing assay that measures chromatin accessibility by inserting adapters into open DNA with a transposase. Used to map regulatory elements and infer transcription factor activity.\nAUPRC (Area under precision–recall curve) [Statistics]: A metric emphasizing performance on the positive class, especially informative when positives are rare. Common in variant and regulatory benchmarks with strong class imbalance.\nAUROC (Area under ROC curve) [Statistics]: A threshold-free metric for binary classification measuring the probability a random positive is ranked above a random negative. Widely used in benchmarks but can be misleading under severe class imbalance. See also: AUPRC.\nAutoregressive language model (AR LM) [ML]: A model that predicts the next token from previous tokens, learning a probability distribution over sequences. In genomics, AR objectives are used for DNA or protein sequence generation and likelihood-based scoring.\nBarcode (cell barcode) [Genomics]: A short DNA sequence attached during library prep that labels reads from the same cell in droplet-based single-cell assays. Enables multiplexing and cell-level counting.\nBase quality score recalibration (BQSR) [Computation]: A post-processing step (commonly in GATK pipelines) that corrects systematic biases in base quality scores reported by sequencers. Helps improve variant calling accuracy by better modeling sequencing error probabilities.\nBatch correction [Statistics]: Methods that remove technical variation while preserving biological signal, often by aligning latent spaces or matching neighbors across datasets. Essential for integrating studies and avoiding spurious clusters.\nBatch effect [Statistics]: Systematic differences introduced by processing samples in different experimental or computational batches. Batch effects can create spurious associations and must be addressed in QC and modeling.\nBenchmark suite [Computation]: A curated collection of datasets, tasks, and scoring rules used to evaluate models consistently. A good suite documents data provenance, splits, metrics, and known failure modes.\nBinary Alignment/Map (BAM) [Computation]: A compressed binary format for storing aligned sequencing reads and their metadata. Typically produced from FASTQ reads after alignment and used as input for variant calling.\nBrier score [Statistics]: A proper scoring rule that measures the mean squared error between predicted probabilities and binary outcomes. Commonly used to quantify probabilistic calibration. See also: calibration.\nCADD (Combined Annotation Dependent Depletion) [ML]: A widely used variant prioritization score that integrates diverse genomic annotations to estimate how deleterious a variant is likely to be. Trained using an evolutionary proxy task contrasting simulated variants with variants observed in humans.\nCalibration [Statistics]: The agreement between predicted probabilities (or risk scores mapped to probabilities) and observed outcome frequencies. Well-calibrated variant scores help clinicians interpret the meaning of ‘0.9 pathogenic’ as a real-world risk.\nCalibration drift [Statistics]: Degradation of calibration over time or across sites as data distributions change. Monitoring drift is important for long-lived clinical deployments.\nCalibration-in-the-large [Statistics]: A calibration check comparing the average predicted risk to the observed event rate. Useful for detecting systematic over- or underestimation after deployment.\nCanonical correlation analysis (CCA) [Statistics]: A technique that finds linear projections of two datasets that are maximally correlated. Used in multi-omic and batch integration to align shared structure across modalities or batches.\nCDS (Clinical decision support) [Clinical]: Software that provides patient-specific recommendations or alerts in clinical workflows. Genomic AI often reaches clinicians via CDS rather than standalone reports.\nChannel (CNN) [ML]: A feature dimension in convolutional inputs/outputs (e.g., number of filters) analogous to color channels in images. In genomics, channels often represent A/C/G/T in one-hot encoding.\nChinchilla-style compute-optimal training [ML]: A scaling-law insight that, for a fixed compute budget, there is an optimal balance between model size and number of training tokens. Helps decide whether to train a bigger genomic model or train longer on more sequence data. See also: scaling laws.\nChromatin accessibility [Genomics]: How open or closed DNA is in the nucleus, influencing whether transcription factors can bind regulatory sites. Often measured by ATAC-seq or DNase-seq and modeled by sequence-to-function foundation models.\nChromatin loop [Genomics]: A focal 3D contact between two genomic loci, often connecting enhancers to promoters or demarcating domains. Loops can be detected in Hi-C/Micro-C and are important for regulatory interpretation.\nCis-regulatory element (CRE) [Genomics]: A DNA region that regulates nearby genes, such as promoters, enhancers, silencers, and insulators. Regulatory foundation models aim to predict CRE activity from sequence and context.\nClass imbalance [Statistics]: A setting where one label is much rarer than the other. Requires appropriate metrics (AUPRC), sampling, and calibration to avoid misleading conclusions.\nCLIA (Clinical Laboratory Improvement Amendments) [Clinical]: U.S. regulations governing laboratory testing quality. Clinical genomic pipelines often require CLIA-compliant validation and documentation.\nClinical category mapping [Clinical]: A procedure that converts model outputs (continuous scores) into discrete clinical labels such as benign, VUS, likely pathogenic, or pathogenic. Requires calibration and careful threshold selection.\nClinical utility [Clinical]: Evidence that using a test or model improves meaningful outcomes (health, decisions, cost) compared to standard care. Utility is often the hardest and most important bar for translation.\nClinVar [Clinical]: A public archive of variant interpretations and supporting evidence linking genetic variants to clinical phenotypes. Widely used in clinical variant interpretation and to label variants for benchmarking.\nClosed-world assumption [Statistics]: Assuming the deployment environment matches the training/test distribution and all relevant classes are known. Often violated in genomics, motivating OOD detection and robustness tests.\nClumping and thresholding (C+T) [Statistics]: A simple polygenic score construction method that selects variants passing a p-value threshold and prunes correlated variants using linkage disequilibrium. Often used as a baseline against more complex methods like LD-aware shrinkage.\nCo-assay (paired multi-omics) [Genomics]: A protocol that measures multiple modalities from the same cell (e.g., RNA + ATAC). Enables direct linking of regulatory state to gene expression without relying solely on computational alignment.\nCohesin [Genomics]: A protein complex that helps form chromatin loops and organize 3D genome structure, often working with CTCF. Cohesin dynamics influence enhancer–promoter interactions.\nConformal prediction [Statistics]: A method that wraps a predictive model to provide prediction sets or intervals with statistical coverage guarantees under mild assumptions. Useful for communicating uncertainty in variant interpretation.\nConfounder [Statistics]: A variable that influences both inputs and outcomes, creating spurious associations. Confounders are common in biomedical data (batch, ancestry, site) and must be addressed in design and evaluation.\nContamination [Clinical]: Unintended mixing of DNA from different samples, leading to incorrect genotypes and false variants. Detected during sample-level QC and can severely distort downstream analyses.\nContamination (data leakage) [Statistics]: Accidental overlap between training and test information, such as shared individuals, highly similar sequences, or duplicated labels. Leakage can yield unrealistically high benchmark performance.\nContext length [ML]: The maximum sequence length a model can process in one forward pass. In genomics, longer context enables modeling distal regulatory interactions and haplotype-level effects.\nContrastive learning [ML]: A representation-learning approach that brings related examples closer in embedding space while pushing unrelated ones apart. Used to align sequences with functional assays or to learn robust embeddings. See also: embedding.\nConvolutional neural network (CNN) [ML]: A neural network that uses convolutions to learn local patterns with weight sharing and translation equivariance. In genomics, CNNs often learn sequence motifs and local regulatory signatures.\nCopy number variant (CNV) [Genomics]: A structural change where a DNA segment is deleted or duplicated, altering the number of copies. CNVs can have large phenotypic effects but are harder to detect than SNPs in short-read data.\nCoverage (Depth) [Genomics]: The number of sequencing reads overlapping a genomic position. Higher depth generally improves confidence in genotype calls, especially for rare variants or low-quality regions.\nCoverage (single-cell) [Statistics]: The number of reads or unique molecules observed per cell. Low coverage increases dropout and uncertainty, affecting clustering and differential expression.\nCTCF [Genomics]: A DNA-binding protein strongly associated with insulators and TAD boundaries. Frequently used as an anchor for interpreting 3D genome structure and loop formation.\nCuration [Clinical]: Manual or semi-automated review of evidence to produce trusted annotations (variant classification, gene–disease validity). Curation quality strongly affects downstream AI.\nData governance [Clinical]: Policies and processes for data access, privacy, consent, and stewardship. Essential for sharing multi-institutional clinical genomics data responsibly.\nDecision threshold [Clinical]: A cutoff on risk or score that triggers an action, balancing false positives and false negatives under real clinical costs. Thresholds should be validated and monitored, not chosen only on internal benchmarks.\nDegree (node degree) [Statistics]: The number of edges connected to a node in a graph (or the sum of edge weights). Used as a basic measure of connectivity in biological networks.\nDeleteriousness score [ML]: A model-derived score intended to rank variants by likelihood of harmful biological impact. Deleteriousness scores are used for variant prioritization but should not be interpreted as direct clinical risk. See also: CADD.\nDifferential expression (DE) [Statistics]: Testing whether gene expression differs between groups of cells or conditions. Requires models that handle count noise and multiple testing. See also: false discovery rate (FDR).\nDigital biomarker [Clinical]: A biomarker derived from digital sources such as wearables, images, or EHR signals. Can complement genomics for risk prediction and monitoring.\nDilation (dilated convolution) [ML]: A convolution variant that spaces kernel elements apart, expanding receptive field without increasing parameter count. Helps CNNs capture longer-range dependencies; see also: receptive field.\nDimensionality reduction [ML]: Methods that map high-dimensional omics data to fewer dimensions for visualization or modeling. Common choices include PCA (linear) and UMAP/t-SNE (nonlinear).\nDistribution shift [Statistics]: A mismatch between the data distribution seen during training and the distribution at deployment. Can degrade variant scoring when moving across ancestries, assays, tissues, or sequencing technologies. See also: out-of-distribution detection.\nDNA language model (DNA LM) [ML]: A language model trained on DNA sequences to learn general-purpose representations or likelihoods, often using masked or autoregressive objectives. Can be adapted to tasks like regulatory prediction or variant effect scoring. See also: tokenization.\nDoublet [Genomics]: An artifact where two cells are captured together and profiled as one, producing mixed expression. Doublets can form spurious clusters if not detected and filtered.\nDropout (single-cell) [Statistics]: Zero counts caused by limited capture efficiency and sampling, not true absence of expression. Dropout complicates interpretation and motivates models that treat zeros probabilistically.\nDrug–gene interaction [Clinical]: A relationship where genetic variation affects drug response, efficacy, or toxicity. Central to pharmacogenomics and precision prescribing.\nEffect size (beta) [Statistics]: A quantitative estimate of how much a variant changes a trait (for continuous traits) or the log-odds of disease (for binary traits). Effect sizes from GWAS are the weights used in many polygenic scores.\nEHR (Electronic health record) [Clinical]: A system storing patient clinical data, including diagnoses, labs, medications, and notes. EHR integration is key for deploying risk models and monitoring real-world performance.\nEmbedding [ML]: A learned continuous vector representation of discrete inputs such as tokens, k-mers, or amino acids. Embeddings capture similarity and enable neural models to operate in continuous space; see also: token.\nENCODE [Genomics]: The Encyclopedia of DNA Elements project providing large-scale functional genomics datasets (e.g., ChIP-seq, accessibility) used to annotate regulatory elements. Central to building training targets and features for genomic models.\nEnhancer [Genomics]: A regulatory DNA element that can increase transcription of a target gene when bound by transcription factors, often acting at a distance. Enhancers are frequently identified via chromatin accessibility and histone mark data.\nEnhancer–promoter interaction [Genomics]: A functional relationship where an enhancer influences transcription at a promoter, often facilitated by 3D looping. Multi-scale models aim to predict these interactions from sequence and context.\nEpistemic uncertainty [Statistics]: Uncertainty due to limited data or model knowledge, which can often be reduced with more data. Important for identifying when a model is extrapolating beyond training coverage.\neQTL (expression quantitative trait locus) [Genomics]: A genetic variant associated with changes in gene expression levels, often in a tissue-specific manner. eQTL maps (e.g., from GTEx) help link noncoding variants to candidate genes.\nEvidence code [Clinical]: A standardized label used in ACMG/AMP-style variant interpretation (e.g., strong, moderate, supporting evidence categories). Model outputs can provide quantitative evidence that must be integrated with other clinical data.\nEvidence synthesis [Statistics]: Combining multiple evidence sources—studies, assays, clinical reports—into a single conclusion. In variant interpretation, synthesis supports consistent classifications and updates.\nEvolutionary proxy task [ML]: A training strategy that uses evolutionary patterns as labels, such as contrasting simulated variants with variants observed in humans, to learn a deleteriousness signal without direct clinical labels. Used by CADD to scale training.\nExome sequencing (WES) [Genomics]: Sequencing focused on protein-coding regions (exons), typically using capture sequencing. Provides high depth in coding regions at lower cost than whole-genome sequencing, but misses most regulatory DNA.\nExpected calibration error (ECE) [Statistics]: A summary statistic of calibration that averages the gap between predicted confidence and observed accuracy across bins. Useful but sensitive to binning choices.\nExternal validation [Clinical]: Testing a model on data from a different site, cohort, or time period than training. Stronger evidence of real-world generalization than internal splits.\nFalse negative (FN) [Statistics]: A missed positive case (e.g., a pathogenic variant labeled benign). In clinical genetics, false negatives can delay diagnosis and appropriate care.\nFalse positive (FP) [Statistics]: An incorrect positive prediction (e.g., benign variant flagged pathogenic). FPs can trigger unnecessary anxiety, testing, or treatment.\nFamily segregation [Clinical]: Evaluating whether a variant tracks with disease within a family. Strong segregation evidence can support pathogenicity classification in rare disease.\nFASTQ [Computation]: A text file format that stores raw sequencing reads along with per-base quality scores. FASTQ is typically the starting point for alignment and variant calling pipelines.\nFDA SaMD (Software as a Medical Device) [Clinical]: A regulatory category for software intended for medical purposes without being part of a hardware device. Some clinical AI tools may be regulated as SaMD depending on claims and use.\nFew-shot learning [ML]: A regime where a model adapts to a new task using only a small number of labeled examples. Foundation models often improve few-shot performance via transferable representations.\nFine-tuning [ML]: Updating a pre-trained model’s parameters on a downstream task, usually with a smaller learning rate and fewer steps than pretraining. Common for adapting DNA/protein LMs to assays or clinical labels.\nFoundation model (FM) [ML]: A large pre-trained model trained on broad data (often self-supervised) that can be adapted to many tasks. In genomics, FMs can operate on sequences, multi-omic profiles, or sequence-to-function mappings.\nFrameshift variant [Genomics]: An insertion or deletion whose length is not a multiple of three, shifting the reading frame of a coding sequence. Often has large effects on protein sequence and is frequently pathogenic.\nFully connected layer (dense layer) [ML]: A layer that applies a learned linear transformation to all input features, often followed by a nonlinearity. Used in classifier heads or MLP blocks; see also: MLP.\nFunctionally annotated variant [Clinical]: A genetic variant supported by evidence from functional assays or model-based predictions. Functional annotation can help resolve VUS and prioritize variants for follow-up.\nGated Linear Unit (GLU) [ML]: A neural network block that multiplies one linear projection by a learned gate from another projection, improving expressiveness. Variants (e.g., SwiGLU) are common in modern transformers.\nGATK (Genome Analysis Toolkit) [Computation]: A widely used software suite for variant discovery and genotyping from sequencing data. Includes steps like BQSR and VQSR in many classical pipelines.\nGELU [ML]: Gaussian Error Linear Unit, an activation function that smoothly gates inputs based on a Gaussian curve. Common in transformer MLPs; see also: activation function.\nGene regulatory network (GRN) [Genomics]: A model of how transcription factors and other regulators interact to control gene expression. Learned or approximated using multi-omic data and sometimes foundation model embeddings.\nGene set enrichment analysis (GSEA) [Statistics]: A method that tests whether predefined gene sets (pathways) are overrepresented among up- or down-regulated genes. Helps interpret DE results in biological terms.\nGenerative model [ML]: A model that can sample new data points (e.g., sequences) from a learned distribution. DNA/protein LMs are generative and can be used for design or likelihood scoring.\nGenetic counseling [Clinical]: Clinical support that explains genetic results, uncertainty, and implications to patients and families. Even strong models require counseling to communicate limitations and next steps.\nGene–disease validity [Clinical]: The strength of evidence that a gene is causally associated with a disease. High validity supports diagnosis; low validity cautions against overinterpretation of variants in that gene.\nGenome sequencing (WGS) [Genomics]: Sequencing most of the genome, including non-coding regions. Increases diagnostic yield for some cases (SVs, regulatory variants) but adds interpretation complexity.\nGenome-wide association study (GWAS) [Statistics]: A study design that scans the genome for variants associated with a trait by testing each variant for statistical association across many individuals. GWAS identifies correlated signals that typically require fine-mapping to suggest causality.\nGenomic holdout (gene/locus holdout) [Statistics]: A split that withholds entire genes, chromosomes, or loci to test extrapolation to unseen genomic regions. Helps reduce local-sequence leakage that can inflate performance.\nGenomic inflation factor (λGC) [Statistics]: A diagnostic statistic summarizing inflation of GWAS test statistics relative to expectation, often used to detect residual confounding or polygenicity. Interpreted alongside QQ plots and study design details.\nGenotype–phenotype correlation [Clinical]: Relationships between genetic variants and observable traits or disease manifestations. Improves diagnosis, prognosis, and therapeutic targeting.\nGERP [Genomics]: A conservation-based score (Genomic Evolutionary Rate Profiling) that estimates evolutionary constraint by comparing observed substitutions to an expected neutral rate. Often used as an input feature in variant prioritization models.\nGIAB (Genome in a Bottle) [Genomics]: A consortium that provides high-confidence benchmark genomes and variant call sets for evaluating sequencing and variant calling accuracy. Frequently used to quantify performance of new callers like DeepVariant.\nGraph attention network (GAT) [ML]: A GNN that learns attention weights over neighbors during message passing. Useful when different neighbors contribute unequally, such as in heterogeneous biological graphs.\nGraph convolutional network (GCN) [ML]: A graph neural network that updates node features by aggregating information from neighbors. Used for tasks like gene function prediction or disease-gene prioritization.\nGraph neural network (GNN) [ML]: A neural model that operates on graphs by iteratively passing and aggregating messages along edges. Enables learning from biological networks such as PPIs or cell–cell interaction graphs.\ngRNA (guide RNA) [Genomics]: An RNA sequence that directs CRISPR systems to a target DNA locus. Used in functional screens and genome editing experiments that generate training data.\nGTEx (Genotype-Tissue Expression) [Genomics]: A consortium resource linking genetic variation to gene expression across many human tissues. Central for eQTL discovery and interpreting regulatory variants.\nHallucination (model output) [ML]: Producing confident but incorrect outputs or explanations. In interpretability and reporting, hallucinations can mislead users if not constrained by evidence.\nHardy–Weinberg equilibrium (HWE) [Statistics]: A population genetics expectation relating genotype frequencies to allele frequencies under random mating and no selection. Deviations can indicate genotyping artifacts or population structure; often used in sample-level QC.\nHGMD (Human Gene Mutation Database) [Clinical]: A curated database of published gene lesions associated with human disease. Often used historically for variant interpretation, though access and curation practices differ from open resources like ClinVar.\nHi-C [Genomics]: A chromosome conformation capture assay that measures 3D contacts between genomic loci. Provides information about long-range regulatory interactions and domain structure.\nHigh-throughput screening (HTS) [Clinical]: Testing large numbers of compounds or perturbations in parallel to identify hits. ML often prioritizes what to screen and how to learn from results.\nHighly variable genes (HVGs) [Statistics]: Genes with higher-than-expected variance across cells, often used as features for clustering and integration. HVG selection helps focus on biological signal but can miss subtle programs.\nHorizon (prediction horizon) [Clinical]: The time window over which a model predicts an outcome, such as 5-year risk or time-to-progression. Horizon choice affects labels, censoring, and clinical utility.\nHPO (Human Phenotype Ontology) [Clinical]: A standardized vocabulary for phenotypic abnormalities used to represent patient features in rare disease. Enables phenotype matching and computational diagnostics.\nHuman-in-the-loop [Clinical]: A deployment pattern where clinicians or curators review model outputs before action. Common for early clinical adoption to manage risk and build trust.\nImputation (single-cell) [Statistics]: Methods that estimate missing or dropped-out expression values. Can improve visualization but may distort variance and inflate confidence if used uncritically.\nIn silico saturation mutagenesis (ISM) [ML]: A technique that measures a model’s sensitivity by systematically mutating each position and observing changes in predictions. Used to interpret regulatory and variant models. See also: saliency map.\nIn-context learning (ICL) [ML]: A capability where a model adapts its behavior from examples provided in the prompt without updating weights. Often associated with large auto-regressive transformers; see also: prompting.\nIndel [Genomics]: A small insertion or deletion of DNA bases relative to the reference sequence. Indels are generally harder to call than SNPs, especially in repetitive regions.\nInformed consent [Clinical]: A process ensuring patients understand what data are collected, how results are used, and what risks exist. Consent scope can limit data use for model training and sharing.\nInterpretability [ML]: Methods that help explain model behavior in human-understandable terms, such as motifs, salient residues, or feature attributions. Supports debugging, trust calibration, and biological insight.\nIRB (Institutional Review Board) [Clinical]: A committee that reviews research involving human subjects for ethics and participant protections. Clinical ML studies often require IRB review even for retrospective analyses.\nKernel (CNN filter) [ML]: A small set of learned weights applied in a convolution to detect a specific local pattern. In genomics, kernels often learn motif-like detectors.\nLabel adjudication [Clinical]: A process where disagreements in labels (e.g., variant classifications) are resolved by expert review or consensus rules. Reduces noise and improves clinical-grade datasets.\nLabel noise [Statistics]: Errors or uncertainty in the target labels used for training and evaluation. Common in biomedical outcomes and variant labels, and can cap achievable performance.\nLayer normalization (LayerNorm) [ML]: A normalization technique that normalizes activations within each example across features, stabilizing training in transformers. Unlike BatchNorm, it does not rely on batch statistics.\nLead compound [Clinical]: A chemically optimized hit with improved potency, selectivity, and drug-like properties. Leads progress into preclinical development.\nLinkage disequilibrium (LD) [Genomics]: Non-random association of alleles at nearby loci, common in human genomes. LD can create confounding in variant benchmarks if splits do not separate correlated regions.\nLinkage disequilibrium (LD) [Statistics]: Non-random correlation between alleles at nearby loci caused by shared ancestry and limited recombination. LD is central to interpreting GWAS peaks and constructing polygenic scores.\nLongitudinal data [Clinical]: Measurements collected over time, such as repeated labs or outcomes. Enables risk prediction, progression modeling, and monitoring of post-deployment drift.\nLoRA (Low-Rank Adaptation) [ML]: A parameter-efficient fine-tuning method that learns low-rank updates to weight matrices while keeping original weights frozen. Often used to adapt large foundation models on limited compute. See also: adapter.\nMasked language modeling (MLM) [ML]: A self-supervised objective where random tokens are masked and the model predicts them from surrounding context. Widely used for DNA and protein foundation models to learn rich embeddings.\nMechanistic interpretability [ML]: Methods that aim to understand internal model computations (circuits, attention patterns, neurons) rather than only correlational feature importance. In genomics, used to connect learned features to motifs, domains, or pathways.\nMendelian disease [Clinical]: A disorder primarily caused by variants in a single gene with strong effects, often following dominant or recessive inheritance. Many rare diseases are Mendelian, making them targets for genomic diagnosis.\nMinimal clinically important difference (MCID) [Clinical]: The smallest change in an outcome that patients perceive as beneficial and that would justify a change in care. Helps translate statistical improvements into meaningful utility.\nMinor allele frequency (MAF) [Genomics]: The frequency of the less common allele at a biallelic locus. MAF thresholds are used in QC, filtering, and defining rare vs common variants.\nmiRNA (microRNA) [Genomics]: A short non-coding RNA that regulates gene expression post-transcriptionally, typically by binding target mRNAs and reducing translation or stability. Important in development and disease.\nMissense variant [Genomics]: A single-nucleotide change that alters the amino acid at a position in a protein. Effects vary widely, making missense interpretation a central VEP challenge.\nMLOps (machine learning operations) [Computation]: Practices for deploying, monitoring, and maintaining models in production. In clinical settings, MLOps includes audit trails, versioning, drift monitoring, and governance.\nMLP (Multi-Layer Perceptron) [ML]: A feed-forward block composed of one or more dense layers with nonlinearities, often used inside transformer blocks. Provides token-wise feature mixing; see also: GELU, GLU.\nModel monitoring [Clinical]: Ongoing tracking of model inputs, outputs, calibration, and outcomes after deployment. Detects drift, bias, and performance degradation in real-world use.\nModel updating [Clinical]: Changing a deployed model using new data, recalibration, or retraining. Requires governance because updates can change clinical behavior and may trigger regulatory review.\nMOFA (Multi-Omics Factor Analysis) [Statistics]: A factor model for jointly analyzing multiple omics modalities to identify shared and modality-specific sources of variation. Useful for interpretability in multi-omic integration.\nMonte Carlo dropout (MC dropout) [Statistics]: Using dropout at inference time to generate multiple predictions and estimate uncertainty from their variability. A practical approximation to Bayesian uncertainty in deep nets.\nMotif [Genomics]: A short recurring sequence pattern that is biologically meaningful, often representing a transcription factor binding preference. CNN kernels and attention heads can learn motif-like detectors; see also: transcription factor.\nMultiome (scRNA+scATAC) [Genomics]: Single-cell protocols that measure transcriptome and chromatin accessibility in the same cell. Supports direct regulatory-to-expression linking and improved cell-state resolution.\nMutual nearest neighbors (MNN) [ML]: An integration idea that matches cells across datasets by identifying pairs that are nearest neighbors of each other in embedding space. Helps align batches or modalities while preserving local structure.\nNet benefit [Clinical]: A decision-analytic quantity balancing true positives against false positives at a given threshold. Used in decision curve analysis to compare clinical utility.\nNet reclassification improvement (NRI) [Statistics]: A metric assessing whether a new model more appropriately moves people across clinically meaningful risk categories compared to a baseline model. Useful when categories drive actions.\nNext-token prediction (NTP) [ML]: A self-supervised objective where the model predicts the next token in a sequence, typically with a causal mask. Used for many generative language models and adapted for biological sequences.\nNon-coding RNA (ncRNA) [Genomics]: RNA molecules that are not translated into proteins, including lncRNA and miRNA. ncRNAs often regulate gene expression and can be disease-relevant.\nNonsense variant [Genomics]: A nucleotide change that introduces a premature stop codon, truncating the protein. Often strongly disruptive and frequently classified as pathogenic, depending on context.\nNormalization (single-cell) [Statistics]: Transformations that make expression counts comparable across cells, often correcting for library size and technical effects. Choices strongly affect downstream clustering and DE.\nNumber needed to treat (NNT) [Clinical]: How many patients must receive an intervention to prevent one adverse outcome. Helps translate risk prediction and treatment decisions into intuitive clinical impact.\nOdds ratio (OR) [Statistics]: A measure of association for binary outcomes, comparing odds of disease between genotypes or exposure groups. Often reported for case-control GWAS; related to the effect size on the log-odds scale.\nOOD detection (out-of-distribution detection) [Statistics]: Methods that identify inputs unlike the training data where predictions may be unreliable. Important for genomic models deployed across species, assays, ancestries, or sequencing pipelines. See also: distribution shift.\nOpen set recognition [ML]: Recognizing when an input belongs to a class not seen during training. Relevant for clinical settings where rare diseases or novel variant mechanisms appear.\nOut-of-distribution (OOD) [Statistics]: Inputs that differ substantially from training data, where predictions may be unreliable. Evaluations often include OOD tests across sites, assays, species, or ancestries.\nOutcome definition [Clinical]: Operationalizing a clinical endpoint from data (often EHR), such as defining ‘myocardial infarction’ by codes and labs. Poor definitions create label noise and confounding.\nParameter-efficient fine-tuning (PEFT) [ML]: A family of methods that adapt a pre-trained model by training only a small subset of parameters (adapters, LoRA, prompt tuning). Reduces compute and storage while preserving base model weights.\nPathogenic [Clinical]: A clinical classification indicating strong evidence that a variant contributes to disease. Common classification schemes also include likely pathogenic, VUS, likely benign, and benign.\nPathogenicity score [Clinical]: A numeric output estimating how likely a variant is to be disease-causing. For clinical utility, the score must be calibrated and validated in representative cohorts.\nPatient stratification [Clinical]: Grouping patients into clinically meaningful subgroups, such as responders vs non-responders or high vs low risk. Stratification can improve trial design and personalized care.\nPeak (chromatin peak) [Genomics]: A genomic region with elevated signal in an accessibility or binding assay (e.g., ATAC-seq peak). Peaks approximate regulatory elements used as features in scATAC-seq.\nPerformance stratification [Statistics]: Reporting metrics separately by subgroup (ancestry, tissue, site, gene class) rather than only overall averages. Reveals inequities and hidden failure modes.\nPharmacodynamics (PD) [Clinical]: What a drug does to the body, including biological effects and biomarker changes. PD helps connect target engagement to outcomes.\nPharmacogenomics (PGx) [Clinical]: The study of how genetic variation influences drug response and toxicity. Supports dose adjustments and drug selection to improve safety and efficacy.\nPharmacokinetics (PK) [Clinical]: What the body does to a drug, including absorption and clearance. PK informs dosing and is a major cause of trial failure when misestimated.\nPhenotyping (computational) [ML]: Extracting structured phenotypes from EHR data, including diagnoses, labs, and notes. Accurate phenotyping is essential for training and evaluating clinical risk models.\nPlatt scaling [Statistics]: A calibration method that fits a logistic regression mapping from model scores to probabilities. Often used to calibrate classifiers, including variant effect predictors. See also: temperature scaling.\nPointwise convolution (1×1 convolution) [ML]: A convolution with kernel size 1 that mixes information across channels at each position. Often used after depthwise convolutions; see also: depthwise separable convolution.\nPolygenic risk score (PRS) [Clinical]: A weighted sum of many genetic variants estimating genetic predisposition to a complex trait. PRS can inform screening and prevention but must be evaluated for calibration and fairness across ancestries.\nPolygenic risk score (PRS) [Statistics]: A polygenic score for a disease outcome, usually reported as a relative risk indicator rather than a diagnosis. Often used interchangeably with PGS, though PRS emphasizes clinical risk contexts.\nPolygenic score (PGS) [Statistics]: A weighted sum of many genetic variants used to predict a trait or disease risk. Weights typically come from GWAS effect sizes and may be adjusted for LD and ancestry.\nPost-market surveillance [Clinical]: Monitoring safety and effectiveness after approval or deployment, using real-world data. Critical for identifying rare adverse events and model drift.\nPosterior inclusion probability (PIP) [Statistics]: In Bayesian fine-mapping, the probability that a given variant is causal (or included) in the model explaining an association signal. Used to prioritize variants within a credible set.\nPredictive interval [Statistics]: A range expected to contain a future observation with a specified probability. For clinical risk, intervals can communicate uncertainty beyond point estimates.\nPretest probability [Clinical]: The probability of disease before considering a test result, based on prevalence and clinical presentation. Determines how strongly a result shifts belief and impacts PPV/NPV.\nPrincipal components (genetic PCs) [Statistics]: Low-dimensional axes summarizing genetic variation patterns across individuals. Included as covariates in GWAS to reduce confounding from population structure.\nPromoter [Genomics]: A regulatory region near a gene’s transcription start site that helps initiate transcription. Promoter activity is often measured via ChIP-seq and chromatin accessibility assays.\nProspective study [Clinical]: A study where outcomes are measured after model deployment or enrollment, reducing certain biases in retrospective datasets. Strong evidence for clinical utility.\nProspective validation [Clinical]: Evaluating a model in a forward-looking setting where outcomes occur after deployment or enrollment. Stronger evidence than retrospective evaluation for clinical utility.\nProtein language model (protein LM) [ML]: A language model trained on amino acid sequences to learn representations related to structure and function. Used for tasks like mutation effect prediction, annotation transfer, and protein design.\nProtein–protein interaction (PPI) [Genomics]: A physical interaction between proteins, often represented as a network. PPIs provide priors for pathway analysis and graph-based prediction.\nProxy endpoint (surrogate endpoint) [Clinical]: A biomarker or intermediate outcome used in place of a direct clinical outcome. Can speed trials but may fail if not causally linked to patient benefit.\nQuality control (QC) [Computation]: Procedures to detect and remove low-quality samples, reads, or variants (e.g., contamination, batch effects, low call rate). QC is essential to avoid spurious associations in GWAS and errors in variant calling.\nQuality management system (QMS) [Clinical]: A formal system of processes and documentation to ensure consistent quality in clinical products. Many regulated clinical tools require a QMS to manage changes and audits.\nQuery–Key–Value (QKV) [ML]: The three learned projections used in attention: queries decide what to look for, keys index what is available, and values carry information to be aggregated. Core to transformer self-attention.\nReal-world evidence (RWE) [Clinical]: Evidence derived from real-world data such as EHRs, claims, and registries. RWE supports post-market surveillance and can complement randomized trials.\nRecalibration [Statistics]: Updating the mapping from scores to probabilities when deploying in a new setting or after drift. Often required for safe clinical use across sites.\nReliability diagram [Statistics]: A plot that compares predicted probabilities to observed frequencies across bins to assess calibration. In well-calibrated models, points lie near the diagonal; see also: calibration.\nRepetitive region [Genomics]: DNA sequences with many similar copies (e.g., STRs, segmental duplications) that are difficult to map with short reads. Such regions increase uncertainty in alignment and variant calling.\nRepresentation learning [ML]: Learning features (embeddings) from data that capture useful structure for many tasks. Self-supervised DNA/protein models are primarily representation learners.\nResidual connection (skip connection) [ML]: A connection that adds a layer’s input to its output, easing optimization and enabling very deep networks. Standard in transformers and modern CNNs; see also: layer normalization.\nRisk model [Clinical]: A model that estimates probability of a future clinical event, such as disease onset or progression. For use in care, it must be calibrated, validated externally, and tied to actions.\nrRNA (ribosomal RNA) [Genomics]: RNA components of ribosomes that are highly abundant in cells. rRNA is often depleted in RNA-seq library prep to focus sequencing on informative transcripts.\nSafety margin (therapeutics) [Clinical]: The gap between effective and toxic dose levels. A narrow safety margin increases the importance of accurate dosing and careful monitoring.\nSaliency map [ML]: An interpretation method that highlights which input positions most influence a model’s prediction, typically using gradients. In genomics, saliency often aligns with motifs or critical residues. See also: ISM.\nSaMD change control [Clinical]: A governance process managing software updates that might affect clinical performance. Important for maintaining compliance and safety when models are updated.\nScaling laws [ML]: Empirical relationships showing how model performance changes with model size, dataset size, and compute, often following power laws. Used to plan compute-optimal training and forecast returns from scaling.\nscATAC-seq (single-cell ATAC-seq) [Genomics]: Single-cell assay that measures chromatin accessibility per cell. Complements scRNA-seq by reflecting regulatory potential and transcription factor activity.\nscRNA-seq (single-cell RNA sequencing) [Genomics]: Single-cell assay measuring transcript abundance per cell. Enables discovery of cell types and states and characterization of heterogeneity in tissues.\nSequence length (context length) [ML]: The number of tokens processed in a single model input. Longer contexts capture more information but increase compute, especially for quadratic-time attention; see also: FlashAttention.\nSequence-to-function model [ML]: A model that predicts functional outputs (e.g., accessibility, expression, binding) directly from DNA sequence, often conditioned on cell type or assay. Bridges sequence and phenotype-relevant measurements.\nSHAP (SHapley Additive exPlanations) [ML]: A feature-attribution approach based on Shapley values that estimates each feature’s contribution to a prediction. Common for tabular clinical models; expensive at scale.\nSIFT [Genomics]: A missense variant effect predictor that estimates whether an amino acid substitution is likely to affect protein function based on sequence conservation. Often used as a baseline comparator to learned scores.\nSingle nucleotide polymorphism (SNP) [Genomics]: A common single-base variant in a population, often used to refer to biallelic SNVs in GWAS arrays. SNPs are the most common variant type in many association studies.\nSingle nucleotide variant (SNV) [Genomics]: A change of a single nucleotide at a specific position relative to the reference genome. SNV is a general term; SNP typically implies the variant is common in a population.\nSplice variant [Genomics]: A variant that disrupts RNA splicing signals (e.g., donor/acceptor sites), potentially causing exon skipping or intron retention. Splicing effects can be modeled by sequence-based predictors.\nState space model (SSM) [Computation]: A sequence modeling architecture that can scale to long contexts with different compute trade-offs than attention. Sometimes explored as an alternative backbone for long-genome modeling.\nStructural variant (SV) [Genomics]: A larger genomic alteration such as deletions, duplications, inversions, translocations, or mobile element insertions. SVs can have large functional effects but are challenging to detect and represent.\nT2T-CHM13 [Genomics]: A telomere-to-telomere human genome assembly that improves representation of previously unresolved regions. Highlights how reference improvements can change mapping and variant interpretation.\nTAD (Topologically associating domain) [Genomics]: A genomic region with elevated internal contact frequency in 3D genome assays. TAD boundaries constrain enhancer–promoter interactions and can be affected by structural variation.\nTarget engagement [Clinical]: Evidence that a drug binds or modulates its intended biological target in vivo. Target engagement is necessary but not sufficient for clinical benefit.\nTarget validation [Clinical]: Experimental and clinical evidence confirming that modulating a target affects disease-relevant outcomes. Validation reduces attrition in drug programs.\nTF (transcription factor) [Genomics]: A protein that binds DNA motifs to regulate gene transcription. TF activity is often inferred from accessibility and expression rather than measured directly.\nThreshold selection [Clinical]: Choosing score cutoffs that balance false positives and false negatives given clinical costs and resources. Should be justified with decision analysis and validated externally.\nToken [ML]: A discrete unit of input to a model, such as a character, k-mer, or subword. Tokenization choices trade off biological resolution, sequence length, and compute; see also: vocabulary.\nTokenization [ML]: The process of converting raw sequences into tokens (characters, k-mers, or learned units) for model input. In genomics, tokenization controls resolution and computational cost. See also: k-mer, context length.\nTopic model (gene program) [ML]: A model that represents cells as mixtures of latent topics/programs, each involving a set of genes or peaks. Useful for capturing continuous variation and shared programs across cell types.\nTopologically associating domain (TAD) [Genomics]: A 3D genomic region with higher internal contact frequency than contacts across its boundaries. TADs help constrain enhancer–promoter interactions and can influence regulatory variant effects.\nTPM (transcripts per million) [Statistics]: A normalization unit that scales transcript counts by gene length and library size to approximate relative abundance. Used primarily in bulk RNA-seq; in single-cell, UMI counts are often preferred.\nTranscription factor (TF) [Genomics]: A protein that binds specific DNA motifs to regulate gene expression. Many genomics models aim to predict TF binding or learn TF-like motif detectors; see also: motif.\nTransformer [ML]: A neural network architecture built around self-attention and feed-forward blocks, enabling flexible modeling of long-range dependencies. Widely used for language and increasingly for biological sequences.\nTriage model [Clinical]: A model used to prioritize cases for further review or testing rather than making final decisions. Often the safest initial deployment pattern for genomic AI.\nUMAP (Uniform Manifold Approximation and Projection) [ML]: A nonlinear embedding method widely used for single-cell visualization that preserves neighborhood structure and can reflect some global geometry. Typically built on a kNN graph.\nUMI (Unique molecular identifier) [Genomics]: A short random sequence attached to molecules before amplification to identify duplicates. UMIs enable more accurate molecule counting and reduce PCR bias in scRNA-seq.\nUncertainty communication [Clinical]: Presenting uncertainty in a way clinicians and patients can act on, such as confidence categories or calibrated probabilities. Essential when reporting VUS or borderline risk.\nUncertainty quantification (UQ) [Statistics]: Methods for estimating how reliable a prediction is, including epistemic uncertainty (model uncertainty) and aleatoric uncertainty (data noise). Critical for clinical deployment of variant predictors.\nVAE (variational autoencoder) [ML]: A generative latent-variable model that learns a probabilistic latent space for high-dimensional data. VAEs underpin many single-cell tools for denoising, integration, and batch correction.\nVariance (metric variance) [Statistics]: The variability of an evaluation metric due to finite sample size or randomness (initialization, sampling). Reporting variance or CIs prevents overclaiming small differences.\nVariant Call Format (VCF) [Computation]: A standard text format for storing variant sites, genotypes, and associated annotations/quality fields across one or more samples. Common output of variant calling and common input to downstream analyses.\nVariant effect prediction (VEP) [Clinical]: Estimating how a genetic variant changes molecular function or disease risk using computational models and evidence. Foundation models contribute by improving coverage and transfer, but still require validation and calibration.\nVariant interaction (epistasis) [Genomics]: A non-additive effect where the impact of one variant depends on another variant. Long-context and haplotype-aware models aim to capture epistatic patterns.\nVariant of uncertain significance (VUS) [Clinical]: A variant whose relationship to disease cannot be confidently classified as benign or pathogenic from available evidence. A major challenge for translating genomic data into clinical decisions.\nVariant prioritization [Clinical]: The process of ranking candidate variants for follow-up or interpretation based on predicted functional impact, frequency, inheritance, and phenotype match. Tools like CADD provide one layer of evidence, not a final diagnosis.\nVariant quality score recalibration (VQSR) [Computation]: A statistical modeling step (commonly in GATK pipelines) that assigns calibrated quality scores to variant calls based on multiple annotation features. Used to separate likely true variants from artifacts.\nWarmup (learning-rate warmup) [ML]: A training schedule that gradually increases the learning rate during early steps to stabilize optimization, especially for transformers. Commonly followed by decay; see also: optimization.\nWeighted nearest neighbors (WNN) [ML]: An integration approach that constructs a neighbor graph using modality-specific similarities combined with learned weights. Often used to integrate RNA with protein or ATAC signals.\nWhole-exome sequencing (WES) [Genomics]: See Exome sequencing (WES).\nWhole-genome sequencing (WGS) [Genomics]: Sequencing of (nearly) the entire genome, including coding and noncoding regions. Provides the most comprehensive variant discovery but is more expensive and data-intensive than targeted approaches.\nWorst-group performance [Statistics]: A robustness/fairness summary that reports the lowest performance across defined subgroups. Helps prevent good average performance from hiding harmful failures.\nZero-shot prediction [ML]: Making predictions on a new task or dataset without task-specific training, often by using a pre-trained model’s likelihood or embeddings. Common for mutation effect scoring from protein LMs.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  }
]