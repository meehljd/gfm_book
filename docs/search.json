[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Genomic Foundation Models",
    "section": "",
    "text": "Introduction\nA single fertilized egg divides into trillions of cells sharing essentially the same genome, yet these cells differentiate into over two hundred distinct types, each with characteristic patterns of gene expression, chromatin accessibility, and regulatory state. The instructions for this differentiation are written in the genome itself: in enhancers and silencers distributed across hundreds of megabases, in splice sites that determine which exons join to form mature transcripts, in three-dimensional chromatin contacts that bring distant regulatory elements together. Reading these instructions requires understanding a regulatory grammar that evolution wrote over billions of years but never documented.\nClassical computational approaches attacked this problem piecemeal. One model predicted splice sites from local sequence context. Another identified transcription factor binding motifs. A third scored variant pathogenicity using evolutionary conservation. Each required hand-crafted features, curated training sets, and careful validation within a narrow domain. Insights rarely transferred: a model trained to recognize promoters knew nothing about enhancers, and neither could predict how a single nucleotide change might alter splicing. The result was a fragmented landscape where each biological question demanded its own specialized tool.\nFoundation models represent a fundamentally different approach. By training on vast corpora of genomic sequence with self-supervised objectives, these models learn representations that capture regulatory logic without explicit supervision on any particular task. The same model that predicts masked nucleotides can, after minimal adaptation, predict chromatin accessibility, identify splice sites, score variant effects, and distinguish pathogenic mutations from benign polymorphisms. This capacity for transfer learning suggests that foundation models have learned something general about how genomes encode function. Understanding what they have learned, how to deploy them effectively, and where they still fail defines the central challenge for practitioners in this field.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-foundation-models-for-genomics",
    "href": "index.html#why-foundation-models-for-genomics",
    "title": "Genomic Foundation Models",
    "section": "Why Foundation Models for Genomics?",
    "text": "Why Foundation Models for Genomics?\nTraditional genomic modeling has been overwhelmingly task-specific. A variant caller is tuned to distinguish sequencing errors from true variants in a particular sequencing platform and sample type. A supervised convolutional network predicts a fixed set of chromatin marks for a specific cell line. A polygenic risk score is fit for one trait, in one ancestry group, using data from one biobank. These models can achieve excellent performance in the settings they were designed for, but they often transfer poorly to new assays, tissues, ancestries, or institutions. When the input distribution shifts, whether because of a new sequencing chemistry, a different population, or a novel cell type, performance degrades in ways that are difficult to anticipate.\nFoundation models address this fragility through three interrelated strategies. First, they leverage scale: training on massive, heterogeneous datasets spanning multiple assays, tissues, species, and cohorts forces the model to learn representations that capture shared biological structure rather than dataset-specific artifacts. Second, they employ self-supervised objectives that do not require manual labels, allowing them to exploit the vast quantities of unlabeled sequence data, perturbation screens, and population variation that genomics generates. Third, they are designed for reusability: rather than training a new model for each task, practitioners probe, adapt, or fine-tune a shared backbone, amortizing the cost of representation learning across many downstream applications.\nThe extent to which this paradigm delivers on its promises in genomics remains an active research question. Some tasks benefit dramatically from pretrained representations; others show marginal improvement over strong classical baselines. Transfer across species, cell types, and assays works better in some settings than others. The computational costs of training and deploying large models create practical constraints that vary across research and clinical environments. Foundation models are not the answer to every genomic problem. Effective practice requires frameworks to evaluate when these approaches help, when simpler methods suffice, and how to design analyses that exploit the strengths of modern architectures while remaining alert to their limitations.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#recurring-themes",
    "href": "index.html#recurring-themes",
    "title": "Genomic Foundation Models",
    "section": "Recurring Themes",
    "text": "Recurring Themes\n\n\n\n\n\n\nCore Questions This Book Addresses\n\n\n\nThroughout the book, we return to these fundamental questions:\n\nWhat did the model actually learn? Distinguishing genuine biological insight from spurious correlations and dataset artifacts\nWhen does complexity help? Identifying where foundation models add value over simpler approaches\nHow do we know it works? Designing evaluations that predict real-world performance rather than benchmark success\nWhat can go wrong? Anticipating failure modes in deployment, especially in clinical settings\nHow do we use it responsibly? Navigating the gap between technical capability and appropriate application\n\n\n\nSeveral threads run through the book, and individual chapters can be read as different perspectives on the same underlying questions.\nThe co-evolution of data and architecture is one such thread. Early variant effect predictors relied on hand-engineered features and shallow models trained on modest curated datasets. Convolutional networks enabled direct learning of regulatory motifs and local grammar from raw sequence, but their fixed receptive fields limited their reach. Transformers and other long-context architectures opened the door to capturing broader regulatory neighborhoods and chromatin structure. Foundation models push toward representations that span multiple assays, tissues, and organisms. At each stage, the question is not simply whether the model is more sophisticated, but how the available data constrain what the model can sensibly learn.\nScaling laws and emergent capabilities represent a related concern. As models grow larger and train on more data, certain capabilities appear discontinuously rather than gradually. The relationship between parameters, training data, and compute follows predictable patterns that inform practical decisions about model development. Understanding these scaling dynamics helps practitioners decide when to train larger models, when existing models suffice, and what capabilities to expect at different scales.\nContext length and genomic geometry present persistent challenges. Many genomic phenomena are intrinsically non-local: enhancers regulate genes across hundreds of kilobases, chromatin loops bring distal elements into contact, and polygenic effects distribute risk across thousands of variants genome-wide. How models represent these long-range dependencies, what architectural choices enable or constrain their reach, and what is gained or lost as context windows scale remain central questions for genomic deep learning.\nThe distinction between prediction and design cuts across multiple chapters. Most current models are used as predictors: given a sequence and context, what molecular or phenotypic outcome is expected? The same models can also be embedded in design workflows, from variant prioritization and library construction to therapeutic sequence optimization. Foundation models change where the boundary lies between analysis and experimental planning, and they introduce new failure modes when generative or optimization objectives are misspecified.\nEvaluation connects benchmark performance to real-world decisions. Benchmark scores are seductive and easy to compare, but biological and clinical decisions are messy, multi-objective, and constrained by data drift, confounding, and poorly specified endpoints. A recurring theme is the gap between state-of-the-art metrics on held-out test sets and actual impact in research or clinical deployment. Careful evaluation, confounder analysis, uncertainty quantification, and calibration can narrow that gap, but only when practitioners understand what their metrics actually measure.\nInterpretability and mechanism warrant sustained attention. Interpretability is not optional decoration but a design constraint that shapes how models should be built and evaluated. Saliency maps, motif extraction, and mechanistic analyses can deepen understanding of what a model has learned, but they can also provide false comfort when applied to confounded or brittle representations. Distinguishing genuine biological insight from pattern-matching artifacts requires both technical tools and careful experimental design.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#typography-and-formatting",
    "href": "index.html#typography-and-formatting",
    "title": "Genomic Foundation Models",
    "section": "Typography and Formatting",
    "text": "Typography and Formatting\nComputational biology, machine learning, and clinical genomics each have distinct conventions for technical terminology. ML researchers recognize VCF as a file format; genomicists know BRCA1 as a tumor suppressor gene; clinicians understand gnomAD as a variant database. Typographic conventions distinguish these categories, helping specialists navigate unfamiliar domains while respecting established standards.\nThe typography system identifies canonical terms that appear in the glossary, distinguishes biological entities from computational infrastructure, and maintains clean prose that does not overwhelm with formatting. Each format choice must earn its place by genuinely aiding comprehension rather than adding visual noise. Databases like gnomAD appear constantly throughout genomic analyses; italicizing every mention would create clutter without improving clarity. In contrast, model names like Enformer and gene names like BRCA1 function as subjects in the narrative: proper nouns that benefit from visual distinction.\nThe hierarchy is simple: Bold marks glossary terms on first mention only. Italics marks proper nouns that function as subjects or actors in the narrative (models, genes, mathematical variables, Latin terms). Regular text with careful capitalization handles databases, consortia, and resources. Monospace signals computational infrastructure (file formats, code, command-line tools). Most prose remains unformatted, with typography providing navigation aids rather than constant emphasis.\nGlossary terms appear in bold on first mention only: “The transformer architecture revolutionized sequence modeling.” Subsequent mentions use regular text. This applies to machine learning concepts (attention mechanism, fine-tuning, embeddings), genomic concepts (single nucleotide polymorphism (SNP), enhancer, phasing), clinical terms (variant of uncertain significance (VUS), penetrance), and statistical concepts (area under ROC curve (auROC), calibration).\nModel names use italics throughout: Enformer, DNABERT, AlphaFold, DeepVariant, SpliceAI. Gene and protein names follow biological convention with italics: BRCA1, TP53, CFTR, CYP2D6. Mathematical variables in prose also use italics: “where n represents sequence length” or “attention score between positions i and j”. Latin and foreign terms are italicized: in silico, de novo, ab initio, in trans.\nMonospace formatting signals computational elements. File formats use monospace: VCF, BAM, FASTA, FASTQ, BED, GTF. Code elements including function names (batch_size, forward()), packages (transformers, torch), and command-line tools (bedtools, samtools, GATK) also use monospace.\nDatabases, consortia, and resources use regular text with careful capitalization: gnomAD, ClinVar, ENCODE, GTEx, UniProt. Sequencing technologies (Illumina, PacBio HiFi, Oxford Nanopore) and biochemical assays (ATAC-seq, ChIP-seq, RNA-seq) similarly use regular text. This reduces visual clutter in passages that reference multiple data sources while preserving clarity through distinctive capitalization patterns.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#structure-and-organization",
    "href": "index.html#structure-and-organization",
    "title": "Genomic Foundation Models",
    "section": "Structure and Organization",
    "text": "Structure and Organization\n\n\n\n\n\n\nFIGURE PLACEHOLDER\n\n\n\n\nFigure 1: [Enhancing] Visual roadmap of the book’s six parts showing progression and dependencies. Part I (Data Foundations) feeds into Part II (Sequence Architectures) and Part III (Foundation Model Families). Parts II and III together inform Part IV (Cellular Context). Part V (Evaluation and Trust) draws from all previous parts and is essential for Part VI (Clinical Translation). Show cross-cutting themes (evaluation, interpretability) as vertical threads.\n\n\n\nSix parts span thirty-one chapters, with six appendices providing supplementary material. Each part can be read independently, but the progression is cumulative.\nPart I: Genomic Foundations lays the genomic and statistical groundwork that later models rest on. 1  From Reads to Variants introduces next-generation sequencing, alignment, and variant calling, highlighting sources of error and the evolution from hand-crafted pipelines to learned variant callers. 2  Data Landscape surveys the core data resources that underlie most modern work: reference genomes, population variation catalogs, clinical variant databases, and functional genomics consortia such as ENCODE and GTEx. 3  GWAS and Polygenic Scores reviews genome-wide association studies, linkage disequilibrium, fine-mapping, and polygenic scores, emphasizing what these variant-to-trait associations do and do not tell us about mechanism. 4  Classical Variant Prediction covers conservation-based and machine-learning-based variant effect predictors such as CADD, including their feature sets, label construction, and issues of circularity and dataset bias. Together, these chapters answer a foundational question: what data and pre-deep-learning tools form the backdrop that any genomic foundation model must respect, integrate with, or improve upon?\nPart II: Sequence Architectures introduces the conceptual and technical foundations of modern sequence modeling. 5  Tokens and Embeddings examines how genomic and protein sequences are converted into model-compatible representations, covering one-hot encodings, k-mers, byte-pair encodings, learned embeddings, and position encodings, showing how these choices shape downstream model behavior. 6  Convolutional Networks examines convolutional approaches that established the field of genomic deep learning, including DeepSEA, Basset, and SpliceAI, analyzing what they learn about motifs and regulatory grammar and where their fixed receptive fields impose limitations that motivate attention-based architectures. 7  Transformers and Attention provides a detailed treatment of attention mechanisms, position encodings, and transformer architectures, with emphasis on how these ideas translate from language to biological sequence. 8  Pretraining Strategies covers pretraining objectives, from masked language modeling and next-token prediction to contrastive and generative approaches, examining how self-supervision extracts structure from unlabeled biological data. 9  Transfer Learning Foundations addresses transfer learning, domain adaptation, and few-shot learning, asking when and how pretrained representations generalize to new tasks, species, and data modalities. 10  Adaptation Strategies examines parameter-efficient fine-tuning methods that adapt foundation models to specific tasks. 11  Benchmarks and Evaluation surveys evaluation benchmarks and methodology, and 12  Confounding and Data Leakage addresses the confounding and leakage issues that pervade genomic evaluation.\nPart III: Foundation Model Families surveys the major foundation model families, organized by modality, and establishes variant effect prediction as the integrating application. 13  Foundation Model Paradigm develops a working definition and taxonomy of foundation models in genomics, distinguishing them from earlier supervised approaches and examining scaling laws that characterize how model capabilities change with size and data. 14  DNA Language Models covers DNA language models such as DNABERT, Nucleotide Transformer, HyenaDNA, and Evo, tracing their training corpora, objectives, evaluation suites, and current capabilities. 15  Protein Language Models describes large protein language models trained on evolutionary sequence databases, their emergent structure and function representations, and applications to structure prediction and design. 16  Regulatory Models covers hybrid CNN-transformer and related architectures designed for long genomic contexts, such as Enformer and Borzoi, which predict regulatory readouts over tens to hundreds of kilobases. 17  Variant Effect Prediction serves as a capstone that integrates these model families, examining how protein-based approaches such as AlphaMissense and DNA-based approaches such as splicing and regulatory models combine to address variant effect prediction, the central interpretive challenge that motivates the field.\nPart IV: Cellular Context examines how foundation model principles extend beyond one-dimensional sequence to embrace cellular and systems-level biology. 18  RNA Structure and Function extends beyond splicing to RNA structure prediction and RNA foundation models, examining how secondary structure and functional context inform representation learning. 19  Single-Cell Models covers foundation models for single-cell transcriptomics and epigenomics, showing how transformer architectures adapt to the unique characteristics of these data types. 20  3D Genome Organization addresses the three-dimensional organization of the genome, from chromatin loops and TAD boundaries to emerging spatial transcriptomics foundation models, examining how 3D structure provides the missing link between sequence and regulatory function. 21  Graph and Network Models turns to graph neural networks and network-based approaches, framing these not as alternatives to sequence models but as higher-level reasoning systems that consume foundation model embeddings as node features. 22  Multi-Omics Integration broadens the view to multi-omics integration, exploring how models can jointly represent genomic, transcriptomic, proteomic, and clinical information to connect sequence variation to phenotype across multiple layers of biological organization.\nPart V: Evaluation and Trust develops frameworks for assessing what models actually learn and how reliably they perform. 23  Uncertainty Quantification addresses uncertainty quantification, examining calibration, epistemic versus aleatoric uncertainty, and practical methods such as ensembles and conformal prediction that help models express when they do not know. 24  Interpretability explores interpretability tools from classical motif discovery and attribution methods to emerging mechanistic approaches, asking when these tools reveal genuine biological mechanisms and when they provide false comfort. 25  Causal Inference with Foundation Models examines causal inference approaches and their intersection with foundation models. 26.1 Regulatory Frameworks for Genomic AI concludes with regulatory and ethical considerations.\nPart VI: Clinical Translation moves from methods to end-to-end workflows in research and clinical practice. 27  Clinical Risk Prediction discusses clinical risk prediction that combines genomic features with electronic health records and environmental data, focusing on discrimination, calibration, fairness, and deployment in health systems. 28  Rare Disease Diagnosis examines how foundation models fit into rare disease and cancer workflows, including variant prioritization pipelines, integration with family and tumor-normal data, and laboratory validation. 29  Drug Discovery looks at how GFMs intersect with target discovery, functional genomics screens, and biomarker development in pharmaceutical and biotechnology settings. 30  Sequence Design covers generative applications, from protein design and therapeutic sequence optimization to synthetic biology and bioengineering workflows. 31  Frontiers and Synthesis examines emerging directions and open problems.\nSix appendices provide supporting material. Appendix A — Deep Learning Primer offers a compact introduction to neural networks, CNNs, transformers, training, and evaluation for readers who want enough machine learning background to engage with the main chapters without consulting external references. Appendix B — Deployment and Compute covers practical considerations for deploying genomic foundation models, including computational requirements, hardware selection, and infrastructure concerns. Appendix C — Data Curation provides guidance on constructing training datasets, covering data sources, quality filtering, deduplication, and contamination detection. Appendix D — Model Reference provides a comprehensive reference table of models discussed in the main chapters, with architecture summaries, training data, and key citations. Appendix E — Resources offers a curated collection of datasets, software tools, courses, and papers for deeper exploration. Appendix F — Glossary defines key terms spanning genomics, machine learning, and clinical applications.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#a-framework-not-a-snapshot",
    "href": "index.html#a-framework-not-a-snapshot",
    "title": "Genomic Foundation Models",
    "section": "A Framework, Not a Snapshot",
    "text": "A Framework, Not a Snapshot\nGenomic foundation models represent a moving target: architectures evolve, datasets expand, and evaluation standards shift. A book of the state of the art in 2024 would be obsolete before publication. The goal instead is to provide a framework for reasoning about new models as they appear, grounding readers in principles stable enough to outlast any particular architecture or benchmark.\n\n\n\n\n\n\nWhat You Will Be Able to Do\n\n\n\nAfter working through this book, you should be able to:\n\nEvaluate new models by understanding their data, architecture, objectives, and evaluation methodology\nDesign analyses that use foundation models appropriately, knowing when they add value and when simpler methods suffice\nRecognize pitfalls in training, evaluation, and deployment—especially the genomics-specific confounds that invalidate standard ML practices\nCommunicate effectively across disciplines, bridging genomics, machine learning, and clinical translation\nDecide where foundation models genuinely advance your work and where they introduce unnecessary complexity or risk\n\n\n\nReaders who work through this material should be equipped to place new models in the landscape of data, architecture, objective, and application. They should be able to design analyses that use foundation models as components (whether as feature extractors, priors, or simulators) without overclaiming what the models can do. They should recognize pitfalls in training, evaluation, and deployment, especially in clinical settings where errors have real consequences. And they should be able to decide where foundation models genuinely add value and where simpler methods remain sufficient.\nThe journey begins with foundations: how raw reads become variants, how variants become the datasets on which all subsequent models depend, and where errors in this upstream process create systematic challenges that propagate through everything built upon them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why I Wrote This Book\nWhat I wanted, but could not find, was a conceptual throughline:\nThis book is my best attempt at answering those questions in a way that is historically grounded, technically honest, and practically oriented.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#why-i-wrote-this-book",
    "href": "preface.html#why-i-wrote-this-book",
    "title": "Preface",
    "section": "",
    "text": "How do we get from reads to variants in a way that a deep model can trust?\nHow should we think about polygenic scores, fine-mapping, and functional assays in the era of foundation models?\nWhen we say a model “understands” regulatory grammar or protein function, what does that actually mean?\nAnd what does it take to move from a promising preprint to a tool that can support decisions about real patients?",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#how-this-book-came-together",
    "href": "preface.html#how-this-book-came-together",
    "title": "Preface",
    "section": "How This Book Came Together",
    "text": "How This Book Came Together\nThe structure of the book reflects the way these ideas evolved in my own work.\nEarly sections grew out of teaching and mentoring conversations: explaining next-generation sequencing, variant calling, and pre-deep-learning interpretation methods to new team members who were strong in statistics or ML but new to genomics (and vice versa).\nThe middle sections emerged from a series of “journal club + experiments” cycles, where we:\n\nread papers on sequence-to-function CNNs, protein language models, and genomic transformers,\ntried to reproduce key results or adapt them to key datasets,\nand documented the pain points: data formats, training instabilities, evaluation pitfalls, which never quite fit into a methods section.\n\nThe later parts were shaped by collaborations around clinical prediction, variant interpretation pipelines, and larger multi-omic models. Many of the examples and caveats come directly from these projects: places where a model that looked excellent on paper behaved in surprising ways when exposed to real-world data, or where simple baselines outperformed much fancier architectures once confounding and distribution shift were handled correctly.\nBecause of that origin, the book has a particular bias: it is written from the perspective of someone who spends much of their time trying to get models to work in messy, high-stakes settings. You will see this in the emphasis on data quality, evaluation, and clinical translation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#how-to-read-this-book",
    "href": "preface.html#how-to-read-this-book",
    "title": "Preface",
    "section": "How to Read This Book",
    "text": "How to Read This Book\n\n\n\n\n\n\nReader Pathways\n\n\n\nThis is not a genomics textbook, a complete review of every DNA or protein model, or a deep-learning-from-scratch course. Instead, it is meant to be:\n\na roadmap to the main kinds of data, models, and objectives that matter for genomic foundation models today\na bridge between classical statistical genetics and modern representation learning\na practical guide to the kinds of failure modes and design choices that matter in real applications\n\n\n\n\n\n\n\n\nIf you are…\nSuggested path\n\n\n\n\nNew to genomics\nStart with Part I, use Appendix A for deep learning background, then proceed sequentially\n\n\nNew to deep learning\nUse Appendix A first, then Part II for sequence architectures, Part I as needed for genomic context\n\n\nExperienced in both\nUse the book as reference; Chapter Overviews help you find specific topics\n\n\nFocused on clinical applications\nPart I → Part V → Part VI, with model chapters as needed\n\n\n\n\n\nYou do not need to read the book cover-to-cover in order.\n\nIf your background is in genomics or statistical genetics, you may want to skim the early deep-learning motivations and focus more on the sections that introduce convolutional models, transformers, and self-supervision, then move on to evaluation and applications.\nIf you come from machine learning, it may be more helpful to start with the genomic data and pre-deep-learning methods, then dive into the sequence-to-function and transformer-based chapters with an eye toward how the data and objectives differ from text or images.\nIf you are a clinician or translational researcher, you might care most about the reliability, confounding, and clinical deployment discussions, dipping back into the modeling parts as needed to interpret results or communicate with technical collaborators.\n\nThe book is organized into six parts:\n\nPart I introduces genomic data and pre-deep-learning interpretation methods, from sequencing and variant calling to early pathogenicity scores and polygenic models.\nPart II focuses on sequence architectures and learning principles, with emphasis on convolutional architectures, attention mechanisms, pretraining objectives, and transfer learning.\nPart III turns to foundation model families, covering protein and DNA language models and hybrid architectures.\nPart IV extends to systems-level modeling: RNA, single-cell, 3D genome, networks, and multi-omics integration.\nPart V examines evaluation, reliability, uncertainty, interpretability, causality, and ethical considerations.\nPart VI looks at applications: clinical risk prediction, rare disease, drug discovery, and biological design.\n\nWithin each part, the goal is not to catalogue every paper, but to highlight representative examples and the design principles they illustrate. References are there to give you starting points, not to serve as a comprehensive literature review.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#what-this-book-assumes-and-what-it-does-not",
    "href": "preface.html#what-this-book-assumes-and-what-it-does-not",
    "title": "Preface",
    "section": "What This Book Assumes (and What It Does Not)",
    "text": "What This Book Assumes (and What It Does Not)\nThe book assumes:\n\nbasic familiarity with probability and statistics (regression, hypothesis testing, effect sizes),\ncore genomics concepts (genes, variants, linkage disequilibrium, GWAS at a high level),\nand some exposure to machine learning ideas (training versus test data, overfitting, loss functions).\n\nIt does not assume that you have implemented deep learning models yourself, or that you are fluent in every area. When a chapter leans heavily on a particular background (for example, causal inference or modern self-supervised learning), it will either provide a brief refresher or point you to an appendix or external resource.\nIf you are missing some of this background, that is fine. The intent is for you to be able to read actively: to pause, look up side topics, and then return to the main arc without feeling lost.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#a-note-on-scope-and-opinions",
    "href": "preface.html#a-note-on-scope-and-opinions",
    "title": "Preface",
    "section": "A Note on Scope and Opinions",
    "text": "A Note on Scope and Opinions\nGenomic foundation models are evolving quickly. Any snapshot is, by definition, incomplete and slightly out of date.\nRather than chasing every new architecture or benchmark, the book focuses on durable ideas:\n\nhow different data types fit together,\nwhat kinds of objectives encourage useful representations,\nhow evaluation can fail in genomics-specific ways,\nand where deep models complement (rather than replace) classical approaches.\n\nInevitably, there are judgment calls about which papers, methods, and perspectives to emphasize. Those choices reflect my own experiences and biases. They are not an official position of any institution I work with, and they will certainly differ from other reasonable views in the field.\nYou should treat the book as one opinionated map of the landscape, not the landscape itself.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#acknowledgements",
    "href": "preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis book exists because of many generous people who shared their time, ideas, and encouragement.\nFirst, I owe a deep debt of gratitude to my colleagues in the Mayo Clinic GenAI and broader data science community. The day-to-day conversations, whiteboard sessions, and “what went wrong here?” post-mortems with this group shaped much of the perspective and many of the examples in the chapters.\nI am especially grateful to the principal investigators and clinicians whose questions kept the focus on real patients and real decisions: Dr. Shant Ayanian, Dr. Elena Myasoedova, and Dr. Alexander Ryu.\nTo leadership at Mayo Clinic who supported the time, computing resources, and institutional patience needed for both the models and this book: Dr. Matthew Callstrom, Dr. Panos Korfiatis, and Matt Redlon.\nTo my data science and machine learning engineering colleagues, whose work and feedback directly shaped many of the workflows and case studies: Bridget Toomey, Carl Molnar, Zach Jensen, and Marc Blasi.\nI am also grateful for the architectural creativity, hardware insight, and willingness to experiment from our collaborators at Cerebras: Natalia Vassilieva, Jason Wolfe, Omid Shams Solari, Vinay Pondenkandath, Bhargav Kanakiya, and Faisal Al-khateeb.\nAnd to our collaborators at GoodFire, whose partnership helped push these ideas toward interpretable and deployable systems: Daniel Balsam, Nicholas Wang, Michael Pearce, and Mark Bissell.\nI would also like to thank my former colleagues at LGC for foundational work and conversations around protein language models and large-scale representation learning: Prasad Siddavatam and Robin Butler.\nBeyond these named groups, I owe a broader debt to the geneticists, molecular biologists, statisticians, clinicians, and engineers whose work this book draws on. The field moves forward because people share code, publish honest benchmarks, and insist that models be connected back to biologically meaningful questions. Thank you for setting that standard.\nFinally, I am grateful to my wife, Alyssa, and our two kids for their patience with the evenings and weekends this book consumed. You gave me the space to finish it and the reasons to step away from it.\nIf this book helps you connect a new model to a real biological question, design a more robust evaluation, or communicate more clearly across disciplinary boundaries then it will have done its job.\n— Josh Meehl",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html",
    "href": "part_1/p1-ch01-ngs.html",
    "title": "1  From Reads to Variants",
    "section": "",
    "text": "1.1 NGS Data Challenges\nThe human genome contains approximately three billion base pairs, yet no instrument can read this sequence in one continuous stretch. Next-generation sequencing (NGS) instead fragments DNA molecules into short pieces, sequences each fragment independently, and produces tens to hundreds of gigabases of sequence data per run (Goodwin, McPherson, and McCombie 2016). The typical output consists of paired-end Illumina reads spanning 100 to 300 base pairs each, with each base assigned a quality score reflecting the instrument’s confidence in that call. This abundance comes at a cost: every read carries non-trivial measurement uncertainty, including substitutions from miscalled bases, context-specific errors near homopolymers, and quality degradation toward read ends.\nLong-read technologies from Pacific Biosciences and Oxford Nanopore extend the observable space dramatically, producing reads of 10 kilobases to over a megabase in length (Wenger et al. 2019; Dabernig-Heinz et al. 2024). These platforms access genomic territory invisible to short reads, including complex structural variants, segmental duplications, and repetitive regions. They carry their own characteristic error profiles, however, and the choice of sequencing platform fundamentally shapes which variants are discoverable and which systematic biases enter downstream analyses. A variant residing within a repetitive element may be invisible to short reads but readily detected by long reads that span the entire repeat.\nThe central problem is deceptively simple in statement but profound in consequence: how do we turn raw reads into a reliable list of genomic variants? Answering this question requires disentangling three fundamentally different sources of signal that manifest identically as mismatches between reads and reference. Sequencing errors arise from instrument noise and PCR artifacts during library preparation, creating false variants that never existed in the original DNA. Alignment artifacts occur when reads are mapped to incorrect genomic locations, particularly in repetitive regions and paralogous gene families, causing true variants to appear at wrong positions or disappear entirely. Genuine biological variation encompasses germline variants inherited from parents, somatic mutations acquired during cellular division, and mosaicism where only a fraction of cells carry a particular change. Historically, complex modular pipelines combining probabilistic models and hand-crafted heuristics addressed this separation (Nielsen et al. 2011). Deep learning now plays an important role in simplifying and improving parts of this stack, but understanding the classical pipeline remains essential for interpreting what downstream models actually learn.\nGermline variant calling in whole-exome and whole-genome sequencing presents the core technical challenge underlying most genomic deep learning applications. Somatic variant calling in cancer and RNA-seq-specific variant calling share many parallels but require additional considerations addressed elsewhere.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-challenges",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-challenges",
    "title": "1  From Reads to Variants",
    "section": "",
    "text": "Stop and Think\n\n\n\nShort-read sequencing produces reads of 100-300 base pairs. What genomic features do you predict would be difficult to analyze with such short fragments? Consider features that span long distances or involve repetitive sequences.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-targeting",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-targeting",
    "title": "1  From Reads to Variants",
    "section": "1.2 Targeting Strategies: Panels, Exomes, and Genomes",
    "text": "1.2 Targeting Strategies: Panels, Exomes, and Genomes\nDifferent clinical and scientific goals demand different sequencing strategies. A patient presenting with sudden cardiac arrest at age 35 needs deep, reliable coverage of KCNQ1, KCNH2, SCN5A, and other ion channel genes associated with long QT syndrome; sequencing her entire genome to find these variants would waste resources and delay clinical decisions. A biobank building training data for polygenic risk scores across hundreds of thousands of participants needs genome-wide coverage, even if individual sites have modest depth. A family searching for the cause of their child’s undiagnosed developmental delay needs comprehensive coverage that leaves no coding exon unexamined. These competing demands drive the choice between targeted panels, whole-exome sequencing, and whole-genome sequencing.\n\n1.2.1 Targeted and Panel Sequencing\nWhen clinicians already know which genes to examine, targeted gene panels capture tens to hundreds of genes selected for a specific clinical indication. Panels for cardiomyopathy, hereditary cancer syndromes, or epilepsy restrict sequencing to regions of known clinical relevance. By limiting the target to a small number of loci, panels achieve very deep coverage (often exceeding 500×) at modest cost, enabling sensitive detection of rare variants and some degree of mosaicism.\nThe narrow scope of panels limits their utility for deep learning and population-scale analysis. Panels miss novel disease genes outside their predefined targets, cannot be easily repurposed for new traits, and often have heterogeneous content across laboratories that complicates data aggregation. For large-scale genomic foundation models, panel data serve better as richly phenotyped anchors than as primary training material: they provide clean labels for specific variants but sparse genomic coverage overall. The sparse coverage of panel data creates specific challenges for transfer learning, as discussed in ?sec-ch09-domain-shift-types.\n\n\n1.2.2 Whole-Exome Sequencing\nProtein-coding sequence represents approximately 1 to 2 percent of the genome but harbors a disproportionate share of variants with known functional consequences. Whole-exome sequencing (WES) enriches coding exons and some splice-adjacent regions through hybridization probes that pull down targeted DNA, followed by short-read sequencing. Typical coverage ranges from 80 to 150× for exonic targets, sufficient for confident heterozygous variant calling in most regions.\nWES has driven Mendelian disease gene discovery for over a decade and powered early biobank-scale efforts, including the exome subsets of gnomAD and many hospital-based cohorts (Karczewski et al. 2020). The capture-based approach introduces systematic biases that propagate into downstream analyses, however. Certain exons consistently fail to capture efficiently, particularly those with extreme GC content, high repetitive content, or unusual length. A variant in the first exon of HTT (the Huntington disease gene) might be missed entirely due to extreme GC richness, and a variant effect predictor trained on WES data will never encounter variants in poorly captured regions. These blind spots are invisible in standard benchmarks but can have substantial clinical consequences. Batch effects tied to reagent lots and evolving panel designs further complicate multi-cohort analyses. Technical confounding from batch effects is systematically analyzed in Section 22.7.1, with mitigation strategies in ?sec-ch22-mitigation.\n\n\n1.2.3 Whole-Genome Sequencing\nNoncoding variants contribute substantially to human disease, and structural variants often span boundaries between exonic and intronic sequence. Whole-genome sequencing (WGS) samples nearly all bases in the genome at typical coverage of 30 to 60×, encompassing both coding and noncoding regions without the biases introduced by capture chemistry. Because there is no enrichment step, WGS produces more uniform depth than WES and enables detection of noncoding regulatory variants, structural variants, and copy-number changes alongside single nucleotide variants (SNVs) and indels (insertions and deletions).\n\n\n\n\n\n\nDeep Dive: Types of Genetic Variants\n\n\n\nFor ML readers: Genetic variants are classified by their size and mechanism:\nSingle Nucleotide Variants (SNVs): A single base pair change (e.g., A→G). Also called SNPs (single nucleotide polymorphisms) when common in the population. The most frequent variant type and easiest to detect.\nInsertions and Deletions (Indels): Addition or removal of 1-50 base pairs. When occurring in coding regions, indels that aren’t multiples of 3 cause frameshifts that disrupt the entire downstream protein sequence.\nStructural Variants (SVs): Large-scale genomic rearrangements (&gt;50 bp):\n\nDeletions: Loss of a genomic segment\nDuplications: Extra copies of a segment\nInversions: A segment flipped in orientation\nTranslocations: Segments moved between chromosomes\n\nCopy Number Variants (CNVs): Changes in the number of copies of a genomic region, ranging from kilobases to megabases. Can encompass entire genes.\nDetection difficulty increases with variant size: SNVs are reliably detected by short reads, while SVs often require long reads or specialized algorithms. Most variant effect predictors focus on SNVs and small indels; SVs require different computational approaches.\n\n\nWGS has become increasingly favored for new large cohorts and rare disease studies. The UK Biobank’s release of 500,000 whole genomes and gnomAD’s expansion to include diverse populations both rely on WGS as the primary data type (Bycroft et al. 2018; Karczewski et al. 2020). The data are reusable for many downstream analyses, including GWAS (Chapter 3), polygenic score development, and rare variant burden tests, and the simplified pipeline eliminates the need to track changing capture designs across time and centers. Whole-genome models typically assume access to WGS-based variant calls, even when actual training sets combine WES and WGS data for practical reasons.\n\n\n1.2.4 Long-Read Sequencing Technologies\nShort reads face a fundamental limitation rooted in information theory: sequences shorter than local repeats, segmental duplications, or structural variants cannot unambiguously resolve these features. Consider the SMN1 and SMN2 genes, which differ by only five nucleotides across their entire coding regions. Distinguishing them is clinically critical for diagnosing spinal muscular atrophy, yet short reads routinely fail this task because a 150-bp read maps equally well to either genomic location.\nPacific Biosciences (PacBio) HiFi sequencing produces reads of 10 to 25 kilobases with per-base accuracy exceeding 99.9% through circular consensus sequencing, where the same molecule is read multiple times to correct random errors (Wenger et al. 2019). Oxford Nanopore Technologies (ONT) instruments generate reads ranging from a few kilobases to over a megabase in length, with rapidly improving raw accuracy and unique capabilities including portable sequencers suitable for field deployment, direct RNA sequencing without reverse transcription, and real-time base calling during sequencing (Dabernig-Heinz et al. 2024). These technologies played central roles in the telomere-to-telomere (T2T) assembly of a complete human genome and in emerging human pangenome references that capture population diversity beyond what any single linear reference can represent (Nurk et al. 2022; Liao et al. 2023).\nLong reads transform variant calling by traversing low-complexity and repetitive regions essentially invisible to short-read technologies. Dedicated variant callers such as PEPPER-Margin-DeepVariant, Clair3, Sniffles2, pbsv, and cuteSV exploit read length and alignment patterns to detect insertions, deletions, inversions, and complex rearrangements (Shafin et al. 2021; Zheng et al. 2022; Smolka et al. 2024; “PacificBiosciences/Pbsv” 2025; Jiang et al. 2020). Single molecules spanning multiple heterozygous sites provide direct phasing information for haplotype resolution without statistical inference. Long reads also inform graph-based references and pangenomes that better represent population diversity than traditional linear references (Liao et al. 2023).\nShort-read pipelines remain the workhorse for large human cohorts due to cost and throughput advantages that will persist for years. Downstream models must accommodate variants discovered by either technology and must be evaluated on composite callsets that integrate short- and long-read information.\n\n\n\nTable 1.1: Comparison of major sequencing platforms\n\n\n\n\n\n\n\n\n\n\n\nFeature\nShort-Read (Illumina)\nLong-Read (PacBio HiFi)\nLong-Read (ONT)\n\n\n\n\nRead length\n100-300 bp\n10-25 kb\n1 kb - 1 Mb\n\n\nError rate\n~0.1%\n~0.1% (after CCS)\n1-5% (improving)\n\n\nCost per Gb\nLow\nHigher\nModerate\n\n\nRepeat resolution\nPoor\nGood\nExcellent\n\n\nBest use case\nPopulation scale\nStructural variants\nReal-time, field\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Guidance: Choosing a Sequencing Strategy\n\n\n\n\nUse targeted panels when: known genes, need deep coverage, cost-constrained, clinical turnaround time matters\nUse WES when: coding variants are primary interest, want reusable data, moderate cost tolerance\nUse WGS when: noncoding variants matter, structural variants expected, want maximally reusable data\nUse long-read when: repetitive regions critical, phasing essential, structural variants are focus",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-classical",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-classical",
    "title": "1  From Reads to Variants",
    "section": "1.3 Classical Variant Calling Pipelines",
    "text": "1.3 Classical Variant Calling Pipelines\nUnderstanding classical approaches matters not merely for historical completeness but because deep learning models like DeepVariant still operate within this overall framework. They replace specific components rather than rebuilding from scratch. The GATK Best Practices, first formalized around 2011 and refined continuously since, represent accumulated wisdom from a decade of methodological development (DePristo et al. 2011; Van der Auwera et al. 2018). These pipelines encode expert intuition about which quality metrics matter, how to balance sensitivity against specificity, and when borderline evidence should be trusted. Modern deep learning approaches inherit this structure even as they replace individual components with learned alternatives.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nBefore continuing, can you articulate the three sources of signal that manifest as mismatches between reads and reference? (Answer: sequencing errors, alignment artifacts, and genuine biological variation)\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe three sources are: (1) sequencing errors from instrument noise and library preparation artifacts, (2) alignment artifacts when reads map to incorrect genomic locations, and (3) genuine biological variation including germline variants, somatic mutations, and mosaicism. All three produce identical-looking mismatches in the data, making it impossible to distinguish them without sophisticated probabilistic models.\n\n\n\n\n\n\n1.3.1 From Sequencer to Aligned Reads\nErrors introduced at the earliest stages of data processing propagate through every downstream analysis; a miscalled base becomes a false variant, and a misaligned read places true variants at incorrect positions. The journey from DNA sample to variant calls begins when instrument software converts fluorescent images or electrical signals to base calls and quality scores through base calling. Reads are demultiplexed by sample barcode into FASTQ files, each containing millions of short sequences with associated Phred-scaled quality scores. These files serve as the raw material for all subsequent analysis, encoding both the sequence content and the instrument’s confidence in each base.\nEach read must then find its position in a reference genome. Alignment algorithms use seed-and-extend strategies to map short sequences against references such as GRCh38 (the traditional linear reference) or T2T-CHM13 (the first complete telomere-to-telomere assembly), with tools like BWA-MEM and minimap2 handling the computational challenge (Li 2013, 2018). The difficulty is substantial: algorithms must cope with mismatches arising from both true variants and sequencing errors, small indels that shift the alignment frame, and repetitive sequences where multiple genomic locations match equally well. When a read could plausibly originate from several locations, the aligner must either choose one (potentially incorrectly), report multiple candidates, or assign a mapping quality score reflecting its uncertainty about the true origin.\n\n\n\n\n\n\nDeep Dive: Quality Scores in Sequencing\n\n\n\nFor ML readers: Two distinct quality scores propagate through variant calling:\nBase Quality (BQ): The sequencer’s confidence in each individual base call, typically Phred-scaled. A Phred score of 30 means a 1-in-1000 probability of error (99.9% accuracy). Quality typically degrades toward read ends. Base quality affects how much each base contributes to genotype likelihood calculations.\nMapping Quality (MAPQ): The aligner’s confidence that a read originated from its reported genomic location rather than somewhere else. MAPQ = 0 means the read maps equally well to multiple locations (completely ambiguous). MAPQ = 60 typically indicates a uniquely mapping read with high confidence.\nWhy both matter: A read might have perfect base quality (the sequencer is confident about what bases it saw) but zero mapping quality (the aligner cannot determine where in the genome those bases came from). Variants called from low-MAPQ reads in repetitive regions are unreliable regardless of base quality. Conversely, high MAPQ with low base quality still produces uncertain genotype calls.\nDeep learning variant callers like DeepVariant implicitly learn to weight both quality types by processing the full pileup image rather than relying on summary statistics.\n\n\n\n\n\n\n\n\nThe variant calling pipeline from DNA sample to VCF\n\n\n\n\nFigure 1.1: The variant calling pipeline transforms raw sequencing data into genotype calls through multiple processing stages. DNA samples undergo fragmentation and library preparation before sequencing on short-read (Illumina) or long-read (PacBio, ONT) platforms. Base calling produces FASTQ files containing reads and quality scores. Alignment maps reads to a reference genome (GRCh38 or T2T-CHM13), generating BAM/CRAM files. Post-alignment processing marks PCR duplicates and recalibrates base quality scores. Per-sample variant calling produces genomic VCF (gVCF) files encoding genotype likelihoods at all positions. Joint genotyping across a cohort merges samples and applies population-informed priors. Final filtering separates high-confidence variants from probable artifacts. File sizes shown are approximate for 30× whole-genome sequencing.\n\n\n\nBefore variant calling can begin, systematic artifacts require correction. PCR duplicates arise when multiple reads are amplified from the same original DNA fragment during library preparation; these inflate apparent coverage and can amplify sequencing errors into false variants that appear well-supported by independent evidence. Duplicate marking identifies and flags these reads based on identical alignment coordinates. Quality scores also require adjustment: base quality score recalibration (BQSR) models systematic errors by comparing observed mismatches to databases of known variants, producing scores that better reflect true error rates in each sequence context (DePristo et al. 2011). Older pipelines also performed local realignment around indels, though modern callers have largely internalized this step.\n\n\n1.3.2 Per-Sample Variant Calling\n\n\n\n\n\n\nCommon Misconception\n\n\n\nA common misconception is that sequencing directly “reads out” the genotype at each position. In reality, variant calling is statistical inference: we observe noisy evidence (reads with errors) and must estimate the most likely underlying genotype. Understanding this distinction is essential for interpreting model confidence.\n\n\nAt each position in the genome, the fundamental question is: given the reads overlapping this site, what is the most likely genotype? For diploid humans at biallelic sites, three possibilities exist: homozygous reference (0/0), heterozygous (0/1), or homozygous alternate (1/1). The task is to compute genotype likelihoods that quantify the probability of the observed read data under each possible genotype, then combine these likelihoods with prior expectations to estimate posterior probabilities.\nGATK HaplotypeCaller approaches this by first identifying regions with evidence of variation, then locally assembling candidate haplotypes from the reads spanning that region, and finally computing likelihoods for each possible diploid genotype. The core calculation uses a pair hidden Markov model (pair-HMM) to marginalize over possible alignments between each read and each candidate haplotype, incorporating base quality scores to weight the contribution of each base (DePristo et al. 2011; Li 2014).\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThe following section uses Bayesian probability notation. Readers unfamiliar with this framework can focus on the intuition: we calculate how likely the observed reads would be under each possible genotype, then choose the genotype that best explains the data.\n\n\nThe mathematical framework is Bayesian. At a given site, the posterior probability of genotype G given read data D follows from Bayes’ theorem. In plain terms, we want to know: given what the reads show, which genotype is most likely? The formula below formalizes this intuition by weighing the evidence from each read against our prior expectations about genotype frequencies:\n\\[\nP(G \\mid D) \\propto P(G) \\prod_{r \\in \\text{reads}} P(r \\mid G)\n\\]\nThe prior P(G) often assumes Hardy-Weinberg equilibrium with a specified allele frequency, while the likelihood P(r | G) captures the probability of observing read r given that the true genotype is G. This formulation assumes conditional independence of reads given the genotype, an assumption violated in practice by systematic sequencing errors, read pair correlations, and library-level artifacts that create dependencies among observations. Classical pipelines attempt to correct for these violations through BQSR and ad hoc filters. Deep learning-based callers can learn these dependencies implicitly by processing entire pileups simultaneously, one of their key advantages.\n\n\n\n\n\n\nWorked Example: Calculating Genotype Likelihoods\n\n\n\nConsider a genomic site with 30 overlapping reads: 14 support the reference allele (A) and 16 support an alternate allele (G).\nStep 1: Define the hypothesis space\nThree possible diploid genotypes: 0/0 (AA), 0/1 (AG), 1/1 (GG)\nStep 2: Calculate expected read proportions under each genotype\n\n\n\nGenotype\nExpected Ref\nExpected Alt\nObserved (14A, 16G)\n\n\n\n\n0/0 (AA)\n~100%\n~0%\nVery unlikely\n\n\n0/1 (AG)\n~50%\n~50%\nGood match!\n\n\n1/1 (GG)\n~0%\n~100%\nVery unlikely\n\n\n\nStep 3: Compute likelihoods (simplified)\nAssuming base quality of Q30 (error rate ~0.001):\n\nP(data | 0/0): Probability of 16 “errors” from a homozygous reference site ≈ \\((0.001)^{16}\\) → essentially zero\nP(data | 0/1): Probability of 14:16 split from a heterozygote ≈ \\(\\binom{30}{14}(0.5)^{30}\\) → high\nP(data | 1/1): Probability of 14 “errors” from a homozygous alternate site ≈ \\((0.001)^{14}\\) → essentially zero\n\nStep 4: Apply Bayes’ theorem\nThe posterior strongly favors heterozygosity (0/1) because only this genotype explains the observed ~47:53 allelic ratio without invoking implausible error rates. This is reported as the called genotype with high genotype quality (GQ).\n\n\nThe per-read likelihoods aggregate into genotype likelihoods, which combine with priors to yield posterior probabilities. These posteriors become the genotype quality (GQ) scores that downstream analyses often treat as ground truth. Per-sample results are output as gVCF files encoding both variant calls and “reference blocks” with estimated confidence at non-variant positions, enabling later joint analysis across samples.\n\n\n\n\n\n\nTry This: Transfer Practice\n\n\n\nExtend the genotype likelihood calculation to a more complex scenario: You have a site with 25 reads, 10 supporting reference allele A, 8 supporting alternate allele G, and 7 supporting a second alternate allele T. How would you modify the likelihood calculation to handle this tri-allelic site? Which diploid genotypes would you need to consider?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nWith three alleles (A/G/T), you must consider six diploid genotypes: AA, AG, AT, GG, GT, TT. The 1/2 (GT) genotype best explains the 8:7 split between G and T alleles, though the 10 reference-supporting reads suggest possible sequencing errors or alignment artifacts.\nKey insight: The number of possible genotypes grows combinatorially with the number of alleles: \\(\\binom{n+1}{2}\\) genotypes for \\(n\\) alternate alleles. This makes multi-allelic sites computationally expensive and more prone to calling errors.\n\n\n\n\n\n\n\n\n\n\n\nThe VCF Format\n\n\n\nThe VCF Format The Variant Call Format (VCF) is the standard file format for storing variant calls. Each variant occupies one row with mandatory columns: CHROM and POS specify genomic location; REF and ALT give the reference and alternate alleles; QUAL provides a Phred-scaled quality score; FILTER indicates whether the variant passed quality filters; and INFO contains semicolon-delimited annotations such as allele frequency or functional predictions. For multi-sample files, a FORMAT column defines per-sample fields (typically GT for genotype, GQ for genotype quality, DP for depth), followed by one column per sample containing those values. Genotypes are encoded as allele indices separated by / (unphased) or | (phased), where 0 represents the reference allele and 1 the first alternate. A heterozygous call appears as 0/1; a phased heterozygote as 0|1 or 1|0 depending on which allele was inherited maternally versus paternally.\n\n\n\n\n1.3.3 Cohort Calling and Filtering\nIndividual samples rarely provide sufficient information for confident rare variant calling. A variant observed in only one sample with modest supporting reads might be a true rare variant or a systematic artifact; examining a single sample cannot distinguish these possibilities. Examining the same site across thousands of samples resolves this ambiguity: true variants appear in multiple individuals following population genetic expectations, while artifacts show patterns inconsistent with inheritance and population structure.\nJoint genotyping combines gVCFs across many samples to produce a multi-sample VCF. This process ensures that all samples are evaluated at the same candidate sites, avoiding the problem of comparing different variant lists, and pools information across carriers to improve sensitivity for rare variants. A variant with marginal evidence in three individuals gains credibility when those individuals share ancestry and the variant frequency matches population expectations from external databases.\nFiltering strategies separate high-confidence variants from probable artifacts. Early approaches applied independent thresholds on quality metrics such as depth, mapping quality, and strand bias, but these hard filters poorly captured the complex, multivariate patterns distinguishing true variants from errors. Variant Quality Score Recalibration (VQSR) instead trains a Gaussian mixture model on known true positives from validated resources (HapMap, 1000 Genomes) and likely false positives (identified by their failure to appear in established truth sets despite meeting other quality criteria), learning a composite quality score that integrates multiple annotation dimensions (DePristo et al. 2011). This approach dominated large-scale variant calling for a decade before machine learning methods began to replace it.\n\n\n1.3.4 Sample-Level Quality Control\nBefore any downstream analysis or model training, variant callsets must pass through sample-level quality control. Sex checks compare reported sex to X chromosome heterozygosity and Y chromosome coverage to detect sample swaps or sex chromosome aneuploidy. Contamination analysis estimates whether DNA from multiple individuals mixed during sample preparation, which would create apparent heterozygosity at sites where the individual is actually homozygous. Relatedness detection identifies unexpected relatives or duplicate sequencing of the same individual, both of which confound association analyses and inflate apparent sample sizes. Ancestry inference estimates genetic ancestry using principal component analysis or model-based clustering, which matters for controlling population stratification in downstream analyses (Chapter 12).\nThese QC steps determine which samples enter training sets, how models are stratified by ancestry, and which samples must be excluded due to technical artifacts. Any callset used for model training or evaluation implicitly assumes that careful QC has already been applied.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-phasing",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-phasing",
    "title": "1  From Reads to Variants",
    "section": "1.4 Haplotype Phasing",
    "text": "1.4 Haplotype Phasing\n\n\n\n\n\n\nReflection\n\n\n\nConsider a heterozygous patient with two variants in the same gene. Why does it matter clinically whether these variants are on the same chromosome (cis) or different chromosomes (trans)?\n\n\nThe clinical stakes of phasing emerge clearly in compound heterozygosity. Consider a child who inherits two rare, potentially pathogenic variants in CFTR, the cystic fibrosis gene. If both variants reside on the chromosome inherited from the mother (in cis), the child retains one functional copy from the father and may be unaffected or merely a carrier. If the variants are on opposite chromosomes (in trans), no functional copy exists and the child will develop cystic fibrosis. This scenario is examined in detail in ?sec-ch26-compound-het. Graph-based approaches to phasing connect to the network methods discussed in ?sec-ch18-biological-networks.\nStandard VCF genotypes cannot distinguish these scenarios, encoding only that heterozygous genotypes exist at two positions without specifying which alleles travel together on the same physical chromosome. The clinical implications are entirely different, yet the data appear identical without phase information. Diploid organisms carry two copies of each autosomal chromosome, one inherited from each parent. Haplotype phasing resolves the ambiguity in unphased genotype calls by assigning each allele to a specific parental chromosome, transforming genotypes such as 0/1 into phased representations like 0|1 or 1|0 where the delimiter indicates that phase has been determined.\n\n1.4.1 Clinical and Analytical Importance\nThe distinction between cis and trans configurations drives clinical decisions for recessive conditions across hundreds of disease genes, from metabolic disorders to hearing loss to retinal degeneration. Beyond compound heterozygosity, phased haplotypes enable several critical analyses. Haplotype-specific expression studies reveal allelic imbalance where one parental copy is preferentially transcribed, a phenomenon with implications for imprinting disorders and variable penetrance. Accurate modeling of linkage disequilibrium (LD) structure in population genetics depends on knowing which alleles are inherited together. Reference panels used for genotype imputation are stored as phased haplotypes; inaccurate phasing in these panels propagates errors to every study that uses them for imputation.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nHow would inaccurate phasing in reference panels affect genotype imputation accuracy? Consider both common and rare variants.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nInaccurate phasing in reference panels degrades imputation through two mechanisms:\nFor common variants: Phasing errors break up true haplotype blocks, creating chimeric haplotypes that don’t actually exist in the population. When the imputation algorithm tries to match a target individual’s genotypes to reference haplotypes, it may select the wrong haplotype block, leading to incorrect imputation of nearby variants. The impact is moderate because multiple reference haplotypes may carry similar common variant patterns.\nFor rare variants: The impact is much more severe. Rare variants are typically carried on specific ancestral haplotypes with distinctive patterns of linked common variants. If phasing errors separate a rare variant from its characteristic haplotype background, the imputation algorithm loses the primary signal for inferring that variant’s presence. This leads to systematic underimputation of rare variants, effectively making them invisible in imputed datasets.\nConsequence for models: Since imputation quality is better for common variants, models trained on imputed data will systematically underweight rare variants compared to models trained on directly sequenced data, potentially missing important rare variant associations.\n\n\n\n\n\nFor deep learning applications, phasing determines whether models receive unordered genotype pairs or structured, haplotype-resolved representations. This choice affects model architecture, training procedures, and ultimately performance. A model that processes phased haplotypes can learn patterns spanning multiple variant sites that would appear as noise in unphased data.\n\n\n1.4.2 Phasing Methods\nDifferent data types enable different phasing strategies, each with characteristic strengths and resolution. Read-backed phasing uses sequencing reads that span multiple heterozygous sites to assign alleles to the same physical molecule. Short reads typically phase variants within tens to hundreds of base pairs, limited by fragment length. Long reads extend this range to tens of kilobases or more, providing direct physical evidence of haplotype structure.\n\n\n\n\n\n\n\n\nTwo chromosomes with variant positions\n\n\n\n\n\n\n\nCis configuration: both variants on same chromosome\n\n\n\n\n\n\n\nTrans configuration: variants on opposite chromosomes\n\n\n\n\n\n\nFigure 1.2: Haplotype phase determines clinical outcome in compound heterozygosity. (A) Two chromosomes carry the CFTR gene, with two pathogenic variant positions indicated. Standard VCF genotypes (0/1 at each site) cannot distinguish the configurations shown in (B) and (C). (B) Cis configuration: both variants reside on the maternal chromosome, leaving the paternal copy functional. The child is an unaffected carrier. (C) Trans configuration: one variant on each parental chromosome eliminates all functional copies. The child has cystic fibrosis. Phased VCF notation (using | delimiter) encodes this distinction: 0|1, 0|1 indicates both alternate alleles on the same haplotype (cis), while 1|0, 0|1 indicates alternate alleles on opposite haplotypes (trans).\n\n\n\n\n\n1.4.3 Phasing Approaches\nWhen read-based phasing reaches its limits, two complementary strategies extend phase inference across longer distances. Population-based methods exploit the fact that chromosomes are inherited in long blocks that persist across generations. Tools such as SHAPEIT, Eagle, and Beagle compare study samples against reference panels of previously phased haplotypes, identifying shared segments that reveal which alleles travel together (O’Connell et al. 2014; Loh et al. 2016; Browning et al. 2021). When an individual’s genotypes match a known haplotype pattern, the algorithm can confidently assign alleles to chromosomes even without direct read evidence spanning the variants. These statistical approaches work well for common variation where linkage disequilibrium provides informative context but struggle with rare variants that lack haplotype representation in reference panels.\n\n\n\n\n\n\nStop and Recall\n\n\n\nBefore reading about family-based phasing, can you think of how parent-offspring genotype data could be used to determine phase? Consider a child who is heterozygous (0/1) at two nearby sites, with one parent homozygous 0/0 and the other heterozygous 0/1 at both sites.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe child must have inherited the 0 allele at both sites from the homozygous parent. Therefore, at both sites, the child’s alternate (1) allele must have come from the heterozygous parent. This means both alternate alleles are on the same chromosome (inherited together), allowing us to phase the child’s genotype as 0|1, 0|1 (both alts on the same haplotype) rather than 1|0, 0|1 (alts on different haplotypes).\nMore generally, any allele present in the child but absent in one parent must have come from the other parent. By comparing genotypes across the trio, we can determine which alleles were co-inherited on the same parental chromosome without requiring reads that physically span the variants.\n\n\n\n\n\nFamily data, when available, resolves phase through Mendelian logic rather than statistical inference. An allele present in a child and one parent but absent in the other must have been inherited from that parent. This deterministic reasoning makes pedigree-based phasing the gold standard for accuracy, though its applicability depends on recruiting family members for sequencing. The two approaches are complementary: statistical phasing provides genome-wide coverage for any individual, while family-based phasing offers higher confidence for the specific variants segregating within a pedigree.\n\n\n\nTable 1.2: Comparison of phasing approaches\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nRange\nRequirements\nStrengths\nLimitations\n\n\n\n\nRead-backed\nbp to kb\nSequencing data\nDirect physical evidence\nLimited by read/fragment length\n\n\nPopulation-based\nGenome-wide\nReference panel\nWorks for any sample\nPoor for rare variants\n\n\nPedigree-based\nGenome-wide\nFamily samples\nDeterministic, highest accuracy\nRequires family recruitment\n\n\n\n\n\n\nModern pipelines often combine these approaches, using statistical phasing anchored by a large reference panel, augmented by read-backed evidence where available, and refined by trio data when present. The resulting phase accuracy varies by variant frequency, local recombination rate, and representation in reference panels.\n\n\n1.4.4 Genotype Imputation and Refinement\nSequencing every individual at high coverage is expensive, but statistical inference from population structure can fill gaps at much lower cost. Genotype imputation matches a cohort with incomplete genotype data (from array genotyping or low-coverage sequencing) against a reference panel of densely phased haplotypes. Statistical models infer missing genotypes and refine uncertain calls by leveraging LD patterns and shared haplotype segments with individuals in the reference (Browning et al. 2021).\nTwo related processes deserve distinction. Imputation of untyped variants infers genotypes at positions not directly observed in the study cohort but present in the reference panel, dramatically increasing variant density without additional sequencing. A genotyping array measuring 500,000 SNPs can yield tens of millions of imputed variants through comparison with panels like the Haplotype Reference Consortium. Genotype refinement improves quality at sites where genotypes were already measured but with uncertainty, particularly useful in low-coverage WGS or WES where stochastic sampling creates noisy calls. Here the same statistical machinery strengthens existing calls rather than generating new ones.\nLow-coverage sequencing presents a distinct opportunity for imputation-based refinement. At 1x genome coverage, any individual position may have zero, one, or a handful of supporting reads, creating high uncertainty in direct genotype calls. Tools like GLIMPSE leverage reference panels to convert these noisy read pileups into high-confidence genotype calls, reducing false call rates by approximately 90% compared to unrefined calls from the same low-coverage data (Rubinacci et al. 2021). The approach works because while any single position has sparse coverage, the aggregate pattern of reads across a haplotype block provides substantial information when matched against reference haplotypes. This principle underlies the growing interest in low-pass whole genome sequencing as a cost-effective alternative to arrays for population-scale studies.\nIntuitively, the attenuation effect occurs because imputation errors act like measurement noise: when imputed genotypes sometimes differ from true genotypes, the observed association is diluted proportionally. The squared correlation between true and imputed genotypes acts as an attenuation factor: an association with true effect size \\(\\beta\\) behaves as if the effect were approximately \\(r^2 \\beta\\) in downstream analyses using imputed dosages.\n\n\n1.4.5 Hybrid Sequencing and Coverage Boosting\nModern clinical sequencing increasingly combines multiple data types to balance cost, coverage, and completeness. Hybrid approaches blend deep whole exome sequencing with low-pass whole genome sequencing in a single assay. The exome component (typically 80-100x coverage) provides high-confidence variant calls in coding regions, while the low-pass genome backbone (1-5x coverage) enables genome-wide imputation of common variants comparable to traditional arrays (Cirulli et al. 2020). Helix’s Exome+ assay exemplifies this design: clinical-grade exome sequencing combined with approximately 300,000 genome-wide SNPs that support imputation accuracy equivalent to 0.5x whole genome sequencing.\nA separate concept, often also called “boosting,” refers to assay design choices that enhance sequencing depth in specific regions. Coverage boosting through customized capture probe design targets regions prone to poor coverage in standard exome sequencing, including GC-rich exons, repetitive elements, and clinically critical genes like CYP2D6 where complete star allele calling requires comprehensive coverage. Helix’s assay achieves greater than 99.5% call rates across approximately 600 medically relevant genes by boosting capture probe density in these regions (Cirulli et al. 2020). This physical enhancement of sequencing depth differs fundamentally from statistical imputation: boosted coverage produces more supporting reads at the problematic positions, while imputation infers genotypes from population patterns without additional sequencing.\nThe distinction matters for downstream applications. Imputed genotypes carry well-calibrated uncertainty (typically reported as dosages between 0 and 2) that reflects confidence in the inference, whereas directly sequenced variants from boosted coverage regions produce conventional genotype likelihoods based on read evidence. Imputation accuracy degrades systematically for rare variants (minor allele frequency below 1%) because population reference panels provide less information about uncommon haplotypes. Coverage boosting, in contrast, performs equivalently for rare and common variants within the targeted regions since it operates on direct sequencing evidence. For clinical applications requiring rare variant detection in specific genes, boosted coverage provides more reliable results than imputation; for genome-wide association studies leveraging common variants, imputation offers greater cost efficiency.\nFor downstream deep learning, these approaches have distinct implications. Imputation dramatically increases the number of variants available as input features for genotype-based models and produces well-calibrated dosages that probabilistic models can exploit rather than forcing hard calls. Imputation ties model performance to the composition and ancestry representation of the reference panel, however: imputation errors are systematically larger when target individuals come from populations underrepresented in the panel, reinforcing themes of bias and confounding addressed in Chapter 12. Coverage-boosted regions provide high-quality calls independent of reference panel ancestry, but only within the targeted regions. Models consuming hybrid sequencing data must account for this heterogeneity: a variant in a boosted exonic region carries different uncertainty characteristics than an imputed intronic variant, even when both appear in the same VCF with comparable quality scores.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-errors",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-errors",
    "title": "1  From Reads to Variants",
    "section": "1.5 Sources of Error and Uncertainty",
    "text": "1.5 Sources of Error and Uncertainty\n\n\n\n\n\n\nKnowledge Check\n\n\n\nBefore reading about error sources, list at least three ways that a variant call might be wrong. What could cause a false positive (calling a variant that isn’t really there)? What could cause a false negative (missing a true variant)?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nFalse positives can arise from sequencing errors (particularly in homopolymers), PCR duplicates amplifying early errors, and reads misaligned to paralogous regions. False negatives occur when coverage is insufficient to detect a heterozygous allele, when variants fall in difficult-to-map repetitive regions, or when reference bias causes reads carrying alternate alleles to be discarded during alignment.\n\n\n\n\n\nEven with sophisticated pipelines, variant calls remain imperfect measurements of biological reality. These errors concentrate in specific genomic contexts and variant types, creating systematic blind spots in training data that downstream models inherit without warning (Li 2014). Understanding where errors arise, and why they cluster where they do, is essential for interpreting model performance and designing robust training strategies.\n\n\n\n\n\n\nTaxonomy of variant calling error sources\n\n\n\n\nFigure 1.3: Variant calling errors arise from three distinct sources with different consequences. Sequencing artifacts from instrument noise and library preparation create false positive (FP) variant calls. Alignment artifacts from ambiguous mapping in repetitive regions cause both false positives (variants called at wrong locations) and false negatives (FN, true variants missed due to mapping uncertainty). Biological complexity including mosaicism and low allele fractions primarily causes false negatives, as genuine variants fail to meet detection thresholds. Understanding error sources is essential for interpreting callset limitations and designing robust training strategies for downstream models.\n\n\n\n\n1.5.1 Mapping Ambiguity and Reference Bias\nWhen reads align almost equally well to multiple genomic locations, no algorithm can confidently determine their true origin. Segmental duplications, paralogous gene families, and repetitive elements create these ambiguous contexts throughout the genome. The consequences flow in both directions: misassigned reads create false positive variants at incorrect locations, while correctly placed reads may be discarded or down-weighted due to mapping uncertainty, creating false negatives.\nReference bias compounds these problems by systematically favoring detection of reference alleles over alternate alleles. A read carrying a non-reference variant may align slightly worse than an identical read matching the reference due to the mismatch penalty, leading to preferential retention of reference-supporting evidence. This bias causes systematic undercalling of alternate alleles, particularly in highly polymorphic regions or for variants that substantially alter local sequence context. Populations divergent from the reference genome experience more severe reference bias, creating ancestry-correlated error patterns.\n\n\n\n\n\n\nKey Insight\n\n\n\nReference bias systematically favors detection of reference alleles, and this bias is worse for populations divergent from the reference genome.\n\n\n\n\n1.5.2 Systematic Sequencing Artifacts\nSequencing chemistry introduces predictable error patterns that differ qualitatively from random noise. Homopolymer runs (stretches of identical nucleotides such as AAAAAAA or GGGGGG) cause polymerase slippage during synthesis, generating false indels at rates far exceeding substitution errors. Why does slippage occur specifically at homopolymers? During DNA synthesis, the template and nascent strands can transiently dissociate and re-anneal. In non-repetitive sequence, there is only one correct re-annealing position. But in a homopolymer, multiple positions are sequence-identical, allowing the nascent strand to slip forward or backward and re-anneal at an offset position—adding or deleting nucleotides without creating a mismatch that would trigger correction. Certain sequence motifs, particularly those with extreme GC content, exhibit systematically elevated error rates that persist even at high coverage. PCR amplification during library preparation can introduce errors early in the process; these errors then propagate into multiple reads, creating correlated false positives that appear well-supported by independent evidence.\nIndex hopping occurs when sample barcodes are misassigned during multiplexed sequencing, causing variants from one sample to appear spuriously in others sharing the same flow cell. Strand bias, where variant-supporting reads cluster on one strand orientation, often indicates systematic artifact rather than true variation. These patterns create correlated errors that cluster by batch, lane, or library preparation method, and they are difficult to distinguish from rare true variants precisely because they can appear in multiple reads with reasonable quality scores.\n\n\n1.5.3 Coverage Gaps and Allelic Imbalance\nStochastic sampling means some genomic regions receive fewer reads than average purely by chance, even when capture or sequencing is nominally uniform. In these low-coverage regions, one or both alleles may be missed entirely, and allelic balance can deviate substantially from the expected 50:50 ratio in heterozygotes. A heterozygous site with 20× coverage and 10 reads supporting each allele is confidently called; the same site with 4× coverage might show 4 reference reads and 0 alternate reads by chance alone, leading to a false homozygous reference call with no indication that an allele was missed.\nAllelic imbalance extends beyond random sampling to encompass systematic biases introduced throughout the sequencing workflow. PCR amplification during library preparation preferentially amplifies certain alleles over others, particularly where one allele creates more stable secondary structure or where GC content differs between alleles. Capture-based enrichment exhibits similar biases: hybridization probes designed against the reference sequence bind reference alleles more efficiently than alternate alleles. These technical biases compound across steps, so a heterozygous variant might consistently show 60:40 or even 70:30 allelic ratios across independent samples processed through the same pipeline.\nThe clinical consequences become acute in contexts where allele fractions carry diagnostic meaning. Loss of heterozygosity in tumor samples, where one allele is deleted or silenced, produces genuine biological imbalance that must be distinguished from technical artifacts. A tumor suppressor gene showing 80:20 allelic ratio could indicate LOH with implications for prognosis and therapy, or could reflect nothing more than capture bias at a technically difficult locus. Without matched normal tissue or population-level calibration of expected allelic ratios, these scenarios remain ambiguous.\nSomatic mosaic variants present at low allele fractions face similar detection challenges. A variant present in 10% of cells produces reads indistinguishable in individual quality from sequencing errors at typical coverage depths. The expected number of alternate reads follows a binomial distribution: at 30× coverage with 10% allele fraction, one expects only 3 alternate reads on average, with substantial probability of observing 0, 1, or 2 reads by chance. Distinguishing this signal from a sequencing error rate of 0.1% to 1% per base requires either much deeper sequencing or molecular techniques like unique molecular identifiers that distinguish true low-frequency variants from PCR duplicates carrying the same error.\nModern variant callers address these challenges by estimating site-specific or sample-specific bias parameters from the data itself, learning from known heterozygous sites throughout the genome rather than assuming uniform 50:50 ratios. These statistical innovations improve sensitivity for mosaic and somatic variant detection but require careful calibration to avoid introducing new sources of systematic error.\n\n\n1.5.4 Complex Variants and Representation\nSmall indels near homopolymers, multi-nucleotide variants (MNVs), and overlapping indels present representation challenges beyond simple detection. The same biological event can often be encoded in multiple equivalent ways depending on alignment and normalization conventions. The variant chr1:100 AT&gt;A might alternatively appear as chr1:101 T&gt;-, with different callers and normalization tools potentially choosing different representations for the identical underlying mutation. These equivalent representations complicate comparisons across pipelines and benchmarks; two callsets may disagree on representation while agreeing on biology, or may appear to agree while representing different events.\nDeep learning models inherit all these errors and uncertainties as their input. If a variant never enters the VCF, no model trained on VCFs can learn its effect. If genotype qualities are miscalibrated, models trained on hard calls may be systematically overconfident in regions where input data are fundamentally noisy. These inherited limitations propagate silently through the analysis chain.\n\n\n\n\n\n\nWhen to Worry\n\n\n\nWhen should analysts be most skeptical of variant calls? Four scenarios warrant extra scrutiny:\n\nVariants in segmental duplications or near paralogous genes\nIndels in homopolymer or tandem repeat regions\nRare variants in underrepresented ancestries\nAny variant in the HLA region from short-read data",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-difficult",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-difficult",
    "title": "1  From Reads to Variants",
    "section": "1.6 Difficult Regions: The Limits of Short-Read Calling",
    "text": "1.6 Difficult Regions: The Limits of Short-Read Calling\nCertain genomic regions resist accurate variant calling regardless of algorithmic sophistication, with their difficulty stemming from fundamental properties of sequence structure that challenge alignment and assembly. These regions are disproportionately responsible for discordant calls between pipelines and technologies (Li 2014). Their clinical importance often exceeds their representation in training data.\n\n\n\n\n\n\nDifficult genomic regions for variant calling\n\n\n\n\nFigure 1.4: Genomic regions where short-read variant calling systematically fails. Ideogram showing the human genome with difficult regions highlighted. Segmental duplications (yellow) including the CYP2D6 pharmacogene region on chromosome 22q11 create mapping ambiguity. The HLA complex (orange) on chromosome 6p21 contains extreme polymorphism that confounds reference-based alignment. Centromeric and pericentromeric regions (gray) lack unique sequence. Tandem repeat disorder loci including HTT (Huntington disease, 4p16) and FMR1 (fragile X, Xq27) resist accurate short-read genotyping. Inset (lower right) shows approximate genome fraction in each difficulty class. Inset (lower left) illustrates how long reads spanning entire difficult regions resolve ambiguities invisible to short reads.\n\n\n\n\n1.6.1 Segmental Duplications and Gene Families\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider the CYP2D6 gene region, which contains the functional gene plus two pseudogenes (CYP2D7, CYP2D8) sharing over 90% sequence identity. What problems would you predict for short-read variant calling in this region?\n\n\nThe CYP2D6 gene illustrates how sequence complexity creates clinical blind spots. This gene encodes a cytochrome P450 enzyme responsible for metabolizing approximately 25% of clinically used drugs, including codeine, tamoxifen, and many antidepressants (zanger_cytochrome_2013?; ingelman-sundberg_genetic_2005?). It resides in a complex genomic region alongside two pseudogenes (CYP2D7 and CYP2D8) sharing over 90% sequence identity. Short reads from one copy map almost equally well to another, producing ambiguous alignments that either receive arbitrarily assigned positions or inflated mapping quality scores that mask the underlying uncertainty.\nVariant callers operating in this region face an impossible choice between sensitivity and specificity. Conservative approaches undercall true variation to avoid false positives; aggressive approaches call spurious variants in the wrong paralog. A patient’s CYP2D6 metabolizer status, critical for drug dosing decisions that can mean the difference between therapeutic efficacy and serious adverse events, may be incorrectly inferred from short-read data alone.\n\n\n1.6.2 Low-Complexity and Repetitive Sequence\nHomopolymers, short tandem repeats (STRs), and other low-complexity regions challenge both sequencing chemistry and alignment algorithms. Indel error rates are especially elevated in these contexts, and many pipelines mask or flag these regions as low confidence. Yet variation in repeats can be biologically critical. Triplet repeat expansion disorders including Huntington disease, fragile X syndrome, and myotonic dystrophy arise from unstable repeat sequences that standard short-read pipelines handle poorly. Models trained on callsets that exclude these regions inherit blind spots at clinically important loci.\n\n\n1.6.3 HLA Region Complexity\nThe human leukocyte antigen (HLA) locus on chromosome 6p21 exemplifies both the biological importance and technical difficulty of complex genomic regions. HLA genes including HLA-A, HLA-B, HLA-C, and HLA-DRB1 encode proteins central to immune recognition and represent some of the most polymorphic sequences in the human genome. The region spans several megabases of near-identical sequences interspersed with gene conversions, copy number variation, and pseudogenes.\nStandard reference-based alignment fails in HLA because the extreme polymorphism means reads carrying common, well-characterized alleles may match the linear reference genome poorly. A read from the HLA-B*57:01 allele (clinically important for predicting abacavir hypersensitivity in HIV treatment) may fail to align or align with low mapping quality, causing systematic undercalling of this medically actionable variant (Mallal et al. 2008). The same problems affect HLA typing for transplant matching, autoimmune disease association studies, and pharmacogenomic testing across diverse therapeutic areas (Robinson et al. 2020; Sakaue et al. 2023).\nSpecialized tools address these challenges through alternative strategies. HLA imputation methods use dense reference panels to infer HLA alleles from array genotypes, enabling large-scale association studies that would otherwise require expensive targeted sequencing (Sakaue et al. 2023). Sequence-based typing tools such as T1K perform HLA and KIR (killer immunoglobulin-like receptor) genotyping directly from WES, WGS, or RNA-seq data by aligning reads against allele databases rather than the linear reference (Song et al. 2022). Graph-based approaches incorporate known HLA alleles as alternate paths through the region, improving both alignment and variant calling (Garrison et al. 2018; Liao et al. 2023).\nHLA exemplifies a broader principle: regions that are biologically rich and clinically actionable are often technically difficult. Deep models trained on callsets that downweight or exclude these regions inherit their absence, creating blind spots precisely where accurate genotyping matters most.\nThese challenging regions create systematic evaluation problems for foundation models. The homology-aware data splitting strategies required to avoid leakage in these regions are detailed in Section 12.2. HLA complexity motivates specialized network-based approaches discussed in ?sec-ch18-biological-networks.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-benchmarks",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-benchmarks",
    "title": "1  From Reads to Variants",
    "section": "1.7 Benchmarking and Ground Truth",
    "text": "1.7 Benchmarking and Ground Truth\nEvaluating variant callers requires high-confidence truth sets and standardized comparison tools. The challenge is that “ground truth” for variant calling is not actually true in any absolute sense; it represents consensus derived from multiple imperfect observations using different technologies and algorithms. Without careful benchmarking design, it is easy to overfit to specific datasets, underestimate errors in difficult regions, or misinterpret the practical significance of small metric improvements. Benchmark construction principles and their limitations for foundation model evaluation are examined comprehensively in Section 11.5 and Chapter 12.\n\n1.7.1 GIAB Reference Samples\nThe Genome in a Bottle (GIAB) Consortium, coordinated by NIST, provides extensively characterized reference samples with validated variant calls across most of the genome (Zook et al. 2019). The primary sample is NA12878 (also known as HG001), a female of European ancestry from the CEPH/Utah pedigree with the longest history of multi-platform characterization. Additional samples span ancestral diversity and family structures: HG002 through HG004 comprise an Ashkenazi Jewish trio enabling trio-based validation, while HG005 through HG007 provide a Han Chinese trio.\nFor each sample, GIAB provides high-confidence variant calls representing consensus from multiple sequencing technologies and variant callers that constitute the best current estimate of true genotypes. Equally important are the high-confidence regions, genomic intervals where the truth set is believed to be reliable. Performance outside these regions remains formally unmeasured. Benchmarking tools such as hap.py and RTG Tools enable standardized comparison of test callsets against truth, implementing reproducible calculation of precision, recall, and F1 metrics by variant type (Krusche et al. 2019; “RealTimeGenomics/Rtg-Core” 2025).\n\n\n1.7.2 Metrics and Their Meaning\nSmall differences in benchmark metrics can obscure large differences in clinical utility, while large headline improvements may concentrate in regions with little practical importance. Standard metrics for variant calling include recall (sensitivity), the fraction of true variants in the benchmark successfully identified by the caller; precision (positive predictive value), the fraction of called variants that are present in the benchmark truth set; and F1 score, the harmonic mean of precision and recall providing a single summary when both matter equally. These metrics are typically reported separately for SNVs and indels and may be stratified by genomic context to reveal where performance degrades.\nMetrics can be defined at different levels: per-variant (did we identify the correct alternate allele?), per-genotype (did we correctly determine zygosity?), or per-site (did we recognize variation at this position regardless of allele?). For downstream models, genotype-level accuracy and sample-level completeness often matter more than simply counting variant matches. A model that receives incorrect genotypes at common regulatory variants will learn corrupted associations even if overall variant-level metrics appear strong.\n\n\n1.7.3 Limitations of Benchmarks\nGIAB truth sets derive primarily from a small number of deeply sequenced samples, predominantly of European ancestry in early releases, and initially focused on genomic regions where high confidence was achievable. High-confidence regions cover approximately 85 to 90 percent of the genome, leaving performance in excluded regions formally unknown. Performance in underrepresented ancestries, in complex structural variant regions, and for novel variant classes may differ substantially from headline GIAB metrics (Zook et al. 2019; Liao et al. 2023).\nWhen benchmarks are reused extensively for method development, the risk of overfitting to benchmark-specific patterns becomes substantial. Pipelines may be tuned to maximize F1 on GIAB-like samples without improving performance on real-world cohorts with different ancestry composition, sequencing protocols, or variant spectra. For deep learning-based callers with large capacity to absorb quirks in training data, this risk is especially acute. Similar concerns apply to benchmarking and evaluation of deep models more broadly (Chapter 11, Chapter 12).\nOngoing efforts from the T2T Consortium and the Human Pangenome Reference Consortium are expanding benchmark scope to include complete genome assemblies and diverse haplotype collections that better represent human genetic diversity (Nurk et al. 2022; Liao et al. 2023).",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-deepvariant",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-deepvariant",
    "title": "1  From Reads to Variants",
    "section": "1.8 DeepVariant: Variant Calling as Image Classification",
    "text": "1.8 DeepVariant: Variant Calling as Image Classification\nClassical variant calling pipelines encode accumulated expert intuition through hand-crafted features and carefully tuned heuristics developed over years of experience with sequencing data. DeepVariant, introduced by Google in 2018, posed a different question: what if we let the model learn these patterns directly from data?\n\n\n\n\n\n\nStop and Think\n\n\n\nClassical variant callers use hand-crafted features like depth, strand bias, and mapping quality. If you were to reformulate variant calling for a neural network, how might you represent the raw read evidence? What representation would preserve maximum information?\n\n\nThe key insight was not better probabilistic modeling of sequencing errors but rather a reformulation of the problem itself. Variant calling becomes image classification, and convolutional neural networks learn to distinguish true variants from artifacts through the same pattern recognition that enables them to classify natural images (Poplin et al. 2018).\n\n\n\n\n\n\nKey Insight\n\n\n\nDeepVariant’s innovation is representational: by encoding pileups as images, variant calling becomes pattern recognition, allowing the model to learn artifact signatures that human experts never explicitly defined.\n\n\n\n1.8.1 Pileup Images as Input\nRepresenting reads as image-like tensors allows the model to learn from raw evidence rather than predefined summary statistics. Patterns invisible to hand-crafted heuristics (strand-biased support clustered at read ends, quality degradation in specific contexts) become learnable. Around each candidate variant site, DeepVariant constructs a multi-channel tensor resembling an image. Each row corresponds to a read overlapping the site, with columns indexing positions relative to the candidate variant. Channels encode multiple features: match or mismatch with the reference, Phred-scaled base quality, mapping quality, strand orientation, support for different alleles, and additional alignment characteristics. The reference sequence and candidate alleles are overlaid as additional channels providing context.\nThis representation transforms the variant calling problem fundamentally. Rather than computing summary statistics (depth, allelic balance, strand bias) and feeding them to a classifier with predefined decision rules, DeepVariant presents the raw evidence to a neural network. The model learns that strand-biased support clustered at read ends looks different from balanced support distributed across read positions without anyone explicitly defining these features or their relative importance. Patterns invisible to hand-crafted heuristics become learnable.\n\n\n\n\n\n\nDeepVariant pileup tensor representation\n\n\n\n\nFigure 1.5: DeepVariant encodes read evidence as a multi-channel pileup tensor for CNN-based genotype classification. Each row represents a sequencing read overlapping the candidate variant position (center column). Multiple channels encode: base identity (matches in gray, mismatches color-coded by nucleotide), base quality (intensity gradient), mapping quality, strand orientation, and allele support. The reference sequence appears at top. This example shows a heterozygous SNV with approximately half of reads supporting each allele. The CNN processes this tensor representation and outputs posterior probabilities over three genotype classes. The called genotype (0/1) corresponds to the highest posterior probability (0.96).\n\n\n\n\n\n1.8.2 Architecture and Training\nThe reformulation of variant calling as image classification enables borrowing architectures proven effective for natural images. DeepVariant uses an Inception-style CNN architecture originally developed for natural image classification. The convolutional architecture that makes this possible is examined in detail in Chapter 6. The network processes the pileup tensor through multiple convolutional layers, pooling operations, and nonlinearities, outputting posterior probabilities over three genotype classes (homozygous reference, heterozygous, homozygous alternate) for each candidate site (Poplin et al. 2018).\nTraining uses high-confidence truth sets such as GIAB genomes. The model observes many examples of true variants and non-variants along with their associated pileup images, learning complex decision boundaries that integrate base quality, mapping quality, local sequence context, and read-level patterns. Where VQSR fits a separate model on hand-selected annotations after initial calling, DeepVariant processes raw evidence directly during the primary classification step.\nThe end-to-end training produces well-calibrated genotype likelihoods across a range of sequencing chemistries, instruments, and read lengths, particularly when fine-tuned for specific experimental contexts (Yun et al. 2021). Once trained, the same architecture generalizes across whole-genome versus whole-exome data, PCR-free versus PCR-amplified libraries, and different sequencing instruments. This adaptability contrasts with classical pipelines where calibration is often a separate, post hoc step requiring platform-specific tuning by experts.\nOn GIAB benchmark HG002, DeepVariant achieved F1 of 99.7% for SNVs compared to 99.5% for GATK HaplotypeCaller, with larger gains for indels (99.4% vs 98.9%).\n\n\n1.8.3 Cohort Calling with GLnexus\nDeepVariant operates primarily at the per-sample level, producing a gVCF of genotype likelihoods for each individual sample. To generate a multi-sample VCF suitable for population-scale analysis, these per-sample results must be combined through joint genotyping.\nGLnexus provides cohort-level integration by combining the per-sample gVCF files introduced in Section 1.3.2, pooling the genotype likelihoods across samples (Yun et al. 2021). The system merges per-sample likelihoods, applies cohort-level priors informed by observed allele frequencies, and performs multi-sample genotype refinement and filtering. Together, DeepVariant and GLnexus form a modular pipeline where deep learning replaces the per-sample likelihood engine while the overall architecture (per-sample calls, joint genotyping, cohort filtering) remains structurally similar to classical approaches.\nJoint calling improves sensitivity for rare variants by pooling evidence across carriers, ensures consistent variant representation across all samples in a cohort, and enables cohort-level quality filters that identify systematic artifacts visible only across many samples. This combination has become a de facto standard for large WES and WGS projects, including recent releases from gnomAD and the UK Biobank (Karczewski et al. 2020; Bycroft et al. 2018).\n\n\n1.8.4 Comparison with Classical Approaches\nThe fundamental difference between DeepVariant and classical pipelines lies in how evidence is combined. Recall that HaplotypeCaller uses pair-HMM models (see Section 1.3.2) that explicitly assume read independence–an assumption that DeepVariant’s CNN architecture circumvents by processing the entire pileup as a single input. HaplotypeCaller uses pair-HMM models with explicit assumptions about read independence and then applies VQSR to recalibrate quality scores using hand-selected annotation features. DeepVariant processes entire pileups simultaneously, implicitly learning correlations among reads that violate the independence assumptions built into classical probabilistic models.\n\n\n\nTable 1.3: Comparison of classical and deep learning variant calling\n\n\n\n\n\n\n\n\n\n\nAspect\nHaplotypeCaller (Classical)\nDeepVariant (Deep Learning)\n\n\n\n\nEvidence combination\nPair-HMM with explicit independence assumption\nCNN processes full pileup simultaneously\n\n\nCalibration\nSeparate VQSR step with hand-selected features\nEmerges from end-to-end training\n\n\nArtifact detection\nHand-crafted filters\nLearned representations\n\n\nTransfer\nRequires platform-specific tuning\nFine-tuning often sufficient\n\n\n\n\n\n\nThis end-to-end approach offers several practical advantages. Calibration emerges from training rather than requiring separate recalibration steps with their own parameter tuning. Transfer across platforms and even species often succeeds with modest fine-tuning rather than complete redevelopment. The model can detect subtle artifact patterns that escape hand-crafted filters, learning representations of error modes that human experts never explicitly described.\nBoth approaches share important limitations. Neither handles structural variants well; both focus primarily on SNVs and small indels. Both operate within the same overall pipeline framework: alignment, duplicate marking, and joint genotyping remain largely unchanged regardless of the per-sample caller used. DeepVariant is best understood as a drop-in replacement for the per-sample calling step, not a complete reimagining of variant discovery from raw data.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-implications",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-implications",
    "title": "1  From Reads to Variants",
    "section": "1.9 Implications for Genomic Deep Learning",
    "text": "1.9 Implications for Genomic Deep Learning\nNGS and variant calling establish the foundation for genomic deep learning. They determine what data downstream models receive, where coverage exists, and where systematic blind spots remain hidden. Understanding how variants are called, and where that process fails, is essential for interpreting the performance and limitations of every model built on this foundation.\n\n1.9.1 Variants as Atomic Units\nThe output of WES and WGS pipelines (a VCF of SNVs, indels, and inferred genotypes) defines the atomic units that many downstream models operate on. Polygenic risk scores treat variants as weighted features summed across the genome (Section 3.5). GWAS summary statistics quantify associations at individual variant positions. Variant annotation tools classify each site by predicted functional consequence. Foundation models that operate on genotypes rather than raw sequence inherit the variant catalog as their effective vocabulary.\nIf a variant is never called, it cannot appear in training data, and no model can learn its effect. False positives introduce noise into labels and features, teaching models to associate spurious variants with phenotypes. False negatives create blind spots where models must extrapolate from incomplete information, often without any indication that data are missing. Choices about phasing, imputation, and variant representation determine whether models see haplotype-structured inputs, unordered genotypes, or scalar dosage summaries. The quality of variant calls directly limits the quality of everything built upon them.\n\n\n1.9.2 Inherited Biases and Blind Spots\nUpstream decisions constrain what downstream models can learn. If an assay rarely observes indels in certain repeat classes, models trained on those callsets effectively learn a world where such variants do not exist. If certain ancestries are underrepresented in reference panels or truth sets, models may perform poorly for those populations while appearing well-calibrated in benchmarks dominated by European samples. High-confidence region definitions determine which variants enter training sets; variants in excluded regions are invisible to models regardless of their biological importance.\nFor regulatory sequence models and variant effect predictors (Chapter 14 for DNA language models, Section 16.2 for regulatory models, Section 6.2 for CNNs, Section 4.3 for classical approaches), upstream variant calling determines which sites appear as candidates and how often certain sequence patterns are observed in association with functional outcomes. The HLA blind spot in short-read calling means that models trained primarily on short-read callsets will systematically underperform for immune-related variants despite their substantial clinical importance for autoimmune disease, transplant rejection, and drug hypersensitivity. The accuracy ceiling imposed by variant calling quality on downstream variant effect prediction is quantified in ?sec-ch14-combining-evidence, where multiple lines of evidence must be integrated despite systematic gaps in each.\n\n\n1.9.3 Effect Sizes Across the Frequency Spectrum\nVariant calling quality modulates the effective effect sizes that downstream models can detect, with different dynamics for common and rare variation. For common variants contributing to highly polygenic traits, modest genotype error acts as additional measurement noise that attenuates effect size estimates without creating spurious large effects. Improving variant calling in already “easy” genomic regions yields diminishing returns compared to simply increasing sample size.\nFor rare variants with large individual effects, the dynamics change substantially. Loss-of-function variants, damaging missense mutations, and splice-altering changes can have substantial effects on disease risk. Here, false negatives dominate the problem: if the variant is never called, its effect is invisible to association tests and to models trained on called genotypes. Small improvements in recall for clinically important rare variants can have outsized impact on gene discovery and interpretation.\nImputed variants introduce their own effect size modulation. The squared correlation between true and imputed genotypes acts as an attenuation factor: an association with true effect size \\(\\beta\\) behaves as if the effect were approximately \\(r^2 \\beta\\) in downstream analyses using imputed dosages. Improvements in imputation quality, particularly for underrepresented ancestries where current panels perform poorly, directly scale the effective signals that models can learn.\nClinically critical loci often present the most challenging technical contexts, creating a systematic mismatch between importance and data quality. Pharmacogenomic variants in CYP gene families, immune-related variants in HLA, and many other medically actionable sites reside in regions where standard pipelines perform poorly. Global accuracy metrics may change only slightly when these regions improve, but the clinical impact can be substantial.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch01-ngs.html#sec-ch01-reliability",
    "href": "part_1/p1-ch01-ngs.html#sec-ch01-reliability",
    "title": "1  From Reads to Variants",
    "section": "1.10 Reliability Landscape",
    "text": "1.10 Reliability Landscape\nVariant calling produces the substrate on which every subsequent model operates. The quality of that substrate varies systematically: high-confidence calls in unique sequence with adequate coverage, uncertain calls in repetitive regions and structural variant breakpoints, systematic gaps where short reads cannot reach. These patterns are not random. They concentrate in genomic regions of particular biological importance: segmental duplications that drive gene family evolution, tandem repeats that modulate gene expression, HLA haplotypes that determine immune response.\nDeepVariant and related approaches demonstrate that learned representations can outperform hand-crafted heuristics, at least on established benchmarks. This paradigm recurs across genomic deep learning: sequence-to-function models that learn regulatory grammar directly from data (Chapter 16), splice predictors that discover motifs without explicit encoding (Chapter 6), language models that learn evolutionary constraint from sequence alone (Chapter 14). In each case, the central question is whether the additional model capacity captures genuine biological signal or overfits to artifacts in training data. The distinction between capturing biological signal versus overfitting to training artifacts is examined systematically in Section 24.4 for interpretability analysis and Section 12.4 for evaluation methodology.\nThe models that follow inherit whatever systematic biases exist in variant calls. A pathogenicity predictor trained on ClinVar labels inherits the sequencing technologies and population composition that generated those labels. A fine-mapping method that trusts variant calls uniformly will miscalibrate its posterior probabilities in difficult regions. Understanding where variant calling succeeds and fails is prerequisite to understanding where downstream models can be trusted.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat are the three fundamentally different sources of signal that manifest as mismatches between reads and reference sequence?\nWhy is haplotype phasing clinically important, particularly for compound heterozygosity in recessive disorders?\nWhat specific approach does DeepVariant use to distinguish true variants from artifacts, and how does this differ from classical variant callers?\nWhy do variant calling errors in difficult genomic regions create systematic blind spots for downstream machine learning models?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nThree signal sources: (1) sequencing errors from instrument noise and library preparation, (2) alignment artifacts when reads map to incorrect genomic locations, and (3) genuine biological variation including germline variants, somatic mutations, and mosaicism. All three produce identical-looking mismatches in the data.\nPhasing importance: Phasing determines whether two variants are on the same chromosome (cis) or different chromosomes (trans). For compound heterozygosity in recessive disorders like cystic fibrosis, cis configuration leaves one functional gene copy (carrier status), while trans configuration eliminates all functional copies (affected status). The clinical outcomes are entirely different despite identical unphased genotypes.\nDeepVariant approach: DeepVariant reformulates variant calling as image classification by encoding read pileups as multi-channel tensors and using a CNN to learn patterns distinguishing true variants from artifacts. Classical callers like HaplotypeCaller use explicit probabilistic models (pair-HMMs) with hand-crafted features and separate recalibration steps (VQSR). DeepVariant learns artifact signatures end-to-end from data rather than requiring expert-defined heuristics.\nBlind spots in downstream models: If a variant is never called due to systematic failures in difficult regions (segmental duplications, HLA, tandem repeats), it cannot appear in training data. Models trained on VCF files effectively learn from an incomplete representation of the genome where entire classes of clinically important variants are invisible. No amount of sophisticated downstream modeling can recover information lost at the variant calling stage.\n\n\n\n\n\n\nKey concepts: NGS data challenges, targeting strategies (panels/WES/WGS), classical variant calling pipelines, phasing, error sources, difficult genomic regions, benchmarking, DeepVariant\nMain takeaways:\n\nVariant calls are statistical inferences that carry systematic errors, especially in repetitive regions, segmental duplications, and the HLA complex\nThe choice of sequencing strategy (panel vs. WES vs. WGS, short-read vs. long-read) determines which variants are discoverable\nDeep learning approaches like DeepVariant learn to distinguish variants from artifacts by processing raw pileup evidence as images\nAll downstream models inherit the blind spots of variant calling–regions that cannot be reliably called become invisible to models trained on VCF files\n\nConnections to later chapters:\n\nEmbedding strategies: Section 5.6\nCNN architectures: Chapter 6, Section 6.2\nEvaluation methodology: Section 12.2, Section 11.5\nConfounding and bias: Chapter 12\nFoundation models: Section 13.2, Chapter 14\nRegulatory models: Chapter 16\n\n\n\n\n\n\n\nBrowning, Brian L., Xiaowen Tian, Ying Zhou, and Sharon R. Browning. 2021. “Fast Two-Stage Phasing of Large-Scale Sequence Data.” American Journal of Human Genetics 108 (10): 1880–90. https://doi.org/10.1016/j.ajhg.2021.08.005.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nCirulli, Elizabeth T., Simon White, Robert W. Read, Gai Elhanan, William J. Metcalf, Francisco Tanudjaja, Donna M. Fath, et al. 2020. “Genome-Wide Rare Variant Analysis for Thousands of Phenotypes in over 70,000 Exomes from Two Cohorts.” Nature Communications 11 (1): 542. https://doi.org/10.1038/s41467-020-14288-y.\n\n\nDabernig-Heinz, Johanna, Mara Lohde, Martin Hölzer, Adriana Cabal, Rick Conzemius, Christian Brandt, Matthias Kohl, et al. 2024. “A Multicenter Study on Accuracy and Reproducibility of Nanopore Sequencing-Based Genotyping of Bacterial Pathogens.” Journal of Clinical Microbiology 62 (9): e00628–24. https://doi.org/10.1128/jcm.00628-24.\n\n\nDePristo, Mark A., Eric Banks, Ryan Poplin, Kiran V. Garimella, Jared R. Maguire, Christopher Hartl, Anthony A. Philippakis, et al. 2011. “A Framework for Variation Discovery and Genotyping Using Next-Generation DNA Sequencing Data.” Nature Genetics 43 (5): 491–98. https://doi.org/10.1038/ng.806.\n\n\nGarrison, Erik, Jouni Sirén, Adam M. Novak, Glenn Hickey, Jordan M. Eizenga, Eric T. Dawson, William Jones, et al. 2018. “Variation Graph Toolkit Improves Read Mapping by Representing Genetic Variation in the Reference.” Nature Biotechnology 36 (9): 875–79. https://doi.org/10.1038/nbt.4227.\n\n\nGoodwin, Sara, John D. McPherson, and W. Richard McCombie. 2016. “Coming of Age: Ten Years of Next-Generation Sequencing Technologies.” Nature Reviews Genetics 17 (6): 333–51. https://doi.org/10.1038/nrg.2016.49.\n\n\nJiang, Tao, Yongzhuang Liu, Yue Jiang, Junyi Li, Yan Gao, Zhe Cui, Yadong Liu, Bo Liu, and Yadong Wang. 2020. “Long-Read-Based Human Genomic Structural Variation Detection with cuteSV.” Genome Biology 21 (1): 189. https://doi.org/10.1186/s13059-020-02107-y.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKrusche, Peter, Len Trigg, Paul C. Boutros, Christopher E. Mason, Francisco M. De La Vega, Benjamin L. Moore, Mar Gonzalez-Porta, et al. 2019. “Best Practices for Benchmarking Germline Small Variant Calls in Human Genomes.” Nature Biotechnology 37 (5): 555–60. https://doi.org/10.1038/s41587-019-0054-x.\n\n\nLi, Heng. 2013. “Aligning Sequence Reads, Clone Sequences and Assembly Contigs with BWA-MEM.” arXiv. https://doi.org/10.48550/arXiv.1303.3997.\n\n\n———. 2014. “Towards Better Understanding of Artifacts in Variant Calling from High-Coverage Samples.” Bioinformatics 30 (20): 2843–51. https://doi.org/10.1093/bioinformatics/btu356.\n\n\n———. 2018. “Minimap2: Pairwise Alignment for Nucleotide Sequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness, Glenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome Reference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nLoh, Po-Ru, Petr Danecek, Pier Francesco Palamara, Christian Fuchsberger, Yakir A Reshef, Hilary K Finucane, Sebastian Schoenherr, et al. 2016. “Reference-Based Phasing Using the Haplotype Reference Consortium Panel.” Nature Genetics 48 (11): 1443–48. https://doi.org/10.1038/ng.3679.\n\n\nMallal, Simon, Elizabeth Phillips, Giampiero Carosi, Jean-Michel Molina, Cassy Workman, Janez Tomažič, Eva Jägel-Guedes, et al. 2008. “HLA-B*5701 Screening for Hypersensitivity to Abacavir.” New England Journal of Medicine 358 (6): 568–79. https://doi.org/10.1056/NEJMoa0706135.\n\n\nNielsen, Rasmus, Joshua S. Paul, Anders Albrechtsen, and Yun S. Song. 2011. “Genotype and SNP Calling from Next-Generation Sequencing Data.” Nature Reviews. Genetics 12 (6): 443–51. https://doi.org/10.1038/nrg2986.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V. Bzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The Complete Sequence of a Human Genome.” Science 376 (6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Connell, Jared, Deepti Gurdasani, Olivier Delaneau, Nicola Pirastu, Sheila Ulivi, Massimiliano Cocca, Michela Traglia, et al. 2014. “A General Approach for Haplotype Phasing Across the Full Spectrum of Relatedness.” PLOS Genetics 10 (4): e1004234. https://doi.org/10.1371/journal.pgen.1004234.\n\n\n“PacificBiosciences/Pbsv.” 2025. PacBio. https://github.com/PacificBiosciences/pbsv.\n\n\nPoplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, et al. 2018. “[DeepVariant] A Universal SNP and Small-Indel Variant Caller Using Deep Neural Networks.” Nature Biotechnology 36 (10): 983–87. https://doi.org/10.1038/nbt.4235.\n\n\n“RealTimeGenomics/Rtg-Core.” 2025. Real Time Genomics. https://github.com/RealTimeGenomics/rtg-core.\n\n\nRobinson, James, Dominic J Barker, Xenia Georgiou, Michael A Cooper, Paul Flicek, and Steven G E Marsh. 2020. “IPD-IMGT/HLA Database.” Nucleic Acids Research 48 (D1): D948–55. https://doi.org/10.1093/nar/gkz950.\n\n\nRubinacci, Simone, Diogo M. Ribeiro, Robin J. Hofmeister, and Olivier Delaneau. 2021. “Efficient Phasing and Imputation of Low-Coverage Sequencing Data Using Large Reference Panels.” Nature Genetics 53 (1): 120–26. https://doi.org/10.1038/s41588-020-00756-0.\n\n\nSakaue, Saori, Saisriram Gurajala, Michelle Curtis, Yang Luo, Wanson Choi, Kazuyoshi Ishigaki, Joyce B. Kang, et al. 2023. “Tutorial: A Statistical Genetics Guide to Identifying HLA Alleles Driving Complex Disease.” Nature Protocols 18 (9): 2625–41. https://doi.org/10.1038/s41596-023-00853-4.\n\n\nShafin, Kishwar, Trevor Pesout, Pi-Chuan Chang, Maria Nattestad, Alexey Kolesnikov, Sidharth Goel, Gunjan Baid, et al. 2021. “Haplotype-Aware Variant Calling with PEPPER-Margin-DeepVariant Enables High Accuracy in Nanopore Long-Reads.” Nature Methods 18 (11): 1322–32. https://doi.org/10.1038/s41592-021-01299-w.\n\n\nSmolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W. Horner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024. “Detection of Mosaic and Population-Level Structural Variants with Sniffles2.” Nature Biotechnology 42 (10): 1571–80. https://doi.org/10.1038/s41587-023-02024-y.\n\n\nSong, Li, Gali Bai, X. Shirley Liu, Bo Li, and Heng Li. 2022. “T1K: Efficient and Accurate KIR and HLA Genotyping with Next-Generation Sequencing Data.” bioRxiv. https://doi.org/10.1101/2022.10.26.513955.\n\n\nVan der Auwera, Geraldine A., Mauricio O. Carneiro, Christopher Hartl, Ryan Poplin, Guillermo del Angel, Ami Levy-Moonshine, Tadeusz Jordan, et al. 2018. “From FastQ Data to High-Confidence Variant Calls: The Genome Analysis Toolkit Best Practices Pipeline.” Current Protocols in Bioinformatics 43 (1): 11.10.1–33. https://doi.org/10.1002/0471250953.bi1110s43.\n\n\nWenger, Aaron M., Paul Peluso, William J. Rowell, Pi-Chuan Chang, Richard J. Hall, Gregory T. Concepcion, Jana Ebler, et al. 2019. “Accurate Circular Consensus Long-Read Sequencing Improves Variant Detection and Assembly of a Human Genome.” Nature Biotechnology 37 (10): 1155–62. https://doi.org/10.1038/s41587-019-0217-9.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll, and Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls Using DeepVariant and GLnexus.” Bioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam, and Ruibang Luo. 2022. “Symphonizing Pileup and Full-Alignment for Deep Learning-Based Long-Read Variant Calling.” Nature Computational Science 2 (12): 797–803. https://doi.org/10.1038/s43588-022-00387-x.\n\n\nZook, Justin M., Jennifer McDaniel, Nathan D. Olson, Justin Wagner, Hemang Parikh, Haynes Heaton, Sean A. Irvine, et al. 2019. “An Open Resource for Accurately Benchmarking Small Variant and Reference Calls.” Nature Biotechnology 37 (5): 561–66. https://doi.org/10.1038/s41587-019-0074-6.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Reads to Variants</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html",
    "href": "part_1/p1-ch02-data.html",
    "title": "2  Data Landscape",
    "section": "",
    "text": "2.1 Reference Genomes and Gene Annotations\nA family arrives at a genetics clinic after their newborn’s screening reveals a potential metabolic disorder. The clinical team orders whole-genome sequencing and receives a report identifying a novel variant in a gene associated with the condition. The variant’s coordinates place it at the boundary between an exon and an intron, potentially disrupting splicing. Yet whether this interpretation is correct depends on decisions made years before the child was born: which positions constitute exon boundaries, which transcript model defines the canonical gene structure, and which sequence serves as the reference against which “variant” is defined. Reference genomes and gene annotations are so foundational that their assumptions often become invisible, yet every downstream analysis inherits the choices embedded in these resources. A model cannot learn about a regulatory element for a transcript that does not exist in the annotation.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-reference",
    "href": "part_1/p1-ch02-data.html#sec-ch02-reference",
    "title": "2  Data Landscape",
    "section": "",
    "text": "2.1.1 Reference Assemblies\nA patient’s clinical sequencing reveals a potentially pathogenic variant in a duplicated region of chromosome 17. The variant calling pipeline reports a confident genotype, the annotation tool predicts a frameshift, and the clinical team prepares to discuss the finding with the family. Yet the “variant” may be an artifact of misalignment: reads from a paralogous sequence elsewhere in the genome mapped incorrectly because the reference assembly collapsed two distinct loci into one. Whether this error occurs, whether it can be detected, and whether the clinical interpretation has any foundation in biological reality all depend on the choice of reference genome.\nMost modern pipelines align reads to a small number of reference assemblies, predominantly Genome Reference Consortium Human Build 38 (GRCh38) or the newer telomere-to-telomere CHM13 assembly (T2T-CHM13) (Nurk et al. 2022). A reference genome is not simply a consensus sequence; it encodes a series of consequential decisions about how to represent duplications, alternate haplotypes, and unresolved gaps. These decisions determine which regions are mappable by short reads, how structural variants are represented, and how comparable results will be across cohorts built on different assemblies. The variant calling pipelines described in Chapter 1 depend fundamentally on these reference choices. Variant callers that rely on these reference assemblies face characteristic failures in difficult regions, as detailed in Section 1.6.\n\n\n\n\n\n\nKey Insight: Reference Choice Shapes What Is Detectable\n\n\n\nThe reference genome is not neutral infrastructure; it is an active filter. Regions absent from the reference cannot be mapped to. Collapsed duplications create spurious variants. Unresolved gaps hide entire genes. When a model fails to detect a variant or makes a false call, ask: could this be a reference assembly limitation rather than a model limitation?\n\n\nGraph-based and pangenome references relax the assumption of a single linear reference, representing multiple haplotypes and ancestries within a unified coordinate system (Liao et al. 2023). Comparative multi-species references, such as those used in mammalian constraint maps from the Zoonomia consortium (Sullivan et al. 2023), extend this idea across species, providing evolutionary conservation scores that feed directly into deleteriousness predictors and gene-level constraint metrics discussed in Section 4.1.1 for evolutionary approaches and Section 4.3 for integrated scoring.\nFor most genomic deep learning applications, the practical reality is still GRCh37 or GRCh38 coordinates, often with incremental patches. Models trained on these resources therefore inherit their blind spots: incomplete or collapsed segmental duplications, underrepresented ancestries in pangenome construction, and uneven quality across chromosomes and regions. These limitations concentrate in precisely the regions where variant interpretation matters most (such as the HLA locus, pharmacogenes with structural variation, and segmental duplications harboring disease genes), creating a systematic mismatch between clinical importance and reference quality.\n\n\n2.1.2 Gene Models\nA child presents with developmental delay and muscle weakness. Whole-genome sequencing identifies a novel variant near the DMD gene, which encodes dystrophin and causes Duchenne muscular dystrophy when disrupted. The annotation pipeline reports the variant as intronic and unlikely to affect protein function. Yet DMD spans 2.2 megabases and includes 79 exons with complex alternative splicing; whether this variant disrupts a tissue-specific isoform depends entirely on which transcript model the annotation tool uses. The clinical implications are entirely different, yet the underlying sequence is identical: only the annotation changes.\nGene annotation databases such as GENCODE and RefSeq define the biological vocabulary overlaid on reference coordinates: exon-intron structures, canonical and alternative transcripts, start and stop codons, and untranslated regions (Frankish et al. 2019; O’Leary et al. 2016). These annotations distinguish coding from non-coding variants, identify splice-disrupting mutations, and map functional genomics signals to genes. They also establish the units (genes, transcripts, exons) that downstream models implicitly operate on. Splicing prediction models in Section 6.5 learn splice site grammar from annotated exon-intron boundaries, then apply those patterns to detect both canonical and cryptic sites.\nThe MANE Select project provides a single matched transcript per protein-coding gene that is identical between GENCODE and RefSeq, simplifying clinical interpretation and variant reporting (Morales et al. 2022). This standardization makes variant descriptions consistent across laboratories, yet it privileges a single isoform over biological complexity. In contexts where tissue-specific or developmentally regulated isoforms drive disease (alternative splicing in muscular dystrophies, isoform-specific expression in neuropsychiatric conditions), the canonical transcript may miss the relevant biology.\n\n\n\n\n\n\nDeep Dive: Alternative Splicing\n\n\n\nFor ML readers: A single gene can produce multiple different proteins through alternative splicing:\nThe basics: After DNA is transcribed into pre-mRNA, introns (non-coding segments) are removed and exons (coding segments) are joined together. Alternative splicing means different combinations of exons can be included in the final mRNA, producing different protein variants (isoforms) from the same gene.\nSplicing patterns:\n\nExon skipping: An exon is excluded from the final transcript\nAlternative 5’ or 3’ splice sites: Different endpoints for exon boundaries\nIntron retention: An intron is kept in the mature mRNA\nMutually exclusive exons: One of two exons is included, never both\n\nScale: Over 95% of human multi-exon genes undergo alternative splicing. The ~20,000 human genes produce an estimated 100,000+ distinct protein isoforms.\nWhy it matters for genomic ML: A variant might be benign in the canonical isoform but pathogenic in a tissue-specific isoform. Models trained only on canonical transcripts miss this complexity. Splicing prediction models like SpliceAI (Section 6.5) predict how variants affect splice site usage.\n\n\nNew isoforms continue to be discovered, alternative splicing remains incompletely cataloged, and cell-type-specific transcripts may be absent from bulk-derived annotations. Non-coding RNA genes and pseudogenes are even more unevenly annotated. These gaps propagate through every tool built on them: variant effect predictors cannot score consequences for transcripts that do not exist in their reference annotation, and expression models cannot predict isoforms they were never trained on.\n\n\n\n\n\n\nStop and Think: Annotation Dependencies\n\n\n\nConsider a variant effect predictor trained exclusively on MANE Select transcripts. What types of pathogenic variants might it systematically miss? Think about tissue-specific isoforms, retained introns, and alternative first exons before reading on.\nThe predictor would miss variants affecting: (1) tissue-specific isoforms not represented in MANE, (2) alternative exons used in specific developmental stages or cell types, (3) non-coding transcripts and regulatory RNAs, and (4) novel splice isoforms not yet annotated. This illustrates how annotation choices propagate into model blind spots.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-population",
    "href": "part_1/p1-ch02-data.html#sec-ch02-population",
    "title": "2  Data Landscape",
    "section": "2.2 Population Variant Catalogs and Allele Frequencies",
    "text": "2.2 Population Variant Catalogs and Allele Frequencies\nA clinical geneticist evaluates a child with an undiagnosed syndrome and identifies a novel missense variant in a candidate gene. The question that determines what happens next is deceptively simple: has anyone else carried this variant? If the variant appears in thousands of healthy adults, it is almost certainly benign. If it has never been observed across hundreds of thousands of sequenced genomes, that absence becomes evidence of selective pressure against the variant, strongly suggesting functional consequence. Without population-scale variant catalogs, this inference is impossible, and every rare variant would demand the same level of scrutiny regardless of its actual likelihood of causing disease.\n\n\n\n\n\n\nAncestry representation disparity across major genomic resources\n\n\n\n\nFigure 2.2: Ancestry representation across major genomic resources. Stacked bars show the proportion of samples from different ancestral backgrounds in key databases. European-ancestry individuals (blue) comprise approximately 78% of GWAS participants despite representing roughly 16% of the global population. This overrepresentation propagates through variant interpretation databases, functional genomics atlases, and polygenic score development, creating systematic gaps in genomic medicine for underrepresented populations. Inset map highlights continental regions with the largest representation deficits relative to population size.\n\n\n\nAllele frequency, the proportion of chromosomes in a reference population carrying a given variant, serves as one of the most powerful priors in variant interpretation. Beyond simple filtering, allele frequencies inform statistical frameworks for case-control association, provide training signal for deleteriousness predictors, and enable imputation of ungenotyped variants through linkage disequilibrium (see Chapter 3). The catalogs described below have progressively expanded in sample size, ancestral diversity, and annotation depth, transforming variant interpretation from an ad hoc exercise into a quantitative discipline.\n\n\n\n\n\n\nDeep Dive: Allele Frequency\n\n\n\nFor ML readers: Allele frequency is the proportion of chromosomes carrying a particular variant in a population:\nMinor Allele Frequency (MAF): The frequency of the less common allele at a biallelic site. If 99% of chromosomes carry A and 1% carry G at a position, MAF = 0.01 (1%).\nFrequency categories in practice:\n\n\n\n\n\n\n\n\nCategory\nMAF\nTypical interpretation\n\n\n\n\nCommon\n&gt;5%\nUnlikely to cause severe disease (would be selected against)\n\n\nLow-frequency\n1-5%\nMay contribute to complex traits\n\n\nRare\n0.1-1%\nCandidates for Mendelian disease\n\n\nUltra-rare\n&lt;0.1%\nStrongest candidates for pathogenicity\n\n\n\nWhy frequency matters:\n\nFiltering: Variants common in healthy populations are unlikely to cause severe disease. A variant at 1% frequency in gnomAD cannot plausibly cause a dominant condition affecting 1 in 100,000 people.\nEvolutionary signal: Rare variants have been exposed to less natural selection. Absence from large population databases suggests the variant may be deleterious.\nPopulation specificity: A variant at 5% in one population but absent from another isn’t “rare”—it’s population-specific. Frequency filtering must be ancestry-aware.\n\n\n\nA crucial nuance shapes model interpretation: these catalogs record variants that are compatible with being sampled in the first place. Gene-lethal variants that cause embryonic death or severe childhood disease rarely appear, even when they are biologically informative. Variants causing late-onset conditions (Alzheimer’s risk alleles, adult-onset cancer predisposition) can persist at appreciable frequencies because selection has not had time to remove them. Models trained on population data can only learn from variants present in these catalogs, which means they systematically underrepresent the most severe loss-of-function mutations.\n\n2.2.1 dbSNP and Variant Identifiers\nTwo laboratories sequence the same patient and report their findings to a tumor board. Laboratory A describes a variant using genomic coordinates on GRCh38; Laboratory B uses HGVS nomenclature relative to a specific transcript. Are they discussing the same variant? Without standardized identifiers, this simple question can consume hours of manual reconciliation. The database of Single Nucleotide Polymorphisms (dbSNP) provides the common currency that cuts through this ambiguity: stable identifiers (rsIDs) that enable integration across tools and publications (Sherry et al. 2001).\nWhen a laboratory reports a variant, when a researcher publishes a GWAS finding, and when a clinician queries a pathogenicity database, they need a common language to ensure they are discussing the same genomic position. Modern whole-exome and whole-genome sequencing routinely discovers millions of previously unseen variants per large cohort, but dbSNP identifiers remain the standard way to reference known single nucleotide polymorphisms (SNPs) and link disparate resources. When a GWAS publication reports an association at rs12345, that identifier traces back to dbSNP and enables integration with functional annotations, clinical databases, and population variant catalogs.\n\n\n2.2.2 1000 Genomes and Early Reference Panels\nGenotyping arrays measure only a sparse subset of genomic positions, yet disease-associated variants may lie anywhere in the genome. How can researchers infer variants at unmeasured positions? The answer lies in patterns of co-inheritance: variants that travel together on ancestral chromosome segments can be inferred from neighboring measured positions. This process of imputation depends entirely on having reference panels that capture the haplotype structure of the population being studied.\nThe 1000 Genomes Project provided one of the first widely used multi-population panels for imputation, sampling individuals from African, European, East Asian, South Asian, and admixed American populations (Auton et al. 2015). The resulting haplotype structure underlies many imputation servers and downstream analyses, enabling genotyping arrays with millions of markers to impute tens of millions of untyped variants through linkage disequilibrium (Yun et al. 2021). Although its sample size (approximately 2,500 individuals) is modest by current standards, 1000 Genomes established the template for how to build and distribute multi-population reference panels, and its samples continue to serve as benchmarks for variant calling performance. The role of imputation in GWAS is discussed further in Chapter 3.\n\n\n2.2.3 Genome Aggregation Database (gnomAD)\nDistinguishing genuinely rare variants from sampling gaps requires population-scale catalogs with two properties: sufficient sample size to detect low-frequency variants reliably, and sufficient ancestral diversity to avoid misclassifying variants common in underrepresented populations. A variant at 1% frequency in African populations but absent from European cohorts would be incorrectly flagged as novel by any database sampling only European individuals. The Genome Aggregation Database (gnomAD) addresses both requirements by aggregating exome and genome sequencing data from research and clinical cohorts worldwide into harmonized allele frequency resources spanning hundreds of thousands of individuals (Karczewski et al. 2020).\ngnomAD provides high-resolution allele frequencies stratified by genetic ancestry, enabling population-matched filtering that accounts for variants common in one ancestry but rare in others. This stratification matters because a variant observed at 1% frequency in African populations but absent from European cohorts would be incorrectly flagged as ultra-rare by a model trained predominantly on European data.\ngnomAD also introduced constraint metrics that have become standard features in variant prioritization. The probability of loss-of-function intolerance (pLI) and loss-of-function observed/expected upper bound fraction (LOEUF) summarize how depleted a gene is for protein-truncating variants relative to expectation. These metrics work by computing how many loss-of-function variants we would expect to see in a gene if such variants were neutral (based on the gene’s length, sequence composition, and trinucleotide mutation rates), then comparing this expectation to the number actually observed across hundreds of thousands of individuals. When observed counts are far below expectation, the missing variants must have been removed by natural selection before carriers could reproduce, indicating that the gene cannot tolerate loss of function. Genes essential for viability show far fewer loss-of-function variants than neutral mutation rates would predict; this depletion provides evidence of selective constraint that transfers to variant interpretation. A novel truncating variant in a highly constrained gene warrants more concern than the same variant class in an unconstrained gene. These constraint metrics serve as important features in many variant effect predictors discussed in Section 4.3 and Chapter 17.\nPopulation frequencies from gnomAD provide critical filtering steps in clinical variant interpretation pipelines, as detailed in ?sec-ch26-frequency-filters. The constraint metrics derived from gnomAD form a foundation for variant effect prediction discussed in Section 4.1.1. Allele frequency distributions also inform fine-mapping approaches that assign causal probability to GWAS-associated variants (Section 3.3).\n\n\n\n\n\n\nInterpreting Constraint Metrics\n\n\n\npLI (probability of loss-of-function intolerance) estimates the probability that a gene falls into the class of haploinsufficient genes where loss of one copy causes disease. Scores range from 0 to 1; genes with pLI &gt; 0.9 are considered highly constrained. pLI’s categorical nature (genes are classified as tolerant, intermediate, or intolerant) limits its resolution for genes with intermediate constraint.\nLOEUF (loss-of-function observed/expected upper bound fraction) provides a continuous measure by computing the ratio of observed to expected loss-of-function variants, with a 90% confidence upper bound. Lower LOEUF values indicate stronger constraint. A gene with LOEUF of 0.2 has observed only 20% as many truncating variants as expected under neutral evolution. LOEUF has largely superseded pLI in contemporary analyses due to its continuous scale and more intuitive interpretation.\n\n\n\n\n\n\n\n\nPredict Before Viewing\n\n\n\nBefore examining the constraint metrics table, consider: If a gene has very few loss-of-function variants observed relative to expectation, what does that tell you about the gene’s importance? Would you expect disease-causing genes to be more or less constrained than average?\n\n\n\n\n\nTable 2.2: Comparison of gnomAD constraint metrics and their appropriate applications. LOEUF has largely superseded pLI for contemporary analyses due to its continuous scale.\n\n\n\n\n\n\n\n\n\n\n\nMetric\nRange\nInterpretation\nWhen to Use\n\n\n\n\npLI\n0-1\nProbability gene is haploinsufficient\nCategorical assessment; pLI &gt; 0.9 indicates high constraint\n\n\nLOEUF\n0-2+\nRatio of observed/expected LoF variants\nContinuous ranking; lower = more constrained\n\n\nMissense Z\nAny\nStandard deviations from expected missense count\nMissense constraint; higher = more constrained\n\n\npRec\n0-1\nProbability gene causes recessive disease\nRecessive disease candidate genes\n\n\n\n\n\n\nThese resources are indispensable for filtering common variants in Mendelian disease diagnostics, distinguishing ultra-rare variants from recurrent ones, and providing population genetics priors for deleteriousness scores like CADD (Rentzsch et al. 2019; Schubach et al. 2024). At the same time, they reflect the composition of the cohorts they aggregate: ancestry representation remains uneven despite ongoing efforts, structural variants and repeat expansions are less completely cataloged than SNVs and short indels, and individuals with severe early-onset disease are underrepresented by design. These biases propagate into every model that uses gnomAD frequencies or constraint scores as features.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-biobanks",
    "href": "part_1/p1-ch02-data.html#sec-ch02-biobanks",
    "title": "2  Data Landscape",
    "section": "2.3 Biobanks and GWAS Data",
    "text": "2.3 Biobanks and GWAS Data\nA pharmaceutical company developing a new cardiac drug needs to understand which genetic variants influence drug response. A health system implementing pharmacogenomic testing needs to know which patients are at risk for adverse reactions. A researcher studying the genetics of depression needs cases and controls with standardized phenotyping. None of these questions can be answered by sequencing alone; they require linking genetic variation to phenotypes at scale, across thousands or hundreds of thousands of individuals. Yet assembling such cohorts introduces its own biases: participants must consent, provide samples, and have phenotypes recorded in standardized ways. The populations enrolled in major biobanks reflect patterns of healthcare access, research infrastructure, and historical priorities that do not represent global genetic diversity.\nThe overrepresentation of European-ancestry individuals in most major biobanks creates systematic gaps in variant discovery, effect-size estimation, and polygenic score portability that propagate through downstream analyses (Sirugo, Williams, and Tishkoff 2019). A variant common in West African populations may be absent or rare in European-dominated catalogs, rendering it invisible to association studies and underrepresented in predictive models. This tension between scientific utility and representational equity shapes every biobank-derived resource and is discussed in detail in Chapter 12.\n\n2.3.1 Large Population Cohorts\n\n\n\n\n\n\nStatistical Note: Sample Size Requirements\n\n\n\nDetecting genetic associations with small effect sizes requires very large sample sizes. A variant increasing disease risk by OR = 1.05 requires approximately 50,000 cases and 50,000 controls to detect at genome-wide significance (\\(\\alpha = 5 \\times 10^{-8}\\)). Required sample size scales with the inverse square of effect size: halving the effect size quadruples the required sample. This relationship explains why modern GWAS require hundreds of thousands of participants.\n\n\nA variant that increases heart disease risk by 5% (OR = 1.05) requires approximately 50,000 cases and 50,000 controls to detect reliably at genome-wide significance (\\(\\alpha = 5 \\times 10^{-8}\\)). A variant shifting a continuous trait like blood pressure by 0.05 standard deviations demands even larger samples, often exceeding 100,000 individuals. The fundamental constraint is statistical: detecting small effect sizes against a backdrop of millions of tested variants requires both stringent significance thresholds and massive sample sizes to achieve adequate power. Required sample size scales with the inverse square of effect size, meaning a variant with half the effect requires four times the sample. This relationship explains why genetic discovery accelerated dramatically when biobanks reached the scale of hundreds of thousands of participants. Chapter 27 provides a detailed treatment of these statistical foundations and their implications for clinical risk prediction.\nUK Biobank, with approximately 500,000 participants and deep phenotyping across thousands of traits, has become a dominant resource for methods development and benchmarking (Bycroft et al. 2018). FinnGen leverages Finland’s population history and unified healthcare records for large-scale disease association discovery (Kurki et al. 2023). The All of Us Research Program prioritizes diversity, aiming to enroll one million participants with deliberate oversampling of historically underrepresented groups (null 2019). deCODE genetics has genotyped a substantial fraction of Iceland’s population, enabling unique studies of rare variants and founder effects in a population with detailed genealogical records (Gudbjartsson et al. 2015). Additional resources include the Million Veteran Program, Mexican Biobank, BioBank Japan, China Kadoorie Biobank, and emerging African genomics initiatives such as H3Africa (Sirugo, Williams, and Tishkoff 2019).\n\n\n\n\n\n\nPredict Before Viewing\n\n\n\nBefore looking at the biobank comparison table, predict: Which characteristic likely determines the statistical power to detect rare variant associations versus common variant associations? Which characteristic most affects whether findings will transfer across global populations?\n\n\n\n\n\nTable 2.3: Major biobanks and their characteristics. Note that most resources remain dominated by European-ancestry participants, creating systematic gaps in variant discovery for other populations.\n\n\n\n\n\n\n\n\n\n\n\nBiobank\nSample Size\nKey Strength\nPrimary Ancestry\n\n\n\n\nUK Biobank\n~500,000\nDeep phenotyping, imaging\nEuropean (94%)\n\n\nFinnGen\n~500,000\nFounder population, linked EHR\nFinnish\n\n\nAll of Us\n1,000,000 (target)\nDiversity prioritized\nMulti-ancestry\n\n\ndeCODE\n~200,000\nGenealogical records\nIcelandic\n\n\nBioBank Japan\n~200,000\nEast Asian representation\nJapanese\n\n\nMillion Veteran Program\n~900,000\nDiverse U.S. veterans\nMulti-ancestry\n\n\n\n\n\n\nTogether, these efforts enable genome-wide association studies (GWAS) for thousands of traits, development and evaluation of polygenic scores, and fine-mapping of causal variants and genes (Marees et al. 2018; Mountjoy et al. 2021). From a modeling perspective, they provide the large-scale genotype-phenotype matrices that power architectures ranging from classical linear mixed models to foundation models trained on biobank-scale data. The practical reality for most GWAS and polygenic score methods in Chapter 3 is data from either array genotyping with imputation or whole-exome/whole-genome sequencing with joint calling, as in DeepVariant/GLnexus-style pipelines (Yun et al. 2021).\n\n\n2.3.2 GWAS Summary Statistics\nIndividual-level genotype and phenotype data are powerful but sensitive. Sharing such data across institutions requires complex data use agreements, institutional review board approvals, and secure computing infrastructure. These barriers would slow scientific progress if every analysis required access to raw data. Summary statistics offer an alternative: per-variant effect sizes, standard errors, and p-values that capture the essential association signal without revealing individual genotypes.\nThe GWAS Catalog compiles published results across thousands of traits, while the PGS Catalog provides curated polygenic score weights and metadata for reproducibility (Sollis et al. 2023; Lambert et al. 2021). Frameworks like Open Targets Genetics integrate fine-mapped signals with functional annotations to prioritize candidate causal genes at associated loci (Mountjoy et al. 2021).\nSummary statistics enable meta-analysis across cohorts without sharing individual-level data, transfer of genetic findings to new populations through methods like PRS-CSx, and integration with functional annotations to distinguish causal variants from linked bystanders (Ruan et al. 2022). For deep learning, summary statistics provide a sparse, trait-level view of the genome. This sparsity creates different challenges than the dense labels available in functional genomics, but also different opportunities: the genetic architecture revealed through GWAS informs polygenic score construction (Section 3.5) and indicates which variants and pathways merit follow-up with regulatory models (Chapter 16) and variant effect predictors (Chapter 17).",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-functional",
    "href": "part_1/p1-ch02-data.html#sec-ch02-functional",
    "title": "2  Data Landscape",
    "section": "2.4 Functional Genomics and Regulatory Landscapes",
    "text": "2.4 Functional Genomics and Regulatory Landscapes\nProtein-coding exons constitute roughly 1.5% of the human genome, yet most disease-associated variants from GWAS fall outside coding regions. A massive study identifies 100 loci associated with schizophrenia, but 90 of them lie in non-coding regions with no obvious connection to any gene. This mismatch creates a fundamental interpretability problem: we can identify non-coding loci that harbor disease risk, but we cannot easily determine which base pairs matter, which genes they regulate, or in which cell types they act. Understanding these non-coding variants requires mapping the regulatory logic that governs when, where, and how much each gene is expressed. Functional genomics assays provide this map, identifying transcription factor binding sites, nucleosome positioning, chromatin accessibility, histone modifications, and three-dimensional genome organization across cell types and conditions.\n\n\n\n\n\n\nKey Insight: The Label Source Determines the Model\n\n\n\nFunctional genomics datasets serve a dual role in genomic deep learning. First, they supply the biological vocabulary for interpreting non-coding variants. Second, they provide the training labels for sequence-to-function models. When DeepSEA learns to predict chromatin accessibility from DNA sequence, it compresses into its parameters the regulatory patterns implicit in thousands of ENCODE experiments. The model learns whatever biology ENCODE measured, and inherits whatever ENCODE missed.\n\n\n\n2.4.1 ENCODE, Roadmap, and Related Consortia\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore reading about ENCODE and Roadmap, consider: If you were designing a reference dataset to train models predicting regulatory activity from DNA sequence, what are the key design decisions you’d face? Think about:\n\nWhich cell types or tissues to profile?\nWhich assay types to prioritize?\nHow to balance depth (many experiments in few cell types) versus breadth (few experiments across many cell types)?\n\nKeep these tradeoffs in mind as you read.\n\n\nA single ChIP-seq experiment for one transcription factor in one cell line provides useful signal, but models that learn general regulatory grammar require thousands of such experiments spanning many factors, marks, and cell types. A researcher training a regulatory model on her own laboratory’s data will produce a model that works well in her specific experimental context but fails to generalize. The key insight behind ENCODE and Roadmap was that coordinated experimental campaigns, with standardized methods and quality control, could create reference datasets serving the entire field.\nThe Encyclopedia of DNA Elements (ENCODE) and Roadmap Epigenomics consortia designed coordinated experimental campaigns that profiled transcription factor binding (ChIP-seq), histone modifications, chromatin accessibility (DNase-seq, ATAC-seq), and chromatin conformation (Hi-C) across cell lines and primary tissues (Kagda et al. 2025; Kundaje et al. 2015). Gene Expression Omnibus (GEO) archives these and many other functional genomics datasets with standardized metadata (Edgar, Domrachev, and Lash 2002).\nThe significance of these consortia lies less in any individual experiment than in the scale and standardization they provide. By generating hundreds of assays across dozens of cell types with consistent protocols, ENCODE and Roadmap created canonical reference datasets that define the regulatory landscape for the cell types they profiled. These resources enabled multiple generations of regulatory models. DeepSEA (Section 6.2) pioneered multi-task learning on ENCODE chromatin accessibility and transcription factor binding, where each prediction task corresponds to a ChIP-seq or accessibility experiment. Enformer (Section 16.2) extended this paradigm with transformer attention mechanisms and longer context windows. The progression from convolutional to attention-based architectures reflects both the richness of ENCODE data and its limitations: models trained on these resources inherit ENCODE’s choices about which cell types, factors, and experimental conditions merit inclusion.\n\n\n\n\n\n\nENCODE and Roadmap Epigenomics data coverage matrix\n\n\n\n\nFigure 2.3: Coverage of the ENCODE and Roadmap Epigenomics data compendium. Each row represents a cell type or tissue; each column represents an assay type (chromatin accessibility, histone modifications, transcription factor binding, gene expression). Color intensity indicates data availability, from absent (white) to comprehensively profiled (dark blue). Tier 1 cell lines (K562, GM12878, HepG2) show near-complete coverage across assay types, while many disease-relevant primary tissues remain sparsely profiled. Models trained on this compendium inherit its coverage biases, performing best for well-characterized cell types and potentially failing for undersampled contexts.\n\n\n\n\n\n\n\n\n\nKnowledge Check: Functional Genomics Coverage\n\n\n\nA model trained to predict chromatin accessibility performs excellently on K562 cells (a leukemia cell line) but poorly on primary pancreatic beta cells. Without looking back, can you explain why this might occur?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nK562 is one of the most heavily profiled cell types in ENCODE with extensive training data. Pancreatic beta cells, despite their importance in diabetes, have sparse representation in training compendia. The model learned patterns specific to well-represented cell types and cannot generalize to cell types absent from training.\n\n\n\n\n\n\n\n2.4.2 Cistrome Data Browser\nYou want to train a model predicting binding sites for a transcription factor implicated in your disease of interest. ENCODE has extensive data, but not for your factor. A literature search reveals that fifteen laboratories have published ChIP-seq experiments for this factor over the past decade, but every dataset uses different peak callers, different quality thresholds, and different normalization schemes. Comparing them directly would be comparing apples to oranges. How do you build a unified training set from this scattered evidence?\nThe Cistrome Data Browser solves exactly this problem by aggregating thousands of human and mouse ChIP-seq and chromatin accessibility datasets from ENCODE, Roadmap, GEO, and individual publications into a uniformly reprocessed repository (Zheng et al. 2019). All datasets pass through standardized quality control and peak calling, enabling comparisons across experiments originally generated with different protocols.\nCistrome provides uniform peak calls, signal tracks, and metadata for cell type, factor, and experimental conditions. The uniform reprocessing is critical because raw ChIP-seq data from different laboratories cannot be directly compared: different peak callers use different statistical thresholds, different normalization schemes produce different signal intensities, and different quality filters exclude different artifacts. By applying identical computational pipelines to all datasets, Cistrome makes experiments comparable even when they were not designed for cross-study analysis. The tradeoff is heterogeneity: while reprocessing harmonizes computational steps, the underlying experiments vary in sample preparation, antibody quality, sequencing depth, and experimental design. Cistrome expands coverage at the cost of the tight experimental control found in the primary consortia, a tradeoff that matters when models learn from noisy or inconsistent labels.\n\n\n\n\n\n\nPredict Before Viewing\n\n\n\nConsider the tradeoff between data quality and data coverage. If you needed transcription factor binding data for a rare cell type not in ENCODE, where would you look? What quality concerns might you need to address?\n\n\n\n\n\nTable 2.4: Comparison of functional genomics data sources. Resources vary in the tradeoff between standardization and coverage.\n\n\n\n\n\n\n\n\n\n\n\nResource\nScope\nQuality Control\nKey Tradeoff\n\n\n\n\nENCODE\nConsortium-generated\nStringent, standardized\nLimited cell type coverage\n\n\nRoadmap Epigenomics\nPrimary tissues\nStandardized protocols\nFewer factors profiled\n\n\nCistrome\nAggregated public data\nUniform reprocessing\nVariable original quality\n\n\nGEO\nAll submitted experiments\nMinimal curation\nHeterogeneous methods\n\n\n\n\n\n\n\n\n2.4.3 From Assays to Training Labels\nHere is the central challenge for regulatory genomics: a ChIP-seq experiment tells you where a transcription factor binds in one cell type under one condition, but a machine learning model needs millions of labeled examples to learn DNA sequence patterns that predict binding in general. How do you transform scattered experimental measurements into the systematic training labels that deep learning requires? This conversion from assays to labels is where the biology meets the algorithm, and where subtle choices about data processing determine what models can and cannot learn.\nSequence-to-function models like DeepSEA (see Chapter 16) draw training labels from ENCODE, Roadmap, and Cistrome-style datasets: each genomic window is associated with binary or quantitative signals indicating transcription factor binding, histone modifications, or chromatin accessibility across hundreds of assays and cell types (Zhou and Troyanskaya 2015; Zhou et al. 2018).\nThe quality, coverage, and biases of these labels directly constrain what models can learn. Cell types absent from the training compendium cannot be predicted reliably. Factors with few high-quality ChIP-seq experiments will have noisier labels. Systematic differences between assay types (binary peak calls versus quantitative signal tracks) shape whether models learn to predict occupancy, accessibility, or something in between. These considerations become central when examining model architectures and training strategies in Chapter 16.\n\n\n2.4.4 Deep Mutational Scanning and Multiplexed Variant Assays\nPopulation variant catalogs tell us which variants survive in healthy individuals, but they cannot tell us what happens when a specific amino acid is changed to every possible alternative. Functional genomics experiments reveal where the genome is active, but they do not directly measure the consequence of each possible mutation. Deep mutational scanning (DMS) fills this gap by measuring the fitness or functional impact of thousands of protein or regulatory variants in a single experiment.\nThese assays systematically introduce mutations (often approaching saturation mutagenesis for a protein domain or regulatory element), subject the resulting library to selection or screening, and use sequencing to quantify the representation of each variant before and after selection. The result is dense, quantitative measurements of variant effects under controlled conditions. Benchmarks such as ProteinGym compile large DMS datasets across proteins to evaluate variant effect predictors. TraitGym curates multiplexed reporter assays and other high-throughput readouts of regulatory variant effects (Notin et al. 2023; Benegas, Eraslan, and Song 2025).\nThese resources sit at the interface between genomic and protein-level modeling. Where gnomAD and biobanks catalog sparse, naturally occurring variation, DMS datasets offer dense, quantitative functional measurements across systematic variant libraries that test most or all possible substitutions. DMS data differ fundamentally from population catalogs: they measure functional impact directly under controlled conditions rather than inferring it from population survival. Protein sequence models (Chapter 15) and regulatory variant predictors (Chapter 17) use these DMS-style datasets as key benchmarks and training sources.\n\n\n\n\n\n\nPredict Before Viewing\n\n\n\nThree different data sources can tell you about variant effects, but each measures something different. Before viewing the table, predict: Which source would best answer “Has this variant been seen in healthy people?” versus “Does this variant disrupt protein function in a lab assay?” versus “Has a clinical geneticist classified this variant as disease-causing?”\n\n\n\n\n\nTable 2.5: Comparison of variant-level data sources. Each provides different evidence types with complementary strengths and limitations.\n\n\n\n\n\n\n\n\n\n\n\nData Type\nMeasurement\nCoverage\nKey Limitation\n\n\n\n\nPopulation catalogs (gnomAD)\nPresence in healthy individuals\nNatural variants only\nSevere variants underrepresented\n\n\nDeep mutational scanning\nDirect functional impact\nSaturation mutagenesis\nOne protein/element at a time\n\n\nClinical databases (ClinVar)\nExpert pathogenicity assessment\nClinically observed variants\nAscertainment toward disease genes",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-expression",
    "href": "part_1/p1-ch02-data.html#sec-ch02-expression",
    "title": "2  Data Landscape",
    "section": "2.5 Expression and eQTL Resources",
    "text": "2.5 Expression and eQTL Resources\nFunctional genomics assays reveal where transcription factors bind and which chromatin regions are accessible, but they do not directly answer the downstream question: does regulatory activity actually change how much RNA a gene produces? A transcription factor may bind a genomic region without altering expression of nearby genes; an accessible chromatin region may not contain active regulatory elements. Regulatory binding and gene expression exist in a many-to-many relationship that cannot be resolved by either measurement alone. Expression datasets complete this link, measuring transcript abundance across tissues, cell types, and genetic backgrounds.\nConnecting non-coding GWAS variants to their effector genes requires mechanistic hypotheses: some indication of which gene a regulatory variant actually regulates. Expression quantitative trait loci (eQTLs) provide exactly this connection, identifying genetic variants statistically associated with transcript-level changes. When a GWAS signal colocalizes with an eQTL for a nearby gene in disease-relevant tissue, that gene becomes a candidate effector. For model training, expression data provide quantitative labels that integrate across many regulatory inputs converging on a single promoter.\n\n2.5.1 Bulk Expression Atlases\nA GWAS identifies a locus associated with coronary artery disease in a non-coding region. Dozens of genes lie within the associated interval. Which one mediates the disease risk? If the lead variant also associates with expression of a nearby gene specifically in arterial endothelial cells, that gene becomes the prime candidate. Without tissue-specific expression data linked to genotypes, this inference is impossible.\nThe Genotype-Tissue Expression (GTEx) consortium provides the most comprehensive resource linking genetic variation to gene expression across human tissues, with RNA-seq profiles from 948 post-mortem donors across 54 tissues (THE GTEX CONSORTIUM 2020). GTEx established foundational insights that inform regulatory genomics models: most genes harbor tissue-specific eQTLs, regulatory variants typically act in cis over distances of hundreds of kilobases, and expression variation explains a meaningful fraction of complex trait heritability.\nGTEx underlies expression prediction models such as PrediXcan, which trains tissue-specific models to impute gene expression from genotypes alone (Gamazon et al. 2015). Transcriptome-wide association studies (TWAS) extend this idea to associate imputed expression with phenotypes (Gusev et al. 2016). Colocalization methods ask whether a GWAS signal and an eQTL share the same causal variant, providing evidence that the associated gene mediates the trait effect.\nThe GTEx design has limitations worth acknowledging. Post-mortem collection introduces agonal stress artifacts that may not reflect living tissue biology. Sample sizes vary considerably across tissues (hundreds for some, dozens for others), affecting statistical power. Some disease-relevant tissues, such as pancreatic islets or specific brain subregions, remain undersampled. Complementary resources like the eQTLGen Consortium aggregate eQTL results from blood across much larger sample sizes, trading tissue diversity for statistical power (Võsa et al. 2021).\n\n\n2.5.2 Single-Cell and Context-Specific Expression\nBulk RNA-seq averages expression across all cells in a tissue sample, obscuring the cell-type-specific programs that often mediate disease biology. A bulk eQTL in brain tissue might reflect astrocytes, neurons, microglia, or oligodendrocytes; the causal cell type matters for understanding mechanism. This averaging creates a fundamental resolution problem: variants may have strong effects in rare cell populations that are diluted to undetectability when mixed with other cell types.\nSingle-cell RNA-seq resolves this heterogeneity, identifying expression signatures for individual cell types, rare populations, and transitional states. Large-scale efforts including the Human Cell Atlas and Tabula Sapiens are building reference atlases that catalog cell types across organs and developmental stages (Regev et al. 2017; THE TABULA SAPIENS CONSORTIUM 2022). For variant interpretation, single-cell data enable cell-type-specific eQTL mapping, revealing that a variant may influence expression in one cell type but not others within the same tissue. Spatial transcriptomics adds anatomical context, preserving tissue architecture while measuring gene expression.\nThese technologies introduce computational challenges: sparsity from dropout effects, batch variation across samples and technologies, and massive scale with millions of cells per study. They also offer an increasingly fine-grained view of the link between genotype, regulatory state, and cellular phenotype. Multi-omics integration (Chapter 22) and systems-level modeling draw heavily on single-cell and spatial resources.\n\n\n\n\n\n\nPredict Before Viewing\n\n\n\nIf you wanted to know which cell type in the brain expresses a candidate gene for a neurological disorder, which technology would be most appropriate? What would you gain and lose compared to bulk RNA-seq?\n\n\n\n\n\nTable 2.6: Expression profiling technologies and their tradeoffs. Higher resolution technologies introduce new computational challenges.\n\n\n\n\n\n\n\n\n\n\n\nTechnology\nResolution\nScale\nKey Challenge\n\n\n\n\nBulk RNA-seq\nTissue average\n100s of samples\nCell type confounding\n\n\nSingle-cell RNA-seq\nIndividual cells\nMillions of cells\nSparsity, dropout\n\n\nSpatial transcriptomics\nCells in tissue context\n1000s of spots\nLower gene coverage",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-protein-databases",
    "href": "part_1/p1-ch02-data.html#sec-ch02-protein-databases",
    "title": "2  Data Landscape",
    "section": "2.6 Protein Databases",
    "text": "2.6 Protein Databases\nA researcher developing a variant effect predictor needs to understand whether a missense mutation disrupts protein function. Sequence conservation across species provides one signal, but structural context adds another dimension: a mutation at an active site or protein-protein interface may be more disruptive than one in a flexible loop. Training models that leverage this structural intuition requires databases cataloging both protein sequences and their three-dimensional structures. These resources have grown from modest beginnings (a few hundred structures in the early PDB) to comprehensive atlases covering essentially all known protein sequences with either experimental or predicted structures.\nProtein databases serve multiple roles in genomic deep learning. Sequence databases provide the training corpora for protein language models (Chapter 15). Structure databases supply the labels for structure prediction and the geometric constraints that inform structure-aware variant effect predictors. The intersection of sequence and structure enables models that learn evolutionary patterns from millions of sequences while grounding predictions in physical reality.\n\n2.6.1 Sequence Databases\nProtein language models learn the grammar of evolution by reading billions of protein sequences, but where do those sequences come from? The answer matters because the training corpus defines what the model can learn. A protein family absent from the database is invisible to the model. A protein family represented by thousands of diverse homologs will be understood deeply. The difference between a model that predicts variant effects accurately and one that fails silently often traces back to whether the relevant protein families were well-represented in training.\nA protein language model learns patterns from the evolutionary record encoded in sequence databases. The depth and diversity of these databases directly constrain what patterns can be learned: a model trained on bacterial sequences alone will miss eukaryotic-specific motifs, while one trained only on well-characterized model organisms will underrepresent the functional diversity of environmental samples.\nUniProt provides the foundational sequence resource, integrating manually curated entries (Swiss-Prot) with computationally annotated sequences (TrEMBL) into a comprehensive protein knowledgebase (uniprot_2023?). The UniRef clusters organize these sequences at different identity thresholds (UniRef100, UniRef90, UniRef50), enabling efficient sampling strategies for model training that balance coverage against redundancy (suzek_uniref_2015?). UniRef50, which clusters sequences at 50% identity, reduces the database size substantially while preserving sequence diversity, making it practical for training large models.\nThe Big Fantastic Database (BFD) extends beyond curated sequences to include metagenomic data, capturing protein diversity from environmental samples that have never been cultured in laboratories (steinegger_bfd_2019?). BFD contains over 2.5 billion protein sequences, an order of magnitude larger than UniProt, representing the vast majority of known protein sequence space. This scale proved critical for training models like ESM-2, where exposure to diverse evolutionary patterns during pretraining improved downstream performance on variant effect prediction and other tasks. However, metagenomic sequences carry higher annotation uncertainty and may include fragments, chimeras, and other artifacts that curated databases exclude.\n\n\n\n\n\n\nStop and Think: Database Tradeoffs\n\n\n\nConsider training a protein language model. You have access to UniProt (curated, ~250 million sequences) and BFD (metagenomic, ~2.5 billion sequences). What are the arguments for using each? What might you lose by using only one?\nUniProt advantages: higher quality annotations, fewer artifacts, established functional knowledge. BFD advantages: 10x more sequences, broader evolutionary coverage, rare protein families. Using only UniProt might miss patterns in understudied protein families; using only BFD might introduce noise from low-quality sequences. Modern approaches often combine both, using BFD for pretraining diversity and UniProt for downstream task fine-tuning.\n\n\n\n\n2.6.2 Structure Databases\nWhen you need to understand why a missense variant disrupts protein function, sequence alone often cannot answer the question. Is the affected residue buried in the hydrophobic core where any substitution would destabilize folding? Does it sit at a protein-protein interface where the mutation would disrupt binding? Is it part of the catalytic site where even conservative changes eliminate activity? Answering these questions requires knowing the three-dimensional structure, and structure databases provide this critical context for variant interpretation.\nExperimental protein structures anchor computational predictions in physical reality. The Protein Data Bank (PDB) archives structures determined by X-ray crystallography, NMR spectroscopy, and cryo-electron microscopy, providing the definitive reference for protein three-dimensional organization (berman_pdb_2000?). As of 2024, the PDB contains over 220,000 structures, though coverage remains uneven: well-studied proteins in model organisms are heavily represented, while the structures of most human proteins remain experimentally undetermined.\nThe AlphaFold Protein Structure Database transformed this landscape by providing predicted structures for essentially all proteins in UniProt (varadi_alphafold_2022?). These predictions, generated by AlphaFold2 (Jumper et al. 2021), achieve accuracy competitive with experimental determination for well-folded domains. The database democratized structural biology, enabling researchers to access structural hypotheses for any protein of interest without experimental effort. For variant interpretation, AlphaFold structures provide context that was previously available only for the small fraction of proteins with experimental structures.\nHowever, predicted structures carry important caveats. AlphaFold provides confidence scores (pLDDT) that indicate prediction reliability, with disordered regions and novel folds receiving lower scores. Models trained on AlphaFold structures inherit both the power of comprehensive coverage and the uncertainty of computational prediction. The distinction between experimental and predicted structures matters when using structural features for clinical variant interpretation, where the evidentiary standards are higher than for research applications. Structure-aware variant effect prediction is discussed in ?sec-ch12-structure-vep, while the role of protein language models in genomic foundation models appears in Chapter 15.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-phenotypes",
    "href": "part_1/p1-ch02-data.html#sec-ch02-phenotypes",
    "title": "2  Data Landscape",
    "section": "2.7 Phenotype Definition and Data Quality",
    "text": "2.7 Phenotype Definition and Data Quality\nEvery model in genomics learns from labels, but phenotype labels carry their own biases distinct from variant annotations or functional genomics measurements. A GWAS for type 2 diabetes depends entirely on how diabetes is defined: by self-report, ICD-10 codes, hemoglobin A1c thresholds, medication records, or clinical adjudication. Each definition captures a different slice of the underlying biology. Self-report misses undiagnosed cases. ICD codes reflect billing practices as much as clinical reality. Laboratory thresholds impose sharp boundaries on continuous metabolic dysregulation. The “same” phenotype defined differently yields different genetic architectures, different effect sizes, and different polygenic score performance.\nThis sensitivity to phenotype definition compounds as biobanks scale. UK Biobank’s 500,000 participants enable discovery at unprecedented statistical power, but that power is limited by the precision of the phenotypes being tested. A GWAS with millions of participants but noisy case definitions may have less effective power than a smaller study with carefully adjudicated outcomes. The trade-off between sample size and phenotype quality pervades modern statistical genetics, and understanding its contours is essential for interpreting what models trained on biobank data actually learn.\nPhenotype quality issues create systematic confounding in GWAS (Section 3.8.3) and clinical risk prediction models (Section 27.4). Deep phenotyping approaches that extract richer representations from EHR data are examined in Section 3.8.4 for GWAS contexts and Section 27.3 for clinical deployment.”\n\n2.7.1 Problem of Binary Disease Definitions\nMost GWAS treat disease as binary: case or control, affected or unaffected. This simplification enables standard statistical machinery but discards information about disease severity, age of onset, trajectory, and subtype. Two patients both labeled “coronary artery disease” may differ in clinically meaningful ways: one experienced an acute myocardial infarction at age 45, the other underwent elective stenting for stable angina at 72. Collapsing this heterogeneity into a single binary label forces genetic analyses to identify variants associated with an artificial composite rather than biologically coherent disease entities.\nThe consequences extend beyond reduced statistical power. Phenotype heterogeneity can induce genetic heterogeneity, where different genetic variants predispose to different subtypes that have been artificially combined. A GWAS for “depression” that includes melancholic depression, atypical depression, and adjustment disorders will identify variants associated with the mixture rather than any specific syndrome. The resulting polygenic scores predict the mixture, potentially missing stronger associations with homogeneous subtypes and providing weaker stratification than would be achievable with cleaner phenotype definitions.\nClinical endpoints also differ in their proximity to genetic effects. Biomarkers such as LDL cholesterol or blood pressure lie closer to gene function than clinical outcomes such as myocardial infarction or stroke, which require the biomarker dysregulation to persist, interact with environmental factors, and culminate in tissue damage. Genetic effects are typically larger and more readily detected for intermediate phenotypes than for distal clinical outcomes. This motivates strategies that analyze biomarkers as outcomes in their own right, then connect genetic effects on biomarkers to disease risk through Mendelian randomization or mediation analysis.\n\n\n2.7.2 Electronic Health Record Quality and Completeness\nElectronic health records promise comprehensive phenotyping at scale: every diagnosis, procedure, medication, and laboratory result captured in structured or semi-structured form. In practice, EHR data are messy, incomplete, and shaped by processes far removed from biology. A diagnosis code reflects not just what the patient has but what the clinician chose to document, what the billing system required, and what the coding specialist interpreted. The same clinical presentation may receive different codes depending on the setting, the clinician’s documentation habits, and institutional coding policies.\nMissing data pervades EHR phenotyping. Laboratory values are measured when clinically indicated, not at random, creating informative missingness where the absence of a measurement conveys information about the patient’s health status. Patients who transfer between health systems appear to have incomplete histories. Conditions managed by specialists outside the health system may be entirely absent from the record. These gaps are not random but systematically related to patient characteristics, healthcare access, and disease severity in ways that can bias genetic analyses.\nTemporal dynamics add further complexity. Disease onset rarely corresponds to diagnosis date; patients carry pathology for years before clinical recognition. Medication records indicate prescriptions but not adherence. Procedure dates capture interventions but not the progression of disease that motivated them. Time-to-event analyses must grapple with left truncation (patients entering observation after disease onset), interval censoring (disease status observed only at discrete timepoints), and the distinction between incident and prevalent cases that confounds cross-sectional analyses.\n\n\n\n\n\n\nPractical Guidance: Working with EHR Phenotypes\n\n\n\nWhen using EHR-derived phenotypes for genetic analysis:\n\nSpecify inclusion/exclusion criteria precisely. Which ICD codes define cases? What time windows apply?\nDocument missingness patterns. Is absence of a code evidence of absence, or missing data?\nConsider ascertainment bias. Who gets tested, screened, or referred? This shapes who appears as cases.\nCheck for temporal artifacts. Did coding practices change during your study period?\nValidate against chart review. Even small validation studies (n=100) reveal systematic misclassification.\n\n\n\n\n\n2.7.3 Coding Inconsistencies and Label Noise\nThe International Classification of Diseases provides a standardized vocabulary, but standardized vocabulary does not guarantee standardized application. ICD-10 contains over 70,000 codes, and clinical coders must choose among them based on physician documentation that may be ambiguous, incomplete, or inconsistent. Studies comparing chart review to coded diagnoses find substantial discordance: some patients with clear clinical disease lack corresponding codes, while others have codes without supporting clinical evidence (birman-deych_icd_2005?; quan_coding_2005?).\nCode usage also evolves over time. The transition from ICD-9 to ICD-10 in the United States (October 2015) created discontinuities in phenotype definitions built on specific codes. Clinical practice changes alter what conditions are tested for, diagnosed, and coded. COVID-19’s emergence created entirely new codes and altered coding patterns for respiratory illness more broadly. These temporal discontinuities matter for genetic studies because they create apparent phenotype changes that have nothing to do with biology: a patient may appear to develop a new condition simply because the coding system changed, or disease prevalence may appear to increase because a screening program was introduced. Longitudinal analyses spanning coding transitions or practice changes must account for these artifacts or risk confusing temporal trends in coding with temporal trends in disease.\nLabel noise from coding errors propagates into every downstream analysis. A phenotype definition with 10% misclassification (5% false positives, 5% false negatives) substantially attenuates genetic effect sizes and reduces GWAS power. For rare diseases where cases are precious, false positives among controls matter less than false negatives among cases, which dilute the genetic signal. For common diseases where controls are presumed healthy, false negatives among controls (undiagnosed cases) similarly attenuate associations. The magnitude of this attenuation depends on disease prevalence, misclassification rates, and their correlation with genetic risk.\n\n\n2.7.4 Deep Phenotyping Approaches\nRecognition of these limitations has motivated deep phenotyping strategies that move beyond binary disease definitions. Quantitative phenotypes, when available, preserve information that binary thresholds discard. Rather than dichotomizing blood pressure into hypertensive versus normotensive, analyzing systolic and diastolic pressure as continuous traits captures the full distribution of genetic effects. Similarly, imaging-derived phenotypes (cardiac MRI measurements, brain volume, bone density) provide precise quantitative endpoints with higher heritability than clinical disease outcomes.\nPhenotype refinement uses clinical features to identify more homogeneous subgroups. Clustering patients by age of onset, comorbidity patterns, or biomarker profiles can reveal subtypes with distinct genetic architectures. Type 2 diabetes, for instance, has been decomposed into clusters defined by age, BMI, insulin resistance, and beta-cell function, with different clusters showing different genetic associations and different disease trajectories (ahlqvist_t2d-clusters_2018?). Such stratification requires sufficient clinical data to define subgroups, limiting its application to well-phenotyped cohorts.\nA more radical approach abandons expert-specified phenotype criteria entirely. Instead of encoding clinical knowledge through hierarchical ontologies, embedding methods learn vector representations of clinical concepts from co-occurrence patterns in EHR data. Word2Vec models trained on ICD-10 code sequences position clinically related codes near each other in this learned space; codes that co-occur in patient records cluster together regardless of their position in the ICD ontology. Large language models can generate similar phenotype embeddings from textual descriptions, capturing semantic relationships encoded in clinical language.\nThese embeddings can serve as phenotypes themselves. Xu et al. demonstrated that GWAS conducted on EHR-embedding dimensions identified heritable components of clinical phenotype structure, with genetic correlations revealing coherent trait clusters such as cardiovascular disease risk factors (Xu et al. 2025). The embeddings capture phenotypic relationships that binary disease definitions obscure, potentially improving the power to detect genetic associations and the transferability of polygenic scores across related traits.\n\n\n2.7.5 Impact on Downstream Modeling\nPhenotype quality constraints propagate through every analysis built on biobank data, creating systematic confounding that affects both GWAS (Section 3.8.3) and clinical risk prediction models (Section 27.4). Polygenic scores trained on noisy phenotypes learn to predict the noise alongside the signal, potentially inheriting coding artifacts, temporal discontinuities, and population-specific documentation practices. Transfer learning from one biobank to another may fail not because the underlying genetic architecture differs but because the phenotype definitions differ in ways that alter what the model learned.\nFoundation models face analogous challenges. A model that learns associations between genetic variants and EHR-derived phenotypes absorbs whatever systematic distortions those phenotypes contain. If a diagnosis is more likely to be coded in patients who receive specialist care, the model learns a genetic signature for healthcare access as much as for disease biology. If a biomarker is measured only in symptomatic patients, the model learns from a biased sample that may not represent the population distribution. Deep phenotyping approaches that extract richer representations from EHR data offer partial solutions, examined in Section 3.8.4 for GWAS contexts and Section 27.3 for clinical deployment.\nThese considerations motivate careful phenotype documentation in model development. Specifying exactly how a phenotype was defined, which codes or criteria were applied, what exclusions were made, and how temporal boundaries were established enables assessment of whether findings will generalize to settings with different definitions. The goal is not perfect phenotyping, which remains unattainable, but transparent phenotyping that allows downstream users to understand what the model actually learned and where its assumptions may break down.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-clinical",
    "href": "part_1/p1-ch02-data.html#sec-ch02-clinical",
    "title": "2  Data Landscape",
    "section": "2.8 Variant Interpretation Databases and Clinical Labels",
    "text": "2.8 Variant Interpretation Databases and Clinical Labels\nA family receives whole-exome sequencing results for their child with developmental delay. The laboratory report lists 50 rare variants in genes associated with neurodevelopmental disorders. For each variant, the clinical team must answer: is this the cause? Allele frequencies tell us what variants survive in healthy populations, and functional genomics data reveal where the genome is biochemically active, but neither directly answers this question. That determination requires integrating multiple lines of evidence (family segregation, functional assays, computational predictions, phenotypic observations) into a structured framework that can be applied consistently.\nClinical variant interpretation databases aggregate these assessments from laboratories, expert panels, and research groups. These databases have become critical infrastructure for both clinical genomics and computational method development, providing labels that inform diagnostic decisions and serve as training data for machine learning models. Their labels carry biases and circularity that propagate through any analysis built on them, yet no viable alternative exists for large-scale model training and evaluation.\n\n2.8.1 ClinVar and Clinical Assertions\nA clinical laboratory sequences a patient with suspected hereditary cancer syndrome and identifies a missense variant in BRCA2. Before returning results, the laboratory searches ClinVar and finds that three other laboratories have evaluated this variant: two classified it as likely pathogenic, one as a variant of uncertain significance. How should this conflicting evidence inform the final report? ClinVar aggregates assertions of variant pathogenicity from clinical laboratories and researchers worldwide, making it the central clearinghouse for clinical variant interpretations (Landrum et al. 2018).\nClinVar provides standardized classifications following ACMG/AMP guidelines (pathogenic, likely pathogenic, benign, likely benign, variant of uncertain significance) that are central to diagnostic pipelines and to benchmarking variant effect predictors. It has become the de facto reference for variant pathogenicity labels, but its contents reflect systematic biases that affect any downstream use. These biases operate at multiple levels and warrant careful consideration.\n\n\n\n\n\n\nKey Insight: The Circularity Problem\n\n\n\nClinVar serves simultaneously as (1) a source of training labels for variant effect predictors and (2) a benchmark for evaluating those same predictors. This dual role creates circularity: if ClinVar classifications increasingly incorporate computational scores, and those scores were trained on earlier ClinVar classifications, the benchmark becomes contaminated. A model may appear to perform well simply by reproducing the computational component of its training labels rather than learning independent biological signal.\n\n\nSubmission heterogeneity poses a fundamental challenge. Annotations come from diverse submitters, including diagnostic laboratories, research groups, expert panels, and database exports. Submitters apply varying evidentiary standards; some provide detailed supporting evidence while others offer only assertions. Conflicting interpretations are common, particularly for variants of uncertain significance.\nClassifications evolve as evidence accumulates. A variant classified as VUS in 2018 may be reclassified as likely pathogenic by 2023 based on new functional studies or additional patient observations. ClinVar releases monthly snapshots rather than maintaining formal version control, so models trained on older releases may learn outdated classifications that have since been revised. Specifying the exact ClinVar release date is essential for reproducibility.\nAncestry and gene coverage biases create uneven representation. Variants in well-studied populations (particularly European ancestry) and well-characterized disease genes are heavily overrepresented. Variants from underrepresented populations are more likely to remain classified as VUS due to insufficient evidence. This creates feedback loops: predictive models perform better on European-ancestry variants because training data is richer, reinforcing the disparity (Landrum et al. 2018).\nClinical assertions in ClinVar become training labels for variant effect predictors like CADD (Section 4.3) and evaluation benchmarks for foundation model approaches (Chapter 17). The role of ClinVar in ACMG-AMP variant classification workflows is detailed in ?sec-ch26-acmg-amp. Calibration of computational scores to ClinVar pathogenicity assertions is examined in ?sec-ch14-acmg-mapping, while systematic evaluation of ClinVar as a benchmark resource appears in Section 11.3.1.\nCircularity with computational predictors represents a subtle but important concern. Clinical submissions increasingly incorporate computational scores like CADD, REVEL, and AlphaMissense as supporting evidence for pathogenicity classification. When these same ClinVar labels are then used to train or evaluate computational predictors, circularity emerges (Schubach et al. 2024). If a laboratory used a high CADD score as supporting evidence for classifying a variant as likely pathogenic, and that variant later appears as a positive label in ClinVar, models trained on ClinVar may partly learn to reproduce CADD itself rather than discovering independent signal. This circularity operates at two levels: evaluation circularity (when models are assessed on benchmarks influenced by the model’s own predictions) and training circularity (when features used in training derive from the same underlying information as the labels). Both forms inflate apparent performance without demonstrating genuine predictive power.\nVariants of uncertain significance constitute the majority of rare variant classifications, reflecting genuinely limited evidence. These variants are both targets for predictive modeling (can computational methods resolve uncertainty?) and potential pitfalls (models trained only on confidently classified variants may not generalize to VUS with different characteristics).\nDespite these limitations, ClinVar remains invaluable. The key is using it appropriately: recognizing biases when training models, accounting for version differences when comparing studies, stratifying performance by ancestry and gene coverage, and treating computational predictions as one line of evidence rather than definitive classifications.\n\n\n\n\n\n\n\n\nClassification distribution showing VUS dominance\n\n\n\n\n\n\n\nGene coverage showing uneven classification density\n\n\n\n\n\n\n\nClassification evolution over time\n\n\n\n\n\n\nFigure 2.4: The ClinVar variant interpretation landscape. (A) Distribution of clinical significance classifications, highlighting that variants of uncertain significance dominate the database. (B) Classification density across genes, showing that well-studied genes have orders of magnitude more classified variants than the long tail of rarely-studied genes. (C) Temporal evolution of classifications, illustrating how variant interpretations change as evidence accumulates. Together, these panels reveal why computational variant interpretation remains challenging: most variants lack confident classifications, coverage is highly uneven across genes, and ground truth labels are themselves unstable over time.\n\n\n\n\n\n2.8.2 Complementary Clinical Databases\nYou search ClinVar for a variant in a rare disease gene and find nothing. Does this mean the variant is novel? Not necessarily. A paper published last month may have reported this exact variant in a patient with your phenotype, but the finding has not yet been submitted to ClinVar. Alternatively, a locus-specific database maintained by experts in this disease may have curated the variant years ago with detailed functional evidence that never made it to ClinVar. Knowing where else to look, and what each resource captures, can be the difference between a missed diagnosis and a solved case.\nClinVar’s open-access model and broad submission base make it the most widely used resource, but it is not the only source of clinical variant interpretations. The Human Gene Mutation Database (HGMD) maintains a curated collection of disease-causing mutations compiled from the published literature, with particular depth in rare Mendelian disorders (Stenson et al. 2017). HGMD’s professional version includes variants not yet publicly released, and its curation emphasizes literature-reported pathogenic variants rather than the full spectrum of classifications in ClinVar. The Leiden Open Variation Database (LOVD) takes a gene-centric approach, with individual databases maintained by gene experts who curate variants according to locus-specific knowledge (Fokkema et al. 2011). LOVD instances often capture variants and functional evidence specific to particular disease communities that may not appear in broader databases.\nThese resources complement ClinVar in important ways: HGMD provides literature-derived pathogenic variants that may precede ClinVar submissions, while LOVD captures expert knowledge from disease-specific research communities. For model development and benchmarking, awareness of these alternative sources matters because training exclusively on ClinVar may miss variants documented elsewhere, and apparent novel predictions may simply reflect incomplete training data rather than genuine generalization.\n\n\n\n\n\n\nPredict Before Viewing\n\n\n\nDifferent clinical databases serve different purposes. Before viewing the table, consider: If you needed the most up-to-date classification from expert panels for gene-disease validity, which resource would you consult? If you needed literature-reported pathogenic variants for a rare disease, where would you look?\n\n\n\n\n\nTable 2.7: Clinical variant databases and their complementary roles. Training on any single database may miss variants documented elsewhere.\n\n\n\n\n\n\n\n\n\n\n\nDatabase\nContent Focus\nAccess Model\nStrengths\n\n\n\n\nClinVar\nAll classifications\nOpen access\nBroad coverage, standardized format\n\n\nHGMD\nLiterature-derived pathogenic\nSubscription\nEarly capture of published variants\n\n\nLOVD\nGene-specific curation\nOpen access\nDeep expert knowledge per gene\n\n\nClinGen\nExpert panel consensus\nOpen access\nHigh-confidence curations\n\n\n\n\n\n\n\n\n2.8.3 ClinGen and Expert Curation\nTwo laboratories classify the same variant in SCN5A, a cardiac arrhythmia gene. One calls it pathogenic based on computational predictions and population frequency; the other calls it a variant of uncertain significance because those same features, applied by a general pipeline, miss nuances specific to ion channel variants. Who is right? When classifications conflict and clinical decisions hang in the balance, the field needs authoritative adjudication from experts who deeply understand both the gene biology and the accumulated evidence. This is the role that ClinGen fills.\nClinical laboratories submitting to ClinVar vary enormously in expertise and evidentiary standards. A submission from a general diagnostic laboratory applying ACMG guidelines to an unfamiliar gene may differ substantially from an assessment by researchers who have studied that gene for decades. The Clinical Genome Resource (ClinGen) addresses this heterogeneity by providing expert-curated assessments at multiple levels (Rehm et al. 2015).\nClinGen expert panels evaluate gene-disease validity (whether variation in a gene can cause a specific disease) and dosage sensitivity (whether haploinsufficiency or triplosensitivity leads to clinical phenotypes). These evaluations build on the catalog of Mendelian phenotypes maintained by OMIM, which provides curated gene-disease associations and clinical synopses (Amberger et al. 2015).\nClinGen also develops calibrated thresholds for computational predictors, specifying score intervals that justify different strengths of evidence (supporting, moderate, strong) for pathogenicity or benignity (Pejaver et al. 2022). The FDA has recognized these curations as valid scientific evidence for clinical validity. These calibrations directly inform how computational scores should be incorporated into variant classification workflows and are discussed further in ?sec-ch14-acmg-mapping for score calibration to ACMG evidence levels and ?sec-ch14-combining-evidence for integration of multiple computational predictors.\n\n\n2.8.4 Pharmacogenomics Resources\nMost variant interpretation focuses on rare mutations that cause or predispose to disease. Pharmacogenomics presents a different paradigm: common polymorphisms that individually may have no disease consequences but profoundly influence how individuals respond to medications. These variants matter not because they cause disease but because they determine whether a drug will work, fail, or cause harm.\nImplementing pharmacogenomics in clinical practice requires three capabilities: curating variant-drug associations from published literature, translating that evidence into actionable dosing guidelines, and automating the path from a patient’s VCF file to a clinical report. PharmGKB addresses the first need, cataloging over 800 genes, 700 drugs, and thousands of variant-drug-phenotype relationships with evidence levels (Whirl-Carrillo et al. 2012). CPIC translates this knowledge into standardized guidelines specifying how to adjust drug selection or dosing based on metabolizer phenotype (Relling et al. 2019). PharmCAT automates annotation, taking VCF files as input and producing CPIC-compliant reports (Sangkuhl et al. 2019). ClinPGx integrates all three into a unified framework spanning variant detection through clinical recommendation (Gong et al. 2025).\n\n\n\n\n\n\nStar-Allele Nomenclature\n\n\n\nPharmacogenes use a specialized nomenclature where haplotypes (combinations of variants on the same chromosome) are designated by star alleles. The reference haplotype is *1, with variant haplotypes numbered sequentially (*2, *3, etc.) as they were discovered. Each star allele represents a specific combination of SNVs, indels, or structural variants that travel together.\nFor CYP2D6, over 150 star alleles have been defined. Some reduce enzyme function (*4, *5), others increase it through gene duplication (*1xN), and many have unknown functional consequences. A patient’s diplotype (the combination of maternal and paternal star alleles) determines their metabolizer phenotype: poor, intermediate, normal, or ultrarapid.\nStar-allele calling requires phasing to determine which variants co-occur on the same chromosome, plus structural variant detection to identify gene deletions and duplications. Standard SNV-focused pipelines miss critical information, which is why specialized tools like PharmCAT exist.\n\n\nThe CYP2D6 gene exemplifies the complexity. This cytochrome P450 enzyme metabolizes approximately 25% of clinically used drugs, including codeine, tamoxifen, and many antidepressants (Nofziger et al. 2019). Patients with loss-of-function CYP2D6 variants cannot activate codeine to morphine, rendering the drug ineffective for pain relief; patients with gene duplications may convert codeine too efficiently, experiencing dangerous opioid toxicity from standard doses. The difference between these scenarios depends entirely on accurate star-allele diplotyping.\nFrom a modeling perspective, pharmacogenomic resources offer a complementary type of label linking variants to molecular and clinical outcomes through different mechanisms than Mendelian disease pathogenicity. Where ClinVar labels indicate whether a variant causes disease, PharmGKB labels indicate how a variant affects drug response in individuals who may be otherwise healthy.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch02-data.html#sec-ch02-constraints",
    "href": "part_1/p1-ch02-data.html#sec-ch02-constraints",
    "title": "2  Data Landscape",
    "section": "2.9 Inherited Constraints",
    "text": "2.9 Inherited Constraints\nEvery genomic model inherits both the power and the biases of its training data. A variant effect predictor trained on ClinVar labels absorbs the ascertainment patterns of clinical sequencing: European ancestry overrepresented, rare diseases enriched, incidental findings undersampled. A chromatin model trained on ENCODE immortalized cell lines learns regulatory patterns that may not generalize to primary tissues with different epigenetic landscapes. Models that estimate genetic constraint quantify how strongly purifying selection acts against damaging variants in each gene, comparing observed variant counts to expectations. But when trained on human population databases, these models systematically miss the most severe cases: gene-lethal variants never appear because carriers do not survive to be sequenced.\nThese biases compound as data flows through analysis pipelines. GWAS summary statistics carry ancestry composition forward into polygenic scores. Conservation scores calculated from biased multiple sequence alignments propagate into variant effect predictions. Foundation model pretraining on reference genomes from limited populations shapes the representations available for all downstream applications. Each transformation amplifies some biases while masking others, making the provenance of model behavior increasingly difficult to trace.\nThe critical question is not whether models trained on these data contain biases; they do. The question is whether those biases can be characterized, bounded, and ultimately corrected. These foundational datasets appear throughout genomic AI as training labels, evaluation benchmarks, and sometimes both simultaneously. Recognizing when the same data sources serve multiple roles is essential for interpreting model performance honestly and anticipating where generalization will fail. Part VI examines these challenges in depth: data partitioning strategies that account for shared ancestry and homology (Section 12.2), population structure effects that confound genetic associations (?sec-ch22-ancestry-confounding), and ascertainment patterns that create circularity in clinical labels (?sec-ch22-label-bias).\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat are the four main layers of the genomic data ecosystem, and what type of evidence does each provide?\nWhy does alternative splicing create challenges for variant annotation, and what percentage of human multi-exon genes undergo alternative splicing?\nHow do systematic ancestry biases in population databases like gnomAD affect downstream models trained on these data?\nWhat is the circularity problem between computational variant predictors and clinical databases like ClinVar?\nWhy do GWAS summary statistics represent a compromise between scientific value and data sharing restrictions?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nFour layers of the genomic data ecosystem:\n\nReference assemblies and gene annotations: coordinate foundation and biological vocabulary\nPopulation catalogs and biobanks: variant frequencies and phenotype associations\nFunctional genomics consortia: biochemical activity across cell types\nClinical databases: pathogenicity interpretations\n\nAlternative splicing challenges: A variant may be benign in the canonical transcript but pathogenic in a tissue-specific isoform. Models trained only on canonical transcripts (like MANE Select) miss this complexity. Over 95% of human multi-exon genes undergo alternative splicing, producing an estimated 100,000+ distinct protein isoforms from ~20,000 genes.\nAncestry bias effects: Models inherit whatever populations were represented in training data. A variant common in West African populations but absent from European-dominated catalogs would be incorrectly flagged as ultra-rare. Constraint metrics (pLI, LOEUF) are poorly calibrated for variants private to underrepresented populations. This propagates through variant prioritization, deleteriousness prediction, and clinical interpretation.\nCircularity problem: ClinVar serves simultaneously as training data for computational predictors and as the benchmark for evaluating those same predictors. Clinical laboratories increasingly incorporate computational scores (CADD, REVEL, AlphaMissense) as supporting evidence for pathogenicity classification. When these ClinVar labels then train or evaluate those same predictors, models may learn to reproduce their own predictions rather than discovering independent biological signal.\nGWAS summary statistics compromise: Individual-level genotype and phenotype data are powerful but sensitive, requiring complex data use agreements, IRB approvals, and secure computing infrastructure. Summary statistics (per-variant effect sizes, standard errors, p-values) capture the essential association signal without revealing individual genotypes, enabling meta-analysis and data sharing while protecting participant privacy.\n\n\n\n\n\n\nKey Concepts Covered:\n\nReference genomes and gene annotations provide the coordinate system for all genomic analysis; choices embedded in these resources propagate through downstream models\nPopulation variant catalogs (dbSNP, 1000 Genomes, gnomAD) establish frequency baselines and constraint metrics; ancestry representation biases affect all models using these data\nBiobanks link genotypes to phenotypes at scale, enabling GWAS and polygenic score development; European ancestry overrepresentation limits global applicability\nFunctional genomics datasets (ENCODE, Roadmap, GTEx) provide training labels for regulatory models; cell type coverage determines what models can learn\nClinical databases (ClinVar, ClinGen) aggregate pathogenicity classifications; circularity between computational predictors and clinical labels complicates benchmarking\nPhenotype quality varies dramatically based on definition, EHR coding practices, and ascertainment; noisy labels attenuate genetic signals\n\nCritical Literacy Framework:\nWhen evaluating any genomic ML model, ask:\n\nWhat data was it trained on, and what populations/cell types are represented?\nWhat are the known biases and blind spots of those training resources?\nDoes the evaluation benchmark share data sources with training (circularity risk)?\nWill the model’s training population match the deployment population?\n\nLooking Ahead:\nThe data landscape described here flows directly into Chapter 3 (Chapter 3), where we examine how GWAS transforms biobank data into genetic associations. The biases introduced by ancestry representation, phenotype quality, and population structure become concrete statistical challenges addressed throughout Part 2 (Learning & Evaluation) and Part 5 (Responsible Deployment).\n\n\n\n\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F. Scott, and Ada Hamosh. 2015. “OMIM.org: Online Mendelian Inheritance in Man (OMIM®), an Online Catalog of Human Genes and Genetic Disorders.” Nucleic Acids Research 43 (D1): D789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAuton, Adam, Gonçalo R. Abecasis, David M. Altshuler, Richard M. Durbin, Gonçalo R. Abecasis, David R. Bentley, Aravinda Chakravarti, et al. 2015. “A Global Reference for Human Genetic Variation.” Nature 526 (7571): 68–74. https://doi.org/10.1038/nature15393.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025. “[TraitGym] Benchmarking DNA Sequence Models for Causal Regulatory Variant Prediction in Human Genetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nEdgar, Ron, Michael Domrachev, and Alex E. Lash. 2002. “Gene Expression Omnibus: NCBI Gene Expression and Hybridization Array Data Repository.” Nucleic Acids Research 30 (1): 207–10. https://doi.org/10.1093/nar/30.1.207.\n\n\nFokkema, Ivo F. A. C., Peter E. M. Taschner, Gerard C. P. Schaafsma, J. Celli, Jeroen F. J. Laros, and Johan T. den Dunnen. 2011. “LOVD v.2.0: The Next Generation in Gene Variant Databases.” Human Mutation 32 (5): 557–63. https://doi.org/10.1002/humu.21438.\n\n\nFrankish, Adam, Mark Diekhans, Anne-Maud Ferreira, Rory Johnson, Irwin Jungreis, Jane Loveland, Jonathan M Mudge, et al. 2019. “GENCODE Reference Annotation for the Human and Mouse Genomes.” Nucleic Acids Research 47 (D1): D766–73. https://doi.org/10.1093/nar/gky955.\n\n\nGamazon, Eric R., Heather E. Wheeler, Kaanan P. Shah, Sahar V. Mozaffari, Keston Aquino-Michaels, Robert J. Carroll, Anne E. Eyler, et al. 2015. “A Gene-Based Association Method for Mapping Traits Using Reference Transcriptome Data.” Nature Genetics 47 (9): 1091–98. https://doi.org/10.1038/ng.3367.\n\n\nGong, Li, Clarissa J Klein, Kelly E Caudle, Ann M Moyer, Stuart A Scott, Michelle Whirl-Carrillo, Teri E Klein, ClinGen Pharmacogenomics Working Group (PGxWG), and on behalf of the. 2025. “Integrating Pharmacogenomics into the Broader Construct of Genomic Medicine: Efforts by the ClinGen Pharmacogenomics Working Group (PGxWG).” Clinical Chemistry 71 (1): 36–44. https://doi.org/10.1093/clinchem/hvae181.\n\n\nGudbjartsson, Daniel F., Patrick Sulem, Hannes Helgason, Arnaldur Gylfason, Sigurjon A. Gudjonsson, Florian Zink, Asmundur Oddson, et al. 2015. “Sequence Variants from Whole Genome Sequencing a Large Group of Icelanders.” Scientific Data 2 (1): 150011. https://doi.org/10.1038/sdata.2015.11.\n\n\nGusev, Alexander, Arthur Ko, Huwenbo Shi, Gaurav Bhatia, Wonil Chung, Brenda W. J. H. Penninx, Rick Jansen, et al. 2016. “Integrative Approaches for Large-Scale Transcriptome-Wide Association Studies.” Nature Genetics 48 (3): 245–52. https://doi.org/10.1038/ng.3506.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “[AlphaFold2] Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data Navigation on the ENCODE Portal.” Nature Communications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. “Integrative Analysis of 111 Reference Human Epigenomes.” Nature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nKurki, Mitja I., Juha Karjalainen, Priit Palta, Timo P. Sipilä, Kati Kristiansson, Kati M. Donner, Mary P. Reeve, et al. 2023. “FinnGen Provides Genetic Insights from a Well-Phenotyped Isolated Population.” Nature 613 (7944): 508–18. https://doi.org/10.1038/s41586-022-05473-8.\n\n\nLambert, Samuel A., Laurent Gil, Simon Jupp, Scott C. Ritchie, Yu Xu, Annalisa Buniello, Aoife McMahon, et al. 2021. “The Polygenic Score Catalog as an Open Database for Reproducibility and Systematic Evaluation.” Nature Genetics 53 (4): 420–25. https://doi.org/10.1038/s41588-021-00783-5.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness, Glenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome Reference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nMarees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan, Emmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018. “[GWAS] A Tutorial on Conducting Genome-Wide Association Studies: Quality Control and Statistical Analysis.” International Journal of Methods in Psychiatric Research 27 (2): e1608. https://doi.org/10.1002/mpr.1608.\n\n\nMorales, Joannella, Shashikant Pujar, Jane E. Loveland, Alex Astashyn, Ruth Bennett, Andrew Berry, Eric Cox, et al. 2022. “A Joint NCBI and EMBL-EBI Transcript Set for Clinical Genomics and Research.” Nature 604 (7905): 310–15. https://doi.org/10.1038/s41586-022-04558-8.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy Schwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021. “An Open Approach to Systematically Prioritize Causal Variants and Genes at All Published Human GWAS Trait-Associated Loci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nNofziger, Charity, Amy J. Turner, Katrin Sangkuhl, Michelle Whirl-Carrillo, José A. G. Agúndez, John L. Black, Henry M. Dunnenberger, et al. 2019. “PharmVar GeneFocus: CYP2D6.” Clinical Pharmacology & Therapeutics 107 (1): 154–70. https://doi.org/10.1002/cpt.1643.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. “ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design.” Advances in Neural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nnull, null. 2019. “The ‘All of Us’ Research Program.” New England Journal of Medicine 381 (7): 668–76. https://doi.org/10.1056/NEJMsr1809937.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V. Bzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The Complete Sequence of a Human Genome.” Science 376 (6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Leary, Nuala A., Mathew W. Wright, J. Rodney Brister, Stacy Ciufo, Diana Haddad, Rich McVeigh, Bhanu Rajput, et al. 2016. “Reference Sequence (RefSeq) Database at NCBI: Current Status, Taxonomic Expansion, and Functional Annotation.” Nucleic Acids Research 44 (D1): D733–45. https://doi.org/10.1093/nar/gkv1189.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel, Sean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022. “Calibration of Computational Tools for Missense Variant Pathogenicity Classification and ClinGen Recommendations for PP3/BP4 Criteria.” American Journal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nRegev, Aviv, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe Benoist, Ewan Birney, Bernd Bodenmiller, et al. 2017. “The Human Cell Atlas.” Edited by Thomas R Gingeras. eLife 6 (December): e27041. https://doi.org/10.7554/eLife.27041.\n\n\nRehm, Heidi L., Jonathan S. Berg, Lisa D. Brooks, Carlos D. Bustamante, James P. Evans, Melissa J. Landrum, David H. Ledbetter, et al. 2015. “ClinGen — The Clinical Genome Resource.” New England Journal of Medicine 372 (23): 2235–42. https://doi.org/10.1056/NEJMsr1406261.\n\n\nRelling, Mary V., Teri E. Klein, Roseann S. Gammal, Michelle Whirl-Carrillo, James M. Hoffman, and Kelly E. Caudle. 2019. “The Clinical Pharmacogenetics Implementation Consortium: 10 Years Later.” Clinical Pharmacology & Therapeutics 107 (1): 171–75. https://doi.org/10.1002/cpt.1651.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRuan, Yunfeng, Yen-Feng Lin, Yen-Chen Anne Feng, Chia-Yen Chen, Max Lam, Zhenglin Guo, Lin He, et al. 2022. “Improving Polygenic Prediction in Ancestrally Diverse Populations.” Nature Genetics 54 (5): 573–80. https://doi.org/10.1038/s41588-022-01054-7.\n\n\nSangkuhl, Katrin, Michelle Whirl-Carrillo, Ryan M. Whaley, Mark Woon, Adam Lavertu, Russ B. Altman, Lester Carter, Anurag Verma, Marylyn D. Ritchie, and Teri E. Klein. 2019. “Pharmacogenomics Clinical Annotation Tool (PharmCAT).” Clinical Pharmacology & Therapeutics 107 (1): 203–10. https://doi.org/10.1002/cpt.1568.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nSherry, S. T., M.-H. Ward, M. Kholodov, J. Baker, L. Phan, E. M. Smigielski, and K. Sirotkin. 2001. “dbSNP: The NCBI Database of Genetic Variation.” Nucleic Acids Research 29 (1): 308–11. https://doi.org/10.1093/nar/29.1.308.\n\n\nSirugo, Giorgio, Scott M. Williams, and Sarah A. Tishkoff. 2019. “The Missing Diversity in Human Genetic Studies.” Cell 177 (1): 26–31. https://doi.org/10.1016/j.cell.2019.02.048.\n\n\nSollis, Elliot, Abayomi Mosaku, Ala Abid, Annalisa Buniello, Maria Cerezo, Laurent Gil, Tudor Groza, et al. 2023. “The NHGRI-EBI GWAS Catalog: Knowledgebase and Deposition Resource.” Nucleic Acids Research 51 (D1): D977–85. https://doi.org/10.1093/nar/gkac1010.\n\n\nStenson, Peter D., Matthew Mort, Edward V. Ball, Katy Evans, Matthew Hayden, Sally Heywood, Michelle Hussain, Andrew D. Phillips, and David N. Cooper. 2017. “The Human Gene Mutation Database: Towards a Comprehensive Repository of Inherited Mutation Data for Medical Research, Genetic Diagnosis and Next-Generation Sequencing Studies.” Human Genetics 136 (6): 665–77. https://doi.org/10.1007/s00439-017-1779-6.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N. Phan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023. “Leveraging Base-Pair Mammalian Constraint to Understand Genetic Variation and Human Disease.” Science 380 (6643): eabn2937. https://doi.org/10.1126/science.abn2937.\n\n\nTHE GTEX CONSORTIUM. 2020. “The GTEx Consortium Atlas of Genetic Regulatory Effects Across Human Tissues.” Science 369 (6509): 1318–30. https://doi.org/10.1126/science.aaz1776.\n\n\nTHE TABULA SAPIENS CONSORTIUM. 2022. “The Tabula Sapiens: A Multiple-Organ, Single-Cell Transcriptomic Atlas of Humans.” Science 376 (6594): eabl4896. https://doi.org/10.1126/science.abl4896.\n\n\nVõsa, Urmo, Annique Claringbould, Harm-Jan Westra, Marc Jan Bonder, Patrick Deelen, Biao Zeng, Holger Kirsten, et al. 2021. “Large-Scale Cis- and Trans-eQTL Analyses Identify Thousands of Genetic Loci and Polygenic Scores That Regulate Blood Gene Expression.” Nature Genetics 53 (9): 1300–1310. https://doi.org/10.1038/s41588-021-00913-z.\n\n\nWhirl-Carrillo, M, E M McDonagh, J M Hebert, L Gong, K Sangkuhl, C F Thorn, R B Altman, and T E Klein. 2012. “Pharmacogenomics Knowledge for Personalized Medicine.” Clinical Pharmacology & Therapeutics 92 (4): 414–17. https://doi.org/10.1038/clpt.2012.96.\n\n\nXu, Leqi, Wangjie Zheng, Jiaqi Hu, Yingxin Lin, Jia Zhao, Gefei Wang, Tianyu Liu, and Hongyu Zhao. 2025. “Improving Polygenic Risk Prediction Performance by Integrating Electronic Health Records Through Phenotype Embedding.” The American Journal of Human Genetics 112 (12): 3030–45. https://doi.org/10.1016/j.ajhg.2025.11.006.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll, and Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls Using DeepVariant and GLnexus.” Bioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei Sun, Chen-Hao Chen, et al. 2019. “Cistrome Data Browser: Expanded Datasets and New Tools for Gene Regulatory Analysis.” Nucleic Acids Research 47 (D1): D729–35. https://doi.org/10.1093/nar/gky1094.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Landscape</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html",
    "href": "part_1/p1-ch03-gwas.html",
    "title": "3  GWAS and Polygenic Scores",
    "section": "",
    "text": "3.1 GWAS Framework\nConsider a clinician counseling a patient about cardiovascular disease risk. Traditional risk factors (age, smoking, cholesterol, blood pressure) explain roughly 50% of the variation in who develops disease (Khera and Kathiresan 2017). Family history suggests that genetics contributes substantially to the remainder, but which genetic variants matter, and how much does each contribute? GWAS provide a systematic approach to answering these questions by testing each of millions of variants for association with the trait of interest.\nThe scale required for well-powered GWAS explains why large-scale biobanks (Section 2.3 for comprehensive biobank descriptions, Section 2.9 for data composition biases) have become essential infrastructure for statistical genetics. UK Biobank, with its 500,000 participants genotyped across hundreds of thousands of variants and linked to extensive phenotypic data, has enabled GWAS for thousands of traits at sample sizes that were unimaginable a decade ago. Similar resources, including the Million Veteran Program, FinnGen, and All of Us, continue to expand the scope of discoverable associations. The biobank paradigm of combining dense genotyping with rich phenotyping at population scale has transformed GWAS from underpowered fishing expeditions into reliable discovery engines.\nThe core logic is straightforward. For each variant in turn, researchers ask whether individuals carrying more copies of a particular allele tend to have higher or lower values of the phenotype (for quantitative traits) or higher or lower probability of disease (for binary outcomes). They estimate an effect size, compute a test statistic under the null hypothesis of no association, and record a p-value. After testing millions of variants, those exceeding a stringent significance threshold are identified, the associated loci reported, and interpretation begins regarding which genes and pathways might be involved.\nThis apparently simple procedure requires careful attention to study design, quality control, and statistical modeling. The phenotype must be measured consistently across individuals. The genotypes must be accurate and the variants well-defined. Confounders that correlate with both genotype and phenotype (most notably population structure) must be controlled. Multiple testing across millions of variants demands stringent significance thresholds. Only after addressing these challenges can GWAS results be trusted and translated into downstream applications.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-gwas-framework",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-gwas-framework",
    "title": "3  GWAS and Polygenic Scores",
    "section": "",
    "text": "3.1.1 Association Models for Quantitative Traits\nChoosing the wrong statistical model for a GWAS does not merely introduce imprecision; it distorts effect size estimates in ways that propagate through every downstream analysis, from fine-mapping to polygenic scores to drug target prioritization. A height GWAS and a schizophrenia GWAS require fundamentally different approaches because one outcome is continuous and the other binary. Applying linear regression to a binary outcome produces fitted values outside the 0-1 probability range and residuals that violate normality assumptions.\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThe following sections introduce statistical models using mathematical notation. Readers unfamiliar with regression can focus on the conceptual intuition: for each variant, we ask whether people with more copies of one allele tend to have higher or lower values of the trait, after accounting for confounding factors like age, sex, and ancestry.\n\n\nFor continuous phenotypes such as height, LDL cholesterol, or blood pressure, the standard approach is linear regression. Let \\(y_i\\) denote the phenotype for individual \\(i\\), and let \\(g_{ij}\\) denote the genotype dosage at variant \\(j\\), encoded as \\(0\\), \\(1\\), or \\(2\\) copies of the alternative allele (or as a fractional value for imputed genotypes). The model is:\n\\[\ny_i = \\alpha + \\beta_j g_{ij} + \\gamma^\\top c_i + \\varepsilon_i\n\\]\nThe coefficient \\(\\beta_j\\) represents the expected change in phenotype per additional copy of the alternative allele, holding covariates \\(c_i\\) fixed. When phenotypes are standardized to zero mean and unit variance, \\(\\beta_j\\) is expressed in standard deviation units per allele. The vector \\(c_i\\) typically includes age, sex, genotyping batch, and principal components capturing ancestry (discussed below). The residual \\(\\varepsilon_i\\) captures unexplained variation, assumed to be independent and identically distributed across individuals.\nFor each variant, a test statistic is computed for the null hypothesis \\(H_0: \\beta_j = 0\\). In large samples, the t-statistic follows approximately a standard normal distribution under the null, yielding a two-sided p-value. With \\(M\\) variants tested (typically \\(10^6\\) to \\(10^7\\) after imputation), multiple comparison correction is essential. The conventional genome-wide significance threshold of \\(5 \\times 10^{-8}\\) approximates a Bonferroni correction for roughly one million effectively independent tests, accounting for correlation among variants due to linkage disequilibrium (Risch and Merikangas 1996; Pe’er et al. 2008).\n\n\n\n\n\n\nDeep Dive: Multiple Testing Correction\n\n\n\nFor ML readers: Testing millions of variants creates a massive multiple testing problem:\nThe problem: If you test 1 million variants at \\(\\alpha = 0.05\\), you expect 50,000 false positives by chance alone. GWAS must distinguish true signals from this noise.\nBonferroni correction: Divide \\(\\alpha\\) by the number of tests. For 1 million tests: \\(0.05 / 1{,}000{,}000 = 5 \\times 10^{-8}\\). This threshold has become the conventional genome-wide significance cutoff.\nWhy it works: Variants in linkage disequilibrium are correlated, so there are roughly 1 million effectively independent tests even when testing 10 million variants. The \\(5 \\times 10^{-8}\\) threshold controls the family-wise error rate (FWER)—the probability of any false positive.\nConsequences for ML:\n\nGWAS hits represent extremely strong statistical signals (typically hundreds or thousands of samples carry each risk allele)\nMarginal effects (variants that contribute but don’t reach significance) are systematically excluded\nPolygenic scores must handle the millions of “sub-threshold” variants that collectively explain substantial heritability\n\n\n\n\n\n\n\n\n\nWorked Example: Interpreting GWAS P-values\n\n\n\nScenario: A GWAS for height tests 8 million variants across 500,000 individuals. One variant (rs12345) shows p = 3 × 10-12 with effect size β = 0.02 SD per allele.\nStep 1: Does it pass genome-wide significance? Threshold: 5 × 10-8 Observed: 3 × 10-12 Since 3 × 10-12 &lt; 5 × 10-8, yes—this variant passes the significance threshold.\nStep 2: How strong is the evidence? The p-value is 1,666× smaller than required (5 × 10-8 / 3 × 10-12 ≈ 16,667). This is a highly robust association unlikely to be a false positive.\nStep 3: What is the expected number of false positives? Under the null hypothesis for 8 million tests at threshold 5 × 10-8: Expected false positives = 8 × 106 × 5 × 10-8 = 0.4\nSo we expect fewer than 1 false positive across the entire study when using this threshold.\nStep 4: How much variance does this variant explain? For a common variant (MAF = 30%), variance explained ≈ 2 × MAF × (1-MAF) × β² = 2 × 0.3 × 0.7 × 0.02² ≈ 0.00017 = 0.017%\nInterpretation: Despite overwhelming statistical evidence, this variant explains only 0.017% of height variance. This illustrates why GWAS require thousands of significant variants to explain substantial heritability.\n\n\n\n\n3.1.2 Association Models for Disease Outcomes\nBinary outcomes create a specific statistical problem that, if ignored, systematically distorts effect size estimates in ways that compound through downstream applications. When the phenotype is disease status (affected or unaffected), linear regression produces nonsensical predictions: fitted values outside the 0-1 probability range and residuals that violate normality assumptions. The consequence extends beyond statistical inelegance. Effect sizes estimated under the wrong model propagate into polygenic scores and risk prediction, potentially misclassifying patients who sit near clinical decision thresholds where intervention recommendations change.\nFor binary phenotypes, logistic regression replaces linear regression. The model relates genotype to the log-odds of disease:\n\\[\n\\log \\frac{P(y_i = 1)}{P(y_i = 0)} = \\alpha + \\beta_j g_{ij} + \\gamma^\\top c_i\n\\]\nHere \\(\\beta_j\\) is the log-odds ratio per allele, and \\(\\exp(\\beta_j)\\) gives the odds ratio (OR). An odds ratio of \\(1.2\\) means that each additional copy of the alternative allele increases the odds of disease by \\(20\\%\\). For rare diseases (prevalence below approximately \\(10\\%\\)), odds ratios approximate relative risks, but the distinction matters for common conditions and when communicating absolute risk to patients.\nCase-control sampling, in which cases are enriched relative to their population frequency, distorts absolute risk estimates but preserves the validity of odds ratio estimation. This mathematical property explains why GWAS conducted in case-control designs can still produce effect sizes useful for polygenic scores, provided downstream applications account for baseline disease incidence. The likelihood function conditions on disease status, making the odds ratio identifiable regardless of sampling scheme.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider a GWAS for type 2 diabetes with 10,000 cases and 10,000 controls, when the true population prevalence is only 8%. If you compute the raw proportion of cases in the study (50%), this clearly misrepresents population risk. Why does the odds ratio remain valid despite this artificial enrichment of cases? What would happen if you tried to convert this odds ratio to absolute risk without accounting for the sampling design?\n\n\n\n\n3.1.3 Manhattan Plots and Q-Q Plots\nThe Manhattan plot has become the iconic visualization of GWAS results, named for its resemblance to the New York City skyline. Each point represents a tested variant, with genomic position along the x-axis (ordered by chromosome) and negative log-transformed p-value on the y-axis. Variants with stronger associations rise higher; those exceeding the genome-wide significance threshold of \\(5 \\times 10^{-8}\\) (typically drawn as a horizontal line at \\(-\\log_{10}(5 \\times 10^{-8}) \\approx 7.3\\)) are considered significant hits.\nThe Manhattan plot reveals both the successes and limitations of GWAS at a glance. Prominent peaks indicate genomic regions harboring trait-associated variants, but each peak typically contains dozens or hundreds of correlated variants rather than a single causal nucleotide. The width of peaks reflects local linkage disequilibrium structure: broader peaks indicate regions where many variants are correlated with the lead signal. The height reflects statistical strength, which depends on effect size, allele frequency, and sample size. Tall, narrow peaks suggest strong, well-localized signals; broad peaks spanning megabases indicate that fine-mapping will be challenging.\nQuantile-quantile (Q-Q) plots complement Manhattan plots by assessing whether the observed p-value distribution matches theoretical expectations under the null hypothesis. Systematic deviation from the diagonal (genomic inflation) suggests either true polygenic signal or residual confounding from population structure. The genomic inflation factor λ quantifies this deviation, with values substantially above 1.0 warranting investigation of potential confounders.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore viewing the Manhattan plot below, make a prediction: If you were conducting a GWAS for height (a highly heritable trait) in 500,000 individuals, what pattern would you expect to see across the genome? Would you expect to find:\n\nOne or two very tall peaks indicating a few genes of large effect?\nMany peaks of varying heights distributed across multiple chromosomes?\nPeaks clustered together on a few chromosomes?\nA relatively flat landscape with few significant associations?\n\nConsider what the heritability of height (~80%) and the concept of polygenicity might imply about the genetic architecture.\n\n\n\n\n\n\n\n\nAnnotated Manhattan plot from genome-wide association study\n\n\n\n\nFigure 3.1: Anatomy of a Manhattan plot from genome-wide association study. Each point represents a tested genetic variant, with genomic position along the x-axis (chromosomes color-alternated) and statistical significance (-log10 p-value) on the y-axis. The horizontal dashed line marks the genome-wide significance threshold (P = 5 x 10-8). Prominent peaks identify associated loci, but each peak typically contains dozens to hundreds of correlated variants in linkage disequilibrium (LD) rather than a single causal nucleotide. The width of peaks reflects local LD structure. Inset: quantile-quantile (Q-Q) plot comparing observed to expected p-value distributions, with genomic inflation factor (lambda) indicating adequate control of population stratification when near 1.0.\n\n\n\n\n\n3.1.4 Population Structure Control\nPopulation structure poses a fundamental challenge to GWAS interpretation because it can generate association signals indistinguishable from true biological effects. If allele frequencies differ systematically across subpopulations and the phenotype also varies across these groups for non-genetic reasons (differences in environment, diet, healthcare access, socioeconomic status), naive association testing will detect variants that mark ancestry rather than causal biology. A variant that is simply more common in one population will appear associated with any trait that differs between populations, regardless of biological mechanism. The resulting false positives waste resources on follow-up studies and, more insidiously, can embed ancestry-related confounding into polygenic scores that are then deployed as if they measured pure genetic risk.\nPrincipal component analysis (PCA) on the genotype matrix captures the major axes of genetic variation across individuals (Price et al. 2006; Patterson, Price, and Reich 2006). The leading principal components often correspond to continental ancestry gradients or finer-scale population structure within a study. PCA works for population structure correction because it exploits a fundamental property of population genetics: allele frequency differences between populations accumulate across thousands of variants in correlated patterns. A variant that is 5% more common in one population is likely to have neighbors that show similar frequency differences, and this genome-wide correlation structure is exactly what PCA captures. By including the top principal components as covariates, the regression model can distinguish true genetic associations (where the variant itself affects the trait) from population stratification artifacts (where the variant merely tags ancestry). Including these PCs as covariates in the regression model attenuates spurious associations driven by ancestry stratification.\nThis correction is imperfect. Subtle structure not captured by the included PCs, cryptic relatedness among individuals, and the interweaving of genetic ancestry with environmental exposures all complicate interpretation. The challenges extend far beyond technical statistical adjustment: ancestry is entangled with healthcare access, environmental exposures, and socioeconomic factors in ways that simple covariate correction cannot fully resolve. These issues become critical when translating GWAS results to clinical applications and when evaluating whether polygenic scores perform equitably across populations. The full complexity of ancestry as a confounder is addressed in Chapter 12.\n\n\n\n\n\n\nKey Insight\n\n\n\nPopulation structure control is a statistical partial fix for a deeper problem. PCA covariates remove associations driven purely by ancestry, but they cannot disentangle genetic effects from environmental factors that correlate with ancestry. A variant associated with a trait after PCA adjustment may still reflect gene-environment correlation rather than direct biological causation.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-heritability",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-heritability",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.2 Heritability: What Genetics Can Explain",
    "text": "3.2 Heritability: What Genetics Can Explain\nBefore GWAS can identify specific variants, a more fundamental question must be answered: how much of the variation in a trait is attributable to genetics at all? A trait entirely determined by environment would yield no GWAS hits regardless of sample size. A trait entirely determined by genetics would, in principle, be fully predictable from genotype. Heritability quantifies where traits fall along this spectrum, but the concept is more subtle than it first appears, and different estimation methods yield systematically different answers.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore viewing the heritability decomposition figure below, consider this scenario: Height has pedigree heritability of ~80% (from twin studies), but even the largest GWAS with millions of participants explain only ~25% of height variance with genome-wide significant hits. The remaining genetic variance hasn’t disappeared—where is it? Predict which of these contributes most to “missing heritability”:\n\nMeasurement error in height phenotypes\nVariants too rare to be on genotyping arrays\nThousands of common variants with effects too small to reach significance individually\nGene-environment interactions not captured by the model\n\n\n\n\n\n\n\n\n\nDecomposition of phenotypic variance showing heritability components\n\n\n\n\nFigure 3.2: Decomposition of phenotypic variance for a highly heritable trait (height). The outermost ring represents total phenotypic variance. The first partition separates genetic (blue, ~80%) from environmental (green, ~20%) contributions. Within the genetic component, additive effects (narrow-sense heritability) dominate over non-additive effects (dominance, epistasis). Within additive heritability, SNP-heritability (~50-60% for height) represents variance captured by common variants on genotyping arrays, while the remainder reflects ‘missing heritability’: rare variants, structural variants, and imperfect tagging of causal alleles. This nested structure explains why GWAS-significant hits explain only a fraction of trait heritability and why polygenic scores built on common variants face a performance ceiling.\n\n\n\n\n3.2.1 Pedigree Heritability\nClassical genetics estimated heritability by comparing phenotypic similarity among relatives. Identical twins share all their genetic variation; fraternal twins share on average half; full siblings also share half; parents and offspring share half; cousins share one-eighth. If genetic variation influences a trait, closer relatives should be more similar. The correlation structure across relationship types allows partitioning of phenotypic variance into genetic and environmental components.\nNarrow-sense heritability (\\(h^2\\)) represents the proportion of phenotypic variance attributable to additive genetic effects. For height, pedigree studies consistently estimate \\(h^2\\) around \\(0.80\\), meaning that \\(80\\%\\) of the variation in height across individuals in the studied population can be attributed to genetic differences (Visscher, Hill, and Wray 2008). For schizophrenia, twin studies estimate \\(h^2\\) around \\(0.80\\) as well (Hilker et al. 2018). For body mass index, estimates cluster around \\(0.50\\) to \\(0.80\\) depending on the population, age group, and study design (Elks et al. 2012).\nThese high heritability estimates established that genetics substantially influences most traits of biomedical interest, motivating the search for specific causal variants. If 80% of height variation is genetic, then genetic variants collectively must explain most of that variation. Finding those variants became the goal of GWAS.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA twin study finds that identical twins have a correlation of 0.90 for height, while fraternal twins have a correlation of 0.45. Using the classical formula \\(h^2 = 2 \\times (r_{MZ} - r_{DZ})\\), what is the estimated heritability? Does this match the published estimates mentioned above? What assumptions does this calculation make about shared versus non-shared environmental influences?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe estimated heritability is \\(h^2 = 2 \\times (0.90 - 0.45) = 0.90\\) (90%), which matches the ~80% published estimates reasonably well. This calculation assumes that shared environmental influences affect identical and fraternal twins equally, and that gene-environment interactions and non-additive genetic effects are negligible—assumptions that may be violated in practice.\n\n\n\n\n\n\n\n3.2.2 SNP-Heritability and the Missing Heritability Problem\nGWAS delivered a puzzle. For height, even the largest studies with hundreds of significant hits explained only a fraction of the heritability estimated from family studies. Early GWAS collectively explained perhaps 5% of height variance when pedigree studies suggested 80% should be genetic. This gap, termed missing heritability, sparked intense debate about where the remaining genetic variance might hide (Manolio et al. 2009).\nThe concept of SNP-heritability (\\(h^2_{\\mathrm{SNP}}\\)) emerged to parse this puzzle into more tractable components. Rather than asking how much variance is explained by genome-wide significant variants, researchers asked how much variance is explained by all common SNPs on genotyping arrays, including those that fail to reach significance. Methods such as GCTA-GREML estimate this quantity by modeling phenotypic similarity as a function of genetic similarity computed across all SNPs (Yang et al. 2010). For height, SNP-heritability estimates reach approximately \\(0.50\\) to \\(0.60\\), substantially higher than variance explained by significant hits alone but still below pedigree estimates.\nThis intermediate value revealed that missing heritability actually comprises two distinct gaps. The first gap separates SNP-heritability from variance explained by GWAS-significant variants. This polygenic gap reflects the architecture of complex traits: thousands of variants each contribute effects too small to reach genome-wide significance individually, yet they collectively explain substantial variance when modeled together. As sample sizes grow and more variants cross the significance threshold, this gap narrows. The polygenic gap is not truly “missing” heritability; the variance is captured by common SNPs, just distributed across too many variants to detect individually.\nThe second gap separates pedigree heritability from SNP-heritability. This hidden heritability reflects genetic variation genuinely absent from common SNP arrays: rare variants below minor allele frequency thresholds, structural variants poorly tagged by single nucleotide polymorphisms, copy number variations, and variants not in linkage disequilibrium with array content. Unlike the polygenic gap, this component cannot be recovered by increasing GWAS sample size; it requires different data types entirely, such as whole-genome sequencing that captures rare variation directly.\nThe distinction matters for how foundation models might contribute. Models trained on common variant data inherit the SNP-heritability ceiling; they cannot learn patterns from variation they never observe. Integrating rare variant data, structural variant calls, or multi-omic measurements represents not merely incremental improvement but access to a fundamentally different component of genetic architecture.\n\n\n3.2.3 Quantifying Polygenicity: The M50% Metric\nSNP-heritability tells us how much variance common variants explain in aggregate, but not how that variance is distributed across the genome. Two traits with identical SNP-heritability can have radically different genetic architectures: one might concentrate signal in a handful of large-effect variants, the other might distribute signal across thousands of variants each contributing infinitesimally. This distinction matters for model building, benchmark design, and understanding when foundation models might provide advantages over simpler approaches.\nThe M50% metric from PolyFun quantifies this distribution directly: it measures the number of SNPs required to explain 50% of a trait’s heritability (Weissbrod et al. 2020). Hair color requires only approximately 28 SNPs to reach M50%, reflecting an oligogenic architecture where pigmentation genes dominate. Height, despite similar heritability estimates, requires roughly 3,400 SNPs—a highly polygenic architecture where thousands of variants each contribute small effects. At the extreme, behavioral traits like number of children require millions of SNPs, indicating genetic architectures that may exceed current modeling capacity entirely.\n\n\n\nTable 3.1: The M50% metric reveals orders-of-magnitude differences in genetic architecture complexity.\n\n\n\n\n\n\n\n\n\n\n\nTrait\nM50%\nArchitecture\nImplications\n\n\n\n\nHair color\n~28 SNPs\nOligogenic\nSimple benchmarks; linear models suffice\n\n\nHeight\n~3,400 SNPs\nHighly polygenic\nGenuine challenge for risk prediction\n\n\nPsychiatric traits\n&gt;10,000 SNPs\nExtremely polygenic\nMay approach fundamental limits\n\n\n\n\n\n\nThe M50% metric provides immediate diagnostic value for model evaluation. When foundation models match linear baseline performance, the M50% helps interpret whether this equivalence reflects benchmark saturation (oligogenic traits where both approaches capture available signal) or model failure (highly polygenic traits where foundation model capacity goes unused). For oligogenic traits, equivalence is expected and uninformative. For highly polygenic traits with low saturation ratios, equivalence suggests foundation models are failing to leverage their theoretical capacity for learning complex representations.\nThe following table summarizes how different heritability concepts relate to each other and to GWAS capabilities:\n\n\n\nTable 3.2: Heritability decomposition for height, illustrating the nested structure of genetic variance.\n\n\n\n\n\n\n\n\n\n\n\nHeritability Concept\nDefinition\nTypical Value (Height)\nWhat It Captures\n\n\n\n\nPedigree (\\(h^2\\))\nPhenotypic variance explained by all genetic factors\n~0.80\nUpper bound from family resemblance\n\n\nSNP-heritability (\\(h^2_{SNP}\\))\nVariance explained by common array SNPs\n~0.50-0.60\nWhat GWAS can potentially capture\n\n\nExplained by GWAS hits\nVariance from genome-wide significant variants\n~0.25 (and rising)\nWhat GWAS has actually found\n\n\nPolygenic gap\n\\(h^2_{SNP}\\) minus GWAS-explained\n~0.25-0.35\nDiscoverable with larger samples\n\n\nHidden heritability\n\\(h^2\\) minus \\(h^2_{SNP}\\)\n~0.20-0.30\nRequires new data types (rare variants, SVs)\n\n\n\n\n\n\n\n\n3.2.4 Implications for GWAS and Polygenic Scores\nThe heritability landscape carries practical implications for what GWAS and polygenic scores can achieve. SNP-heritability sets an upper bound on the predictive accuracy of polygenic scores built from common variants: a PGS cannot explain more variance than is captured by the SNPs it uses. For height, with SNP-heritability around 0.50, the best possible common-variant PGS could explain at most half of phenotypic variance. Current PGS for height in European-ancestry populations approach this bound, explaining roughly 25% of variance with continued gains as sample sizes grow (Yengo et al. 2022).\nFor diseases, the relationship between heritability and predictive accuracy is more complex. A highly heritable disease might have low predictive accuracy if the causal variants are rare, if gene-environment interactions dominate, or if the heritability is distributed across thousands of variants each with tiny effects. Conversely, a moderately heritable disease with a few common variants of large effect might be more predictable. The architecture of genetic effects matters as much as total heritability.\nMissing heritability also motivates the integration of rare variant analysis with GWAS of common variants. Whole-genome sequencing studies can capture rare variants invisible to genotyping arrays, potentially recovering some of the genetic variance missing from common-variant analyses. Foundation models trained on sequence data, rather than genotype arrays, may ultimately capture genetic effects across the full allele frequency spectrum, a possibility explored in Chapter 17.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-ld",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-ld",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.3 Linkage Disequilibrium and the Association-Causation Gap",
    "text": "3.3 Linkage Disequilibrium and the Association-Causation Gap\n\n\n\n\n\n\nStop and Think\n\n\n\nImagine you are conducting a GWAS and find that variant A shows a strong association with heart disease (p &lt; 10-20). You check and find that variant A is in strong linkage disequilibrium (r2 = 0.95) with nearby variant B. Based on the GWAS result alone, can you determine which variant, if either, is causal? What additional information would you need?\n\n\nGWAS test variants one at a time, but the genome is not inherited one variant at a time. Nearby variants travel together on haplotypes and are co-inherited across generations except when recombination separates them. This correlation structure, known as linkage disequilibrium (LD), is both essential to GWAS power and the source of their fundamental interpretive limitation. Without LD, GWAS would need to genotype every variant in the genome directly; with LD, statistical association cannot distinguish cause from correlation.\n\n\n\n\n\n\nDeep Dive: Linkage Disequilibrium\n\n\n\nFor ML readers: Linkage disequilibrium (LD) is the non-random association of alleles at different genomic positions:\nWhy it exists: During meiosis, chromosomes are inherited in large blocks. Variants that are physically close tend to be inherited together because recombination events between them are rare. Over many generations, recombination gradually breaks down these correlations.\nMeasuring LD: The most common metric is \\(r^2\\), the squared correlation between allele counts at two positions:\n\n\\(r^2 = 1.0\\): Perfect correlation (variants always inherited together)\n\\(r^2 = 0.5\\): Moderate correlation\n\\(r^2 = 0\\): No correlation (variants segregate independently)\n\nLD decay: Correlation decreases with physical distance. In European populations, LD typically extends 10-100 kb; in African populations, LD decays faster due to older population history and more accumulated recombination events.\nImplications for genomics:\n\nGWAS power: A genotyping array with 1 million variants can “tag” most common variation because ungenotyped variants are correlated with genotyped ones\nCausal ambiguity: When variant A associates with disease, any variant B in high LD with A will also show association—GWAS cannot distinguish which is causal\nPopulation specificity: LD patterns differ across populations, causing polygenic scores to transfer poorly\n\n\n\nWhen a GWAS identifies a significant association at variant j, three possibilities exist. The variant itself may be causal, directly influencing the phenotype through some molecular mechanism. Alternatively, variant j may simply be correlated with a nearby causal variant \\(k\\) due to LD, with the association signal reflecting this correlation rather than direct causation. In complex regions, multiple causal variants may exist, and the observed association pattern reflects their joint effects filtered through the local LD structure. Distinguishing these scenarios from GWAS summary statistics alone is often impossible. The causal variant and its tag look identical in the association data, yet only the causal variant represents a valid drug target or mechanistic insight.\nFine-mapping methods attempt to resolve this ambiguity by modeling LD structure explicitly. Reference panels from the 1000 Genomes Project (Section 2.2.2) and gnomAD (Section 2.2.3) enable LD calculation across diverse populations, providing the correlation matrices these methods require. Statistical fine-mapping narrows the set of plausible causal variants but often cannot identify a single culprit. Incorporating functional priors from regulatory models and variant effect predictors can further prioritize candidates; this integration of foundation model representations into fine-mapping is examined in ?sec-ch14-fm-paradigm.\n\n\n\n\n\n\n\n\nHaplotype structure showing how nearby variants co-segregate\n\n\n\n\n\n\n\nLD matrix displaying pairwise correlations across a genomic region\n\n\n\n\n\n\n\nPopulation-specific LD patterns affecting PGS portability\n\n\n\n\n\n\nFigure 3.3: Linkage disequilibrium and the distinction between causal and tag variants. (A) Haplotype structure showing how nearby variants co-segregate on ancestral chromosome segments. The causal variant (star) is statistically correlated with neighboring tag variants, making them indistinguishable in GWAS. (B) LD matrix displaying pairwise correlations (r2) across a genomic region, revealing block structure where many variants within blocks show high correlation. (C) Population-specific LD patterns: the same causal variant shows different correlation structures in populations with different demographic histories, explaining why polygenic scores derived in one population transfer poorly to others with different LD patterns.\n\n\n\n\n3.3.1 Structure of Linkage Disequilibrium\nUnderstanding why LD creates interpretive ambiguity requires understanding how LD arises and decays. Recombination during meiosis shuffles genetic material between parental chromosomes, with crossover events occurring at an average rate of roughly one per 100 megabases, meaning each chromosome arm typically experiences only one or two exchanges per generation. Over many generations, recombination breaks down long-range correlations between variants while preserving short-range structure. The timescale of this decay matters: LD between variants separated by a few kilobases persists for thousands of generations, while correlations spanning megabases decay within tens of generations. The result is a mosaic pattern: regions of high LD (haplotype blocks) where many variants are strongly correlated, interspersed with recombination hotspots where LD decays rapidly.\nRecombination does not occur uniformly across the genome. Crossover hotspots, typically spanning 1 to 2 kilobases, concentrate the majority of recombination events into a small fraction of genomic sequence. These hotspots are enriched for specific sequence motifs recognized by the zinc finger protein PRDM9, which directs the recombination machinery to particular locations. The consequence is that haplotype blocks can extend for hundreds of kilobases across regions lacking hotspots, while adjacent blocks may be separated by sharp boundaries where recombination has effectively randomized allelic associations.\nThe squared correlation coefficient \\(r^2\\) quantifies LD between pairs of variants. Unlike Pearson correlation, which measures linear relationships between continuous variables, \\(r^2\\) for LD is computed from allele frequencies and haplotype counts in a \\(2\\times 2\\) contingency table. The metric equals \\((p_{AB} - p_A p_B)^2 / (p_A p_a p_B p_b)\\), where \\(p_{AB}\\) is the frequency of the AB haplotype and \\(p_A\\), \\(p_a\\), \\(p_B\\), \\(p_b\\) are the individual allele frequencies. This formulation captures the deviation from random association expected under linkage equilibrium. The notation unfortunately overlaps with the coefficient of determination from linear regression, but the quantities measure different phenomena: regression \\(R^2\\) captures variance explained by a fitted model, while LD \\(r^2\\) captures non-random association between alleles at two loci. When \\(r^2\\) approaches \\(1\\), the two variants are nearly always observed together on the same haplotypes; when \\(r^2\\) approaches \\(0\\), they segregate independently. From a GWAS perspective, if a causal variant \\(k\\) has strong association with the phenotype and variant \\(j\\) is in high LD with \\(k\\) (high \\(r^2\\)), then variant \\(j\\) will also show strong association even if it has no direct causal role. The statistical signal propagates through LD, creating ambiguity about which variant is actually functional.\nLD patterns vary across populations because demographic history shapes which haplotypes persist and at what frequencies. Founder effects occur when a small number of individuals establish a new population, carrying only a subset of the ancestral haplotype diversity. The Finnish population, descended from a small founder group roughly 4,000 years ago, exhibits extended LD blocks and elevated frequencies of otherwise rare disease alleles. Bottlenecks produce similar effects: dramatic population contractions eliminate rare haplotypes and reduce diversity, leaving survivors with correlated genetic backgrounds. In contrast, large stable populations accumulate recombination events over many generations, breaking down LD more completely. A variant that tags a causal allele effectively in one population may be a poor proxy in another where different recombination history has decoupled the correlation. This population-specificity of LD structure is one reason polygenic scores fail to transfer across ancestries, a problem examined in detail in Section 3.7.\n\n\n\n\n\n\nKey Insight\n\n\n\nLD is both the foundation and the limitation of GWAS. It enables discovery by allowing a limited set of genotyped variants to capture signals from ungenotyped causal variants. But it prevents identification of specific causal variants because the statistical signal cannot distinguish cause from correlation. This tension is fundamental and cannot be resolved by larger sample sizes alone.\n\n\n\n\n3.3.2 Causal Variants, Tag Variants, and GWAS Catalogs\nThe distinction between causal and tag variants determines whether GWAS results can translate into biological insight or clinical action. A causal variant directly influences the phenotype, whether by altering protein sequence, disrupting transcription factor binding, affecting splicing, or modifying chromatin state. A tag variant is merely correlated with a causal variant through LD, serving as a statistical proxy without direct functional consequence. The distinction is invisible to GWAS: both produce association signals, and in the presence of strong LD, those signals are statistically indistinguishable.\nGWAS catalogs therefore report associated loci, not causal variants. The “lead SNP” at each locus (the variant with the smallest p-value) is often a tag rather than the causal variant, particularly when the causal variant is rare, poorly genotyped, or not present on the array. Even when a locus is robustly associated, dozens or hundreds of correlated variants may be statistically indistinguishable from the lead SNP.\nThis limitation has concrete practical consequences. Drug development requires identifying causal genes and mechanisms, not just associated regions; targeting a tag variant or the wrong gene wastes years of development effort. Clinical variant interpretation needs to distinguish functional mutations from neutral passengers; reporting a tag as pathogenic misleads patients and clinicians. Polygenic scores built on tag SNPs may lose power when applied to populations with different LD patterns, since the tag-causal correlation that made the tag useful may not hold. The gap between association and causation motivates the fine-mapping approaches considered next.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-fine-mapping",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-fine-mapping",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.4 Fine-Mapping: From Loci to Causal Variants",
    "text": "3.4 Fine-Mapping: From Loci to Causal Variants\nA pharmaceutical company evaluating a GWAS hit for drug development faces a concrete problem: the associated locus spans 500 kilobases, contains 200 correlated variants, and overlaps three genes. Which gene should they target? Which variant drives the association? Investing hundreds of millions of dollars in a program targeting the wrong gene would be catastrophic, yet GWAS summary statistics alone cannot resolve the ambiguity. Fine-mapping attempts to address this gap, moving from “this region is associated” to “these specific variants are most likely causal” by exploiting the joint behavior of correlated variants under explicit statistical models.\n\n3.4.1 Statistical Framework\nThe core insight of fine-mapping is that while multiple variants may show similar marginal association statistics, their joint behavior under a model that accounts for LD can discriminate among them. A causal variant should show association beyond what can be explained by LD with its neighbors; a tag variant should not. This distinction, invisible when variants are tested one at a time, becomes apparent when their correlations are modeled jointly.\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThe following section presents the Bayesian framework for fine-mapping. The key intuition is that we compare how well different “causal configurations” (which variants are assumed causal) explain the observed data. Variants that consistently appear in high-probability configurations are more likely to be causal.\n\n\nBayesian fine-mapping methods approach the problem by specifying a prior distribution over which variants in a region might be causal, then computing posterior probabilities given the observed association statistics and local LD structure (Maller et al. 2012; Hormozdiari et al. 2014). The key outputs are posterior inclusion probabilities (PIPs), which estimate the probability that each variant is among the causal set, and credible sets, which are minimal sets of variants that contain the true causal variant(s) with specified probability (commonly 95%).\nThe mathematical foundation rests on comparing models that differ in which variants are causal. Consider a region containing \\(m\\) variants, and let \\(\\gamma\\) denote a configuration specifying which variants are causal (a binary vector of length \\(m\\)). Under a linear model, the observed GWAS summary statistics (effect estimates \\(\\hat{\\beta}\\) and their standard errors) follow a multivariate normal distribution whose mean depends on the true causal effects and whose covariance depends on the LD matrix \\(\\Sigma\\). The LD matrix enters the likelihood because correlated variants produce correlated effect estimates: if variants A and B are in high LD, then when the causal variant is A, variant B will also show a strong effect estimate simply because individuals carrying A tend to carry B. The statistical model must account for this correlation structure to determine which configuration of causal variants best explains the observed pattern of associations. The likelihood of observing the data under configuration \\(\\gamma\\) can be written as:\n\\[P(\\hat{\\beta} \\mid \\gamma, \\Sigma) = \\int P(\\hat{\\beta} \\mid \\beta_\\gamma, \\Sigma) \\, P(\\beta_\\gamma) \\, d\\beta_\\gamma\\]\nwhere \\(\\beta_\\gamma\\) represents the true effect sizes for causal variants in configuration \\(\\gamma\\), and \\(P(\\beta_\\gamma)\\) is the prior on effect sizes (typically Gaussian with variance \\(\\sigma^2\\)) (Benner et al. 2016). This integral has a closed-form solution when both the likelihood and prior are Gaussian, yielding a Bayes factor comparing each configuration to the null model of no associations.\nThe posterior probability of configuration \\(\\gamma\\) follows from Bayes’ theorem:\n\\[P(\\gamma \\mid \\hat{\\beta}, \\Sigma) = \\frac{P(\\hat{\\beta} \\mid \\gamma, \\Sigma) \\, P(\\gamma)}{\\sum_{\\gamma'} P(\\hat{\\beta} \\mid \\gamma', \\Sigma) \\, P(\\gamma')}\\]\nThe prior \\(P(\\gamma)\\) encodes assumptions about the number and distribution of causal variants. Common choices include a fixed maximum number of causal variants \\(K\\) (often 1 to 5) with uniform probability across configurations of equal size, or a binomial prior where each variant has independent probability \\(\\pi\\) of being causal (Wang et al. 2020). The denominator sums over all possible configurations, a computation that becomes intractable for large regions (with \\(m\\) variants and up to \\(K\\) causal variants, the number of configurations scales as \\(C(m, K)\\)).\nThe posterior inclusion probability for variant \\(j\\) marginalizes over all configurations in which that variant appears:\n\\[\\text{PIP}_j = \\sum_{\\gamma : j \\in \\gamma} P(\\gamma \\mid \\hat{\\beta}, \\Sigma)\\]\nThis quantity answers the question: given everything we observed, what is the probability that variant \\(j\\) is among the causal variants? Methods like FINEMAP, CAVIAR, and SuSiE differ primarily in how they approximate the intractable sum over configurations and in their prior specifications, but all produce PIPs as their primary output (Benner et al. 2016; Hormozdiari et al. 2014; Wang et al. 2020).\n\n\n\n\n\n\nStop and Think\n\n\n\nThe fine-mapping equations above compute posterior probabilities for different “configurations” of causal variants. Before reading further, consider: Why must fine-mapping methods sum over all possible configurations rather than just selecting the configuration with the best GWAS p-values? What would go wrong if we simply picked the top 3 most significant variants and called those causal?\n\n\nCredible sets provide a complementary summary. A 95% credible set is the smallest set of variants whose cumulative PIP exceeds 0.95. When a single variant dominates (PIP above 0.95), the credible set contains only that variant. When LD distributes probability across many variants, credible sets expand accordingly. The SuSiE method produces one credible set per inferred causal signal, enabling regions with multiple independent associations to be decomposed into distinct sets (Wang et al. 2020).\nVariants with high PIPs (above 0.5 or 0.9) are strong candidates for functional follow-up. Credible sets that contain few variants are more actionable than those containing dozens. The width of credible sets reflects both the strength of the association signal and the local LD structure: tight LD means many variants remain plausible even with strong statistical evidence. In some regions, fine-mapping narrows thousands of candidates to a handful; in others, the ambiguity remains irreducible given available data.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA fine-mapping analysis reports that variant X has PIP = 0.35 and variant Y has PIP = 0.30, with X and Y in strong LD (r2 = 0.92). The 95% credible set contains 5 variants including both X and Y. What can you conclude about whether X or Y is causal? What would make you more confident in one over the other?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nYou cannot confidently determine which variant is causal based on these statistics alone—both have modest PIPs and are strongly correlated through LD. Functional annotations (e.g., X disrupts a known regulatory motif while Y is in a non-functional region), multi-ancestry fine-mapping showing differential LD patterns, or experimental validation would increase confidence in one variant over the other.\n\n\n\n\n\n\n\n3.4.2 Functional Annotation Priors\nStatistical fine-mapping alone cannot resolve regions where multiple variants are in near-perfect LD; the data simply cannot distinguish variants that are always co-inherited. Functional annotations offer a path forward by incorporating biological plausibility: not all genomic positions are equally likely to harbor causal variants. Variants disrupting coding sequences, altering transcription factor binding sites, or falling within active enhancers carry higher prior probability of functional relevance than variants in unannotated intergenic regions.\nAnnotation-informed approaches update fine-mapping priors based on these external data sources. Variants in coding regions, promoters, enhancers, or regions of evolutionary constraint may be assigned higher prior probability of causality. Integration with chromatin accessibility data (from ATAC-seq or DNase-seq), transcription factor binding maps (from ChIP-seq), or expression quantitative trait loci (eQTL) can further prioritize variants with plausible regulatory mechanisms.\nThe functional scores introduced in Chapter 2 provide systematic frameworks for quantifying variant-level annotations. Scores such as CADD, DANN, and Eigen integrate diverse genomic features into single numbers that can inform fine-mapping priors (Kircher et al. 2014; Quang, Chen, and Xie 2015; Ionita-Laza et al. 2016). More recently, foundation models trained on genomic sequence have produced variant effect predictions that capture functional information beyond what traditional annotations provide (Chapter 17). These scores transform fine-mapping from a purely statistical exercise into an integrative analysis that combines association evidence with mechanistic plausibility.\nLarge-scale resources now link GWAS summary statistics, fine-mapping results, and functional genomic annotations across hundreds of traits and thousands of loci (Buniello et al. 2025; Mountjoy et al. 2021). These datasets enable systematic identification of variants that are both statistically prioritized and functionally plausible, though the biological validation required to confirm causal mechanisms remains laborious and is completed for only a small fraction of associated loci.\n\n\n3.4.3 Multi-Ancestry Fine-Mapping\nSingle-ancestry fine-mapping encounters a fundamental resolution limit: when variants are in tight LD within the study population, no amount of statistical sophistication can distinguish them. Multi-ancestry approaches break through this limit by exploiting the population-specificity of LD structure. A variant in tight LD with twenty neighbors in Europeans may have only three correlated variants in African-ancestry populations, where shorter LD blocks (reflecting larger historical effective population size) provide greater resolution.\nJoint fine-mapping across ancestries leverages these differences systematically (Kichaev et al. 2017). When a variant remains strongly associated across populations despite different local LD structure, confidence in its causal role increases. The logic is straightforward: a true causal variant should show consistent association regardless of which other variants happen to be correlated with it in any particular population. A tag variant, by contrast, may appear associated in one population (where it correlates with the causal variant) but not in another (where that correlation is absent).\nMulti-ancestry approaches grow increasingly important as large biobanks expand to include diverse populations, though they require careful attention to potential effect size heterogeneity across populations. The core assumption that causal variants produce consistent effects worldwide can be violated through several mechanisms. Gene-environment interactions represent one such mechanism: a variant’s phenotypic effect may depend on environmental exposures that differ systematically across populations. The FTO obesity-associated variants, for instance, show stronger effects in sedentary populations than in physically active ones, and lactase persistence variants in LCT produce metabolic consequences only where dairy consumption is common. When populations differ in relevant environmental contexts, effect sizes will differ even for genuinely causal variants.\nGenetic background effects present a second complication. A variant’s impact may depend on epistatic interactions with other loci, and if modifier variants differ in frequency across populations, the focal variant will appear to have population-specific effects. A causal variant might produce large effects only when a particular haplotype is present at an interacting locus; if that haplotype is common in one population but rare in another, the apparent effect of the causal variant will vary despite its genuine causal role. These complexities do not invalidate multi-ancestry fine-mapping, but they do mean that variants showing heterogeneous effects across populations should not be automatically dismissed. The method gains statistical power by assuming effect consistency, yet biological reality sometimes violates this assumption in ways that could exclude true causal variants or reduce confidence in them.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-pgs-construction",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-pgs-construction",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.5 Polygenic Score Construction",
    "text": "3.5 Polygenic Score Construction\nA 35-year-old woman with a family history of breast cancer asks her physician whether she should begin mammography screening earlier than guidelines recommend. Traditional risk models incorporate family history, age, and reproductive factors, but cannot capture the cumulative effect of thousands of common variants, each conferring small increases in risk, that together may substantially elevate her probability of disease. Polygenic scores address this gap by aggregating variant effects across the genome into a single number:\n\\[\n\\text{PGS}_i = \\sum_{j} w_j g_{ij}\n\\]\nThe weight \\(w_j\\) reflects the estimated effect of variant \\(j\\), and \\(g_{ij}\\) is the genotype dosage for individual \\(i\\). The simplest approach uses GWAS effect size estimates directly as weights; more sophisticated methods adjust for LD, apply shrinkage, or incorporate fine-mapping information. The clinical promise is substantial: for diseases with significant genetic components, polygenic scores can identify individuals at elevated risk years or decades before disease onset, potentially enabling targeted screening or prevention.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore viewing the comparison below, predict: If you’re building a polygenic score for height (which has thousands of contributing variants) using two methods:\n\nMethod A: Select only genome-wide significant variants (p &lt; 5×10-8), prune away correlated variants, use remaining ~300 variants\nMethod B: Keep all variants but shrink effect sizes based on statistical evidence and LD structure, effectively using millions of variants with modulated weights\n\nWhich method would you expect to perform better, and why? Consider what we learned about the “polygenic gap” and missing heritability.\n\n\n\n\n\n\n\n\n\n\nClumping and thresholding discards most variants\n\n\n\n\n\n\n\nLD-aware Bayesian methods retain all variants with modulated weights\n\n\n\n\n\n\nFigure 3.4: Comparison of polygenic score construction approaches. (A) Clumping and thresholding (C+T) applies successive filters to GWAS summary statistics, discarding variants below a p-value threshold and pruning correlated variants to retain only independent lead SNPs. While computationally simple, this approach discards substantial genetic signal. (B) LD-aware Bayesian methods (LDpred, PRS-CS) model the correlation structure explicitly, applying shrinkage priors that modulate effect sizes based on local LD patterns while retaining contributions from all variants. The Bayesian approach preserves information distributed across correlated variants, typically improving prediction accuracy for highly polygenic traits where many variants each contribute small effects.\n\n\n\n\n\n\n\n\n\nTerminology: PGS versus PRS\n\n\n\nThe literature uses overlapping terminology. Polygenic risk score (PRS) is common in clinical contexts, emphasizing disease risk prediction. Polygenic score (PGS) is more general, encompassing both disease and quantitative trait prediction. Genomic risk score and related terms also appear, often interchangeably. This book uses PGS as the default, adding “risk” when specifically discussing disease outcomes. Methodological overviews provide detailed guidance on construction and evaluation (Choi, Mak, and O’Reilly 2020).\n\n\nIntegration of polygenic scores with foundation model features represents an emerging approach to clinical risk prediction. Feature extraction from genomic foundation models (Section 9.3) can provide complementary signals to traditional PGS. The integration strategies for combining PGS with deep learning approaches are examined in Section 27.1.\n\n3.5.1 Clumping and Thresholding\nThe challenge of constructing a useful polygenic score is not mathematical but statistical: GWAS provide noisy estimates of millions of effects, many of which are correlated through LD, and naive summation produces scores dominated by noise rather than signal. Clumping and thresholding (C+T) represents the simplest solution: reduce both the noise and the correlation by aggressive filtering, accepting substantial information loss in exchange for robustness.\nThe procedure begins with clumping: variants are ranked by p-value, then iteratively the most significant variant is selected and all variants within a specified window (typically \\(250\\ \\mathrm{kb}\\)) in LD above a threshold (typically \\(r^2 &gt; 0.1\\)) are removed. This greedy selection prioritizes the most statistically significant variants under the assumption that they carry the strongest causal signals, while the LD pruning removes redundant variants that would otherwise inflate the score by counting the same underlying signal multiple times. This yields a set of approximately independent index variants. A p-value cutoff then retains only variants below threshold, which serves to exclude variants whose estimated effects are dominated by noise. Finally, weights are set equal to the GWAS effect size estimate for retained variants, and zero otherwise.\nThe hyperparameters (LD window, \\(r^2\\) threshold, p-value threshold) are typically chosen by grid search to maximize predictive performance in a held-out validation set. This tuning introduces overfitting risk, particularly in small samples or when the validation population differs from the eventual deployment population.\nC+T is transparent and computationally simple, but it discards substantial information. Most variants are excluded, LD is handled only through coarse pruning, and variants with modest p-values that collectively explain meaningful variance may be entirely omitted. For highly polygenic traits where thousands of variants each contribute small effects, this information loss substantially degrades prediction accuracy. The method treats LD as a problem to be eliminated rather than a correlation structure to be modeled, an approach that sacrifices power for simplicity.\n\n\n3.5.2 LD-Aware Bayesian Methods\nThe information discarded by C+T is not random noise; it contains genuine signal about genetic effects distributed across correlated variants. Rather than pruning away this structure, a more principled approach models the joint distribution of effect sizes explicitly, treating the true effects \\(\\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_M)\\) as random variables drawn from a prior distribution. Given GWAS summary statistics and an LD reference panel, these methods infer posterior mean effect sizes that serve as PGS weights. The key insight is that LD becomes information rather than nuisance: correlated variants constrain each other’s likely effects, improving estimation for all.\nLDpred assumes that a fraction \\(p\\) of variants have nonzero effects drawn from a Gaussian distribution, while the remainder have zero effect (Vilhjálmsson et al. 2015). The method uses GWAS summary statistics and LD from a reference panel (computed from a subset of individuals or external dataset matching the target ancestry) to compute approximate posterior effect sizes. The Bayesian framework provides principled shrinkage: variants with weak evidence (small effect estimates relative to their standard errors) are shrunk heavily toward zero, while variants with strong evidence retain most of their estimated effect. Crucially, the LD structure is incorporated so that when multiple correlated variants all show association, the posterior recognizes that they likely reflect a single causal signal and distributes the total effect appropriately rather than counting it multiple times. These posteriors shrink noisy estimates toward zero, borrow strength across correlated variants, and generally outperform C+T when properly tuned.\nPRS-CS extends this framework by placing a continuous shrinkage prior on effect sizes, which better accommodates the highly polygenic architecture of complex traits and reduces sensitivity to the sparsity hyperparameter (Ge et al. 2019). The continuous prior assigns most variants small but nonzero effects rather than forcing a binary causal/non-causal distinction. The method has shown strong performance across a range of traits and ancestries, though like all methods it requires an LD reference that reasonably matches the target population.\nRelated approaches (lassosum, SBayesR, and others) use different priors or optimization strategies but share the core insight: jointly modeling effect sizes under LD yields better predictions than pruning LD away. Performance differences among methods are often modest when each is well-tuned, and the choice may depend on computational resources, availability of validation data, and specific trait architecture.\nThe following table compares the major PGS construction approaches:\n\n\n\nTable 3.3: Comparison of polygenic score construction methods. All methods use GWAS summary statistics as input.\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nLD Handling\nComputational Cost\nKey Hyperparameters\nBest For\n\n\n\n\nC+T\nPrune away\nLow\nWindow size, r2 threshold, p-value threshold\nQuick baseline, sparse architectures\n\n\nLDpred\nJoint model\nMedium\nFraction causal (p), LD radius\nModerately polygenic traits\n\n\nPRS-CS\nContinuous shrinkage\nMedium-High\nGlobal shrinkage, LD reference\nHighly polygenic traits\n\n\nSBayesR\nMixture prior\nHigh\nMixture components, LD reference\nWhen architecture unknown\n\n\n\n\n\n\n\n\n3.5.3 Fine-Mapping-Informed Scores\nPolygenic scores built on tag SNPs face a fundamental portability problem: the tag-causal correlation that justified including a variant may not hold in populations with different LD structure. Fine-mapping outputs, particularly posterior inclusion probabilities, offer a potential solution by identifying variants more likely to be causal. Causal variants should remain predictive regardless of population-specific LD patterns, since their effects are direct rather than mediated through correlation.\nTwo strategies incorporate fine-mapping information into PGS construction. Selection approaches retain only variants above a PIP threshold (typically 0.1 or 0.5), focusing the score on high-confidence causal candidates. Weighting approaches modulate each variant’s contribution by its PIP, downweighting likely tags while preserving information from variants with intermediate evidence.\nFine-mapping-informed approaches aim to concentrate weight on variants that are biologically meaningful rather than merely statistically associated. In principle, this should improve cross-ancestry transferability since causal variants remain causal regardless of population-specific LD patterns. In practice, gains depend on fine-mapping resolution, which is limited in regions of tight LD. The approaches remain an active area of methodological development, with potential for substantial improvement as multi-ancestry fine-mapping resources expand.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-pgs-interpretation",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-pgs-interpretation",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.6 Polygenic Score Interpretation",
    "text": "3.6 Polygenic Score Interpretation\nA polygenic score is a number, but numbers do not make clinical decisions. A patient told they are in the 95th percentile of genetic risk may interpret this as near-certain disease development, while a physician may recognize it as modest risk elevation insufficient to change management. Converting a score into actionable information requires understanding what it represents, how it relates to disease risk or trait values, and where its interpretation breaks down. Miscommunication at this stage can transform a useful risk stratification tool into a source of inappropriate anxiety or false reassurance.\n\n3.6.1 Relative Risk and Percentiles\nThe most immediate clinical question about a high polygenic score is: how much does it increase risk? Polygenic scores are most naturally interpreted in relative terms by fitting a logistic regression in a validation cohort:\n\\[\n\\log \\frac{P(y_i = 1)}{P(y_i = 0)} = \\alpha + \\theta \\cdot \\mathrm{PGS}_i + \\eta^\\top z_i\n\\]\nwhere \\(z_i\\) contains covariates and \\(\\theta\\) captures the effect of the PGS. After standardizing the score to unit variance, \\(\\exp(\\theta)\\) gives the odds ratio per standard deviation of the PGS. This metric allows statements such as “individuals one standard deviation above the mean have 1.5-fold higher odds of disease.”\nPercentile-based communication is common in clinical contexts. The risk for individuals in the top 1% or 5% of the PGS distribution can be compared to those near the median or in the bottom percentiles. For some conditions, individuals in the top percentiles have risk comparable to or exceeding that conferred by single high-penetrance mutations: the top 8% of the coronary artery disease PGS distribution has risk equivalent to familial hypercholesterolemia carriers, and the top 1% of the breast cancer PGS distribution has lifetime risk approaching that of BRCA2 mutation carriers (Khera and Kathiresan 2017; Mavaddat et al. 2019). This finding makes polygenic scores potentially relevant for clinical risk stratification, though the appropriate thresholds and clinical actions remain subjects of ongoing research and debate.\nTranslation of PGS into clinical decision-making requires careful attention to calibration and uncertainty quantification. Clinical risk prediction frameworks that integrate multiple evidence types are detailed in Section 27.2. Calibration requirements for clinical deployment appear in Section 23.2 for general principles and Section 27.6.2 for clinical-specific considerations.\n\n\n\n\n\n\nStop and Think\n\n\n\nA direct-to-consumer genetics company reports that you are in the 90th percentile for coronary artery disease genetic risk. Before panicking (or celebrating if you were in the 10th percentile), what additional information would you need to interpret this result? Consider: What population was the percentile calculated from? What is your baseline risk given age, sex, and other factors? How does genetic risk interact with modifiable risk factors?\n\n\n\n\n3.6.2 Absolute Risk\nA physician cannot act on relative risk alone; clinical decisions require knowing the probability that this specific patient will develop disease over a specified time horizon. Relative risk statements can mislead when baseline risk varies substantially. A 1.5-fold increase in odds for a disease with 1% baseline risk means absolute risk rises from 1% to roughly 1.5%; the same relative increase for a disease with 20% baseline risk means absolute risk rises from 20% to roughly 26%. A patient told they have “50% higher risk” may react very differently depending on whether baseline risk is low or high.\nConverting PGS to absolute risk requires combining the score with baseline incidence rates, which vary by age, sex, and other factors. The hazard ratio per standard deviation of PGS, combined with age-specific incidence curves from population registries, can yield personalized risk trajectories. Such calculations demand careful attention to calibration: the model must produce well-calibrated probabilities in the population where it will be deployed, not just the population where it was trained. A model calibrated in UK Biobank may systematically over- or under-estimate risk when applied to a U.S. clinical population with different baseline incidence rates or healthcare practices. Clinical deployment of PGS is addressed in detail in Chapter 27.\n\n\n3.6.3 Explained Variance and Discrimination\nPopulation-level performance metrics determine whether a polygenic score has any utility, but they can mask the substantial uncertainty that remains for any individual patient. For quantitative traits, the coefficient of determination (\\(R^2\\)) between PGS and phenotype provides a direct measure of explanatory power. Height PGS now explain roughly \\(25\\%\\) of phenotypic variance in European-ancestry populations, approaching the theoretical maximum given current sample sizes and the heritability of the trait (Yengo et al. 2022). For binary traits, the \\(R^2\\) on the liability scale (the underlying continuous risk) is more interpretable than the observed-scale \\(R^2\\), which depends on disease prevalence.\nArea under the receiver operating characteristic curve (auROC) measures discrimination: the probability that a randomly selected case has a higher PGS than a randomly selected control. auROC values of 0.5 indicate no discrimination (random guessing); values approaching 1.0 indicate near-perfect separation. For most complex diseases, PGS achieve auROC values in the 0.55 to 0.70 range when used alone, with incremental gains when combined with traditional risk factors (Torkamani, Wineinger, and Topol 2018; Lambert, Abraham, and Inouye 2019). These values reflect meaningful stratification at the population level but limited utility for individual prediction.\nEven a PGS that explains 10% of trait variance leaves 90% unexplained by factors genetic and environmental. High-risk individuals by PGS may never develop disease; low-risk individuals may be affected. Polygenic scores provide probabilistic risk stratification, not deterministic prediction. This distinction is critical for clinical communication and for setting appropriate expectations about what genomic risk information can and cannot offer.\n\n\n\n\n\n\nKey Insight\n\n\n\nA PGS with auROC of 0.65 correctly ranks a random case above a random control only 65% of the time. At the individual level, this means substantial overlap between cases and controls in their score distributions. Polygenic scores are population stratification tools, not individual diagnostic tests. Their value lies in identifying subgroups warranting different management, not in predicting individual outcomes with certainty.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-portability",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-portability",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.7 Ancestry, Portability, and Fairness",
    "text": "3.7 Ancestry, Portability, and Fairness\nThe vast majority of GWAS participants have been of European ancestry: as of 2019, approximately 78% of participants were European despite Europeans comprising roughly 16% of the global population (Martin et al. 2019). This historical imbalance has profound consequences for who benefits from polygenic scores and who may be harmed by their limitations. A technology that works well for some populations and poorly for others is not merely incomplete; deployed without appropriate caution, it risks widening existing health disparities rather than narrowing them.\nPopulation structure creates systematic portability challenges that extend beyond GWAS to all genomic models. The sources of this population stratification are detailed in Section 2.2, while systematic analysis of ancestry confounding appears in ?sec-ch22-ancestry-confounding. Training data diversity requirements for foundation models that generalize across populations are discussed in Section 2.9.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore examining the portability data below, make a prediction: A polygenic score for coronary artery disease was developed using 500,000 European-ancestry individuals. When applied to African-ancestry individuals, what level of performance would you expect relative to Europeans? Consider what you learned about:\n\nLinkage disequilibrium patterns differing across populations\nThe “78% problem” (78% of GWAS participants are European)\nTag variants that work in one population but not another\n\nWould you expect: (A) similar performance (90-100%), (B) modest reduction (70-90%), (C) substantial reduction (40-70%), or (D) near-complete failure (&lt;40%)?\n\n\n\n\n\n\n\n\nPolygenic score portability across ancestry groups\n\n\n\n\nFigure 3.5: Portability of polygenic scores across ancestry groups. Bar heights show relative prediction accuracy when European-derived polygenic scores are applied to individuals of different ancestries, with European performance as the reference (100%). Accuracy declines substantially for non-European populations, with African-ancestry individuals experiencing 40-75% reductions in predictive power. Multiple factors contribute to this portability gap: differences in linkage disequilibrium structure between populations, divergent allele frequencies that alter variant informativeness, potential heterogeneity in effect sizes across genetic backgrounds, and the overwhelming European composition of GWAS training data. These disparities raise fundamental questions about equitable deployment of polygenic risk prediction in diverse clinical populations.\n\n\n\n\n3.7.1 Portability Problem\nPolygenic scores derived from European-ancestry GWAS show markedly reduced performance in other populations. African-ancestry individuals typically experience 40% to 75% reductions in prediction accuracy compared to European-ancestry individuals, even for the same trait measured in the same study (Duncan et al. 2019; Martin et al. 2019). The pattern holds across traits and across methods, though the magnitude varies with genetic architecture and the degree of shared causal variants.\n\n\n\n\n\n\nStop and Think\n\n\n\nYou’re a genetic counselor and a patient of African ancestry receives a direct-to-consumer polygenic score placing them at the 85th percentile for type 2 diabetes risk. The score was derived from European GWAS data. Given what you now know about portability, what concerns would you have about interpreting this result? What would you tell the patient about the reliability of this percentile ranking?\n\n\nSeveral factors contribute to this portability failure. LD structure differs across populations: tag SNPs that effectively proxy causal variants in Europeans may be poor proxies in populations with different recombination history. Allele frequencies differ: variants common in one population may be rare or absent in another. Effect sizes may genuinely differ across populations due to gene-environment interactions or genetic background effects. And GWAS in smaller non-European samples have less power to detect associations, yielding noisier effect estimates that further degrade prediction.\nMulti-ancestry GWAS and methods designed to leverage diverse training data offer partial solutions. Including multiple ancestries in discovery improves transferability, and methods that explicitly model ancestry-specific LD or effect sizes can enhance performance (Márquez-Luna et al. 2017). Yet even state-of-the-art approaches do not fully close the gap, and substantial research is needed before PGS perform equitably across populations.\n\n\n\n\n\n\nPractical Guidance: Evaluating PGS Across Populations\n\n\n\nWhen assessing whether a polygenic score is appropriate for a given patient or population:\n\nCheck training ancestry: Was the discovery GWAS conducted in a population matching the patient’s ancestry?\nLook for validation data: Has the score been validated in diverse populations? What was the performance gap?\nConsider clinical context: Would reduced accuracy change clinical utility? A score with 50% reduced accuracy may still provide useful stratification for some applications.\nAvoid false precision: Do not report exact percentile rankings in populations where the score was not validated.\nDocument limitations: Explicitly note when scores are applied outside their validated populations.\n\n\n\n\n\n3.7.2 Fairness and Health Equity\nThe performance gap across ancestries is not merely a technical nuisance; it raises fundamental questions about fairness in precision medicine. If genomic models work primarily for individuals of European ancestry, deploying these models in diverse clinical populations risks exacerbating existing health disparities rather than ameliorating them. The communities historically excluded from genetic research would continue to receive inferior genomic medicine, now encoded in algorithmic form.\nConsider a scenario where PGS are used for risk-stratified screening. If the score identifies high-risk individuals more accurately in Europeans than in other groups, Europeans receive more targeted and efficient screening while others receive either under-screening (if falsely classified as low risk) or over-screening (if falsely classified as high risk). The benefits of precision medicine accrue disproportionately to those already overrepresented in research, while the costs of miscalibration fall on those historically excluded.\nThese challenges extend beyond PGS to every genomic model. Foundation models can learn to exploit ancestry signals as shortcuts, achieving high benchmark performance while performing poorly on underrepresented groups. Aggregate performance metrics mask inequities across populations. Deployment in diverse clinical settings requires explicit evaluation of performance stratified by ancestry, along with transparent reporting of limitations and appropriate caution in populations where validation is limited. These issues receive comprehensive treatment in Chapter 12, with governance and policy responses addressed in Section 26.1.\n\n\n\n\n\n\nThe 78 Percent Problem\n\n\n\nAs of 2019, approximately 78% of GWAS participants were of European ancestry despite Europeans comprising roughly 16% of the global population (Martin et al. 2019). This disparity propagates through every layer of genomic medicine. Polygenic scores derived from European-ancestry GWAS show 40-75% reductions in prediction accuracy for African-ancestry individuals, even for the same trait measured in the same study. Variant databases like ClinVar contain far more pathogenic classifications for European-ancestry variants, leaving variants from underrepresented populations more likely to remain classified as VUS due to insufficient evidence. Foundation models inherit these biases at the root: a model trained on skewed data cannot be corrected post hoc to achieve the performance it would have achieved with representative training data.\nThe disparity is not merely statistical. If genomic risk scores are used for screening decisions, Europeans receive more accurate risk stratification while other populations receive either under-screening (if falsely classified as low risk) or over-screening (if falsely classified as high risk). The benefits of precision medicine accrue disproportionately to those already overrepresented in research.\nThis problem recurs throughout the book: in variant effect prediction (Chapter 17), model confounding (Chapter 12), clinical risk prediction (Chapter 27), and the governance frameworks needed to address it (Section 26.1) (chen_rates_2023?).",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-phewas",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-phewas",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.8 Phenome-Wide Association Studies",
    "text": "3.8 Phenome-Wide Association Studies\nGWAS answer a specific question: which variants associate with this phenotype? The reverse question is equally informative: which phenotypes associate with this variant, or with this set of variants aggregated into a polygenic score? Phenome-wide association studies (PheWAS) systematically test associations between genetic variants and hundreds or thousands of phenotypes, revealing pleiotropy that single-phenotype analyses cannot detect. A variant initially discovered for its association with coronary artery disease may also associate with type 2 diabetes, lipid levels, and blood pressure, connections that illuminate shared biology and inform variant interpretation.\nThis reversal of the GWAS paradigm has proven particularly valuable for understanding polygenic score biology. A polygenic score constructed for one trait often predicts other traits, sometimes to a surprising degree. The coronary artery disease PGS predicts not only heart attacks but also diabetes, hypertension, and mortality from other vascular causes. These cross-phenotype associations reflect the shared genetic architecture among related traits and the pleiotropic effects of common variants. They also reveal where phenotype definitions may be capturing overlapping constructs or where biological pathways connect seemingly distinct outcomes.\n\n3.8.1 PheWAS Framework\nPheWAS implementations parallel GWAS but with dimensions transposed. Rather than testing millions of variants against one phenotype, PheWAS tests one variant (or score) against hundreds of phenotypes. The phenotype vocabulary typically derives from EHR codes grouped into clinically meaningful categories. Phecodes collapse ICD-9 and ICD-10 billing codes into approximately 1,800 phenotype groups, aggregating related codes (such as the many ICD codes for diabetes mellitus) into unified disease concepts while distinguishing diseases that occupy nearby code ranges but represent different conditions.\nThe statistical framework mirrors GWAS: logistic regression for binary phenotypes, linear regression for quantitative traits, adjustment for covariates including age, sex, and genetic ancestry. Multiple testing correction accounts for the hundreds of tests performed; the Bonferroni threshold at \\(1{,}800\\) phecodes requires \\(p &lt; 2.8 \\times 10^{-5}\\) for significance. False discovery rate control offers a less conservative alternative appropriate when characterizing the landscape of associations rather than declaring individual findings.\nInterpretation requires attention to the correlation structure among phenotypes. A variant associated with obesity will, by mechanical consequence, associate with any phenotype more common in obese individuals. True pleiotropy (the variant affecting multiple traits through independent biological pathways) cannot be distinguished from mediated pleiotropy (the variant affecting one trait that causes others) through PheWAS alone. Colocalization analysis, conditional testing, and Mendelian randomization provide complementary evidence about whether associations reflect shared causal variants or confounded correlations.\n\n\n3.8.2 PheWAS for Polygenic Score Interpretation\nSingle variants have modest effects on complex traits, limiting the power of variant-level PheWAS for common diseases. Polygenic scores aggregate these effects across thousands of variants, providing sufficient signal for phenome-wide characterization. PRS-PheWAS tests the association between a polygenic score and each phenotype in the vocabulary, revealing the full spectrum of traits that share genetic architecture with the index phenotype.\nXu et al. applied this framework to interpret EHR-embedding-based polygenic scores, finding that scores derived from cardiovascular-related embedding dimensions associated strongly with circulatory system diagnoses across the phenome (Xu et al. 2025). The PheWAS results explained why cardiovascular traits showed the largest improvements from embedding-enhanced prediction: the embeddings captured genetic signal shared across the cardiovascular phenotype cluster. This approach provides a systematic method for understanding what a polygenic score actually predicts and whether its cross-phenotype associations match biological expectations.\nPRS-PheWAS also reveals unexpected associations that may indicate shared biology, confounding, or phenotype definition artifacts. A diabetes PGS that associates with billing codes for insulin pumps reflects healthcare utilization rather than disease biology. A depression PGS that associates with chronic pain diagnoses may indicate shared genetic liability, diagnostic conflation, or the medical consequences of depression. Distinguishing these possibilities requires domain knowledge and follow-up analyses that the PheWAS itself cannot provide.\nPRS-PheWAS for clinical interpretation of polygenic scores is examined in detail in Section 27.4.3, where phenome-wide associations inform which traits share genetic architecture and can benefit from multi-trait prediction.\n\n\n3.8.3 Phenotype Quality and PheWAS Power\nThe power of PheWAS depends critically on phenotype quality, which varies enormously across the hundreds of conditions in a typical phecode vocabulary. Well-captured phenotypes with clear diagnostic criteria (type 2 diabetes, hypothyroidism) yield stronger associations than poorly captured phenotypes that depend on documentation practices (chronic fatigue syndrome, fibromyalgia). Rare phenotypes with few cases lack power regardless of effect size. Common phenotypes with high misclassification rates suffer attenuated effects.\nThis heterogeneity complicates interpretation of phenome-wide results. The absence of an association may reflect genuine lack of pleiotropy or insufficient power to detect it. The pattern of associations across phenotype categories may reflect genuine biological clustering or differential phenotype quality across clinical domains. Cardiovascular phenotypes in hospital-based EHRs are typically well-captured because they drive admissions and procedures; psychiatric phenotypes are poorly captured because they are often managed in outpatient settings that may not feed into the research EHR.\nRecognition of these limitations has motivated phenotype quality assessment as a prerequisite for PheWAS. Metrics such as the proportion of cases with supporting laboratory values, the consistency of coding over time, and the agreement between algorithmic definitions and chart review provide evidence about which phenotypes can support reliable association testing. Restricting analyses to high-quality phenotypes improves specificity at the cost of comprehensiveness.\nThe phenotype quality challenges described here recur in multi-omic integration (?sec-ch19-clinical-integration) and clinical deployment of risk models (?sec-ch25-evaluation). Systematic approaches to phenotype quality assessment connect to label quality discussions in ?sec-ch22-label-bias.\n\n\n3.8.4 Deep Phenotyping and Embedding-Enhanced GWAS\nThe limitations of binary phecode phenotypes have motivated alternative approaches that leverage richer phenotypic representations. Rather than testing association with categorical disease labels, these methods test association with continuous phenotypic embeddings that capture clinical similarity and co-occurrence structure. A patient’s position in embedding space reflects their full clinical profile rather than the presence or absence of specific diagnoses.\nEHR-embedding GWAS treats each dimension of a phenotype embedding as a quantitative trait, conducting standard GWAS to identify variants associated with that dimension. Xu et al. found that such embedding dimensions show significant heritability and genetic correlation with diverse clinical traits, suggesting they capture biologically meaningful phenotypic variation (Xu et al. 2025). Hierarchical clustering of traits by their genetic correlation profiles with embedding dimensions recovered clinically coherent groups, including a cardiovascular cluster comprising coronary artery disease, ischemic stroke, peripheral artery disease, type 2 diabetes, and related conditions.\nThese embedding-based approaches offer several advantages over traditional phenotype definitions. They avoid the information loss inherent in binary case-control classification. They capture phenotypic relationships that expert-defined definitions may miss. They can be constructed from the same EHR data used for genotype-phenotype analysis, requiring no additional phenotyping effort. Their limitations include interpretability (what does association with embedding dimension 7 mean biologically?) and potential circularity (if embeddings capture coding practices, GWAS may identify variants associated with healthcare utilization rather than disease) (Mukherjee et al. 2024).\nThe integration of phenotype embeddings with polygenic prediction represents an active research frontier. Embedding-enhanced polygenic risk scores combine traditional single-trait scores with scores derived from EHR-embedding GWAS, leveraging the genetic correlations among related phenotypes to improve prediction for the target trait. For cardiovascular outcomes where phenotypes cluster together and share genetic architecture, this integration has shown substantial improvements over single-trait scores. The approach is examined in detail in Section 27.3 for feature extraction strategies and Section 27.4 for EHR embedding approaches.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch03-gwas.html#sec-ch03-mechanism",
    "href": "part_1/p1-ch03-gwas.html#sec-ch03-mechanism",
    "title": "3  GWAS and Polygenic Scores",
    "section": "3.9 From Association to Mechanism",
    "text": "3.9 From Association to Mechanism\nGWAS and polygenic scores have delivered thousands of robust trait associations, clinically useful risk stratification for some conditions, and fundamental insights into the polygenic architecture of complex phenotypes. They have also exposed a persistent gap between statistical association and biological understanding. Most GWAS hits lie in noncoding regions, often within enhancers, promoters, or other regulatory elements. The variant is associated; the mechanism is obscure. Fine-mapping narrows the list of candidates but rarely identifies a single causal nucleotide with confidence. Even when a variant is prioritized, the path from sequence change to molecular consequence to cellular phenotype to disease remains opaque.\nThis mechanistic gap limits translation in concrete ways. Drug development requires actionable targets, not associated regions. Clinical variant interpretation needs to explain why a variant matters, not just that it correlates with disease. Polygenic scores stratify population risk but offer little guidance on individual intervention. Multiple complementary strategies address this gap: regulatory sequence models predict how variants alter transcription factor binding and chromatin accessibility (Chapter 16), variant effect predictors assess functional impact at nucleotide resolution (Chapter 17), and multi-omics integration approaches connect genetic variation to intermediate molecular phenotypes (Chapter 22). Network-based approaches that integrate GWAS results with protein interaction networks and pathway information are examined in ?sec-ch18-disease-gene for gene prioritization and ?sec-ch18-drug-target for therapeutic target identification.\nThe goal is not to replace statistical genetics but to build on it. Association provides the map of where trait-relevant variation resides; mechanistic modeling attempts to explain how that variation produces its effects. The combination of statistical association and mechanistic interpretation offers the most promising path toward genomic medicine that is both predictive and understood.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhy does linkage disequilibrium (LD) create a fundamental gap between statistical association and biological causation in GWAS?\nWhat are the three layers of heritability (pedigree, SNP-heritability, missing heritability), and what does each represent?\nHow does the C+T (clumping and thresholding) method construct polygenic scores, and what is its main limitation?\nWhy do European-ancestry polygenic scores perform substantially worse in other populations, and what does the “78% problem” refer to?\nWhat is the purpose of fine-mapping, and what do credible sets represent?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n1. LD and the association-causation gap: Linkage disequilibrium means nearby variants are inherited together in correlated haplotype blocks. When a causal variant affects a trait, all variants in high LD with it will also show statistical association—even if they have no functional role. GWAS cannot distinguish the causal variant from correlated “tag” variants based on association statistics alone, creating fundamental ambiguity about which variants actually matter biologically.\n2. Three layers of heritability: - Pedigree heritability (h²): Estimated from family resemblance (e.g., twin studies), represents the total proportion of phenotypic variance attributable to genetic factors (~80% for height). This is the upper bound. - SNP-heritability (h²_SNP): Variance explained by common variants on genotyping arrays (~50-60% for height). This sets the ceiling for what common-variant polygenic scores can achieve. - Missing heritability: The gap between pedigree and SNP-heritability, representing genetic variation not captured by common SNPs (rare variants, structural variants, poorly tagged regions).\n3. C+T method and limitation: C+T selects the most significant variant at each locus, removes correlated variants within a window (clumping), applies a p-value threshold, and uses GWAS effect sizes as weights. The main limitation is massive information loss—it discards thousands of sub-threshold variants that collectively explain substantial variance, particularly problematic for highly polygenic traits.\n4. Portability failure and the 78% problem: Polygenic scores fail to transfer across ancestries primarily because LD patterns differ between populations—a tag SNP that works in Europeans may not correlate with the causal variant in African-ancestry populations where LD blocks are shorter. Allele frequency differences and effect heterogeneity also contribute. The “78% problem” refers to the fact that ~78% of GWAS participants have been European despite Europeans comprising only ~16% of the global population, creating systematic bias in which variants are discovered and weighted.\n5. Fine-mapping purpose and credible sets: Fine-mapping moves from “this region is associated” to “these specific variants are most likely causal” by modeling how variants behave jointly given LD structure. A 95% credible set is the smallest set of variants with cumulative posterior probability ≥0.95 of containing the true causal variant(s). Small credible sets (1-5 variants) are actionable for functional follow-up; large sets indicate unresolvable ambiguity due to tight LD.\n\n\n\n\n\nKey concepts covered:\n\nGenome-wide association studies (GWAS), SNPs, effect sizes, odds ratios\nPopulation structure and PCA-based correction\nHeritability (pedigree, SNP-heritability, missing heritability)\nLinkage disequilibrium (LD), r2, tag vs. causal variants\nFine-mapping, posterior inclusion probabilities, credible sets\nPolygenic scores (PGS/PRS), C+T, LDpred, PRS-CS\nPortability, ancestry bias, the 78% problem\nPheWAS and phenome-wide characterization\n\nMain takeaways:\n\nGWAS identify signposts, not causes. Association signals point to genomic regions harboring trait-relevant variation, but linkage disequilibrium prevents identification of specific causal variants from statistics alone.\nHeritability has layers. Pedigree heritability sets the upper bound on genetic prediction; SNP-heritability sets the ceiling for common-variant polygenic scores; the gap between them represents genetic variation (rare variants, structural variants) that GWAS cannot capture.\nPolygenic scores aggregate risk across thousands of variants. For some conditions, individuals in the top percentiles have risk comparable to carriers of high-penetrance monogenic mutations, making PGS potentially clinically relevant.\nPortability is a fundamental challenge. European-derived PGS perform substantially worse in other populations due to LD differences, allele frequency differences, and training data bias. This creates equity concerns for clinical deployment.\nThe path to mechanism requires additional tools. Foundation models for regulatory sequence and variant effect prediction (Chapter 16, Chapter 17) attempt to bridge the gap between statistical association and biological understanding.\n\nLooking ahead: The statistical framework presented here provides the foundation for understanding how deep learning models can contribute to genetic analysis. Chapter 4 examines how classical approaches predict variant effects, setting the stage for foundation model approaches in later chapters.\n\n\n\n\n\n\nBenner, Christian, Chris C. A. Spencer, Aki S. Havulinna, Veikko Salomaa, Samuli Ripatti, and Matti Pirinen. 2016. “FINEMAP: Efficient Variable Selection Using Summary Data from Genome-Wide Association Studies.” Bioinformatics 32 (10): 1493–1501. https://doi.org/10.1093/bioinformatics/btw018.\n\n\nBuniello, Annalisa, Daniel Suveges, Carlos Cruz-Castillo, Manuel Bernal Llinares, Helena Cornu, Irene Lopez, Kirill Tsukanov, et al. 2025. “Open Targets Platform: Facilitating Therapeutic Hypotheses Building in Drug Discovery.” Nucleic Acids Research 53 (D1): D1467–75. https://doi.org/10.1093/nar/gkae1128.\n\n\nChoi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O’Reilly. 2020. “[PRS] Tutorial: A Guide to Performing Polygenic Risk Score Analyses.” Nature Protocols 15 (9): 2759–72. https://doi.org/10.1038/s41596-020-0353-1.\n\n\nDuncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R. Peterson, and B. Domingue. 2019. “Analysis of Polygenic Risk Score Usage and Performance in Diverse Human Populations.” Nature Communications 10 (1): 3328. https://doi.org/10.1038/s41467-019-11112-0.\n\n\nElks, Cathy E., Marcel Den Hoed, Jing Hua Zhao, Stephen J. Sharp, Nicholas J. Wareham, Ruth J. F. Loos, and Ken K. Ong. 2012. “Variability in the Heritability of Body Mass Index: A Systematic Review and Meta-Regression.” Frontiers in Endocrinology 3 (February). https://doi.org/10.3389/fendo.2012.00029.\n\n\nGe, Tian, Chia-Yen Chen, Yang Ni, Yen-Chen Anne Feng, and Jordan W. Smoller. 2019. “Polygenic Prediction via Bayesian Regression and Continuous Shrinkage Priors.” Nature Communications 10 (1): 1776. https://doi.org/10.1038/s41467-019-09718-5.\n\n\nHilker, Rikke, Dorte Helenius, Birgitte Fagerlund, Axel Skytthe, Kaare Christensen, Thomas M. Werge, Merete Nordentoft, and Birte Glenthøj. 2018. “Heritability of Schizophrenia and Schizophrenia Spectrum Based on the Nationwide Danish Twin Register.” Biological Psychiatry, Novel Mechanisms in Schizophrenia Pathophysiology, 83 (6): 492–98. https://doi.org/10.1016/j.biopsych.2017.08.017.\n\n\nHormozdiari, Farhad, Emrah Kostem, Eun Yong kang, Bogdan Pasaniuc, and Eleazar Eskin. 2014. “Identifying Causal Variants at Loci with Multiple Signals of Association.” In Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics, 610–11. BCB ’14. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2649387.2660800.\n\n\nIonita-Laza, Iuliana, Kenneth McCallum, Bin Xu, and Joseph D. Buxbaum. 2016. “A Spectral Approach Integrating Functional Genomic Annotations for Coding and Noncoding Variants.” Nature Genetics 48 (2): 214–20. https://doi.org/10.1038/ng.3477.\n\n\nKhera, Amit V., and Sekar Kathiresan. 2017. “Genetics of Coronary Artery Disease: Discovery, Biology and Clinical Translation.” Nature Reviews Genetics 18 (6): 331–44. https://doi.org/10.1038/nrg.2016.160.\n\n\nKichaev, Gleb, Megan Roytman, Ruth Johnson, Eleazar Eskin, Sara Lindström, Peter Kraft, and Bogdan Pasaniuc. 2017. “Improved Methods for Multi-Trait Fine Mapping of Pleiotropic Risk Loci.” Bioinformatics 33 (2): 248–55. https://doi.org/10.1093/bioinformatics/btw615.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory M. Cooper, and Jay Shendure. 2014. “A General Framework for Estimating the Relative Pathogenicity of Human Genetic Variants.” Nature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nLambert, Samuel A, Gad Abraham, and Michael Inouye. 2019. “Towards Clinical Utility of Polygenic Risk Scores.” Human Molecular Genetics 28 (R2): R133–42. https://doi.org/10.1093/hmg/ddz187.\n\n\nMaller, Julian B., Gilean McVean, Jake Byrnes, Damjan Vukcevic, Kimmo Palin, Zhan Su, Joanna M. M. Howson, et al. 2012. “Bayesian Refinement of Association Signals for 14 Loci in 3 Common Diseases.” Nature Genetics 44 (12): 1294–1301. https://doi.org/10.1038/ng.2435.\n\n\nManolio, Teri A., Francis S. Collins, Nancy J. Cox, David B. Goldstein, Lucia A. Hindorff, David J. Hunter, Mark I. McCarthy, et al. 2009. “Finding the Missing Heritability of Complex Diseases.” Nature 461 (7265): 747–53. https://doi.org/10.1038/nature08494.\n\n\nMárquez-Luna, Carla, Po-Ru Loh, South Asian Type 2 Diabetes (SAT2D) Consortium, The SIGMA Type 2 Diabetes Consortium, and Alkes L. Price. 2017. “Multiethnic Polygenic Risk Scores Improve Risk Prediction in Diverse Populations.” Genetic Epidemiology 41 (8): 811–23. https://doi.org/10.1002/gepi.22083.\n\n\nMartin, Alicia R., Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada, Benjamin M. Neale, and Mark J. Daly. 2019. “Clinical Use of Current Polygenic Risk Scores May Exacerbate Health Disparities.” Nature Genetics 51 (4): 584–91. https://doi.org/10.1038/s41588-019-0379-x.\n\n\nMavaddat, Nasim, Kyriaki Michailidou, Joe Dennis, Michael Lush, Laura Fachal, Andrew Lee, Jonathan P. Tyrer, et al. 2019. “Polygenic Risk Scores for Prediction of Breast Cancer and Breast Cancer Subtypes.” The American Journal of Human Genetics 104 (1): 21–34. https://doi.org/10.1016/j.ajhg.2018.11.002.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy Schwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021. “An Open Approach to Systematically Prioritize Causal Variants and Genes at All Published Human GWAS Trait-Associated Loci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nMukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom Soare, Raghav Tandon, David Amar, et al. 2024. “EmbedGEM: A Framework to Evaluate the Utility of Embeddings for Genetic Discovery.” Bioinformatics Advances 4 (1). https://doi.org/10.1093/bioadv/vbae135.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006. “Population Structure and Eigenanalysis.” PLOS Genetics 2 (12): e190. https://doi.org/10.1371/journal.pgen.0020190.\n\n\nPe’er, Itsik, Roman Yelensky, David Altshuler, and Mark J. Daly. 2008. “Estimation of the Multiple Testing Burden for Genomewide Association Studies of Nearly All Common Variants.” Genetic Epidemiology 32 (4): 381–85. https://doi.org/10.1002/gepi.20303.\n\n\nPrice, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E. Weinblatt, Nancy A. Shadick, and David Reich. 2006. “Principal Components Analysis Corrects for Stratification in Genome-Wide Association Studies.” Nature Genetics 38 (8): 904–9. https://doi.org/10.1038/ng1847.\n\n\nQuang, Daniel, Yifei Chen, and Xiaohui Xie. 2015. “DANN: A Deep Learning Approach for Annotating the Pathogenicity of Genetic Variants.” Bioinformatics 31 (5): 761–63. https://doi.org/10.1093/bioinformatics/btu703.\n\n\nRisch, Neil, and Kathleen Merikangas. 1996. “The Future of Genetic Studies of Complex Human Diseases.” Science 273 (5281): 1516–17. https://doi.org/10.1126/science.273.5281.1516.\n\n\nTorkamani, Ali, Nathan E. Wineinger, and Eric J. Topol. 2018. “The Personal and Clinical Utility of Polygenic Risk Scores.” Nature Reviews Genetics 19 (9): 581–90. https://doi.org/10.1038/s41576-018-0018-x.\n\n\nVilhjálmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev, Sara Lindström, Stephan Ripke, Giulio Genovese, et al. 2015. “Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores.” American Journal of Human Genetics 97 (4): 576–92. https://doi.org/10.1016/j.ajhg.2015.09.001.\n\n\nVisscher, Peter M., William G. Hill, and Naomi R. Wray. 2008. “Heritability in the Genomics Era — Concepts and Misconceptions.” Nature Reviews Genetics 9 (4): 255–66. https://doi.org/10.1038/nrg2322.\n\n\nWang, Gao, Abhishek Sarkar, Peter Carbonetto, and Matthew Stephens. 2020. “A Simple New Approach to Variable Selection in Regression, with Application to Genetic Fine Mapping.” Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (5): 1273–1300. https://doi.org/10.1111/rssb.12388.\n\n\nWeissbrod, Omer, Farhad Hormozdiari, Christian Benber, Roeland Buber, Steven Gazal, Ross Dann, Po-Ru Loh, et al. 2020. “Functionally Informed Fine-Mapping and Polygenic Localization of Complex Trait Heritability.” Nature Genetics 52 (12): 1355–63. https://doi.org/10.1038/s41588-020-00735-5.\n\n\nXu, Leqi, Wangjie Zheng, Jiaqi Hu, Yingxin Lin, Jia Zhao, Gefei Wang, Tianyu Liu, and Hongyu Zhao. 2025. “Improving Polygenic Risk Prediction Performance by Integrating Electronic Health Records Through Phenotype Embedding.” The American Journal of Human Genetics 112 (12): 3030–45. https://doi.org/10.1016/j.ajhg.2025.11.006.\n\n\nYang, Jian, Beben Benyamin, Brian P. McEvoy, Scott Gordon, Anjali K. Henders, Dale R. Nyholt, Pamela A. Madden, et al. 2010. “Common SNPs Explain a Large Proportion of the Heritability for Human Height.” Nature Genetics 42 (7): 565–69. https://doi.org/10.1038/ng.608.\n\n\nYengo, Loïc, Sailaja Vedantam, Eirini Marouli, Julia Sidorenko, Eric Bartell, Saori Sakaue, Marielisa Graff, et al. 2022. “A Saturated Map of Common Genetic Variants Associated with Human Height.” Nature 610 (7933): 704–12. https://doi.org/10.1038/s41586-022-05275-y.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GWAS and Polygenic Scores</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html",
    "href": "part_1/p1-ch04-vep-classical.html",
    "title": "4  Classical Variant Prediction",
    "section": "",
    "text": "4.1 Conservation-Based Approaches\nA clinical geneticist evaluating a novel intronic variant faces an immediate problem: no functional annotation exists for most of the genome, and no clinical database has seen this specific change before. The variant lies outside any protein-coding region, no regulatory element overlaps it, and the patient’s phenotype offers no clear mechanistic hypothesis. Yet one source of information spans the entire genome and predates any experimental annotation by billions of years. If a genomic position has remained unchanged across species separated by hundreds of millions of years of evolution, mutations at that position are likely to be deleterious. Natural selection has already performed the largest functional screen imaginable, running experiments across countless organisms over evolutionary time, and conservation scores quantify the results.\nConservation signals derive from the population variant catalogs described in Section 2.2.3, where patterns of variation across populations reveal which positions tolerate change. The variant calling quality from Section 1.3 directly affects which variants appear in constraint calculations, potentially biasing constraint metrics in regions with systematic calling errors.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-conservation",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-conservation",
    "title": "4  Classical Variant Prediction",
    "section": "",
    "text": "Stop and Think\n\n\n\nBefore reading further, consider: if a position shows the same nucleotide across humans, mice, chickens, and fish (species that diverged hundreds of millions of years ago), what can you infer about that position? What assumptions does this inference require?\n\n\n\n\n\n\n\n\n\nDeep Dive: Variant Types by Coding Effect\n\n\n\nFor ML readers: Variants in protein-coding regions are classified by their effect on the amino acid sequence:\nSynonymous (Silent): The nucleotide change does not alter the amino acid due to genetic code redundancy (e.g., both GGU and GGC encode glycine). Often assumed neutral, but can affect:\n\nSplicing (exonic splice enhancers/silencers)\nmRNA stability and structure\nTranslation efficiency (codon usage bias)\n\nNonsynonymous (Missense): The change substitutes one amino acid for another (e.g., valine → glutamate). Effect depends on:\n\nChemical similarity of amino acids\nPosition in protein structure\nFunctional importance of the residue\n\nNonsense: Creates a premature stop codon, truncating the protein. Usually loss-of-function, but may escape nonsense-mediated decay if near the last exon.\nFrameshift: Insertion or deletion not divisible by 3, shifting the reading frame. Typically severe loss-of-function.\n\n\n\nVariant Type\nEffect\nTypical Consequence\n\n\n\n\nSynonymous\nSame amino acid\nUsually neutral; can affect splicing\n\n\nMissense\nDifferent amino acid\nVariable; depends on context\n\n\nNonsense\nPremature stop\nLoss-of-function\n\n\nFrameshift\nAltered sequence\nLoss-of-function\n\n\n\nKey insight: Protein-level predictors (SIFT, PolyPhen) only assess missense variants. Other types require different prediction approaches.\n\n\n\n\n\n\n\n\n\n\nMultiple sequence alignment revealing conservation\n\n\n\n\n\n\n\nGenome-wide distribution of phyloP scores\n\n\n\n\n\n\n\nConservation provides evidence in annotation-sparse regions\n\n\n\n\n\n\nFigure 4.2: Evolutionary conservation as a proxy for functional importance. (A) Multiple sequence alignment comparing a highly conserved position (left, same nucleotide across species) with a neutrally evolving position (right, variable nucleotides). Phylogenetic tree indicates evolutionary relationships. (B) Genome-wide distribution of phyloP scores, with most positions near zero (neutral) and a long right tail representing constrained elements. Positions with scores above 2 show strong evidence of purifying selection. (C) Genome browser view of an intronic variant with no regulatory annotations but deep conservation, illustrating how evolutionary signal provides evidence of function in annotation-sparse regions.\n\n\n\n\n4.1.1 Evolutionary Constraint Metrics\nA clinical report arrives stating that a variant has “phyloP = 3.2” and “GERP = 4.5”—but what do these numbers actually mean? Without understanding the quantitative framework behind conservation scores, these values are opaque. The challenge is translating the qualitative intuition that “conservation implies function” into precise numerical values that can be compared across variants and combined with other evidence.\nThe logic of conservation is straightforward: if a position matters for survival or reproduction, mutations there will be removed by selection before they can spread through the population. Quantifying this signal requires comparing sequences across species to identify positions where substitutions occur less frequently than expected under neutral evolution. Conservation scores translate this evolutionary signal into numerical values that can inform variant interpretation.\nPhyloP scores quantify the deviation of observed substitution rates from neutral expectation at individual positions (Siepel et al. 2005). The score is computed by comparing the observed pattern of bases at each alignment column against a neutral evolutionary model (typically fit to ancestral repeat sequences that are assumed to evolve without selective constraint). The choice of ancestral repeats for calibrating the neutral model is deliberate: these sequences arose from transposon insertions that have lost their original function and now accumulate mutations at rates reflecting only mutation pressure, without selective filtering. This provides a baseline against which constraint at functional positions can be measured. Positive phyloP scores indicate conservation, meaning evolution is slower than expected under neutrality. Negative scores indicate acceleration, suggesting faster evolution that may reflect positive selection. A phyloP score of 2 indicates that the observed base is approximately 100-fold more conserved than expected under neutrality, providing strong evidence that mutations at this position have been systematically removed by selection.\nGERP takes a complementary approach by estimating rejected substitutions at each position: the number of substitutions that would have been expected under neutrality but are absent from the observed alignment (Davydov et al. 2010). Large positive GERP scores indicate strong constraint. For a position conserved across 30 mammalian species, a GERP score of 5 implies that approximately five substitutions were rejected by selection over mammalian evolution. This interpretation connects directly to the biological process of purifying selection but depends on accurate neutral rate estimation and alignment quality.\nphastCons provides element-level rather than position-level conservation by identifying contiguous stretches of constrained sequence (Siepel et al. 2005). Using a hidden Markov model, phastCons classifies each position as belonging to a conserved or non-conserved state, then outputs the posterior probability of conservation. The resulting scores are smoother than position-level metrics, capturing functional elements that span multiple nucleotides even when individual positions show moderate conservation. This element-level view proves particularly valuable for identifying regulatory sequences where the overall constraint matters more than any single nucleotide.\n\n\n\nTable 4.1: Comparison of major conservation metrics\n\n\n\n\n\n\n\n\n\n\n\nMetric\nWhat It Measures\nScale\nInterpretation\n\n\n\n\nphyloP\nDeviation from neutral evolution rate\nContinuous; positive = conserved\nScore of 2 = ~100x slower than neutral\n\n\nGERP\nRejected substitutions\nContinuous; higher = more constrained\nScore of 5 = ~5 expected mutations removed\n\n\nphastCons\nProbability of conserved state\n0-1 probability\nScore of 0.9 = 90% likely constrained element\n\n\n\n\n\n\nConservation scores from phastCons and phyloP serve as foundational features throughout computational genomics. Integrative methods like CADD (Section 4.3) combine conservation with dozens of other annotations to score variant deleteriousness. Fine-mapping algorithms use conservation to weight prior probabilities when prioritizing causal variants within GWAS loci (Section 3.3). Yet conservation measures evolutionary fitness, not disease relevance, and variants pathogenic in humans may be invisible to cross-species constraint. This evolutionary proxy problem creates fundamental limitations for clinical interpretation, examined in Section 4.1.2.\n\n\n4.1.2 What Conservation Measures Versus What Clinicians Need\n\n\n\n\n\n\nKey Insight\n\n\n\nConservation scores answer the question: “Has this position been important for organismal fitness across evolutionary time?” Clinicians need to answer: “Will this variant cause disease in my patient?” These are related but fundamentally different questions.\n\n\nConservation scores measure evolutionary constraint: the degree to which a position has resisted substitution over millions of years. This is not the same as clinical relevance. A position can be evolutionarily constrained for functions unrelated to human disease, or clinically important despite modest conservation. The assumption underlying conservation-based interpretation is that positions under strong constraint are more likely to be functionally important and therefore more likely to cause disease when mutated. This assumption is often correct but not universally so.\nThe clinician wants to know: will this variant cause disease in my patient? Conservation provides indirect evidence: this position has been important for organismal fitness across evolutionary time. The gap between these questions creates interpretive challenges. A variant at a highly conserved position in a gene with no known disease association provides evolutionary evidence of functional importance but no direct path to clinical interpretation. Conversely, a variant at a modestly conserved position in a well-established disease gene may be clinically significant despite weak conservation signal.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA patient has a variant at a position with phyloP = 0.5 (weak conservation) in BRCA1, a well-established breast cancer gene. Another patient has a variant with phyloP = 4.0 (strong conservation) in a gene with no known disease associations. Which variant is more likely to be clinically actionable? What additional information would you need?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe BRCA1 variant is more likely to be clinically actionable despite weak conservation, because gene-disease association trumps position-level constraint when making clinical decisions. Additional information needed includes: variant type (missense vs. nonsense), population frequency, other computational predictions, family history, and whether the variant has been reported in clinical databases like ClinVar.\n\n\n\n\n\n\n\n4.1.3 Clinical Application and Boundaries\nWhen should a clinical laboratory trust a conservation score, and when should they treat it with skepticism? The answer depends on understanding both where conservation signals excel and where they systematically fail. A deep intronic variant with no regulatory annotations but strong conservation demands attention; a variant in an immune gene with weak conservation may still be pathogenic. Knowing when to rely on evolutionary evidence—and when to look elsewhere—separates effective variant interpretation from mechanical score-chasing.\nConservation scores prove particularly valuable for non-coding variant interpretation, where direct functional annotations are often incomplete or absent. A deeply conserved intronic position likely participates in splicing regulation, gene expression control, or other functional processes even if no explicit annotation overlaps it. Under ACMG-AMP guidelines for variant classification, strong conservation provides computational evidence (the PP3 criterion) supporting pathogenicity (Richards et al. 2015). A variant falling at a position with phyloP greater than 2 and GERP greater than 4 carries significantly more weight than one at an unconserved position, even when no other annotation is available. These scores remain central to clinical variant interpretation workflows, as examined in Chapter 27, where they contribute evidence alongside population frequency, functional studies, and segregation data.\nThe boundaries of conservation-based approaches are equally important to recognize, and these boundaries are not merely technical inconveniences but reflect fundamental gaps in what evolutionary signal can reveal. Conservation requires evolutionary time to accumulate signal. Recently evolved functional elements, including human-specific regulatory sequences and primate-specific genes, may show little conservation despite genuine function. A position can be functionally critical in humans yet unconserved because the function arose too recently for selection to leave a detectable signature. The fraction of the human genome showing evidence of human-specific function since the human-chimpanzee split (including thousands of human accelerated regions) presents exactly this challenge: important to human biology, yet invisible to conservation metrics (hubisz_exploring_2014?).\nConservation patterns vary dramatically by functional context. Neural development genes tend to be highly conserved across vertebrates, while immune genes evolve rapidly under positive selection. Critically, lack of conservation does not prove neutrality: a position may be diverging under positive selection, or may serve lineage-specific functions absent in the comparison species.\nConservation scores also face technical challenges from alignment quality. In repetitive regions, segmental duplications, and rapidly evolving gene families, reliable alignments may be impossible to construct, leaving conservation scores undefined or unreliable precisely where variant interpretation is most difficult. The HLA region, immunoglobulin loci, and centromeric sequences are clinically important yet systematically difficult to assess by conservation. The regions most difficult to interpret computationally are frequently those of greatest clinical interest.\nThese boundaries do not diminish the value of conservation; they define where that value applies. Conservation provides information largely orthogonal to population frequency (which reflects recent human history rather than deep evolutionary constraint) and to functional genomics annotations (which capture biochemical activity rather than selective importance). Integrative methods such as CADD combine conservation with these other signals to achieve better performance than any single source. Protein language models, examined in Chapter 15, learn conservation-like signals directly from sequence data without requiring explicit alignments, potentially addressing some technical limitations while introducing their own assumptions about what constitutes functional constraint.\n\n\n\n\n\n\nACMG-AMP Variant Classification Framework\n\n\n\nThe ACMG-AMP framework classifies variants into five clinical tiers: Pathogenic, Likely Pathogenic, Variant of Uncertain Significance (VUS), Likely Benign, and Benign (Richards et al. 2015). Classification combines multiple evidence types at different strength levels (very strong through supporting). Computational predictions occupy the supporting tier through criteria PP3 (predicted damaging) and BP4 (predicted benign), meaning they can contribute to but not independently establish pathogenicity. The full evidence framework, including thresholds for upgrading computational evidence strength, is detailed in Section 17.5.3.\n\n\n\n\n\n\nACMG-AMP five-tier classification framework\n\n\n\n\nFigure 4.3: The ACMG-AMP framework for clinical variant classification. Variants are classified into five tiers based on combined evidence. Computational predictions (PP3/BP4) provide supporting-level evidence, reflecting appropriate caution about their evidentiary weight.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-protein-predictors",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-protein-predictors",
    "title": "4  Classical Variant Prediction",
    "section": "4.2 Protein-Level Predictors",
    "text": "4.2 Protein-Level Predictors\nA diagnostic laboratory receives exome sequencing results for a 45-year-old woman with early-onset breast cancer and a family history suggesting hereditary cancer syndrome. Among hundreds of rare variants, one stands out: a missense change in BRCA2 substituting glycine for arginine at a conserved position. Is this the explanation for her cancer, or an incidental finding? No previous case report exists for this exact variant. No functional assay has tested its effect. The question of whether this amino acid substitution disrupts BRCA2 function determines whether her siblings should be tested and whether she qualifies for PARP inhibitor therapy. The clinical stakes could not be higher, yet the evidence available is entirely computational. Protein-level predictors attempt to answer such questions by encoding biological intuition about which amino acid changes matter.\n\n\n\n\n\n\nStop and Think\n\n\n\nFor a missense variant (amino acid substitution), what properties would you consider when predicting whether it disrupts protein function? Think about the amino acid itself, its position in the protein, and what you could learn from evolution.\n\n\n\n4.2.1 SIFT: Sequence Homology as Functional Constraint\nConservation scores can identify constrained positions, but they cannot distinguish which substitutions at those positions are tolerated. A position might be highly conserved overall yet accept certain amino acid changes that preserve function. For missense variants specifically, the relevant question is not whether the position is constrained but whether the specific amino acid substitution disrupts function. SIFT addresses this distinction by examining which amino acids have been accepted at each position across evolutionary history (Ng and Henikoff 2003).\nSIFT collects homologous protein sequences from diverse species, constructs a multiple sequence alignment, and examines which amino acids appear at each position across the alignment. Positions that are highly conserved (showing the same or similar amino acids across species) are predicted to be functionally important; substitutions introducing amino acids not observed at that position are predicted to be deleterious.\nThe method computes a normalized probability for each possible amino acid at each position based on the diversity observed in the alignment. The SIFT score for a substitution is the probability of observing the mutant amino acid, scaled by the position’s overall diversity. The scaling by position diversity serves a critical purpose: without it, highly variable positions would yield low scores for any substitution simply because no single amino acid dominates, while conserved positions would appear to tolerate nothing. The scaling ensures that scores reflect deviation from position-specific expectations rather than raw conservation. Scores range from 0 to 1, with low scores (typically below 0.05) indicating predicted damage. A SIFT score of 0.01 for a particular missense variant indicates that the mutant amino acid is rarely or never observed at that position across the sequence family, suggesting functional constraint has prevented its fixation throughout evolution.\nWorked Example: Consider a leucine-to-proline substitution at position 285 in a highly conserved enzyme. SIFT examines the alignment of 500 homologous sequences and finds that position 285 contains leucine in 495 sequences, isoleucine in 4, and valine in 1. Proline never appears. The probability of observing proline at this position is essentially zero, yielding a SIFT score near 0, predicting the substitution as deleterious. The biological intuition: leucine sits in an alpha-helix, and proline would break the helix structure.\nSIFT’s simplicity is both its strength and its limitation. The method requires only protein sequence information and a database of homologs; it makes no assumptions about protein structure, physicochemistry, or mechanism of damage. This generality allows application to any protein with sufficient homologs in sequence databases. For proteins with few homologs, young gene families, or positions with limited alignment depth, predictions may be unreliable. The method captures only the evolutionary signal present in the alignment, missing functional constraints that arose recently or affect only a subset of species.\n\n\n4.2.2 PolyPhen-2: Integrating Structure and Sequence\nSIFT’s reliance on sequence alone ignores substantial information about how amino acid substitutions affect protein function. A glycine buried in a protein’s hydrophobic core will disrupt structure differently than one on a surface loop. A substitution at a catalytic site matters more than one far from any functional region. PolyPhen-2 extends sequence-based prediction by incorporating protein structure features and amino acid physicochemistry, recognizing that the same substitution can have different consequences depending on its structural context (Adzhubei et al. 2010).\nThe method uses a naive Bayes classifier trained to distinguish disease-causing mutations from neutral polymorphisms based on a collection of sequence-derived and structure-derived features. The choice of naive Bayes reflects a principled tradeoff: although the “naive” independence assumption between features is biologically unrealistic (conservation and solvent accessibility are correlated), the classifier remains robust when features are redundant and performs well with limited training data. More complex models might overfit to the relatively small sets of annotated disease variants available when PolyPhen-2 was developed. The feature set includes sequence conservation (similar to SIFT) but adds several structural descriptors when three-dimensional structure data is available: solvent accessibility (whether the position is buried or exposed), secondary structure context (alpha-helix, beta-sheet, or random coil), and proximity to known functional sites. Solvent accessibility matters because buried residues must maintain hydrophobic packing to preserve protein stability, while surface-exposed residues face fewer constraints unless they participate in binding interfaces. Amino acid physicochemical properties inform predictions about whether substitutions are conservative or radical. The Grantham distance, a measure of biochemical dissimilarity between amino acid pairs based on composition, polarity, and molecular volume, contributes to assessing substitution severity (grantham_amino_1974?). Where Grantham distance derives from physicochemical properties, BLOSUM matrices capture empirical substitution frequencies observed across evolutionarily related proteins (Henikoff and Henikoff 1992). A glycine-to-arginine substitution (Grantham distance of 125) represents a far more radical change than a leucine-to-isoleucine substitution (Grantham distance of 5).\nPolyPhen-2 provides two models trained on different datasets: HumDiv, trained on disease-causing and neutral variants from protein sequence databases, and HumVar, trained on Mendelian disease mutations versus common human polymorphisms. The choice of training set affects score interpretation; HumVar produces more conservative predictions appropriate for clinical Mendelian disease variant classification, while HumDiv is more sensitive and may be preferable for research applications where missing a true positive is more costly than false positives.\nPolyPhen-2 scores range from 0 to 1, with higher scores indicating greater predicted deleteriousness. The output includes qualitative classifications (benign, possibly damaging, probably damaging) based on score thresholds. A PolyPhen-2 score of 0.95 with a “probably damaging” classification indicates high confidence that the substitution disrupts protein function, though the clinical significance depends on additional evidence about the specific disease context.\n\n\n\nTable 4.2: Comparison of SIFT and PolyPhen-2\n\n\n\n\n\n\n\n\n\n\nAspect\nSIFT\nPolyPhen-2\n\n\n\n\nInput\nSequence only\nSequence + structure (when available)\n\n\nScore range\n0-1 (low = damaging)\n0-1 (high = damaging)\n\n\nThreshold\n&lt; 0.05 = damaging\n&gt; 0.85 = probably damaging\n\n\nStrength\nWorks with alignment alone\nIncorporates structural context\n\n\nLimitation\nMisses structural effects\nRequires structure data for full power\n\n\nUse case\nAny protein with homologs\nBest when structure available\n\n\n\n\n\n\n\n\n4.2.3 From Sequence to Function\nA clinical report states that a variant is “probably damaging” according to PolyPhen-2 and “deleterious” according to SIFT. The patient’s physician asks: “So this variant causes disease?” The answer is more complicated than the scores suggest. A “probably damaging” designation means the substitution likely impairs the protein’s normal function—but whether that impairment causes the patient’s disease depends on factors these tools cannot assess.\nPolyPhen-2, SIFT, and similar tools answer a mechanistic question: does this amino acid substitution disrupt protein function? They assess whether the new residue fits the structural context, whether the position tolerates variation across species, and whether the physicochemical change is drastic. These are genuine molecular insights. A “probably damaging” designation from PolyPhen-2 means the substitution likely impairs the protein’s normal function, but the clinical significance of that impairment requires reasoning that protein-level predictors cannot provide.\nProtein language models like ESM-2 (Section 15.1) provide alternative approaches to variant effect prediction that learn evolutionary constraint from sequence alone, without explicit multiple sequence alignments. ESM-based variant scoring is examined in ?sec-ch14-protein-vep, where zero-shot prediction paradigms avoid the limitations of supervised training on biased clinical labels.\n\n\n4.2.4 Boundaries of Protein-Level Prediction\nSeveral fundamental boundaries constrain all protein-level predictors, and these boundaries are not merely technical inconveniences but reflect deep gaps in what sequence and structure analysis alone can reveal. Protein-level tools are restricted to missense variants; nonsense, frameshift, splice-altering, and non-coding variants lie entirely outside their scope. A patient’s most important variant may be intronic or synonymous, yet protein-level predictors have nothing to say about it. This constraint is absolute: these methods analyze amino acid substitutions and cannot be extended to other variant types without fundamental redesign.\n\n\n\n\n\n\nDeep Dive: Inheritance Patterns\n\n\n\nFor ML readers: How a genetic disease is inherited determines whether one or two mutant copies cause disease:\nAutosomal Dominant: One mutant copy is sufficient to cause disease.\n\nAffected individuals typically have one affected parent\n50% risk of transmission to each child\nExamples: Huntington’s disease, Marfan syndrome, many cancer predisposition syndromes (BRCA1/2)\n\nAutosomal Recessive: Two mutant copies required (one from each parent).\n\nParents are typically unaffected carriers\n25% risk when both parents are carriers\nExamples: Cystic fibrosis, sickle cell disease, phenylketonuria\n\nX-Linked: Gene located on X chromosome.\n\nMales (XY) are typically more severely affected (no backup copy)\nFemales (XX) may be carriers or variably affected\nExamples: Duchenne muscular dystrophy, hemophilia\n\nWhy this matters for variant interpretation:\n\n\n\nInheritance\nHeterozygous (one copy)\nHomozygous (two copies)\n\n\n\n\nDominant\nDisease\nDisease (often more severe)\n\n\nRecessive\nCarrier (healthy)\nDisease\n\n\n\nA “damaging” variant in a recessive disease gene may be clinically benign in a heterozygous carrier. Variant effect predictors cannot distinguish these scenarios—they score molecular impact, not clinical consequence.\n\n\n\n\n\n\n\n\nCommon Misconception: Gain-of-Function Variants\n\n\n\nThe terminology “gain-of-function” can mislead: it sounds beneficial, yet these mutations cause some of the most severe human diseases. A gain-of-function variant does not improve the protein; it creates aberrant activity that the cell cannot regulate. Protein-level predictors designed to detect structural disruption will often miss these variants entirely because the protein remains well-folded and active—it simply does something it should not.\n\n\nProtein-level predictors estimate impact on protein function without specifying the mechanism or clinical consequence. A variant predicted to damage function might impair enzymatic activity, disrupt protein folding, eliminate a binding interface, or alter stability. The clinical relevance depends on which function is affected and whether the phenotype results from loss of function or gain of function. A predicted-damaging variant in a tumor suppressor behaves very differently from one in an oncogene, yet protein-level predictors provide no information about this distinction. The same high score can indicate completely different clinical implications depending on biological context.\nIn FGFR3, gain-of-function mutations cause the receptor to signal constitutively, driving achondroplasia by suppressing bone growth. In PIK3CA, activating mutations produce uncontrolled cell proliferation. In ion channels like SCN1A, gain-of-function can cause epileptic encephalopathy through neuronal hyperexcitability. These variants may preserve or even enhance protein structure and activity, precisely the opposite of what conservation-based predictors flag as damaging. A perfectly folded protein with enhanced catalytic activity will score as “tolerated” by methods designed to detect disruption, yet its clinical consequences may be devastating. Loss-of-function and gain-of-function mutations in the same gene can cause entirely different diseases: loss of RET function causes Hirschsprung disease (failed neural crest migration), while gain of RET function causes multiple endocrine neoplasia (tumor predisposition). Protein-level predictors cannot distinguish these mechanisms because they assess structural perturbation, not the direction of functional change.\nThese tools also provide no information about inheritance mode, penetrance, or expressivity. A strongly predicted-damaging variant in a gene with high tolerance to heterozygous loss may be clinically benign in carriers. Protein-level predictors cannot distinguish between a variant causing severe disease in homozygotes and one causing no disease at all when heterozygous. This distinction becomes critical when counseling families, where the mode of inheritance fundamentally changes recurrence risk and management recommendations.\n\n\n\n\n\n\nDeep Dive: Penetrance and Expressivity\n\n\n\nFor all readers: These clinical genetics terms describe why the same variant causes disease in some carriers but not others:\nPenetrance: The probability that a person carrying a pathogenic variant will actually develop the associated disease.\n\nComplete penetrance (100%): All carriers develop disease (e.g., Huntington’s disease)\nIncomplete/reduced penetrance (&lt;100%): Only some carriers develop disease (e.g., BRCA1 mutations have ~70% lifetime breast cancer risk)\nAge-dependent penetrance: Disease probability increases with age\n\nExpressivity: The range of severity among individuals who do develop disease. Even with complete penetrance, some carriers may have mild symptoms while others have severe manifestations.\nWhy this matters for variant interpretation:\n\nA variant with high predicted deleteriousness may cause no disease if penetrance is low\nUnaffected family members can carry pathogenic variants (reduced penetrance)\nVariant effect predictors measure molecular disruption, not penetrance—a “damaging” score tells you nothing about whether the carrier will develop disease\nPopulation databases (gnomAD) may contain carriers of pathogenic variants who haven’t yet manifested disease or have incomplete penetrance\n\nThe gap: VEP scores predict functional impact; clinical outcomes require additional information about inheritance mode, penetrance, and genetic background.\n\n\nProtein-level predictors inherit the training data biases present in their underlying databases. Disease mutations in training sets are enriched for severe, early-onset Mendelian conditions with clear inheritance patterns. Variants causing subtle effects, incomplete penetrance, or complex phenotypes may be systematically mispredicted. The well-studied genes that dominate training data may not generalize to poorly characterized genes where variants are most difficult to interpret.\nSIFT and PolyPhen-2 remain widely used in clinical practice and serve as features within more sophisticated ensemble methods. Their scores appear in diagnostic reports, contribute to ACMG-AMP classification criteria, and inform variant prioritization in research and clinical pipelines. Understanding their construction and limitations is essential for appropriate interpretation.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-cadd",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-cadd",
    "title": "4  Classical Variant Prediction",
    "section": "4.3 CADD Framework",
    "text": "4.3 CADD Framework\nThe protein-level predictors and conservation scores examined above each capture one aspect of variant function, yet clinical interpretation requires weighing multiple lines of evidence simultaneously. A variant might fall in a conserved region, alter a moderately constrained amino acid, and overlap a predicted enhancer. How should these signals be combined? More fundamentally, how can we train a predictor when curated pathogenic variants number in the hundreds of thousands while the genome contains billions of possible mutations? These questions expose a fundamental tension: the variants we most need to interpret (rare, novel, never before seen) are precisely those for which training labels do not exist.\nCADD addressed these challenges by reframing variant effect prediction as a large-scale machine learning problem (Kircher et al. 2014; Rentzsch et al. 2019). The key insight was not better feature engineering or more sophisticated classification, but rather a reconceptualization of the labeling problem itself. Instead of training directly on small sets of known pathogenic versus benign variants, which are scarce and biased toward certain genes and variant types, CADD contrasts variants that have survived purifying selection in the human lineage with matched simulated variants that could have occurred but did not. This evolutionary proxy strategy transforms the labeling problem, yielding millions of training examples where curated datasets provide thousands.\n\n\n\n\n\n\nKey Insight\n\n\n\nCADD’s breakthrough was not algorithmic sophistication but creative problem reframing. By using evolutionary proxy labels (observed vs. simulated variants) instead of clinical labels, CADD transformed a problem with thousands of training examples into one with millions—at the cost of measuring evolutionary tolerance rather than clinical pathogenicity directly.\n\n\n\n4.3.1 Evolutionary Proxy Training and Label Sources\nThe conceptual foundation of CADD rests on constructing training labels from evolutionary signal rather than clinical curation. The method builds two proxy classes of variants that serve as training labels, each designed to approximate a category that cannot be observed directly. Understanding these label sources is essential because the same proxy labeling strategies reappear throughout genomic machine learning, including in the foundation models discussed in Chapter 13 and Chapter 8.\n\n\n\n\n\n\n\n\nProxy-neutral class from human-derived alleles\n\n\n\n\n\n\n\nProxy-deleterious class from simulated variants\n\n\n\n\n\n\n\nClassification learning distinguishes the two classes\n\n\n\n\n\n\nFigure 4.4: The CADD training paradigm: evolutionary proxy labeling. (A) The proxy-neutral class consists of human-derived alleles that reached high frequency since the human-chimpanzee split, having survived millions of years of purifying selection. (B) The proxy-deleterious class comprises simulated variants matching human mutational processes, representing changes that could occur but are generally not observed at high frequency, enriching for alleles disfavored by selection. (C) A classifier learns to distinguish these classes based on integrated genomic annotations, outputting an ‘evolutionary tolerance’ score. This proxy labeling strategy transforms the variant effect prediction problem from one with scarce clinical labels to one with millions of evolutionary training examples, though the gap between evolutionary tolerance and clinical pathogenicity remains irreducible.\n\n\n\n\n\n\n\n\n\nStop and Think\n\n\n\nCADD trains on “proxy-neutral” variants (observed in humans) versus “proxy-deleterious” variants (simulated but not observed). Why might this be more powerful than training on known pathogenic versus benign variants? What are the tradeoffs?\n\n\nThe proxy-neutral class consists of variants that have been tolerated by purifying selection. CADD draws these from sequence differences that arose on the human lineage since the split from chimpanzees and became fixed or nearly fixed in modern humans. These are identified by their derived allele frequency: alleles that differ from the inferred ancestral state (typically determined by comparison to chimpanzee and other great ape sequences) and are present at very high frequency in human populations. Because these derived alleles have persisted over millions of years of evolution, most are presumed to be neutral or only weakly deleterious. This is not a perfect proxy: some observed alleles are genuinely pathogenic, particularly those with incomplete penetrance, late onset, or context-dependent effects. The proxy-neutral class is, on average, substantially enriched for tolerated alleles relative to a random sample of possible mutations.\nThe proxy-deleterious class is constructed by simulating mutations across the genome according to realistic mutational processes. The simulation matches local sequence context (typically using trinucleotide frequencies to capture the strong dependence of mutation rates on flanking bases). CpG dinucleotides, for example, have elevated mutation rates due to spontaneous deamination of methylated cytosines, and the simulation accounts for this by generating more CpG transitions. Regional variation in mutation rates, driven by factors including replication timing and chromatin state, is similarly incorporated.\nThe logic underlying this construction is subtle but powerful. Simulated variants represent changes that could plausibly occur under human mutational processes but are generally not observed at high frequency in population databases. The proxy-deleterious class as a whole is enriched for alleles disfavored by selection, because the set of possible mutations includes many that disrupt conserved elements, alter protein function, or perturb regulatory sequences. By contrasting this set with the proxy-neutral class (high derived allele frequency variants that survived selection), CADD learns to recognize the annotation signatures that distinguish variants under purifying selection from those that have been tolerated.\nThis proxy labeling strategy has important implications. CADD does not learn to distinguish pathogenic from benign variants directly; it learns to distinguish tolerated-by-evolution from possible-but-not-observed. The assumption is that variants depleted by selection are enriched for functional effects and therefore enriched for disease relevance. This assumption is often correct but introduces a systematic gap between what CADD measures (evolutionary tolerance) and what clinicians need (disease causation).\n\n\n4.3.2 Feature Integration\nConservation scores measure evolutionary constraint. Protein-level predictors assess amino acid substitution severity. Regulatory annotations mark biochemically active regions. Each signal captures genuine biology, but no single annotation captures the full complexity of variant function. A missense variant in a constrained gene might be tolerated if it falls in an unconserved loop region; a synonymous variant might be pathogenic if it disrupts splicing. The power of CADD lies in learning how these heterogeneous signals interact, upweighting annotations that distinguish proxy-deleterious from proxy-neutral variants and downweighting those that do not.\nCADD integrates more than 60 features, far exceeding what explicit combination rules could accommodate. Gene model annotations describe the local transcript and coding context of each variant. The most fundamental is the predicted sequence consequence: whether a variant is synonymous, missense, nonsense, frameshift, splice-site disrupting, or located in untranslated or intronic regions. Distance to exon-intron boundaries and proximity to canonical splice sites provide additional context. Gene-level attributes including constraint metrics (pLI, LOEUF from gnomAD) quantify how tolerant each gene is to damaging variation; these metrics are described in detail in ?sec-ch02-constraint.\n\n\n\n\n\n\nConnecting Concepts: Haploinsufficiency\n\n\n\nThe gene-level constraint metrics used by CADD derive from a fundamental concept introduced in ?sec-ch02-constraint: haploinsufficiency. Most genes tolerate heterozygous loss because a single functional copy produces sufficient protein for normal function. Haploinsufficient genes are different; they require both copies to maintain adequate protein levels, making heterozygous loss-of-function variants pathogenic. A pLI approaching 1.0 indicates that loss-of-function variants are nearly absent from healthy individuals. By incorporating these gene-level constraint metrics, CADD can weight identical variants differently depending on whether they occur in dosage-sensitive or dosage-tolerant genes.\n\n\nCADD draws on three families of evidence. Evolutionary constraint from phyloP, GERP, and phastCons provides the conservation signals described earlier; incorporating multiple metrics computed from different alignments captures complementary aspects of selective pressure. For coding variants, amino acid substitution predictions from SIFT and PolyPhen-2 assess structural and functional disruption, supplemented by physicochemical properties, Grantham distances, and domain annotations from Pfam. Non-coding variants receive context from ENCODE and Roadmap Epigenomics annotations capturing chromatin accessibility, histone modifications, and transcription factor binding.\nAdditional features capture local sequence context (GC content, CpG density), genomic architecture (segmental duplications, repetitive elements), and chromosomal position. The model learns how to weight and combine these heterogeneous signals from the data rather than from expert specification.\nThe feature engineering approach reaches a performance ceiling because manually designed features encode only what biologists already know. This limitation motivates the shift to learned representations examined in Section 5.6 for tokenization strategies and Section 9.3 for foundation model features. The contrast between hand-crafted features and learned representations illuminates the paradigm shift toward foundation models discussed in Section 13.1.\n\n\n4.3.3 Model Architecture and Scoring\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThis section explains the PHRED scaling used for CADD scores. If the formula is unfamiliar, focus on the interpretation: higher scores indicate more extreme predicted deleteriousness, with each 10-point increase representing a 10-fold enrichment.\n\n\nRaw classifier outputs are not directly interpretable as probabilities or biological effect sizes. A clinician presented with a support vector machine decision value has no intuitive understanding of what that number means. To address this, CADD defines PHRED-scaled scores based on the rank of each variant among all possible single-nucleotide substitutions in the reference genome. A scaled score of 10 indicates that a variant falls in the top 10% of predicted deleteriousness. A score of 20 indicates the top 1%, and a score of 30 indicates the top 0.1%. This rank-based transformation ensures comparability across CADD versions and provides immediate interpretability: a clinician can understand that a score of 25 places this variant among the most extreme 0.3% of possible mutations without needing to understand the underlying classifier.\n\n\n\n\n\n\nWorked Example: CADD PHRED Scoring\n\n\n\nSuppose CADD’s classifier assigns a raw score to a missense variant in BRCA1. To convert this to the interpretable scaled score, CADD ranks this variant against all ~8.6 billion possible single-nucleotide substitutions in the reference genome.\nStep 1: The classifier produces a raw score (e.g., 2.34 on the SVM decision boundary).\nStep 2: CADD determines where this score ranks among all possible variants. If this variant’s raw score places it at the 99th percentile (more extreme than 99% of possible variants), it receives a scaled score of 20.\nStep 3: The PHRED formula: scaled score = \\(-10 \\times \\log_{10}(\\text{percentile rank})\\)\n\n\n\nRaw Percentile\nCalculation\nScaled Score\n\n\n\n\nTop 10% (0.10)\n\\(-10 \\times \\log_{10}(0.10)\\)\n10\n\n\nTop 1% (0.01)\n\\(-10 \\times \\log_{10}(0.01)\\)\n20\n\n\nTop 0.1% (0.001)\n\\(-10 \\times \\log_{10}(0.001)\\)\n30\n\n\nTop 0.03% (0.0003)\n\\(-10 \\times \\log_{10}(0.0003)\\)\n~35\n\n\n\nEach 10-point increase represents a 10-fold enrichment in predicted deleteriousness. A variant with a scaled score of 25 is rarer (more extreme) than 99.7% of all possible substitutions.\n\n\nCADD’s classifier operates on the high-dimensional feature vector assembled for each variant. The original CADD model (v1.0) used a linear support vector machine trained to discriminate proxy-neutral and proxy-deleterious variants based on approximately 30 million training examples. The choice of a linear model was deliberate and pragmatic: with tens of millions of training examples and dozens of features, a linear classifier is computationally tractable while capturing the main structure of the classification problem. More fundamentally, the proxy labeling strategy means training labels are inherently noisy: the proxy-neutral class contains some deleterious variants, and the proxy-deleterious class contains some neutral ones. A simple linear model is less likely to overfit to this label noise than a complex nonlinear model, which might learn to memorize spurious patterns in the noisy labels rather than the generalizable structure that distinguishes evolutionary constraint from neutrality. Subsequent versions (v1.1 onward) transitioned to logistic regression, which offers comparable discriminative performance while providing native probability outputs and faster scoring of new variants (rentzsch_cadd-splice_2021?). Version 1.7 expanded the feature set to include protein language model scores and regulatory predictions from deep learning models (Schubach et al. 2024).\n\n\n\n\n\n\nPractical Guidance: Interpreting CADD Scores\n\n\n\n\n\n\n\n\n\n\n\nCADD Score\nPercentile\nClinical Interpretation\n\n\n\n\n10\nTop 10%\nWeak evidence; many benign variants here\n\n\n15\nTop 3%\nModest evidence; consider with other factors\n\n\n20\nTop 1%\nModerate evidence; commonly used filter threshold\n\n\n25\nTop 0.3%\nStrong evidence; most known pathogenic missense variants\n\n\n30\nTop 0.1%\nVery strong evidence; enriched for severe effects\n\n\n\nRemember: CADD scores represent evolutionary tolerance, not clinical pathogenicity. A score of 30 does not mean “pathogenic”—it means “evolutionarily unusual.”\n\n\nIn clinical laboratories, CADD scaled scores commonly serve as filters to enrich for potentially pathogenic variants. Typical thresholds range from 15 (top 3%) to 20 (top 1%) or higher. Variants with scores at or above 20 are considered moderately high deleteriousness candidates, while scores at or above 30 are frequently interpreted as strongly enriched for functional impact. A diagnostic pipeline might use CADD greater than or equal to 20 as an initial filter, reducing 25,000 exome variants to several hundred candidates for expert review. These filters serve as prioritization tools that reduce the variant burden to a manageable number rather than as definitive pathogenicity calls.\n\n\n4.3.4 Evolutionary Proxy Problem\nCADD’s training signal derives entirely from evolutionary history: variants that survived natural selection versus those depleted by it. This creates a fundamental mismatch with clinical questions. Evolution optimizes for reproductive fitness across populations and timescales; clinical genetics asks whether a specific variant causes disease in a specific patient.\nThe mismatch manifests in predictable ways. Constraint varies across tissues and developmental stages, but CADD assigns a single genome-wide score. A variant in a deeply conserved neural enhancer receives a high score regardless of whether the patient presents with a cardiac or neurological phenotype. The constraint is real, but its clinical relevance depends on context CADD cannot assess.\nMore fundamentally, purifying selection only removes variants that reduce fitness before or during reproductive years. Late-onset diseases like Alzheimer’s or many cancers exert minimal selective pressure; variants causing these conditions may show little evolutionary depletion despite clear pathogenicity. Gain-of-function mutations present an even sharper challenge. A variant that creates a novel toxic function has no evolutionary precedent to deplete; CADD’s framework cannot recognize pathogenic mechanisms that evolution never encountered.\nThese limitations are not failures of implementation but consequences of the proxy relationship between evolutionary constraint and disease causation. CADD measures what evolution preserved and eliminated. Whether that corresponds to clinical pathogenicity depends on whether the disease mechanism falls within evolution’s purview.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-ensemble-methods",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-ensemble-methods",
    "title": "4  Classical Variant Prediction",
    "section": "4.4 Other Ensemble Methods",
    "text": "4.4 Other Ensemble Methods\nThe clinical geneticist focused exclusively on rare missense variants in Mendelian disease faces a different optimization problem than the researcher screening the entire genome for regulatory variants. CADD’s genome-wide generality may sacrifice accuracy within specific variant classes, accepting modest performance everywhere to achieve coverage anywhere. For diagnostic laboratories where missense variants in known disease genes dominate the caseload, specialized ensemble methods offer an alternative: models trained directly on curated disease variants, optimized for the specific task rather than general prioritization. This tension between generality and specialization recurs throughout computational biology, and different clinical contexts demand different tradeoffs.\nEnsemble principles for combining multiple predictors connect to the deep ensemble approaches for uncertainty quantification examined in Section 23.4.1. Integration strategies that combine classical scores with foundation model predictions are examined in ?sec-ch14-combining-evidence.\n\n4.4.1 REVEL\nA missense variant in a known disease gene presents a narrower interpretive challenge than an arbitrary variant anywhere in the genome. The variant is protein-coding, the gene has established disease associations, and the question is specifically whether this amino acid substitution is pathogenic. This focused scope permits a different training strategy than CADD’s evolutionary proxy approach.\nREVEL represents a missense-specific ensemble predictor widely used in clinical laboratories (Ioannidis et al. 2016). Rather than training on evolutionary proxy labels, REVEL directly discriminates pathogenic missense variants (curated from HGMD and other disease databases) from rare putatively neutral missense variants observed in population datasets.\nREVEL integrates predictions from a panel of individual tools: SIFT, PolyPhen-2, PROVEAN, MutationAssessor, FATHMM, GERP++, phyloP, and phastCons, among others. A random forest model learns to combine these scores, weighting each according to its discriminative power for the pathogenic versus neutral classification. The training set is carefully constructed to avoid label contamination, excluding variants present in both disease and population databases.\nREVEL scores range from 0 to 1, with higher values implying greater pathogenicity likelihood. Common interpretation thresholds treat scores above 0.5 as supporting evidence for pathogenicity, with scores above 0.75 providing stronger evidence. REVEL is restricted to missense single-nucleotide variants, making it more specialized than CADD but often more accurate within its scope. This specialization reflects a deliberate choice: by giving up coverage of non-coding and structural variants, REVEL gains the ability to train on directly relevant labels rather than evolutionary proxies.\n\n\n4.4.2 M-CAP\nDiagnostic laboratories evaluating potential Mendelian disease variants face asymmetric consequences for errors. Calling a benign variant pathogenic can lead to unnecessary surgeries, psychological burden, and inappropriate cascade testing of family members. Missing a pathogenic variant delays diagnosis but typically permits later reclassification as evidence accumulates. This asymmetry argues for prioritizing specificity over sensitivity in clinical settings.\nM-CAP addresses specifically the challenge of distinguishing pathogenic from benign rare missense variants in Mendelian disease contexts, with explicit attention to this asymmetry (Jagadeesh et al. 2016). The method uses gradient boosting on a feature set including conservation scores, protein structure features, and amino acid properties.\nM-CAP was explicitly designed to minimize false positives while maintaining reasonable sensitivity. The developers tuned their classifier to achieve less than 5% false positive rate on known pathogenic variants, accepting some reduction in sensitivity as a tradeoff. This design philosophy differs from methods that balance sensitivity and specificity equally, reflecting M-CAP’s intended use in diagnostic settings where false positive pathogenicity calls have serious consequences.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA clinical laboratory is evaluating tools for a rare disease diagnostic pipeline. They need high specificity (few false positives) because false pathogenic calls lead to unnecessary interventions. Which of CADD, REVEL, or M-CAP would you recommend for their primary filter, and why?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nM-CAP is the best choice for this use case because it was explicitly designed to minimize false positives (high specificity) at the cost of some sensitivity. REVEL offers high accuracy for missense variants but balances sensitivity and specificity equally, while CADD provides genome-wide coverage but may have more false positives within specific variant classes.\n\n\n\n\n\n\n\n4.4.3 Comparison and Selection\nA clinical laboratory must choose which computational predictors to incorporate into their diagnostic pipeline. Should they use CADD for its universal coverage, REVEL for its missense accuracy, or M-CAP for its low false-positive rate? The wrong choice can mean missing pathogenic variants or overwhelming clinical reviewers with false alarms. Understanding what each method optimizes for—and what it trades away—is essential for matching tools to clinical context.\nNo single ensemble method dominates across all variant types and clinical contexts. CADD provides the broadest coverage (genome-wide, all variant types) but may sacrifice accuracy within specific variant classes to achieve this generality. REVEL often outperforms CADD on missense-only benchmarks, reflecting its focused training objective. M-CAP prioritizes specificity over sensitivity, appropriate for clinical settings where avoiding false positives is paramount.\n\n\n\nTable 4.3: Comparison of ensemble variant effect predictors\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nCoverage\nTraining Labels\nKey Strength\nKey Limitation\n\n\n\n\nCADD\nAll variants, genome-wide\nEvolutionary proxy\nUniversal applicability\nMeasures evolutionary tolerance, not pathogenicity\n\n\nREVEL\nMissense only\nCurated pathogenic vs. neutral\nHigher accuracy for missense\nCannot score non-coding variants\n\n\nM-CAP\nRare missense\nPathogenic vs. benign\nLow false positive rate\nLower sensitivity than alternatives\n\n\n\n\n\n\nClinical variant interpretation typically incorporates multiple computational scores rather than relying on any single predictor. Different scores may agree, providing stronger evidence, or disagree, flagging variants requiring careful manual review. A variant with CADD greater than or equal to 25, REVEL greater than or equal to 0.8, and M-CAP “possibly pathogenic” presents a consistent computational picture; one where CADD and REVEL disagree prompts closer examination of the underlying features. Understanding the construction, training data, and intended use case of each method is essential for appropriate interpretation. The integration of these scores into clinical workflows is examined in detail in Chapter 27 and Chapter 28, where computational evidence must be weighed alongside functional studies, segregation data, and clinical presentation.\n\n\n\n\n\n\nROC comparison of variant effect predictors\n\n\n\n\nFigure 4.5: Performance comparison of variant effect predictors on a held-out missense variant benchmark. ROC curves compare ensemble methods (CADD, REVEL, M-CAP; solid lines) against individual component scores (SIFT, PolyPhen-2, phyloP; dashed lines). Ensemble methods consistently outperform individual predictors by integrating multiple signals. Circular markers indicate common clinical operating thresholds with corresponding sensitivity and specificity values. Note that benchmark performance may be inflated by circularity: variants classified using these same predictors appear in clinical databases that form the evaluation set. True clinical utility may be lower than benchmark metrics suggest, particularly for novel variants in understudied genes.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-circularity",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-circularity",
    "title": "4  Classical Variant Prediction",
    "section": "4.5 Circularity and Ascertainment Bias",
    "text": "4.5 Circularity and Ascertainment Bias\nA diagnostic laboratory classifies a novel missense variant as pathogenic based partly on its high CADD score. That classification enters ClinVar. Two years later, a benchmarking study evaluates CADD performance on ClinVar pathogenic variants and reports excellent accuracy. Is the high performance genuine, or has the benchmark been contaminated by the predictor’s own influence on the labels it is evaluated against? This scenario illustrates the first of the pervasive problems affecting all variant effect predictors: circularity between scores and clinical databases. Compounding this circularity is ascertainment bias in available training and testing variants. These issues do not invalidate classical scores, but they counsel appropriate humility about performance claims and careful attention in both development and application. The broader methodological concerns around benchmark design and confounding are examined in Chapter 12 and Chapter 11.\n\n\n\n\n\n\nKey Insight\n\n\n\nThe same scores used to classify variants end up in the databases used to evaluate those scores, creating a self-reinforcing loop. This circularity inflates apparent performance—methods appear to work better than they actually do because they influenced the labels they are tested against.\n\n\n\n\n\n\n\n\nThe circularity problem in variant effect prediction\n\n\n\n\nFigure 4.6: The circularity problem in variant effect prediction. A self-reinforcing feedback loop operates between computational predictors and clinical databases. (1) A high computational score (e.g., CADD) contributes evidence supporting a pathogenic classification. (2) The classified variant enters ClinVar. (3) Benchmarking studies evaluate computational predictors on ClinVar variants. (4) Apparent high performance encourages clinical adoption. (5) Greater clinical use increases the influence of computational scores on future classifications, returning to step 1. This circularity inflates benchmark performance metrics because predictors are evaluated on labels they helped create. Intervention strategies include temporal holdouts (evaluating only on classifications made before predictor adoption), functional assay ground truth (using experimentally measured variant effects), and prospective evaluation (testing on newly classified variants not yet influenced by the predictor).\n\n\n\n\n4.5.1 Circularity Problem\nClinVar and similar clinical databases increasingly incorporate computational predictions as evidence supporting variant classification. When a clinical laboratory classifies a variant as pathogenic, the CADD score, PolyPhen-2 prediction, or other computational evidence may have contributed to that determination. When CADD is subsequently evaluated on ClinVar pathogenic variants, its performance is artificially inflated: the benchmark contains variants that were labeled partly because CADD assigned them high scores. The predictor appears to perform well because it was already part of the labeling process.\nThis circularity operates through multiple pathways. Direct use occurs when clinical laboratories explicitly cite computational scores in their classifications. Indirect influence arises when computational predictions shape clinical suspicion, affecting which variants receive functional testing or expert review. Selection bias in benchmark construction can compound the problem: benchmark creators may preferentially include variants with strong computational evidence, excluding ambiguous cases that would provide a more stringent test.\nThe consequence is that benchmark performance may overestimate real-world utility. A method that has been widely adopted will appear to perform well on benchmarks populated by variants classified using that method, even if its true discriminative power is more limited. This concern applies to all established computational tools, including conservation scores, protein-level predictors, and ensemble methods. The more influential a method becomes, the more its benchmark performance becomes self-reinforcing.\nAddressing circularity requires careful benchmark construction. Temporal holdouts (using only classifications made before a method’s widespread adoption) can reduce but not eliminate the problem. Functional assays that directly measure variant effects provide ground truth independent of computational predictions but are available for only a small fraction of variants. Prospective evaluation on newly classified variants offers the cleanest test but requires patience and ongoing data collection. The foundation model evaluations discussed in Chapter 17 face these same challenges, and the confounding issues examined in Chapter 12 show how these problems persist and evolve as methods become more sophisticated.\n\n\n4.5.2 Ascertainment Bias\nBeyond circularity lies a more fundamental problem: the variants available for training and evaluation represent a systematically skewed sample of disease-causing mutations. Clinical databases are dominated by variants in well-studied genes, particularly those causing severe Mendelian phenotypes with clear inheritance patterns. Protein-coding variants are overrepresented because they are easier to interpret and more often tested. Variants in genes associated with common diagnostic panels appear frequently; those in rarely tested genes are sparse.\nThis ascertainment bias shapes what models learn and how they perform. A predictor trained predominantly on variants in constrained genes may learn that gene-level constraint is the primary signal for pathogenicity. When applied to variants in less constrained genes (where pathogenic variants also occur, but less frequently), the model may systematically underestimate risk. Similarly, models trained on European-ancestry samples may encode population-specific patterns that transfer poorly to other populations, compounding health disparities by providing less accurate predictions for underrepresented groups.\nThe consequences extend to evaluation. Benchmark variants inherit the ascertainment biases of their source databases. Strong performance on benchmark sets may not translate to the rare genes, unusual variant types, or underrepresented populations encountered in real clinical practice. Variants that are “easy” to classify (stop-gains in highly constrained genes) are overrepresented in benchmarks, while diagnostically challenging variants (missense variants in moderate-constraint genes, non-coding variants) are underrepresented. The benchmark tells us how well the method performs on the easy cases; it may reveal little about the hard cases where computational assistance is most needed.\nAscertainment bias in sequencing creates blind spots in difficult genomic regions (Section 1.6). The circularity between training labels and evaluation benchmarks affects both classical and modern methods, as examined systematically in ?sec-ch22-label-circularity. Benchmark construction strategies that mitigate these issues appear in Section 11.5.\n\n\n4.5.3 Implications for Clinical Use\nThese limitations do not render classical scores useless, but they counsel appropriate humility in interpretation. Computational predictions provide one line of evidence among several in variant interpretation. Strong scores in expected directions support clinical suspicion; unexpected scores prompt careful review. No computational score should override clear clinical or functional evidence, and borderline scores in complex cases may warrant agnosticism rather than confident prediction.\nThe ACMG-AMP framework for variant classification appropriately treats computational predictions as supporting evidence (PP3 for predictions supporting pathogenicity, BP4 for predictions supporting benign status) rather than standalone criteria (Richards et al. 2015). Multiple lines of computational evidence may be combined, but the weight assigned should reflect the limitations outlined here. Variants classified primarily on computational grounds should be flagged for potential reclassification as additional evidence emerges. The ACMG-AMP framework’s integration of computational evidence is detailed in ?sec-ch26-acmg-amp for complete clinical workflows. Calibration of computational scores to ACMG evidence strength levels is examined in ?sec-ch14-acmg-mapping for foundation model approaches.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_1/p1-ch04-vep-classical.html#sec-ch04-feature-limitations",
    "href": "part_1/p1-ch04-vep-classical.html#sec-ch04-feature-limitations",
    "title": "4  Classical Variant Prediction",
    "section": "4.6 Limitations of the Feature Engineering Paradigm",
    "text": "4.6 Limitations of the Feature Engineering Paradigm\nClassical variant effect prediction achieved substantial success in prioritizing potentially pathogenic variants and established conceptual foundations that persist in modern methods. The integration of diverse annotations, use of evolutionary signals as proxy labels, and genome-wide precomputation all anticipate contemporary practices. Yet a fundamental tension remains: manually designed features encode only what biologists already know, and the complexity of genotype-phenotype relationships exceeds what explicit feature engineering can capture. The features are not wrong; they are incomplete in ways that cannot be remedied by adding more of the same.\n\n4.6.1 Feature Ceiling\nFeature-engineered methods encode human knowledge about which genomic properties matter for variant function. Conservation scores capture evolutionary constraint; protein-level predictors encode structural intuitions; regulatory annotations mark biochemically active regions. This encoded knowledge is valuable but necessarily incomplete. Biologists cannot specify all relevant patterns in advance, and the interactions between features may be too complex for simple combination rules to capture.\nThe performance of feature-engineered methods is therefore bounded by the quality and completeness of the features themselves. Adding more features provides diminishing returns as the most informative signals are exhausted. Interactions between features (a variant in a conserved enhancer within a constrained gene) may require explicit specification or rely on simple combination rules that miss nonlinear relationships. The linear SVM at the heart of CADD cannot represent the complex feature interactions that characterize biological regulation.\n\n\n4.6.2 Limited Context\nClassical features typically describe variants in isolation or with minimal context. Conservation scores examine each position independently. Protein-level predictors consider amino acid substitutions without full protein context. Gene-level features apply uniformly across entire genes regardless of position-specific effects. This limited context prevents classical methods from learning the complex sequence patterns that determine variant effects.\nA variant disrupting a critical transcription factor binding motif may escape detection if the motif is not annotated, even though the underlying sequence pattern is learnable from data. A missense variant at a protein-protein interface may be more damaging than one in a loop region, but capturing this distinction requires understanding protein structure and interactions that feature engineering incompletely represents. The local sequence context surrounding a variant often matters, but which contexts matter and how requires learning from data rather than specification by experts.\n\n\n4.6.3 Persistent Gap Between Measurement and Need\nEach classical method measures something related to but distinct from clinical pathogenicity. Conservation scores measure evolutionary constraint. Protein-level predictors measure functional disruption. CADD measures evolutionary tolerance. Each provides genuine biological signal, but none directly answers the clinical question: will this variant cause disease in this patient?\nThis gap is not merely a limitation of specific methods but reflects something deeper about the variant interpretation problem. The clinically relevant question depends on context (which tissue, which genetic background, which environmental exposures) that no current method captures. Even perfect prediction of functional disruption would not resolve questions of penetrance, expressivity, and disease mechanism. Classical methods provide important evidence for variant interpretation, but they cannot substitute for the integrative clinical reasoning examined in Chapter 27.\n\n\n4.6.4 From Features to Representations\nThe transition from CADD to deep learning methods represents a fundamental shift in how variant effect prediction is approached: from encoding biological knowledge as hand-crafted features to learning representations directly from sequence data. Where SIFT computes conservation from explicit multiple sequence alignments, ESM-2 learns similar signals implicitly through masked language modeling on protein sequences. Where PolyPhen-2 engineers features from solved protein structures, AlphaFold2 learns geometric constraints from evolutionary covariation (Chapter 15). The tokenization strategies and embedding approaches that enable this representation learning are detailed in Section 5.6. The shift is real and consequential.\nYet learned representations do not automatically overcome the limitations that constrain classical methods. The circularity between training labels and evaluation benchmarks affects transformer-based predictors exactly as it affects logistic regression. A model trained on ClinVar pathogenic variants inherits ClinVar’s ascertainment biases whether it uses 100 features or 100 million parameters. Rare variants remain difficult because they are rare in training data. Novel mechanisms remain invisible because no labeled examples exist. The variant effect prediction problem is fundamentally difficult, and methodology alone cannot resolve difficulties rooted in data availability and biological complexity. The zero-shot scoring paradigm (?sec-ch14-zeroshot-supervised) offers a partial escape from label circularity by deriving variant scores from unsupervised pretraining objectives rather than explicit pathogenicity annotations, though this approach introduces its own assumptions about what pretraining captures.\nClassical methods remain valuable: as baselines that establish the performance floor modern methods must exceed, as interpretable components when understanding matters as much as prediction, and as reminders of what the field has learned about which signals carry predictive information. Foundation models have advanced variant effect prediction (Chapter 17), but honest evaluation requires understanding what classical methods achieved and where all approaches, classical and modern alike, continue to struggle. The evaluation methodology required to fairly assess both classical and learned approaches is detailed in Chapter 12, with genomics-specific metric considerations in Section 12.5.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat do conservation scores like phyloP and GERP actually measure, and why is this different from clinical pathogenicity?\nHow do SIFT and PolyPhen-2 assess missense variant severity, and what are the key differences in their approaches?\nWhy did CADD use evolutionary proxy labels (observed vs. simulated variants) rather than clinical pathogenicity labels for training?\nWhat is the circularity problem between variant effect predictors and clinical databases like ClinVar?\nWhy do classical variant effect predictors struggle with rare variants and novel mechanisms?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nConservation scores measure evolutionary constraint, not disease relevance: phyloP quantifies deviation from neutral evolution rates, while GERP estimates rejected substitutions across evolutionary time. These scores measure whether positions have been important for reproductive fitness across species, which differs from clinical pathogenicity because evolution cannot detect late-onset diseases, tissue-specific effects, or recently evolved human-specific functions.\nSIFT uses sequence homology alone, while PolyPhen-2 integrates structure: SIFT examines which amino acids appear at each position across homologous sequences and predicts that substitutions introducing amino acids not observed evolutionarily are deleterious. PolyPhen-2 extends this by incorporating protein structure features (solvent accessibility, secondary structure, proximity to functional sites) and physicochemical properties (Grantham distance, BLOSUM matrices) to assess whether substitutions are conservative or radical in their structural context.\nEvolutionary proxy labels provided millions of training examples from unlabeled data: Clinical pathogenicity labels were scarce (thousands of variants) and biased toward well-studied genes, while evolutionary proxy labels (observed human-derived alleles versus simulated variants) provided millions of training examples. The proxy-neutral class consists of variants that survived purifying selection, while the proxy-deleterious class represents mutations that could occur but are generally not observed at high frequency, allowing CADD to learn at scale despite measuring evolutionary tolerance rather than clinical pathogenicity directly.\nCircularity creates a self-reinforcing loop that inflates performance: Clinical laboratories use computational scores like CADD as evidence when classifying variants in ClinVar, then benchmarking studies evaluate those same predictors on ClinVar variants. This means predictors are tested on labels they helped create, artificially inflating their apparent performance because they influenced the ground truth labels used for evaluation.\nRare variants and novel mechanisms lack training examples: Classical predictors learn from patterns observed in training data, so variants with mechanisms not represented in curated databases (gain-of-function mutations, human-specific regulatory elements, recently evolved functions) have no labeled examples to learn from. Ascertainment bias means training sets over-represent variants in well-studied genes with severe Mendelian phenotypes, while diagnostically challenging cases in rare genes or with unusual mechanisms are systematically underrepresented.\n\n\n\n\n\n\nCore concepts covered:\n\nConservation scores (phyloP, GERP, phastCons) quantify evolutionary constraint by comparing observed substitution rates to neutral expectations\nProtein-level predictors (SIFT, PolyPhen-2) assess amino acid substitution severity using sequence homology and structural features\nCADD pioneered evolutionary proxy training, using observed vs. simulated variants to generate millions of training labels\nEnsemble methods (REVEL, M-CAP) combine multiple scores for improved performance on specific variant classes\nCircularity between predictors and clinical databases inflates apparent performance\nAscertainment bias means benchmarks over-represent easy cases from well-studied genes\n\nThe fundamental insight: Every classical predictor measures a proxy for clinical relevance, not clinical relevance itself. Conservation reflects evolutionary fitness, not human disease. Protein disruption does not determine penetrance. Benchmark performance does not guarantee clinical utility.\nLooking ahead: The limitations of hand-crafted features motivate the shift to learned representations (Section 5.6), where models learn relevant patterns from data rather than expert specification. Foundation models (Section 13.2) extend this approach by pretraining on massive unlabeled datasets, enabling variant effect prediction without the label circularity that affects supervised methods (Chapter 17).\nKey skills to retain:\n\nGiven a conservation score (phyloP = 3.5, GERP = 4.2), explain what it means and what it does not mean\nCompare what SIFT and PolyPhen-2 measure and when each is most useful\nExplain why CADD uses evolutionary proxy labels rather than clinical labels\nIdentify circularity concerns when interpreting benchmark performance\n\n\n\n\n\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky, Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev. 2010. “A Method and Server for Predicting Damaging Missense Mutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper, Arend Sidow, and Serafim Batzoglou. 2010. “Identifying a High Fraction of the Human Genome to Be Under Selective Constraint Using GERP++.” PLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nHenikoff, S, and J G Henikoff. 1992. “Amino Acid Substitution Matrices from Protein Blocks.” Proceedings of the National Academy of Sciences 89 (22): 10915–19. https://doi.org/10.1073/pnas.89.22.10915.\n\n\nIoannidis, Nilah M., Joseph H. Rothstein, Vikas Pejaver, Sumit Middha, Shannon K. McDonnell, Saurabh Baheti, Anthony Musolf, et al. 2016. “REVEL: An Ensemble Method for Predicting the Pathogenicity of Rare Missense Variants.” The American Journal of Human Genetics 99 (4): 877–85. https://doi.org/10.1016/j.ajhg.2016.08.016.\n\n\nJagadeesh, Karthik A., Aaron M. Wenger, Mark J. Berger, Harendra Guturu, Peter D. Stenson, David N. Cooper, Jonathan A. Bernstein, and Gill Bejerano. 2016. “M-CAP Eliminates a Majority of Variants of Uncertain Significance in Clinical Exomes at High Sensitivity.” Nature Genetics 48 (12): 1581–86. https://doi.org/10.1038/ng.3703.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory M. Cooper, and Jay Shendure. 2014. “A General Framework for Estimating the Relative Pathogenicity of Human Genetic Variants.” Nature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT: Predicting Amino Acid Changes That Affect Protein Function.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie Gastier-Foster, Wayne W. Grody, et al. 2015. “Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology.” Genetics in Medicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs, Minmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005. “[PhastCons] Evolutionarily Conserved Elements in Vertebrate, Insect, Worm, and Yeast Genomes.” Genome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.",
    "crumbs": [
      "Part I: Data Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Variant Prediction</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html",
    "href": "part_2/p2-ch05-representations.html",
    "title": "5  Tokens and Embeddings",
    "section": "",
    "text": "5.1 One-Hot Encoding: The CNN Foundation\nA child inherits a DMD variant from her mother. Whether this variant causes Duchenne muscular dystrophy or remains clinically silent depends on its exact position relative to the exon-intron boundary: one nucleotide can determine whether the splicing machinery recognizes the junction. This is why single-nucleotide resolution is not a technical nicety but a clinical necessity. The earliest deep learning approaches to genomic sequence modeling recognized this requirement and adopted the simplest representation capable of preserving it: one-hot encoding, where each nucleotide becomes a sparse binary vector with a single active element indicating its identity. Adenine is encoded as \\([1, 0, 0, 0]\\), cytosine as \\([0, 1, 0, 0]\\), guanine as \\([0, 0, 1, 0]\\), and thymine as \\([0, 0, 0, 1]\\). A sequence of length \\(L\\) thus becomes a matrix of dimensions \\(4 \\times L\\), interpretable as four channels analogous to the RGB channels of an image plus one.\nThe properties that made one-hot encoding dominant in the convolutional neural network (CNN) era stem from this simple design. The representation is lossless, preserving every nucleotide explicitly without information compression. It maintains single-nucleotide resolution, enabling detection of effects from individual SNPs. The encoding exhibits translation equivariance, meaning convolutional filters learn position-invariant motifs recognizable anywhere in the sequence. And it requires no preprocessing, vocabulary construction, or tokenizer training, making implementation straightforward. DeepSEA, ExPecto, and SpliceAI all employed one-hot encoding without modification, with convolutional layers learning to detect sequence patterns directly from the binary representation. These convolutional architectures and their learned pattern detectors are examined in Chapter 6.\nThe key insight underlying CNN success with one-hot encoding is that convolutions process sequences through local operations. Each filter examines only a small window of positions at a time, and the sparse, orthogonal nature of one-hot vectors poses no obstacle to this local processing. First-layer filters effectively learn position weight matrices that score short \\(k\\)-mer patterns, while deeper layers capture combinations and spatial arrangements of these primitive motifs. The representation worked because it aligned with the architectural inductive bias of convolutions: local pattern detection does not require global sequence compression.\nFor transformer architectures, one-hot encoding creates a fundamental mismatch. Transformers compute attention between all pairs of positions, with computational cost scaling quadratically with sequence length. A 10 kb sequence requires 100 million pairwise attention computations per layer, quickly becoming prohibitive for the long sequences genomic applications require. The problem compounds because transformers learn dense embeddings for each token, but with only four possible nucleotides, the embedding layer has minimal opportunity for rich representation learning.\nThis mismatch forces an impossible choice between the long contexts needed for regulatory modeling and computational tractability. Transformer context windows of 512 to 4,096 tokens translate to only 512 to 4,096 base pairs when using one-hot encoding, a tiny fraction of genes or regulatory regions. Compare this to Enformer’s 200 kb receptive field or SpliceAI’s 10 kb context, both achieved through architectural innovations operating on one-hot encoded sequence (Chapter 6). Sub-quadratic architectures like HyenaDNA resolve this tension through a different approach: maintaining single-nucleotide tokenization while replacing attention with operations that scale more gently with sequence length (Section 5.4). For standard transformer architectures, however, the quadratic barrier motivated the search for alternative representations that compress genomic sequences into fewer tokens while preserving biological information.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-onehot",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-onehot",
    "title": "5  Tokens and Embeddings",
    "section": "",
    "text": "Key Insight\n\n\n\nOne-hot encoding’s success with CNNs was not accidental—it aligned perfectly with the architectural inductive bias. Convolutions learn local patterns, and one-hot’s orthogonal, sparse representation posed no obstacle to this local processing. The representation matched the architecture’s strengths.\n\n\n\n\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading on, consider: if transformers require fewer tokens to process long sequences efficiently, how might you reduce the number of tokens while still representing the full sequence? What tradeoffs might each approach involve?",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-kmer",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-kmer",
    "title": "5  Tokens and Embeddings",
    "section": "5.2 K-mer Tokenization: The DNABERT Approach",
    "text": "5.2 K-mer Tokenization: The DNABERT Approach\nThe computational constraints of one-hot encoding for transformers led researchers to explore sequence compression through \\(k\\)-mer tokenization. This approach treats overlapping subsequences of length \\(k\\) as tokens, drawing an analogy between k-mers and words in natural language. Just as sentences compose words carrying meaning through sequence and combination, genomic sequences might be understood as \\(k\\)-mer “words” encoding biological function through their arrangement. DNABERT pioneered this approach for genomic transformers in 2021, using 6-mers as tokens and training a BERT-style masked language model on human reference sequences (Ji et al. 2021).\nThe \\(k\\)-mer vocabulary has a fixed size of \\(4^k\\) possible tokens. For 6-mers, this yields 4,096 distinct tokens, comparable to vocabulary sizes in some natural language models. Each token represents six consecutive nucleotides, creating direct correspondence between subsequence and token identity. DNABERT used overlapping k-mers: for a sequence like ACGTACGT, successive 6-mer tokens share five nucleotides with their neighbors. The sequence position advances by one nucleotide at a time, generating one token per position (minus the \\(k\\)-1 positions at the sequence end where a complete \\(k\\)-mer cannot form).\nDNABERT provided valuable proof of concept for genomic transformers. It demonstrated that self-supervised pretraining on raw DNA sequences could improve performance over training from scratch, that learned embeddings could capture biologically meaningful regularities even when trained only on the reference genome, and that BERT-style architectures could transfer across multiple downstream tasks. DNABERT achieved strong performance on promoter prediction, splice site identification, and transcription factor binding site recognition after fine-tuning with relatively small amounts of task-specific labeled data. The model’s architecture and subsequent developments are examined in Chapter 14, while the transfer learning approaches that enable adaptation to specific tasks are treated in Chapter 9.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nConsider a single-nucleotide variant (SNP) at some position in a sequence tokenized with overlapping 6-mers. How many tokens would this single nucleotide change affect? Think about which 6-mer windows would include that position.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nA single nucleotide change affects exactly 6 tokens in overlapping 6-mer tokenization. The changed nucleotide appears in six different 6-mer windows: the one starting at that position, plus the five windows that start before but include that position. This propagation of changes across multiple tokens complicates variant effect interpretation.\n\n\n\n\n\nSubsequent analysis revealed fundamental limitations rooted in the overlapping design. DNABERT-2 articulated these problems clearly in 2024 (Zhou et al. 2024). Overlapping k-mers provide no sequence compression: the number of tokens equals the number of nucleotides (minus a small constant), so context window limitations persist unchanged. A 10 kb sequence still requires approximately 10,000 tokens, and the quadratic attention complexity remains prohibitive for long sequences. The very design that seemed to add biological meaning through \\(k\\)-mer structure failed to address the computational bottleneck motivating the approach.\nThe overlapping design creates additional complications beyond computational cost. A single nucleotide contributes to \\(k\\) different tokens (each \\(k\\)-mer containing that position), complicating interpretation of which token drives any given prediction. This ambiguity becomes particularly problematic for variant effect interpretation, where understanding how a specific nucleotide change alters model predictions is essential. The effect of a single substitution propagates through \\(k\\) different tokens in ways that can be difficult to disentangle. The model must also learn that overlapping tokens share nucleotides, a relationship obvious from the tokenization scheme but requiring discovery through training. This redundancy consumes model capacity that could otherwise capture more complex biological patterns. The fixed 4^\\(k\\) vocabulary does not adapt to corpus statistics; frequent and rare k-mers receive equal representation capacity in the embedding table despite potentially differing importance for prediction.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-bpe",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-bpe",
    "title": "5  Tokens and Embeddings",
    "section": "5.3 Byte Pair Encoding: Learning the Vocabulary",
    "text": "5.3 Byte Pair Encoding: Learning the Vocabulary\nThe limitations of \\(k\\)-mer tokenization raise a question: what if the vocabulary itself could be learned from data? Consider how a child learning to read progresses from sounding out individual letters to recognizing common letter combinations (“th,” “ing,” “tion”) as single units. The brain naturally groups frequently co-occurring patterns into chunks, making reading faster without losing the ability to decode unfamiliar words letter-by-letter. Byte Pair Encoding (BPE) applies this same principle to DNA: it discovers which nucleotide combinations appear frequently together and groups them into single tokens, while rare sequences remain as individual nucleotides.\nBPE addresses vocabulary learning by constructing vocabulary through iterative discovery of frequent subsequences rather than defining tokens through a fixed rule. The algorithm, originally developed for data compression, builds vocabulary through a simple procedure. BPE initializes the vocabulary with single nucleotides: {A, C, G, T}. It then scans the training corpus to count all adjacent token pairs, identifies the most frequent pair, merges this pair into a new token added to the vocabulary, and replaces all instances in the corpus with the merged token. The process repeats through many iterations (typically thousands), building a vocabulary of variable-length tokens capturing frequently occurring sequence patterns. [Citation Needed]\n\n\n\n\n\n\nWorked Example: BPE in Action\n\n\n\nConsider a tiny corpus consisting of a single sequence: ACGTACGTACGT\nStep 1: Initialize vocabulary Vocabulary: {A, C, G, T} Tokenized sequence: A-C-G-T-A-C-G-T-A-C-G-T (12 tokens)\nStep 2: Count adjacent pairs - AC appears 3 times - CG appears 3 times - GT appears 3 times - TA appears 2 times\nStep 3: Merge most frequent pair (tie-break: AC) New vocabulary: {A, C, G, T, AC} Tokenized sequence: AC-G-T-AC-G-T-AC-G-T (9 tokens)\nStep 4: Count pairs again - ACG appears 3 times - GT appears 3 times - TAC appears 2 times\nStep 5: Merge ACG Vocabulary: {A, C, G, T, AC, ACG} Tokenized sequence: ACG-T-ACG-T-ACG-T (6 tokens)\nAfter just 2 merge iterations, our 12-nucleotide sequence compressed from 12 tokens to 6 tokens—a 2× compression. The algorithm discovered that “ACG” is a repeating unit worth representing as a single token. In real genomic applications with millions of sequences, BPE discovers biologically meaningful patterns like Alu element fragments, common regulatory motifs, and repetitive sequences.\n\n\nWhy does frequency-based merging produce biologically meaningful tokens? Frequency in genomic sequence is not random: sequences that appear repeatedly throughout the genome often represent functional units or structural patterns. Alu elements, the most abundant transposable elements in the human genome, appear over one million times; their characteristic subsequences will be merged into dedicated tokens early in BPE training. Regulatory motifs like the TATA box or common transcription factor binding sites recur across thousands of promoters. Microsatellites and other repetitive elements have characteristic patterns that appear genome-wide. By iteratively merging what co-occurs most often, BPE’s vocabulary converges on tokens that reflect genuine patterns in genome organization rather than imposing arbitrary boundaries. A token that appears frequently across the training corpus likely represents something biologically coherent—whether a functional motif, a repetitive element, or a conserved structural pattern—while unique random sequences remain decomposed into shorter subunits.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading on, consider: if BPE merges the most frequent adjacent pairs, what kinds of genomic sequences do you predict would be compressed most aggressively (represented by long tokens)? What kinds would remain as short tokens or single nucleotides?\n\n\nThe critical difference from \\(k\\)-mer tokenization is that BPE produces genuine sequence compression through non-overlapping tokens. Unlike overlapping k-mers where each nucleotide generates its own token, BPE creates tokens spanning multiple nucleotides without overlap. A 10 kb sequence might compress to 2,000 or 3,000 tokens depending on its repetitive structure, enabling transformers to process substantially longer sequences within the same context window.\nDNABERT-2 replaced 6-mer tokenization with BPE and demonstrated dramatic improvements (Zhou et al. 2024). The new model achieved comparable performance to state-of-the-art approaches while using 21 times fewer parameters and requiring approximately 92 times less graphics processing unit (GPU) time in pretraining. The Nucleotide Transformer (Chapter 14) similarly employs BPE tokenization, as do protein language models that must handle amino acid sequences with different compositional properties (Chapter 15). These efficiency gains stem directly from non-overlapping tokenization: actual sequence compression enables processing longer sequences with the same computational budget, and eliminating overlapping token redundancy allows the model to focus capacity on learning biological patterns rather than token relationships.\n\n\n\n\n\n\nKey Insight\n\n\n\nBPE’s power lies in corpus-adaptive vocabulary construction. Repetitive elements like Alu sequences receive dedicated long tokens because they appear frequently, while rare sequences decompose into short subunits. The vocabulary learns genomic structure rather than imposing arbitrary boundaries.\n\n\nThe BPE vocabulary learns corpus statistics through its construction process. Repetitive elements appearing frequently throughout the genome (such as Alu sequences or common regulatory motifs) receive dedicated tokens spanning many nucleotides. These long tokens enable efficient representation of repetitive regions while preserving single-nucleotide resolution for unique sequences. Rare sequences that BPE never encountered during vocabulary construction are represented as concatenations of shorter subunits, maintaining the ability to encode any sequence while allocating more representation capacity to common patterns.\nGROVER (Genome Rules Obtained Via Extracted Representations) extended this approach by training BPE specifically on the human genome and selecting vocabulary using a custom next-\\(k\\)-mer prediction task (Sanabria et al. 2024). Analysis of the resulting token embeddings revealed that the learned vocabulary encodes biologically meaningful structure without explicit supervision. Common tokens cluster separately from rare ones in embedding space. GC-rich tokens segregate from AT-rich tokens, reflecting the different properties of these sequence compositions. Token length correlates with specific embedding dimensions, allowing the model to represent both the content and extent of each token. Some tokens appear primarily in repetitive regions while others distribute broadly across the genome, and this localization pattern emerges in the learned representations.\nBPE introduces complications of its own that matter for clinical applications. Variable-length tokens mean that variant positions fall at different locations relative to token boundaries depending on local sequence context. A SNP might fall in the middle of a long token in one sequence context but at a token boundary in another, potentially affecting how the model represents and processes the variant. The same nucleotide change may alter different numbers of tokens depending on surrounding sequence, creating inconsistent input representations for what should be comparable biological events. The tradeoff between compression and interpretability becomes a design choice depending on intended application.\nThe following table summarizes the key differences between the tokenization strategies discussed so far.\n\n\n\nTable 5.1: Comparison of tokenization strategies showing the tradeoffs between sequence compression, vocabulary size, and variant interpretation clarity.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrategy\nTokens per kb\nVocabulary Size\nCompression\nVariant Localization\nBest For\n\n\n\n\nOne-hot\n1,000\n4\nNone\nExact\nCNNs, short contexts\n\n\nOverlapping k-mers\n~1,000\n\\(4^k\\) (e.g., 4,096)\nNone\nAmbiguous (affects k tokens)\nProof-of-concept transformers\n\n\nBPE\n200-500\nTunable (4K-32K)\n2-5x\nContext-dependent\nLong-context transformers\n\n\nSingle-nucleotide\n1,000\n4-5\nNone\nExact\nSub-quadratic architectures",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-single-nucleotide",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-single-nucleotide",
    "title": "5  Tokens and Embeddings",
    "section": "5.4 Single-Nucleotide Tokenization: Maximum Resolution",
    "text": "5.4 Single-Nucleotide Tokenization: Maximum Resolution\nWhile \\(k\\)-mer and BPE tokenization compress sequences to enable longer context windows, they sacrifice the single-nucleotide resolution essential for variant effect prediction. A single nucleotide polymorphism (SNP) can completely alter protein function through mechanisms ranging from amino acid substitution to splice site disruption to regulatory element ablation. When a pathogenic variant and a benign variant differ by one nucleotide position, multi-nucleotide tokens obscure exactly where variants fall and how they relate to the boundaries of biological features.\nHyenaDNA took the opposite approach in 2023, using single-nucleotide tokens with no compression whatsoever (Nguyen et al. 2023). Each nucleotide (A, C, G, T) becomes a separate token, maintaining maximum possible resolution. Every nucleotide is independently represented, SNP effects can be isolated to specific token positions without ambiguity, and no tokenization artifacts depend on surrounding sequence context.\nThe challenge is sequence length. A 1 Mb region requires 1 million tokens, far beyond standard transformer capacity. HyenaDNA addressed this through architectural innovation rather than tokenization compromise. The Hyena architecture replaces the attention mechanism with implicit convolutions (long convolutions parameterized by a small neural network) that scale sub-quadratically with sequence length. Where attention computes explicit pairwise interactions between all positions, Hyena achieves similar representational power through operations whose cost grows only slightly faster than linearly. This enables processing sequences hundreds of times longer than attention-based transformers within the same computational budget. The architectural principles underlying Hyena and related sub-quadratic approaches are examined in detail in Chapter 7.\nThe practical impact was substantial: a 500-fold increase in context length over dense attention models while maintaining single-nucleotide resolution. HyenaDNA could process 1 Mb sequences where DNABERT was limited to approximately 500 bp and the Nucleotide Transformer to approximately 6 kb. On the Nucleotide Transformer benchmarks, HyenaDNA reached state-of-the-art performance on 12 of 18 datasets with orders of magnitude fewer parameters and less pretraining data. On GenomicBenchmarks, it surpassed prior state-of-the-art on 7 of 8 datasets by an average of 10 accuracy points.\nHyenaDNA also demonstrated the first use of in-context learning in genomics. The model could perform tasks based on examples provided in the context window without any fine-tuning (conditioning on demonstration sequences rather than updating parameters). This capability, familiar from large language models, had not previously been shown for genomic sequences.\nWhy does sub-quadratic complexity enable in-context learning? In-context learning requires both (1) long context to hold demonstration examples and (2) sufficient model capacity for those examples to influence computation. Attention’s quadratic cost forced prior models to choose: short context to save computation, or tiny embeddings to fit within memory limits. Sub-quadratic architectures eliminate this constraint, enabling both long context and rich representations simultaneously. With a 1 Mb context window, the model can see many demonstration sequences before making a prediction—creating the conditions for in-context learning to emerge from scale rather than from architectural changes. See section ?sec-ch09-emerging-approaches for more on in-context learning.\n\n\n\n\n\n\nKey Insight\n\n\n\nSub-quadratic architectures fundamentally changed the tokenization calculus. When computational constraints no longer force a choice between resolution and context length, single-nucleotide tokenization becomes the natural choice for applications requiring precise variant interpretation. HyenaDNA decoupled the resolution decision from the context length decision.\n\n\nThe development of sub-quadratic architectures including Hyena, Mamba, and state space models has fundamentally changed the tokenization calculus [Citations Needed]. When computational constraints no longer force a choice between resolution and context length, single-nucleotide tokenization becomes the natural choice for applications requiring precise variant interpretation. The architectural innovations examined in Chapter 7 effectively decouple the resolution decision from the context length decision, eliminating what had seemed like an inherent tradeoff. HyenaDNA and Caduceus, examined in Chapter 14, demonstrate how these architectures enable million-base contexts at single-nucleotide resolution.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-biological-tokenization",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-biological-tokenization",
    "title": "5  Tokens and Embeddings",
    "section": "5.5 Biologically-Informed Tokenization",
    "text": "5.5 Biologically-Informed Tokenization\nStandard tokenization schemes treat DNA as a homogeneous string of characters, ignoring the biological reality that different genomic regions serve fundamentally different functions and follow different structural rules. Coding sequences obey a strict codon structure where every three nucleotides encode an amino acid; noncoding regions have no such constraint. Treating these regions identically wastes an opportunity to build biological knowledge directly into the representation.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider a mutation that changes the third position of a codon from AAG (lysine) to AAA (also lysine)—a synonymous change that preserves amino acid identity. How would different tokenization strategies represent this mutation? Would a k-mer approach capture that this is synonymous? Would codon-level tokenization?\n\n\nFor protein-coding regions, the natural unit of sequence is the codon rather than the individual nucleotide. GenSLMs pioneered codon-level tokenization for genomic foundation models in 2022, treating each three-nucleotide codon as a single token and exploiting the fact that codons are the biologically meaningful units of protein-coding sequence (Zvyagin et al. 2022). The 64-codon vocabulary captures the complete space of possible genetic code words, with each token corresponding to either an amino acid or a stop signal. This alignment with translation semantics means that mutations affecting amino acid identity (nonsynonymous changes) alter the token sequence, while synonymous mutations within a codon alter the specific token used but maintain broader codon-family structure.\n\n\n\n\n\n\nBiologically-informed tokenization strategies\n\n\n\n\nFigure 5.2: Biologically-informed tokenization strategies. Standard BPE (top) tokenizes across codon boundaries in coding regions, potentially obscuring the fundamental three-nucleotide unit of protein translation. Codon-aware tokenization (middle, as in GenSLMs and Life-Code) respects reading frame, with each codon becoming a single token from a 64-element vocabulary. This alignment with biological structure enables learning synonymous versus nonsynonymous substitution patterns directly. BioToken-style tokenization (bottom) extends further by incorporating explicit variant tokens, regulatory element markers, and structural annotations, treating tokens as rich entities bundling sequence with functional context.\n\n\n\nLife-Code extended codon-aware tokenization to broader genomic contexts in 2025, encoding coding and noncoding regions in a way that respects reading frame and local biological function (Liu et al. 2025). Coding regions are tokenized by codons, aligning token boundaries with the fundamental unit of protein translation. Noncoding regions, lacking codon structure, are tokenized by learned patterns capturing regulatory motifs and other functional elements. This biologically-informed design enables Life-Code to learn protein structure through knowledge distillation from protein language models, capture interactions between coding and noncoding regions within a unified framework, and achieve state-of-the-art results across tasks involving DNA, RNA, and protein.\nBioToken extends tokenization further to include explicit genomic structural annotations (Medvedev et al. 2025). Rather than treating variants as implicit changes in the sequence string, BioToken creates tokens explicitly representing SNPs, insertions, and deletions. Known regulatory elements receive dedicated tokens encoding their presence and type. Gene structure, chromatin state, and other functional annotations integrate directly into the token representation. This approach treats tokens as rich entities bundling nucleotides with positional, functional, or experimental context.\nVariant-aware representations hold particular promise for clinical applications, where the input is often “reference plus variant” rather than a generic sequence. By incorporating biological inductive biases directly into tokenization, BioToken’s associated model achieves competitive or superior performance to specialized models like Enformer and SpliceAI with significantly fewer parameters. This efficiency suggests that appropriate representation can partially substitute for model scale by making the learning problem easier through informed structure.\nThe broader principle is that tokenization can and should incorporate biological structure when that structure is known and relevant. BPE learns statistical patterns from the corpus, but those patterns need not correspond to biological units. Codon tokenization imposes biological semantics directly, at the cost of applicability to noncoding regions. Future approaches might combine these strategies: codon-aware tokenization for coding regions, BPE or single-nucleotide tokens for noncoding sequence, and explicit variant tokens for clinical interpretation tasks.\n\n\n\n\n\n\n\n\nGC content organization\n\n\n\n\n\n\n\nToken frequency organization\n\n\n\n\n\n\n\nGenomic context organization\n\n\n\n\n\n\nFigure 5.3: Emergent structure in learned DNA token embeddings. UMAP projections of token embeddings from a trained DNA language model reveal biologically meaningful organization that emerges without explicit supervision. (A) GC content creates a major gradient, with AT-rich and GC-rich tokens segregating. (B) Token frequency organizes embeddings, with common tokens clustering distinctly from rare tokens. (C) Genomic context (coding, regulatory, repetitive) corresponds to distinct embedding regions. This organization demonstrates that pretraining on sequence prediction objectives induces representations capturing genuine biological structure.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-embeddings",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-embeddings",
    "title": "5  Tokens and Embeddings",
    "section": "5.6 From Tokens to Embeddings: Learning Representations",
    "text": "5.6 From Tokens to Embeddings: Learning Representations\nA patient’s genome contains a variant of uncertain significance (VUS) in SCN5A, a cardiac ion channel gene. Whether this variant affects protein function depends on subtle sequence features that determine how the protein folds, where it localizes, and how it interacts with other cellular components. The clinical question is binary (pathogenic or benign), but the biological answer emerges from continuous biophysical properties. Classical methods for variant interpretation (Chapter 4) capture some of these relationships through hand-crafted features; learned embeddings offer an alternative approach where relevant patterns emerge from data.\nThink of embeddings like giving each word a GPS coordinate rather than a dictionary index. A dictionary assigns arbitrary numbers to words (apple = 47,231; orange = 89,102), revealing nothing about their relationships. GPS coordinates, by contrast, place similar items near each other: the coordinates for “apple” and “orange” would cluster in the fruit section, while “hammer” and “screwdriver” would cluster elsewhere. Embeddings work the same way for tokens—they assign coordinates in a mathematical space where similar tokens naturally end up near each other, enabling the model to generalize from one to another.\nThis gap between discrete genetic variation and continuous biological effect is precisely what embedding layers must bridge: transforming discrete tokens into dense numerical representations that neural networks can process and from which they can learn.\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThe following section introduces embedding mathematics. Readers unfamiliar with matrix notation can focus on the core intuition: embedding layers convert discrete tokens into continuous vectors, and through training, these vectors organize to reflect meaningful relationships—even relationships not explicitly taught.\n\n\nThe operation itself is simple: a lookup table assigns each token to a learned vector. The embedding layer maintains a matrix \\(E\\) of dimensions \\(V \\times d\\), where \\(V\\) is vocabulary size and d is embedding dimension. Each token maps to a row of this matrix, and during training, backpropagation adjusts the embedding vectors to support downstream prediction. This simplicity belies its importance; the distinction between discrete tokens and their dense representations shapes what models can learn.\nConsider the difference between one-hot encoding and learned embeddings. A one-hot representation treats each nucleotide as maximally distinct from every other: the dot product between any two different nucleotides is zero, providing no information about their relationships. Adenine and thymine are equally different from each other as adenine and guanine, despite the biological reality that purines (A, G) share structural properties distinct from pyrimidines (C, T), and that complementary base pairs (A-T, G-C) have special significance for DNA structure and function.\nLearned embeddings allow the model to discover such relationships from data. If distinguishing purines from pyrimidines helps the model predict regulatory function, the embedding space will organize to reflect this distinction. If complementary relationships matter, they will emerge in the geometry of the learned space.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nIn a one-hot encoding, the vectors for A, C, G, and T are all orthogonal (perpendicular) to each other. What biological relationships would you expect to emerge in a learned embedding space trained on regulatory sequence prediction? Consider which nucleotides might end up closer together and why.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nIn learned embeddings for regulatory prediction, you would expect purines (A, G) to cluster closer together and pyrimidines (C, T) to cluster closer together, reflecting their similar chemical properties and interchangeable roles in many binding motifs. Additionally, complementary base pairs (A-T and G-C) might show proximity because they appear in similar sequence contexts when models learn strand-invariant patterns.\n\n\n\n\n\nThe embedding dimension \\(d\\) controls representational capacity. Small embeddings of 32 to 64 dimensions suffice for simple tokenization schemes like single nucleotides, where only four vectors must be distinguished. Larger vocabularies require larger embeddings: DNABERT-2’s BPE tokens use 768-dimensional embeddings, comparable to natural language models. The choice involves a tradeoff between expressiveness and efficiency, as larger embeddings increase both model capacity and computational cost. [Citation Needed]\nAnalysis of trained DNA language models reveals that embedding spaces organize around biologically meaningful properties even without explicit supervision. GC content, often considered a nuisance variable in genomics, emerges as a major axis of variation in embedding space because it correlates with many functional properties including gene density, chromatin accessibility, and mutation rate. Repetitive elements cluster together in embedding space. Coding sequence embeddings differ systematically from noncoding embeddings, even when the tokenization scheme makes no explicit distinction between these region types. [Citation Needed]\nThis emergent organization has practical implications. The structure learned in the embedding layer propagates through all subsequent computations. If embeddings fail to capture relevant distinctions, later layers must learn them from scratch. If embeddings encode spurious correlations, the model may exploit them inappropriately. Understanding what embeddings learn, and whether that learning aligns with biological reality, becomes an important diagnostic for model behavior. Systematic probing of these learned representations (Section 9.3) reveals what patterns models have captured, while interpretability methods (Chapter 24) trace how these representations influence downstream predictions.\nThe relationship between tokenization and embedding deserves emphasis. Coarse tokenization through large k-mers or aggressive BPE creates more token types, each with room for rich embedding representations but requiring the model to learn more parameters. Fine tokenization through single nucleotides creates fewer token types with simpler embeddings but forces the model to build complex representations through composition across layers. Neither approach is uniformly superior; the optimal choice depends on available training data, model scale, and task requirements.\n\n5.6.1 Position in Sequence\nA mutation at position 3 of the HBB gene is not equivalent to one at position 300, even if both create the same codon change. Position determines proximity to the promoter TATA box, distance from splice junctions, and location relative to regulatory elements that fine-tune expression. The canonical HBB promoter mutation at position -28 causes beta-thalassemia by disrupting the TATA box, while an identical sequence change elsewhere would be benign. Position is not metadata—it is biology.\nTokenization converts sequence to discrete symbols, but genomic function depends on where those symbols appear. A transcription factor binding site has entirely different effects depending on whether it sits in a promoter, an enhancer, or a gene body. The same variant at position -30 relative to a transcription start site carries different implications than at position +500. Transformers are inherently permutation-invariant: shuffling token order changes nothing about how attention weights are computed. Position must be explicitly encoded.\nPositional encodings address this by injecting location information into token representations. Strategies range from learned embeddings (which assign a trainable vector to each position) to mathematical schemes like sinusoidal encodings or rotary position embeddings that can extrapolate to sequence lengths beyond training. The choice of positional encoding determines what spatial relationships a model can learn and how well it generalizes across genomic scales. Detailed treatment of positional encoding strategies appears in Section 7.2, where they are examined alongside the attention mechanisms they enable.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-biological-special",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-biological-special",
    "title": "5  Tokens and Embeddings",
    "section": "5.7 Special Considerations for Biological Sequences",
    "text": "5.7 Special Considerations for Biological Sequences\nThe double-stranded nature of DNA creates an ambiguity that has no parallel in natural language: should a model treat the forward and reverse complement strands as the same sequence, different sequences, or related-but-distinct entities? A transcription factor binding site for p53 functions when bound to either strand, yet the gene it regulates is transcribed from only one. This strand ambiguity ripples through every aspect of model design, from data augmentation to architectural constraints to output interpretation.\nA sequence ACGT on the forward strand corresponds to ACGT read 5’ to 3’, but also implies the reverse complement TGCA on the opposite strand read in the opposite direction. Some biological features are strand-specific: a gene on the forward strand is transcribed from that strand only. Other features are strand-agnostic: many transcription factor binding sites function identically on either strand. Representation schemes must decide whether to treat strands as equivalent through data augmentation with reverse complements, as distinct through explicit strand encoding, or as related-but-different through equivariant architectures processing both strands jointly.\nThe Nucleotide Transformer addressed strand by including both orientations during training, using data augmentation to ensure the model sees sequences from both directions (Dalla-Torre et al. 2023). Caduceus introduced a more elegant solution in 2024: a bidirectional architecture processing forward and reverse complement strands simultaneously through shared computation (Schiff et al. 2024). The model outputs are equivariant to reverse complementation (reversing and complementing the input produces correspondingly transformed outputs). This inductive bias ensures consistent treatment of strand without requiring augmentation or doubling computational cost.\nCircular genomes present another topological consideration. Bacterial chromosomes and plasmids, mitochondrial DNA, and many viral genomes are circular, with no natural start or end position. Linear position encodings impose arbitrary boundaries on these sequences. Some models address this through circular position encodings that wrap around at sequence boundaries, while others process circular genomes as linear sequences with the understanding that boundary effects may introduce artifacts. [Citation Needed]\nGenomic coordinates carry information absent from raw sequence. The position chr17:41,276,045 refers to a specific location in the BRCA1 gene, and variants at this position have been extensively studied. Knowing the genomic coordinate enables lookup of prior knowledge: population frequencies from gnomAD, clinical interpretations from ClinVar, functional annotations from ENCODE. Some representation schemes incorporate coordinate information explicitly, enabling models to learn position-specific patterns and integrate with external databases. Others deliberately exclude coordinates to force models to learn purely from sequence, trading prior knowledge for generalization to novel sequences or other species.\nMultiple sequence inputs arise frequently in genomic applications. Variant effect prediction requires comparing reference and alternate alleles. Comparative genomics involves aligned sequences from multiple species. Some regulatory predictions require input from multiple genomic regions, such as promoter plus enhancer. Representation schemes must accommodate these multi-sequence inputs through concatenation, paired encoding, or specialized architectures processing multiple sequences jointly.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-tradeoffs",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-tradeoffs",
    "title": "5  Tokens and Embeddings",
    "section": "5.8 Tradeoffs and Practical Guidance",
    "text": "5.8 Tradeoffs and Practical Guidance\nThe choice between tokenization strategies involves multiple competing considerations depending on the intended application. Understanding these tradeoffs enables informed design decisions rather than arbitrary choices.\n\n\n\n\n\n\nThe compression-resolution tradeoff\n\n\n\n\nFigure 5.4: The compression-resolution tradeoff in genomic tokenization. This plot positions tokenization strategies along two axes: sequence compression (tokens per kilobase) and nucleotide resolution. One-hot and single-nucleotide tokenization occupy the upper-left corner, providing maximum resolution with no compression. Overlapping k-mers also provide no compression but reduce resolution to k-nucleotide granularity. Non-overlapping k-mers and BPE occupy the middle ground, trading resolution for compression that enables longer context windows with standard transformer architectures. The clinical implication appears in variant interpretation: a single-nucleotide polymorphism affects 1 token with single-nucleotide tokenization but k tokens with overlapping k-mers, complicating effect attribution. Sub-quadratic architectures (HyenaDNA, Caduceus) escape this tradeoff entirely, enabling full resolution at any context length.\n\n\n\n\n5.8.1 Resolution Versus Compression\nA splice site mutation at a precise GT dinucleotide causes disease; missing it by one nucleotide means missing the diagnosis entirely. Yet understanding why that splice site is used requires seeing the branch point 20-50 nucleotides upstream, the polypyrimidine tract, and competing splice sites hundreds of bases away. Biology demands both precision and panorama simultaneously.\nThe tension between compression and resolution represents the fundamental tradeoff. Higher compression enables longer context windows within fixed computational budgets but loses precision for identifying exactly where variants fall and how they relate to biological features. One-hot encoding and single-nucleotide tokenization provide no compression but maintain full resolution. Non-overlapping k-mers achieve approximately \\(k\\)-fold compression at the cost of \\(k\\)-nucleotide resolution. BPE provides variable compression depending on sequence repetitiveness, with correspondingly variable resolution. For variant effect prediction (Chapter 17), where single nucleotide changes can have dramatic phenotypic consequences, resolution is paramount and the computational costs of long single-nucleotide sequences are often justified.\n\n\n5.8.2 Vocabulary Size and Model Capacity\nTranscription factors recognize specific 6-12 nucleotide motifs, but these motifs come in families with degenerate positions that tolerate multiple bases. A vocabulary containing all possible 6-mers can represent each motif variant as a distinct token, potentially capturing family relationships in embedding space. A vocabulary of only four nucleotides forces the model to learn these motif patterns compositionally across layers. The question is whether richer vocabularies accelerate learning or simply shift where the learning happens.\nVocabulary size affects both model capacity and efficiency in ways that interact with embedding design. Larger vocabularies require bigger embedding tables but may capture more complex patterns directly in the token representation. Smaller vocabularies are parameter-efficient but require the model to learn compositional structure through multiple layers. One-hot encoding’s vocabulary of four tokens (plus special tokens) minimizes embedding parameters but maximizes the compositional learning burden. K-mer vocabularies scale exponentially with \\(k\\), reaching 4,096 for 6-mers. BPE vocabularies are tunable, typically ranging from 4,096 to 32,000 tokens for genomic applications. [Citation Needed]\n\n\n5.8.3 Computational Efficiency\nEnhancers can regulate genes from a million base pairs away, requiring models to consider vast genomic contexts. Processing such distances at single-nucleotide resolution with naive attention would require a trillion pairwise comparisons per layer—clearly impractical. Either we compress the sequence into fewer tokens, or we need architectures that scale more gently. This computational reality directly constrains which biological questions models can even attempt to answer.\nComputational efficiency depends on both tokenization and architecture in ways that have shifted as new architectures have emerged. For standard attention with \\(O(L^2)\\) complexity, any compression directly reduces cost: non-overlapping k-mers reduce attention cost by a factor of \\(k^2\\), and BPE with average compression \\(c\\) reduces cost by \\(c^2\\). Sub-quadratic architectures like Hyena and Mamba change this calculus entirely, making single-nucleotide tokenization computationally feasible at long contexts and eliminating the need to trade resolution for efficiency (Chapter 7).\n\n\n5.8.4 Variant Interpretation Requirements\nVariant interpretation has specific requirements favoring certain representation choices. Single-nucleotide tokens enable clean comparison of reference and alternate alleles at the same token position with no ambiguity about effect localization. K-mer tokens complicate matters because a single SNP changes \\(k\\) overlapping tokens, requiring aggregation across affected tokens and introducing potential boundary effects. BPE tokens create context-dependent effects where the same variant may fall at different positions relative to token boundaries depending on surrounding sequence. Foundation model approaches to variant effect prediction (Chapter 17) must navigate these tokenization constraints when scoring single-nucleotide changes.\n\n\n5.8.5 Practical Heuristics\nSeveral heuristics have emerged from practical experience. Single-nucleotide tokens work best when variant-level reasoning or high-resolution interpretability is central to the application. K-mers or BPE provide advantages when context length is the primary bottleneck and tasks do not require base-level precision. Biologically-informed tokens merit consideration when integrating multi-modal or annotation-rich data. Position encoding should match task requirements: relative encodings for tasks where absolute position is arbitrary, coordinate-aware encodings for clinical applications requiring integration with external databases (Section 7.2).\n\n\n\n\n\n\nPractical Guidance: Choosing a Tokenization Strategy\n\n\n\nFor variant effect prediction: Use single-nucleotide tokenization with sub-quadratic architectures (HyenaDNA, Caduceus). Resolution matters more than context length for SNP interpretation.\nFor long-range regulatory modeling: Use BPE with standard transformers, or single-nucleotide with sub-quadratic architectures. Context length matters; modest compression is acceptable.\nFor protein-coding regions: Consider codon-level tokenization (GenSLMs, Life-Code) to align tokens with biological units of translation.\nFor clinical interpretation with annotation integration: Consider BioToken-style representations that explicitly encode variants and functional elements.\nWhen in doubt: Start with single-nucleotide tokens and a sub-quadratic architecture. You can always add compression later, but recovering lost resolution is impossible.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch05-representations.html#sec-ch05-foundation",
    "href": "part_2/p2-ch05-representations.html#sec-ch05-foundation",
    "title": "5  Tokens and Embeddings",
    "section": "5.9 Representation as Foundation",
    "text": "5.9 Representation as Foundation\nWhen researchers trained a model on 6-mer tokens to predict gene expression, they later discovered it could not be repurposed for splice site detection—the tokenization that compressed regulatory sequences had fragmented the critical GT-AG dinucleotides across token boundaries. The preprocessing decision made during pretraining had permanently constrained what downstream tasks were achievable.\nThese choices propagate through every subsequent modeling decision. Position encodings in transformers must align with token boundaries. Convolutional receptive fields span tokens, not nucleotides, making effective genomic range dependent on tokenization (Chapter 6). Transfer learning inherits the tokenization of the pretrained model, constraining how representations can be adapted to new tasks (Chapter 9). A model pretrained with 6-mer tokenization cannot be fine-tuned for single-nucleotide variant interpretation without architectural modification.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA research team has pretrained a DNA language model using BPE tokenization with an average compression ratio of 4:1. They want to fine-tune it for splice site prediction, which requires identifying exact dinucleotide boundaries (GT…AG). What challenges might they face? How might they address these challenges?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe main challenge is that BPE tokens have variable length and may not align with the precise GT and AG dinucleotides that define splice boundaries. A token might contain part of GT plus neighboring bases, making it difficult to pinpoint the exact junction. They could address this by: (1) using single-nucleotide tokenization for the fine-tuning task despite the compression loss, (2) adding position-specific prediction heads that operate on nucleotide-level representations, or (3) using a hybrid approach where BPE captures long-range context but final predictions use nucleotide-resolution features.\n\n\n\n\n\nThe field has moved from treating tokenization as fixed preprocessing to recognizing it as a fundamental design decision shaping what models can learn. Some architectures now learn tokenization jointly with prediction, discovering representations optimized for specific tasks rather than fixed in advance. As contexts extend to chromosome scale and models grow to billions of parameters, the representation problem will remain central to genomic foundation model design.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhy does overlapping k-mer tokenization provide no sequence compression, and what problem does this create for transformers processing long genomic contexts?\nExplain how byte-pair encoding (BPE) learns its vocabulary from corpus statistics, and why repetitive genomic elements receive longer tokens than unique sequences.\nA single-nucleotide variant falls within a 6-mer token. How many different tokens does this SNP affect, and why does this complicate variant effect interpretation?\nWhat architectural innovation enabled HyenaDNA to process million-base-pair contexts at single-nucleotide resolution, escaping the compression-resolution tradeoff?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nOverlapping k-mer compression: Overlapping k-mer tokenization generates one token per nucleotide position (minus k-1 at the end), meaning a 10 kb sequence still requires approximately 10,000 tokens. This fails to address transformers’ quadratic attention complexity, which scales as O(L^2) with sequence length, making long genomic contexts computationally prohibitive despite the apparent use of multi-nucleotide tokens.\nBPE vocabulary learning: BPE iteratively identifies the most frequent adjacent token pair in the corpus, merges it into a new token, and repeats this process thousands of times. Repetitive genomic elements like Alu sequences appear over one million times in the human genome, causing their characteristic subsequences to be merged early into long dedicated tokens, while unique random sequences remain decomposed into shorter subunits or single nucleotides.\nSNP effect on k-mers: A single-nucleotide variant affects exactly k tokens (6 tokens for 6-mers) because the changed nucleotide appears in k different overlapping windows: the window starting at that position plus the k-1 windows starting before but including that position. This propagation complicates interpretation because the effect must be aggregated across multiple tokens, making it difficult to isolate which token drives the prediction and whether boundary effects influence model behavior.\nSub-quadratic architectures: HyenaDNA replaced the attention mechanism with implicit long convolutions (parameterized by a small neural network) that scale sub-quadratically with sequence length. Where attention computes explicit pairwise interactions between all positions (O(L^2)), Hyena achieves similar representational power through operations whose cost grows only slightly faster than linearly, enabling 500-fold longer contexts than dense attention models while maintaining single-nucleotide resolution.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nKey concepts: tokenization, one-hot encoding, k-mer tokenization, byte pair encoding (BPE), single-nucleotide tokenization, biologically-informed tokenization, embeddings, positional encodings, strand handling\nMain takeaways:\n\nTokenization constrains learning: How sequence is segmented into tokens determines what resolution the model can achieve and what patterns it can detect. This is a design decision, not mere preprocessing.\nCompression versus resolution: One-hot and single-nucleotide tokenization preserve full resolution but create long sequences. BPE compresses sequences but at variable and context-dependent resolution. Sub-quadratic architectures (HyenaDNA, Caduceus) eliminate this tradeoff.\nEmbeddings discover structure: Learned embeddings organize to reflect biologically meaningful properties (GC content, coding vs. noncoding, repetitive elements) even without explicit supervision.\nBiology can inform tokenization: Codon-aware tokenization (GenSLMs, Life-Code) and variant-aware representations (BioToken) incorporate biological knowledge directly into the representation layer.\nMatch representation to task: Variant effect prediction demands single-nucleotide resolution. Long-range regulatory modeling demands long context. The optimal tokenization strategy depends on the biological question.\n\nLooking ahead: The architectural innovations that process these representations—convolutional filters, attention mechanisms, and sub-quadratic alternatives—are examined in Chapter 6 and Chapter 7. How these learned representations can be probed and interpreted appears in Section 9.3 and Chapter 24.\n\n\n\n\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.\n\n\nZvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. “GenSLMs: Genome-Scale Language Models Reveal SARS-CoV-2 Evolutionary Dynamics.” bioRxiv. https://doi.org/10.1101/2022.10.10.511571.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tokens and Embeddings</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html",
    "href": "part_2/p2-ch06-cnn.html",
    "title": "6  Convolutional Networks",
    "section": "",
    "text": "6.1 Convolutions as Sequence Pattern Detectors\nA variant in an enhancer 50 kilobases from its target gene cannot be connected to that gene by a model that sees only 1,000 base pairs of context. Consider a patient with familial hypercholesterolemia whose whole-genome sequencing reveals a novel variant upstream of LDLR. The variant sits within a known enhancer region, but the enhancer and the LDLR promoter lie beyond the window any convolutional layer can span. The model might correctly identify regulatory features at the variant position, but it cannot learn that those features regulate LDLR rather than some other gene. This receptive field constraint, inherent to convolutional architectures, determines what relationships these networks can and cannot discover. The constraint is not a limitation of training data or compute; it is architectural.\nA convolutional filter slides across an input sequence, computing similarity scores at each position. For genomic applications, the input is typically one-hot encoded DNA: a binary matrix with four rows (A, C, G, T) and columns for each position (see Chapter 5 for detailed treatment of sequence encoding strategies). Filters learn weight patterns that respond to specific nucleotide arrangements. A filter of width 8 nucleotides, for instance, computes a weighted sum of the underlying nucleotides at each position, producing high activation when the sequence matches its learned pattern and low activation otherwise. This operation is mathematically equivalent to scanning a position weight matrix across the sequence, but with a crucial difference: the filter weights are learned during training rather than derived from aligned binding site sequences.\nThe first layer of a genomic CNN typically contains hundreds of such filters, each learning to detect different local patterns. Analysis of trained filters consistently reveals correspondence to known transcription factor binding motifs. The CTCF insulator motif, the ETS family consensus sequence, the AP-1 binding site: these patterns emerge from training on chromatin data without any explicit motif supervision. The network identifies them because they predict the training labels. Supervision on chromatin state induces discovery of the sequence patterns that create chromatin state, providing unsupervised motif learning as a byproduct of supervised prediction.\nDeeper layers operate on the output of earlier layers rather than raw sequence. A second-layer filter might learn to detect specific arrangements of first-layer motifs: two ETS sites within 20 base pairs, or a CTCF motif flanked by particular spacing patterns. This hierarchical feature learning enables CNNs to capture regulatory grammar beyond individual motifs, including spacing constraints, orientation preferences, and combinatorial requirements that govern transcription factor cooperativity.\nBetween convolutional layers, spatial resolution must decrease while the receptive field expands. Pooling operations achieve this tradeoff: max pooling selects the strongest activation within a window, achieving position-invariant detection where the network responds to a motif’s presence somewhere in a region rather than its exact position. Why does this position invariance matter? Transcription factors typically locate their binding sites through scanning and diffusion rather than measuring exact distances from fixed landmarks. A CTCF site functions whether it sits at position 150 or 180 within an enhancer; the regulatory logic depends on which motifs co-occur, not their precise coordinates. Pooling encodes this biological flexibility directly into the architecture, preventing the network from memorizing position-specific patterns that would fail to generalize across genomic contexts.\nThe receptive field of a convolutional network defines how much input sequence can influence a single output prediction. For a network with kernel width \\(k\\), pooling factor \\(p\\), and \\(L\\) layers, the receptive field grows with depth but remains fundamentally limited by architecture. A three-layer network with typical parameters might integrate information from 200 to 1,000 base pairs. Reaching further requires either more layers (increasing computational cost and training difficulty) or dilated convolutions that space filter weights to sample larger regions. When biological dependencies span tens of kilobases, this receptive field ceiling becomes the fundamental constraint that no amount of training data can overcome.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-convolutions",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-convolutions",
    "title": "6  Convolutional Networks",
    "section": "",
    "text": "Stop and Think\n\n\n\nBefore reading about how convolutions work, consider: if you wanted to detect a transcription factor binding site (typically 6-12 nucleotides) at any position along a DNA sequence, what properties would your detection algorithm need? How would it differ from simply searching for an exact sequence match?\n\n\n\n\n\n\n\n\n\n\nKey Insight\n\n\n\nConvolutional filters are learned position weight matrices. The difference from classical PWMs is not in what they compute, but in how they are derived: through gradient descent on prediction tasks rather than sequence alignment. This means filters discover patterns that predict the training labels, not just patterns that recur in sequences.\n\n\n\n\n\n\n\n\nWorked Example: Convolution Computation\n\n\n\nConsider a filter of width 4 learning to detect the TATA box motif. Why width 4? The TATA box consensus is approximately 4-6 bp, so a 4-bp filter can capture its core pattern. Filters narrower than the motif miss context; filters much wider waste capacity on flanking sequence that varies between binding sites. The first CNN layer typically uses multiple filter widths (4, 8, 12, 16 bp) to capture motifs of different lengths simultaneously—transcription factor binding sites range from 6-12 bp, so no single width suffices.\nAfter training, the filter weights might be:\n\n\n\nPosition\nA\nC\nG\nT\n\n\n\n\n1\n-0.2\n-0.5\n-0.5\n1.8\n\n\n2\n1.5\n-0.3\n-0.4\n-0.2\n\n\n3\n-0.2\n-0.5\n-0.5\n1.9\n\n\n4\n1.6\n-0.4\n-0.3\n-0.3\n\n\n\nFor input sequence “TATA”:\n\nPosition 1 (T): weight = 1.8\nPosition 2 (A): weight = 1.5\nPosition 3 (T): weight = 1.9\nPosition 4 (A): weight = 1.6\nTotal activation: 6.8 (high match)\n\nFor input sequence “GCGC”:\n\nPosition 1 (G): weight = -0.5\nPosition 2 (C): weight = -0.3\nPosition 3 (G): weight = -0.5\nPosition 4 (C): weight = -0.4\nTotal activation: -1.7 (low match)\n\nThe filter produces high activation for sequences matching its learned pattern (TATA) and low/negative activation for mismatches. As this filter slides across a long sequence, it produces a profile of TATA box likelihood at each position.\n\n\n\n\n\n\n\n\n\n\n\n\nSliding convolution\n\n\n\n\n\n\n\nLearned filter as sequence logo\n\n\n\n\n\n\n\nMultiple filter diversity\n\n\n\n\n\n\nFigure 6.1: Convolutional filters as learned position weight matrices. (A) A filter of width 8 slides across one-hot encoded DNA, producing activation scores at each position. High activation indicates sequence matching the learned pattern. (B) Visualizing filter weights as sequence logos reveals correspondence to known transcription factor motifs. This filter has learned the CTCF binding site consensus matching the JASPAR database entry. (C) Multiple first-layer filters detect diverse motifs including CTCF, ETS, AP-1, and TATA box elements. This specialization emerges from training on chromatin prediction without explicit motif supervision.\n\n\n\n\n\n\n\n\n\n\nDeep Dive: Receptive Field\n\n\n\nFor biology readers: The receptive field is how much input sequence can influence each output prediction:\nThe concept: Imagine looking at a photo through a small window—you can only see a portion at a time. A CNN’s receptive field is like this window: it determines how much genomic context the model can “see” when making predictions at any position.\nHow it grows:\n\nA single convolutional layer with filter width 8 sees 8 nucleotides\nStacking layers expands the view: each layer adds context from the previous layer’s outputs\nPooling (downsampling) dramatically expands receptive field by compressing spatial information\n\nTypical sizes:\n\n\n\nModel\nReceptive Field\nWhat It Can Capture\n\n\n\n\nDeepSEA\n~1,000 bp\nLocal motifs, TF binding\n\n\nSpliceAI\n~10,000 bp\nSplice sites + nearby regulatory elements\n\n\nEnformer\n~200,000 bp\nEnhancer-promoter interactions\n\n\n\nThe limitation: Biology doesn’t respect receptive field boundaries. If an enhancer 50 kb from a gene affects its expression, a model with 10 kb receptive field cannot learn this relationship—it simply cannot see both elements simultaneously. This architectural ceiling motivated the move to attention mechanisms.\n\n\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThe following paragraph introduces the receptive field formula. If you prefer to focus on intuition, the key point is: more layers and wider filters increase how much sequence a CNN can see, but these increases are limited by computational constraints.\n\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nBefore continuing, make sure you can answer: (1) What is a convolutional filter learning to detect? (2) Why does pooling create position invariance? (3) What determines how far a CNN can “see” in the input sequence?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nA convolutional filter learns to detect a specific sequence pattern or motif, responding strongly when that pattern appears in its receptive field. (2) Pooling creates position invariance by taking the maximum (or average) activation within a region, so the filter responds similarly whether the motif appears at position 10 or position 15 within that region. (3) The receptive field size determines how far a CNN can see, which is determined by the combination of filter width, number of layers, pooling operations, and dilation rates across the network.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-deepsea",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-deepsea",
    "title": "6  Convolutional Networks",
    "section": "6.2 DeepSEA: Regulatory Prediction from Sequence",
    "text": "6.2 DeepSEA: Regulatory Prediction from Sequence\nA patient presents with a rare disease phenotype, and whole-genome sequencing reveals a novel variant in an intron 15 kilobases from the nearest exon. The variant does not disrupt any annotated regulatory element. No prior patient in any database carries this exact change. The clinician must decide: is this variant pathogenic, or is it an irrelevant passenger? Annotation-based methods offer no guidance. The variant overlaps nothing cataloged, so overlap-based interpretation returns nothing useful. Yet introns harbor splice regulatory elements, and 15 kilobases places the variant well within range of enhancers that might control the adjacent gene.\nExisting approaches to noncoding variant interpretation relied on this overlap paradigm. If a variant fell within a ChIP-seq peak or DNase hypersensitive site, it might be flagged as potentially regulatory. The strategy grounded predictions in experimental observations, but it could not predict whether a variant would strengthen or weaken regulatory activity, could not score variants in regions lacking experimental coverage, and provided no mechanism for quantifying effect magnitude. A variant might fall within an enhancer, but would it matter? The data indicated where regulatory elements existed; they did not indicate how sequence changes would affect them.\nDeepSEA, introduced by Zhou and Troyanskaya, reframed the problem: rather than asking whether a variant overlaps known annotations, ask what regulatory activities a sequence encodes and how mutations would alter them (Zhou and Troyanskaya 2015). The shift from annotation lookup to sequence-based prediction enabled scoring any variant in any genomic context, including regions never assayed in any experiment. This reframing would prove more consequential than any specific architectural choice.\n\n\n\n\n\n\nKey Insight\n\n\n\nDeepSEA’s lasting contribution was conceptual, not architectural. The shift from “does this variant overlap an annotation?” to “what does this sequence encode?” changed variant interpretation from database lookup to computational prediction. This reframing enabled scoring variants in any genomic context, including regions never experimentally assayed.\n\n\n\n\n\n\n\n\n\n\nArchitecture schematic\n\n\n\n\n\n\n\nFirst-layer filter motif match\n\n\n\n\n\n\n\nAllelic imbalance validation\n\n\n\n\n\n\nFigure 6.2: DeepSEA: regulatory prediction from sequence. (A) Architecture schematic showing progression from 1,000 bp one-hot input through three convolutional layers to 919 chromatin feature predictions. (B) First-layer filters learn to recognize known transcription factor motifs, with this example matching JASPAR’s CTCF consensus. (C) Variant effect predictions validated against allelic imbalance measurements, confirming that sequence-based predictions capture genuine regulatory variation. DeepSEA demonstrated that deep learning on functional genomics data could discover regulatory patterns without encoding human assumptions about what matters.\n\n\n\n\n6.2.1 Architecture and Training\nThe clinical scenario described above demands a model that can predict function from sequence alone. DeepSEA’s architecture was deliberately simple by contemporary standards, placing the emphasis on the learning framework rather than architectural complexity.\nInput sequences of 1,000 base pairs, one-hot encoded, passed through three convolutional layers with 320, 480, and 960 filters respectively. Max pooling after each convolution compressed spatial dimensions. A fully connected layer with 925 units integrated information across the compressed representation, and a final output layer with 919 sigmoid units produced independent probability predictions for each chromatin profile.\nTraining data came from ENCODE and Roadmap Epigenomics (see Chapter 2 for comprehensive treatment of these resources): 690 transcription factor binding profiles, 104 histone modification profiles, and 125 DNase I hypersensitivity profiles spanning diverse cell types (Kagda et al. 2025; Kundaje et al. 2015). For each 1,000 bp input, the model predicted whether the central 200 bp region exhibited each chromatin feature. Chromosome 8 was held out for evaluation.\nThe multi-task learning formulation proved essential for generalization. Predicting 919 features simultaneously forced the network to learn shared representations useful across many prediction problems. Why does joint training help? Consider the alternative: 919 separate models, each learning from scratch that certain nucleotide patterns indicate regulatory activity. Each model would independently discover that GC-rich regions often mark promoters, that the CTCF motif signals insulator function, that splice site dinucleotides follow consensus sequences. Multi-task learning amortizes this redundant effort. The first convolutional layer learns general sequence patterns (GC content, common dinucleotides, ubiquitous motifs); these representations then feed task-specific combinations in later layers. Joint training also provides implicit regularization: a filter that helps predict many chromatin features captures genuine sequence grammar, while a filter useful for only one task may reflect dataset-specific artifacts. This pressure toward shared representations prevents overfitting to any single task while ensuring that learned features generalize across regulatory contexts.\n\n\n\n\n\n\nDeep Dive: Multi-Task Learning\n\n\n\nFor biology readers: Multi-task learning means training one model to predict many outputs simultaneously:\nThe concept: Instead of training 919 separate models (one per chromatin feature), DeepSEA trains one model with 919 output nodes. All predictions share the same learned sequence features.\nWhy it helps:\n\nShared learning: Basic motifs (CTCF, ETS family) are useful for predicting many chromatin features. Learning them once and sharing across tasks is more efficient than re-learning for each task.\nRegularization: Predicting many related outputs prevents the model from memorizing quirks of any single dataset. It must learn general patterns useful across contexts.\nTransfer: Features learned from abundant data (e.g., well-studied cell lines) help predictions for scarce data (e.g., rare cell types with few experiments).\n\nThe biology analog: Think of it like this—understanding general transcription factor binding helps you interpret specific experiments across many cell types, rather than starting from scratch for each.\nTrade-off: Multi-task learning assumes tasks share structure. If tasks are unrelated, forcing them to share features can hurt performance. For chromatin features, the shared regulatory grammar makes multi-task learning highly effective.\n\n\n\n\n\nTable 6.1: Summary of DeepSEA’s key architectural and training decisions.\n\n\n\n\n\n\n\n\n\n\nDesign Choice\nDeepSEA Implementation\nRationale\n\n\n\n\nInput length\n1,000 bp\nBalance between context and computation\n\n\nEncoding\nOne-hot (4 × L matrix)\nPreserves single-nucleotide resolution\n\n\nArchitecture\n3 conv layers + 1 FC\nSufficient depth for motif combinations\n\n\nOutput targets\n919 chromatin profiles\nMulti-task learning improves generalization\n\n\nLoss function\nBinary cross-entropy\nIndependent predictions per feature\n\n\nValidation\nChromosome 8 held out\nEnsures genomic generalization\n\n\n\n\n\n\n\n\n6.2.2 Learned Representations and Biological Validation\nAny sequence model faces a fundamental question: do learned features correspond to biological reality, or do they exploit statistical shortcuts that happen to correlate with labels? DeepSEA provided the first large-scale evidence that deep learning could recover genuine regulatory logic.\n\n\n\n\n\n\nStop and Think\n\n\n\nWhy is it significant that DeepSEA’s learned filters matched known transcription factor motifs? What would it mean if the filters learned to predict chromatin state without capturing recognizable biological patterns?\n\n\nAnalysis of first-layer filters revealed learned patterns matching known transcription factor motifs. The network had independently recovered sequence preferences cataloged in JASPAR and TRANSFAC, confirming that the training objective (predicting chromatin state) induced biologically meaningful feature extraction. This interpretability distinguished deep learning from prior black-box approaches and suggested that the models captured genuine regulatory logic rather than spurious correlations. Systematic methods for extracting and visualizing these learned representations, from filter analysis to attribution mapping, are examined in Chapter 24.\nDeeper layers combined first-layer patterns into more complex representations, capturing motif spacing requirements, orientation preferences, and cooperative binding arrangements. The network encoded relationships between sequence features that position weight matrices, operating independently at each motif, could not represent.\nDeepSEA outperformed gkm-SVM (gapped \\(k\\)-mer support vector machines) on nearly all transcription factor binding prediction tasks (Zhou and Troyanskaya 2015). The pattern of improvement revealed something fundamental: gkm-SVM showed no benefit from longer input sequences, while DeepSEA performance improved substantially with additional context. K-mer methods tally motif occurrences but cannot learn relationships between patterns at different positions. Hierarchical feature learning enables exactly what \\(k\\)-mer methods cannot provide: representations of combinatorial regulatory logic.\n\n\n6.2.3 Variant Effect Prediction\nWith a trained sequence-to-chromatin model, variant scoring becomes straightforward: predict chromatin profiles for reference and alternative sequences, compute the difference. This in silico mutagenesis produces a 919-dimensional vector describing predicted changes across all features. The model never encounters variant data during training; effect prediction emerges from learned sequence-function relationships applied to mutations the model has never seen.\nValidation used allelic imbalance data from digital genomic footprinting. For variants showing allele-specific DNase I sensitivity, DeepSEA predictions correlated with experimentally observed biases: variants predicted to increase accessibility tended to show higher accessibility on the corresponding allele. This correlation would not exist if the model merely learned coarse sequence features insensitive to point mutations.\nSystematic characterization of regulatory elements requires more than single-variant scoring. In silico saturation mutagenesis predicts effects of all possible substitutions across a regulatory element, identifying positions where mutations most strongly perturb function. These critical positions typically correspond to transcription factor binding motifs, providing motif discovery that emerges from learned representations rather than explicit sequence alignment. The approach enables characterization of any regulatory element, including those in cell types or conditions never experimentally profiled. Foundation models extend these principles to longer contexts and richer representations (Chapter 17), while clinical integration requires calibration approaches that map model outputs to actionable categories (Chapter 28).\n\n\n\n\n\n\nCheckpoint: What You Should Understand So Far\n\n\n\nBefore continuing, make sure you can explain:\n\nHow convolutional filters work: Learned weight patterns that slide across sequences, producing high activation when input matches the learned motif\nWhy multi-task learning helps: Shared representations across 919 chromatin features prevent overfitting and discover general regulatory patterns\nIn silico mutagenesis: Predict effects by comparing reference vs. alternative sequence predictions—no variant training data required\nThe receptive field limitation: CNNs can only integrate information within their architectural context window (typically 1-10 kb)\n\nIf any of these are unclear, re-read the relevant sections before proceeding. The remaining sections build on these foundations.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-basset",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-basset",
    "title": "6  Convolutional Networks",
    "section": "6.3 Cell-Type Specificity and Regulatory Grammar",
    "text": "6.3 Cell-Type Specificity and Regulatory Grammar\nA variant that disrupts cardiac-specific gene regulation may be lethal in the heart but entirely silent in neurons. A regulatory element active during embryonic development may be permanently silenced in adult tissues. Clinical variant interpretation therefore requires models that capture not just what sequence patterns predict regulatory activity, but how those predictions vary across the dozens of cell types and developmental stages where a variant might act. DeepSEA’s 919 chromatin features spanned multiple cell types, but the question remained: could architectural modifications better capture cell-type-specific programs or learn richer representations of the combinatorial grammar governing transcription factor cooperativity?\nBasset, introduced by Kelley et al. in 2016, focused specifically on predicting chromatin accessibility from sequence (Kelley, Snoek, and Rinn 2016). Rather than DeepSEA’s diverse chromatin features, Basset predicted DNase-seq peaks across 164 cell types, enabling detailed analysis of cell-type-specific regulatory activity. The architectural refinements Basset introduced would influence subsequent models: batch normalization after convolutional layers stabilized training and enabled deeper networks, while larger filters in early layers (19 nucleotides in the first layer) captured longer motifs directly rather than requiring the network to compose them from smaller patterns.\nThe key contribution was demonstrating that in silico saturation mutagenesis profiles from trained models could identify causal variants underlying disease-associated haplotypes. GWAS identifies associated regions but cannot distinguish the causal variant from nearby variants in linkage disequilibrium (see Chapter 3 for the statistical foundations of association studies). Basset’s saturation mutagenesis provided a principled approach: the variant with the strongest predicted regulatory effect within an associated haplotype is the most likely causal candidate. Foundation models like Enformer (Chapter 16) extend this principle to longer contexts that capture more distal regulatory influences on GWAS signals. This moved beyond simple peak overlap toward mechanistic variant prioritization, and the original Basset study confirmed that model-prioritized variants (high-PICS SNPs) showed higher rates of predicted accessibility change than nearby SNPs selected by GWAS association strength alone (Kelley, Snoek, and Rinn 2016).\nDanQ explored whether regulatory grammar involves sequential dependencies that convolutions alone might miss, combining convolutional layers with bidirectional LSTMs to integrate motif detections across the input window (Quang and Xie 2016). The hybrid architecture achieved modest improvements on chromatin prediction benchmarks, though the recurrent components introduced costs examined in Section 6.7.\n\n\n\nTable 6.2: Progression of CNN-based regulatory genomics models. Note the trend toward longer input contexts and more specialized prediction targets.\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nYear\nPrediction Target\nInput Length\nKey Innovation\n\n\n\n\nDeepSEA\n2015\n919 chromatin features\n1,000 bp\nMulti-task learning, in silico mutagenesis\n\n\nBasset\n2016\n164 cell-type DNase-seq\n600 bp\nBatch normalization, larger first-layer filters\n\n\nDanQ\n2016\n919 chromatin features\n1,000 bp\nCNN + bidirectional LSTM hybrid\n\n\nExPecto\n2018\n218 tissue expression\n40 kb (aggregated)\nTissue-specific expression from chromatin\n\n\nSpliceAI\n2019\nSplice sites\n10,000 bp\nDilated convolutions, 32-layer depth\n\n\n\n\n\n\nThese variations illustrated a broader principle: multiple architectures could learn useful regulatory representations from sequence. The specific choices (filter sizes, layer depths, recurrent components) mattered less than the fundamental framework of learning from one-hot encoded sequence to predict chromatin labels. This robustness suggested that the underlying signal, sequence determinants of regulatory activity, was strong enough to be captured by diverse architectural approaches. For clinical applications, prediction quality depends more on training data quality and task definition than on architectural details within the CNN family.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-expecto",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-expecto",
    "title": "6  Convolutional Networks",
    "section": "6.4 ExPecto: From Chromatin to Expression",
    "text": "6.4 ExPecto: From Chromatin to Expression\nA patient’s tumor harbors a somatic variant in a putative enhancer region. Chromatin profiling in matching tissue shows the region is accessible. The variant is predicted to disrupt a transcription factor binding site. Yet the clinician’s question remains unanswered: does this variant actually change expression of a target gene? Which gene? By how much? In which tissues?\n\n\n\n\n\n\nStop and Think\n\n\n\nChromatin accessibility tells us where the genome is “open” for regulation, but accessibility alone does not tell us which genes are affected or by how much. What additional information would a model need to predict gene expression from sequence? How would you connect a variant in an enhancer to its target gene?\n\n\nChromatin accessibility and transcription factor binding are intermediate phenotypes, means rather than ends. The ultimate functional readout for most regulatory variants is their effect on gene expression. A variant might disrupt a binding site, but sites can be redundant, effects can be buffered, and the relationship between binding and expression is not one-to-one. Predicting expression change from sequence requires integrating regulatory signals across distances that determine which enhancers control which promoters. A variant that disrupts binding but does not alter expression is unlikely to be pathogenic, while a variant with modest chromatin effects but strong expression consequences may drive disease.\nExPecto, introduced by Zhou et al. in 2018, addressed these questions by extending sequence-to-chromatin prediction toward tissue-specific gene expression (Zhou et al. 2018). The framework predicts expression levels across 218 tissues and cell types by integrating predicted chromatin signals across a 40 kb promoter-proximal window. This context expansion, from DeepSEA’s 1 kb to ExPecto’s 40 kb, represented a significant architectural commitment: expression prediction requires integrating regulatory signals from distances far exceeding typical motif sizes.\n\n\n\n\n\n\nExPecto pipeline: from chromatin to expression\n\n\n\n\nFigure 6.3: ExPecto pipeline: from chromatin to expression. The modular architecture comprises three components. (1) Beluga CNN scans a 40kb window around each transcription start site with 2kb sliding windows, predicting 2,002 chromatin features at 200 spatial positions and generating over 400,000 features per gene. (2) Spatial transformation applies exponential decay functions separately for upstream and downstream regions, encoding the prior that nearby elements contribute more to expression than distant ones, reducing dimensionality to approximately 20,000 features. (3) Tissue-specific linear regression models (218 total, one per tissue) predict log expression from transformed features. This modular design separates shared sequence-to-chromatin processing from tissue-specific expression modeling, enabling interpretable analysis of which chromatin features drive expression in each context.\n\n\n\n\n6.4.1 Modular Architecture\nExPecto comprises three sequential components, each addressing a distinct computational challenge. The separation proved essential: jointly optimizing all components end-to-end would be computationally prohibitive, and the modular design enables interpretability at each stage.\nThe first component, an enhanced CNN called Beluga, predicts 2,002 chromatin profiles from 2,000 bp input sequences. Beluga incorporated architectural improvements over DeepSEA: six convolutional layers with residual connections, expanded chromatin targets, and broader cell-type coverage. This CNN scans the 40 kb region surrounding each transcription start site with a moving window, generating chromatin predictions at 200 spatial positions and producing over 400,000 features per gene.\nThe second component transforms these high-dimensional features through spatial aggregation. Ten exponential decay functions, applied separately to upstream and downstream regions, encode the prior belief that nearby elements contribute more than distant ones. This transformation reduces dimensionality while preserving spatial relationships, producing approximately 20,000 features per gene that capture both which chromatin features are predicted and where they occur relative to the TSS.\nThe final component comprises 218 L₂-regularized linear regression models, one per tissue, predicting log expression from spatially-transformed features. Linear models were chosen deliberately: they provide interpretability, prevent overfitting given the high-dimensional feature space, and enable coefficient analysis to identify which chromatin features drive expression in each tissue. The combination of a shared sequence-to-chromatin CNN with separate tissue-specific linear heads cleanly separates sequence-level regulatory grammar from tissue-specific regulatory programs.\n\n\n\n\n\n\nKey Insight\n\n\n\nExPecto’s modular design reflects a key principle: different aspects of gene regulation require different modeling approaches. Convolutional networks excel at learning local sequence patterns (motifs), while linear models with spatial priors capture how these patterns integrate across distance. This separation enables interpretation at each stage and prevents end-to-end black boxes.\n\n\n\n\n6.4.2 Expression Prediction and Variant Effects\nExPecto achieved 0.819 median Spearman correlation between predicted and observed expression across tissues. Analysis of model coefficients revealed automatic learning of cell-type-relevant features: the liver expression model weighted HepG2-derived transcription factor features most heavily; breast tissue models emphasized estrogen receptor features from breast cancer cell lines. These tissue-specific patterns emerged purely from learning to predict expression, without tissue identity information provided to the chromatin model.\nVariant effect prediction follows the same logic as DeepSEA: compare expression predictions for reference and alternative sequences. Because the model never trains on variant data, predictions are unconfounded by linkage disequilibrium, a critical distinction from association-based methods (see Chapter 12 for detailed treatment of confounding in genomic models). ExPecto correctly predicted expression change direction for 92% of the strongest GTEx eQTL variants, and experimental validation confirmed that model-prioritized variants (not the GWAS lead SNPs) showed allele-specific regulatory activity in luciferase reporter assays (Zhou et al. 2018).\nThe 40 kb window represents an empirically optimized trade-off. Smaller windows decreased performance; larger windows showed negligible improvement. Most promoter-proximal regulatory information lies within 40 kb of the TSS, at least within ExPecto’s linear modeling framework. Distal enhancers beyond this window, while biologically important, require architectural approaches that can model longer-range dependencies. This limitation points toward the transformer architectures and hybrid models examined in Chapter 16.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-spliceai",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-spliceai",
    "title": "6  Convolutional Networks",
    "section": "6.5 SpliceAI: Clinical-Grade Splicing Prediction",
    "text": "6.5 SpliceAI: Clinical-Grade Splicing Prediction\nA child presents with developmental delay and dysmorphic features consistent with a known genetic syndrome. Clinical exome sequencing reveals no pathogenic coding variants in the implicated gene. The case is signed out as “unsolved,” the family left without answers. Three years later, research RNA sequencing identifies aberrant splicing in the syndromic gene: an intronic variant 150 base pairs from the nearest exon creates a cryptic splice site, inserting a premature stop codon. The diagnosis was hiding in plain sight, invisible to methods that only examine canonical splice dinucleotides.\nThis scenario, replicated across thousands of unsolved rare disease cases, illustrates a systematic blind spot in clinical genomics. Splice-disrupting mutations represent a major mechanism of Mendelian disease (see Chapter 28 for broader treatment of rare disease diagnosis), yet variants affecting splicing outside canonical GT/AG dinucleotides are systematically underascertained. Prior splice prediction methods captured essential splice site motifs but could not model the long-range determinants contributing to splicing specificity. MaxEntScan operates on approximately 9 bp of context around donor and acceptor sites (Yeo and Burge 2004). The method established the paradigm of quantitative splice site scoring using maximum entropy distributions and remains a standard baseline for variant interpretation, but its narrow window fundamentally limits what biology it can capture. These methods produced many false positives and missed variants acting through distal mechanisms: branch points, exonic splicing enhancers, and intron length constraints that previous models could not see.\nSpliceAI, introduced by Jaganathan et al. in 2019, demonstrated that deep neural networks could learn sequence-intrinsic splicing rules sufficient to predict the majority of splice sites used by the spliceosome (Jaganathan et al. 2019). The model predicts splice site locations directly from pre-mRNA sequence using 10,000 nucleotides of context, an order of magnitude beyond prior methods. This context expansion enabled recognition of distant splicing determinants invisible to annotation-based approaches.\n\n\n\n\n\n\nStop and Think\n\n\n\nSpliceAI uses 10,000 nucleotides of context, compared to MaxEntScan’s 9 nucleotides. What biological features might be captured by this expanded context? Think about the components involved in splicing beyond just the GT/AG dinucleotides.\n\n\n\n6.5.1 Architecture: Depth and Dilation\nLearning splicing rules from 10 kb of sequence context requires an architecture that can integrate information across this entire span while maintaining nucleotide-level resolution. SpliceAI achieves this through two innovations: extreme depth enabled by residual connections, and dilated convolutions that expand receptive fields without proportional parameter growth.\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThe following section describes residual connections and dilated convolutions. The key intuition: residual connections let gradients flow through very deep networks by providing “shortcuts,” and dilated convolutions let filters “skip” positions to see further without adding parameters.\n\n\nSpliceAI employs an ultra-deep residual network with 32 convolutional layers. Residual connections address the vanishing gradient problem that otherwise prevents training at this depth:\n\\[\n\\text{output} = \\text{input} + F(\\text{input})\n\\]\nWhy does this simple reformulation enable deeper networks? During backpropagation, gradients must flow through every layer to update early parameters. In standard networks, each layer multiplies the gradient by its weight matrix; when these multiplications compound across 32 layers, gradients either explode (if weights are large) or vanish toward zero (if weights are small). Residual connections provide an alternative path: the gradient can flow directly through the identity shortcut, bypassing the learned transformation \\(F(\\text{input})\\) entirely. This means early layers receive gradient signal regardless of what intermediate layers have learned. The network can then focus on learning what to add to the identity mapping rather than learning the entire transformation from scratch, a substantially easier optimization problem. Skip connections from every fourth residual block feed directly to the penultimate layer, further stabilizing training dynamics by providing multiple gradient highways that bypass potential bottlenecks.\nDilated convolutions expand the receptive field efficiently. A dilated convolution with rate d samples input positions at intervals of d rather than consecutively. Why use dilation rather than simply larger filters or more layers? Standard convolutions face a tradeoff: expanding the receptive field requires either wide filters (adding parameters proportionally) or deep stacking (adding parameters and gradient path length). Dilation circumvents this tradeoff by reusing the same filter weights while sampling input at wider intervals. A 3-wide filter with dilation rate 8 effectively spans 17 positions (3 samples with 8-position gaps) while using only 3 weight values. Stacking convolutions with increasing dilation rates (1, 2, 4, 8, 16, and so on) allows the network to integrate information across the full 10 kb window while maintaining sensitivity to local patterns at early layers where dilation is small. Standard convolutions with small kernels would require impractical depth to achieve equivalent receptive fields.\nFor each position in the pre-mRNA sequence, SpliceAI outputs three probabilities: splice acceptor, splice donor, or neither. This per-position classification enables fine-grained predictions across entire transcripts. Training used GENCODE annotations, with odd and even chromosomes split for training and testing.\n\n\n\n\n\n\n\n\nDilated convolutions\n\n\n\n\n\n\n\nResidual block structure\n\n\n\n\n\n\nFigure 6.4: SpliceAI architecture innovations for long-range splicing prediction. (A) Dilated convolutions expand the receptive field efficiently by sampling input positions at intervals. Stacking layers with dilation rates 1, 2, 4, 8, 16… enables integration of 10,000 nucleotides of context without proportional parameter growth. Lower layers with small dilation capture local splice site grammar while upper layers with large dilation integrate distal determinants like branch points and exonic splicing enhancers. (B) Residual block structure with skip connections from every 4th block to the output. This enables training of 32 layers by providing multiple gradient pathways, preventing vanishing gradients that would otherwise block learning in such a deep network.\n\n\n\n\n\n6.5.2 Performance and Validation\nSpliceAI achieved 95% top-\\(k\\) accuracy for splice site identification (compared to 57% for MaxEntScan) and 0.98 precision-recall area under the curve (auPRC). Complex genes exceeding 100 kb are often reconstructed to nucleotide precision. Performance improved dramatically with context length:\n\n\n\nTable 6.3: SpliceAI performance improves substantially with context length, confirming that distal sequence features contribute meaningfully to splicing decisions. Diminishing returns above 2 kb suggest most determinants lie within this range.\n\n\n\n\n\nModel Variant\nContext (each side)\nauPRC\n\n\n\n\nSpliceAI-80nt\n40 bp\n0.87\n\n\nSpliceAI-400nt\n200 bp\n0.93\n\n\nSpliceAI-2k\n1,000 bp\n0.96\n\n\nSpliceAI-10k\n5,000 bp\n0.98\n\n\n\n\n\n\nThis progression confirms that distal sequence features contribute meaningfully to splicing decisions. The diminishing returns above 2 kb suggest that most splicing determinants lie within this range, though the additional context still provides measurable benefit.\nThe delta score quantifies variant effects by comparing predictions for reference and alternative sequences:\n\\[\n\\Delta\\text{score} = \\max_{|p - v| \\leq 50} \\left| P_{\\text{alt}}(p) - P_{\\text{ref}}(p) \\right|\n\\]\nValidation against GTEx RNA-seq showed that mutations with higher delta scores showed higher validation rates at novel splice junctions: approximately \\(50\\%\\) at \\(\\Delta \\geq 0.2\\), \\(75\\%\\) at \\(\\Delta \\geq 0.5\\), and \\(85\\%\\) at \\(\\Delta \\geq 0.8\\). Population genetics provided orthogonal support: predicted cryptic splice variants showed \\(78\\%\\) depletion at common allele frequencies, nearly matching the depletion of frameshift and stop-gain variants. Natural selection treats these variants as deleterious, confirming their functional impact.\n\n\n6.5.3 Clinical Impact\nSpliceAI’s most significant contribution may be quantifying cryptic splice mutations as a major, previously underappreciated cause of rare genetic disorders. Analysis of de novo mutations in over \\(4{,}000\\) individuals with intellectual disability found significant enrichment of predicted splice-disrupting variants compared to unaffected controls (\\(1.51\\)-fold, \\(p = 4.2 \\times 10^{-4}\\)). Approximately \\(9\\%\\) of pathogenic de novo mutations in intellectual disability act through cryptic splicing (Jaganathan et al. 2019). Including these variants in gene discovery analyses identified additional candidate genes that would have fallen below discovery thresholds when considering only protein-coding mutations.\n\n\n\n\n\n\nPractical Guidance: Using SpliceAI Scores\n\n\n\nWhen interpreting SpliceAI delta scores in clinical contexts:\n\nDelta &lt; 0.2: Low confidence; variant unlikely to affect splicing\nDelta 0.2-0.5: Moderate confidence; consider RNA-seq validation if clinically relevant\nDelta 0.5-0.8: High confidence; strong candidate for splice effect\nDelta &gt; 0.8: Very high confidence; treat similarly to canonical splice site variants\n\nRemember that SpliceAI predicts whether splicing changes, not whether the change is pathogenic. A variant might create a new in-frame splice site that has minimal functional impact.\n\n\nThis clinical utility explains SpliceAI’s rapid adoption. Illumina integrated SpliceAI into their annotation pipelines. Clinical genetics laboratories worldwide use delta scores to flag potential splice-affecting variants for RNA-seq follow-up. The model exemplifies how task-specific deep learning can achieve clinical-grade accuracy on well-defined problems. SpliceAI’s integration into modern variant interpretation workflows is examined in Chapter 17, and its role in rare disease diagnosis pipelines appears in Chapter 28.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-receptive-field",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-receptive-field",
    "title": "6  Convolutional Networks",
    "section": "6.6 Receptive Field Ceiling",
    "text": "6.6 Receptive Field Ceiling\nConsider a 45-year-old woman with early-onset breast cancer and a family history suggesting hereditary risk. Whole-genome sequencing identifies a novel variant 80 kilobases upstream of BRCA1, within an established enhancer region. The enhancer is known to regulate BRCA1 expression in mammary epithelium. Does this variant reduce BRCA1 expression enough to increase cancer risk? DeepSEA can predict whether the variant disrupts transcription factor binding at that position. SpliceAI confirms no splice effects. ExPecto’s 40 kb window cannot reach from the variant to the BRCA1 promoter. No convolutional model can connect the enhancer variant to its target gene because the distance exceeds their receptive fields. The clinical question remains unanswered.\n\n\n\n\n\n\nKey Insight\n\n\n\nThe receptive field limitation is architectural, not statistical. No amount of training data can teach a CNN to model dependencies that exceed its receptive field. This fundamental constraint, not model performance on short-range tasks, motivated the architectural shift to attention mechanisms.\n\n\nThis case illustrates a fundamental limitation rooted in architecture: convolutional networks can only integrate information within their receptive fields. DeepSEA’s three-layer architecture effectively considers roughly 1 kb of context. ExPecto’s Beluga component operates on 2 kb windows, aggregated across a 40 kb region by the spatial transformation layer. SpliceAI pushes to 10 kb through dilated convolutions and 32 layers. Each expansion required significant architectural engineering, and each reached a practical ceiling beyond which further expansion yielded diminishing returns or became computationally prohibitive.\nThe limitation matters because genomic regulation routinely operates across distances these models cannot reach. Enhancers regulate promoters 50 to 500 kilobases away. The beta-globin locus control region sits 40 to 60 kb from the genes it activates. Polycomb-mediated repression involves chromatin contacts spanning megabases. Topologically associating domains organize regulatory interactions across hundreds of kilobases (see Chapter 20 for detailed treatment of chromatin architecture). When regulatory elements and their targets lie beyond a model’s receptive field, the model cannot learn their relationship regardless of how much training data is available. The constraint is architectural, not statistical. Attention mechanisms (Chapter 7) provide the architectural solution, while hybrid models like Enformer (Chapter 16) combine convolutional motif detection with transformer-based long-range integration.\nThis creates a systematic mismatch between biological importance and computational accessibility. A variant within a distal enhancer may have profound effects on gene expression, but a model with a 10 kb receptive field cannot connect the enhancer sequence to its target promoter. The model might correctly predict that the enhancer sequence contains regulatory features, but it cannot predict which gene those features regulate or how strongly. We can predict local regulatory potential, yet we cannot predict long-range regulatory effects.\nThe architectural response to this challenge evolved through two stages. Recurrent networks initially seemed promising, carrying context through hidden states rather than expanding receptive fields. When recurrence proved insufficient, attention mechanisms provided the architectural solution that modern genomic models required.\n\n\n\n\n\n\nThe receptive field ceiling of convolutional architectures\n\n\n\n\nFigure 6.5: The receptive field ceiling of convolutional architectures. Context windows of representative CNN-based models compared to biologically relevant regulatory distances. DeepSEA integrates approximately 1kb of context; SpliceAI extends to 10kb through dilated convolutions; ExPecto aggregates predictions across 40kb. These contexts suffice for local features like transcription factor binding sites (~10bp) and promoter elements (~1kb) but cannot capture enhancer-promoter interactions typically spanning 10-100kb. Even SpliceAI’s 10kb context falls short of the distances at which most GWAS signals reside from their target genes. This architectural limitation, not data scarcity, motivated the shift to attention mechanisms that compute direct interactions across arbitrary distances.\n\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nBefore proceeding, ensure you can explain: (1) Why can’t a CNN learn dependencies that exceed its receptive field, even with more training data? (2) What biological phenomena require context longer than typical CNN receptive fields? (3) What architectural approaches extend receptive fields in CNNs?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nA CNN cannot learn dependencies beyond its receptive field because positions outside this window have zero gradient with respect to the output—information simply cannot flow between them, regardless of training data quantity. (2) Distal enhancer-promoter interactions (tens of kilobases), long-range chromatin loops, TAD boundary effects, and coordinated regulation across large genomic domains all require longer context than typical CNN receptive fields of 200-1000 bp. (3) Receptive fields can be extended through dilated convolutions, deeper networks with more layers, or hybrid architectures that combine CNNs with attention mechanisms.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-sequential",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-sequential",
    "title": "6  Convolutional Networks",
    "section": "6.7 Sequential Approaches and Their Costs",
    "text": "6.7 Sequential Approaches and Their Costs\nIf convolutional networks cannot reach far enough, why not simply carry information forward through the sequence? Recurrent neural networks offered an intuitive solution to the receptive field problem: maintain a hidden state that accumulates context as the network processes each position in turn. Where a convolutional filter sees only its local window, a recurrent neural network (RNN)’s hidden state can, in principle, carry information from the beginning of a sequence to its end. For biological sequences, this seemed natural. DNA is read by polymerases in one direction; transcripts are processed sequentially by ribosomes; regulatory elements exert effects that propagate through chromatin. A computational architecture that mirrors this sequential logic appeared well-suited to genomic modeling.\nThe hidden state mechanism works as follows. At each position t, the network combines the current input \\(x_t\\) with the previous hidden state \\(h_{t-1}\\) to produce a new hidden state \\(h_t\\). This recurrence allows information from early positions to influence computations at later positions through the chain of hidden states. A regulatory element at position 1,000 can, in theory, affect predictions at position 50,000 because its influence persists in the hidden state across all intervening positions. No receptive field limits this reach; the constraint becomes whether information survives the journey.\n\n6.7.1 Vanishing Gradient Problem\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThis section discusses gradient propagation through recurrent networks. The key intuition: when you multiply small numbers many times, the result quickly approaches zero. This is why RNNs struggle to learn long-range dependencies.\n\n\nInformation rarely survives. Training RNNs requires backpropagating gradients through time, computing how errors at late positions depend on parameters applied at early positions. These gradients pass through the same recurrent weight matrix at each step. When gradients are multiplied through hundreds or thousands of steps, they either explode (growing exponentially) or vanish (shrinking toward zero). The vanishing gradient problem makes it nearly impossible for RNNs to learn dependencies spanning more than a few dozen positions. A regulatory element 10,000 base pairs upstream might as well not exist: by the time gradients propagate backward through 10,000 recurrent steps, they have decayed to numerical insignificance.\nGating mechanisms that control information flow resolved the vanishing gradient problem. Long Short-Term Memory (LSTM) networks achieve this through a separate cell state with learned gates that determine what information to store, what to forget, and what to output (Hochreiter and Schmidhuber 1997). The forget gate can preserve information indefinitely by setting its value near one, allowing gradients to flow through the cell state without repeated multiplication by small values. Gated Recurrent Units (GRUs) simplified this design by combining gates while retaining the core insight: learned gating prevents gradient decay (Cho et al. 2014).\nThese gated architectures extended effective memory from tens to hundreds of positions, sometimes thousands. For natural language, where most dependencies span fewer than 50 words, LSTMs proved transformative. For genomic sequences, where relevant context can span tens of kilobases (tens of thousands of nucleotides), even gated recurrence falls short. The mathematics of gradient propagation through recurrent connections imposes limits that no gating mechanism fully overcomes.\n\n\n\nTable 6.4: Comparison of sequence modeling architectures for learning long-range dependencies. Theoretical range assumes perfect gradient flow; practical range reflects empirical observations.\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nTheoretical Range\nPractical Range\nKey Limitation\n\n\n\n\nVanilla RNN\nUnlimited\n~10-50 positions\nVanishing gradients\n\n\nLSTM/GRU\nUnlimited\n~100-1000 positions\nGradient decay through gates\n\n\nBidirectional LSTM\nUnlimited\n~100-1000 positions\nSequential computation bottleneck\n\n\nAttention\nUnlimited\nLimited by memory\nQuadratic complexity\n\n\n\n\n\n\n\n\n6.7.2 DanQ: Combining Convolutions and Recurrence\nThe DanQ model represented the most influential attempt to apply recurrent architectures to regulatory genomics (Quang and Xie 2016). Rather than replacing convolutions entirely, DanQ combined them: convolutional layers first extracted local sequence motifs, then a bidirectional LSTM integrated these motif detections across the 1,000 base pair input window. The architecture recognized that convolutions excel at detecting local patterns while recurrence might capture their long-range relationships.\nDanQ processed sequences in both directions simultaneously (bidirectional recurrence), allowing each position to incorporate context from both upstream and downstream. Training on the same DeepSEA chromatin prediction task, DanQ achieved modest improvements over the purely convolutional baseline, with the LSTM component learning to weight motif combinations based on their relative positions and co-occurrence patterns.\nThe improvement was real but limited. Within a 1,000 base pair window, convolutional receptive fields already capture most relevant dependencies, leaving less room for recurrence to contribute. The fundamental problem remained: neither convolutions nor recurrence could reach the 50 to 100 kilobase distances where enhancers regulate their target genes. DanQ demonstrated that hybrid architectures could outperform pure convolutions, but the gains did not justify the added complexity for most applications. The model saw limited adoption compared to simpler convolutional alternatives.\n\n\n6.7.3 Sequential Bottleneck\nEven if recurrence could maintain gradients across genomic distances, a more fundamental constraint would remain. RNNs process sequences one position at a time. Each hidden state \\(h_t\\) depends on the previous hidden state \\(h_{t-1}\\), creating an inherently sequential computation that cannot be parallelized. Training on a 100,000 base pair sequence requires 100,000 sequential steps, each waiting for the previous step to complete. Modern GPUs achieve their speed through massive parallelism; sequential dependencies eliminate this advantage.\nThis computational bottleneck made RNNs impractical for the long contexts that genomic applications require. A transformer processes all positions simultaneously, computing attention scores in parallel across the entire sequence. For a 100 kilobase context, a transformer performs one parallel operation where an RNN would require 100,000 sequential steps. The difference in training time is not incremental; it is the difference between feasible and infeasible. When Enformer extended genomic modeling to 200 kilobase contexts (see Chapter 16), recurrent architectures were not considered. The sequential bottleneck had already disqualified them.\nThe attention mechanism resolved both limitations simultaneously. Self-attention computes direct interactions between all positions without sequential dependencies, enabling parallel processing across arbitrary context lengths. Attention weights are computed through matrix operations that GPUs execute efficiently, and gradients flow directly between any two positions without passing through intermediate states. The path from position 1 to position 100,000 involves a single attention computation rather than 100,000 recurrent steps. This architectural shift, examined in Chapter 7, enabled the long-range modeling that genomic applications demand.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch06-cnn.html#sec-ch06-specialization",
    "href": "part_2/p2-ch06-cnn.html#sec-ch06-specialization",
    "title": "6  Convolutional Networks",
    "section": "6.8 Specialization and Its Limits",
    "text": "6.8 Specialization and Its Limits\nThe convolutional models examined here established paradigms that persist in modern genomic AI. End-to-end learning from one-hot encoded sequence demonstrated that gradient descent on functional labels could discover regulatory patterns without encoding human assumptions about what matters. Multi-task training across hundreds of chromatin features showed that shared representations improve both accuracy and generalization. In silico mutagenesis, comparing predictions for reference and alternative sequences, established the dominant approach for deep learning-based variant effect prediction: scoring variants without training on variant labels, thereby avoiding the ascertainment biases that confound association-based methods (see Chapter 12).\n\n\n\n\n\n\nStop and Think\n\n\n\nSpliceAI achieves clinical-grade accuracy on splice prediction that general-purpose foundation models have not matched. Under what circumstances might a specialized model outperform a general-purpose foundation model? When might the reverse be true?\n\n\nThese principles carry forward into foundation model architectures. What CNNs could not resolve was the receptive field limitation. Genomic regulation operates across scales that exceed practical convolutional depth: enhancers modulating genes across hundreds of kilobases, topologically associating domains spanning megabases. Dilated convolutions and deeper networks extend reach but cannot fundamentally escape the constraint that convolutions aggregate local information through hierarchical composition.\nYet specialization retains value even as general-purpose models advance. SpliceAI achieves clinical-grade splice site prediction that broader foundation models have not matched. When the prediction target is well-defined, training data abundant, and the relevant context fits within architectural constraints, task-specific models remain competitive with or superior to general-purpose approaches. This tension between specialized accuracy and general capability defines architectural choices across genomic AI. For clinical deployment requiring high reliability on specific tasks, specialized architectures may remain preferred. For discovery applications requiring broad coverage across diverse molecular mechanisms, the foundation model paradigm (see Chapter 13) offers different trade-offs. Attention mechanisms provide the architectural substrate for long-range modeling while inheriting the end-to-end learning principles that convolutional networks established.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nHow do convolutional filters differ from classical position weight matrices, and why does this difference matter for discovering regulatory patterns?\nExplain why multi-task training across 919 chromatin features helps regularization and improves generalization compared to single-task training.\nA CNN has three convolutional layers with kernel width 8 and pooling factor 2. Approximately what receptive field does this architecture achieve, and why does this limit regulatory modeling?\nWhy did SpliceAI use dilated convolutions with increasing dilation rates across 32 layers, and what biological patterns required this architectural choice?\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nKey concepts covered: Convolutional filters as learned position weight matrices, receptive fields and their architectural limits, multi-task learning for regularization, in silico mutagenesis for variant effect prediction, dilated convolutions for expanded context, residual connections for deep networks, the sequential bottleneck of recurrent architectures.\nModels examined:\n\n\n\n\n\n\n\n\nModel\nContribution\nLimitation Revealed\n\n\n\n\nDeepSEA\nEnd-to-end learning discovers motifs; in silico mutagenesis for VEP\n1 kb context insufficient for distal regulation\n\n\nBasset\nCell-type specific accessibility; causal variant prioritization\nCannot model enhancer-promoter relationships\n\n\nExPecto\nExpression from chromatin; tissue-specific prediction\n40 kb aggregation, not end-to-end\n\n\nSpliceAI\nClinical-grade splicing; dilated convolutions\n10 kb still misses some long-range effects\n\n\nDanQ\nHybrid CNN-RNN\nRecurrence adds cost without solving receptive field problem\n\n\n\nMain takeaways:\n\nCNNs proved that gradient descent on DNA sequence discovers regulatory patterns matching experimental biology\nMulti-task learning across hundreds of chromatin features enables generalization and interpretability\nIn silico mutagenesis enables variant effect prediction without training on variant data\nReceptive field limitations are architectural constraints that no amount of training data overcomes\nSpecialized models (SpliceAI) can achieve clinical-grade accuracy that general models do not match\nAttention mechanisms, not recurrence, provide the path beyond receptive field constraints\n\nLooking ahead: Chapter 7 examines how self-attention computes direct interactions between all positions, enabling the long-range modeling that CNNs cannot achieve. Understanding CNNs’ strengths (motif detection, multi-task learning) and limitations (receptive fields) clarifies why modern genomic foundation models typically combine convolutional pattern detection with attention-based long-range integration.\n\n\n\n\n\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. “On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.” arXiv. https://doi.org/10.48550/arXiv.1409.1259.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data Navigation on the ENCODE Portal.” Nature Communications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKelley, David R., Jasper Snoek, and John L. Rinn. 2016. “Basset: Learning the Regulatory Code of the Accessible Genome with Deep Convolutional Neural Networks.” Genome Research 26 (7): 990–99. https://doi.org/10.1101/gr.200535.115.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. “Integrative Analysis of 111 Reference Human Epigenomes.” Nature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nQuang, Daniel, and Xiaohui Xie. 2016. “DanQ: A Hybrid Convolutional and Recurrent Deep Neural Network for Quantifying the Function of DNA Sequences.” Nucleic Acids Research 44 (11): e107. https://doi.org/10.1093/nar/gkw226.\n\n\nYeo, Gene, and Christopher B. Burge. 2004. “Maximum Entropy Modeling of Short Sequence Motifs with Applications to RNA Splicing Signals.” Journal of Computational Biology 11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Networks</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html",
    "href": "part_2/p2-ch07-attention.html",
    "title": "7  Transformers and Attention",
    "section": "",
    "text": "7.1 Self-Attention Mechanism\nA 28-year-old woman presents with dilated cardiomyopathy and a variant of uncertain significance in the LMNA gene’s promoter region. Her clinician needs to determine whether this variant disrupts regulatory elements that control LMNA expression in cardiac tissue. The relevant information spans thousands of base pairs: transcription factor binding sites flanking the variant, enhancers that drive cardiac-specific expression, and insulators that constrain regulatory interactions. A model that can only aggregate local context (through convolutional or recurrent operations) must pass information through many intermediate layers, each adding noise and limiting what survives the journey. When this information pathway fails, the variant appears as noise rather than the pathogenic regulatory disruption it may represent. The fundamental question is how to let any position in a sequence directly access information from any other position, regardless of distance.\nSelf-attention answers this question by computing all pairwise interactions simultaneously, allowing the model to directly relate any position to any other regardless of distance. Where convolutions apply fixed filters uniformly across the sequence, attention performs dynamic routing: each position queries the entire sequence and aggregates information based on content-dependent relevance scores. The routing changes for every input because attention weights depend on what the sequence contains, not just where positions sit relative to each other. For the LMNA variant, this means the model can directly assess whether the variant position interacts with known cardiac enhancers without that signal degrading through layer after layer of local aggregation.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-self-attention",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-self-attention",
    "title": "7  Transformers and Attention",
    "section": "",
    "text": "Input embeddings\n\n\n\n\n\n\n\nQuery, Key, Value projections\n\n\n\n\n\n\n\nAttention score matrix\n\n\n\n\n\n\n\n\n\nAttention weight matrix\n\n\n\n\n\n\n\nWeighted value aggregation\n\n\n\n\n\n\nFigure 7.1: Step-by-step visualization of self-attention on a regulatory sequence. (A) Input embeddings represent each nucleotide position as a learned vector. (B) Linear projections produce query (what to seek), key (what to advertise), and value (what to send) vectors through learned weight matrices. (C) Attention scores are computed as scaled dot products between all query-key pairs, capturing content-based relevance. (D) Softmax normalization converts scores to attention weights forming probability distributions over positions. (E) Each output is a weighted sum of value vectors, with weights determining how much each position contributes. This mechanism enables direct communication between any two positions regardless of sequence distance.\n\n\n\n\n\n7.1.1 Query, Key, and Value Vectors\n\n\n\n\n\n\nMathematical Content Ahead\n\n\n\nThe next section presents the mathematical formulation of attention. The key equations are the scaled dot-product (\\(\\text{score}(q_i, k_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\\)) and the weighted aggregation. If you find the notation challenging, focus on the intuition: queries ask questions, keys advertise content, and values carry the actual information that flows between positions.\n\n\nAt each position in the input sequence, self-attention computes three vectors: a query, a key, and a value. These vectors emerge from multiplying the input embedding at that position by three learned weight matrices \\(W^Q\\), \\(W^K\\), and \\(W^V\\). The query represents what information this position seeks from other positions. The key represents what information this position offers to queries from elsewhere. The value represents the actual information this position contributes when attended to. This query-key-value structure separates the question of “which positions should interact” (determined by query-key similarity) from “what information flows between them” (determined by values).\n\n\n\n\n\n\nKey Insight: The Information Retrieval Analogy\n\n\n\nThink of attention like a search engine for sequence positions. Each position issues a query describing what it’s looking for. Every position also publishes a key describing what it has to offer. The dot product between query and key measures relevance, just as a search engine ranks documents by query match. High-scoring positions then send their value (the actual content) to the querying position. The separation of key (what I advertise) from value (what I actually send) allows the same position to advertise relevance for one type of information while sending different information when attended to.\n\n\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore seeing the attention formula, consider: What similarity measure would you use to compare queries (Q) and keys (K)? Why might we want to scale this similarity?\n\n\nThe attention mechanism computes similarity scores between each query and all keys. For position \\(i\\), we compute the dot product between its query \\(q_i\\) and every key \\(k_j\\) across all positions \\(j = 1, \\ldots, L\\), where \\(L\\) is sequence length. These scores are scaled by \\(\\sqrt{d_k}\\) (the square root of the key dimension) to prevent the dot products from growing large in high dimensions:\n\\[\n\\text{score}(q_i, k_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\n\\]\nWhy is this scaling necessary? The dot product between two random vectors with unit-variance components has variance proportional to the number of dimensions. In a 512-dimensional space, the dot product between typical query and key vectors will be roughly \\(\\sqrt{512} \\approx 22\\) times larger than in a single dimension. Without scaling, these large values push softmax outputs toward extreme distributions where nearly all weight concentrates on a single position, effectively collapsing attention into hard selection rather than soft weighting. The gradients through such saturated softmax outputs vanish, making learning impossible. Dividing by \\(\\sqrt{d_k}\\) normalizes the variance regardless of dimension, keeping softmax in its useful operating regime where it can distribute attention across multiple relevant positions.\n\n\n\n\n\n\nWorked Example: Why Scaling Matters\n\n\n\nConsider query and key vectors in 512 dimensions, where each component is drawn from a standard normal distribution (mean 0, variance 1).\nWithout scaling: The dot product of two such 512-dimensional vectors has expected variance of 512. Typical scores might range from +30 to -25—values so extreme that softmax assigns nearly 100% weight to the highest-scoring position, collapsing to hard selection.\nWith scaling: Dividing by \\(\\sqrt{512} \\approx 22.6\\) normalizes scores to variance ~1. Now scores are typically between -3 and +3, and softmax can meaningfully distribute attention across multiple positions.\n\n\n\nScenario\nScore Range\nSoftmax Behavior\n\n\n\n\nUnscaled (d=512)\n±25 to ±30\n~100% on one position\n\n\nScaled (d=512)\n±2 to ±3\nDistributed weights\n\n\n\nThis is why scaling is essential: it keeps attention “soft” rather than collapsing to hard selection, allowing the model to attend to multiple relevant positions simultaneously.\n\n\nA softmax function converts these scores into attention weights \\(\\alpha_{ij}\\) that form a probability distribution over positions:\n\\[\n\\alpha_{ij} = \\frac{\\exp(\\text{score}(q_i, k_j))}{\\sum_{j'=1}^L \\exp(\\text{score}(q_i, k_{j'}))}\n\\]\n\n\n\n\n\n\nDeep Dive: The Softmax Function\n\n\n\nFor biology readers: Softmax converts raw scores into probabilities that sum to 1:\nWhat it does: Given a set of scores (which could be any real numbers), softmax produces positive values that sum to 1, interpretable as probabilities or weights.\nThe formula intuition:\n\nExponentiate each score: \\(\\exp(\\text{score})\\) makes all values positive\nDivide by the sum: ensures outputs sum to 1\n\nExample: If three positions have scores [2.0, 1.0, 0.5], softmax produces:\n\nPosition 1: \\(\\exp(2.0)/(\\exp(2.0)+\\exp(1.0)+\\exp(0.5)) \\approx 0.59\\)\nPosition 2: \\(\\exp(1.0)/... \\approx 0.22\\)\nPosition 3: \\(\\exp(0.5)/... \\approx 0.13\\)\n\nKey properties:\n\nHigher scores → higher weights (exponential amplifies differences)\nAll weights are positive and sum to 1\nVery high scores dominate; very low scores become negligible\n\nIn attention: Softmax turns similarity scores into “how much to pay attention to each position.” High-scoring positions receive most of the attention weight; low-scoring positions are effectively ignored.\n\n\nThese weights determine how strongly position \\(i\\) attends to each other position. High weight means position \\(i\\) aggregates substantial information from position \\(j\\); low weight means position \\(j\\) contributes little to the output at position \\(i\\). The final output at position \\(i\\) is a weighted sum of all value vectors:\n\\[\n\\text{output}_i = \\sum_{j=1}^L \\alpha_{ij} v_j\n\\]\nThis weighted aggregation forms the core of self-attention. Each output position receives a mixture of information from across the entire sequence, with mixture proportions learned through backpropagation. For genomic sequences, this means a position near a splice site can attend to both the upstream exon and downstream intron, integrating context that determines whether splicing occurs. A position in a promoter can attend to distant enhancers, learning which distal elements influence expression at this gene. When predicting the pathogenicity of a variant in the SCN5A promoter (mutations in which cause Brugada syndrome and long QT syndrome, affecting approximately 1 in 2,000 individuals), the model can simultaneously consider the core promoter elements, upstream enhancers that drive cardiac-specific expression, and downstream regulatory regions that modulate expression levels.\n\n\n\n\n\n\nStop and Think: Query-Key-Value Separation\n\n\n\nConsider a transcription factor binding site within a promoter region.\n\nWhat kind of “query” might this position issue? (What information would help determine its regulatory function?)\nWhat “key” might it advertise? (What aspect of its identity is relevant to other positions?)\nWhat “value” might it send when attended to? (What information should flow to positions that find it relevant?)\n\nThe separation of these three roles is what gives attention its flexibility. A TATA box might query for nearby transcription start sites while advertising its identity as a core promoter element.\n\n\n\n\n7.1.2 Multi-Head Attention\n\n\n\n\n\n\nMulti-head attention captures diverse biological relationships\n\n\n\n\nFigure 7.2: Multi-head attention captures diverse biological relationships. Different heads in a trained genomic transformer learn specialized attention patterns. Head 1 attends locally to nearby positions, capturing motif context. Head 2 shows periodic attention at approximately 200 base pair intervals, potentially reflecting nucleosome spacing. Head 3 attends specifically to positions matching sequence motifs like CTCF binding sites. Head 4 exhibits long-range attention connecting distant positions, consistent with enhancer-promoter interactions. This specialization emerges from training without explicit supervision, demonstrating that the chromatin prediction objective induces biologically meaningful attention patterns.\n\n\n\nA patient presenting with a complex arrhythmia may carry variants affecting both a cardiac ion channel’s coding sequence and its distal enhancer. Understanding this case requires the model to simultaneously track local splice site context around the coding variant and enhancer-promoter relationships spanning 50 kilobases. A single attention operation cannot capture both patterns effectively: when forced to learn one pattern of position interactions, the model faces an impossible choice between attending strongly to nearby positions for local regulatory context or attending to distant positions for enhancer-gene relationships. Genomic sequences exhibit multiple types of dependencies simultaneously, and forcing all these interaction types through a single attention pattern creates destructive competition.\nMulti-head attention extends the basic mechanism by running multiple attention operations in parallel, each with independent learned projections (Vaswani et al. 2023). Think of it like a team of specialists reviewing the same medical case: a cardiologist focuses on heart-related findings, a geneticist examines hereditary patterns, and a pathologist scrutinizes cellular abnormalities. Each specialist attends to the same patient record but extracts different relevant information. Similarly, each attention head learns to focus on different types of relationships in the sequence. If we use H heads, we split the model dimension d into H subspaces of dimension d/H, compute separate queries, keys, and values for each head, run attention independently, concatenate outputs, and project back to dimension d. Different heads can specialize in different interaction types without competing for attention capacity.\nIn genomic models, one head might attend to nearby positions (capturing local motif context) while another attends to positions at characteristic distances (capturing nucleosome spacing or enhancer-promoter loops). Empirical analysis of trained genomic transformers reveals diverse attention patterns: some heads attend locally regardless of content, others attend to specific sequence motifs like TATA boxes or CTCF binding sites, and still others show distance-dependent patterns suggestive of chromatin organization (Avsec et al. 2021). This specialization emerges from training without explicit supervision, reflecting the model’s discovery that different types of interactions require different aggregation patterns. Methods for visualizing and interpreting these learned attention patterns are examined in Chapter 24.\n\n\n\n\n\n\n\n\nEnhancer-promoter attention\n\n\n\n\n\n\n\nGenomic context overlay\n\n\n\n\n\n\n\nLocal attention head\n\n\n\n\n\n\nFigure 7.3: Attention patterns in genomic transformers reveal learned regulatory relationships. (A) An attention head showing strong weights between a promoter and distal enhancer approximately 50kb apart, demonstrating learned long-range regulatory attention. (B) The same pattern overlaid on genomic context, showing how attention arcs connect the promoter to its regulatory enhancer. (C) A different head from the same model shows local attention patterns, capturing nearby sequence context. This head specialization emerges from training: some heads learn to integrate long-range regulatory signals while others focus on local motif relationships.\n\n\n\n\n\n\n\n\n\nKey Insight: Head Specialization\n\n\n\nMulti-head attention is not just about capacity; it is about division of labor. Different heads learn to track different biological relationships:\n\nLocal heads capture motif context and nearby regulatory grammar\nPeriodic heads may learn nucleosome spacing (~147 bp) or helical periodicity (~10.5 bp)\nLong-range heads capture enhancer-promoter interactions across tens of kilobases\nMotif-specific heads attend selectively to known binding sites (CTCF, TATA)\n\nThis specialization emerges without supervision. The model discovers that genomic function depends on multiple relationship types operating simultaneously.\n\n\nThe multi-head structure also provides redundancy that aids training. If one head fails to learn useful patterns, others can compensate. Gradient flow through multiple parallel paths stabilizes optimization. For genomic applications where training data may be limited compared to natural language corpora, this redundancy helps prevent individual heads from overfitting to spurious correlations. The number of heads represents a design choice: too few heads limit the diversity of learnable patterns, while too many heads reduce the dimensionality available to each head, potentially limiting their individual expressiveness. Most genomic transformers use 8 to 16 heads, balancing diversity against per-head capacity.\n\n\n\n\n\n\nCheckpoint: Core Attention Concepts\n\n\n\nBefore continuing to positional encodings, verify you understand:\n\nQuery-Key-Value mechanism: Queries ask “what do I need?”, keys advertise “what do I have?”, values carry “what I’ll send”\nScaled dot-product: Scores = \\((Q \\cdot K) / \\sqrt{d_k}\\) prevents softmax saturation\nSoftmax normalization: Converts scores to attention weights (probabilities summing to 1)\nMulti-head attention: Parallel attention operations that specialize for different relationship types\n\nQuick self-test: If a CTCF binding site needs to “know” about a promoter 10kb away, which attention component enables this direct communication? (Answer: The weighted aggregation of value vectors based on query-key similarity allows any position to receive information from any other position.)",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-positional-encoding",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-positional-encoding",
    "title": "7  Transformers and Attention",
    "section": "7.2 Positional Encoding",
    "text": "7.2 Positional Encoding\nA patient with hypertrophic cardiomyopathy carries a variant in the MYH7 gene’s promoter region. Determining pathogenicity requires knowing precisely where the variant sits relative to the transcription start site: a variant at position -30 (where the TATA box resides) carries entirely different implications than the same sequence at position +500 (within the 5’ UTR). Position is not merely bookkeeping for genomic sequences; it encodes biological function. The canonical TATA box must appear 25 to 30 base pairs upstream of transcription initiation to function; the same sequence elsewhere carries no regulatory significance. Splice site recognition depends on the invariant GT and AG dinucleotides appearing at precise distances from exon boundaries. Enhancer-promoter interactions require specific distance relationships that vary by locus and cell type. A model that cannot distinguish position 100 from position 10,000 cannot learn the positional grammar that governs gene regulation.\nSelf-attention, by design, computes interactions based purely on content: the attention weight between positions depends only on their query and key vectors, not on where they sit in the sequence. The basic concepts of position encoding were introduced in Section 5.6.1; here we examine the specific implementations that transformer architectures employ. Shuffling input token order changes nothing about how attention weights are computed. The model has no inherent notion of sequence order, a property called permutation invariance (meaning the output is identical regardless of input ordering). Consider a sentence like “The dog bit the man” versus “The man bit the dog”—the same words, but position changes meaning entirely. For genomic data where position matters fundamentally, this blindness to order would be catastrophic. DNA has 5’ to 3’ directionality that determines transcription direction. Distance from transcription start sites determines promoter versus enhancer classification. Strand orientation distinguishes sense from antisense transcription. Positional encodings inject information about token positions into the model, breaking permutation invariance by making the model aware of where each token sits in the sequence.\n\n\n\n\n\n\nStop and Think: Why Position Matters\n\n\n\nBefore reading about specific positional encoding methods, consider these questions:\n\nA TATA box sequence (TATAAA) appears at two locations: 28 bp upstream of a transcription start site, and 5,000 bp upstream. Should the model treat these identically?\nAn enhancer-promoter interaction works when the enhancer is 50 kb upstream but not when it is 500 kb upstream. What does the model need to learn about distance?\nIf you were designing a positional encoding for genomic sequences, would you encode absolute positions (position 1, 2, 3…) or relative distances between positions? What are the tradeoffs?\n\n\n\n\n\n\n\n\n\n\n\nLearned absolute positional embeddings\n\n\n\n\n\n\n\nSinusoidal positional encoding\n\n\n\n\n\n\n\n\n\nALiBi attention bias\n\n\n\n\n\n\n\nRotary Position Embeddings\n\n\n\n\n\n\nFigure 7.4: Positional encoding approaches for transformers. (A) Learned absolute embeddings assign a trained vector to each position; effective but limited to training sequence length. (B) Sinusoidal encodings use fixed sine/cosine functions at different frequencies, creating unique position fingerprints that generalize beyond training length. (C) ALiBi applies linear attention penalties based on distance, encouraging local attention while naturally extrapolating to long sequences. (D) RoPE encodes position through geometric rotation, making dot products depend on relative rather than absolute positions. Each approach encodes different assumptions about how position information should influence attention.\n\n\n\n\n7.2.1 Absolute Position Encodings\n\n\n\n\n\n\nMathematical Content Ahead\n\n\n\nThe sinusoidal positional encoding formulas below may appear complex. The key intuition is that different dimensions of the encoding oscillate at different frequencies, creating a unique “fingerprint” for each position. Low-frequency components indicate coarse position (beginning vs. end of sequence), while high-frequency components indicate fine position (exact nucleotide location).\n\n\nThe original transformer used sinusoidal functions with different frequencies for each embedding dimension (Vaswani et al. 2023). For position \\(pos\\) and dimension \\(i\\):\n\\[\n\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_\\text{model}}}\\right)\n\\]\n\\[\n\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_\\text{model}}}\\right)\n\\]\nThese fixed patterns have useful properties. They are deterministic (the same for all sequences), allow the model to learn to attend by relative positions (since \\(\\mathrm{PE}(pos+k)\\) can be expressed as a linear function of \\(\\mathrm{PE}(pos)\\)), and generalize to sequence lengths not seen during training. The different frequencies across dimensions create a unique “fingerprint” for each position: lower-frequency components capture coarse position while higher-frequency components capture fine position.\nMany genomic models use learned positional embeddings instead: lookup tables where each position has a learned vector added to the input embedding. DNABERT and Nucleotide Transformer both employ learned positional embeddings, allowing the model to discover position-dependent patterns specific to genomic data (Ji et al. 2021; Dalla-Torre et al. 2023). The trade-off is that learned embeddings do not automatically extrapolate to longer sequences. A model trained with maximum sequence length of 512 tokens has no learned embedding for position 513, creating a hard boundary on sequence length at inference time. This fixed maximum context proves particularly restrictive for genomics, where biological phenomena span scales from individual binding sites to megabase regulatory domains.\n\n\n7.2.2 Relative Position Encodings\nAbsolute encodings treat position 1,000 and position 1,001 as having different representations even though their relative relationship (adjacent positions) may matter more than their absolute locations. For genomic applications, relative distance often carries more biological meaning than absolute coordinates: nucleosomes are spaced approximately 200 base pairs apart regardless of genomic location, and enhancer-promoter interactions depend on distance rather than absolute position. A transcription factor binding site 50 bases upstream of a transcription start site has similar effects whether the TSS sits at genomic position 1,000 or 1,000,000. Relative positional encodings address this by encoding distances between positions rather than absolute coordinates.\nT5-style relative position bias adds a learnable scalar to attention scores based on the distance between query and key positions (Raffel et al. 2023). This bias helps the model learn that nearby positions often interact more strongly than distant ones while remaining agnostic about absolute position. The learned biases can capture genomic-specific distance preferences, such as the characteristic spacing of regulatory elements or the periodicity of nucleosome positioning.\nExtrapolation to longer sequences than seen during training poses a persistent challenge for learned position embeddings. A model trained on 1-kilobase sequences may behave unpredictably when asked to process 10-kilobase sequences at inference, since the position embeddings for distant positions were never optimized. Attention with Linear Biases (ALiBi) addresses this limitation by adding a fixed linear penalty to attention scores based on distance, without learned parameters (Press, Smith, and Lewis 2022). For a head with slope \\(m\\), attention between positions separated by distance \\(|i - j|\\) is penalized by \\(m|i - j|\\). Different heads use different slopes, encouraging some to focus locally and others globally. Because the linear penalty extrapolates naturally, ALiBi generalizes well beyond training lengths, making it attractive for genomic applications where sequence length varies dramatically. The linear distance penalty may not perfectly capture biological relationships (where some regulatory interactions span consistent long distances while others operate locally), but the simplicity and extrapolation properties have proven valuable.\nAn alternative approach encodes position through geometric transformation rather than additive bias. Rotary Position Embeddings (RoPE) multiply query and key vectors by rotation matrices whose angles depend on position (Su et al. 2024). The dot product between rotated query and key then depends on their relative distance, combining benefits of relative encoding with efficient implementation. RoPE has become standard in recent large language models and appears increasingly in genomic transformers, including variants of Nucleotide Transformer, offering a balance between the flexibility of learned embeddings and the extrapolation capability of fixed schemes. The choice between ALiBi and RoPE often depends on whether the application prioritizes aggressive length extrapolation (favoring ALiBi) or compatibility with pretrained language model architectures (favoring RoPE).\nThe following table summarizes the key tradeoffs among positional encoding approaches:\n\n\n\nTable 7.1: Comparison of positional encoding approaches for genomic transformers. Length extrapolation indicates whether the encoding generalizes to sequences longer than seen during training.\n\n\n\n\n\n\n\n\n\n\n\nEncoding Type\nHow Position is Encoded\nLength Extrapolation\nGenomic Models Using It\n\n\n\n\nSinusoidal (fixed)\nSine/cosine at different frequencies\nGood\nOriginal Transformer\n\n\nLearned absolute\nLookup table per position\nPoor (hard limit)\nDNABERT, Nucleotide Transformer\n\n\nT5-style relative\nLearned bias by distance\nModerate\nSome genomic variants\n\n\nALiBi\nLinear attention penalty by distance\nExcellent\nLong-context models\n\n\nRoPE\nQuery/key rotation by position\nGood\nNT-v2, recent LLMs\n\n\n\n\n\n\n\n\n7.2.3 Genomic Position Considerations\nGenomic sequences impose requirements on positional encoding beyond what natural language demands. DNA has strand directionality: ACGT on the forward strand has different regulatory meaning than the same sequence on the reverse strand. Positional encodings should enable the model to learn strand-specific patterns. Some genomic transformers encode both strands separately and combine predictions; others rely on the model learning strand orientation from sequence content alone.\nGenomic coordinates pose another design choice. Should position 1 correspond to a fixed genomic landmark (transcription start site, gene start) or simply indicate sequence order without biological reference? Models predicting regulatory activity often center sequences on promoters, using positions relative to the TSS. Foundation models trained on random genomic segments typically use positional encodings reflecting sequence order without genomic coordinate reference. The choice affects what the model can learn: TSS-relative positions enable learning of distance-dependent regulatory patterns, while sequence-order positions require the model to learn these patterns implicitly from content.\nAbsolute genomic coordinates carry accumulated knowledge with no linguistic analog. The position chr17:41,276,045 indexes decades of clinical observations, population frequencies, and functional studies that inform variant interpretation before a model processes a single nucleotide. Some recent models have explored incorporating these absolute coordinates, allowing models to learn position-specific patterns like centromeric sequences or telomeric regions. Circular genomes like mitochondrial DNA and bacterial chromosomes create additional complexity: they have no beginning or end, creating wraparound relationships that linear position encodings cannot naturally represent. These adaptations illustrate that position encoding is not merely a technical detail but a design choice shaping what biological patterns a model can capture.\nThe choice of positional encoding interacts with tokenization strategies (see Chapter 5). K-mer tokenization reduces sequence length (and thus attention cost) but changes what “position” means: position 1 might represent nucleotides 1 through 6 rather than a single base. A model using 6-mer tokens with learned positional embeddings learns different position-dependent patterns than one using single-nucleotide tokens, even if both cover the same genomic region.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-transformer-block",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-transformer-block",
    "title": "7  Transformers and Attention",
    "section": "7.3 Transformer Block",
    "text": "7.3 Transformer Block\nA clinician interpreting a BRCA1 variant needs a model that does more than identify isolated motifs or single long-range interactions. The variant’s pathogenicity depends on how multiple regulatory signals integrate: local splice site grammar, enhancer contacts from 20 kilobases upstream, and transcription factor binding sites whose effects depend on chromatin context. Single attention layers identify pairwise relationships, but understanding complex regulatory logic requires building hierarchical representations where simple patterns combine into compound signals. This hierarchical integration emerges from stacking transformer blocks, the modular units that combine attention and nonlinear processing to build increasingly abstract representations through repeated application.\n\n\n\n\n\n\nTransformer block architecture with pre-norm configuration\n\n\n\n\nFigure 7.5: Transformer block architecture with pre-norm configuration. Input embeddings (dimension d) pass through layer normalization before multi-head self-attention, which computes interactions across all positions. A residual connection adds the input directly to the attention output, creating gradient highways for stable training. A second layer normalization precedes the position-wise feed-forward network, which expands representations to 4d dimensions, applies GELU nonlinearity, and projects back to d dimensions. Another residual connection completes the block. Stacking multiple blocks (inset) enables hierarchical representation learning, with early layers capturing local patterns and later layers integrating them into complex regulatory predictions.\n\n\n\n\n7.3.1 Block Components\nEach transformer block accomplishes two distinct functions: enabling positions to share information across the sequence, and transforming that aggregated information through nonlinear processing. The multi-head self-attention layer handles global communication, allowing each position to gather information from the entire sequence. The position-wise feed-forward network processes each position independently, applying nonlinear transformations to the aggregated information. Separating these functions into distinct components allows each to be optimized independently and provides clear computational semantics: attention determines which positions are relevant to each other (the “what to consider” question), while the feed-forward network determines how to combine that information (the “what to conclude” question).\n\n\n\n\n\n\nKey Insight: Attention vs. Feed-Forward\n\n\n\nThe transformer block’s two-stage design reflects a fundamental separation of concerns:\n\nAttention: “Which other positions should I consider?” (inter-position communication)\nFeed-forward: “Given what I’ve gathered, what should I conclude?” (per-position computation)\n\nThis separation explains why transformers can be so effective: attention handles the variable-length, content-dependent routing problem, while feed-forward networks handle the fixed-size, position-local computation problem. Each component can be optimized for its specific role.\n\n\nThe feed-forward network consists of two linear transformations with a nonlinearity between them. Typically, this expands the dimension by a factor of four (from model dimension d to 4d), applies GELU or similar activation, then projects back to dimension d. This expansion allows processing through a high-dimensional nonlinear transformation before producing output for the next layer. The position-wise nature means each position is transformed identically but independently; cross-position information flows only through attention.\nDeep transformer networks would be untrainable without mechanisms to stabilize gradient flow. Layer normalization addresses activation scale by normalizing across the feature dimension at each position, preventing the explosive growth or collapse of activations that would otherwise occur across dozens of layers. Two conventions exist for placement: post-norm applies normalization after each sublayer, while pre-norm applies it before. Pre-norm has become standard because it improves training stability for deep networks, though post-norm can achieve slightly better final performance with careful tuning (Xiong et al. 2020).\nNormalization alone cannot solve the vanishing gradient problem that plagues deep networks. Residual connections provide the second essential stabilization mechanism by adding each sublayer’s input directly to its output, creating gradient highways that bypass the transformations within attention and feed-forward operations. These connections serve two critical functions. First, they allow gradients to flow directly through many layers without repeated transformation, enabling training of very deep networks. Second, they create an inductive bias toward incremental refinement: each layer makes small adjustments to the representation rather than constructing entirely new representations from scratch. For genomic models, this incremental refinement maps naturally onto biological interpretation, where early layers might identify motifs, middle layers might recognize motif combinations, and later layers might integrate these patterns into regulatory predictions.\n\n\n7.3.2 Information Flow and Depth\nThe flow through a pre-norm transformer block proceeds as follows. Input X is normalized, processed by multi-head attention to produce X’, and added back via residual connection, yielding X + X’. This sum is normalized, passed through the feed-forward network to produce X’’, and added via another residual connection, yielding final output X + X’ + X’’. Each layer thus adds refinements to the representation while preserving information from earlier processing.\nStacking depth determines how many times this refinement occurs. Shallow transformers (6 layers or fewer) are parameter-efficient but may lack capacity for complex hierarchical patterns. Deep transformers (12 to 24 layers) can learn sophisticated representations that capture how promoter elements, enhancer contacts, and chromatin state combine to determine expression. Most genomic transformers use 6 to 24 layers, varying by application. Models for short sequences (small RNAs, individual binding sites) might use fewer layers, while foundation models for long genomic contexts often use deeper stacks to build representations that integrate information across multiple biological scales.\nThe choice of depth balances capacity against trainability. Deeper networks learn more complex functions but are harder to optimize, prone to overfitting without sufficient data, and more expensive at training and inference. For genomic models, depth often correlates with the complexity of patterns being modeled. Simple motif recognition tasks might benefit more from wider layers (larger d) than deeper stacks, while tasks requiring hierarchical integration (understanding how promoter-enhancer-insulator relationships determine expression) may benefit from additional depth that builds increasingly abstract representations layer by layer.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nTest your understanding of transformer block architecture:\n\nWhat are the two main components of a transformer block, and what does each do?\nWhy do residual connections enable training of deep networks?\nIf a model has 12 layers with 8 attention heads each, how many different attention patterns can it learn?\nWhat is the difference between pre-norm and post-norm layer placement?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nMulti-head attention for inter-position communication allows positions to exchange information; feed-forward network performs per-position computation to transform representations. (2) Residual connections create gradient highways that allow signals to flow directly through many layers without degradation, preventing vanishing gradients in deep networks. (3) 96 different attention patterns (12 layers × 8 heads = 96 independent attention mechanisms). (4) Pre-norm normalizes before each sublayer, providing more stable training; post-norm normalizes after, which can achieve better final performance but is harder to train.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-scaling",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-scaling",
    "title": "7  Transformers and Attention",
    "section": "7.4 Scaling to Genomic Sequences",
    "text": "7.4 Scaling to Genomic Sequences\nA 52-year-old patient presents with unexplained cardiomyopathy, and whole-genome sequencing reveals a structural variant spanning 500 kilobases on chromosome 14, disrupting the MYH7 locus and several upstream regulatory elements. The clinical team needs to assess whether this variant explains the patient’s phenotype. Standard transformers cannot help: the quadratic complexity of self-attention makes 500-kilobase contexts computationally intractable. This gap between clinical need and computational capability defines a central challenge for genomic AI. The attention mechanism enables long-range modeling in principle, but practical constraints on memory and computation limit what contexts can actually be processed. Effective application of transformers to genomics requires strategies for managing these constraints.\n\n7.4.1 Quadratic Barrier\nComputing all pairwise attention scores requires \\(O(L^2)\\) operations, where L is sequence length. For a 10-kilobase sequence tokenized at single-nucleotide resolution, this means 100 million attention computations per layer. A 200-kilobase sequence requires 40 billion computations per layer. Memory requirements scale similarly because the attention matrix must be stored for backpropagation.\nThis scaling constraint directly limits what clinical questions transformers can address. The HLA region (critical for transplant matching and autoimmune disease risk in the approximately \\(40{,}000\\) organ transplants performed annually in the United States) spans approximately \\(4\\) megabases and contains the most polymorphic genes in the human genome. Modeling this region with standard self-attention would require \\(16 \\times 10^{12}\\) attention computations per layer, far exceeding practical limits. Structural variant detection often requires analyzing megabase-scale contexts to identify breakpoints and assess functional impact, yet these contexts remain computationally intractable for standard transformers. A patient with a suspected chromosomal translocation cannot benefit from transformer-based analysis when the relevant context exceeds computational capacity.\n\n\n7.4.2 Parameter Considerations\nThe number of parameters a transformer can effectively utilize depends on both training data quantity and the complexity of patterns to be learned. Transformer parameters come primarily from two sources. Width (model dimension d) determines embedding and hidden state sizes; increasing width allows more complex pattern representation at each position but increases parameters quadratically because weight matrices scale as d × d. Depth (number of layers) determines how many refinement steps occur; increasing depth allows hierarchical abstractions through repeated processing but increases parameters linearly.\nScaling laws from natural language processing suggest performance improves smoothly with increased parameters, data, and compute (Kaplan et al. 2020). Similar principles apply to genomics, though optimal ratios may differ. Genomic sequences are less compressible than natural language: each nucleotide carries less predictable information than words in structured sentences. The entropy of DNA sequence is higher than English text, meaning more parameters may be needed to model the same sequence length. This asymmetry suggests genomic models might benefit relatively more from depth (more processing of high-entropy information) than from width (more dimensions per position when each position carries limited structure).\nThe architectural landscape of genomic foundation models reveals distinct design philosophies. Table 7.1 compares representative models across key architectural dimensions, illustrating how different approaches balance model capacity, context length, and computational efficiency.\n\n\n\nTable 7.2: Architectural comparison of genomic foundation models. Width refers to model/hidden dimension; depth to number of layers. Context length is reported in base pairs (bp) where applicable. Models marked with dagger use non-transformer architectures.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nArchitecture\nWidth (d)\nDepth (Layers)\nHeads\nParameters\nContext\n\n\n\n\nDNABERT\nEncoder-only\n768\n12\n12\n110M\n~0.5 kb\n\n\nDNABERT-2 (zhou_dnabert2_2024?)\nEncoder-only\n768\n12\n12\n117M\nVariable\n\n\nNucleotide Transformer (500M)\nEncoder-only\n1,024\n24\n20\n500M\n6 kb\n\n\nNucleotide Transformer (2.5B)\nEncoder-only\n2,560\n32\n20\n2.5B\n6 kb\n\n\nNT-v2 (250M)\nEncoder-only\n—\n—\n—\n250M\n12 kb\n\n\nHyenaDNA\nHyena stack\n128–256\n2–8\n—\n0.4M–6.6M\nup to 1M bp\n\n\nCaduceus\nBiMamba\n256\n16\n—\n~7M\n131 kb\n\n\nEvo 2 (7B)\nStripedHyena 2\n—\n—\n—\n7B\n1M bp\n\n\nEvo 2 (40B)\nStripedHyena 2\n—\n—\n—\n40B\n1M bp\n\n\n\n\n\n\nSeveral patterns emerge from this comparison. First, traditional transformer-based models like DNABERT and Nucleotide Transformer inherit architectures closely resembling their NLP counterparts, with DNABERT using the same 12-layer, 768-dimension configuration as BERT-base (Ji et al. 2021). The Nucleotide Transformer family scales this approach, with the 500M variant using 24 layers and 20 attention heads, and the 2.5B variant expanding to 32 layers while maintaining the same head count (Dalla-Torre et al. 2023). This scaling primarily increases width (hidden dimension grows from 1,024 to 2,560) rather than dramatically increasing depth, following conventional transformer scaling practices.\nSecond, models designed for long-range genomic modeling adopt fundamentally different architectures to circumvent attention’s quadratic complexity. HyenaDNA replaces attention entirely with implicit long convolutions, achieving million-base-pair contexts with models containing only 2 to 8 layers and a few million parameters (Nguyen et al. 2023). Caduceus extends the Mamba state-space architecture with bidirectional processing and reverse-complement equivariance, using 16 layers to achieve 131 kb context (Schiff et al. 2024). Evo 2 represents the current frontier, with 7B and 40B parameter variants achieving million-token context through the StripedHyena 2 architecture (Brixi et al. 2025). These non-transformer approaches demonstrate that architectural innovation can achieve genomic-scale context lengths that remain computationally intractable for standard attention mechanisms.\nThird, the relationship between parameter count and context length is not straightforward. HyenaDNA achieves the longest context among early models (1 million bp) with fewer than 7 million parameters, while Nucleotide Transformer 2.5B processes only 6 kb despite 400 times more parameters. This inversion reflects a fundamental tradeoff: dense attention captures rich pairwise interactions but scales poorly with sequence length, while subquadratic alternatives like Hyena operators and state-space models sacrifice some interaction modeling capacity for computational tractability. The optimal choice depends on whether the task requires capturing dense local interactions or sparse long-range dependencies.\nThe relationship between parameter count and downstream task performance is not always monotonic: a well-trained smaller model can outperform a poorly trained larger one, and task-specific fine-tuning often matters more than pretraining scale for focused clinical applications. Nucleotide Transformer v2 demonstrated this principle dramatically, achieving comparable or superior performance to the 2.5B-parameter v1 models with only 250M parameters by incorporating architectural improvements and training for more tokens (Dalla-Torre et al. 2023). Similarly, HyenaDNA achieves state-of-the-art results on 12 of 18 benchmarks from the Nucleotide Transformer suite while using 1,500 times fewer parameters. These results suggest that architectural efficiency and training strategy may matter as much as raw parameter count for genomic applications.\n\n\n7.4.3 Context Length Strategies\nStandard self-attention’s \\(O(L^2)\\) complexity becomes prohibitive for long genomic contexts, forcing architectural choices that trade expressiveness for tractability. The strategies employed reflect different assumptions about which interactions matter most for genomic modeling.\nThe quadratic complexity of full attention becomes prohibitive for genomic sequences, but different applications tolerate different trade-offs between efficiency and expressiveness. When most relevant interactions are local (as often holds for regulatory sequences where nearby elements interact more strongly than distant ones), restricting attention to fixed windows reduces complexity to \\(O(Lw)\\) where w is window size. For clinical variant interpretation in coding sequences, where splice sites and reading frame context typically lie within a few hundred bases, local attention may capture the relevant biology. The trade-off is missing long-range interactions that fall outside windows, potentially critical for understanding distal enhancer effects or structural variant consequences.\nHierarchical approaches recover some long-range capability while maintaining efficiency. Lower layers can use local windows to capture fine-grained patterns, while upper layers attend to every \\(k\\)-th position to capture global structure. Hybrid models like Enformer apply CNNs to downsample sequences before transformer layers, reducing the effective sequence length that attention must handle (Avsec et al. 2021). A 200-kilobase genomic region might be compressed to roughly 1,500 positions after CNN processing, making full attention tractable at the cost of single-nucleotide resolution in transformer layers.\nMathematical approximations offer yet another path, preserving dense connectivity while reducing computational cost. Linformer approximates the attention matrix through low-rank decomposition, reducing complexity to linear in sequence length (Wang et al. 2020). Performer uses random feature methods to approximate attention scores without explicitly computing the full \\(L\\timesL\\) matrix (Choromanski et al. 2022). These approximations trade some expressiveness for efficiency and may miss certain long-range dependencies that low-rank structure cannot capture. The choice among sparse patterns, hierarchical designs, and mathematical approximations depends on whether the target biology demands single-nucleotide resolution, long-range connectivity, or both.\n\n\n\n\n\n\nPractical Guidance: Choosing Context Length Strategies\n\n\n\nWhen selecting a context length strategy for a genomic application, consider:\nUse local/windowed attention when: - The biological signal is primarily local (splice sites, TF binding sites) - Single-nucleotide resolution is critical - Training data is limited\nUse hierarchical/hybrid approaches when: - You need both local detail and long-range context - The regulatory architecture involves enhancer-promoter interactions - You can tolerate some loss of fine-grained resolution\nUse subquadratic architectures (Hyena, Mamba) when: - Context length is the primary constraint (&gt;100 kb) - You need single-nucleotide resolution over long ranges - You’re willing to adopt newer, less mature architectures\n\n\n\n\n7.4.4 Memory and Precision\nMemory requirements compound computational challenges for genomic transformers. Training requires storing activations for backpropagation, and attention matrices are particularly memory-intensive. A \\(100\\)-kilobase sequence with \\(16\\) attention heads and \\(12\\) layers requires storing \\(16 \\times 12 \\times 100{,}000 \\times 100{,}000\\) attention weights, approximately \\(2\\) terabytes at \\(32\\)-bit precision before considering other activations.\nMemory constraints often limit model size and sequence length more than computational budget. Recomputing activations during the backward pass rather than storing them (a technique called gradient checkpointing) trades additional computation for reduced memory footprint, enabling training of larger models or longer sequences on fixed hardware at the cost of 20 to 30 percent additional training time. Further memory savings come from reducing numerical precision: using 16-bit floating point for most computations while maintaining 32-bit precision for critical operations like loss computation and optimizer updates. Modern GPUs accelerate 16-bit arithmetic substantially, providing near 2x speedup with minimal precision loss. Flash Attention implements memory-efficient attention computation that avoids materializing the full attention matrix, enabling longer contexts within fixed memory budgets (Dao et al. 2022). Together, these optimizations determine the practical limits of what can be trained on available hardware.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-variants",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-variants",
    "title": "7  Transformers and Attention",
    "section": "7.5 Architectural Variants for Genomics",
    "text": "7.5 Architectural Variants for Genomics\nFoundation model architectures encode assumptions about what matters. A model optimized for scoring existing variants differs fundamentally from one designed to generate novel sequences, which differs again from one predicting molecular interactions. The choice of architecture shapes what questions can be asked.\n\n\n\n\n\n\n\n\nEncoder-only architecture\n\n\n\n\n\n\n\nDecoder-only architecture\n\n\n\n\n\n\n\nHybrid CNN-Transformer architecture\n\n\n\n\n\n\nFigure 7.6: Transformer architectural variants for genomic applications. (A) Encoder-only models use bidirectional attention where every position attends to every other, providing rich representations for classification and understanding tasks. (B) Decoder-only models use causal attention where each position sees only preceding context, enabling autoregressive sequence generation. (C) Hybrid CNN-Transformer architectures use convolutions to compress long sequences before applying attention, balancing long-range context with computational tractability. The choice of architecture should match the downstream task: understanding requires bidirectional context, generation requires causal structure, and long-range prediction benefits from hybrid designs.\n\n\n\n\n7.5.1 Encoder-Only Transformers\nWhen a clinical laboratory queries a pathogenicity database for a novel missense variant, they need a model that integrates information from the entire protein sequence: upstream domains that establish structural context, downstream regions that complete functional units, and evolutionary patterns that distinguish tolerated from deleterious changes. Encoder-only transformers process sequences bidirectionally, allowing each position to attend to all other positions including those that follow in the sequence. This bidirectional context produces richer representations than unidirectional processing because each position’s representation incorporates information from the entire sequence.\nDNABERT exemplifies this architecture, trained with masked language modeling objectives where random tokens are masked and predicted from bidirectional context (Ji et al. 2021). The model learns to predict held-out k-mers based on surrounding sequence, implicitly learning sequence patterns and constraints that transfer to downstream tasks. Nucleotide Transformer follows similar principles at larger scale (Dalla-Torre et al. 2023). These models excel at representation learning: producing embeddings that capture biological properties useful for variant effect prediction, function classification, or other tasks that require fixed-length representations of variable-length sequences.\nBidirectional attention suits tasks where both upstream and downstream context matters for understanding a position. Transcription factor binding depends on flanking sequence in both directions. Splice site recognition requires seeing both exonic and intronic context. Variant pathogenicity may depend on protein domain context from both N-terminal and C-terminal directions. The limitation is that encoder-only architectures cannot generate sequences autoregressively because they require seeing the full sequence to produce any output; they answer “what does this sequence mean” rather than “what sequence should come next.”\n\n\n7.5.2 Decoder-Only Transformers\nGenerating synthetic genomic sequences for therapeutic design, creating diverse antibody libraries for drug discovery, or sampling from learned regulatory grammars all require models that produce sequences one token at a time. Decoder-only transformers use causal attention where each position attends only to itself and preceding positions. This structure enables autoregressive generation: the model produces sequences one token at a time, conditioning each new token on all previous tokens.\nGenSLM applies this architecture to genomic data, training on next-token prediction to learn sequence distributions (Zvyagin et al. 2022). The model learns to predict the next nucleotide or \\(k\\)-mer given all preceding context, implicitly learning the statistical regularities of genomic sequence. This objective aligns naturally with generation tasks: sampling proceeds by repeatedly predicting the next token and appending it to the sequence. Causal attention is essential for generation because the model must produce each position before it can condition subsequent positions on that output.\nThe trade-off is that causal attention produces less rich representations for fixed sequences because each position sees only partial context. Position 100 in a 1000-position sequence has access to only the first 100 positions, not the remaining 900 that might provide relevant information. For variant effect prediction where downstream context matters, this limitation can be substantial. The choice between encoder and decoder architectures reflects a fundamental tension: representation learning benefits from bidirectional context, while generation requires causal structure.\n\n\n7.5.3 Encoder-Decoder Transformers\nSome genomic tasks require transforming one sequence into another of different length or structure. Predicting protein sequence from coding DNA, generating variant descriptions from sequence context, or translating between sequence representations all involve input-output relationships that neither pure encoder nor pure decoder architectures handle naturally. Encoder-decoder architectures combine bidirectional encoding with autoregressive decoding (Vaswani et al. 2023).\nThe encoder processes an input sequence with full bidirectional attention, producing contextualized representations. The decoder then generates output tokens autoregressively, attending both to its own previous outputs (through causal self-attention) and to encoder representations (through cross-attention). This cross-attention allows each decoder position to query the full encoded input when generating output, combining the benefits of bidirectional understanding with autoregressive generation.\nEncoder-decoder models are less common in genomic applications than encoder-only or decoder-only variants because most genomic tasks either need representations (favoring encoders) or generation (favoring decoders), not both simultaneously. Machine translation exemplifies the encoder-decoder use case: encode a sentence in one language, decode into another. Genomic analogs might include predicting protein sequences from codon-optimized DNA or generating clinical variant reports from sequence features, but these applications remain less developed than pure representation or generation tasks.\nThe following table summarizes when to use each architectural variant:\n\n\n\nTable 7.3: Architectural variants for genomic transformers and their optimal use cases. The attention pattern determines what context each position can access during computation.\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nAttention Pattern\nOptimal Use Cases\nExample Models\n\n\n\n\nEncoder-only\nBidirectional (full)\nVariant effect prediction, sequence classification, embedding extraction\nDNABERT, Nucleotide Transformer, ESM-2\n\n\nDecoder-only\nCausal (triangular)\nSequence generation, design, sampling from learned distributions\nGenSLM, Evo, GPT-style models\n\n\nEncoder-decoder\nBidirectional + causal\nSequence-to-sequence tasks, translation, structured output\nMachine translation models\n\n\nHybrid (CNN + transformer)\nHierarchical\nLong-context regulatory prediction with downsampling\nEnformer, Borzoi\n\n\n\n\n\n\n\n\n7.5.4 Hybrid CNN-Transformer Models\nThe most successful genomic transformers combine convolutional and attention mechanisms rather than using transformers alone. This hybrid approach exploits CNNs’ efficiency for local pattern extraction while using transformers for long-range integration, matching the multi-scale structure of genomic regulation where both local motifs and distal interactions determine function.\nEnformer and Borzoi apply convolutional stems to long sequences, downsampling through pooling, then pass compressed representations through transformer layers (Avsec et al. 2021; linder_borzoi_2023?). The CNN layers handle motif recognition, nucleosome positioning signals, and local chromatin features with parameter efficiency that pure transformers cannot match. Transformer layers then integrate across the broader regulatory landscape, learning enhancer-promoter relationships and TAD boundary effects. This division of labor achieves state-of-the-art performance on regulatory prediction tasks while remaining computationally tractable for 200-kilobase contexts. The regulatory sequence models in Chapter 16 examine Enformer and Borzoi comprehensively, including their applications to variant effect prediction and expression modeling.\nThe hybrid approach also addresses the quadratic attention bottleneck indirectly. By downsampling sequences before transformer layers (often by factors of 128 or more), hybrids reduce effective sequence length and thus attention cost. A 200-kilobase genomic region compressed to 1,500 positions requires only 2.25 million attention computations per layer rather than 40 billion for the uncompressed sequence. The cost is loss of single-nucleotide resolution in transformer layers, though the CNN stem preserves local detail that attention layers integrate but do not need to resolve. Chapter 16 examines Enformer and related hybrid architectures in detail.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-training",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-training",
    "title": "7  Transformers and Attention",
    "section": "7.6 Training Dynamics",
    "text": "7.6 Training Dynamics\nWhen a model trained to predict pathogenic variants misclassifies a disease-causing BRCA1 mutation as benign, the consequences extend beyond benchmark metrics. Clinical laboratories may return incorrect results; patients may forego preventive surgeries that could save their lives. Training failures matter clinically because they determine what models learn and what they miss. The evaluation methodology in Chapter 12 examines how to detect such failures before clinical deployment, while Chapter 12 addresses systematic biases that cause models to fail on clinically important subgroups. A model that overfits to common polymorphisms in training data will fail on the rare variants that matter most for diagnosis. A model whose gradients vanish during training will never learn the subtle regulatory patterns that distinguish pathogenic from benign promoter variants. Understanding training dynamics helps predict and prevent these failures.\n\n7.6.1 Optimization\nGenomic transformers inherit their training foundations from natural language processing but require adjustments for biological data. The Adam optimizer and its variant AdamW remain standard, using adaptive learning rates that maintain per-parameter estimates adjusted based on gradient statistics (Loshchilov and Hutter 2019). AdamW applies weight decay directly to parameter updates rather than to the loss function, improving generalization and training stability.\nLearning rate schedules typically use warmup (linearly increasing learning rate from near-zero to peak over the first several thousand steps) followed by decay (linear or cosine decrease over the remaining training). Warmup addresses a specific instability: transformers with random initialization can produce extreme gradients early in training, and adaptive optimizers need time to build accurate gradient statistics. Warmup allows the optimizer to stabilize before applying full learning rates. Skipping warmup often causes training collapse in the first few hundred steps, manifesting as loss spikes or NaN values.\nFor genomic data, learning rate tuning may require adjustment from NLP defaults. Regulatory sequences with highly conserved motifs (TATA boxes, splice site dinucleotides) create strong signals that models can overfit quickly; lower learning rates may prevent latching onto these patterns before learning subtler regulatory grammar. Protein sequences exhibit weaker positional conservation than regulatory DNA, potentially benefiting from higher rates that encourage broader exploration of the loss landscape. Empirically, genomic transformers often use peak learning rates of 1e-4 to 5e-4 (Avsec et al. 2021), somewhat lower than the 1e-3 to 3e-3 common in language modeling.\n\n\n7.6.2 Regularization\nRegularization prevents overfitting, particularly important when training data is limited relative to model size. Genomic datasets, while growing rapidly, remain smaller than the trillion-token corpora used for large language models. A model with 100 million parameters trained on 10 billion nucleotides faces different overfitting risks than one trained on 1 trillion tokens.\nTransformers’ high parameter counts create substantial overfitting risk, particularly for genomic applications where labeled training data may be limited. Two complementary regularization strategies have proven essential. Randomly zeroing activations during training (dropout) forces the network to learn robust features that remain informative even when some information pathways are blocked. Applying dropout to attention weights specifically prevents over-reliance on particular position pairs, encouraging distributed representations. Genomic transformers, like DNABERT, use standard dropout rates of 0.1 to 0.2.\nConstraining parameter magnitudes provides orthogonal regularization. Weight decay penalizes large parameter values, encouraging smaller weights that generalize better to unseen data. For transformers, this penalty is typically applied to all parameters except biases and layer normalization parameters. The coefficient requires careful tuning: too little provides insufficient regularization; too much constrains capacity and reduces model expressiveness. Values of 0.01 to 0.1 are common, with higher values for smaller datasets where overfitting risk is greater. The interaction between dropout and weight decay means that optimal settings for each depend on the other, requiring joint tuning rather than independent optimization.\n\n\n7.6.3 Gradient Stability\nGradient issues plague deep network training and require specific attention for genomic transformers. Vanishing gradients occur when gradients become extremely small through many layers, preventing learning in early layers. Exploding gradients are the opposite: gradients grow exponentially and destabilize training. Transformers mitigate vanishing gradients through residual connections that provide direct gradient paths, allowing gradients to flow from output to early layers without passing through potentially attenuating transformations. Exploding gradients are addressed through gradient clipping, which rescales gradients when their norm exceeds a threshold.\nFor genomic transformers, gradient issues manifest differently than in language models. Natural language has nested grammatical organization (words form phrases, phrases form clauses, clauses form sentences), and this hierarchy creates structural landmarks that concentrate attention on syntactically meaningful positions. Genomic sequences lack equivalent organization: functional elements like promoters, splice sites, and enhancers are scattered without consistent positional relationships, so attention patterns must be learned entirely from data without structural priors to guide gradient flow. This absence of hierarchy interacts with imbalanced token frequencies to compound the problem. Common k-mers receive large gradients from frequent occurrence while rare but biologically important tokens (such as k-mers containing the stop codon TAG) receive small gradients from infrequent appearance. In language, frequent function words are grammatically constrained in their attention patterns; in genomic sequences, common k-mers have no such constraints, allowing their gradients to dominate through frequency alone rather than biological importance. Addressing these imbalances may require loss reweighting or adaptive sampling that ensures rare tokens appear frequently enough for effective learning.\n\n\n7.6.4 Distributed Training\nThe computational scale of genomic foundation models typically exceeds single-graphics processing unit (GPU) capacity, requiring distributed training strategies. Data parallelism replicates the model across GPUs, splitting batches across devices and aggregating gradients. This approach scales well up to batch sizes limited by convergence requirements but does not help when the model itself exceeds GPU memory. Model parallelism splits the model across devices, necessary when parameters exceed single-GPU memory. Pipeline parallelism divides layers across devices and pipelines forward and backward passes, interleaving computation to improve device utilization.\nBatch size selection involves competing considerations. Larger batches provide more stable gradient estimates and better GPU utilization but require more memory and may reduce generalization. Genomic transformers often use gradient accumulation to simulate large batches: small batches process sequentially, gradients accumulate, then a single parameter update occurs. This strategy provides large-batch gradient stability without the memory cost, though it increases training time proportionally. Effective batch sizes of 256 to 4096 sequences are common for genomic transformers, achieved through accumulation over many smaller physical batches (e.g., Nucleotide Transformer’s 2.5B model accumulated gradients from physical batches of just 2 sequences to reach 1 million tokens per update (dallatorre_nucleotide_2024?)).",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-limitations",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-limitations",
    "title": "7  Transformers and Attention",
    "section": "7.7 Limitations and Emerging Alternatives",
    "text": "7.7 Limitations and Emerging Alternatives\nA 48-year-old patient presents with a suspected Lynch syndrome diagnosis, and genetic testing reveals a structural variant spanning 3 megabases on chromosome 2 that may disrupt the MSH2 gene and its upstream regulatory region. The clinical team needs to determine whether this variant explains the patient’s early-onset colorectal cancer and guides surveillance recommendations for family members. Standard transformers cannot address this question: the quadratic complexity of self-attention makes 3-megabase contexts computationally intractable. Current models can span 200 kilobases with hybrid architectures, yet the structural variants and chromosomal rearrangements that cause many inherited cancer syndromes remain beyond reach. This gap between clinical need and computational capability defines the frontier of genomic AI.\n\n\n\n\n\n\nThe quadratic complexity ceiling limits transformer context length\n\n\n\n\nFigure 7.7: The quadratic complexity ceiling limits transformer context length. Computational cost versus sequence length on log-log axes reveals the scaling challenge. Standard self-attention (O(L²)) becomes intractable beyond ~10-50 kilobases depending on hardware. Sparse attention variants reduce cost but still scale superlinearly. State space models like Hyena and Mamba achieve linear O(L) scaling, enabling million-base contexts. Vertical lines mark biologically relevant scales: most enhancer-promoter interactions (50-100 kb) exceed standard transformer capacity, while TAD-scale analysis (~1 Mb) requires sub-quadratic architectures. This scaling constraint, not data availability, motivates architectural innovations beyond standard attention.\n\n\n\n\n7.7.1 Quadratic Ceiling\nThe quadratic complexity of self-attention remains transformers’ most severe limitation for genomics. Computing all pairwise attention scores requires \\(O(L^2)\\) operations and memory. For genomic contexts exceeding \\(100\\) kilobases (roughly \\(100{,}000\\) single-nucleotide tokens), this becomes prohibitive. Even with sparse approximations and efficient implementations, transformers struggle at megabase scales where many regulatory interactions occur and structural variants manifest their effects.\n\n\n\n\n\n\nBig-O Notation\n\n\n\nBig-O notation describes how computational cost scales with input size. For genomic models, the input size L is typically sequence length. When we say attention has \\(O(L^2)\\) complexity, we mean that doubling the sequence length quadruples the computation: a 10,000 bp sequence requires 100 million pairwise comparisons. Sub-quadratic approaches like Hyena’s \\(O(L \\log L)\\) scale far more gently: doubling the sequence length only slightly more than doubles the cost. For chromosome-scale sequences of millions of base pairs, this difference determines whether analysis is feasible at all.\n\n\nRecent models have pushed context lengths substantially. Enformer handles 200 kilobases; emerging models approach 1 megabase. But these achievements rely on hybrid architectures with significant downsampling or hierarchical windowing that may miss certain long-range patterns or single-nucleotide resolution details. Pure transformers without such modifications remain limited to shorter contexts. The fundamental constraint shapes what questions transformers can address and motivates alternatives that escape quadratic scaling.\n\n\n7.7.2 State Space Models\nState space models (SSMs) address the quadratic barrier directly by achieving linear complexity while maintaining long-range modeling capability. Rather than computing all pairwise interactions, SSMs represent sequences as continuous-time dynamical systems, maintaining memory through recurrent state updates that propagate information across positions without explicit pairwise computation.\nArchitectures like S4, Hyena, and Mamba have demonstrated competitive or superior performance to transformers on various sequence modeling tasks while scaling to much longer contexts (Gu et al. 2022; Poli et al. 2023; Gu and Dao 2024). For genomics, this capability enables whole-chromosome or potentially whole-genome modeling that remains intractable for standard transformers. HyenaDNA processes sequences up to 1 million nucleotides at single-nucleotide resolution, enabling analysis of structural variants and long-range regulatory interactions that transformers cannot approach (Nguyen et al. 2023). The Evo model extends this further, achieving context lengths sufficient for bacterial genome-scale modeling (Nguyen et al. 2024). The DNA language model architectures in Chapter 14 examine HyenaDNA, Caduceus, and Evo in detail, exploring how linear complexity enables new categories of genomic analysis including sequence design applications (Chapter 30).\n\n\n7.7.3 Choosing Architectures\nThe choice between transformers and alternatives depends on the biological question and computational constraints. Transformers excel when global context matters but sequences are not extremely long (under 10 to 50 kilobases depending on computational resources). Attention maps provide interpretability, showing which positions the model considers relevant for predictions. Transformers benefit from extensive tooling and pretrained models from NLP that transfer readily to genomics.\nCNNs remain preferable when computational efficiency is paramount and local patterns dominate. For splice site prediction or promoter classification where relevant context spans at most a few hundred base pairs, a well-designed CNN may outperform transformers while using far fewer parameters. The inductive bias toward local patterns also regularizes against overfitting when training data is limited. The convolutional architectures examined in Chapter 6 established these design principles.\nHybrid approaches often achieve the best practical results for intermediate-scale problems. Models combining CNNs for local feature extraction with transformers for long-range integration outperform pure architectures on regulatory prediction tasks, as Chapter 16 demonstrates with Enformer and related models. The optimal combination depends on the specific biological question and the scale of relevant interactions.\nThe transition toward sub-quadratic architectures continues. Early results suggest SSMs match or exceed transformers on some genomic benchmarks while scaling to longer contexts. The question is no longer whether alternatives to quadratic attention exist, but which tasks benefit most from linear-complexity architectures and which retain advantages from explicit pairwise attention computation.\n\n\n\n\n\n\nStop and Think: Architecture Selection\n\n\n\nYou are developing a model for a specific genomic task. For each scenario below, which architecture would you choose and why?\n\nPredicting pathogenicity of missense variants in a 300-amino-acid protein domain. Context is ~900 bp, and you need rich bidirectional representations.\nGenerating novel regulatory sequences that drive tissue-specific expression. You need to sample from a learned distribution of functional sequences.\nPredicting the effects of a 2 Mb structural variant on nearby gene expression. The variant spans multiple genes and regulatory elements.\nReal-time variant annotation in a clinical setting where inference speed matters and context is limited to the immediate gene region.\n\nConsider the tradeoffs between context length, computational cost, generation vs. representation, and interpretability for each case.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch07-attention.html#sec-ch07-conclusion",
    "href": "part_2/p2-ch07-attention.html#sec-ch07-conclusion",
    "title": "7  Transformers and Attention",
    "section": "7.8 Capacity Without Direction",
    "text": "7.8 Capacity Without Direction\nThe transformer architecture provides the computational substrate for modern genomic foundation models, but architecture alone does not determine what models learn. Attention mechanisms enable pairwise interaction modeling across arbitrary sequence distances. Position encodings break permutation invariance to preserve the sequential structure essential to regulatory grammar. Stacked blocks build hierarchical representations through iterative refinement. These components create capacity; training objectives and data determine how that capacity is used.\nSelf-supervised pretraining transforms architectural capacity into biological knowledge. Masked language modeling teaches models to predict held-out tokens from context, implicitly learning the sequence patterns and evolutionary constraints that determine biological function. Next-token prediction in autoregressive models captures sequential dependencies required for sequence generation. Applied to massive genomic datasets, these objectives enable transformers to learn representations that transfer across diverse downstream tasks without task-specific supervision. Foundation models like DNABERT for regulatory sequence, ESM-2 for proteins, and Enformer for expression prediction each demonstrate that transformers trained on biological sequence capture patterns that generalize beyond their training objectives. The pretraining objectives that shape these learned representations are examined in Chapter 8.\nAttention introduced a paradigm shift in how genomic models access context. Where convolutional networks aggregate local information through hierarchical composition, attention enables direct communication between any two positions regardless of distance. The computational challenge shifts from extending receptive fields to managing the quadratic complexity of pairwise attention. State space models and linear attention variants address this bottleneck while maintaining long-range capability, and whether these alternatives ultimately complement or displace standard transformers remains an open question. What is clear is that attention-based architectures have become the default substrate for genomic foundation models, with the pretraining objectives examined in Chapter 8 determining what biological knowledge they acquire.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nIn the query-key-value mechanism, what role does each component play, and why are these three vectors separated rather than using a single representation?\nWhy does attention complexity scale as O(L²) with sequence length, and what practical limit does this impose on genomic modeling with standard transformers?\nExplain why decoder models trained with next-token prediction often require layer hunting for optimal embedding extraction, while encoder models can reliably use final-layer representations.\nHow do state space models like Hyena achieve linear complexity while maintaining long-range modeling capability, and what tradeoff do they make compared to full attention?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nQuery-Key-Value Separation: Queries represent “what information this position seeks,” keys advertise “what this position offers,” and values carry “the actual information to send when attended to.” This separation allows the model to decouple relevance determination (query-key matching via dot products) from information flow (value aggregation). A position can advertise itself as relevant for one type of query (high key-query similarity) while sending different information (value content) when actually attended to, providing flexibility that a single representation cannot achieve.\nQuadratic Complexity and Practical Limits: Attention computes pairwise scores between all positions, requiring L × L comparisons for a sequence of length L. This O(L²) scaling means doubling sequence length quadruples computation and memory requirements. For genomic sequences, standard transformers are practically limited to approximately 10-50 kilobases depending on hardware, forcing hybrid architectures or downsampling for longer contexts. Many clinically important phenomena (structural variants spanning megabases, TAD-scale regulatory interactions) remain beyond reach without alternative architectures.\nLayer Hunting in Decoder vs. Encoder Models: Decoder models use causal attention where each position sees only preceding context, creating a hierarchical processing pattern where early layers capture local patterns and later layers integrate broader context. The optimal embedding layer depends on the downstream task’s need for local vs. global features, requiring empirical evaluation (“layer hunting”). Encoder models use bidirectional attention where all positions see full context at every layer, allowing later layers to consistently refine representations that incorporate both upstream and downstream information, making final-layer embeddings reliably informative.\nState Space Model Linear Complexity: State space models like Hyena represent sequences as continuous-time dynamical systems, maintaining hidden state through recurrent updates that propagate information sequentially rather than computing all pairwise interactions. This achieves O(L) or O(L log L) complexity, enabling million-base-pair contexts at single-nucleotide resolution. The tradeoff is that SSMs use implicit long-range interactions through state propagation rather than explicit pairwise attention, potentially missing certain dense interaction patterns that transformers capture directly, though empirical results suggest this limitation is minor for many genomic tasks.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nCore Concepts:\n\nSelf-attention computes pairwise interactions between all sequence positions, enabling direct modeling of long-range dependencies that convolutions cannot capture\nThe query-key-value mechanism separates “what to look for” (query-key matching) from “what to send” (values), providing flexible information routing\nMulti-head attention allows parallel specialization: different heads learn to track local motifs, periodic spacing, long-range interactions, and motif-specific patterns\nPositional encodings break permutation invariance; relative encodings (ALiBi, RoPE) often outperform absolute encodings for length generalization\n\nArchitectural Variants:\n\nEncoder-only (bidirectional): Best for variant effect prediction, classification, embeddings\nDecoder-only (causal): Required for sequence generation and design\nHybrid CNN-Transformer: Combines local pattern detection with long-range integration\n\nKey Limitations:\n\nQuadratic complexity (\\(O(L^2)\\)) limits practical context to ~10-50 kb for standard transformers\nState space models (HyenaDNA, Mamba, Evo) achieve linear complexity for megabase-scale contexts\nThe choice of architecture encodes assumptions about what biological relationships matter\n\nLooking Ahead: Chapter 8 examines how self-supervised objectives (masked language modeling, next-token prediction, contrastive learning) transform architectural capacity into biological knowledge. The specific pretraining strategy determines what patterns the model learns and what downstream tasks it can address.\n\n\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nChoromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2022. “Rethinking Attention with Performers.” arXiv. https://doi.org/10.48550/arXiv.2009.14794.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDao, Tri, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.” Advances in Neural Information Processing Systems 35 (December): 16344–59. https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.\n\n\nGu, Albert, and Tri Dao. 2024. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” In. https://openreview.net/forum?id=tEYskw1VY2.\n\n\nGu, Albert, Karan Goel, Ankit Gupta, and Christopher Ré. 2022. “On the Parameterization and Initialization of Diagonal State Space Models.” Advances in Neural Information Processing Systems 35 (December): 35971–83. https://proceedings.neurips.cc/paper_files/paper/2022/hash/e9a32fade47b906de908431991440f7c-Abstract-Conference.html.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. “Scaling Laws for Neural Language Models.” arXiv. https://doi.org/10.48550/arXiv.2001.08361.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2019. “Decoupled Weight Decay Regularization.” arXiv. https://doi.org/10.48550/arXiv.1711.05101.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence Modeling and Design from Molecular to Genome Scale with Evo.” Science 386 (6723): eado9336. https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nPoli, Michael, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. 2023. “Hyena Hierarchy: Towards Larger Convolutional Language Models.” In Proceedings of the 40th International Conference on Machine Learning, 28043–78. PMLR. https://proceedings.mlr.press/v202/poli23a.html.\n\n\nPress, Ofir, Noah A. Smith, and Mike Lewis. 2022. “Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation.” arXiv. https://doi.org/10.48550/arXiv.2108.12409.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” arXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nSu, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” Neurocomputing 568 (February): 127063. https://doi.org/10.1016/j.neucom.2023.127063.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://doi.org/10.48550/arXiv.1706.03762.\n\n\nWang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. “Linformer: Self-Attention with Linear Complexity.” arXiv. https://doi.org/10.48550/arXiv.2006.04768.\n\n\nXiong, Ruibin, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. 2020. “On Layer Normalization in the Transformer Architecture.” In Proceedings of the 37th International Conference on Machine Learning, 10524–33. PMLR. https://proceedings.mlr.press/v119/xiong20b.html.\n\n\nZvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. “GenSLMs: Genome-Scale Language Models Reveal SARS-CoV-2 Evolutionary Dynamics.” bioRxiv. https://doi.org/10.1101/2022.10.10.511571.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformers and Attention</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html",
    "href": "part_2/p2-ch08-pretraining.html",
    "title": "8  Pretraining Strategies",
    "section": "",
    "text": "8.1 Masked Language Modeling\nConsider predicting whether a splice site variant in DMD will cause exon skipping in Duchenne muscular dystrophy. The model must recognize the canonical GT-AG splice signals, encode how flanking sequences modulate splicing efficiency, and integrate information from both the upstream exon and downstream intron. A model trained only on labeled splice variants would see perhaps a few hundred DMD examples across the entire clinical literature. A model pretrained on billions of nucleotides learns splice grammar across the entire genome, then applies that knowledge to the specific clinical question. Masked language modeling provides this pretraining by teaching models to predict missing sequence content from surrounding context, and the bidirectional attention it requires captures exactly the upstream-downstream integration that splice prediction demands.\nMLM treats sequences as partially observed and trains models to reconstruct missing content. The procedure is straightforward: randomly mask portions of an input sequence, feed the corrupted sequence to the model, and train the model to predict the original tokens at masked positions. A masking strategy replaces selected tokens with a special [MASK] token, leaving the surrounding context intact. The model processes the masked sequence through its layers and produces predictions for the masked positions, typically optimizing cross-entropy loss over the vocabulary at each masked location.\nMLM encourages bidirectional context integration, and this bidirectionality has direct clinical relevance. Unlike autoregressive models that condition only on preceding tokens, MLM models see both left and right context when predicting masked positions. For genomics, this matches biological reality: regulatory function depends on patterns both upstream and downstream of any given position. A transcription factor binding site is recognized through flanking sequences on both sides. Splicing signals require coordination between donor and acceptor sites separated by hundreds of bases. Missense variants disrupt protein function through effects that depend on the entire domain context, not just the preceding amino acids. The bidirectional attention mechanisms examined in Chapter 7 naturally capture these dependencies.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-mlm",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-mlm",
    "title": "8  Pretraining Strategies",
    "section": "",
    "text": "Worked Example: MLM on a Promoter Sequence\n\n\n\nConsider this 12-nucleotide promoter fragment containing a TATA box:\nOriginal sequence: GCTATAAAGCTT\nStep 1 - Masking (15%): Randomly mask ~2 positions GCT[MASK]TAAGC[MASK]T\nStep 2 - Model prediction: The model sees the masked sequence and, for each [MASK] position, outputs a probability distribution over {A, C, G, T}.\nStep 3 - Loss computation: For position 4 (true answer: A), suppose the model predicts:\n\n\n\nNucleotide\nProbability\n\n\n\n\nA\n0.70\n\n\nT\n0.20\n\n\nG\n0.05\n\n\nC\n0.05\n\n\n\nCross-entropy loss = \\(-\\log(0.70) = 0.36\\)\nWhat the model learns: Positions within TATA boxes strongly predict A/T nucleotides from surrounding context. After seeing millions of promoters, the model internalizes that sequences matching ...TAT_AA... almost always have A or T in the masked position—this is exactly the regulatory grammar that transfers to splice prediction and variant interpretation.\n\n\n\n\n\n\n\n\nKey Insight: What MLM Prediction Difficulty Reveals\n\n\n\nThe key insight is that accurate prediction requires learning genuine sequence structure. To predict a masked position in a transcription factor binding site, the model must recognize the surrounding motif context. To predict masked splice donor sequences, the model must encode the consensus GT dinucleotide and the flanking patterns that modulate splicing strength. Over millions of training examples, models build distributed representations of motifs, compositional rules, and sequence constraints that transfer to tasks never seen during pretraining. The DMD splice variant can be evaluated using patterns learned from every splice site in the genome.\n\n\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore continuing, consider: If a model confidently predicts the masked nucleotides in a TATA box (high probability for A or T at each masked position), but struggles to predict nucleotides in a random intergenic region (nearly uniform probabilities), what does this tell you about functional constraint at these two locations?\n\n\n\n\n\n\n\n\n\nStop and Think\n\n\n\nHow would span masking (masking consecutive tokens) differ from random masking in terms of what the model learns? What biological sequences might benefit from each approach?\n\n\n\n8.1.1 Masking Strategies and Their Implications\n\n\n\n\n\n\n\n\nRandom token masking\n\n\n\n\n\n\n\nSpan masking\n\n\n\n\n\n\nFigure 8.2: Masking strategies encode different learning pressures. (A) Random token masking distributes [MASK] tokens throughout the sequence. Individual masked positions can often be predicted from immediately adjacent context, encouraging local pattern learning. (B) Span masking replaces contiguous blocks with single sentinel tokens, removing local context entirely. Predicting masked spans requires reasoning from more distant sequence features, forcing the model to learn compositional patterns and longer-range dependencies. For regulatory sequence modeling, span masking may better capture how transcription factor binding sites and other functional elements operate as integrated units.\n\n\n\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading about span masking, consider: If you wanted a model to learn that transcription factor binding motifs function as units rather than as collections of independent nucleotides, how would you modify the masking strategy to force this understanding?\n\n\nPredicting whether a regulatory variant disrupts an entire transcription factor binding site or merely alters its affinity requires models that learn compositional patterns, not just local nucleotide statistics. The tension between local and compositional learning plays out in masking strategy design.\nRandom masking of individual tokens creates predictions that are relatively local: each masked position can often be inferred from immediately adjacent nucleotides. This approach is efficient but may not force models to learn higher-order structure. Span masking, which masks contiguous blocks of tokens, forces models to infer longer-range dependencies and compositional patterns. If an entire transcription factor binding motif is masked, the model cannot rely on partial motif information and must instead recognize the motif’s role from surrounding regulatory context.\nWhy does span masking force compositional learning while random masking does not? When individual nucleotides are masked, adjacent positions provide strong local cues—if you see TATA_A, the missing position is almost certainly A or T based on TATA box grammar. But when an entire 6-bp motif is masked, those local cues vanish. The model must now ask: “Given that there’s an enhancer upstream and a core promoter downstream, what kind of regulatory element belongs here?” This requires learning which regulatory elements co-occur and why—the compositional grammar of regulation rather than mere nucleotide statistics. For clinical variant interpretation, span masking may better capture whether a regulatory variant disrupts an entire binding site or merely modulates its affinity.\nMasking rates present a fundamental tradeoff between supervision density and prediction difficulty. Higher masking rates (30-40% of tokens) provide more supervision per sequence but make prediction harder and may destabilize training. Since each masked token becomes a prediction target, higher rates extract more learning signal from a single forward pass through the model. Lower masking rates (10-15%) produce more stable training but require more data to achieve equivalent coverage. The standard 15% rate from BERT represents a reasonable compromise, though genomic models have explored values ranging from 10% to 40% depending on context length and tokenization granularity (Devlin et al. 2019). DNABERT used 15% masking on 6-mer tokens, while later models have experimented with adaptive masking rates that increase as training progresses, starting conservatively and becoming more aggressive as the model’s predictions improve (Ji et al. 2021).\n\n\n\nTable 8.1: Comparison of masking strategies for genomic MLM pretraining.\n\n\n\n\n\n\n\n\n\n\n\n\nMasking Strategy\nMechanism\nStrengths\nLimitations\nBest For\n\n\n\n\nRandom token (15%)\nMask individual tokens uniformly\nSimple, stable training, efficient\nMay learn only local patterns\nGeneral pretraining, limited compute\n\n\nHigher rate (30-40%)\nMore tokens masked per sequence\nMore supervision signal per example\nHarder optimization, may destabilize\nLarge datasets, robust architectures\n\n\nSpan masking\nMask contiguous blocks\nForces compositional learning\nMore complex implementation\nRegulatory elements, motif grammar\n\n\nAdaptive rate\nIncrease masking as training progresses\nCurriculum effect, stable-to-aggressive\nRequires tuning schedule\nLong training runs\n\n\n\n\n\n\nTokenization interacts with masking in ways that affect what biological patterns models learn (see Chapter 5 for comprehensive treatment of tokenization strategies). DNABERT pioneered MLM for genomic sequences by applying it to overlapping \\(k\\)-mer tokens: rather than treating DNA as individual nucleotides, DNABERT tokenizes sequences into all possible 6-mers with overlapping windows (Ji et al. 2021). Masking then operates at the \\(k\\)-mer level, with entire 6-mers masked as units. This design encourages learning of \\(k\\)-mer level patterns corresponding to transcription factor binding motifs (typically 6-12 base pairs) and other short functional elements. DNABERT-2 adopted byte-pair encoding (BPE) tokenization, which learns a vocabulary of variable-length subword units from the training corpus (Zhou et al. 2024). BPE tokens represent single nucleotides, common motifs, or repeated elements depending on their frequency. MLM with BPE balances flexibility with compositional structure, though the learned vocabulary may not align with biological functional units in interpretable ways.\nThe design decisions explored by DNABERT and DNABERT-2 established patterns that subsequent DNA language models have built upon and refined. Chapter 14 examines how these architectural and tokenization choices have evolved as the field has scaled to longer contexts and larger training corpora.\n\n\n8.1.2 What Masked Language Models Learn\nMLM objectives drive models to capture multiple levels of sequence organization, from local nucleotide statistics to long-range regulatory grammar. At the lowest level, models learn base composition and local constraints: CpG dinucleotide frequencies, GC content biases, and simple repeat patterns. These basic properties are necessary but not sufficient for biological function prediction.\nAt higher levels, MLM captures motif patterns and sequence grammar. Predicting masked positions in regulatory regions requires recognizing transcription factor binding sites, understanding how motifs combine in enhancers and promoters, and learning context-dependent usage patterns. If certain transcription factor motifs co-occur at specific distances (as they do in developmental enhancers where factors like HOX proteins bind cooperatively), masking one motif and predicting it from the other reinforces this grammatical relationship. This compositional learning is difficult to achieve with supervised learning alone, which typically provides coarse binary labels (“enhancer” versus “non-enhancer”) rather than fine-grained structural information about sequence organization.\n\n\n\n\n\n\nStop and Think\n\n\n\nPause and recall: What are the three main levels of sequence organization that MLM objectives help models learn? How does each level contribute to downstream variant interpretation tasks?\n\n\nMLM also captures evolutionary conservation patterns implicitly, and this has direct relevance for clinical variant interpretation. Conserved sequences are constrained because mutations would disrupt function. By learning to predict conserved patterns from surrounding context, models encode which sequence features are under selection. This knowledge transfers to variant effect prediction, where the model recognizes when a mutation disrupts a learned conserved pattern. A variant that replaces a highly predictable position (one the model confidently fills in during MLM) is more likely to be damaging than one at a position where the model is uncertain. The connection between pretraining on raw sequence and downstream variant interpretation illustrates how self-supervised objectives capture biologically meaningful structure without explicit functional labels. The variant effect prediction approaches in Chapter 17 leverage these learned patterns directly, while probing methods (Section 9.3) reveal what specific patterns models have captured.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-autoregressive",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-autoregressive",
    "title": "8  Pretraining Strategies",
    "section": "8.2 Next-Token Prediction",
    "text": "8.2 Next-Token Prediction\nDesigning a novel promoter sequence for gene therapy requires generating DNA that respects learned regulatory grammar while achieving specific expression characteristics. Masked language modeling can evaluate whether a candidate sequence looks “natural,” but it cannot generate sequences from scratch. A gene therapy team optimizing a CAR-T construct needs promoter variants to test; they cannot simply evaluate candidates one by one when the search space spans \\(4^{500}\\) possible 500-base-pair sequences. Next-token prediction provides the generative capability missing from MLM, learning to predict each token given only preceding tokens and thereby acquiring the ability to sample coherent novel sequences that respect learned biological constraints.\nNext-token prediction represents an alternative paradigm where models learn to predict each token in a sequence given only the preceding tokens. The intuition is familiar from everyday experience: predicting the next word in “The cat sat on the…” is easier than predicting a word from the middle of a sentence, because you have a clear thread of context leading up to the prediction point. Weather forecasting works similarly—tomorrow’s weather is predicted from today’s conditions, and next week’s weather is predicted by chaining together day-by-day forecasts. This autoregressive approach, popularized by GPT-style language models, treats sequence generation as a core capability rather than a secondary feature.\nFor a sequence of length \\(T\\), the model predicts token \\(t\\) from tokens \\(1\\) through \\(t-1\\), maximizing the likelihood of the observed sequence under the model’s learned distribution. The probability of a sequence factors as the product of conditional probabilities for each token given its predecessors—just as the probability of a week’s weather is the product of each day’s probability given the preceding days:\n\\[P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_1, \\ldots, x_{t-1})\\]\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThe autoregressive factorization above is exact by the chain rule of probability. However, it requires choosing an ordering over tokens. For natural language, left-to-right reading order provides a natural choice. For DNA, there is no inherent directionality, which presents a challenge discussed below.\n\n\nAlgorithmically, next-token prediction requires causal masking in the attention mechanism. Each position attends only to earlier positions, ensuring predictions at position \\(t\\) depend exclusively on positions \\(1\\) through \\(t-1\\). Why enforce this restriction during training when we have access to the full sequence? The constraint ensures that the model learns exactly the conditional distributions it will use during generation. If training allowed each position to peek at future tokens, the model would learn different representations than what it needs when generating novel sequences where future tokens do not yet exist. This alignment between training and inference is what makes autoregressive generation principled rather than ad hoc. The loss function is cross-entropy over the vocabulary, computed at every position rather than only at masked locations. During training, teacher forcing allows efficient parallel computation: the model predicts all positions simultaneously by feeding in the ground truth sequence shifted by one position. Generation at inference time is inherently sequential, predicting one token at a time and conditioning each prediction on all previous outputs.\nAutoregressive models develop systematic positional bias during training. Early positions in a sequence are predicted from minimal context: the first token has no conditioning information at all, the second token conditions only on the first, and so on. Later positions benefit from increasingly rich context as the full preceding sequence informs each prediction. This creates asymmetric representation quality across the sequence, with early positions learned less reliably than later ones. For natural language, this asymmetry is partially justified by syntactic structure (sentence openings are more formulaic than continuations), but genomic sequences have no such directional bias. Position 1 of a regulatory element carries as much functional information as position 500. Training dynamics that systematically disadvantage early positions introduce artifacts unrelated to biology.\nSeveral strategies mitigate positional bias. Training on both forward and reverse-complement sequences ensures that each position appears early in some training examples and late in others, averaging out directional effects. Prefix conditioning provides bidirectional context for an initial segment before autoregressive generation begins, giving all generated positions access to rich conditioning information. Some architectures incorporate bidirectional “warm-up” layers before causal attention, building position-independent representations that subsequent autoregressive layers can condition on. The severity of positional bias depends on sequence length and model capacity; for short sequences (under 1000 tokens), the effect is modest, but for genome-scale contexts exceeding 100 kilobases, early positions may be substantially underrepresented in learned distributions.\nThe fundamental difference from MLM lies in what the model can see during prediction. Autoregressive models build representations from unidirectional context, learning to generate sequences that respect learned constraints. This makes autoregressive pretraining attractive for sequence design applications (see Chapter 30). Sampling new sequences proceeds naturally: predict the first token, condition on it to predict the second, and continue token by token. The generation process directly uses the learned conditional distributions without requiring additional architectural modifications or iterative refinement procedures.\n\n8.2.1 Genomic Applications\nDNA sequences present a complication that natural language does not: they have no inherent directionality. Both strands encode information, and regulatory function is often strand-agnostic. A transcription factor binding site functions identically whether read 5’-to-3’ or on the reverse complement strand. This contrasts with natural language, where left-to-right reading order carries meaning. Early autoregressive genomic models addressed this by training separate models for forward and reverse strands or by augmenting training data with reverse-complement sequences. More recent approaches treat strand symmetry as an architectural constraint, ensuring that forward and reverse complement sequences produce equivalent representations through weight sharing or explicit symmetrization.\nEvo represents a large-scale autoregressive genomic model trained on whole genomes with long-context architectures (Nguyen et al. 2024). Using StripedHyena layers to achieve contexts exceeding 100 kilobases, Evo learns long-range dependencies including gene structure, repeat organization, and regulatory architecture spanning tens of kilobases. This enables generating coherent synthetic genomes that respect higher-order structure, not just local motif patterns. For therapeutic applications, Evo’s generative capability could design synthetic regulatory circuits, generate diverse candidate sequences for directed evolution, or produce training data through synthetic augmentation when real labeled data is scarce. The Evo architecture is examined in detail in Chapter 14, while sequence design applications are treated in Chapter 30.\nProtein sequence models trained autoregressively typically generate N-terminus to C-terminus, matching ribosomal synthesis and co-translational folding. Whether this biological asymmetry meaningfully improves learned representations remains unclear: autoregressive models learn conditional sequence distributions, not physical processes, and bidirectional masked language models like ESM-2 perform excellently despite having no inherent directionality. For design applications, generation direction is likely a second-order effect. ESM models and protein design systems like ProtGPT2 predict amino acid sequences autoregressively, learning protein grammar and evolutionary constraints that transfer to structure prediction and function annotation (Ferruz, Schmidt, and Höcker 2022). For designing therapeutic proteins (antibodies, enzymes, peptide drugs), autoregressive generation produces candidates that respect learned constraints on foldability and function. Chapter 15 examines these protein language models in detail.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-comparison",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-comparison",
    "title": "8  Pretraining Strategies",
    "section": "8.3 MLM and Autoregressive Comparison",
    "text": "8.3 MLM and Autoregressive Comparison\n\n\n\n\n\n\n\n\nBidirectional context in MLM\n\n\n\n\n\n\n\nCausal context in autoregressive models\n\n\n\n\n\n\nFigure 8.3: Information flow determines downstream capabilities. (A) Masked language models use bidirectional attention, allowing each position to integrate information from the entire sequence. This produces richer representations suited for understanding tasks like variant interpretation, where both flanking regions inform the prediction. (B) Autoregressive models use causal attention, where each position sees only preceding context. This restriction is essential for generation (future tokens cannot exist during sampling) but produces less informed representations. The choice between objectives should match the intended downstream application: understanding tasks favor bidirectional pretraining; generation tasks require autoregressive structure.\n\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA researcher wants to predict whether a missense variant in the middle of a protein domain is pathogenic. They need to assess how the variant disrupts interactions with residues both before and after it in the sequence. Which pretraining objective would provide better representations for this task, and why?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nMasked language modeling (MLM) would provide better representations for this task. MLM uses bidirectional attention, allowing the model to integrate information from both upstream and downstream residues when evaluating the variant’s impact. Since pathogenicity depends on how the variant disrupts interactions with the entire domain (both before and after the mutation), having access to full sequence context produces richer representations than autoregressive models, which only see preceding residues.\n\n\n\n\n\nThe tension between bidirectional understanding and generative capability represents the fundamental tradeoff between these objectives. For tasks requiring understanding of full sequence context, MLM’s bidirectional attention provides richer representations. Predicting transcription factor binding at a specific location benefits from seeing both upstream and downstream sequence, information that autoregressive models cannot access during inference. Variant effect prediction similarly benefits from full context: a missense variant’s impact depends on the entire domain, not just the preceding residues.\nAutoregressive models offer more principled generation. Their sequential prediction structure matches the generation process exactly, whereas generating from MLM models requires iterative masking and filling procedures that were not part of pretraining. A promoter design task using MLM would require starting with random sequence, masking positions, predicting fills, remasking, and iterating until convergence. This procedure is ad hoc and may not produce sequences that lie on the learned distribution. Autoregressive generation is direct: sample token by token from learned conditionals.\n\n\n\n\n\n\nKey Insight: Alignment Principle\n\n\n\nTask-specific performance depends on alignment between pretraining and downstream objectives. If the downstream task involves predicting missing information from context (variant effect prediction, binding site identification, conservation scoring), MLM pretraining provides better transfer. If the downstream task involves generation or sequential decision-making (sequence design, sampling from conditional distributions, therapeutic protein generation), autoregressive pretraining aligns more naturally.\n\n\nTraining efficiency differs between objectives in ways that affect practical decisions. MLM predicts only 15% of tokens per sequence but uses bidirectional context for each prediction. Autoregressive models predict all tokens but with unidirectional context. The effective supervision per sequence is higher for autoregressive training, but each prediction is less informed. For fixed compute budgets, the tradeoffs roughly balance, with optimal choice depending on downstream applications rather than training efficiency alone.\n\n\n\nTable 8.2: Comparison of MLM and autoregressive pretraining objectives for genomic applications.\n\n\n\n\n\n\n\n\n\n\nCriterion\nMLM (BERT-style)\nAutoregressive (GPT-style)\n\n\n\n\nContext available\nBidirectional (full sequence)\nUnidirectional (preceding only)\n\n\nPrimary strength\nUnderstanding, classification\nGeneration, sampling\n\n\nSupervision per sequence\n~15% of tokens\n100% of tokens\n\n\nStrand symmetry\nNatural (no ordering)\nRequires augmentation\n\n\nVariant effect prediction\nStrong (sees flanking context)\nWeaker (misses downstream)\n\n\nSequence design\nIterative, ad hoc\nDirect sampling\n\n\nLayer selection for embeddings\nFinal layer typically best\nIntermediate layers often better\n\n\nExample models\nDNABERT, ESM-2\nEvo, ProtGPT2\n\n\n\n\n\n\nThis alignment extends beyond task type to affect how practitioners extract and use model representations. Encoder models trained with MLM produce final-layer embeddings that work reliably across diverse downstream tasks because the pretraining objective shaped representations for general utility. Decoder models trained with next-token prediction specialize their final layers for vocabulary prediction, often making intermediate layers superior for classification and regression tasks. This layer hunting problem adds hyperparameter search burden when using decoder models for non-generative applications, sometimes requiring evaluation across all layers to identify where task-relevant information concentrates. The practical implications for model deployment are examined in ?sec-ch09-layer-selection.\n\n8.3.1 Hybrid Architectures\nThe dichotomy between MLM and autoregressive objectives is not absolute. Hybrid architectures attempt to capture bidirectional understanding while retaining generative capability, though they add complexity and training cost.\nPermutation language modeling, introduced in XLNet, trains on all possible token orderings rather than a fixed left-to-right sequence (yang_xlnet_2019?). For each training example, the model samples a random permutation of positions and predicts tokens in that order, with each position attending only to positions earlier in the sampled permutation. Across many permutations, every token eventually conditions on every other token, achieving bidirectional context in expectation while maintaining autoregressive structure for any single forward pass. The approach is elegant but computationally expensive: the permutation sampling and bookkeeping add overhead, and generation still requires committing to a specific ordering. For genomic applications, permutation LM could address strand symmetry naturally (forward and reverse orderings are equally likely), but implementations remain rare in the biological literature. One example is ProtXLNet examined in ?sec-ch12-alternative-architectures.\nPrefix language modeling offers a more practical hybrid. The model processes an initial prefix bidirectionally, building rich contextualized representations, then switches to autoregressive generation for the remainder. This architecture underlies encoder-decoder models like T5 and has been adapted for protein design, where a conditioning context (desired function, scaffold structure, or homologous sequences) is encoded bidirectionally before generating novel sequence autoregressively. ProGen2 applies this pattern to conditional protein generation, encoding functional annotations or partial sequences as prefix context before sampling completions (nijkamp_progen2_2023?). The prefix provides the “understanding” that guides generation, combining MLM-style bidirectional encoding where context is known with autoregressive sampling where novelty is needed. For therapeutic design, this enables specifying desired properties (binding target, expression level, stability) as encoded context while generating diverse candidate sequences that respect those constraints.\nThe cost of hybrid approaches is architectural complexity and training overhead. Pure MLM and pure autoregressive models have simpler implementations and clearer training dynamics. Whether the benefits of hybridization justify the costs depends on application requirements: tasks demanding both rich understanding and flexible generation may warrant the complexity, while tasks emphasizing one capability over the other are better served by the appropriate pure objective.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-denoising",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-denoising",
    "title": "8  Pretraining Strategies",
    "section": "8.4 Span Corruption and Denoising",
    "text": "8.4 Span Corruption and Denoising\nClinical variant interpretation must be robust to sequencing errors, population polymorphisms, and batch effects between discovery and validation cohorts. A pathogenic variant identified in a research study must remain classifiable as pathogenic when sequenced on a different platform in a clinical laboratory, surrounded by different technical artifacts and population-specific polymorphisms. A model trained only on pristine reference sequence may fail when encountering the noise and variation present in real patient data. Denoising objectives address this by training models on corrupted inputs, building tolerance to the kinds of perturbations that occur in clinical genomics pipelines.\nSpan corruption generalizes masked language modeling by introducing more complex forms of input degradation. The T5 model popularized this approach for natural language (Raffel et al. 2023), and the principles transfer to genomic sequences with biological adaptations. Rather than masking individual tokens, span corruption masks contiguous spans of variable length and replaces each span with a single sentinel token. The model then generates the original content of all masked spans in sequence, learning to reconstruct substantial missing regions rather than isolated positions.\nThis objective teaches different aspects of sequence structure than standard MLM. Reconstructing entire spans requires understanding longer-range dependencies and compositional patterns. If a span encompasses an entire transcription factor binding motif (typically 6-12 base pairs), the model cannot infer the motif from partial information and must instead reason about the motif’s role from surrounding regulatory context. Span lengths are typically sampled from a distribution (geometric or uniform) with a mean around 3-5 tokens, creating a mix of short and long reconstruction challenges within each training example.\n\n8.4.1 Corruption Beyond Masking\nReal clinical sequencing data contains substitution errors, missing bases, and spurious insertions that simple masking does not prepare a model to handle. If a model has only ever seen clean reference sequence with masked positions, will it recognize a pathogenic variant when the surrounding bases contain sequencing errors? Training with diverse corruption strategies builds the robustness that clinical deployment demands.\nDenoising objectives extend beyond masking to include other forms of corruption that mirror real-world data degradation. Token substitution replaces input tokens with random tokens from the vocabulary, creating corrupted sequences that resemble sequencing errors or natural variation. The model learns to distinguish correct from incorrect tokens based on surrounding context, encouraging representations that capture local consistency and motif structure. Deletion and insertion corruptions remove or add tokens at random positions, teaching models about position-invariant features that remain identifiable despite surrounding changes. For genomics, insertions and deletions are biologically realistic mutation types (indels account for approximately 15% of pathogenic variants in ClinVar (Landrum et al. 2018)), and models that handle them during pretraining may better predict their effects downstream.\n\n\n8.4.2 Biologically Motivated Corruption\nThe most effective corruption strategies mirror actual sources of noise in clinical genomics data. Simulating sequencing errors provides corruption strategies that match experimental reality. Base miscalls follow platform-specific patterns: Illumina sequencing shows characteristic substitution biases (favoring certain nucleotide transitions over transversions, with error rates of 0.1-1% depending on read position and quality score), while nanopore sequencing exhibits distinct error profiles concentrated in homopolymer regions where the signal for consecutive identical bases becomes ambiguous [Citation Needed]. Training with corruptions that mimic these error patterns may improve generalization to real sequencing data with platform-specific artifacts. The sequencing technologies producing these error patterns are examined in Chapter 1, while the confounding effects of platform-specific artifacts on model evaluation appear in Chapter 12.\nVariant augmentation introduces biologically realistic sequence changes based on population variation. Randomly substituting alleles at known polymorphic sites or injecting variants from databases like gnomAD creates corrupted sequences reflecting natural genetic diversity (Karczewski et al. 2020). This teaches models that common polymorphisms are normal variation rather than errors to be corrected, potentially improving robustness for variant effect prediction where distinguishing pathogenic variants from benign polymorphisms is the central challenge. A model trained only on reference sequence might flag any deviation as potentially damaging; a model trained with variant augmentation learns which deviations are within normal population variation.\nStructural variation simulation models larger-scale genomic changes: tandem duplications, copy number variation, and segmental rearrangements. These corruptions are harder to implement but capture realistic sources of genomic diversity beyond single-nucleotide changes. Models trained with structural variation corruptions may better understand how gene dosage changes, enhancer duplications, or domain boundary disruptions affect function. For clinical applications involving copy number variants (which underlie conditions ranging from developmental disorders like DiGeorge syndrome to cancer predisposition in hereditary breast cancer), this training signal could improve predictive accuracy.\nThe benefit of denoising pretraining extends to robustness under distribution shift. If downstream applications involve sequences from different populations, experimental platforms, or tissue contexts than the pretraining corpus, models pretrained with appropriate corruptions can maintain performance despite distribution mismatch. This matters in clinical genomics, where validation cohorts often differ from discovery cohorts in ancestry composition, sequencing technology, or phenotyping protocols. A model trained with corruptions spanning these sources of variation generalizes more reliably than one trained only on pristine reference sequence. The confounding and evaluation challenges arising from such distribution shifts are examined in Chapter 12 and Chapter 12.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-contrastive",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-contrastive",
    "title": "8  Pretraining Strategies",
    "section": "8.5 Contrastive Learning",
    "text": "8.5 Contrastive Learning\nCross-population generalization presents a persistent challenge in clinical genomics. A variant classifier trained on European ancestry cohorts may perform poorly on African ancestry patients due to different patterns of linkage disequilibrium and background polymorphism. The classifier learned to recognize pathogenic variants against a European genetic background; African genomes present the same functional variants but surrounded by different neutral polymorphisms. Contrastive learning addresses this by teaching models to recognize functional equivalence despite sequence-level differences, producing representations where a regulatory element is recognizable regardless of the population-specific variants surrounding it.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading further, consider: If you wanted to train a model that recognizes a CTCF binding site as functionally equivalent whether it appears in a European or African genome, what kind of training pairs would you construct? What should be “similar” and what should be “different”?\n\n\nContrastive learning takes a fundamentally different approach to self-supervised pretraining than reconstruction-based objectives. Rather than recovering corrupted inputs, contrastive objectives train models to produce similar representations for different views of the same sequence while distinguishing them from representations of unrelated sequences. The intuition is that augmented versions of a sequence (with minor corruptions, reverse complementation, or variants) should map to nearby points in representation space, while unrelated sequences should map to distant points. This teaches invariance to transformations that do not change function.\nThe algorithmic framework constructs positive pairs and negative samples. For a given anchor sequence, positive pairs are created through augmentation: reverse complementation, random cropping, variant injection, or other transformations that preserve functional identity. Negative samples are drawn from other sequences in the training batch. The model produces embeddings for all sequences, and the contrastive loss encourages anchor and positive embeddings to be similar (high cosine similarity) while pushing apart anchor and negative embeddings.\nInfoNCE loss is the most common contrastive objective (Oord, Li, and Vinyals 2019). The intuition is like a matching game: given a photo of a person (the anchor), you must identify which of several voice recordings belongs to the same person (the positive) among many strangers’ voices (the negatives). The loss function rewards correctly matching anchor to positive while distinguishing them from negatives. For an anchor embedding \\(z_i\\) and positive embedding \\(z_i^{+}\\), InfoNCE maximizes:\n\\[\n\\mathcal{L} = -\\log \\frac{\\exp\\!\\left(z_i \\cdot z_i^{+} / \\tau\\right)}{\\sum_{j} \\exp\\!\\left(z_i \\cdot z_{j} / \\tau\\right)}\n\\]\nwhere the sum runs over the positive and all negative samples, and \\(\\tau\\) is a temperature parameter controlling the sharpness of the distribution. Why take this particular mathematical form? The objective frames contrastive learning as a classification problem: given the anchor, identify which of the many candidates is the true positive pair. The softmax structure ensures the model can only increase the score for the positive pair by simultaneously decreasing scores for negatives, forcing it to learn discriminative features rather than simply inflating all similarity scores. The temperature \\(\\tau\\) controls how harshly the model is penalized for near-misses. Lower temperatures make the model more discriminative, requiring cleaner separation between positives and negatives; a low temperature means that even small differences in similarity are amplified into large differences in the loss. The objective is equivalent to classifying the positive pair among all possible pairs, and the model learns representations that make this classification easy.\n\n8.5.1 Augmentation Design for Genomic Sequences\nIf you pair a sequence with its reverse complement and call them “similar,” the model learns strand symmetry. If you pair a sequence with itself plus a common SNP and call them “similar,” the model learns robustness to population variation. The augmentations you choose define what “similarity” means, and therefore what invariances your model acquires. Choosing the wrong augmentations teaches the model to ignore differences that actually matter.\nA CTCF binding site must be recognizable whether it appears on a European or African genetic background, whether read on the forward or reverse strand, and whether the surrounding sequence contains common polymorphisms or reference alleles. Augmentation design is critical for contrastive learning because augmentations must preserve functional identity while introducing variability. If augmentations change function, the contrastive objective will learn meaningless invariances. Several augmentation strategies are biologically grounded and preserve the functional relationships that matter for downstream clinical applications.\nThe double-stranded nature of DNA provides the simplest and most reliable augmentation. Many regulatory elements function identically on either strand, and a model that fails to recognize this symmetry has learned an incomplete representation of genomic function. Reverse complementation trains the model to treat forward and reverse complement sequences as equivalent, capturing strand symmetry inherent in molecular biology. This augmentation is universally applicable and introduces no risk of changing functional identity; a TATA box is a TATA box regardless of which strand is reported.\nPosition invariance presents a subtler challenge. A transcription factor binding site should be recognizable regardless of where it falls within an input window, yet models naturally learn position-specific features. Random cropping addresses this by extracting overlapping windows from longer sequences. If a binding site appears in multiple cropped windows at different positions, the model learns that the site itself is the functionally relevant feature, not its coordinates. This proves particularly useful for tasks where genomic location matters less than local sequence content. The augmentation also provides practical benefits: a single long sequence becomes many training examples, increasing effective data diversity without collecting new data.\nPopulation diversity creates perhaps the most clinically consequential augmentation challenge. A classifier trained only on reference sequence may treat any deviation as potentially significant, when in fact most human genetic variation is neutral. Variant injection addresses this by introducing common polymorphisms or simulated mutations as augmentation. If the variants are neutral (common variants from gnomAD with high allele frequency, which are unlikely to be damaging; see Chapter 2 for gnomAD resource details), treating variant and reference sequences as positive pairs teaches robustness to genetic background. This is particularly valuable for cross-population generalization, where models must recognize functional elements despite surrounding sequence polymorphism that differs between ancestry groups. A model trained with variant augmentation learns that a CTCF binding site is functionally equivalent whether it appears on European or African genetic background.\n\n\n\nTable 8.3: Augmentation strategies for genomic contrastive learning.\n\n\n\n\n\n\n\n\n\n\n\nAugmentation Type\nHow It Works\nInvariance Taught\nClinical Relevance\n\n\n\n\nReverse complement\nSwap strand, reverse sequence\nStrand symmetry\nTF binding orientation-agnostic\n\n\nRandom cropping\nExtract overlapping windows\nPosition invariance\nMotif recognition anywhere in window\n\n\nVariant injection\nInsert common polymorphisms\nPopulation robustness\nCross-ancestry generalization\n\n\nNucleotide substitution\nRandom base changes\nNoise tolerance\nSequencing error robustness\n\n\nOrtholog pairing\nPair sequences across species\nSpecies invariance\nModel organism to human transfer\n\n\n\n\n\n\nThe choice of negative samples shapes what distinctions the model learns to make. Random genomic sequences provide straightforward negatives but may be too easy to distinguish: any functional regulatory sequence is readily separable from random intergenic sequence. Harder negatives force more informative learning. Sequences from paralogous genes share evolutionary history but have diverged in function; distinguishing them requires learning subtle functional signatures. Pseudogenes resemble their functional counterparts but lack activity; recognizing this difference teaches the model what makes a gene functional. Orthologous regions in distant species test whether the model has learned species-invariant features. The difficulty of negatives should match the granularity of distinctions required for downstream tasks.\n\n\n8.5.2 Cross-Species Contrastive Learning\n\n\n\n\n\n\nCross-species contrastive learning for species-invariant representations\n\n\n\n\nFigure 8.4: Cross-species contrastive learning for species-invariant representations. Orthologous sequences from human and mouse share functional identity despite 75 million years of divergence and substantial nucleotide differences (shown in alignment). Treating orthologs as positive pairs teaches the model to extract conserved functional features while ignoring species-specific sequence differences. Non-orthologous sequences serve as negatives. The resulting embedding space clusters orthologs together regardless of species, enabling transfer from model organism experiments to human predictions.\n\n\n\nLeveraging evolutionary relationships for self-supervision enables a distinctive form of contrastive learning. Orthologous sequences from different species share functional identity despite nucleotide divergence accumulated over millions of years of evolution. Treating orthologous pairs as positives and non-orthologous pairs as negatives teaches the model to extract species-invariant functional features. A human enhancer and its mouse ortholog should map to similar embeddings despite 75 million years of sequence divergence, while unrelated sequences should map to distant embeddings.\nThis approach has direct implications for drug development and therapeutic translation. Many drug targets are validated in mouse models before human clinical trials; roughly 95% of cancer drugs that succeed in mouse models fail in human trials, often because the models do not adequately capture human biology [Citation Needed]. A model pretrained with human-mouse contrastive pairs may generalize better to predicting drug response in humans based on mouse efficacy data, or to transferring regulatory circuit designs from model organisms to human cell types. The evolutionary record provides implicit labels about functional equivalence that would be expensive to obtain through direct experimental annotation.\nSequence embedding quality improves with contrastive pretraining in ways that benefit clinical applications. Models trained contrastively produce embedding spaces where functionally similar sequences cluster together, enabling nearest-neighbor search for annotating novel variants (finding similar characterized variants), sequence retrieval for identifying regulatory homologs, and unsupervised clustering of regulatory elements. For variant effect prediction, contrastive pretraining improves robustness: if the model learns that sequences differing only by neutral variants are functionally equivalent, it will better distinguish truly disruptive variants from benign polymorphisms.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-multitask",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-multitask",
    "title": "8  Pretraining Strategies",
    "section": "8.6 Multi-Task Pretraining",
    "text": "8.6 Multi-Task Pretraining\nPredicting variant pathogenicity requires integrating multiple lines of evidence: evolutionary conservation, protein structure effects, splicing changes, and regulatory disruption. A variant in TTN (the gene encoding titin, mutated in 25% of dilated cardiomyopathy cases [Citation Needed]) might be pathogenic because it disrupts protein folding, because it alters splicing, or because it affects regulatory binding sites. No single assay captures all these dimensions. Multi-task pretraining addresses this by jointly optimizing for diverse prediction tasks, learning representations that capture the multiple facets of genomic function relevant to clinical interpretation.\nMulti-task pretraining combines multiple related objectives during the same training run, jointly optimizing for several prediction tasks. Different tasks provide complementary supervision signals: masking captures local sequence patterns, chromatin prediction captures regulatory function, conservation scoring captures evolutionary constraint, and expression prediction captures transcriptional consequences. Representations that satisfy all tasks simultaneously develop richer and more general features than any single objective alone.\n\n8.6.1 Task Selection and Architecture\nA model trained to predict chromatin accessibility learns different features than one trained to predict gene expression, even from identical sequences. Add evolutionary conservation prediction, and the representations shift again. The tasks you choose for multi-task pretraining determine what biological signals your model captures, so how do you select the right combination?\nThe first design decision is which tasks to include. Ideally, tasks should be diverse enough to provide distinct supervision signals but related enough to benefit from shared representations. For genomic models, effective combinations include masked language modeling for general sequence structure, chromatin accessibility prediction for regulatory function, gene expression prediction for transcriptional output, evolutionary conservation scoring for functional constraint, and variant frequency prediction from population databases. Each task operates on the same input sequence but predicts different outputs using task-specific head layers. The shared backbone encoder processes the sequence into intermediate representations, and separate prediction heads map these representations to task-specific outputs.\n\n\n8.6.2 Loss Weighting and Balancing\nTraining a model on five tasks sounds straightforward until you discover that chromatin accessibility loss is ten times larger than expression prediction loss, causing the model to optimize almost exclusively for chromatin while ignoring expression. Worse, the expression task may actually be more important for your downstream clinical application. How you weight the contribution of each task to the total loss can make or break multi-task pretraining.\n\n\n\n\n\n\nTechnical Challenge\n\n\n\nOnce tasks are selected, their relative contributions to the total loss must be determined. This is a significant source of training instability and requires careful tuning.\n\n\nWith \\(\\mathcal{L}_1, \\ldots, \\mathcal{L}_K\\) representing individual task losses, the multi-task loss combines them:\n\\[\\mathcal{L}_{\\text{total}} = \\sum_{k=1}^K w_k \\mathcal{L}_k\\]\nwhere w_k are task weights. Why not simply sum losses with equal weights? Different tasks operate at fundamentally different scales. A regression task predicting gene expression might have losses in the range of 0.1-1.0, while a classification task predicting binary binding states might have losses ranging from 0.01-0.1 depending on class balance. Without weighting, the high-loss task dominates gradient updates, effectively ignoring the signal from other tasks. Even with similar loss scales, tasks differ in how quickly they are learned. If one task converges rapidly while another requires extended training, equal weighting means the converged task continues providing gradient signal that may interfere with learning the harder task. Dynamic weighting approaches address this by adjusting weights during training based on learning progress, using uncertainty estimation, gradient norms, or task-specific validation performance as signals for rebalancing. Uncertainty-based weighting learns task weights as parameters, treating high-loss tasks as inherently more uncertain and down-weighting their contribution. Gradient-based methods normalize gradients across tasks to prevent any single task from dominating updates.\n\n\n8.6.3 Large-Scale Multi-Task Examples\nEnformer exemplifies large-scale multi-task pretraining for genomics (Avsec et al. 2021). The model predicts over 5,000 genomic assays simultaneously: ChIP-seq signals for hundreds of transcription factors and histone marks, DNase-seq and ATAC-seq accessibility across cell types, CAGE transcription initiation profiles, and more. This massive multi-task objective (covering 674 DNase-seq, 4,675 ChIP-seq, and 638 CAGE experiments from ENCODE and Roadmap Epigenomics (Kagda et al. 2025)) forces the model to learn representations capturing diverse regulatory signals.\nThe task diversity in Enformer provides supervision far richer than any single assay. A model trained only on DNase-seq learns general accessibility patterns but misses transcription factor specificity: it cannot distinguish which factors bind to accessible regions. A model trained only on H3K27ac ChIP-seq captures active enhancers but misses repressive marks that indicate silenced regulatory elements. Training on all assays jointly allows the model to disentangle overlapping and complementary signals, learning representations that generalize across regulatory contexts. For clinical variant interpretation, this means Enformer can predict how a regulatory variant affects enhancer activity, chromatin state, transcription factor binding, and gene expression simultaneously. Chapter 16 examines Enformer and related regulatory models in detail.\n\n\n\n\n\n\nMulti-task pretraining architecture for comprehensive regulatory prediction\n\n\n\n\nFigure 8.5: Multi-task pretraining architecture for comprehensive regulatory prediction. A shared encoder backbone (convolutional layers for local patterns, transformer layers for long-range integration) processes input sequences. Multiple prediction heads branch from shared representations to predict diverse genomic readouts: chromatin accessibility (DNase-seq, ATAC-seq), histone modifications (H3K27ac, H3K4me3, H3K27me3), transcription factor binding for hundreds of factors, and gene expression via CAGE. Enformer jointly predicts over 5,000 tracks (674 DNase + 4,675 ChIP-seq + 638 CAGE), forcing shared representations to capture diverse regulatory signals. This multi-task pressure produces representations that generalize beyond any single assay.\n\n\n\nBorzoi extends this paradigm to full RNA-seq coverage prediction, jointly modeling transcription initiation, splicing, and transcript abundance (Linder et al. 2025). By predicting continuous coverage across gene bodies rather than just expression levels, Borzoi captures splicing patterns that are invisible to models predicting only total expression. This has direct clinical relevance: many pathogenic variants act through splicing disruption rather than protein-coding changes, and models that capture splicing patterns can identify variants that traditional expression-based approaches miss.\nCombining MLM with functional prediction represents another multi-task configuration. The model predicts masked tokens through a language modeling head while simultaneously predicting chromatin accessibility or other functional readouts through regression heads. This hybrid objective balances sequence-level pretraining with functional supervision. The MLM component ensures the model learns general sequence patterns even in regions without functional annotations (the majority of the genome lacks chromatin or expression measurements in any given cell type), while the functional prediction component focuses learning on biologically relevant features.\n\n\n8.6.4 When Multi-Task Learning Fails\nMore tasks should mean more learning signal, so why does adding a third task sometimes make performance on the original two tasks worse? Multi-task learning can fail in surprising ways, and understanding these failure modes is essential before committing computational resources to joint training.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider two tasks: (1) predicting splice site usage from a 20bp window, and (2) predicting enhancer-promoter interactions from a 200kb window. Why might jointly training these tasks hurt performance on both compared to training them separately? What property of representations might create conflict?\n\n\nTask interference presents the primary concern with multi-task learning. If tasks require conflicting representations, jointly optimizing for both may compromise performance on each compared to single-task baselines. In genomics, this might occur if one task benefits from very local features (splice site prediction, which depends on short consensus sequences spanning roughly 10 base pairs) while another requires long-range context (enhancer activity prediction, which depends on distant promoter interactions spanning 100 kilobases). The shared backbone must compromise, potentially learning suboptimal representations for both.\nNegative transfer occurs when adding a task actually hurts downstream performance compared to training without it. This can happen if the additional task introduces noise (poorly measured assays with high experimental variance), if task weights are poorly balanced (causing one task to dominate gradients), or if the auxiliary task shifts learned representations away from features useful for target applications. The risk of negative transfer increases with task diversity: distantly related tasks are more likely to require conflicting representations.\nThe benefits of multi-task pretraining are largest when tasks are complementary and data for individual tasks is limited. If chromatin data is sparse for a particular cell type but gene expression data is abundant, jointly training on both may improve performance on both compared to single-task models. The shared representations allow information to flow between tasks, compensating for data scarcity in any single modality. When functional labels exist at scale and tasks are genuinely related, multi-task pretraining consistently outperforms single-task alternatives.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-staged",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-staged",
    "title": "8  Pretraining Strategies",
    "section": "8.7 Staged Pretraining Strategies",
    "text": "8.7 Staged Pretraining Strategies\nTraining a foundation model in a single phase rarely produces optimal results. The computational constraints that make pretraining expensive also make experimentation prohibitive: once committed to training at scale, practitioners cannot easily adjust hyperparameters, data mixtures, or objectives mid-run. Staged pretraining addresses this by decomposing training into sequential phases, each optimized for different learning goals. A model might learn basic sequence statistics from shorter contexts before extending to long-range dependencies, or acquire general sequence grammar before specializing to regulatory regions. These staged approaches improve both training stability and final model quality compared to monolithic training on the full data and context from the start.\nThe biological rationale mirrors curriculum learning in human education: master fundamentals before tackling advanced material. A medical student learns anatomy before pathology; a genomic model might learn local motif structure before enhancer-promoter interactions spanning 100 kilobases. When HyenaDNA attempted direct training on million-base contexts, optimization diverged. Progressive context extension, starting at shorter windows and gradually increasing, proved essential for stable learning (Nguyen et al. 2023). This curriculum effect appears across architectures: the structure of what is learned first shapes what can be learned later.\n\n8.7.1 Context Length Curricula\nAn enhancer 50 kilobases from its target promoter can only be modeled if the context window spans that entire distance. But training directly on 100-kilobase sequences often fails: the optimization diverges, the model never converges, and weeks of compute are wasted. How do you build a model that understands long-range regulatory interactions when training on long sequences is so unstable?\nLong-range genomic dependencies present a fundamental training challenge. Attention mechanisms scale quadratically with context length, making training on long sequences orders of magnitude more expensive than short sequences. Beyond computational cost, optimization dynamics change with context length: models processing thousands of tokens face different gradient distributions than those processing hundreds. Context length curricula address both challenges by training first on tractable short contexts, then progressively extending to longer sequences.\nHyenaDNA exemplifies this approach. Initial pretraining used contexts of a few thousand bases, allowing rapid iteration through the genome and stable optimization. As training progressed, context windows expanded through intermediate stages (8 kilobases, then 32 kilobases) until reaching the target of one million bases. Each stage inherited weights from the previous stage, with learning rate warmup to accommodate the new context regime. The curriculum proved necessary for convergence: ablations attempting direct long-context training without warmup phases showed instability and degraded final performance.\nGene42 extended this pattern with explicit continuous pretraining stages (gene42_2024?). The model trained initially at 4,096 tokens, then continued pretraining at 8,192, 16,384, 32,768, 65,536, and finally 192,000 tokens. Each context extension required adjustments to positional encodings (specifically, modifications to rotary position embedding parameters to prevent distant-token interactions from collapsing). The staged approach enabled dense attention at scales where training from scratch would be computationally prohibitive. Notably, the longest-context checkpoints required only incremental compute beyond the shorter-context stages, amortizing the total training cost across the curriculum.\n\n\n\n\n\n\nContext length curriculum for stable long-range pretraining\n\n\n\n\nFigure 8.6: Context length curriculum for stable long-range pretraining. Training begins at short contexts (1-4 kb) where optimization is stable and local patterns are learned efficiently. At each stage transition, weights are inherited from the previous checkpoint with learning rate warmup to accommodate the new context regime. Progressive extension through intermediate stages (16 kb, 64 kb) enables the model to learn medium-range dependencies before tackling full long-range contexts. This curriculum proved essential for HyenaDNA: direct training at million-base contexts without warmup led to divergence. The inset shows how attention patterns become sparser at longer contexts, requiring more training steps to develop the structured patterns that capture distant regulatory relationships.\n\n\n\nThe mechanism underlying context curricula relates to how attention patterns develop. Early in training, attention distributions are nearly uniform (each position attends similarly to all others). As learning progresses, sparse, structured attention patterns emerge: promoter positions attend to enhancer regions; splice site positions attend to branch points. These structured patterns require many training steps to develop. Starting at long contexts forces the model to learn both basic sequence statistics and long-range structure simultaneously, competing objectives that can interfere. The curriculum separates these learning phases: master local patterns first (at short context), then learn to integrate them across distance (at extended context).\n\n\n8.7.2 Domain-Adaptive Pretraining\nYou have a pretrained model on human DNA and want to apply it to bacterial genomes. Do you continue training from the human checkpoint, hoping to preserve useful sequence patterns while adapting to different GC content and codon usage? Or do you start fresh, reasoning that bacterial genomes are different enough that the human pretraining provides no benefit and may even hurt? This decision can save months of compute or waste it entirely.\nWhen should a genomic model build on existing pretrained weights versus train from scratch? The question parallels a broader tension in NLP, where domain-adaptive pretraining (continuing training from a general-domain checkpoint on domain-specific data) competes with from-scratch domain pretraining (training exclusively on domain-specific data). The answer depends on data abundance, domain distance, and whether vocabulary transfer is feasible.\nBioBERT pioneered domain-adaptive pretraining for biomedical text, initializing from general-domain BERT weights and continuing pretraining on PubMed abstracts (lee_biobert_2020?). This approach leverages general language understanding (syntax, semantics, common knowledge) acquired during the initial pretraining phase, requiring only adaptation to domain-specific vocabulary and concepts. The strategy proved effective when biomedical data was limited and general-domain pretraining captured useful structure.\nPubMedBERT challenged this assumption by demonstrating that from-scratch pretraining on biomedical text alone could outperform domain-adaptive approaches when sufficient domain data exists (gu_pubmedbert_2021?). The key insight was vocabulary mismatch: general-domain tokenizers fragment biomedical terms into meaningless subwords (“lymphoma” becomes “l”, “##ym”, “##ph”, “##oma”), forcing the model to reconstruct meaning from pieces rather than representing concepts directly. Training from scratch with a domain-specific vocabulary eliminated this overhead. When domain-specific data is abundant (as with PubMed’s millions of abstracts), the benefits of general-domain initialization may not justify the vocabulary mismatch cost.\nFor genomic foundation models, these lessons translate directly. DNA sequence tokenizers (k-mers, BPE, single nucleotides) differ fundamentally from text tokenizers, making vocabulary transfer impossible. A general-purpose language model cannot serve as initialization for a DNA model; sequence statistics must be learned from genomic data. The relevant decision becomes whether to continue pretraining a genomic model on new data (adding species, adding functional annotations) or train a new model from scratch.\n\n\n8.7.3 Continued Pretraining on Expanded Data\nAs new genomic data becomes available (additional reference genomes, expanded population sequencing, new functional assays), practitioners face a choice: retrain from scratch incorporating all data, or continue pretraining the existing model on new data. Continued pretraining offers computational efficiency but risks catastrophic forgetting, where learning new patterns overwrites previously acquired knowledge.\nThe risk of catastrophic forgetting is real but manageable. When DNABERT checkpoints are continued on new species, performance on original species may degrade if the new training distribution differs substantially. Mitigation strategies include replay (mixing old and new data during continued pretraining), elastic weight consolidation (penalizing changes to weights important for prior tasks), and modular architectures that isolate new learning from established representations (McCloskey and Cohen 1989).\n\n\n\n\n\n\nPractical Guidance\n\n\n\nWhen to continue pretraining vs. train from scratch:\n\nContinue pretraining when: New data is similar to original training data; you want to add species within the same kingdom; computational budget is limited; you need to preserve performance on original tasks\nTrain from scratch when: New tokenization scheme is needed; target domain differs fundamentally (e.g., viral genomes after training on mammals); original model shows systematic biases you want to eliminate\nHybrid approach: Use replay buffers mixing old and new data to balance adaptation with retention\n\n\n\nContinued pretraining makes sense when new data complements rather than contradicts prior training. Adding closely related species to a model pretrained on mammals will likely transfer well; adding bacterial genomes with fundamentally different GC content, codon usage, and regulatory logic may require more careful integration or separate models. The decision should be guided by biological similarity between old and new data distributions.\n\n\n8.7.4 Multi-Objective Schedules\nBeyond data and context curricula, the pretraining objective itself can be staged. A model might train with masked language modeling to learn sequence statistics, then switch to or add contrastive objectives to learn functional similarity, then incorporate task-specific prediction heads. Each objective teaches different aspects of sequence function.\nDNABERT-S demonstrates staged objective curricula (zhou_dnabert-s_2024?). The model’s Curriculum Contrastive Learning (C²LR) strategy divides training into two phases. Phase I applies standard contrastive learning (distinguishing similar from dissimilar sequences using straightforward positive and negative pairs). Phase II introduces harder anchors through Manifold Instance Mixup, creating challenging training examples by mixing hidden representations at random layers. The curriculum ensures the model first masters basic discrimination before tackling the more difficult mixed-representation task.\nMulti-task schedules represent another form of objective staging. Rather than training all tasks jointly from the start, some practitioners introduce tasks sequentially: begin with the primary self-supervised objective, then add auxiliary tasks once representations have stabilized. This staging prevents auxiliary tasks from dominating early learning when the model has not yet acquired basic sequence understanding. The optimal schedule depends on task interactions: complementary tasks (MLM plus chromatin prediction) may benefit from joint training, while potentially conflicting tasks (short-range splice prediction plus long-range enhancer prediction) may benefit from staging.\n\n\n8.7.5 Data Complexity Curricula\nShould the model see simple repetitive sequences first and complex regulatory regions later, or dive straight into the hardest examples? The order in which training data is presented affects what the model learns and how efficiently it learns it. Presenting complex enhancer grammar before the model has mastered basic motif recognition may waste training steps on examples the model cannot yet learn from.\nNot all genomic sequences present equal learning difficulty. Repetitive regions offer little to learn beyond detecting repeats; complex regulatory regions require learning combinatorial motif grammar; intergenic regions provide evolutionary constraint signal distinct from coding regions. Data complexity curricula order training examples from simple to complex, allowing models to build representations progressively.\nComplexity ordering can be implicit or explicit. Implicit ordering emerges from data sampling: if training oversamples certain regions early (promoters, conserved sequences), the model learns those patterns first. Explicit ordering requires defining complexity metrics (sequence entropy, motif density, evolutionary conservation, expression variability) and scheduling examples accordingly. While less explored in genomics than context curricula, data complexity scheduling offers potential for improving sample efficiency, particularly when some sequence classes are over-represented in training corpora.\n\n\n8.7.6 Practical Considerations\nStaged pretraining introduces complexity that must be weighed against benefits. Each stage requires decisions about duration (training steps or epochs), transition criteria (loss plateaus, validation metrics), learning rate schedules (warmup for each stage, decay patterns), and checkpoint selection (which intermediate checkpoint to continue from). Poor choices at stage transitions can negate the benefits of staging.\nDiagnostic monitoring becomes more important with staged training. Track not only aggregate loss but per-stage metrics: does performance on short-context tasks degrade when extending to long contexts? Do earlier-stage representations remain useful? Does adding new data cause forgetting of prior patterns? These diagnostics require evaluation infrastructure beyond simple loss tracking but provide essential feedback for curriculum design.\nThe computational tradeoffs favor staging in most scenarios. Training a single long-context model from scratch requires expensive long-sequence batches for the entire training run. Staged training front-loads cheap short-context training, investing in expensive long-context training only after the model has learned basic patterns. The total compute may be similar or even higher with staging, but the amortized cost per useful representation is often lower because more learning happens during the efficient early stages.\nWhen staged pretraining fails, the causes typically involve poor stage transitions or misaligned curricula. If later stages require unlearning early-stage representations (because the curriculum taught the wrong patterns first), staging may harm rather than help. Careful alignment between curriculum structure and intended final capabilities remains essential. The goal is not staging for its own sake but decomposing a difficult learning problem into tractable sequential subproblems.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-data",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-data",
    "title": "8  Pretraining Strategies",
    "section": "8.8 Data Strategies for Pretraining",
    "text": "8.8 Data Strategies for Pretraining\nCorpus construction establishes the foundation for pretraining and determines what patterns the model can learn. A clinical variant classifier is only as good as the evolutionary and population diversity captured in its pretraining corpus. If the training data underrepresents African genetic variation (African populations harbor more genetic diversity than all other continental populations combined, yet constitute a small fraction of most reference panels [Citation Needed]), the resulting model will underperform on African ancestry patients. These data decisions have direct consequences for health equity and clinical utility.\n\n8.8.1 Reference Genomes and Population Diversity\nHuman genome assemblies like GRCh38 provide the standard starting point, offering high-quality, contiguous sequence spanning all chromosomes (roughly 3.1 billion base pairs of assembled sequence, representing about 92% of the full genome before telomere-to-telomere completion [Citation Needed]). Training on the reference genome allows models to learn patterns characteristic of human DNA: base composition, repeat structure, gene organization, and regulatory architecture. The reference genome represents a single haploid consensus, missing variation present in human populations, but provides the foundation for most pretraining approaches.\nPopulation-scale variation can be incorporated through variant databases. Rather than training only on reference sequence, injecting variants at observed population frequencies creates synthetic diploid genomes reflecting real genetic diversity. This teaches models that common polymorphisms are normal variation, potentially improving robustness and variant effect prediction. gnomAD provides allele frequencies across over 800,000 individuals spanning diverse ancestries, enabling population-aware training. Pan-genome approaches extend this by representing multiple high-quality assemblies from diverse individuals, capturing structural variation and population-specific haplotypes that a single reference cannot represent (Karczewski et al. 2020). Chapter 2 examines these data resources and their construction in detail.\n\n\n8.8.2 Repeat Handling\nHalf the human genome consists of repetitive sequences, but should your model spend half its training capacity learning to recognize LINE elements and Alu repeats? The answer depends entirely on your downstream task. For variant interpretation in coding regions, repeats are noise; for studying repeat expansion disorders like Huntington disease, they are the signal. How you handle repeats during pretraining shapes what your model can and cannot do.\nRepeat handling impacts pretraining in ways that depend on downstream applications. Simple repeats, tandem repeats, and transposable elements occupy roughly half of the human genome but contribute less directly to protein-coding function than unique sequences [Citation Needed]. Hard-masking repeats (replacing repetitive bases with N characters, rendering ATCGATCGATCG as NNNNNNNNNNNN) reduces training data but may discard information relevant to some tasks; many regulatory elements derive from transposable elements, and some disease-associated repeats (like the CGG expansion in FMR1 causing Fragile X syndrome, or the CAG expansion in HTT causing Huntington disease) are clinically important. Soft-masking retains sequence information while using lowercase to flag repetitive regions (atcgatcgatcg), allowing models to learn differential representations for repeats and unique sequences. Tools like RepeatMasker produce these annotations, and training pipelines can be configured to treat masked regions differently: exclude them entirely, downweight their contribution to loss, or process them normally while preserving the distinction in tokenization.\n\n\n8.8.3 Multi-Species and Augmentation Strategies\nIncorporating genomes from model organisms and related species enables models to learn evolutionary conservation patterns and may improve transfer between species. Including mouse, zebrafish, and other commonly used experimental organisms provides training signal about which sequence features are functionally constrained across evolution. For therapeutic development that relies on animal model data, multi-species pretraining provides the foundation for cross-species generalization.\nData augmentation strategies (see Section 8.5.1) complement multi-species training by artificially increasing diversity within species. These augmentations are typically applied on-the-fly during training rather than pre-computed, maintaining flexibility in the training pipeline and ensuring the model sees different augmented versions across epochs.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-optimization",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-optimization",
    "title": "8  Pretraining Strategies",
    "section": "8.9 Optimization and Scaling",
    "text": "8.9 Optimization and Scaling\nTraining a model to predict variant effects in genes like BRCA1 requires not just the right objective but also stable optimization that converges to useful representations. A model that diverges during training or gets stuck in poor local minima will fail clinically regardless of how well-designed its architecture may be. The optimization details that seem merely technical have direct consequences for whether the final model can reliably distinguish pathogenic from benign variants.\n\n8.9.1 Optimization Hyperparameters\nA learning rate that is too high causes loss to spike and never recover; a learning rate that is too low means the model crawls toward convergence over months instead of weeks. Gradient explosions can corrupt a week of training in a single batch. These optimization details seem purely technical until they determine whether your model learns useful representations or produces garbage.\nStable training requires careful attention to learning rate scheduling, gradient management, and numerical precision. Learning rate warmup gradually increases the learning rate from near-zero over the first several thousand steps, preventing early training instability when the model has random initializations and large gradient variance. After warmup, cosine decay schedules reduce the learning rate following a cosine curve from peak to near-zero over training, providing aggressive learning early when gradients are most informative and gentle refinement late as the model approaches convergence.\nGradient clipping (see Section 7.6.3) uses a norm threshold of 1.0 in most genomic pretraining configurations. Without clipping, a single anomalous batch can destabilize training irreversibly.\nModern pretraining relies on mixed precision arithmetic (float16 or bfloat16 instead of float32) to reduce memory consumption and accelerate computation on modern GPUs. Loss scaling prevents numerical underflow in float16, and careful handling of gradient updates ensures stability. Mixed precision is now standard for large-scale pretraining, roughly doubling throughput with minimal impact on model quality.\n\n\n8.9.2 Scaling Laws and Emergence\n\n\n\n\n\n\nAdvanced Topic\n\n\n\nThis section discusses scaling laws and emergent capabilities that remain active research areas. The quantitative relationships described here are empirically observed but may not generalize to all architectures or domains.\n\n\nPretraining scales with model size, sequence length, and dataset size in predictable ways that have profound implications for what models can learn. Larger models with more parameters capture more complex patterns but require more data and compute to train. ESM-2’s largest variant has 15 billion parameters (Lin et al. 2022) (roughly one parameter for every two amino acids in its training corpus), enabling it to capture subtle evolutionary constraints invisible to smaller models. Longer sequence contexts enable learning of long-range dependencies but increase memory requirements quadratically for standard attention. More diverse training data improves generalization but requires proportionally more training time.\nThe relationships between scale and capability follow power laws that predict optimal resource allocation (Hoffmann et al. 2022). For a fixed computational budget, there exists an optimal balance between model size and training data: models that are too large undertrain on available data, while models that are too small cannot capture the complexity present in abundant data. These scaling laws, first characterized systematically for language models (Kaplan et al. 2020), appear to hold for genomic foundation models as well, though the precise exponents and constants differ. Understanding these relationships guides decisions about when to scale up versus when to improve data quality or model architecture. Chapter 13 examines these scaling relationships in detail, formalizing the observations introduced here into quantitative laws that define the foundation model paradigm.\nThese empirical scaling laws contradict classical intuitions from statistical learning theory. A helpful analogy: traditional theory says that a student who memorizes 100 flashcards cannot pass a 1000-question exam—there simply are not enough examples to learn general rules. The Vapnik-Chervonenkis framework formalizes this intuition, predicting generalization error scaling as \\(O(1/\\sqrt{N})\\): halving the error requires quadrupling the training data, with model complexity strictly bounded by available examples (vapnik_statistical_1998?). Models with parameters vastly exceeding training examples should memorize rather than generalize—like a student with more flashcard slots than study examples. Yet foundation models operate precisely in this “overparameterized” regime and still improve predictably with scale, as if the student discovered underlying patterns that make even unseen questions answerable. This benign overfitting reflects properties of gradient descent and high-dimensional loss landscapes that classical worst-case bounds did not anticipate (zhang_understanding_2017?; belkin_reconciling_2019?).\nBeyond smooth improvements in loss, scale produces qualitative changes in model capabilities that were absent at smaller scales. Language models exhibit emergent behaviors (in-context learning, chain-of-thought reasoning, few-shot generalization) that appear only above certain parameter thresholds [Citation Needed]. Whether genomic models exhibit analogous emergent capabilities remains an active research question with early evidence suggesting they do. ESM-2, trained on evolutionary sequence databases containing hundreds of millions of protein sequences from UniRef (Suzek et al. 2007), develops structural understanding of proteins despite receiving no explicit structural supervision: the three-dimensional contacts emerge from predicting amino acid sequences alone. Evo, trained autoregressively on genomes, learns to generate sequences with realistic gene structure and regulatory organization. These emergent properties cannot be predicted by extrapolating from smaller models, making them both scientifically interesting and practically difficult to anticipate.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-diagnostics",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-diagnostics",
    "title": "8  Pretraining Strategies",
    "section": "8.10 Training Diagnostics",
    "text": "8.10 Training Diagnostics\nA two-week pretraining run that begins diverging on day three but is not detected until day thirteen wastes ten days of compute and forces rollback to earlier checkpoints. The failure is not losing everything; it’s continuing to train a model that stopped learning useful representations long before anyone noticed. Early detection of training issues is essential for avoiding wasted computation and ensuring models achieve the representations necessary for clinical utility.\n\n8.10.1 Monitoring Loss and Gradients\nWhen loss suddenly spikes on day five of a two-week training run, is it a temporary anomaly that will self-correct, or the beginning of catastrophic divergence that will waste the remaining nine days? Knowing which metrics to watch, and what patterns signal trouble, lets you catch problems early and decide whether to continue, roll back, or restart with different hyperparameters.\nTraining loss curves should decrease smoothly in early stages, eventually plateauing as the model approaches convergence. Sudden spikes suggest numerical instability (often from learning rate issues or gradient explosion), inappropriate optimization hyperparameters, or corrupted data batches. Persistent plateaus may indicate insufficient model capacity, inappropriate objectives, or learning rates that prevent further improvement. Tracking loss on held-out validation data monitors generalization: if training loss decreases while validation loss increases, the model is overfitting to the training corpus.\nGradient norms indicate whether optimization is proceeding normally. Very small gradients suggest the vanishing gradient problem, preventing effective learning in early layers. Very large gradients suggest instability that gradient clipping should catch. Tracking per-layer gradient norms helps diagnose where problems originate in deep networks; if early layers show vanishing gradients while later layers have healthy magnitudes, the architecture may need residual connections or different initialization.\n\n\n8.10.2 Functional Probing\nLoss can decrease steadily for weeks while the model learns patterns useless for your downstream task. A model might become excellent at predicting repeat sequences (which dominate the genome) while learning nothing about regulatory elements (which matter for variant interpretation). Probing intermediate checkpoints on biologically meaningful tasks reveals whether learning is on track, regardless of what the loss curve shows.\nProbing tasks provide functional sanity checks during pretraining that loss curves alone cannot capture. Simple downstream evaluations (predicting known splice sites, identifying transcription factor binding motifs, distinguishing exons from introns) can be run periodically on intermediate checkpoints to verify that learned representations capture biologically meaningful patterns. If probing performance plateaus or degrades while pretraining loss continues improving, the model may be learning patterns that do not transfer to downstream tasks. This dissociation between pretraining loss and probe performance signals a problem with the pretraining objective or data that would otherwise go undetected until final evaluation.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-selection",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-selection",
    "title": "8  Pretraining Strategies",
    "section": "8.11 Strategy Selection",
    "text": "8.11 Strategy Selection\nA clinician asking “will this BRCA1 variant cause disease?” needs a model pretrained with objectives that capture protein function and evolutionary constraint. A synthetic biologist asking “can you design me a promoter with 10-fold higher expression?” needs generative capabilities that MLM does not provide. Selecting a pretraining approach involves matching computational investment to the clinical or research questions the model must ultimately answer.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nFor each scenario below, identify the most appropriate pretraining objective and justify your choice:\n\nBuilding a model to predict which regulatory variants disrupt transcription factor binding\nDesigning novel enzyme sequences for industrial biocatalysis\nCreating a variant classifier that works across diverse human populations\nPredicting splice-altering variants in rare disease patients\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nMLM - predicting binding disruption requires bidirectional context to assess how the variant affects the motif and flanking regions. (2) Autoregressive/next-token prediction - designing new sequences requires generation capabilities that sample coherent proteins respecting learned sequence grammar. (3) Contrastive learning - robustness across populations benefits from pretraining that learns invariance to genetic variation through contrastive objectives on variant pairs. (4) MLM - splice site prediction needs to assess how variants affect both donor and acceptor sites with full bidirectional context around the junction.\n\n\n\n\n\n\nFor most general-purpose DNA or protein models, MLM pretraining provides a strong default. It learns bidirectional context, scales efficiently, and transfers well to diverse downstream tasks. DNABERT and DNABERT-2 exemplify this approach for genomics, while ESM models demonstrate its effectiveness for proteins. Start with MLM unless there is a specific reason to prefer alternatives.\nNext-token prediction is preferred when generation is the primary goal. If designing sequences from scratch (therapeutic proteins, synthetic promoters, regulatory circuits), sampling from autoregressive models produces coherent outputs respecting learned grammar. Evo and similar models demonstrate this for genomic sequence generation. The autoregressive structure makes conditional generation straightforward, enabling design applications that MLM does not naturally support.\nMulti-task pretraining makes sense when functional labels are available at scale and tasks are complementary. Enformer’s success with thousands of chromatin assays demonstrates the power of multi-task learning when data supports it. The infrastructure requirements are higher (handling heterogeneous data, balancing losses across tasks, maintaining separate prediction heads), but the resulting representations capture functional information that pure sequence-based objectives miss.\nContrastive learning is valuable for cross-species applications or when robustness to variation is critical. If transferring models trained on model organisms to related species, or improving robustness to genetic polymorphism across human populations, contrastive pretraining on orthologous pairs or variant-augmented sequences provides targeted benefits.\n\n\n\n\n\n\nPractical Guidance: Decision Tree for Objective Selection\n\n\n\nStart here: What is your primary downstream task?\n\nVariant effect prediction / classification → MLM (needs bidirectional context)\nSequence generation / design → Autoregressive (needs sampling capability)\nCross-population / cross-species transfer → Contrastive (needs invariance)\nMultiple regulatory predictions → Multi-task (needs diverse functional features)\n\nSecondary considerations: - Limited compute? → Start with existing pretrained model, fine-tune - Need both understanding AND generation? → Consider hybrid (prefix LM) or ensemble - Clinical deployment with equity requirements? → Ensure training data includes diverse populations\n\n\nWhen deciding whether to pretrain from scratch or start from existing models, starting from pretrained checkpoints is almost always preferable if an appropriate model exists. Fine-tuning a DNABERT-2 checkpoint on a new task is faster and more data-efficient than training from scratch. Pretraining from scratch is necessary when using new tokenization schemes (incompatible vocabularies prevent weight transfer), targeting species without suitable existing models, or experimenting with fundamentally different architectures where pretrained weights cannot transfer. Chapter 9 examines these adaptation strategies in detail.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-case-studies",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-case-studies",
    "title": "8  Pretraining Strategies",
    "section": "8.12 Pretraining in Practice: Case Studies",
    "text": "8.12 Pretraining in Practice: Case Studies\nExamining how successful models were pretrained provides concrete lessons and design patterns that inform new projects. Each case study illustrates how architectural choices, data decisions, and optimization strategies combine to produce models with distinct capabilities.\n\n8.12.1 DNABERT\nDNABERT introduced MLM pretraining to genomics by adapting BERT’s architecture to DNA sequences with overlapping \\(k\\)-mer tokenization (Ji et al. 2021). The model was pretrained on the human genome with 6-mer tokens, masking 15% of tokens at random. Standard BERT hyperparameters proved effective: AdamW optimizer with warmup, dropout regularization, and layer normalization. The key lessons include the importance of tokenization choice (k-mers capture motif-level patterns better than single nucleotides for regulatory prediction), the value of reverse complement augmentation for strand symmetry, and the transferability of representations across tasks never seen during pretraining. The full DNABERT architecture and its subsequent developments (DNABERT-2, DNABERT-S) are examined in Chapter 14.\n\n\n8.12.2 HyenaDNA\nHyenaDNA demonstrated that efficient long-range architectures enable pretraining on extremely long contexts (Nguyen et al. 2023). By using Hyena layers with subquadratic complexity, HyenaDNA scaled to contexts spanning one million bases (compared to typical transformer limits of a few thousand bases), far beyond standard transformers. Pretraining used single-nucleotide next-token prediction with a curriculum that progressively increased context length from shorter windows to full million-base sequences. This curriculum learning proved essential: training directly on long contexts without warmup led to instability. The lessons include the feasibility of million-base contexts with appropriate architectures, the benefits of curriculum learning for context scaling, and the emergence of long-range regulatory patterns when models have sufficient receptive field.\n\n\n8.12.3 Enformer\nEnformer pioneered multi-task chromatin prediction at scale (Avsec et al. 2021). The model was pretrained jointly on over 5,000 assays from ENCODE, Roadmap Epigenomics, and related consortia, using a hybrid convolutional-transformer architecture with 200 kilobase context (spanning typical enhancer-promoter distances in mammalian genomes). Task weighting was balanced to prevent any single assay from dominating. Key insights include the power of large-scale multi-task learning for capturing diverse regulatory signals, the effectiveness of combining convolutions for local patterns with transformers for long-range interactions, and the interpretability benefits of attention patterns that reveal learned enhancer-promoter relationships. Chapter 16 examines Enformer’s architecture and regulatory predictions in detail.\n\n\n8.12.4 ESM-2\nESM-2 represents the state of the art for protein language models, scaling to 15 billion parameters trained on UniRef databases containing sequences from hundreds of millions of protein families (Lin et al. 2022). Pretraining used standard MLM on amino acid sequences at unprecedented scale. The lessons include the continued benefit of scaling (larger models and more data improve even at billions of parameters, with no plateau in sight), the value of evolutionary diversity (pretraining on distinct protein families captures constraints invisible in any single genome), and the emergence of structural understanding from sequence alone (ESM-2 representations encode three-dimensional contacts despite no explicit structural supervision during pretraining). Chapter 15 examines ESM-2 and related protein language models comprehensively.\n\n\n\nTable 8.4: Summary of pretraining strategies across major genomic foundation models.\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nObjective\nContext\nKey Innovation\nPrimary Application\n\n\n\n\nDNABERT\nMLM (15%)\n512 tokens\nk-mer tokenization\nRegulatory prediction\n\n\nDNABERT-2\nMLM\n512 tokens\nBPE tokenization\nMulti-species transfer\n\n\nHyenaDNA\nAutoregressive\n1M bases\nSubquadratic attention\nLong-range dependencies\n\n\nEvo\nAutoregressive\n100kb+\nStripedHyena layers\nGenome generation\n\n\nEnformer\nMulti-task regression\n200kb\n5,000+ chromatin tracks\nRegulatory variant effects\n\n\nESM-2\nMLM\nFull protein\n15B parameters\nProtein structure/function",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-open-questions",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-open-questions",
    "title": "8  Pretraining Strategies",
    "section": "8.13 Open Questions",
    "text": "8.13 Open Questions\nDespite rapid progress, fundamental questions about genomic pretraining remain open, and resolving them will determine whether the next generation of models can achieve clinical-grade reliability.\nOptimal objective combinations remain unclear: should we jointly train with MLM and chromatin prediction, or train sequentially? How many auxiliary tasks help before diminishing returns? Do contrastive and generative objectives complement each other or interfere? These questions have different answers for different downstream applications, and systematic characterization is incomplete.\nIncorporating biological priors versus learning from scratch presents a design tension. Known motifs, pathway structure, and evolutionary constraints could be encoded in model architecture or initialization. Hand-engineered features risk encoding false assumptions, but pure data-driven learning may rediscover basic biology inefficiently. Hybrid approaches combining priors with learned representations remain underexplored.\nContinual pretraining as new data arrives is increasingly relevant. As sequencing technologies improve and new assays emerge, updating pretrained models without catastrophic forgetting of prior knowledge presents challenges. Online learning and elastic weight consolidation are potential solutions that remain largely untested in genomics at scale.\nThe relationship between pretraining scale and downstream performance follows predictable patterns that are still being characterized for genomic models. Understanding these relationships more precisely would guide resource allocation and set realistic expectations for what different scales of pretraining can achieve. These scaling considerations connect to the broader foundation model paradigm examined in Chapter 13.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch08-pretraining.html#sec-ch08-sequence-to-knowledge",
    "href": "part_2/p2-ch08-pretraining.html#sec-ch08-sequence-to-knowledge",
    "title": "8  Pretraining Strategies",
    "section": "8.14 From Sequence Statistics to Biological Knowledge",
    "text": "8.14 From Sequence Statistics to Biological Knowledge\nThe fundamental insight underlying self-supervised pretraining is that patterns relevant to biological function are embedded in sequence statistics themselves. A model that learns to predict masked nucleotides must implicitly capture the evolutionary constraints, regulatory grammar, and structural requirements that determine what sequences are viable. A model that learns to generate plausible protein sequences must internalize the constraints that distinguish functional proteins from random polymers. These objectives extract biological knowledge from sequence without requiring explicit functional labels, transforming abundant unlabeled data into learned representations that improve data efficiency for downstream applications.\nThe choice of pretraining objective shapes what models learn in ways that propagate to clinical utility. Masked language modeling teaches bidirectional sequence understanding, making it the natural choice for variant interpretation and regulatory prediction where full flanking context informs the prediction. Next-token prediction teaches generative capabilities essential for therapeutic protein design and synthetic sequence generation. Contrastive learning teaches invariance to perturbations, building robustness that transfers across species and populations. Aligning pretraining objectives with intended applications improves transfer; misalignment creates representational gaps that fine-tuning may struggle to bridge.\nSelf-supervised pretraining has become the default approach for building genomic foundation models. The DNA language models in Chapter 14, protein language models in Chapter 15, and regulatory sequence models in Chapter 16 each employ variants of these objectives tailored to their sequence modalities and downstream applications. The transfer learning methods examined in Chapter 9 determine how effectively pretrained representations can be adapted to specific clinical and research tasks, completing the pipeline from raw sequence through learned representation to deployed application.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nHow does masked language modeling encourage bidirectional context integration, and why does this make MLM-pretrained models better suited for variant effect prediction than autoregressive models?\nExplain why span masking forces compositional learning while random token masking encourages local pattern learning. Which is better for regulatory element prediction?\nWhat is the positional bias problem in autoregressive models, and why does it create asymmetric representation quality across sequence positions?\nWhen does zero-shot transfer succeed without any task-specific fine-tuning, and what alignment between pretraining and downstream tasks makes this possible?\nWhy does context length curriculum (training first on short sequences, then progressively extending) improve optimization stability compared to training directly on long contexts?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nMLM uses bidirectional attention, allowing each position to integrate information from the entire sequence. During prediction of a masked token, the model sees both upstream and downstream context. For variant effect prediction, pathogenicity often depends on how a variant disrupts interactions with residues or motifs on both sides, making bidirectional context essential. Autoregressive models only see preceding tokens, missing critical downstream information.\nRandom token masking allows models to predict each masked position from immediately adjacent context (e.g., predicting the missing base in TATA_A is easy from local cues). Span masking removes entire contiguous blocks, eliminating local context entirely. To predict a masked 6bp transcription factor binding motif, the model must reason from more distant regulatory context about what kind of element belongs there, forcing it to learn compositional patterns. Span masking is better for regulatory element prediction because it forces the model to treat motifs as integrated functional units.\nAutoregressive models predict each token given only preceding tokens. Early positions in a sequence have minimal conditioning information (the first token has none), while later positions benefit from rich preceding context. This creates asymmetric representation quality: early positions are learned less reliably than later ones. For genomic sequences with no inherent directionality, this introduces artifacts unrelated to biology.\nZero-shot transfer succeeds when the pretraining objective perfectly aligns with the downstream task structure. For example, if a model is pretrained to predict masked tokens in transcription factor binding sites, and the downstream task is to identify disrupted binding sites, the representations learned during pretraining directly solve the downstream task without additional adaptation. This requires that the pretraining objective captures exactly the patterns the downstream task needs.\nStarting with long contexts forces models to learn both basic sequence statistics and long-range structure simultaneously, creating competing optimization pressures that can interfere. Context curricula separate these phases: models first learn local patterns efficiently at short contexts where optimization is stable, then learn to integrate these patterns across distance at extended contexts. This staged approach allows attention patterns to develop gradually rather than requiring structured long-range attention from random initialization.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nCore Concepts:\n\nPretraining objectives encode assumptions about what matters in biological sequences. The objective you choose determines what your model learns and what it can do downstream.\nMLM (masked language modeling) teaches bidirectional context integration. Best for: variant effect prediction, binding site identification, any task where both upstream and downstream context matters.\nNext-token prediction (autoregressive) teaches generation. Best for: sequence design, therapeutic protein generation, sampling novel sequences that respect learned constraints.\nContrastive learning teaches invariance to perturbations. Best for: cross-population generalization, cross-species transfer, robustness to genetic background variation.\nMulti-task pretraining learns representations capturing multiple facets of function. Best for: when diverse functional labels are available and tasks are complementary.\nStaged pretraining decomposes difficult learning into tractable phases. Context curricula, objective curricula, and data curricula all improve training stability.\nData strategy determines what can be learned. Population diversity, species coverage, and repeat handling all affect downstream clinical utility.\n\nKey Tradeoffs:\n\n\n\nChoice\nFavors…\nAt the cost of…\n\n\n\n\nMLM\nUnderstanding\nGeneration\n\n\nAutoregressive\nGeneration\nBidirectional context\n\n\nContrastive\nRobustness\nReconstruction ability\n\n\nMulti-task\nRich features\nTraining complexity\n\n\nLong context\nLong-range patterns\nCompute cost\n\n\n\nLooking Ahead: Chapter 9 examines how to adapt pretrained models to specific downstream tasks through fine-tuning, feature extraction, and other transfer learning strategies.\n\n\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. https://doi.org/10.48550/arXiv.1810.04805.\n\n\nFerruz, Noelia, Steffen Schmidt, and Birte Höcker. 2022. “ProtGPT2 Is a Deep Unsupervised Language Model for Protein Design.” Nature Communications 13 (1): 4348. https://doi.org/10.1038/s41467-022-32007-7.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2203.15556.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data Navigation on the ENCODE Portal.” Nature Communications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. “Scaling Laws for Neural Language Models.” arXiv. https://doi.org/10.48550/arXiv.2001.08361.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nMcCloskey, Michael, and Neal Cohen. 1989. “Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem.” In Psychology of Learning and Motivation, 24:109–65. Academic Press. https://doi.org/10.1016/S0079-7421(08)60536-8.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence Modeling and Design from Molecular to Genome Scale with Evo.” Science 386 (6723): eado9336. https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nOord, Aaron van den, Yazhe Li, and Oriol Vinyals. 2019. “Representation Learning with Contrastive Predictive Coding.” arXiv. https://doi.org/10.48550/arXiv.1807.03748.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” arXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. “UniRef: Comprehensive and Non-Redundant UniProt Reference Clusters.” Bioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pretraining Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html",
    "href": "part_2/p2-ch09-transfer.html",
    "title": "9  Transfer Learning Foundations",
    "section": "",
    "text": "9.1 Source and Target Domains\nWhen a cardiologist requests variant interpretation for a patient with hypertrophic cardiomyopathy, the clinical need (classifying a specific MYH7 variant) differs fundamentally from the data available during model development (millions of protein sequences sampled across all of evolution). Bridging this gap requires understanding what properties of pretraining determine whether transfer will succeed. When this bridge fails, patients receive confident predictions based on patterns irrelevant to their clinical context.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning Foundations</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-source-target",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-source-target",
    "title": "9  Transfer Learning Foundations",
    "section": "",
    "text": "Domain shift in genomic transfer learning\n\n\n\n\nFigure 9.1: Domain shift in genomic transfer learning. The source domain (left) contains billions of diverse genomic sequences from which pretrained models learn statistical regularities including local motifs, sequence composition, and conservation patterns. The target domain (right) presents sparse labeled examples for specific clinical tasks such as pathogenic variant classification or tissue-specific enhancer prediction. In the learned representation space (center), some features transfer effectively (solid arrows): local motif recognition and conservation patterns align between domains. Other features transfer poorly (dashed arrows): long-range regulatory logic and tissue-specific patterns present in targets may be absent or misleading in source representations. The challenge is that transfer failures are silent—models produce confident predictions regardless of whether underlying features are appropriate for the target task.\n\n\n\n\n9.1.1 Gap Between Pretraining and Deployment\nThe source domain encompasses the data and objectives used during pretraining. For DNA foundation models, source domains typically include reference genomes, pan-genomic collections spanning population diversity, or metagenomic assemblies sampling environmental sequence space (Ji et al. 2021; Dalla-Torre et al. 2023). For protein models, databases like UniRef provide billions of sequences representing the diversity of evolutionary history (Suzek et al. 2007). Pretraining objectives (masked language modeling, next-token prediction, contrastive learning) encourage models to capture statistical regularities that help predict held-out tokens: local motifs, compositional patterns, and the signatures distinguishing functional from random sequence (see Chapter 8 for detailed treatment of these objectives). These learned regularities become the representations that might transfer to downstream tasks.\nThe target domain presents a fundamentally different challenge. Rather than abundant unlabeled sequence, the target domain offers sparse labeled examples of a specific clinical or biological question: a few thousand enhancer sequences with luciferase measurements, several hundred variants with expert pathogenicity classifications, chromatin profiles across a handful of disease-relevant cell types. The target distribution often looks nothing like pretraining data. Pathogenic variants are rare outliers, not typical protein sequences. Tissue-specific enhancers exhibit patterns that genome-wide pretraining may never emphasize. Disease-associated regulatory elements may have been systematically underrepresented in reference data (Kircher et al. 2014).\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider a protein language model pretrained on UniRef sequences. You want to use it to predict pathogenicity of novel missense variants. What types of patterns learned during pretraining might help this task? What patterns might be irrelevant or misleading?\nThink about what the pretraining objective actually rewarded the model for learning, and whether those patterns correlate with what makes a variant pathogenic.\n\n\n\n\n9.1.2 Recognizing Transfer Outcomes\nNot all transfer helps, and distinguishing outcomes requires explicit validation. Positive transfer accelerates learning or improves final performance beyond training from scratch. Negative transfer occurs when pretraining actively hurts, either because learned features conflict with task requirements or because pretrained initialization creates optimization difficulties (Wang et al. 2019). Why would pretraining ever hurt? Similar to how learning British English spelling conventions can interfere with American English writing—“colour” feels right even when “color” is required—prior knowledge sometimes points in the wrong direction. Consider a model pretrained on protein-coding sequences that learns to recognize patterns like codon usage bias, amino acid composition, and reading frame consistency. When applied to noncoding regulatory sequences, these coding-specific patterns become noise that the model must unlearn before it can capture regulatory motif patterns. The pretrained initialization points the model in a direction that conflicts with the target task, and gradient descent must first undo this initialization before making progress—wasting optimization steps and potentially never fully escaping the misleading starting point. Neutral transfer describes situations where pretraining neither helps nor hurts, wasting computational resources on pretrained models without benefit. When a cardiology team adapts a DNA language model for KCNQ1 long QT syndrome variant classification, they must empirically verify which outcome applies to their specific task rather than assuming transfer will help because it helped elsewhere.\n\n\n\nTable 9.1: Possible outcomes when applying pretrained models to new tasks. The critical challenge is detecting negative transfer, which often manifests only on out-of-distribution examples.\n\n\n\n\n\n\n\n\n\n\n\nTransfer Outcome\nDefinition\nExample\nDetection Strategy\n\n\n\n\nPositive transfer\nPretrained model improves task performance\nESM embeddings improve variant classification over one-hot encoding\nLinear probe outperforms random features\n\n\nNegative transfer\nPretraining hurts task performance\nCoding-sequence model produces misleading features for noncoding regions\nFine-tuned model underperforms from-scratch training\n\n\nNeutral transfer\nPretraining neither helps nor hurts\nModel captures irrelevant patterns; adaptation simply overwrites them\nSimilar performance with and without pretraining",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning Foundations</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-transfer-factors",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-transfer-factors",
    "title": "9  Transfer Learning Foundations",
    "section": "9.2 Factors Determining Transfer Success",
    "text": "9.2 Factors Determining Transfer Success\nFour factors determine whether this distributional gap can be bridged. Task relatedness measures whether target predictions depend on patterns the model learned during pretraining; predicting transcription factor binding after sequence pretraining succeeds because both involve local motif recognition, while predicting three-dimensional chromatin contacts may require spatial relationships the pretraining objective never captured (see Chapter 20 for chromatin contact prediction approaches). Target data quantity constrains which adaptation strategies avoid overfitting; with thousands of labeled examples, aggressive fine-tuning can reshape representations, but with dozens, only the lightest approaches remain viable. Model expressiveness influences adaptation flexibility, as larger models encode richer internal representations that can potentially serve more diverse downstream tasks but also risk memorizing small target datasets. Distribution overlap between source and target determines how much learned knowledge applies; human regulatory elements share patterns with mouse elements (enabling cross-species transfer) but diverge in species-specific enhancers (limiting it).\nUnderstanding why transfer succeeds or fails requires examining four interacting factors that collectively determine whether pretrained representations serve a new task. These factors are not independent: a highly related task may still fail with insufficient data, while abundant data cannot rescue transfer when source and target distributions fundamentally diverge. Practitioners must evaluate all four before committing to a transfer learning approach.\n\n\n\n\n\n\nMathematical Content Ahead\n\n\n\nThe following sections discuss quantitative thresholds and factor interactions. The numerical guidance (e.g., “fewer than 500 examples”) is approximate and context-dependent. Focus on the underlying logic: why each factor matters and how they interact.\n\n\n\n9.2.1 Task Relatedness\nTransfer succeeds when target predictions depend on patterns the model learned during pretraining. This dependency is not always obvious from surface-level task descriptions. A model pretrained on DNA sequence using masked language modeling learns to predict nucleotides from context, which implicitly requires learning motifs, sequence composition, and local dependencies. Predicting transcription factor binding sites succeeds because binding depends on sequence motifs that the pretraining objective directly rewarded the model for recognizing. Predicting three-dimensional chromatin contacts typically fails because spatial relationships between distant genomic loci depend on protein-mediated interactions, chromatin accessibility, and nuclear architecture that sequence statistics alone cannot capture (see Chapter 20 for approaches that explicitly model chromatin structure).\nThe key question is not whether source and target tasks share a domain (both involve genomics) but whether they share relevant features. Protein language models pretrained on evolutionary sequences learn representations that capture structural constraints, functional domains, and evolutionary conservation. Variant effect prediction succeeds because pathogenic variants often disrupt these same structural and functional properties. Protein-protein interaction prediction may succeed partially (interaction surfaces correlate with evolutionary conservation) but fail for interaction specificity (which residues determine which partners bind), because the pretraining objective never distinguished between interacting and non-interacting proteins.\nPractitioners can estimate task relatedness before committing to transfer through three approaches. First, linear probing (see Section 9.3.1) reveals whether frozen pretrained representations contain task-relevant information; if a simple classifier on frozen embeddings outperforms random features, the pretraining objective captured something useful. Second, examining what the pretraining objective explicitly rewards clarifies what patterns the model was incentivized to learn; masked language modeling rewards local context prediction, contrastive learning rewards distinguishing related from unrelated sequences, and next-token prediction rewards sequential dependencies. Third, consulting the literature for related transfer attempts provides empirical guidance; if similar transfers have failed for this model class, success is unlikely without architectural or data modifications.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nBefore continuing, ensure you can answer: What is the difference between task relatedness based on domain (both are “genomics”) versus feature alignment (both require the same learned patterns)? Why does the latter matter more for transfer success?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nDomain relatedness refers to tasks being in the same field (both involving DNA or proteins), while feature alignment means tasks require similar learned patterns in their representations. Feature alignment matters more because transfer success depends on whether the pretrained features are actually useful for the target task, not just whether tasks share a domain label. For example, predicting gene expression and variant pathogenicity are both “genomics” but require different features, while splice site prediction and variant effect prediction both benefit from learning local sequence constraints despite different prediction targets.\n\n\n\n\n\nWhen task relatedness is low, three strategies may salvage transfer. Intermediate fine-tuning on a related auxiliary task can build bridge representations: a model pretrained on general DNA sequence might be fine-tuned on chromatin accessibility prediction before the final adaptation to enhancer-gene linking, because chromatin accessibility provides intermediate features more relevant to regulatory relationships than raw sequence statistics. Multi-task fine-tuning that includes the target task alongside related tasks can encourage the model to extract shared features. Alternatively, practitioners may conclude that transfer is inappropriate for this task and proceed with from-scratch training, which remains a valid choice when pretrained representations offer no advantage.\n\n\n9.2.2 Target Data Quantity\nAvailable labeled data constrains which adaptation strategies avoid overfitting, creating a fundamental limit on adaptation complexity. The thresholds are approximate but provide useful guidance: with fewer than 500 labeled examples, only linear probing remains viable because any approach that updates pretrained parameters will overfit catastrophically. Between 500 and 5,000 examples, parameter-efficient methods like LoRA introduce enough flexibility to improve over frozen features while maintaining implicit regularization through low-rank constraints and frozen backbone parameters. Above 10,000 examples, full fine-tuning becomes feasible for adapting pretrained representations to fundamentally different target distributions.\n\n\n\nTable 9.2: Approximate data thresholds for different adaptation strategies. These boundaries are guidelines, not rules—effective thresholds depend on task complexity, class balance, and data quality.\n\n\n\n\n\n\n\n\n\n\n\nData Quantity\nViable Strategies\nWhy\nRisk\n\n\n\n\n&lt; 500 examples\nLinear probing only\nToo few examples to learn new parameters without overfitting\nUnderfitting if frozen features lack task-relevant information\n\n\n500 - 5,000 examples\nPEFT (LoRA, adapters)\nLow-rank constraints provide implicit regularization\nHyperparameter sensitivity; overfitting still possible\n\n\n5,000 - 10,000 examples\nPEFT or careful full fine-tuning\nEnough data for some parameter updates\nCatastrophic forgetting if learning rate too high\n\n\n&gt; 10,000 examples\nFull fine-tuning viable\nSufficient data to reshape representations without memorization\nComputational cost; still validate on held-out data\n\n\n\n\n\n\nThese thresholds interact with data quality in ways that complicate simple counting. Five thousand noisy labels from high-throughput screening contribute less information than five hundred expert-curated annotations. Class imbalance matters: a dataset with 10,000 examples split 9,900 negative and 100 positive effectively provides only hundreds of examples for learning positive class features. Redundancy in training data (multiple variants from the same gene, or cells from the same patient) reduces effective sample size because nominally independent examples share confounding factors. The relevant quantity is not raw example count but effective information content for the target task.\nData augmentation can stretch limited examples further, but augmentation strategies must preserve task-relevant properties. Reverse-complementing DNA sequences provides valid augmentation for tasks with strand-symmetric biology (transcription factor binding is typically strand-symmetric) but introduces noise for tasks with strand-specific signals (RNA secondary structure depends on transcript strand). Random nucleotide masking followed by model infilling can generate plausible sequence variants, but these variants may not span the relevant distribution of task-specific variation. The safest augmentation strategies involve domain knowledge about what transformations preserve task labels.\nWhen data is severely limited (dozens of examples), practitioners face a choice between three imperfect options. Linear probing on frozen features provides the most stable approach but may miss task-specific patterns not captured in pretrained representations. Few-shot learning methods (see Section 10.6.1) attempt to adapt with minimal examples by leveraging structured prompts or metric learning, but success varies dramatically across tasks. Collecting more data, though often expensive, may be the only path to reliable adaptation.\n\n\n9.2.3 Model Expressiveness\nLarger models encode richer internal representations that can potentially serve more diverse downstream tasks, but this expressiveness creates a tension with overfitting risk. A 3-billion parameter protein language model captures subtle evolutionary signals invisible to smaller models, encoding relationships between distant residues, complex motif interactions, and nuanced conservation patterns. These rich representations enable zero-shot transfer to tasks the model was never explicitly trained for, because the pretraining objective forced the model to learn features that happen to correlate with task-relevant properties. ESM-2 at 15 billion parameters predicts protein structure contact maps despite never seeing structure labels during training, because evolutionary constraints that determine which sequences survive (the pretraining signal) are the same constraints that determine which structures fold stably (the transfer target).\nThe same expressiveness that enables rich transfer creates memorization risk when adaptation data is limited. A highly expressive model can memorize thousands of training examples without learning generalizable patterns, achieving perfect training accuracy while failing entirely on held-out data. This risk scales with model capacity relative to dataset size: a 3-billion parameter model fine-tuned on 500 variants will almost certainly overfit, while the same model fine-tuned on 500,000 variants may generalize effectively.\n\n\n\n\n\n\nStop and Think\n\n\n\nYou have two options: (1) a 150-million parameter model and 1,000 labeled examples, or (2) a 3-billion parameter model with the same 1,000 examples. Which would you expect to generalize better, and why? What adaptation strategy might make the larger model viable?\n\n\nParameter-efficient methods mitigate this tension by constraining which model behaviors can change during adaptation. Why does restricting the adaptation space help? The core insight is that most of the pretrained model’s capacity encodes generally useful features, while only a small subspace needs to change for task-specific adaptation. LoRA restricts updates to low-rank subspaces, limiting the effective capacity available for memorization while preserving the rich pretrained representations for transfer. If a model’s behavior can be adapted with rank-8 updates (adding only 8 parameters per dimension to adapt), then the model cannot memorize thousands of unique examples through those 8 degrees of freedom—the low-rank bottleneck prevents it. Adapter layers introduce small trainable modules between frozen layers, enabling task-specific computation without overwriting general knowledge. The rank, placement, and number of adapted parameters become hyperparameters that balance adaptation flexibility against overfitting risk.\nModel selection thus involves matching expressiveness to available data and task complexity. For tasks with abundant data and substantial divergence from pretraining, larger models provide more capacity to learn task-specific representations. For tasks with limited data that closely align with pretraining objectives, smaller models may transfer more reliably because their simpler representations leave less room for spurious memorization. The optimal model size depends on the interaction between all four transfer factors, not on model quality in isolation.\n\n\n9.2.4 Distribution Overlap\nThe degree of overlap between source and target distributions determines how much learned knowledge applies directly versus requires adaptation. Human and mouse genomes share regulatory syntax for housekeeping genes whose expression patterns were established before the mammalian radiation, enabling direct transfer of core promoter recognition, splice site identification, and basic transcriptional logic. Human-specific enhancers that evolved after the human-mouse divergence (roughly 75 million years ago) have no mouse counterparts from which to transfer, creating blind spots for human enhancer prediction based on mouse training data.\nDistribution overlap operates at multiple scales that practitioners must evaluate separately. At the sequence level, nucleotide composition, k-mer frequencies, and local motif distributions may diverge between source and target. Protein sequences from thermophilic organisms differ systematically in amino acid composition from mesophilic training data, potentially confusing models that implicitly learned composition-dependent features. At the feature level, the relationship between sequence patterns and biological function may shift: a motif that indicates enhancer activity in one cell type may be repressive in another due to cofactor availability. At the label level, the definition of positive and negative examples may differ: “pathogenic” variants in ClinVar reflect clinical ascertainment patterns that differ systematically from the evolutionary selection captured in pretraining.\nCross-species transfer illustrates distribution overlap challenges concretely. Models pretrained on human sequences and applied to non-human primates succeed for conserved elements (core promoters, splice sites, essential genes) because evolutionary proximity ensures feature preservation. Application to more distant species (zebrafish, Drosophila, plants) succeeds only for deeply conserved features and fails progressively for lineage-specific innovations. Kelley demonstrated that training simultaneously on human and mouse data improves regulatory prediction for both species compared to single-species training, because shared evolutionary history provides implicit labels about functional conservation while species-specific examples reveal where that conservation breaks down (Kelley 2020).\n\n\n\n\n\n\nKey Insight\n\n\n\nDistribution shift can be subtle. A model trained on coding variants may fail on synonymous variants not because the sequences look different, but because the relationship between sequence features and pathogenicity differs. The model learned “this amino acid change is rare in evolution, therefore damaging”—a pattern that does not apply to synonymous changes.\n\n\nDetecting distribution shift requires comparing source and target distributions before deployment (see Section 10.5.2 for methods). Statistical divergence measures quantify distribution differences numerically; embedding visualizations reveal whether target examples occupy familiar or novel regions of representation space; canary examples that should always be predicted correctly provide early warning of catastrophic shift. When shift is detected, practitioners must choose between domain adaptation techniques (which attempt to bridge the gap), acceptance that certain target subpopulations cannot be served by this model, or collection of target-distribution training data to enable proper adaptation.\n\n\n9.2.5 Factor Interactions\nThe four factors interact in ways that preclude simple rules. High task relatedness cannot rescue transfer when target data is too limited for any adaptation; abundant data cannot overcome fundamental distribution mismatch; an expressive model provides no advantage when pretrained representations lack task-relevant features. Practitioners must evaluate all four factors jointly, using the linear probing and validation approaches described in subsequent sections to empirically determine whether transfer succeeds for their specific combination of model, task, and data.\n\n\n\n\n\n\n\n\nTask relatedness determines feature relevance\n\n\n\n\n\n\n\nData quantity constrains adaptation strategy\n\n\n\n\n\n\n\n\n\nModel expressiveness vs. overfitting risk\n\n\n\n\n\n\n\nDistribution overlap enables knowledge transfer\n\n\n\n\n\n\nFigure 9.2: Four interacting factors determine transfer success. (A) Task relatedness: when source and target tasks share relevant features (motif detection, conservation patterns), pretrained knowledge transfers; when they require different features (coding vs. noncoding), transfer fails or hurts. (B) Data quantity thresholds: fewer than 500 examples limits you to linear probing; 500-5,000 enables PEFT; above 10,000 enables full fine-tuning. (C) Model expressiveness creates a double-edged sword: larger models capture richer features but risk memorizing limited target data. (D) Distribution overlap: cross-species transfer succeeds for conserved elements (housekeeping promoters) but fails for lineage-specific innovations (human-specific enhancers).\n\n\n\nThe most reliable path forward is conservative escalation: establish frozen feature baselines first to assess task relatedness and distribution overlap; try parameter-efficient methods next if frozen features show promise but leave room for improvement; reserve full fine-tuning for cases where simpler methods demonstrably fail and sufficient data exists to justify the risk; and maintain from-scratch training as a valid comparison throughout. Each escalation step provides information about which factors limit transfer, guiding both immediate decisions and future model development.\n\n\n\n\n\n\nPractical Guidance: The Conservative Escalation Protocol\n\n\n\nWhen approaching a new transfer learning problem, follow this sequence:\n\nLinear probe first. Train a simple classifier on frozen embeddings. If this fails badly (near-random performance), the pretrained features may lack task-relevant information.\nCompare to random features. If linear probe on pretrained embeddings barely beats random features, question whether transfer helps at all.\nTry PEFT if linear probe shows promise. If frozen features provide reasonable accuracy but leave headroom, parameter-efficient methods can capture task-specific patterns.\nReserve full fine-tuning for abundant data. Only with 10,000+ examples and evidence that PEFT is insufficient should full parameter updates be considered.\nAlways maintain a from-scratch baseline. This reveals whether transfer actually helps or whether you are simply training on your target data.\n\n\n\n\n\n\n\n\n\nConservative escalation decision flowchart\n\n\n\n\nFigure 9.3: Conservative escalation protocol for transfer learning decisions. Start with linear probing on frozen embeddings (Step 1). If probe performance exceeds random baseline, pretrained features encode task-relevant information (Step 2a); proceed to PEFT if headroom remains. If probe performance matches random, question whether transfer helps (Step 2b). PEFT methods like LoRA add flexibility while constraining overfitting (Step 3). Reserve full fine-tuning for cases with abundant data (10,000+ examples) where PEFT is insufficient (Step 4). Throughout, maintain from-scratch baseline to verify transfer provides genuine benefit. Red paths indicate stopping points; green paths indicate progression.\n\n\n\n\n\n\n\n\n\nWorked Example: Applying the Conservative Escalation Protocol\n\n\n\nA team wants to predict pathogenicity of BRCA1 variants using ESM-2 embeddings. They have 800 labeled variants.\nStep 1 (Linear probe): Train logistic regression on frozen ESM-2 embeddings. - Result: 78% accuracy\nStep 2 (Random baseline): Train the same classifier on random embeddings. - Result: 52% accuracy - Interpretation: The 26-percentage-point gap confirms pretrained embeddings encode pathogenicity-relevant information.\nStep 3 (PEFT consideration): With 78% accuracy but room for improvement, they try LoRA (rank 8). - Result: 84% accuracy - Interpretation: Some task-specific reorganization helps.\nStep 4 (Decision): With 800 examples, they stop here. Full fine-tuning risks overfitting, and 84% meets requirements.\nStep 5 (Baseline check): A CNN trained from scratch achieves 71% accuracy. - Interpretation: Transfer provides genuine 13-point benefit over from-scratch training.\n\n\n\nApproach\nAccuracy\nTrainable Params\nRisk Level\n\n\n\n\nRandom baseline\n52%\n—\nReference\n\n\nFrom-scratch CNN\n71%\n2M\nModerate\n\n\nLinear probe (frozen ESM-2)\n78%\n1K\nMinimal\n\n\nLoRA (rank 8)\n84%\n50K\nLow\n\n\nFull fine-tuning\nNot attempted\n650M\nHigh with 800 examples",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning Foundations</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-feature-extraction",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-feature-extraction",
    "title": "9  Transfer Learning Foundations",
    "section": "9.3 Feature Extraction and Representation Analysis",
    "text": "9.3 Feature Extraction and Representation Analysis\nClinical laboratories processing hundreds of variants daily cannot afford to fine-tune models for each new gene or variant class. When a novel gene enters diagnostic panels, classifiers must be deployed rapidly using whatever labeled examples exist. A molecular diagnostics team with 200 annotated RYR1 variants for malignant hyperthermia risk prediction cannot fine-tune a 500-million parameter model; they need an approach that works with minimal data while avoiding adaptation risk entirely.\nFrozen feature extraction addresses this constraint by treating pretrained models as fixed representation engines. All backbone parameters remain frozen; only a lightweight classifier trained on the extracted representations learns from labeled data. The backbone never changes, eliminating catastrophic forgetting entirely and enabling deployment within hours rather than weeks. The fundamental tradeoff is clear: frozen features sacrifice adaptation flexibility for speed, safety, and efficiency.\n\n9.3.1 Linear Probing\nWhy does the simplest possible classifier often suffice? If pretrained representations already encode task-relevant features in linearly separable form, adding complexity provides no benefit and risks overfitting. Linear probing tests this hypothesis by introducing only \\(d \\times c\\) parameters (where \\(d\\) is the embedding dimension and \\(c\\) is the number of output classes). Pass input sequences through the frozen model to obtain embeddings, typically from the final layer or from a designated [CLS] token aggregating sequence information, then train a linear classifier mapping embeddings to task labels.\n\n\n\n\n\n\nWorked Example: Linear Probing for Splice Site Classification\n\n\n\nSuppose you want to classify whether a sequence contains a functional splice donor site using frozen DNABERT embeddings.\nStep 1: Extract embeddings. For each 200-bp sequence centered on a potential splice site, obtain the 768-dimensional [CLS] embedding from the frozen model.\nStep 2: Train a linear classifier. With 5,000 labeled examples (2,500 true splice sites, 2,500 negative controls), train a logistic regression: \\(p(\\text{splice}) = \\sigma(w^\\top h + b)\\) where \\(h\\) is the embedding, \\(w\\) is a 768-dimensional weight vector, and \\(b\\) is a scalar bias.\nStep 3: Evaluate on held-out data. If the linear probe achieves 92% accuracy while random features achieve 60%, the pretrained embeddings encode splice-relevant information that transferred successfully.\nStep 4: Interpret the result. The 92% accuracy suggests motif patterns learned during pretraining (the GT dinucleotide, surrounding sequence context) are preserved in the embeddings. You now have evidence that transfer works for this task.\n\n\n\n\n\n\n\n\nLinear probing workflow for transfer diagnostics\n\n\n\n\nFigure 9.4: Linear probing workflow for diagnosing transfer potential. Step 1: Pass input sequences through frozen pretrained model to extract embeddings (typically [CLS] token or final layer). Step 2: Train lightweight classifier (logistic regression, small MLP) on frozen embeddings using labeled target data. Step 3: Compare probe accuracy against random embedding baseline and from-scratch training. Interpretation guide: If probe &gt;&gt; random, pretrained features encode task-relevant information (positive transfer likely). If probe ≈ random, pretrained features lack relevance (consider different model or from-scratch training). If probe &lt; from-scratch, pretraining may hurt (negative transfer risk).\n\n\n\nJi et al. demonstrated that DNABERT embeddings paired with linear probes achieve competitive chromatin accessibility prediction from a few hundred positive and negative examples, matching convolutional neural network baselines requiring far more labeled data (Ji et al. 2021). Dalla-Torre et al. showed similar results with Nucleotide Transformer, where linear probes on frozen embeddings approached fine-tuned performance for promoter detection and splice site recognition (Dalla-Torre et al. 2023). These successes reflect alignment between pretraining objectives (predicting masked tokens from local context) and target tasks (distinguishing sequences based on motif patterns the model already learned to recognize).\n\n\n9.3.2 When Linear Probing Fails\nLinear probes fail when relevant information exists in embeddings but requires nonlinear transformation to extract. Shallow multilayer perceptrons (one or two hidden layers) extend linear probing by enabling more complex decision boundaries while maintaining computational efficiency. With several thousand labeled examples, shallow MLPs on HyenaDNA embeddings improve splice site prediction over linear probes by capturing interactions between features that linear models cannot represent (Nguyen et al. 2023). The additional expressiveness helps when task-relevant patterns are distributed across embedding dimensions in ways that linear combination cannot capture.\nThe more fundamental limitation cannot be addressed by classifier complexity: performance caps at how well pretrained representations already encode task-relevant features. If the pretraining objective emphasized patterns irrelevant to the downstream task, or if required features were actively suppressed during pretraining, frozen features will underperform models trained from scratch regardless of classifier sophistication. A model pretrained exclusively on coding sequence may encode features misleading for noncoding regulatory prediction; no linear probe can overcome representations that point in the wrong direction.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA linear probe on frozen protein language model embeddings achieves 85% accuracy for predicting whether variants are pathogenic. Adding a two-layer MLP increases accuracy to 86%. Adding a five-layer MLP increases accuracy to 86.5%. What do these results suggest about:\n\nWhether the pretrained embeddings contain pathogenicity-relevant information?\nWhether more complex classifiers are likely to help?\nWhat the ceiling on frozen-feature performance might be?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nYes, the 85% accuracy with a linear probe indicates the frozen embeddings contain substantial pathogenicity-relevant information that is linearly separable. (2) No, the minimal gains from adding complexity (only 1.5% improvement with a five-layer MLP) suggest more complex classifiers won’t help much - the useful information is already captured linearly. (3) The ceiling appears to be around 86-87%, indicating that further improvements likely require updating the pretrained model itself rather than just adding more classifier capacity. This pattern suggests linear probing is sufficient and parameter-efficient fine-tuning or full fine-tuning would be needed to go beyond this ceiling.\n\n\n\n\n\n\n\n\n9.3.3 Probing Representations\nA variant effect predictor built on ESM embeddings achieves 85% accuracy in initial testing, but the team deploying it needs to understand why. Does the model genuinely capture evolutionary constraint relevant to pathogenicity, or has it learned spurious correlations that will fail on out-of-distribution variants? Before committing computational resources to adaptation, practitioners benefit from understanding what the pretrained model actually learned.\nProbing classifiers answer these diagnostic questions by systematically interrogating representations before deployment. The methodology converts the abstract question “will transfer help?” into concrete evidence about representation content: train lightweight classifiers to predict properties of interest from frozen embeddings, then examine how accurately different properties can be decoded. If chromatin accessibility can be predicted with 85% accuracy from a linear probe, the representations already encode accessibility-relevant features and frozen feature extraction will likely succeed. If transcription factor binding requires a deep nonlinear classifier to reach the same accuracy, relevant information exists but is not linearly separable, suggesting PEFT might help by reorganizing representations for easier extraction. If a property cannot be predicted above chance even with flexible classifiers, the representations may lack necessary information entirely, and transfer to this task may fail regardless of adaptation strategy.\n\n\n9.3.4 What Probing Reveals About Pretrained Models\nSystematic probing reveals what models learn during pretraining. Rives et al. demonstrated that ESM protein embeddings encode secondary structure so thoroughly that linear probes achieve near state-of-the-art helix/sheet/coil prediction accuracy (Rives et al. 2021). Contact prediction (which residues are spatially close in folded structure) requires nonlinear probes but still achieves strong performance, indicating that tertiary structure information is present but requires transformation to extract. DNA language models show similar patterns: local motif information is recoverable by linear probes while long-range dependencies require multi-layer networks (Ji et al. 2021). The ESM family and its learned structural knowledge are examined in Chapter 15, while DNA language model probing appears in Chapter 14.\nLayer-wise probing reveals how information transforms through the model. Early layers typically encode local compositional features (\\(k\\)-mer frequencies, simple motifs, sequence statistics) while later layers capture more abstract patterns (regulatory signatures, evolutionary constraints, functional classifications) (Jawahar, Sagot, and Seddah 2019). Why does this layer-wise organization emerge? The structure reflects how neural networks compose features hierarchically: early layers detect simple patterns (individual motifs, dinucleotide frequencies) because they operate on raw input with limited receptive field; later layers combine these detections into higher-order features (motif combinations, spacing patterns, evolutionary signatures) that summarize broader context. The implication for practitioners is that optimal layer selection depends on task complexity: tasks requiring raw motif detection may benefit from early layers, while tasks requiring integration of multiple signals benefit from later layers. Layer selection becomes another hyperparameter to optimize during adaptation.\n\n\n\n\n\n\nKey Insight\n\n\n\nProbing is diagnostic, not just evaluative. The goal is not just to measure performance but to understand what the model knows. This understanding guides adaptation strategy: if probing reveals that secondary structure is encoded but contact information requires nonlinear extraction, you know that contact prediction will benefit from PEFT more than secondary structure prediction.\n\n\n\n\n9.3.5 Probe-Guided Adaptation\nThe diagnostic value extends beyond predicting which adaptation strategy to use. When probing reveals that required features are absent from pretrained representations, practitioners face a choice: commit to full fine-tuning with sufficient target data (hoping the model can learn missing features), switch to a different foundation model whose pretraining objective better aligns with task requirements, or proceed with from-scratch training that does not inherit inappropriate inductive biases. The investment in probing before adaptation often saves months of wasted effort on transfer that was doomed from the start.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning Foundations</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch09-transfer.html#sec-ch09-summary",
    "href": "part_2/p2-ch09-transfer.html#sec-ch09-summary",
    "title": "9  Transfer Learning Foundations",
    "section": "9.4 Summary",
    "text": "9.4 Summary\nTransfer learning succeeds when pretrained representations encode features relevant to downstream tasks, and fails when they do not. The four factors examined in this chapter—task relatedness, target data quantity, model expressiveness, and distribution overlap—collectively determine whether pretrained knowledge transfers productively to new applications.\nFeature extraction and linear probing provide the essential diagnostic tools for assessing transfer potential before committing to more complex adaptation. When linear probes on frozen representations outperform random baselines, the pretrained model has captured task-relevant structure. When probing accuracy approaches fine-tuned performance, simpler adaptation strategies may suffice. When probing fails entirely, more aggressive adaptation or alternative models may be necessary.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nExplain the four factors that determine transfer success (task relatedness, data quantity, model expressiveness, distribution overlap) and how they interact.\nWhy can transfer learning fail silently, producing confident predictions despite learning nothing useful? What makes this particularly dangerous for clinical applications?\nA linear probe on frozen ESM-2 embeddings achieves 78% accuracy for pathogenicity prediction, while random embeddings achieve 52%. What does this 26-point gap reveal about the pretrained representations?\nWhen does negative transfer occur, and why would pretraining sometimes hurt performance compared to training from scratch?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nFour factors and interactions: Task relatedness measures whether the target task requires patterns learned during pretraining (e.g., motif recognition transfers well from masked language modeling). Data quantity constrains which adaptation strategies avoid overfitting (fewer than 500 examples limits you to linear probing). Model expressiveness determines how rich the pretrained representations are, but larger models risk overfitting with limited data. Distribution overlap quantifies similarity between source and target data (human-mouse regulatory elements share patterns, enabling cross-species transfer). These factors interact: high task relatedness cannot rescue transfer with insufficient data, abundant data cannot overcome fundamental distribution mismatch, and expressive models provide no advantage when pretrained representations lack task-relevant features.\nSilent failures in transfer learning: Transfer learning fails silently because models produce confident predictions regardless of whether they learned relevant patterns or spurious correlations. A protein language model trained on human sequences may confidently score mouse variants based on human-specific evolutionary pressures completely irrelevant to mouse biology, but the output format looks identical to successful predictions. For clinical applications, this is particularly dangerous because confident but wrong predictions can lead to misdiagnosis, inappropriate treatment decisions, or missed pathogenic variants, with no internal signal that the model’s learned features are misaligned with the clinical task.\nInterpreting the 26-point gap: The 26-percentage-point gap between ESM-2 embeddings (78%) and random embeddings (52%) confirms that the pretrained representations encode substantial pathogenicity-relevant information in a form that is already linearly separable. This suggests the pretraining objective (masked language modeling on evolutionary sequences) successfully captured patterns correlated with variant pathogenicity, such as evolutionary constraints, structural preferences, and functional domain information. The gap provides strong evidence that transfer learning will help this task and justifies exploring parameter-efficient methods to capture additional task-specific patterns beyond what frozen features provide.\nNegative transfer mechanisms: Negative transfer occurs when pretraining actively hurts performance because learned features conflict with task requirements or create optimization difficulties. For example, a model pretrained on protein-coding sequences learns patterns like codon usage bias, reading frame consistency, and amino acid composition constraints. When applied to noncoding regulatory sequences, these coding-specific patterns become noise that misleads the model or must be unlearned during fine-tuning. The pretrained initialization points gradient descent in a direction that conflicts with the target task, wasting optimization steps and potentially never fully escaping the misleading starting point, resulting in worse performance than training from scratch without these inappropriate biases.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nCore concepts:\n\nSource and target domains: Pretraining data differs systematically from deployment data; understanding this gap is essential\nTransfer outcomes: Positive, negative, and neutral transfer are all possible—only validation distinguishes them\nFour factors: Task relatedness, data quantity, model expressiveness, and distribution overlap jointly determine transfer success\nLinear probing: The essential first diagnostic—reveals what pretrained representations encode before committing to adaptation\nConservative escalation: Start with frozen features, escalate to PEFT, reserve full fine-tuning for abundant data\n\nMain takeaways:\n\nTransfer failures are silent. Models produce confident predictions whether transfer has succeeded or failed catastrophically.\nTask relatedness depends on shared features, not shared domain. “Both are genomics” does not guarantee transfer will help.\nData quantity constrains adaptation complexity. With limited data, simpler methods (linear probing, PEFT) avoid overfitting.\nProbing before adaptation saves wasted effort. Understanding what the model knows guides strategy selection.\nThe conservative escalation protocol provides a systematic path from diagnosis to deployment.\n\nLooking ahead: Chapter 10 examines the practical adaptation strategies that operationalize these principles: parameter-efficient fine-tuning (LoRA, adapters), layer selection for embedding extraction, full fine-tuning, and the emerging paradigms that extend transfer to minimal-data scenarios.\n\n\n\n\n\n\n\n\nSelf-Assessment\n\n\n\nBefore moving to Chapter 10, ensure you can:\n\nExplain why transfer learning fails silently and what makes this dangerous for clinical applications\nDescribe the four factors that determine transfer success and how they interact\nOutline the linear probing procedure and interpret its results\nArticulate when frozen features suffice versus when more aggressive adaptation is necessary\nApply the conservative escalation protocol to a new transfer learning problem\n\n\n\n\n\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nJawahar, Ganesh, Benoît Sagot, and Djamé Seddah. 2019. “What Does BERT Learn about the Structure of Language?” In ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy. https://inria.hal.science/hal-02131630.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nKelley, David R. 2020. “[Basenji2] Cross-Species Regulatory Sequence Activity Prediction.” PLOS Computational Biology 16 (7): e1008050. https://doi.org/10.1371/journal.pcbi.1008050.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory M. Cooper, and Jay Shendure. 2014. “A General Framework for Estimating the Relative Pathogenicity of Human Genetic Variants.” Nature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. “UniRef: Comprehensive and Non-Redundant UniProt Reference Clusters.” Bioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nWang, Zirui, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. 2019. “Characterizing and Avoiding Negative Transfer.” In, 11293–302. https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Characterizing_and_Avoiding_Negative_Transfer_CVPR_2019_paper.html.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transfer Learning Foundations</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html",
    "href": "part_2/p2-ch10-adaptation.html",
    "title": "10  Adaptation Strategies",
    "section": "",
    "text": "10.1 Parameter-Efficient Fine-Tuning",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-peft",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-peft",
    "title": "10  Adaptation Strategies",
    "section": "",
    "text": "10.1.1 Low-Rank Adaptation\nLow-Rank Adaptation (LoRA) has emerged as the dominant PEFT technique in genomic applications because it directly operationalizes this insight. Think of it like adding a correction lens to a camera rather than rebuilding the optics from scratch: the original lens (pretrained weights) stays intact, while a small, carefully shaped addition adjusts the focus for your specific purpose. Rather than updating a large weight matrix \\(W\\) directly, LoRA introduces two smaller matrices \\(A\\) and \\(B\\) whose product approximates the desired weight change: \\(W' = W + BA\\) (Hu et al. 2021). During fine-tuning, \\(W\\) remains frozen while only \\(A\\) and \\(B\\) receive gradient updates. The rank of these matrices (typically 8 to 64 for genomic models) controls adaptation expressiveness: lower ranks introduce fewer parameters and stronger implicit regularization; higher ranks enable more flexible task-specific modification at greater overfitting risk.\n\n\n\n\n\n\nKey Insight: Why Low-Rank Works\n\n\n\nThe success of LoRA rests on a remarkable empirical finding: task-specific adaptations often lie in a low-dimensional subspace of weight space. You do not need to modify all 500 million parameters to adapt a model for a new tissue type. Instead, a low-rank update (modifying perhaps 2-5 million parameters) captures most of the required adaptation while providing implicit regularization against overfitting. This is analogous to principal component analysis, where a few components often capture most variance in high-dimensional data.\n\n\nThe efficiency gains prove substantial. A transformer with 500 million parameters might require updating only 2 to 5 million LoRA parameters (representing the low-rank decompositions applied to attention weight matrices), reducing memory requirements by an order of magnitude compared with full fine-tuning. This efficiency enables training on consumer GPUs for models that would otherwise require specialized infrastructure, and enables systematic hyperparameter search that would be prohibitive with full parameter updates. Zhou et al. demonstrated that LoRA adapters on Nucleotide Transformer enable tissue-specific chromatin accessibility prediction, where separate low-rank matrices capture tissue-specific regulatory patterns while the pretrained backbone encodes general sequence understanding (Zhou et al. 2024). Clinical applications of parameter-efficient fine-tuning for risk prediction appear in Chapter 27.\n\n\n\n\n\n\nLow-Rank Adaptation (LoRA) architecture\n\n\n\n\nFigure 10.1: Low-Rank Adaptation (LoRA) architecture. Rather than updating the full weight matrix W directly, LoRA introduces two smaller matrices A and B whose product approximates the desired weight change: W’ = W + BA. During fine-tuning, W remains frozen (gray, 500 million parameters typical) while only A and B receive gradient updates (blue, 2-5 million parameters). The rank r (typically 8-64) controls adaptation expressiveness: lower ranks introduce fewer parameters and stronger implicit regularization; higher ranks enable more flexible task-specific modification at greater overfitting risk.\n\n\n\n\n\n10.1.2 Configuring Low-Rank Adaptation\nSelecting LoRA hyperparameters requires balancing expressiveness against overfitting risk, with optimal choices depending on task alignment and available data. The rank parameter controls how many dimensions of modification are possible. Ranks of 4 to 16 typically suffice for tasks closely aligned with pretraining objectives, where small perturbations to pretrained weights capture the required adaptation. When target tasks diverge more substantially from pretraining, ranks of 32 to 64 may prove necessary, though higher ranks approach the parameter count where full fine-tuning becomes competitive. Empirical comparison across ranks on held-out validation data remains the most reliable selection method; theoretical guidance for optimal rank given task characteristics does not yet exist.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading about layer selection, consider: if you had to choose between adapting (a) only the first few layers, (b) only the middle layers, or (c) only the final layers of a pretrained transformer for a new classification task, which would you choose and why? How might your answer differ for encoder vs. decoder models?\n\n\nThe question of which layers to adapt depends critically on whether the foundation model uses encoder or decoder architecture. Encoder models like DNABERT and Nucleotide Transformer process entire sequences bidirectionally, building representations that integrate context from both directions at every layer. For these models, middle and later layers typically encode the most task-relevant features: early layers capture local sequence patterns (motifs, k-mer statistics) while deeper layers integrate these into higher-order representations (see Chapter 8 for discussion of layer-wise representation learning). Adapting only the final third of transformer layers often achieves most of the performance gain at a fraction of the parameter cost. Linear probing experiments across layers can identify where task-relevant information concentrates before committing to adapter placement.\nDecoder models like HyenaDNA in autoregressive mode and GPT-style genomic models present different considerations. These architectures process sequences left-to-right, with each position attending only to preceding context. The causal attention mask means that later layers have seen more integrated context, but the unidirectional flow creates different feature hierarchies than bidirectional encoders. For decoder models, adapting attention layers proves particularly important because the causal structure means attention patterns determine what contextual information flows forward. Practitioners often find that adapting both attention projections (queries, keys, values, and output) and feed-forward layers in decoder models yields better results than attention-only adaptation that works well for encoders.\nWithin layers, LoRA can be applied to query, key, value, and output projection matrices in attention, and to the two weight matrices in feed-forward blocks. Attention weight adaptation alone often suffices for encoder models on classification tasks, where the key adaptation involves changing what information the model attends to. Feed-forward adaptation becomes more important when the required transformation involves learning new feature combinations rather than reweighting existing attention patterns. When computational budget permits, adapting all weight matrices with lower rank often outperforms adapting fewer matrices with higher rank.\n\n\n\n\n\n\nPractical Guidance: LoRA Configuration Checklist\n\n\n\nWhen configuring LoRA for a new task:\n\nStart with rank 8-16 for tasks similar to pretraining; increase to 32-64 if validation performance plateaus\nFor encoder models: adapt final third of layers; prioritize attention weights\nFor decoder models: adapt attention and feed-forward weights; expect to search across layers\nAlways validate: compare against frozen features baseline and from-scratch training on identical data\nMonitor for overfitting: use early stopping based on validation loss, not training loss\n\n\n\nThese heuristics provide starting points, not guarantees. The interaction between model architecture, pretraining objective, target task, and available data creates a combinatorial space that resists simple rules. Systematic hyperparameter search over rank, layer selection, and weight matrix targeting, guided by validation performance on data matching the deployment distribution, remains the most reliable path to effective adaptation.\n\n\n\n\n\n\nWorked Example: Configuring LoRA for Tissue-Specific Prediction\n\n\n\nConsider adapting Nucleotide Transformer (500M parameters) for predicting chromatin accessibility in liver tissue, with 2,000 labeled training examples.\nStep 1: Choose initial rank. With moderate data (2,000 examples) and a task similar to pretraining (sequence to chromatin state), start with rank 16.\nStep 2: Select layers. Nucleotide Transformer is an encoder model with 24 layers. Apply LoRA to layers 17-24 (final third), prioritizing attention weights (Q, K, V projections).\nStep 3: Count parameters.\n\n\n\n\n\n\n\n\nComponent\nCalculation\nParameters\n\n\n\n\nOriginal attention (per layer)\n\\(4 \\times d \\times d = 4 \\times 1024 \\times 1024\\)\n4.2M\n\n\nLoRA matrices (per layer)\n\\(4 \\times (d \\times r + r \\times d) = 4 \\times 2 \\times 1024 \\times 16\\)\n131K\n\n\n8 adapted layers\n\\(8 \\times 131K\\)\n~1M trainable\n\n\n\nResult: ~1M trainable parameters vs. 500M frozen—a 500-fold reduction.\nStep 4: Validate. Compare against frozen features baseline. If validation performance plateaus, try rank 32 or extend to feed-forward weights. If performance matches frozen features, the task may not require adaptation beyond linear probing.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-layer-selection",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-layer-selection",
    "title": "10  Adaptation Strategies",
    "section": "10.2 Layer Selection for Embedding Extraction",
    "text": "10.2 Layer Selection for Embedding Extraction\nA research team attempting to use HyenaDNA for splice site classification discovers an unexpected problem. Following standard practice from encoder models, they extract embeddings from the final transformer layer and train a linear classifier. Performance barely exceeds random guessing. Frustrated, they try layer 6 of 12 on a hunch and accuracy jumps by 15 percentage points. Layer 4 performs better still for their particular task. The team has stumbled onto a systematic challenge that distinguishes decoder-based foundation models from their encoder counterparts: the optimal layer for embedding extraction varies dramatically by task, and the final layer is often the worst choice.\nThis phenomenon, sometimes called the layer hunting problem, arises from a fundamental asymmetry between how encoder and decoder models are trained. Encoder models like DNABERT and Nucleotide Transformer are optimized to produce representations useful for reconstructing masked tokens from bidirectional context. Every layer contributes to this reconstruction, and the final layer aggregates information specifically designed to support prediction. The [CLS] token or mean-pooled final layer representations work reliably across diverse downstream tasks because the pretraining objective directly shaped these representations for general utility.\nDecoder models face a different optimization pressure. The next-token prediction objective trains the final layer specifically to predict vocabulary distributions over the next token. This specialization is precisely what enables fluent generation, but it creates representations optimized for a narrow purpose rather than general-purpose embeddings. The final layer learns to transform rich intermediate representations into the specific format needed for token prediction, discarding information irrelevant to that task but potentially critical for downstream classification or regression.\n\n10.2.1 The Encoder Advantage\nJawahar et al. (Jawahar, Sagot, and Seddah 2019) demonstrated that BERT develops an interpretable layer hierarchy: lower layers encode surface features (word length, capitalization), middle layers capture syntactic structure (constituency, dependency relations), and upper layers represent semantic content (coreference, semantic roles). This progression means practitioners can make principled choices about layer selection based on task requirements. Tasks requiring syntactic understanding benefit from middle layers; tasks requiring semantic similarity benefit from upper layers. Crucially, the final layer remains a reasonable default because it integrates information from all levels while retaining semantic content useful for most applications.\nThe bidirectional attention mechanism ensures that every position’s representation incorporates information from the entire sequence at every layer. A nucleotide’s representation in layer 12 reflects constraints from both upstream promoter elements and downstream coding sequence. This global integration makes encoder representations naturally suited for tasks where context on both sides matters, which describes most genomic classification problems. Variant effect prediction, transcription factor binding, and splice site recognition all benefit from knowing what lies both before and after the position of interest.\nEncoder models also exhibit relatively stable layer-wise performance for frozen feature extraction. While middle layers sometimes outperform final layers for specific tasks, the differences are typically modest (a few percentage points) and the final layer rarely fails catastrophically. Practitioners can extract final-layer embeddings with reasonable confidence that performance will be competitive, reserving layer search for optimization rather than treating it as a requirement for basic functionality.\n\n\n10.2.2 The Decoder Dilemma\n\n\n\n\n\n\nDifficulty Warning\n\n\n\nThis section explains why decoder models require careful layer selection for classification tasks. The underlying concepts involve understanding how causal attention and next-token prediction objectives shape representations differently than bidirectional encoders. If you are less familiar with these architectural differences, consider reviewing Section A.5 before proceeding.\n\n\nDecoder models trained with causal attention create fundamentally different representation hierarchies. Each layer can only integrate information from preceding positions due to the causal mask. Position 500 in a 1000-token sequence has rich representations of the first 499 tokens but no information about the following 500. This asymmetry propagates through layers, creating representations that emphasize historical context over global sequence properties.\nThe next-token prediction objective compounds this asymmetry by specializing the final layers for a specific output format. Consider what the final layer must learn: transform the current hidden state into a probability distribution over vocabulary tokens. This transformation discards information about the input sequence that is irrelevant for predicting the immediate next token. Evolutionary conservation patterns 200 positions upstream, motif co-occurrence statistics, and global sequence composition may all inform intermediate representations but contribute nothing to next-token prediction and can be safely discarded by the final layer.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA decoder model has 12 layers. You are using it for promoter classification (binary: promoter vs. non-promoter). Based on the layer hunting problem, which layers would you expect to perform best for this task, and why? What would you predict about the final layer’s performance?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nYou would expect intermediate layers (roughly layers 4-8, the middle third of the network) to perform best for promoter classification. These layers contain general-purpose sequence representations before they are transformed into next-token predictions. The final layer would likely perform poorly because it has been specialized to produce token probability distributions, discarding information about global sequence properties like “is this a promoter?” that are irrelevant for predicting the immediate next token but crucial for classification.\n\n\n\n\n\nEmpirically, practitioners using decoder models for classification consistently find that intermediate layers outperform final layers, often dramatically. For HyenaDNA on regulatory element classification, layers in the middle third of the network frequently achieve the best linear probing accuracy. For GPT-style genomic models, the optimal layer can vary by 30-50% of network depth depending on the specific downstream task. A splice site classifier might perform best with layer 4 representations while a promoter classifier using the same model achieves optimal performance at layer 8. The task-dependence of optimal layer selection adds a hyperparameter dimension that does not exist for encoder models.\nThe following table summarizes how encoder and decoder architectures differ for embedding extraction:\n\n\n\nTable 10.1: Encoder vs. Decoder Models for Embedding Extraction\n\n\n\n\n\n\n\n\n\n\nProperty\nEncoder Models\nDecoder Models\n\n\n\n\nAttention pattern\nBidirectional (all positions see all positions)\nCausal (positions see only preceding context)\n\n\nPretraining objective\nMasked token reconstruction\nNext-token prediction\n\n\nFinal layer purpose\nGeneral-purpose representations\nToken probability distribution\n\n\nBest layer for classification\nFinal layer (usually)\nMiddle layers (varies by task)\n\n\nLayer search required?\nRarely; final layer works well\nYes; performance varies 15-30% across layers\n\n\nExample models\nDNABERT, Nucleotide Transformer, ESM\nHyenaDNA (autoregressive), GPT-style genomic models\n\n\n\n\n\n\n\n\n10.2.3 Practical Consequences\nThe layer hunting problem creates concrete challenges for deploying decoder-based foundation models. First, it increases computational cost: practitioners must evaluate downstream performance across all layers (or a representative subset) before committing to an adaptation strategy. A 12-layer model requires 12 separate linear probing experiments rather than one. Second, it complicates model comparison: reporting results from the best layer for each model can obscure whether the improvement comes from the model or from more thorough hyperparameter search. Third, it limits reproducibility: papers that report only final-layer performance for decoder models may dramatically underestimate achievable accuracy, while papers that report best-layer performance without specifying the layer make replication difficult.\nThe problem intensifies when decoder models grow deeper. A 12-layer model has 12 candidate extraction points; a 48-layer model has 48. The search space grows linearly with depth, and there is no theoretical guidance for narrowing the search a priori. Heuristics like “try middle layers first” help but do not eliminate the need for empirical validation on each new task.\n\n\n10.2.4 Layer Averaging and Weighted Combinations\nSeveral strategies address the layer hunting problem without exhaustive search. Layer averaging computes embeddings as the mean across all layers (or a subset), combining information from different levels of abstraction. This approach works surprisingly well in practice because it captures both syntactic features from early layers and more abstract features from later layers. The cost is that averaging dilutes task-specific signal present in particular layers, sometimes underperforming the optimal single layer by several percentage points.\nWeighted layer combinations learn task-specific weights for each layer’s contribution to the final embedding. Given layer representations \\(h_1, h_2, \\ldots, h_L\\), the combined representation is \\(h = \\sum_{l=1}^{L} \\alpha_l h_l\\) where \\(\\alpha_l\\) are learned weights (often softmax-normalized to sum to one). Why learn weights rather than simply averaging? Different tasks extract different types of information from pretrained models. A splice site classifier needs the local motif patterns strongly encoded in early layers; an enhancer-promoter association task needs the integrated contextual features from deeper layers. Uniform averaging dilutes task-relevant signal by mixing it with irrelevant representations. Learned weights allow the model to emphasize the layers where task-relevant information concentrates while suppressing layers that add noise. This approach was popularized by ELMo (peters_deep_2018?) and remains effective for foundation model adaptation. The weights themselves become informative: high weights on early layers suggest the task relies on surface features; high weights on late-middle layers suggest reliance on contextual integration.\nLearned layer weights add minimal parameters (one scalar per layer) while substantially reducing the manual hyperparameter search. The weights can be trained jointly with the downstream classifier using the same labeled data, requiring no additional supervision. For decoder models where optimal layer varies by task, learned combinations often match or exceed single-layer performance while eliminating the need to identify the optimal layer through trial and error.\n\n\n10.2.5 Systematic Layer Probing\nWhen using decoder models for transfer learning, systematic layer probing should precede any adaptation strategy that depends on embedding quality. The procedure is straightforward: extract representations from each layer for the downstream task’s training data, train identical lightweight classifiers (linear or shallow MLP) on each layer’s representations, and evaluate on held-out validation data. The layer achieving the best validation performance indicates where task-relevant information concentrates.\nThis probing step reveals not just the optimal layer but the shape of performance across layers. A sharp peak suggests highly localized task-relevant features; broad performance across middle layers suggests distributed representation. Monotonically increasing performance toward middle layers (then decreasing toward the final layer) is the typical pattern for decoder models on classification tasks. Anomalous patterns (best performance at layer 1, or best performance at the final layer) warrant investigation: they may indicate task-pretraining alignment issues or data quality problems.\nFor genomic models specifically, probing results often correlate with task properties. Tasks requiring recognition of local sequence motifs (transcription factor binding) show best performance in earlier layers where positional patterns are most directly encoded. Tasks requiring integration of broader context (enhancer-promoter association, long-range regulatory effects) show best performance in deeper middle layers where more context has been accumulated through the causal attention stack. Tasks most aligned with the next-token prediction objective (predicting the next nucleotide) show best performance in later layers, as expected.\n\n\n\n\n\n\n\n\nEncoder model layer-wise performance\n\n\n\n\n\n\n\nDecoder model layer-wise performance\n\n\n\n\n\n\nFigure 10.2: Layer-wise probing reveals fundamental differences between encoder and decoder architectures. (A) Encoder models like DNABERT show relatively stable performance across layers, with the final layer providing reliable representations for classification. The bidirectional attention mechanism integrates information from the entire sequence at every layer. (B) Decoder models like HyenaDNA show an inverted-U pattern, with peak performance at intermediate layers and degraded performance at the final layer. The next-token prediction objective specializes final layers for a narrow purpose, creating what practitioners call the “layer hunting problem”: optimal embedding extraction requires systematic search across layers rather than defaulting to final-layer representations.\n\n\n\n\n\n10.2.6 Implications for Model Selection\nThe layer hunting problem should inform model architecture selection, not just adaptation strategy. When downstream applications primarily involve classification, regression, or embedding-based retrieval (most clinical genomics applications), encoder architectures offer practical advantages beyond their representational benefits. The reliable performance of final-layer embeddings simplifies deployment pipelines, reduces hyperparameter search burden, and improves reproducibility. The bidirectional context that encoders provide aligns naturally with variant interpretation, where surrounding sequence on both sides determines functional impact.\nDecoder architectures remain essential when generation is the primary goal: designing novel regulatory sequences, sampling protein variants, or producing synthetic training data. For these applications, the final layer’s specialization for next-token prediction is a feature rather than a bug. Hybrid strategies that use decoder models for generation but encoder models (or carefully selected decoder layers) for classification can capture benefits of both architectures, though at the cost of maintaining multiple models.\nWhen decoder models must be used for classification (perhaps because they offer superior long-context handling or because they are the only available pretrained model for a particular sequence type), the layer hunting cost should be budgeted explicitly. Plan for layer-wise probing experiments. Consider learned layer weighting from the start. Report which layer produced reported results, and consider reporting performance across layers to enable fair comparison with future work.\n\n\n10.2.7 Cross-Reference to Pretraining Objectives\nThe layer hunting problem is a direct consequence of pretraining objective choice, connecting this practical deployment consideration back to the foundational decisions examined in Chapter 8. Masked language modeling trains all layers to support bidirectional context integration, producing representations useful for diverse downstream tasks throughout the network. Next-token prediction trains final layers for a specific output format, creating the representation collapse that makes layer search necessary. Understanding this connection helps practitioners anticipate adaptation challenges before committing to a foundation model: if your downstream tasks are primarily predictive rather than generative, the reliable final-layer embeddings of encoder models may outweigh other architectural considerations.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-full-finetuning",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-full-finetuning",
    "title": "10  Adaptation Strategies",
    "section": "10.3 Full Fine-Tuning",
    "text": "10.3 Full Fine-Tuning\nWhen Avsec et al. sought to predict gene expression from sequence across hundreds of cell types, they required a model capturing tissue-specific regulatory logic unavailable from any generic pretrained representation (Avsec et al. 2021). With millions of labeled examples spanning thousands of genomic tracks, they could afford to update all model parameters, reshaping internal representations entirely for their specific predictive task. Constrained adaptation would have left tissue-specific regulatory patterns unlearned.\nFull fine-tuning offers maximum flexibility: every parameter becomes tunable, enabling the model to learn whatever features the target task requires regardless of pretraining emphasis. This flexibility comes with risks proportional to its power.\n\n10.3.1 Making Full Fine-Tuning Work\nFull fine-tuning updates all model parameters during adaptation but requires careful attention to optimization dynamics. Learning rates must be substantially lower than during pretraining (often 10 to 100 times smaller) to avoid catastrophically disrupting learned representations in early training steps (Howard and Ruder 2018). Gradual unfreezing, where top layers update first and deeper layers progressively join training, helps preserve low-level features (local motifs, basic sequence statistics) while allowing high-level task-specific adjustment. Why does unfreezing order matter? Early layers encode fundamental sequence patterns—dinucleotide frequencies, basic motifs, compositional statistics—that are broadly useful across tasks. These patterns are hard to relearn and easy to destroy with large gradient updates. Later layers encode task-specific combinations of these features and are the layers that most need adaptation. By training top layers first while keeping bottom layers frozen, gradual unfreezing allows task-specific learning to proceed while protecting transferable low-level features. When bottom layers eventually unfreeze, the top layers have already learned stable task-specific patterns, producing smaller and more structured gradients that refine rather than overwrite the preserved features. Regularization through weight decay, dropout, and early stopping on validation data prevents overfitting to target datasets.\nThe approach suits scenarios when labeled datasets are large (tens of thousands of examples or more), when the target task diverges substantially from pretraining such that constrained adaptation proves insufficient, or when performance requirements justify computational investment. Enformer fine-tuning for new chromatin assays requires updating most parameters to capture assay-specific signal characteristics distinct from original training conditions. Expression prediction across novel cell types benefits from full adaptation when sufficient tissue-specific data exists.\nThe risks of unconstrained adaptation are proportional to its flexibility. Catastrophic forgetting occurs when fine-tuning overwrites general knowledge useful for related tasks or out-of-distribution inputs; a model fine-tuned aggressively on lymphocyte data may lose performance on epithelial cells it previously handled well (McCloskey and Cohen 1989). Overfitting afflicts small target datasets, where the model memorizes training examples rather than learning generalizable patterns. Computational expense can be prohibitive for models with billions of parameters. When negative transfer occurs (pretraining initialization actually hurts optimization), full fine-tuning may underperform training from scratch despite the additional expense.\nThe conservative strategy is to start with simpler methods and escalate only when they demonstrably fail. Establish frozen feature baselines first. If frozen features outperform random initialization, try PEFT methods before committing to full fine-tuning. Compare fine-tuned models against properly-tuned from-scratch baselines on the same target data. Monitor for overfitting through validation curves and early stopping. The goal is achieving required performance with minimal adaptation complexity.\n\n\n10.3.2 The [CLS] Token and Sequence Aggregation\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider a 512-nucleotide sequence processed by a transformer, producing 512 embedding vectors (one per position). If you need to classify the entire sequence as “promoter” or “non-promoter,” how would you convert these 512 vectors into a single prediction? What are the tradeoffs of different approaches?\n\n\nSequence classification requires a fixed-size representation regardless of input length. A promoter classifier must produce the same prediction format whether the input spans 200 or 2,000 nucleotides. A pathogenicity model must output a single score whether analyzing a 50-residue peptide or a 3,000-residue multidomain protein. Transformers produce per-position representations: for a 512-token input, the model generates 512 embedding vectors, one for each position. Converting these variable-length outputs into fixed-size vectors suitable for classification constitutes the sequence aggregation problem.\nThe [CLS] token provides one solution, introduced in BERT and adopted widely across encoder architectures including DNABERT and Nucleotide Transformer (Devlin et al. 2019). The approach works like a note-taker sitting at the front of a meeting: this special token attends to everything being said (all sequence positions) and distills the discussion into a summary that captures the essential points. The approach prepends a special classification token to every input sequence before processing. This token participates in attention computations like any other position, attending to all sequence positions and being attended to by them. Unlike content tokens that represent actual nucleotides or amino acids, the [CLS] token carries no intrinsic meaning. Its representation emerges entirely from aggregating information across the sequence through the attention mechanism.\nThe critical insight is that training shapes the [CLS] representation specifically for sequence-level tasks. During BERT’s pretraining, the [CLS] token was used for next-sentence prediction, requiring it to encode information sufficient to determine whether two sentences were contiguous in the original text. This training pressure transforms the [CLS] position into a natural aggregation point: its final-layer representation captures sequence-level properties distilled from positional representations throughout the network. When DNABERT applies this architecture to genomic sequences, the [CLS] token learns to aggregate regulatory signals, motif patterns, and compositional features into a single vector suitable for downstream classification.\nThe computational mechanism is straightforward. For an input sequence of \\(n\\) tokens, the model prepends [CLS] to create an \\((n+1)\\)-token input. After processing through all transformer layers, the final representation at position 0 (the [CLS] position) serves as the sequence embedding. A linear classifier or shallow neural network trained on this embedding produces the final prediction. The [CLS] approach adds exactly one token to the input, creating negligible computational overhead while providing a principled aggregation mechanism shaped by pretraining.\nThe [CLS] token’s effectiveness depends on the pretraining objective. When pretraining includes explicit sequence-level tasks (next-sentence prediction in BERT, similar objectives in some genomic models), the [CLS] representation receives direct training signal to encode sequence properties. When pretraining uses only token-level objectives like masked language modeling, the [CLS] representation is shaped indirectly through its participation in attention. DNABERT used masked language modeling without an explicit sequence-level pretraining task, yet its [CLS] representations still proved effective for downstream classification, suggesting that attention-based aggregation suffices even without task-specific pretraining signal (Ji et al. 2021).\n\n\n10.3.3 Mean Pooling and Alternatives\nMean pooling offers a simpler alternative: average all per-position embeddings to obtain a single sequence representation. For a sequence with token representations \\(h_1, h_2, \\ldots, h_n\\), the pooled representation is simply \\(\\bar{h} = \\frac{1}{n}\\sum_{i=1}^{n} h_i\\). This approach requires no special tokens, no architectural modifications, and no assumptions about which position aggregates sequence information. Every position contributes equally to the final representation.\nMean pooling often matches or exceeds [CLS] performance for genomic and protein sequences, particularly when pretraining did not include explicit sequence-level objectives (naderializadeh_aggregating_2025?). The explanation lies in information distribution across positions. In natural language, sentence meaning concentrates in specific tokens: the subject, main verb, and key modifiers carry most semantic content while articles and prepositions contribute less. The [CLS] token can learn to weight positions according to their informativeness. In genomic sequences, relevant information may distribute more uniformly: every nucleotide in a transcription factor binding site contributes to recognition, every residue in a protein domain contributes to function. Mean pooling captures this distributed signal naturally, while [CLS] must learn through attention what mean pooling provides by construction.\nMax pooling takes element-wise maxima across positions, capturing the strongest activation for each embedding dimension regardless of where it occurs. For regulatory element classification, max pooling can identify whether any position contains a strong motif signal, potentially outperforming mean pooling when a single strong feature determines the label. The tradeoff is sensitivity to outliers and potential loss of information about feature co-occurrence: max pooling cannot distinguish a sequence with one strong signal from a sequence with many moderate signals.\nAttention-based pooling learns to weight positions dynamically, computing attention scores that determine each position’s contribution to the final representation (hoang_locality-aware_2025?). This generalizes both mean pooling (uniform weights) and [CLS] aggregation (learned weights through attention to a special token). Attention pooling adds parameters but can capture position-dependent importance when the downstream task requires it. For protein sequences, attention pooling has shown advantages over both [CLS] and mean pooling for tasks where specific regions (active sites, binding interfaces) determine function, allowing the model to focus on relevant positions while downweighting uninformative regions.\n\n\n\n\n\n\nKey Insight: Matching Aggregation to Information Distribution\n\n\n\nThe optimal aggregation strategy depends on how task-relevant information distributes across the sequence:\n\nLocalized signal (e.g., a single binding motif determines the label): Max pooling or attention pooling excels\nDistributed signal (e.g., overall sequence composition matters): Mean pooling works well\nLearned weighting (e.g., some positions matter more than others): [CLS] token or attention pooling adapts\n\nFor genomic sequences, information often distributes more uniformly than in natural language, making mean pooling surprisingly competitive with the more complex [CLS] approach.\n\n\n\n\n10.3.4 Practical Considerations for Genomic Sequences\nGenomic sequence properties create specific considerations for aggregation strategy choice. Sequences often contain substantial length variation: promoter regions might span hundreds of base pairs while enhancer elements vary from tens to thousands. Mean pooling implicitly normalizes for length (the denominator scales with sequence size), while [CLS] representations can encode absolute length information through attention patterns. For tasks where length itself is informative, this distinction matters.\nRepetitive elements present another challenge. Genomic sequences frequently contain Alu elements, LINE repeats, and other repetitive content that may dominate mean-pooled representations simply through their abundance. The [CLS] token can learn to downweight repetitive regions if they are uninformative for the classification task, while mean pooling treats all positions equally regardless of their uniqueness or informativeness.\nThe choice between [CLS] and mean pooling often comes down to empirical validation on the specific task. For DNABERT applied to chromatin accessibility prediction, Ji et al. found that [CLS] representations achieved strong performance, but subsequent work with other DNA language models has shown comparable or superior results with mean pooling (Dalla-Torre et al. 2023). The Nucleotide Transformer evaluation suite includes comparisons across pooling strategies, generally finding modest differences that vary by task. A pragmatic approach extracts both [CLS] and mean-pooled representations during initial experiments, selecting the better performer for production deployment.\nFor protein language models, the comparison is similarly equivocal. ESM-2 embeddings work well with both [CLS] and mean pooling for most classification tasks. Recent work on optimal transport-based aggregation suggests that both standard approaches lose information present in per-residue representations, motivating more sophisticated aggregation schemes for tasks requiring fine-grained sequence understanding (naderializadeh_aggregating_2025?). These advanced methods add complexity and computational cost; whether the improvement justifies the overhead depends on task requirements and deployment constraints.\nWhen using decoder models for classification, the aggregation question becomes more complex. Decoder architectures typically lack a [CLS] token because their training objective (next-token prediction) does not require sequence-level representations. The final token’s representation aggregates information from all preceding positions due to causal attention, making it a natural candidate for sequence embedding. Mean pooling over decoder representations faces the asymmetry problem discussed in Section 10.2: later positions have richer context than earlier positions, creating systematic bias in averaged representations. Some practitioners mean-pool only the final portion of the sequence where representations have accumulated sufficient context, though optimal truncation points vary by model and task.\nThe interaction between pooling strategy and layer selection deserves attention. For encoder models, [CLS] and mean pooling both work reasonably well with final-layer representations because the pretraining objective shaped all positions for general utility. For decoder models, the optimal layer for [CLS]-style aggregation (using the final token) may differ from the optimal layer for mean pooling, adding another dimension to the hyperparameter search. When computational budget permits, systematic evaluation across both pooling strategies and layer choices provides the most reliable path to effective transfer.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-choosing-strategy",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-choosing-strategy",
    "title": "10  Adaptation Strategies",
    "section": "10.4 Choosing an Adaptation Strategy",
    "text": "10.4 Choosing an Adaptation Strategy\nThe preceding sections described what each adaptation approach does; here we address when to use each. Two factors dominate the decision: how much labeled data exists, and how closely the target task aligns with pretraining objectives. Figure 10.3 provides a decision framework, but the underlying logic is straightforward.\nData quantity determines what is possible. With fewer than 500 labeled examples, linear probing represents the only viable approach; more complex adaptation overfits catastrophically. Between 500 and 5,000 examples, PEFT methods offer favorable tradeoffs, introducing enough flexibility to improve over frozen features while maintaining implicit regularization. Above 10,000 examples, full fine-tuning becomes viable and may be necessary when target tasks diverge substantially from pretraining. Task similarity determines what is necessary. When targets closely resemble pretraining patterns (predicting transcription factor binding after sequence pretraining), frozen features often suffice. When tasks diverge moderately (tissue-specific expression after genome-wide pretraining), PEFT enables selective adaptation. When tasks fundamentally differ from pretraining (three-dimensional chromatin contacts from sequence-only pretraining), full fine-tuning may be required to learn features the pretraining objective never emphasized. Computational constraints impose practical limits: linear probing requires minutes on CPUs, LoRA requires hours on single GPUs, and full fine-tuning of large models requires days on multiple GPUs.\nThe following table summarizes the key tradeoffs across adaptation strategies:\n\n\n\nTable 10.2: Adaptation Strategy Selection Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrategy\nTrainable Parameters\nData Required\nCompute Cost\nOverfitting Risk\nBest When…\n\n\n\n\nLinear probing\nTask head only (thousands)\n&lt; 500 examples\nMinutes (CPU)\nLow\nTask aligns with pretraining; limited data\n\n\nLoRA / Adapters\n1-5% of model (millions)\n500-5,000 examples\nHours (1 GPU)\nModerate\nModerate data; need some flexibility\n\n\nFull fine-tuning\n100% of model (billions)\n&gt; 10,000 examples\nDays (multi-GPU)\nHigh\nLarge data; task diverges from pretraining\n\n\nFrom scratch\n100% of model\n&gt; 100,000 examples\nDays-weeks\nTask-dependent\nNo suitable pretrained model; abundant data\n\n\n\n\n\n\nThese heuristics indicate which strategies merit trying first, but empirical validation supersedes any rule. No formula reliably predicts which approach will succeed for a specific combination of model, task, and data. The conservative path is to establish frozen feature baselines first, escalate to PEFT when frozen features prove insufficient, and reserve full fine-tuning for cases where simpler methods demonstrably fail and sufficient data exists to justify the risk.\n\n\n\n\n\n\nDecision framework for adaptation strategy selection\n\n\n\n\nFigure 10.3: Decision framework for adaptation strategy selection. Data quantity dominates the decision: with fewer than 500 labeled examples, only linear probing avoids catastrophic overfitting; between 500 and 5,000 examples, parameter-efficient methods like LoRA offer favorable tradeoffs; above 10,000 examples, full fine-tuning becomes viable. Task similarity to pretraining and computational constraints further refine the choice. Terminal nodes indicate recommended strategies with expected tradeoffs: linear probing requires minimal compute but limits flexibility; LoRA balances adaptation capacity with regularization; full fine-tuning offers maximum flexibility at highest overfitting risk; from-scratch training remains appropriate when no suitable pretrained model exists.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-domain-shift",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-domain-shift",
    "title": "10  Adaptation Strategies",
    "section": "10.5 Domain Shift and Cross-Context Transfer",
    "text": "10.5 Domain Shift and Cross-Context Transfer\nThe CYP2D6 gene encodes a cytochrome P450 enzyme metabolizing approximately 25% of clinically used drugs, including codeine (where poor metabolizers experience no analgesic effect) and tamoxifen (where poor metabolizers show reduced breast cancer treatment efficacy) (Gaedigk et al. 2018). CYP2D6 poses particular challenges for variant interpretation due to its complex structural variation and population-specific haplotypes (see Chapter 28 for clinical workflow considerations). A foundation model trained on human genomic data and adapted for CYP2D6 variant classification might achieve 90% accuracy on common variants well-represented in training data. But the variants most important clinically are rare: novel star alleles in underrepresented populations, structural variants creating gene duplications or deletions, population-specific haplotypes absent from reference databases. Domain shift between training and deployment distributions creates systematic blind spots precisely where clinical stakes are highest.\n\n10.5.1 Types of Domain Shift in Genomics\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading about specific types of domain shift, consider: if you trained a variant pathogenicity model on European-ancestry samples and deployed it on African-ancestry samples, what kinds of failures might you expect? Why might performance degrade even if the underlying biology is the same?\n\n\nThree types of domain shift commonly afflict genomic transfer learning, each creating different patterns of failure.\nEvolutionary divergence creates the most fundamental barrier to cross-species transfer. When models trained on human sequences are applied to other organisms, differences in regulatory syntax, motif grammar, and functional constraints can undermine predictions entirely. Human-to-mouse regulatory prediction works reasonably for conserved housekeeping genes but fails for rodent-specific enhancers that never existed in the human training distribution. Strategies for cross-species success include pretraining on multi-species data to learn conservation patterns, fine-tuning with species-specific adapters, and focusing on highly conserved features (core promoter elements, splice site consensus sequences) that transfer more readily than species-specific innovations (Kelley 2020).\nTissue-specific regulatory programs create equally severe challenges for cross-tissue transfer. Chromatin accessibility varies dramatically across tissues, with thousands of tissue-specific enhancers and repressors controlling cell-type identity. Models trained predominantly on one tissue may miss regulatory logic specific to others. Effective approaches include shared backbones with tissue-specific prediction heads (each head learns tissue-specific transformations of shared representations), tissue-conditional models accepting tissue identity as additional input, and meta-learning frameworks training across many tissues to extract general principles applicable to novel tissue types (Avsec et al. 2021).\nPopulation structure introduces a form of domain shift with direct clinical consequences. Models trained predominantly on European-ancestry data perform systematically worse when applied to other populations, with polygenic scores showing 40 to 75 percent reductions in prediction accuracy for African-ancestry individuals (Section 3.7). The mechanisms are both technical and biological: linkage disequilibrium patterns differ across populations (making tag SNPs poor proxies for causal variants in non-training ancestries), allele frequencies shift (variants common in training data may be rare elsewhere), and effect sizes may genuinely differ due to gene-environment interactions or genetic background effects (Martin et al. 2019). Foundation models offer potential improvement by learning sequence-based features that transfer across ancestries without relying on population-specific LD patterns, but this potential remains unrealized without explicit evaluation across diverse populations. Multi-ancestry pretraining and ancestry-stratified fine-tuning represent emerging approaches, though the fundamental data imbalance (78% of GWAS participants are European despite comprising 16% of the global population) constrains what any model can learn about underrepresented groups. The broader implications of ancestry confounding receive comprehensive treatment in ?sec-ch22-ancestry-confounding.\nTechnical variation across sequencing platforms, library preparation protocols, and analysis pipelines creates batch effects that masquerade as biological signal. Different instruments produce distinct error profiles; capture kits determine which regions receive adequate coverage; alignment algorithms and variant callers make different decisions at ambiguous positions. When samples from a particular batch disproportionately represent a specific label class (cases sequenced at one center, controls at another), models learn to distinguish batches rather than biology (yu_assessing_2024?). Foundation models are not immune: a DNA language model pretrained on data from one sequencing platform may encode platform-specific artifacts in its representations, producing embeddings that cluster by sequencing center rather than by biological phenotype. Detection requires comparing embeddings across batches using visualization or statistical divergence measures; mitigation strategies include explicit batch correction during preprocessing, domain-adversarial training that penalizes batch-predictive features, and careful data curation ensuring batch balance across labels (varoquaux_preventing_2021?). The relationship between batch effects and institutional confounding is explored further in Section 22.7.1.\nDifferences in molecular readout technology create a more subtle form of shift that affects cross-assay transfer. ChIP-seq and ATAC-seq both measure chromatin state but with different biochemistry, resolution, and signal characteristics. Models trained on one assay may learn assay-specific artifacts rather than underlying biology, producing predictions that fail when applied to related assays measuring the same phenomenon differently. Multi-task pretraining across assays helps models distinguish biological signal from assay-specific noise.\n\n\n10.5.2 Detecting and Mitigating Shift\nDetecting domain shift before deployment prevents silent clinical failures. Statistical divergence measures comparing source and target distributions quantify distribution differences. Embedding visualizations (t-SNE or UMAP projections) reveal whether target examples fall within the source distribution or occupy unfamiliar regions of representation space. Monitoring performance on canary examples (known easy cases that should always be predicted correctly) provides early warning of severe shift during deployment.\nWhen domain shift is detected, mitigation strategies include domain-adaptive fine-tuning, importance weighting of training examples, and explicit modeling of shift through domain-adversarial training (Ganin et al. 2016). When shift is severe and cannot be mitigated, acknowledging that transfer is inappropriate for this context prevents overconfident deployment of models that will fail.\n\n\n\n\n\n\n\n\nEmbedding space visualization\n\n\n\n\n\n\n\nCalibration comparison\n\n\n\n\n\n\n\nPerformance degradation curve\n\n\n\n\n\n\nFigure 10.4: Detecting domain shift before deployment prevents silent clinical failures. (A) UMAP visualization of embedding space shows test examples colored by distance from training distribution. Out-of-distribution examples (red) occupy regions where model predictions cannot be trusted. (B) Calibration analysis reveals that in-distribution predictions follow the diagonal (well-calibrated) while out-of-distribution predictions fall below (overconfident). This overconfidence makes OOD failures particularly dangerous. (C) Performance degradation quantifies the relationship between distributional distance and accuracy, enabling threshold-based decisions about when to trust predictions versus abstain.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-minimal-data",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-minimal-data",
    "title": "10  Adaptation Strategies",
    "section": "10.6 Minimal-Data and Emerging Transfer Paradigms",
    "text": "10.6 Minimal-Data and Emerging Transfer Paradigms\nA geneticist studying a newly characterized neurodevelopmental disorder has identified 15 patients with variants in a previously unstudied gene. Functional studies confirm pathogenicity for 8 variants; the remaining 7 are benign. Training a classifier from 15 examples using standard supervised learning would be absurd, yet the clinical need for variant interpretation is immediate. Parents are waiting for diagnoses. Few-shot learning and zero-shot learning address these extreme data scarcity scenarios that characterize many genomic applications, where clinical urgency outpaces data availability. The rare disease diagnosis workflow in Chapter 28 illustrates how these methods integrate into clinical practice.\n\n10.6.1 Few-Shot Learning with Minimal Examples\nWhen only 10 to 100 examples per class exist, standard adaptation overfits catastrophically. The core insight of few-shot learning is that models can be trained explicitly for rapid adaptation by optimizing across many few-shot tasks during a meta-training phase (Finn, Abbeel, and Levine 2017). Model-Agnostic Meta-Learning (MAML) exemplifies this approach by finding parameter initializations that can be fine-tuned effectively from minimal data. The initialization represents a point in parameter space from which a few gradient steps reach good task-specific solutions. At deployment, the meta-trained model adapts to new tasks from a handful of labeled examples, having learned during meta-training what features are generally useful and how to adapt quickly.\nA simpler alternative avoids gradient updates at deployment entirely. Prototypical networks learn to embed sequences such that examples from the same class cluster together (Snell, Swersky, and Zemel 2017). At inference, class prototypes are computed as the mean embedding of the few available examples per class, and novel sequences are classified based on distance to prototypes. With 10 pathogenic and 10 benign variants as prototypes, novel variants are classified by which prototype cluster they fall nearest in embedding space. The approach requires no parameter updates during deployment, only forward passes to compute embeddings and distances.\n\n\n10.6.2 Zero-Shot Transfer Without Task-Specific Data\nThe most extreme adaptation scenario eliminates task-specific examples entirely. Zero-shot transfer makes predictions using only the pretrained model’s outputs, without any task-specific adaptation. For protein variant effect prediction, ESM log-likelihood ratios score variants by how much they reduce the model’s probability of the observed sequence (Meier et al. 2021). Variants that violate the model’s learned expectations for natural proteins (disrupting conserved residues, introducing destabilizing substitutions) receive low likelihood ratios, flagging them as potentially deleterious. This approach proves competitive with supervised methods for ClinVar pathogenicity prediction because evolutionary constraint (what masked language modeling learns to predict) correlates with functional importance (what pathogenicity classification measures). The ESM variant scoring methodology and its calibration to clinical thresholds are examined in Chapter 17.\n\n\n\n\n\n\nKey Insight: When Zero-Shot Works\n\n\n\nZero-shot transfer succeeds when the pretraining objective implicitly captures the target task. For protein variant effect prediction, masked language modeling learns evolutionary constraint, and evolutionary constraint correlates with pathogenicity. The model never saw pathogenicity labels during training, yet its predictions are clinically useful because it learned something closely related.\nThis insight guides model selection: ask not just “what was this model trained to do?” but “what did it implicitly learn that might transfer to my task?”\n\n\nZero-shot methods require strong alignment between pretraining objectives and target tasks. When this alignment exists (evolutionary constraint predicts pathogenicity), zero-shot approaches provide immediate predictions without any labeled data. When alignment is weaker (tissue-specific regulatory activity depends on factors beyond sequence conservation), few-shot learning with even a handful of examples typically outperforms zero-shot baselines. For most practical genomic applications, some labeled data improves predictions; few-shot rather than true zero-shot represents the realistic minimal-data regime.\n\n\n10.6.3 Emerging Approaches\nSeveral paradigms are extending the boundaries of what minimal-data transfer can achieve. Very large language models exhibit the capacity for in-context learning, performing tasks by observing demonstrations rather than through explicit fine-tuning (Brown et al. 2020). Early evidence suggests genomic foundation models at sufficient scale may exhibit similar behavior, classifying variants based on a few pathogenic and benign examples included in the input prompt. This could transform deployment: rather than training adapters or fine-tuning parameters, practitioners would provide examples of desired behavior at inference time.\nAdaptation need not be confined to training time. Test-time adaptation updates models during inference based on characteristics of test examples rather than freezing parameters after training (D. Wang et al. 2021). For genomic applications facing distribution shift between development and deployment populations, test-time adaptation could adjust model behavior to match deployment-specific characteristics without requiring labeled examples from the deployment distribution. A model developed on European-ancestry data could adapt its uncertainty calibration when encountering African-ancestry variants that differ from training distributions.\nPrivacy constraints have motivated development of federated transfer learning, which enables collaborative model development across institutions without sharing raw genomic data (Rieke et al. 2020). Institutions train local models on private patient data and share only aggregated parameter updates, enabling foundation models to learn from far more diverse data than any single institution can access while preserving patient privacy. This approach could help address the population bias in current genomic datasets by enabling contributions from institutions serving underrepresented populations (see Chapter 2 for discussion of population representation in genomic databases).\n\n\n10.6.4 Toward Theoretical Foundations\nTheoretical foundations for predicting transfer success based on measurable properties of source and target domains would reduce trial-and-error (Ben-David et al. 2010). Currently practitioners must empirically test whether transfer helps; theoretical guidance specifying when transfer will succeed based on domain divergence measures, task similarity metrics, or representation analysis could focus effort on promising applications and avoid wasted investment in doomed transfer attempts.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-label-imbalance",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-label-imbalance",
    "title": "10  Adaptation Strategies",
    "section": "10.7 Label and Class Imbalance",
    "text": "10.7 Label and Class Imbalance\n\n\n\n\n\n\nDifficulty Warning\n\n\n\nThis section addresses a pervasive challenge in clinical genomics: class imbalance. The material requires careful attention because imbalance affects every stage of the machine learning pipeline differently, and standard metrics can obscure catastrophic failures. If you are less familiar with precision, recall, and ROC/PR curves, consider reviewing Section 12.5.1 first.\n\n\nThe clinically important variants are precisely the ones that rarely appear in training data. A pathogenicity classifier might train on thousands of benign polymorphisms but only dozens of confirmed pathogenic variants in any given gene family. A splice disruption predictor might see millions of canonical splice sites but only hundreds of cryptic sites that cause disease. This extreme imbalance is not a data curation failure but a reflection of biology: pathogenic variants are rare because purifying selection removes them from populations, and rare variants are rarely observed because they are, by definition, rare. The challenge for transfer learning is that models adapted on imbalanced target data learn to predict the majority class with high confidence while failing on the minority class that matters most.\nClass imbalance creates problems at every stage of adaptation. During fine-tuning, gradient updates are dominated by the abundant class because most training examples belong to that class. A model fine-tuned on 10,000 benign variants and 100 pathogenic variants receives 100 times more gradient signal pushing it toward correct benign predictions than toward correct pathogenic predictions. Standard cross-entropy loss weights each example equally, so the model learns that predicting “benign” is almost always correct. Early stopping based on aggregate accuracy reinforces this bias: a model that classifies everything as benign achieves 99% accuracy and appears to have converged, yet provides no clinical value whatsoever.\nThe imbalance problem compounds across the genomic landscape. ClinVar contains roughly ten times more benign than pathogenic variants for well-studied genes like BRCA1, but the ratio exceeds 100:1 for less-studied genes where expert review is rare (Landrum et al. 2018). Population databases like gnomAD contain millions of variants, the vast majority benign common polymorphisms, while disease-causing variants constitute a tiny fraction concentrated in specific functional regions. When foundation models are adapted using these resources, the inherited imbalance creates systematic underconfidence for pathogenic predictions and systematic overconfidence for benign predictions.\n\n10.7.1 Manifestations During Transfer\nImbalance affects different adaptation strategies in different ways. Linear probing on frozen representations inherits whatever class structure exists in the embedding space. If pretrained representations do not separate rare pathogenic variants from common benign variants (because the pretraining objective emphasized patterns common in the training corpus, which are by definition not rare), no amount of reweighting during probe training can recover the missing information. The probing classifier may achieve perfect separation on training data through overfitting while failing completely on held-out pathogenic variants that occupy different regions of embedding space.\nParameter-efficient fine-tuning methods like LoRA can partially address imbalance by learning task-specific transformations, but they remain susceptible when the pathogenic signal is weak relative to the benign signal. If only a small number of adapter parameters are tuned and most gradients come from the majority class, the adapters learn transformations that improve majority-class predictions without capturing minority-class patterns. Increasing adapter rank provides more capacity but also more opportunity for overfitting to the few minority examples.\nFull fine-tuning offers the most flexibility to reshape representations for imbalanced tasks but carries the greatest overfitting risk. With 100 pathogenic examples and millions of model parameters, the model can memorize every pathogenic example while learning nothing generalizable about what makes variants pathogenic. The resulting model performs perfectly on training pathogenic variants and randomly on held-out pathogenic variants, a form of catastrophic overfitting invisible to training metrics dominated by the benign class.\n\n\n10.7.2 Mitigation Strategies\n\n\n\n\n\n\nKnowledge Check\n\n\n\nConsider a variant classifier trained on 10,000 benign variants and 100 pathogenic variants. The model achieves 99% accuracy on the test set. Is this model performing well? What additional information would you need to assess its clinical utility?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nNo, 99% accuracy does not indicate good performance in this imbalanced scenario. A trivial baseline that predicts “benign” for every variant would achieve 99% accuracy (10,000/10,100 correct). You would need to see: (1) sensitivity and specificity separately to know if the model actually detects pathogenic variants, (2) auPRC (not just auROC) which is sensitive to class imbalance, (3) confusion matrix showing true/false positives and negatives, and (4) performance at clinically relevant decision thresholds. The model might be achieving high accuracy by simply predicting benign for everything, making it clinically useless.\n\n\n\n\n\nAddressing class imbalance requires intervention at data, loss, and evaluation levels. No single strategy suffices; effective pipelines combine multiple approaches.\nResampling strategies modify the training distribution to achieve more balanced class representation. Oversampling the minority class by duplicating rare examples increases their influence on gradients, though excessive oversampling causes overfitting to specific minority examples. SMOTE and related methods generate synthetic minority examples by interpolating between existing examples, but interpolation in sequence space or embedding space may produce biologically implausible variants that mislead the model (chawla_smote_2002?). Undersampling the majority class reduces imbalance but discards potentially useful information; stratified undersampling that preserves the diversity of benign variants across variant types and genomic contexts performs better than random undersampling.\nLoss reweighting assigns higher penalties to minority-class errors. Inverse frequency weighting multiplies the loss for each class by the inverse of its training frequency, so a class comprising 1% of training data receives 100 times the loss weight. Why does reweighting help when the gradient directions remain unchanged? Consider what drives learning: the model parameters adjust to reduce loss, and the magnitude of adjustment scales with loss value. Without reweighting, 99 benign examples each contributing loss 0.1 produce aggregate gradient magnitude 9.9 from the benign class, while 1 pathogenic example contributing loss 0.1 produces aggregate gradient magnitude 0.1 from the pathogenic class. The benign signal overwhelms the pathogenic signal by 99:1. With inverse frequency weighting, that single pathogenic example contributes weighted loss \\(0.1 \\times 100 = 10\\), matching the benign aggregate. Now both classes contribute equally to gradient updates, and the model can learn both decision boundaries. Class-balanced loss variants address the diminishing returns of adding more majority-class examples, weighting by effective number of samples rather than raw counts (cui_class-balanced_2019?). Focal loss downweights easy examples (confident correct predictions) to focus learning on hard examples, many of which are minority-class instances that the model currently misclassifies (lin_focal_2017?). These loss modifications change gradient magnitudes without changing gradient directions, so they work best when the model has sufficient capacity to learn minority-class patterns if given appropriate training signal.\nThreshold adjustment and calibration address the deployment manifestation of imbalance. A model trained on imbalanced data learns decision boundaries skewed toward predicting the majority class. Adjusting the classification threshold post-training, typically by lowering the threshold for minority-class predictions, can recover sensitivity without retraining. Platt scaling and temperature scaling recalibrate predicted probabilities to match observed frequencies, essential when downstream applications depend on accurate probability estimates rather than just rankings (see Section 23.3 for calibration methods). The appropriate threshold depends on deployment prevalence, which may differ from training prevalence; a variant predictor trained on balanced batches but deployed where 0.1% of variants are pathogenic requires threshold adjustment to avoid overwhelming clinical workflows with false positives.\nTwo-stage approaches train separate models for different aspects of the problem. A first-stage model distinguishes clearly benign variants from potentially interesting variants, filtering the vast majority of benign variants at high specificity. A second-stage model, trained on the filtered subset where class balance is more favorable, distinguishes pathogenic from uncertain among the remaining candidates. This cascade architecture reduces imbalance at each stage while maintaining overall pipeline sensitivity, though it requires careful coordination to avoid error compounding across stages.\n\n\n\n\n\n\nPractical Guidance: Imbalance Mitigation Checklist\n\n\n\nWhen adapting models for imbalanced genomic classification:\n\nNever trust accuracy alone. A 99% accurate model may be clinically useless.\nReport sensitivity and specificity separately. These reveal what aggregate metrics hide.\nUse auPRC, not just auROC. auPRC is sensitive to imbalance; auROC is not.\nCombine strategies: Use class-weighted loss and threshold calibration and stratified evaluation.\nTest at deployment prevalence. Training-time metrics do not reflect clinical performance.\nConfidence intervals matter more for rare classes. With 50 pathogenic test examples, even a good model shows high variance.\n\n\n\n\n\n10.7.3 Evaluation Under Imbalance\nStandard metrics obscure imbalance-driven failures. Accuracy, which measures the fraction of correct predictions, reaches 99% when a model predicts “benign” for everything in a dataset with 1% pathogenic variants. This apparent success masks complete failure on the task that matters: identifying pathogenic variants.\nThe auPRC provides a more informative view for imbalanced classification. Unlike auROC, which measures pairwise ranking between one positive and one negative example, auPRC measures precision across recall levels where precision is explicitly sensitive to the number of false positives relative to true positives. When positives are rare, achieving high precision requires correctly ranking the vast majority of negatives below positives, not just typical negatives. A model moving from a balanced validation set to a 1000:1 imbalanced deployment setting will show stable auROC but collapsing auPRC, mirroring the explosion in false discovery rate that clinical users will experience (Section 12.5.1 examines this distinction in detail).\nStratified evaluation by class reveals whether aggregate metrics hide minority-class failure. Report sensitivity (true positive rate) and specificity (true negative rate) separately rather than combining them into a single number. Report precision at clinically relevant recall thresholds: if a diagnostic pipeline requires 95% sensitivity for pathogenic variants, what precision does the model achieve at that operating point? This stratified reporting reveals the tradeoffs that aggregate metrics obscure.\nConfidence intervals matter more for minority-class metrics. With 100 pathogenic variants in the test set, sensitivity estimates have wide confidence intervals purely from sampling variation. A model with true 80% sensitivity might show anywhere from 70% to 90% sensitivity on a particular test set by chance alone. Multiple test sets, bootstrap confidence intervals, or analytic interval calculations (Wilson score intervals for proportions) provide appropriate uncertainty quantification. Presenting point estimates without intervals overstates confidence in minority-class performance.\n\n\n10.7.4 Imbalance as Fundamental Constraint\nClass imbalance in genomic transfer learning reflects a fundamental biological reality rather than a correctable data curation problem. Pathogenic variants are rare because evolution works. Deleterious mutations are removed from populations by natural selection, so the variants that remain and accumulate in databases are predominantly benign. This creates a tension: the variants we most need to classify are the variants we have the least data to learn from.\nThis constraint shapes realistic expectations for transfer learning. A foundation model pretrained on typical genomic sequence has learned patterns of typical sequence, not patterns of pathogenic deviation from typical sequence. Transfer to pathogenic variant classification asks the model to extrapolate to a distribution it has never seen. Techniques for handling class imbalance mitigate but cannot eliminate this fundamental challenge. When only 50 pathogenic variants exist for a gene family, no amount of loss reweighting or sampling strategy can substitute for the missing data. The most honest response may be appropriate uncertainty quantification and abstention on predictions where evidence is insufficient (see Section 23.7).\nThe clinical implications extend beyond model performance to workflow design. If a variant classifier has 80% sensitivity for pathogenic variants, 20% of disease-causing variants will be missed. For a rare disease diagnosis where a single pathogenic variant determines diagnosis, this false negative rate translates directly to missed diagnoses and delayed treatment. Understanding class imbalance as a structural constraint rather than a tunable hyperparameter is essential for setting appropriate clinical expectations and designing safety-net workflows that catch the variants models miss.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-diagnosing-transfer",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-diagnosing-transfer",
    "title": "10  Adaptation Strategies",
    "section": "10.8 Diagnosing Transfer: Validation and Failure Modes",
    "text": "10.8 Diagnosing Transfer: Validation and Failure Modes\nThe research team had done everything right. They selected a state-of-the-art DNA foundation model pretrained on diverse genomic sequences. They applied LoRA adaptation using 5,000 carefully curated training examples. Validation accuracy reached 88%. But when deployed on prospectively collected samples, performance collapsed to 62%, barely better than chance for a binary classification task. Transfer had failed. For the patients whose variants were misclassified during those weeks before the failure was detected, the consequences were real: delayed diagnoses, inappropriate treatments, unnecessary anxiety. Detecting transfer failure before deployment requires understanding its root causes.\n\n10.8.1 Diagnosing Negative Transfer\nNegative transfer occurs when pretraining actively hurts performance, producing adapted models worse than those trained from scratch on target data alone. Pretraining on human coding sequences may encode codon usage patterns and amino acid preferences that create false expectations when applied to bacterial sequences with different GC content and codon bias. Pretraining on healthy tissue samples may learn features of normal cellular function that prove misleading for cancer samples where regulatory programs are fundamentally altered. The pretrained initialization, rather than providing a useful starting point, creates an optimization landscape that leads to poor task-specific solutions (Z. Wang et al. 2019).\nDiagnostic steps identify whether transfer helps or hurts. The most fundamental comparison pits the adapted model against a from-scratch baseline trained on identical target data with equivalent hyperparameter tuning; if the pretrained model does not meaningfully outperform from-scratch training, transfer provides no benefit and the computational overhead of working with large pretrained models is wasted. Adaptation complexity should also scale appropriately: if linear probing fails, full fine-tuning rarely succeeds unless target data is very large, so simpler strategies should be exhausted before investing in more complex ones. Embedding visualization using dimensionality reduction can reveal whether pretrained representations contain task-relevant features; if target task classes are not separated in embedding space, the pretrained model may lack the representations the task requires. Finally, ablating pretraining entirely by comparing against randomly initialized models of identical architecture isolates whether pretrained weights provide value or whether architectural choices alone drive performance.\nWhen diagnostics reveal fundamental mismatches, several remediation strategies apply. Task-specific pretraining on data more closely aligned with target requirements can bridge the gap; pretraining specifically on regulatory regions for regulatory prediction tasks rather than genome-wide pretraining may produce more suitable representations. Hybrid architectures combining pretrained and randomly-initialized components allow selective use of transfer where it helps while avoiding its limitations elsewhere. Trying alternative foundation models whose pretraining objectives better match task requirements may reveal that the problem was model selection rather than transfer learning generally. And accepting that transfer provides no benefit for this specific task, proceeding with from-scratch training, remains a valid conclusion when evidence supports it.\n\n\n10.8.2 Validation and Common Pitfalls\nA research group reports that their foundation model adaptation achieves 95% accuracy for splice variant classification, far exceeding previous methods. Six months later, clinical deployment reveals performance closer to 70%, with systematic failures on the rare variants that matter most. The initial evaluation was not wrong, but it was misleading. Proper validation separates genuine transfer success from evaluation artifacts that dissolve on contact with clinical reality. The benchmark landscape and its limitations are examined in Chapter 11, evaluation methodology in Chapter 12, and systematic sources of inflated performance in Chapter 12.\n\n\n\n\n\n\n\n\nData leakage through overlap\n\n\n\n\n\n\n\nTemporal leakage\n\n\n\n\n\n\n\n\n\nBaseline comparison issues\n\n\n\n\n\n\n\nStratified performance hidden\n\n\n\n\n\n\nFigure 10.5: Common validation pitfalls that inflate transfer learning claims. (A) Overlap between pretraining data and benchmarks creates leakage, allowing models to “remember” test examples. (B) Temporal leakage occurs when training includes information from after benchmark creation. (C) Weak baseline comparisons exaggerate transfer benefits; fair evaluation requires properly-tuned from-scratch baselines. (D) Aggregate metrics conceal stratified failures; rare variants may show near-random performance while being masked by high accuracy on common variants. Together, these pitfalls can make ineffective transfer appear successful until clinical deployment reveals the failures.\n\n\n\n\n\n10.8.3 Sources of Spurious Success\n\n\n\n\n\n\nStop and Think\n\n\n\nA paper reports 95% accuracy for a new variant classifier, substantially exceeding prior methods. Before accepting this claim, what questions would you ask about how the evaluation was conducted? What forms of data leakage or evaluation bias might inflate these results?\n\n\nTest set overlap with pretraining data creates artificial performance inflation. Foundation models trained on massive genomic corpora may inadvertently include sequences later used for evaluation. When benchmarking on variants that appeared in pretraining data (even if unlabeled at pretraining time), the model has seen the sequences and may have memorized relevant patterns. Verifying that test sequences were excluded from pretraining requires careful provenance tracking, which published benchmarks often lack (Sainz et al. 2023). Chromosome-based splits help but do not fully address the problem when pretraining spans multiple species or includes population-level diversity (see Chapter 12 for detailed treatment of data leakage).\nTemporal leakage uses future information unavailable at prediction time. Evaluating a variant pathogenicity model on variants annotated after training data was collected creates an unrealistically favorable setting; the model may have seen related variants or learned from the same evidence that later informed annotations. Temporal splits (training on variants discovered before a cutoff, evaluating on variants discovered afterward) provide more realistic assessment of prospective performance (Landrum et al. 2018).\nInappropriate baselines inflate apparent transfer benefits. Comparing adapted foundation models against weak or poorly-tuned from-scratch baselines makes transfer look more valuable than warranted. Strong baselines require equivalent hyperparameter tuning, appropriate architectures for the task, and sufficient training on the same target data. When properly-tuned CNNs match or exceed foundation model performance for a task, the additional complexity of pretrained models may not be justified.\nEvaluation practices that reveal true performance counter these pitfalls. Single-metric reporting obscures important performance characteristics: a model achieving 90% overall accuracy may show 95% accuracy on common variants and 50% accuracy on rare variants, with the clinically important rare cases hidden by aggregate metrics. Stratified evaluation by allele frequency, variant type, gene family, and other clinically relevant categories reveals whether transfer benefits generalize or concentrate in particular subgroups. Confidence interval reporting and multiple training runs reveal performance variability, since a single training run may produce misleadingly good or bad results through random initialization effects or data sampling. Testing on multiple independent datasets rather than a single benchmark reveals whether gains generalize beyond the specific evaluation setting.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-case-studies",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-case-studies",
    "title": "10  Adaptation Strategies",
    "section": "10.9 Case Studies in Transfer Learning",
    "text": "10.9 Case Studies in Transfer Learning\nTransfer succeeds when pretraining objectives align with downstream requirements; it fails when they diverge. Three successes and one failure illustrate the conditions that distinguish effective transfer from wasted effort.\n\n10.9.1 Successful Transfer: Alignment Between Pretraining and Task\nDNABERT demonstrates how pretraining-task alignment enables efficient transfer. Ji et al. pretrained the model using masked language modeling on \\(k\\)-mer tokenized human genomic sequence (Ji et al. 2021). For ATAC-seq peak classification, linear probes on [CLS] token embeddings achieved competitive performance with CNNs trained from scratch while requiring approximately 10 times less labeled data. Success reflected alignment between pretraining and target: both involve recognizing local sequence motifs (transcription factor binding sites, nucleosome positioning signals) that determine chromatin state. The pretrained representations already encoded the relevant patterns; the linear probe simply learned to separate accessible from inaccessible regions in this well-structured embedding space.\nESM illustrates how implicit alignment can support even zero-shot transfer. Rives et al. pretrained ESM on UniRef protein sequences using masked language modeling (Rives et al. 2021). For ClinVar pathogenicity classification, Meier et al. showed that zero-shot scoring based on variant effects on sequence likelihood proved competitive with supervised methods (Meier et al. 2021). Adding a linear probe on ESM embeddings improved performance further, but the zero-shot baseline was already strong. Success reflected implicit alignment: evolutionary constraint (what masked language modeling captures) correlates with functional importance (what pathogenicity measures). The pretraining objective, though never explicitly targeting variant classification, learned representations directly relevant to it.\nEnformer shows that transfer can succeed even when substantial fine-tuning is required, given sufficient data. Avsec et al. pretrained the model on thousands of chromatin and expression tracks spanning dozens of cell types (Avsec et al. 2021). Fine-tuning with tissue-specific prediction heads captured regulatory logic unavailable from frozen features, outperforming both frozen Enformer and from-scratch models trained on individual tissues. Success required both the large scale of pretraining (establishing general sequence-to-function mappings) and extensive fine-tuning data (enabling tissue-specific adaptation). With smaller fine-tuning datasets, the approach would have overfit; without diverse pretraining, the model would have lacked transferable regulatory knowledge.\n\n\n10.9.2 When Transfer Fails: Cross-Species Prediction\nModels pretrained on human regulatory sequences and applied to zebrafish enhancer prediction often underperform zebrafish-specific models despite the apparent relevance of regulatory sequence patterns (Kelley 2020). The failure reflects both sequence divergence (zebrafish regulatory motifs differ from human) and lineage-specific regulatory innovations (teleost-specific enhancers have no human homologs from which to transfer). Cross-species contrastive learning approaches (Section 8.5) offer one strategy for building representations that emphasize conserved features over species-specific patterns, but fundamental distributional mismatch cannot always be bridged.\nThe boundary between success and failure corresponds to evolutionary conservation: patterns shared across species transfer; species-specific patterns do not. Transfer succeeds for deeply conserved elements (core promoters, splice sites) but fails for lineage-specific regulatory logic. This case illustrates a general principle: when the target domain contains features absent from pretraining data, no adaptation strategy can manufacture missing knowledge. Recognizing these limits before deployment prevents confident predictions that are systematically wrong.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch10-adaptation.html#sec-ch10-conclusion",
    "href": "part_2/p2-ch10-adaptation.html#sec-ch10-conclusion",
    "title": "10  Adaptation Strategies",
    "section": "10.10 What Transfers, What Breaks",
    "text": "10.10 What Transfers, What Breaks\nTransfer learning amplifies the value of pretrained models by connecting learned representations to specific applications. A foundation model pretrained on billions of sequences encodes patterns that would require orders of magnitude more labeled data to learn from scratch. Effective transfer realizes this investment; ineffective transfer inherits hidden limitations without the promised benefits.\nThe risks are concrete. Domain shift between pretraining and deployment contexts causes silent failures: models trained on research cohorts may miscalibrate on clinical populations, models trained on one species may fail unpredictably on another, models trained on one assay technology may not generalize to its successor. These failures produce confident predictions that are systematically wrong, often in ways that correlate with clinically relevant subgroups. Detection through distribution divergence measures and embedding visualization can identify shift before deployment, but mitigation requires either domain-adaptive fine-tuning or acceptance that some shifts cannot be bridged.\nValidating transfer claims requires adversarial rigor. Test for contamination between pretraining and evaluation data through sequence-level deduplication. Implement temporal splits that respect real-world prediction scenarios. Compare against properly-tuned baselines trained from scratch with equivalent effort. Stratify performance by ancestry, variant type, and other clinically meaningful categories. The goal is establishing whether transfer provides genuine benefit under realistic deployment conditions, not optimizing for favorable benchmarks. Foundation model applications assume that transfer succeeds; the methods here determine whether that assumption holds for specific contexts.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nHow does LoRA achieve parameter-efficient adaptation through low-rank decomposition, and why does constraining adaptation to low-rank subspaces provide implicit regularization?\nExplain the layer hunting problem for decoder models. Why do intermediate layers often outperform final layers for classification, and how does this differ from encoder models?\nA classifier trained on 10,000 benign variants and 100 pathogenic variants achieves 99% accuracy. Why might this model be clinically useless despite high accuracy?\nWhat is domain shift, and how does it differ from data leakage? Give an example where a model experiences distribution shift without any leakage.\nWhen should you use linear probing versus LoRA versus full fine-tuning? What data quantity thresholds guide this decision?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nLow-Rank Adaptation: LoRA introduces two smaller matrices \\(A\\) and \\(B\\) whose product approximates the desired weight change: \\(W' = W + BA\\). During fine-tuning, only \\(A\\) and \\(B\\) receive gradient updates while \\(W\\) remains frozen. This reduces trainable parameters from hundreds of millions to just 2-5 million, achieving a 500-fold reduction in parameter count. The low-rank constraint provides implicit regularization because task-specific adaptations empirically lie in low-dimensional subspaces; forcing adaptation through limited dimensions prevents overfitting while capturing most required changes.\nLayer Hunting Problem: Decoder models are trained with next-token prediction, which specializes final layers to produce token probability distributions rather than general-purpose representations. This optimization discards information irrelevant for predicting the immediate next token but crucial for classification tasks. Intermediate layers (typically middle third of the network) contain richer general-purpose sequence representations before this specialization occurs, often outperforming final layers by 15-30%. Encoder models avoid this problem because bidirectional masked language modeling shapes all layers for general utility, making final-layer representations reliably effective for downstream tasks.\nClass Imbalance: With a 100:1 ratio of benign to pathogenic variants, a trivial baseline that predicts “benign” for every variant achieves 99% accuracy (10,000/10,100 correct). High accuracy masks complete failure on the clinically important minority class. The model might have zero sensitivity for pathogenic variants while achieving near-perfect specificity on benign variants, making it useless for clinical diagnosis. Proper evaluation requires sensitivity and specificity reported separately, auPRC (not just auROC), and performance at clinically relevant decision thresholds.\nDomain Shift vs. Leakage: Domain shift occurs when training and deployment distributions differ due to biological or technical variation (e.g., training on European-ancestry samples and deploying on African-ancestry samples, training on one tissue type and deploying on another). This creates systematic failures even when datasets are properly separated with no overlap. Data leakage, by contrast, occurs when test examples appear in training data or when future information unavailable at prediction time influences training. Example: a model trained on liver chromatin data shows poor performance on heart tissue (domain shift) even though the heart test sequences were never seen during training (no leakage).\nAdaptation Strategy Selection: Use linear probing with fewer than 500 labeled examples; more complex adaptation will overfit catastrophically. Between 500 and 5,000 examples, LoRA and other PEFT methods offer favorable tradeoffs between flexibility and regularization. With more than 10,000 examples, full fine-tuning becomes viable if the target task diverges substantially from pretraining. However, data quantity alone does not determine strategy; task alignment with pretraining matters equally. Always establish frozen feature baselines first, then escalate to more complex methods only when simpler approaches demonstrably fail.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nCore concepts:\n\nParameter-efficient fine-tuning (PEFT): LoRA and adapters update only 1-5% of model parameters, enabling adaptation on limited data and compute while providing implicit regularization\nLayer hunting problem: Decoder models require systematic layer search for optimal embedding extraction; encoder models reliably use final-layer representations\nSequence aggregation: [CLS] tokens and mean pooling both work for classification; choice depends on how information distributes across positions\nDomain shift: Species, tissue, population, and technical batch effects create systematic failures when training and deployment distributions differ\n\nKey decision rules:\n\n&lt;500 examples: Linear probing only\n500-5,000 examples: LoRA/adapters recommended\n&gt;10,000 examples: Full fine-tuning viable\nAlways compare against from-scratch baselines\n\nCritical warnings:\n\nClass imbalance hides failures: A 99% accurate model may miss most pathogenic variants\nTest set contamination inflates results: Verify separation between pretraining and evaluation data\nAggregate metrics mislead: Stratify by ancestry, variant type, and allele frequency\nTransfer can hurt: Negative transfer produces models worse than random initialization\n\nConnections to other chapters:\n\nPretraining objectives shape adaptation requirements (Chapter 8)\nEvaluation methodology separates real from spurious gains (Chapter 12)\nConfounding sources inflate benchmarks (Chapter 12)\nUncertainty quantification enables appropriate abstention (Chapter 23)\nClinical deployment requires calibration to deployment prevalence (Chapter 27)\n\n\n\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nBen-David, Shai, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. “A Theory of Learning from Different Domains.” Machine Learning 79 (1): 151–75. https://doi.org/10.1007/s10994-009-5152-4.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=transaction&utm_medium=email&utm_campaign=linkedin_newsletter.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. https://doi.org/10.48550/arXiv.1810.04805.\n\n\nFinn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.” In Proceedings of the 34th International Conference on Machine Learning, 1126–35. PMLR. https://proceedings.mlr.press/v70/finn17a.html.\n\n\nGaedigk, Andrea, Magnus Ingelman-Sundberg, Neil A. Miller, J. Steven Leeder, Michelle Whirl-Carrillo, Teri E. Klein, and the PharmVar Steering Committee. 2018. “The Pharmacogene Variation (PharmVar) Consortium: Incorporation of the Human Cytochrome P450 (CYP) Allele Nomenclature Database.” Clinical Pharmacology & Therapeutics 103 (3): 399–401. https://doi.org/10.1002/cpt.910.\n\n\nGanin, Yaroslav, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. 2016. “Domain-Adversarial Training of Neural Networks.” Journal of Machine Learning Research 17 (59): 1–35. http://jmlr.org/papers/v17/15-239.html.\n\n\nHoward, Jeremy, and Sebastian Ruder. 2018. “Universal Language Model Fine-Tuning for Text Classification.” arXiv. https://doi.org/10.48550/arXiv.1801.06146.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2106.09685.\n\n\nJawahar, Ganesh, Benoît Sagot, and Djamé Seddah. 2019. “What Does BERT Learn about the Structure of Language?” In ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy. https://inria.hal.science/hal-02131630.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nKelley, David R. 2020. “[Basenji2] Cross-Species Regulatory Sequence Activity Prediction.” PLOS Computational Biology 16 (7): e1008050. https://doi.org/10.1371/journal.pcbi.1008050.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nMartin, Alicia R., Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada, Benjamin M. Neale, and Mark J. Daly. 2019. “Clinical Use of Current Polygenic Risk Scores May Exacerbate Health Disparities.” Nature Genetics 51 (4): 584–91. https://doi.org/10.1038/s41588-019-0379-x.\n\n\nMcCloskey, Michael, and Neal Cohen. 1989. “Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem.” In Psychology of Learning and Motivation, 24:109–65. Academic Press. https://doi.org/10.1016/S0079-7421(08)60536-8.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nRieke, Nicola, Jonny Hancox, Wenqi Li, Fausto Milletarì, Holger R. Roth, Shadi Albarqouni, Spyridon Bakas, et al. 2020. “The Future of Digital Health with Federated Learning.” Npj Digital Medicine 3 (1): 119. https://doi.org/10.1038/s41746-020-00323-1.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSainz, Oscar, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. “NLP Evaluation in Trouble: On the Need to Measure LLM Data Contamination for Each Benchmark.” In Findings of the Association for Computational Linguistics: EMNLP 2023, edited by Houda Bouamor, Juan Pino, and Kalika Bali, 10776–87. Singapore: Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.findings-emnlp.722.\n\n\nSnell, Jake, Kevin Swersky, and Richard Zemel. 2017. “Prototypical Networks for Few-Shot Learning.” In Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html.\n\n\nWang, Dequan, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. 2021. “Tent: Fully Test-Time Adaptation by Entropy Minimization.” arXiv. https://doi.org/10.48550/arXiv.2006.10726.\n\n\nWang, Zirui, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. 2019. “Characterizing and Avoiding Negative Transfer.” In, 11293–302. https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Characterizing_and_Avoiding_Negative_Transfer_CVPR_2019_paper.html.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptation Strategies</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html",
    "href": "part_2/p2-ch11-benchmarks.html",
    "title": "11  Benchmarks and Evaluation",
    "section": "",
    "text": "11.1 Protein Language Model Benchmarks\nProtein language models (Chapter 15) benefit from the longest-established and most systematic evaluation ecosystem in genomic AI, reflecting both the longer history of computational protein science and the relative tractability of protein structure and function prediction compared to the regulatory genomics tasks discussed in Chapter 16. The maturity of protein benchmarks reflects both the longer history of computational protein science and the relative tractability of protein structure and function prediction compared to regulatory genomics.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-protein-benchmarks",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-protein-benchmarks",
    "title": "11  Benchmarks and Evaluation",
    "section": "",
    "text": "11.1.1 TAPE: Tasks Assessing Protein Embeddings\nHow do you know if one protein representation is better than another? The challenge parallels standardized testing in education: before SAT and GRE exams existed, comparing students from different schools was nearly impossible because each school used different grading scales and curricula. A 3.8 GPA from one school might mean something very different from a 3.8 at another. Standardized benchmarks solve this by providing a common yardstick that everyone agrees to use, enabling fair comparison across diverse candidates. Before 2019, comparing protein language models faced the same problem—different papers ran different evaluations with inconsistent protocols, making apples-to-apples comparison nearly impossible. The Tasks Assessing Protein Embeddings (TAPE) benchmark, introduced in 2019, established the template for systematic protein representation evaluation (Rao et al. 2019). TAPE frames protein language model assessment as transfer learning evaluation (Chapter 9): pretrained models generate embeddings (Section 5.6), which are then used as features for supervised prediction on downstream tasks. This framework decouples representation quality from task-specific modeling, enabling comparison across architectures that may have very different inductive biases.\nTAPE comprises five tasks spanning different aspects of protein biology. Secondary structure prediction requires classifying each residue as helix, sheet, or coil, testing whether embeddings capture local structural preferences. Contact prediction asks whether residue pairs are spatially proximate in the folded structure, probing the representation’s ability to encode tertiary structure information from sequence alone. Remote homology detection requires classifying proteins into structural superfamilies, testing whether embeddings capture evolutionary relationships that transcend sequence similarity. Fluorescence prediction and stability prediction use data from deep mutational scanning experiments to assess whether embeddings encode fitness landscapes.\nThe benchmark’s design reflects deliberate methodological choices. Train, validation, and test splits enforce sequence identity thresholds to prevent homology-based leakage (Section 12.4). Evaluation uses simple linear or shallow neural network heads rather than complex task-specific architectures, isolating representation quality from modeling capacity. Standardized preprocessing and data loading eliminate confounds from inconsistent implementation.\n\n\n\n\n\n\nKey Insight: The Transfer Learning Evaluation Framework\n\n\n\nTAPE established a crucial principle: evaluate representations separately from task-specific modeling. By using simple linear classifiers on top of frozen embeddings, TAPE isolates what the pretrained model learned from what a complex head might learn during fine-tuning. This approach became the standard template for foundation model evaluation across genomics, enabling fair comparison between models with different architectures and pretraining objectives.\n\n\nTAPE’s influence extended beyond its specific tasks. The benchmark established norms for protein representation evaluation: systematic coverage of diverse prediction targets, controlled transfer learning protocols, and explicit attention to data splitting. Subsequent benchmarks adopted and extended this framework.\n\n\n11.1.2 FLIP: Function-Linked Protein Benchmark\nCan a model predict what a protein actually does, not just what it looks like? TAPE’s labels include computationally inferred structure and conservation, but these are indirect proxies for function. A researcher wanting to know whether a mutation affects enzymatic activity needs predictions grounded in experimental measurements of that activity. The FLIP (Function-Linked Integrated Protein) benchmark addresses this gap by focusing on experimentally measured functional properties (Dallago et al. 2022). Where TAPE includes structurally derived labels and computational annotations, FLIP emphasizes high-throughput experimental assays that directly measure protein fitness.\nFLIP aggregates deep mutational scanning datasets across diverse proteins and functional readouts. The benchmark includes assays measuring enzymatic activity, binding affinity, thermostability, and expression level. Each dataset provides quantitative measurements for thousands of single-point mutations, enabling evaluation of fine-grained variant effect prediction.\nThe benchmark’s value lies in its experimental grounding. Computational structure predictions and evolutionary conservation scores, while useful, are indirect proxies for function. Deep mutational scanning provides direct measurements of how sequence changes affect the property of interest. Models that perform well on FLIP demonstrate the ability to predict experimentally validated functional consequences rather than computationally inferred annotations.\nFLIP also introduced systematic evaluation of different splitting strategies. Random splits, where training and test variants are sampled uniformly from the same protein, represent the easiest setting. Contiguous splits, where training and test variants occupy different sequence regions, test spatial generalization. Modulo splits, which interleave training and test positions along the sequence, provide intermediate difficulty. Performance typically degrades from random to contiguous splits, revealing how much models rely on local sequence context versus genuine functional understanding.\n\n\n\n\n\n\nStop and Think: Splitting Strategy Implications\n\n\n\nBefore reading on, consider: If a model achieves 0.85 correlation on FLIP with random splits but only 0.60 correlation with contiguous splits, what does this reveal about what the model has learned? What kind of information would be available in random splits but not contiguous splits?\nHint: Think about what information from nearby positions might “leak” across random splits.\n\n\n\n\n11.1.3 ProteinGym: Comprehensive Variant Effect Evaluation\nWhich variant effect predictor should you trust? With dozens of models claiming state-of-the-art performance on different proteins, the field needed a unified benchmark comprehensive enough to reveal which approaches genuinely generalize. ProteinGym has emerged as the most comprehensive benchmark for protein variant effect prediction, compiling 217 deep mutational scanning assays across diverse protein families (Notin et al. 2023). The benchmark’s scale enables statistically robust comparison across modeling approaches while its diversity reveals where different methods excel or struggle.\nThe primary evaluation metric is Spearman correlation between predicted and experimentally measured fitness effects. This rank-based metric is appropriate for deep mutational scanning data, where absolute fitness values depend on assay-specific calibration but relative rankings are more comparable across experiments. ProteinGym reports correlations for each assay individually and aggregated across the full benchmark, enabling both global comparison and identification of task-specific strengths.\nProteinGym distinguishes between zero-shot and supervised evaluation regimes. In zero-shot evaluation, models predict variant effects without any task-specific training, relying entirely on representations learned during pretraining. Models like ESM-1v (Section 15.1) compute effects as log-likelihood ratios under the pretrained language model, while structure-based methods like AlphaMissense (Section 17.2.3) incorporate predicted structural consequences. In supervised evaluation, models are fine-tuned on a subset of measured variants before predicting held-out effects. The gap between zero-shot and supervised performance indicates how much task-specific information improves over general-purpose representations.\nThe benchmark reveals systematic patterns in model performance. Protein language models generally outperform conservation-based methods, particularly for variants in regions with sparse evolutionary sampling. Structure-aware models show advantages for variants affecting protein stability or buried residues. Ensemble methods that combine multiple predictors often achieve the highest correlations, suggesting that different approaches capture complementary information.\nProteinGym’s limitations mirror those of its constituent datasets. Deep mutational scanning experiments are biased toward well-studied proteins amenable to high-throughput screening. Assay-specific selection pressures affect which variants appear deleterious: a variant may strongly affect enzymatic activity while leaving thermostability unchanged, or vice versa. The benchmark measures correlation with specific experimental readouts rather than clinical pathogenicity, which integrates multiple functional consequences in complex ways.\n\n\n\nTable 11.1: Comparison of major protein benchmarks. Each benchmark makes different trade-offs between scale, label quality, and evaluation rigor.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\nTasks\nLabels\nSplitting\nStrengths\nLimitations\n\n\n\n\nTAPE\n5 (structure, homology, fitness)\nMixed (experimental + computational)\nHomology-aware\nEstablished template; diverse tasks\nSome computational labels; limited DMS coverage\n\n\nFLIP\nMultiple DMS datasets\nExperimental only\nMultiple strategies (random, contiguous, modulo)\nExperimental grounding; systematic split analysis\nLimited to proteins with DMS data\n\n\nProteinGym\n217 DMS assays\nExperimental only\nZero-shot + supervised\nComprehensive scale; diverse protein families\nBiased toward well-studied proteins\n\n\n\n\n\n\n\n\n11.1.4 Structure Prediction Benchmarks\nStructure prediction has long served as the ultimate test of whether we truly understand proteins: if you can predict how a sequence folds, you have captured something fundamental about protein physics. Protein structure prediction benchmarks derive from the Critical Assessment of protein Structure Prediction (CASP) tradition, which has evaluated computational methods against experimentally determined structures since 1994 (kryshtafovych_critical_2021?). The dramatic success of AlphaFold2 at CASP14 in 2020 transformed the field, but structure prediction benchmarks remain relevant for evaluating single-sequence methods and assessing whether language model pretraining improves structural accuracy.\nStructure prediction quality is typically assessed using the Global Distance Test (GDT-TS) and Template Modeling score (TM-score). GDT-TS measures the percentage of residues that can be superimposed within various distance thresholds, providing a single number between 0 and 100 that correlates well with visual assessment of structural similarity. TM-score normalizes by protein length, enabling comparison across proteins of different sizes.\nFor protein language models, the relevant evaluation setting is single-sequence structure prediction, where the model receives only the target sequence without multiple sequence alignments. This tests whether pretraining on evolutionary sequence databases enables structure prediction without explicit evolutionary analysis at inference time. ESMFold (Section 15.4) demonstrated that single-sequence prediction can approach MSA-based methods for many proteins, though performance gaps remain for sequences with sparse evolutionary coverage.\nStructure prediction benchmarks complement sequence-based evaluations by testing whether learned representations encode biophysical constraints. A model that achieves high accuracy on contact prediction or secondary structure classification may still fail to integrate these local predictions into globally consistent structures. The emergence of accurate single-sequence structure prediction from language model embeddings suggests that pretraining captures substantial structural information, even without explicit structural supervision.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-dna-benchmarks",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-dna-benchmarks",
    "title": "11  Benchmarks and Evaluation",
    "section": "11.2 DNA and Regulatory Benchmarks",
    "text": "11.2 DNA and Regulatory Benchmarks\nDNA foundation models (Chapter 14) and regulatory models (Chapter 16) face a less mature but rapidly developing benchmark landscape compared to the protein ecosystem. Early deep learning work in genomics focused on individual tasks derived from ENCODE-style assays (Section 2.4.1), establishing evaluation paradigms that later benchmark suites would systematize. Recent efforts have introduced benchmark suites that attempt to standardize evaluation across multiple tasks, tissues, and species.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore diving into DNA benchmarks, consider: Why might DNA benchmarks be “less mature” than protein benchmarks? What makes regulatory prediction fundamentally harder to benchmark than protein structure or function prediction?\nHint: Think about the length scales involved and whether regulatory activity is an intrinsic sequence property.\n\n\n\n11.2.1 Classical Regulatory Prediction Tasks\nWhere in the genome does gene regulation actually happen? Early deep learning researchers needed benchmarks to test whether neural networks could learn to recognize promoters, enhancers, and transcription factor binding sites from DNA sequence alone. The earliest deep learning benchmarks for genomics framed regulatory prediction as classification over short sequence windows. Transcription factor binding prediction asks whether a specific TF ChIP-seq peak overlaps a given sequence window, typically around 1 kilobase centered on the binding site. Open chromatin prediction requires classifying regions as accessible or inaccessible based on DNase-seq or ATAC-seq signal. Histone mark prediction asks whether a chromatin modification peak (H3K27ac, H3K4me3, etc.) is present at each position.\nThese tasks derive from consortia like ENCODE and Roadmap Epigenomics (Section 2.4.1), which systematically profiled chromatin states across cell types. Benchmark construction typically involves defining positive regions from called peaks and sampling negative regions from elsewhere in the genome, extracting fixed-length sequences centered on each region, and evaluating binary classification using auROC or average precision.\nModels such as DeepSEA, Basset, and DanQ established baseline performance on these tasks (Chapter 6 for architectural details). Their success demonstrated that convolutional networks could learn sequence features predictive of regulatory state without hand-crafted motifs. Modern foundation models still report performance on similar tasks as sanity checks, though these classical benchmarks have significant limitations.\n\n\n\n\n\n\nLimitation Alert: Binary Classification Over Short Windows\n\n\n\nThe primary limitation of classical regulatory benchmarks is that binary classification over short windows fails to capture the quantitative, cell-type-specific, and long-range nature of transcriptional regulation. A region may be weakly accessible in some cell types and strongly accessible in others; binary labels collapse this continuous variation. Short windows cannot assess whether models capture distal regulatory interactions that span tens to hundreds of kilobases. Evaluation on curated peak regions may overestimate performance relative to genome-wide prediction, where the vast majority of positions are regulatory “background.”\n\n\n\n\n11.2.2 Quantitative Regulatory Prediction\nBeyond binary classification, benchmarks increasingly require prediction of quantitative regulatory readouts. Signal regression asks models to predict per-base or per-bin signal intensity from ChIP-seq, ATAC-seq, or related assays. Gene expression prediction requires predicting transcript abundance (TPM, counts) from promoter sequences or larger genomic contexts. Massively parallel reporter assays (MPRAs) provide systematic measurements of enhancer or promoter activity for thousands of sequences, enabling evaluation of quantitative activity prediction.\nHybrid architectures like Enformer (Section 16.2) popularized benchmarks combining large receptive fields with dense quantitative targets across many assays and cell types. Evaluation metrics shift from auROC to Pearson or Spearman correlation between predicted and observed profiles. Some benchmarks report correlation relative to replicate concordance, establishing an upper bound set by experimental reproducibility.\nQuantitative benchmarks better reflect the continuous nature of regulatory activity but introduce new challenges. Heterogeneous noise across assays and laboratories complicates aggregation: should a model be penalized equally for poor performance on a low-quality assay versus a high-quality one? Cell-type diversity raises questions about how to weight performance across tissues: is accurate prediction in a rare cell type more or less important than in a common one? The relationship between predicted and observed signal depends on assay-specific calibration that may not transfer across experimental batches.\n\n\n11.2.3 Genomic Benchmarks\nReproducibility in DNA modeling has been notoriously difficult: different papers used different data preprocessing, different train-test splits, and different evaluation metrics, making it nearly impossible to compare methods fairly. The Genomic Benchmarks resource addresses this fragmentation by providing standardized classification datasets for DNA sequence models (Grešová et al. 2023). The benchmark compiles tasks including enhancer identification, promoter recognition, splice site detection, and coding sequence classification across multiple species. Standardized train, validation, and test splits enable direct comparison of different architectures without confounds from inconsistent data processing.\nGenomic Benchmarks emphasizes accessibility and reproducibility. Datasets are available in a unified format with documented preprocessing. Baseline results for multiple architectures provide reference points for new models. The benchmark includes tasks of varying difficulty, from relatively easy (distinguishing coding from non-coding sequence) to challenging (identifying tissue-specific enhancers).\nThe benchmark’s limitations reflect its design priorities. Focus on classification rather than regression excludes quantitative prediction tasks. Task difficulty varies substantially, with some tasks approaching saturation where gains become difficult to measure. Species coverage, while broader than many benchmarks, remains biased toward well-studied model organisms.\n\n\n11.2.4 BEND: Benchmark for DNA Language Models\nAs DNA language models proliferated, each reporting impressive results on different tasks, a pressing question emerged: which model should you actually use for your application? BEND (Benchmark for Evaluating DNA Models) provides a unified framework for evaluating genomic foundation models across diverse tasks (Marin et al. 2024). The benchmark includes regulatory element classification, chromatin accessibility prediction, variant effect scoring, and gene expression prediction. Standardized splits and evaluation protocols enable fair comparison across model families.\nBEND’s design reflects lessons learned from earlier benchmarks. Tasks span multiple biological scales, from nucleotide-level variant effects to kilobase-scale regulatory elements. Evaluation includes both zero-shot settings (using pretrained representations directly) and fine-tuned settings (adapting models to specific tasks). Performance is reported separately for each task rather than aggregated into a single score, acknowledging that different models may excel at different aspects of genomic prediction.\nComparative evaluations using BEND reveal that no single model dominates across all tasks. Architecture choices (CNN versus transformer versus state space model), tokenization schemes (single nucleotide versus k-mer versus BPE), and pretraining corpora all influence task-specific performance (Chapter 5). These patterns inform model selection for specific applications while highlighting the limitations of aggregate benchmarks that obscure such variation.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nTest your understanding of the DNA benchmark landscape:\n\nWhat is the key difference between classical regulatory benchmarks (binary classification) and modern quantitative benchmarks?\nWhy might a model perform well on Genomic Benchmarks but poorly on BEND?\nWhat does it mean when “no single model dominates across all tasks” in BEND?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nClassical benchmarks focus on binary classification (promoter vs. non-promoter, enhancer vs. background) while modern quantitative benchmarks predict continuous values (expression levels, binding affinity, chromatin accessibility), requiring models to capture magnitude rather than just presence/absence. (2) A model might excel at binary classification tasks in Genomic Benchmarks by learning to distinguish broad sequence classes, but fail on BEND’s diverse tasks requiring different capabilities like long-range dependencies, quantitative prediction, or cross-species transfer. (3) When no single model dominates, it means different architectures excel at different task types - suggesting that architectural choices matter and that benchmark diversity successfully measures complementary capabilities rather than a single underlying skill.\n\n\n\n\n\n\n\n\n11.2.5 Long-Range Benchmarks\nLong-range regulatory interactions, where enhancers tens to hundreds of kilobases from their target genes influence expression, require benchmarks that specifically test extended context modeling. Consider the challenge of understanding a sentence versus understanding a novel: predicting the next word from the previous five words tests local grammar, while predicting plot resolution from earlier foreshadowing tests whether a model truly comprehends narrative structure across hundreds of pages. Similarly, short-context benchmarks test whether models recognize local motifs, while long-range benchmarks test whether they understand how distant regulatory elements coordinate gene expression—the “narrative structure” of the genome.\nThe Long Range Benchmark (LRB) evaluates models’ ability to integrate information across large genomic distances, with tasks including predicting distal enhancer-promoter interactions, modeling topologically associating domain (TAD) boundary effects, and identifying long-range regulatory dependencies [Citation Needed]. TADs are regions of the genome (typically 100 kb to 2 Mb) that preferentially interact with themselves rather than with neighboring regions—like chapters in a book where scenes within a chapter relate closely to each other but less to scenes in other chapters. TAD boundaries constrain which enhancers can reach which promoters; variants disrupting these boundaries can cause disease by allowing inappropriate regulatory crosstalk.\nDNALongBench extends evaluation to ultra-long contexts spanning up to millions of base pairs [Citation Needed]. Tasks at this scale test whether models can leverage chromosome-level context for regulatory prediction, potentially capturing effects from 3D chromatin organization and large-scale chromatin domains.\nThese benchmarks are particularly relevant for evaluating efficient attention mechanisms, state space models, and other architectures designed to extend effective context length (Section 7.4). Performance on long-range benchmarks does not necessarily correlate with short-range task performance, indicating that different architectural choices optimize for different aspects of sequence modeling.\n\n\n11.2.6 Cross-Species Evaluation\nGenBench and related resources test whether models trained on one organism generalize to related species [Citation Needed]. Cross-species evaluation is important for several reasons. Many applications require predictions in non-human organisms (agricultural genomics, model organism research, comparative genomics). Multi-species training may improve within-species performance by providing additional evolutionary signal (Section 8.8.3). The ability to transfer across species indicates that models have learned general principles of genome organization rather than species-specific artifacts.\nCross-species benchmarks typically evaluate models on held-out species not seen during training. Performance degradation from training to held-out species indicates the degree to which learned representations depend on species-specific features. Some architectures show better cross-species transfer than others, suggesting differences in how well they capture conserved regulatory principles.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-vep-benchmarks",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-vep-benchmarks",
    "title": "11  Benchmarks and Evaluation",
    "section": "11.3 Variant Effect Prediction Benchmarks",
    "text": "11.3 Variant Effect Prediction Benchmarks\nVariant effect prediction (VEP) benchmarks connect sequence changes to molecular or phenotypic consequences, addressing the clinically central question of which variants matter. These benchmarks span multiple biological levels, from molecular function to clinical pathogenicity.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore exploring VEP benchmarks, pause to consider: What makes variant effect prediction fundamentally different from other benchmark tasks we’ve covered? If a model achieves 0.95 auROC on a variant pathogenicity benchmark, what does that tell you—and what doesn’t it tell you?\nHint: Consider the difference between ranking variants and making clinical decisions about individual patients.\n\n\n\n11.3.1 Clinical Variant Databases\nHow do you know if a variant you have never seen before is pathogenic? Clinical laboratories face this question daily, and the answer increasingly depends on computational predictions. ClinVar provides the most widely used labels for clinical variant effect prediction, aggregating pathogenicity assertions from clinical laboratories and researchers worldwide (Section 2.8.1). Benchmarks derived from ClinVar frame variant interpretation as classification: given a variant, predict whether it is pathogenic, likely pathogenic, benign, or likely benign.\nClinVar’s value as a benchmark stems from its clinical relevance. Variants classified in ClinVar represent the actual population of variants encountered in clinical testing. Performance on ClinVar directly addresses whether a model can assist variant interpretation workflows. The database’s scale (over 2 million variant submissions as of 2024) enables statistically robust evaluation (Landrum et al. 2018).\n\n\n\n\n\n\nCritical Limitation: ClinVar Circularity\n\n\n\nClinVar’s limitations as a benchmark are equally important. Submission heterogeneity means that label quality varies dramatically: expert-curated panels provide high-confidence classifications while single-laboratory submissions may reflect limited evidence. Version sensitivity means that benchmark composition changes over time as new submissions arrive and old classifications are updated. Most consequentially, circularity with computational predictors creates feedback loops: variants may have been classified using the very tools being evaluated, inflating apparent performance. This circularity problem, examined in detail for classical predictors in Section 4.5 and for its broader confounding implications in ?sec-ch22-label-bias, represents one of the most insidious forms of benchmark contamination.\n\n\nAncestry and gene coverage biases profoundly shape what ClinVar benchmarks measure. Variants from European ancestry individuals and well-studied disease genes are heavily overrepresented. High performance on ClinVar demonstrates accuracy for this specific population rather than robust generalization across human genetic diversity (Section 3.7). Benchmarks stratified by ancestry reveal substantial performance gaps, with models typically performing worse on variants from underrepresented populations.\nBest practices for using ClinVar as a benchmark include specifying the exact database version and download date, excluding variants with conflicting assertions, stratifying performance by evidence level and ancestry, and comparing to baselines using only allele frequency to detect circularity. These practices are detailed in Section 12.4, with specific guidance on detecting label leakage in Section 12.4.1.\n\n\n11.3.2 CAGI: Critical Assessment of Genome Interpretation\nCan a benchmark ever be truly leak-proof? The fundamental problem with retrospective evaluation is that someone, somewhere, might have seen the test data. The Critical Assessment of Genome Interpretation (CAGI) challenges provide prospective evaluation of variant effect predictors on unpublished datasets (hoskins_cagi6_2023?). Unlike retrospective benchmarks that evaluate models on historical data, CAGI distributes prediction targets before ground truth is available, preventing any possibility of overfitting to known labels.\nCAGI challenges cover diverse prediction targets. Some challenges focus on molecular phenotypes: predicting the effect of variants on protein stability, binding affinity, or enzymatic activity. Others target clinical phenotypes: predicting disease risk, drug response, or clinical severity from individual genomes. The diversity of challenges tests whether models generalize across different types of variant effects.\n\n\n\n\n\n\nKey Insight: The Value of Prospective Evaluation\n\n\n\nThe prospective design of CAGI provides several crucial advantages over retrospective benchmarks:\n\nPredictions must be made before labels are known, eliminating leakage from any source\nThe timeline forces models to commit to predictions rather than post-hoc optimization\nCommunity participation enables fair comparison across many approaches under identical conditions\n\nThis prospective design represents the gold standard for benchmark validity. When evaluating any retrospective benchmark result, ask: “Would this performance hold up in a CAGI-style prospective evaluation?”\n\n\nCAGI’s limitation is scale: challenges include hundreds to thousands of variants rather than the millions available in databases like ClinVar. Statistical power to detect small performance differences is correspondingly limited. The challenges also depend on experimental collaborators willing to withhold data until after the prediction deadline, limiting the range of phenotypes that can be assessed.\n\n\n11.3.3 Deep Mutational Scanning Benchmarks\nWhat if you could measure the effect of every possible mutation in a protein, not just the ones that happen to occur in patients? Deep mutational scanning (DMS) provides exactly this: systematic experimental measurement of variant effects across entire proteins or regulatory elements (Section 2.4.4). DMS benchmarks test whether models can predict these experimentally determined effects, providing direct validation against measured functional consequences rather than inferred clinical classifications.\nMaveDB aggregates DMS datasets in a standardized format, enabling systematic benchmarking across diverse proteins and assays (esposito_mavedb_2019?). ProteinGym’s DMS component (discussed above) represents the most comprehensive benchmark in this space. For non-coding variants, MPRA datasets provide analogous systematic measurements of regulatory activity.\nDMS benchmarks have distinct strengths and limitations compared to clinical databases. The experimental grounding means that labels reflect actual measured effects rather than clinical inference that may involve multiple assumptions. The relationship between DMS fitness and clinical pathogenicity is complex: a variant may substantially affect enzymatic activity without causing disease if the residual activity suffices for normal physiology. DMS benchmarks measure one component of the variant interpretation puzzle rather than the full clinical picture.\n\n\n11.3.4 Regulatory and Non-Coding Variant Benchmarks\nNon-coding variants require specialized benchmarks because their effects operate through different mechanisms than coding variants. The foundation model approaches to non-coding variant effect prediction are examined in ?sec-ch14-dna-vep, with the underlying regulatory models detailed in Chapter 16. MPRA-based benchmarks test whether models can predict the quantitative effect of variants on enhancer or promoter activity measured in reporter assays. Expression quantitative trait locus (eQTL)-based benchmarks use naturally occurring variants associated with expression changes, treating the statistical evidence for eQTL status as a proxy for regulatory impact.\nThe challenge for non-coding benchmarks is connecting molecular effects to phenotypic consequences. A variant may alter chromatin accessibility without affecting any gene’s expression. A variant may affect expression without influencing disease risk. This gap between molecular and clinical effects complicates interpretation: high performance on MPRA prediction does not necessarily translate to accurate regulatory disease variant interpretation.\nFine-mapped genome-wide association study (GWAS) variants provide another benchmark source for non-coding VEP. Statistical fine-mapping identifies putatively causal variants within associated loci (Section 3.4), and models can be evaluated on their ability to prioritize these variants over nearby non-causal variants. Performance on fine-mapping tasks more directly assesses clinical relevance than molecular phenotype prediction, though fine-mapping itself has substantial uncertainty.\n\n\n\nTable 11.2: Comparison of variant effect prediction benchmark types. Each provides different evidence about model capabilities, with distinct validity trade-offs.\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark Type\nLabel Source\nStrengths\nLimitations\nBest For\n\n\n\n\nClinVar\nClinical assertions\nClinical relevance; scale\nCircularity; ancestry bias; heterogeneous quality\nClinical deployment validation (with caveats)\n\n\nCAGI\nProspective experiments\nNo leakage possible; forces commitment\nLimited scale; infrequent\nGold-standard validation\n\n\nDMS/MaveDB\nHigh-throughput assays\nDirect experimental measurement\nAssay-specific; fitness != pathogenicity\nMolecular mechanism understanding\n\n\nMPRA\nReporter assays\nQuantitative; regulatory focus\nReporter != endogenous; context-dependent\nRegulatory variant effects\n\n\neQTL/GWAS\nStatistical associations\nPopulation-level evidence\nCorrelation != causation; LD confounding\nCommon variant prioritization",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-trait-benchmarks",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-trait-benchmarks",
    "title": "11  Benchmarks and Evaluation",
    "section": "11.4 Trait and Population-Level Benchmarks",
    "text": "11.4 Trait and Population-Level Benchmarks\nAt the individual and population level, benchmarks assess whether models improve prediction of complex traits and disease risk.\n\n11.4.1 Polygenic Score Evaluation\nPolygenic score (PGS) benchmarks evaluate how well genotype-derived scores predict disease risk or quantitative traits (Section 3.5). Common evaluation settings include within-biobank evaluation, where a single large cohort is partitioned into training and test sets, and cross-biobank evaluation, where models trained in one population are tested in another. The integration of foundation model features with PGS approaches represents an emerging research direction (Section 27.1).\nMetrics depend on the phenotype. For quantitative traits, benchmarks report the coefficient of determination (\\(R^2\\)) or incremental \\(R^2\\) over non-genetic covariates. For binary disease outcomes, auROC and area under the precision-recall curve (auPRC) quantify discrimination. Calibration metrics assess whether predicted risks match observed event rates (Section 23.2). The clinical utility of PGS, discussed in Chapter 27, depends on all these properties: a score may discriminate well (high auROC) while being poorly calibrated (predicted risks do not match actual event rates).\nCross-population evaluation is particularly important because PGS portability is a major limitation of current methods (Section 3.7). Benchmarks stratified by ancestry typically reveal substantial performance degradation from European ancestry (where most GWAS have been conducted) to other populations. This degradation stems from multiple sources: different linkage disequilibrium patterns mean that tag SNPs identify different causal variants, population-specific variants are absent from training data, and effect sizes may differ across populations due to gene-environment interactions.\n\n\n11.4.2 TraitGym\nDo foundation models actually improve disease risk prediction, or do they just add computational overhead to methods that already work? Traditional polygenic scores use simple weighted sums of variant effects; the burden is on foundation models to prove they add value. TraitGym provides a framework specifically designed to assess complex trait prediction using genomic foundation models (yan_traitgym_2024?). The benchmark evaluates whether foundation model embeddings or variant scores improve prediction beyond traditional polygenic score methods.\nTraitGym’s design addresses several limitations of standard PGS benchmarks. Ancestry stratification is built into the evaluation protocol, requiring models to report performance separately for different population groups. Multiple phenotypes spanning different genetic architectures (highly polygenic versus more oligogenic) test generalization across trait types. Comparison to appropriate baselines (standard PGS methods, clinical covariates alone) isolates the contribution of foundation model features.\nThe benchmark is particularly relevant for assessing claims that genomic foundation models add predictive value beyond classical statistical genetics. Foundation models incur substantial computational costs compared to linear PGS models; TraitGym helps determine whether these costs are justified by improved prediction.\n\n\n11.4.3 EmbedGEM Framework\nA foundation model embedding might correlate with disease outcomes for the wrong reasons: batch effects, population structure, or other confounders that happen to track with health status. How do you distinguish models that have discovered genuine biology from those that have merely learned to recognize data artifacts? The EmbedGEM framework evaluates whether foundation model embeddings capture biologically meaningful genetic signal, as opposed to technical artifacts or confounders (Mukherjee et al. 2024). The framework assesses embeddings along two axes: heritability and disease relevance.\nThe heritability axis measures how much genetic signal an embedding captures. EmbedGEM counts the number of genome-wide significant loci associated with embedding components and quantifies the strength of association through mean chi-squared statistics. Higher values indicate that the embedding reflects heritable biology rather than noise.\nThe disease relevance axis measures whether embedding-associated variants predict clinically meaningful outcomes. Polygenic scores constructed from embedding GWAS hits are evaluated for their ability to predict disease in independent cohorts. Incremental predictive value over standard clinical models indicates that the embedding captures disease-relevant genetic information.\nThis two-axis evaluation addresses a critical question for foundation model deployment: do learned representations discover novel biology or merely recapitulate known associations with additional computational overhead? Embeddings that show high heritability but low disease relevance may capture biological signal that is not clinically actionable. Embeddings that show disease relevance without novel genetic discoveries may not add value beyond existing PGS methods.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-benchmark-construction",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-benchmark-construction",
    "title": "11  Benchmarks and Evaluation",
    "section": "11.5 Benchmark Construction and Hidden Assumptions",
    "text": "11.5 Benchmark Construction and Hidden Assumptions\nBeyond cataloging benchmark suites, understanding how benchmarks are constructed reveals assumptions that shape what they measure and what they miss.\n\n11.5.1 Data Sources and Label Provenance\nBenchmark labels derive from diverse sources with different properties. Experimental assays (ChIP-seq, DMS, MPRA) provide direct measurements but are limited by assay-specific artifacts and selection pressures. Computational annotations (gene calls, functional predictions, conservation scores) provide broader coverage but introduce circular dependencies if models are trained and evaluated on overlapping sources. Clinical classifications aggregate expert judgment but reflect the evidence available at classification time, which may include the very predictors being benchmarked.\nThe provenance of benchmark labels determines what success on that benchmark actually means. High performance on experimentally derived labels suggests the model captures the specific molecular process assayed. High performance on clinical labels may indicate genuine clinical utility or may reflect circularity with existing prediction tools. Understanding label provenance is prerequisite to interpreting benchmark results.\n\n\n\n\n\n\nStop and Think: Label Provenance\n\n\n\nConsider a variant effect predictor that achieves 0.95 auROC on a ClinVar benchmark. Before interpreting this result, you should ask:\n\nWhat evidence types contributed to the ClinVar classifications? (Functional assays? Segregation? Computational predictions?)\nDid any of those computational predictions use similar features to your model?\nHow would you detect whether circularity inflated your performance?\n\nThese questions apply to any benchmark with aggregated or curated labels.\n\n\n\n\n11.5.2 Splitting Strategies and Leakage\nHow benchmarks partition data into training and test sets determines whether evaluation measures generalization or memorization (Chapter 12). Random splitting, where examples are assigned to splits uniformly at random, represents the weakest form of evaluation. In genomics, random splits often permit homology-based leakage: training and test sequences may share sufficient similarity that memorization suffices for good performance.\nHomology-aware splitting clusters sequences by similarity before assigning clusters to splits, ensuring that test sequences are evolutionarily distant from training sequences. This approach is standard for protein benchmarks (using tools like CD-HIT or MMseqs2) but less consistently applied for DNA benchmarks.\nChromosome-based splitting holds out entire chromosomes for testing, preventing any position-based leakage within chromosomes. This approach is common for regulatory benchmarks but does not account for homologous sequences on different chromosomes. Temporal splitting reserves recent data for testing, appropriate when benchmarks derive from databases with submission timestamps. Each splitting strategy tests different aspects of generalization; the choice should match the intended deployment scenario.\n\n\n\n\n\n\nData leakage pathways in genomic foundation model evaluation\n\n\n\n\nFigure 11.2: Data leakage pathways in genomic foundation model evaluation. The legitimate pipeline (blue) flows from pretraining through fine-tuning to benchmark evaluation. Leakage pathways (red dashed) create spurious performance: direct overlap when pretraining includes benchmark sequences; homology leakage when training and test sets share high-similarity sequences; label circularity when computational predictions influence ground truth labels that later serve as training targets; resource sharing when databases like ENCODE appear in both pretraining and evaluation; and community iteration when the field collectively overfits to standard benchmarks through publication cycles. Each pathway inflates apparent performance without improving genuine biological understanding.\n\n\n\n\n\n11.5.3 Metric Selection and Aggregation\nBenchmark metrics determine what aspects of model performance are measured. Discrimination metrics (auROC, auPRC, correlation) assess whether models rank predictions correctly. Calibration metrics (expected calibration error, reliability diagrams) assess whether predicted probabilities match observed frequencies (Section 23.2). Clinical utility metrics (net benefit, decision curves) assess whether predictions improve decisions compared to treating all patients the same (Chapter 27).\nDifferent metrics can yield different rankings of models. A model with superior discrimination may have poor calibration, predicting the right relative order but wrong absolute probabilities. Choosing which metric to optimize, and how to aggregate across multiple tasks or datasets, involves implicit decisions about what matters for downstream use.\nAggregation across tasks raises additional issues. Mean performance across many tasks weights each task equally, regardless of clinical importance or dataset quality. Median performance is robust to outliers but obscures variation. Reporting full distributions of task-level performance provides more information but complicates comparison. The choice of aggregation method can substantially affect which model appears best.\n\n\n11.5.4 Goodhart’s Law and Benchmark Gaming\nBenchmarks create incentive structures, and incentive structures invite optimization. Goodhart’s Law, that a measure ceases to be a good measure once it becomes a target, applies with particular force to machine learning evaluation. When model development prioritizes leaderboard position, the benchmark becomes the optimization target rather than a proxy for the underlying capability it was designed to measure.\nGaming takes multiple forms in genomic AI. Architectural choices may be tuned specifically to benchmark characteristics: receptive fields sized to match benchmark sequence lengths, output heads designed for benchmark label distributions, hyperparameters selected through extensive benchmark-specific search. Such tuning improves benchmark performance without necessarily improving generalization to deployment scenarios that differ from benchmark conditions.\nMore subtle gaming arises from selective reporting. Models may be evaluated on many benchmarks with only favorable results published. Benchmark versions may be chosen to maximize apparent performance. Evaluation protocols may deviate from published standards in ways that inflate metrics. The cumulative effect is a literature where reported performance systematically overestimates deployment capability.\nThe circularity between predictors and databases creates particularly insidious gaming dynamics. When ClinVar classifications incorporate computational predictions, and those predictions are then benchmarked against ClinVar, the benchmark rewards models that resemble their predecessors rather than models that provide independent information (Chapter 12). This circularity is rarely acknowledged in benchmark reporting, yet it fundamentally compromises the validity of performance claims.\nMitigating gaming requires structural changes to evaluation practice: prospective benchmarks like CAGI where predictions precede labels, held-out evaluation consortia that resist optimization pressure, and reporting standards that require disclosure of all benchmarks attempted rather than only those where performance was favorable. The field’s maturation depends on developing evaluation cultures that reward honest assessment over leaderboard position.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-saturation-staleness",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-saturation-staleness",
    "title": "11  Benchmarks and Evaluation",
    "section": "11.6 Benchmark Saturation and Staleness",
    "text": "11.6 Benchmark Saturation and Staleness\nBenchmarks have finite useful lifetimes. As models improve, benchmarks saturate; as data and methods evolve, benchmarks become stale.\n\n11.6.1 Saturation: When Benchmarks Stop Discriminating\nA benchmark saturates when the best models achieve performance that cannot be meaningfully improved. Saturation may reflect fundamental limits (the benchmark approaches the Bayes error rate), measurement noise (the benchmark’s labels are too noisy to support finer discrimination), or ceiling effects (the metric itself cannot distinguish between excellent and perfect performance).\nSaturation is problematic because it removes the benchmark’s value for model selection. When all reasonable models achieve 0.97 auROC, differences between 0.970 and 0.975 are unlikely to reflect meaningful capability differences. Yet benchmark reporting conventions often emphasize such decimal places, creating an illusion of progress.\nDetecting saturation requires estimating the irreducible error. For benchmarks with replicate measurements, comparing model performance to replicate concordance provides an upper bound: models cannot systematically outperform the reproducibility of the underlying assay. For benchmarks without replicates, saturation is harder to diagnose. One heuristic is tracking the rate of improvement: when new methods provide diminishing gains despite substantial architectural innovations, saturation is likely.\nThe response to saturation should be moving to harder benchmarks that still discriminate between methods, developing new benchmarks that capture aspects of performance that existing benchmarks miss, and retiring saturated benchmarks from active leaderboard competition while retaining them as sanity checks.\n\n\n\n\n\n\n\n\nBenchmark saturation over time\n\n\n\n\n\n\n\nBenchmark staleness timeline\n\n\n\n\n\n\nFigure 11.3: Benchmark saturation and staleness limit evaluation validity. (A) Performance saturation: multiple model generations converge toward benchmark ceilings where marginal improvements no longer indicate meaningful advances. The saturation zone (shaded) suggests benchmarks have lost discriminative power. (B) Benchmark staleness: growing temporal gaps between benchmark creation and current evaluation. Benchmarks created years ago may reflect outdated annotations, superseded biological understanding, or distributions that no longer match contemporary data. Together, saturation and staleness motivate development of new benchmarks that capture aspects of biological prediction that current standards miss.\n\n\n\n\n\n11.6.2 Staleness: When Benchmarks Diverge from Practice\nBenchmarks become stale when they no longer reflect current data, methods, or clinical practice. Assays evolve: a benchmark constructed from early ENCODE data may not represent current experimental protocols. Annotations improve: gene models, variant classifications, and functional element maps are continuously updated. Clinical practice shifts: treatment guidelines and diagnostic criteria change the meaning of historical labels.\nStaleness is insidious because it erodes benchmark validity gradually rather than abruptly. A benchmark that accurately represented regulatory prediction in 2015 may systematically misrepresent it in 2025, yet the benchmark’s continued use perpetuates optimization for an outdated target.\nAddressing staleness requires periodic benchmark refresh with updated data and annotations, version control that documents exactly what each benchmark version contains, and awareness that performance on historical benchmarks may not predict performance on current data.\n\n\n11.6.3 Leakage from Scale\nModern foundation models are pretrained on corpora that may include most publicly available genomic data. This creates novel leakage risks distinct from classical train-test overlap. A model pretrained on all ENCODE data may effectively have seen the exact experiments used in many regulatory benchmarks. A model pretrained on all UniRef may have seen sequences highly similar to protein benchmark test sets. This pretraining-benchmark overlap inflates performance in ways that are difficult to detect and even more difficult to correct.\nLeakage from scale is particularly problematic because it is often undocumented. Model papers rarely enumerate exactly which datasets were included in pretraining corpora, and benchmark papers rarely specify which datasets should be excluded. The result is ambiguity about whether benchmark success reflects genuine generalization or memorization from pretraining.\nMitigating leakage from scale requires explicit documentation of pretraining corpora, tools or hashes that help identify overlap between pretraining data and benchmark test sets, and held-out evaluation consortia that reserve data specifically for assessment without any use in pretraining.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-deployment-gap",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-deployment-gap",
    "title": "11  Benchmarks and Evaluation",
    "section": "11.7 Benchmark-Deployment Gap",
    "text": "11.7 Benchmark-Deployment Gap\nHigh benchmark performance does not guarantee deployment success. Understanding why requires examining the systematic differences between benchmark settings and real-world applications.\n\n\n\n\n\n\nThe proxy-target gap in genomic AI evaluation\n\n\n\n\nFigure 11.4: The proxy-target gap in genomic AI evaluation. What benchmarks measure (left) differs systematically from what we ultimately want (right). ClinVar labels proxy for clinical impact but reflect curation biases and circularity with computational predictions. Held-out auROC proxies for deployment discrimination but assumes matched distributions. DMS correlations proxy for protein function but capture only selected perturbations in specific assay conditions. Expression prediction accuracy proxies for regulatory understanding but may reflect technical confounds. Arrow widths indicate proxy strength; gap annotations identify sources of misalignment. The central insight: high benchmark performance does not guarantee deployment success.\n\n\n\n\n11.7.1 Distribution Shift\nBenchmark test sets sample from the same distribution as training sets. Deployment populations may differ systematically. For variant effect prediction, benchmark variants are typically common enough to appear in multiple databases, while deployment often targets rare variants seen in single individuals. For regulatory prediction, benchmarks derive from well-studied cell types and tissues, while deployment may require prediction in understudied contexts.\nDistribution shift manifests as degraded performance, but the pattern of degradation varies. The transfer learning framework in ?sec-ch09-domain-shift examines how models handle distribution shift from a methodological perspective, while Section 11.7.1 addresses the confounding implications when shift correlates with protected attributes. Some models degrade gracefully, maintaining reasonable accuracy across the distribution shift. Others degrade catastrophically, with confident predictions that prove systematically wrong. Benchmarks that include held-out subpopulations or out-of-distribution test sets provide some information about robustness, but cannot anticipate every deployment scenario.\n\n\n11.7.2 Calibration Requirements\nClinical deployment requires not just accurate rankings but accurate probability estimates (Section 23.2). A variant classifier that achieves 0.95 auROC by assigning probability 0.9 to all pathogenic variants and 0.3 to all benign variants discriminates well but provides miscalibrated uncertainty. Clinical decisions that depend on thresholded predictions (reporting variants above a certain probability) will perform poorly if those probabilities do not reflect actual pathogenicity rates.\nMost benchmark metrics emphasize discrimination over calibration. auROC is invariant to monotonic transformations of predicted probabilities. Correlation measures rank preservation. As a result, models may be optimized for benchmark success through strategies that damage calibration. The benchmark-deployment gap for calibration can be large even when discrimination metrics are excellent.\n\n\n11.7.3 Metric Mismatch\nBenchmarks optimize specific metrics that may not align with deployment objectives. auROC weights errors equally regardless of where they occur on the score distribution, but clinical utility may depend primarily on performance at specific operating points. Correlation rewards getting the overall pattern right but may not penalize systematic errors in clinically important regions.\nThe gap between optimized metrics and deployment objectives creates misaligned incentives. Model developers optimize for benchmark success, which rewards specific metric improvements. Deployment success may require different tradeoffs: prioritizing calibration over discrimination, minimizing false negatives over false positives, or performing well on specific subpopulations rather than overall.\n\n\n11.7.4 Practical Constraints\nDeployment environments impose constraints that benchmarks typically ignore. Inference speed matters when predictions must be returned in clinical timescales. Model size matters when deployment hardware has limited memory. Interpretability matters when predictions must be explained to clinicians or patients (Chapter 24). Benchmarks that evaluate only accuracy miss these dimensions of deployment fitness.\nThe benchmark-deployment gap is not merely a technical inconvenience. It represents a fundamental tension between evaluation tractability and deployment validity. Benchmarks are valuable precisely because they are standardized, reproducible, and comparable across methods. Deployment is valuable precisely because it addresses the specific needs of real-world applications. Bridging this gap requires benchmark designs that better approximate deployment conditions and deployment evaluations that provide feedback to benchmark development.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-systematic-gaps",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-systematic-gaps",
    "title": "11  Benchmarks and Evaluation",
    "section": "11.8 Systematic Gaps in Current Benchmarks",
    "text": "11.8 Systematic Gaps in Current Benchmarks\nDespite the proliferation of benchmark suites, systematic gaps remain in the genomic evaluation landscape.\nVariant types remain inadequately covered: structural variants, inversions, copy number variants, and complex rearrangements are rarely evaluated despite accounting for substantial genomic variation and disease burden (Section 1.5.4). Repeat regions are often excluded or masked. Multi-variant effects and haplotype-specific phenomena receive minimal attention; the phasing challenges that underlie compound heterozygosity interpretation (Section 1.4.1) rarely appear in benchmark settings.\nPopulation representation shows profound disparities: non-European ancestry groups remain severely underrepresented (Section 3.7). The confounding implications of this underrepresentation extend beyond benchmark validity to fairness concerns examined in ?sec-ch22-fairness. Performance stratified by ancestry reveals gaps that aggregate metrics conceal. Environmental diversity (lifestyle, exposures, treatments) that shapes phenotypic expression is rarely incorporated.\n\n\n\n\n\n\nCross-population performance reveals systematic failures\n\n\n\n\nFigure 11.5: Cross-population performance reveals systematic failures for underrepresented ancestries. Relative performance (European as reference) degrades substantially for non-European groups across multiple model types: polygenic risk scores (blue), variant classifiers (green), and regulatory predictors (orange). African-ancestry individuals show 40-75% performance reductions despite constituting only 2% of typical training data while representing 16% of the global population. These disparities reflect both training data composition (inset: ~78% European) and fundamental differences in linkage disequilibrium structure across populations. Aggregate benchmark metrics that do not report stratified performance by ancestry conceal these failures.\n\n\n\nModality coverage remains uneven: long-read sequencing data is scarce in benchmarks despite its advantages for structural variants and phasing (Section 1.2.4). Single-cell benchmarks are emerging but remain limited compared to bulk assay benchmarks; the evaluation challenges specific to single-cell models are examined in ?sec-ch16-evaluation. Spatial transcriptomics and other emerging modalities have minimal coverage, though multi-omic integration approaches (Chapter 22) are beginning to address cross-modality assessment.\nClinical endpoints are underrepresented: most benchmarks use molecular surrogates rather than hard clinical endpoints. Disease incidence, progression, treatment response, and patient-reported outcomes are rarely the direct prediction target. The gap between molecular proxy accuracy and clinical utility remains poorly characterized.\nThese gaps mean that strong benchmark performance may not predict utility for underserved populations, understudied variant classes, or clinical applications that depend on endpoints the benchmarks do not measure.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-proxy-problem",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-proxy-problem",
    "title": "11  Benchmarks and Evaluation",
    "section": "11.9 The Proxy Problem",
    "text": "11.9 The Proxy Problem\nBenchmarks structure the incentives of genomic AI development. The specific tasks, metrics, and leaderboards that the community adopts determine what models are optimized for, what claims of progress are evaluated against, and what capabilities receive attention versus neglect. A benchmark that emphasizes European-ancestry variants produces models tuned for European-ancestry performance. A benchmark that rewards discrimination (auROC) over calibration produces models that rank variants well but estimate probabilities poorly. A benchmark that reuses training data from widely available resources creates indirect leakage that inflates apparent performance. The benchmark landscape is not neutral infrastructure but an active force shaping what the field builds.\nThe landscape surveyed here spans protein benchmarks (TAPE, FLIP, ProteinGym), DNA and regulatory benchmarks (Genomic Benchmarks, BEND), variant effect benchmarks (ClinVar, CAGI, DMS), and trait-level benchmarks (TraitGym, EmbedGEM). Across all categories, persistent challenges emerge: saturation that reduces discriminative power as models approach ceiling performance, staleness that erodes validity as benchmarks age, leakage risks that inflate apparent capabilities, and systematic gaps in population diversity, variant type coverage, and clinical endpoint representation.\nThe benchmark-deployment gap represents perhaps the most consequential limitation. Strong performance on established benchmarks does not guarantee that models will behave reliably when deployed in clinical or research settings with different data distributions, patient populations, or outcome definitions. Proper benchmark use requires attention to experiment design, metric selection, and common pitfalls (Chapter 12). The confounding issues that plague both benchmark construction and model training receive dedicated treatment in Chapter 12, while uncertainty quantification methods (Chapter 23) provide tools for assessing when benchmark performance translates to deployment confidence. Interpretability approaches (Chapter 24) reveal whether benchmark success reflects genuine biological learning or exploitation of shortcuts. Together with this catalog of what benchmarks exist, these methodological principles provide the critical apparatus for evaluating genomic foundation model claims.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-evaluation-methodology",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-evaluation-methodology",
    "title": "11  Benchmarks and Evaluation",
    "section": "11.10 Evaluation Methodology",
    "text": "11.10 Evaluation Methodology\nThe preceding sections examined what benchmarks measure. This section examines how to evaluate models properly—the methodological foundations that determine whether benchmark results translate to deployment success.\n\n\n\n\n\n\nDifficulty Warning: Methodological Rigor\n\n\n\nThe following sections on evaluation methodology require careful attention. The concepts of leakage, confounding, and proper experimental design are subtle but essential. A model developer who masters these principles will avoid the common pitfalls that produce misleading benchmark results. Take time with each section; the investment will pay dividends in every evaluation you conduct.\n\n\nCD-HIT: Li W, Godzik A. Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences. Bioinformatics. 2006;22(13):1658-1659. doi:10.1093/bioinformatics/btl158 MMseqs2: Steinegger M, Söding J. MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature Biotechnology. 2017;35(11):1026-1028. doi:10.1038/nbt.3988\nStatistical Methods\nDeLong’s method: DeLong ER, DeLong DM, Clarke-Pearson DL. Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. Biometrics. 1988;44(3):837-845. Benjamini-Hochberg procedure: Benjamini Y, Hochberg Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society, Series B. 1995;57(1):289-300. doi:10.1111/j.2517-6161.1995.tb02031.x\nKinship Estimation\nKING: Manichaikul A, Mychaleckyj JC, Rich SS, Daly K, Sale M, Chen WM. Robust relationship inference in genome-wide association studies. Bioinformatics. 2010;26(22):2867-2873. doi:10.1093/bioinformatics/btq559\nCalibration Methods\nPlatt scaling: Platt J. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers. 1999;10(3):61-74. Temperature scaling: Guo C, Pleiss G, Sun Y, Weinberger KQ. On calibration of modern neural networks. Proceedings of the 34th International Conference on Machine Learning (ICML). 2017;70:1321-1330.\nOptional (Twilight Zone Reference)\nTwilight zone (30% sequence identity): Rost B. Twilight zone of protein sequence alignments. Protein Engineering. 1999;12(2):85-94. doi:10.1093/protein/12.2.85 :::",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-random-splits-fail",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-random-splits-fail",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.1 Why Random Splits Fail",
    "text": "12.1 Why Random Splits Fail\nThe standard machine learning recipe calls for randomly partitioning data into training, validation, and test sets. For image classification or sentiment analysis, this approach works well because individual examples are approximately independent. A photograph of a cat shares no special relationship with another photograph of a different cat beyond their common label. Random assignment ensures that training and test distributions match, and performance on the test set provides an unbiased estimate of performance on new examples from the same distribution.\nGenomic data violates these assumptions at every level. Consider a protein dataset where the goal is to predict stability from sequence. Proteins in the same family share evolutionary history and often similar structures. If a training set includes beta-lactamase variants from E. coli and the test set includes beta-lactamase variants from Klebsiella, the model may appear to generalize to “new” proteins while actually recognizing sequence patterns it saw during training. The test performance reflects memorization of family-specific features rather than general principles of protein stability.\n\n\n\n\n\n\n\n\nImages: Independent samples\n\n\n\n\n\n\n\nProteins: Related by evolution\n\n\n\n\n\n\n\nVariants: Multiple dependencies\n\n\n\n\n\n\nFigure 12.1: Why random data splits fail for genomic machine learning. (A) Image classification: samples are truly independent, so random assignment provides valid performance estimates. (B) Protein classification: evolutionary relationships create dependencies; random splits place homologs (&gt;80% identity) across train/test, enabling memorization of shared sequences rather than learning generalizable features. (C) Variant prediction: multiple dependencies compound—same genes across splits, related individuals sharing rare variants, and population structure confounding features and labels. These dependencies require structured splitting strategies that explicitly account for sequence homology, family relatedness, and population structure.\n\n\n\nThe problem compounds when sequence identity is high. Two proteins sharing 80% sequence identity will typically have similar structures and functions. A model trained on one and tested on the other is not really being tested on a novel example; it is being asked to interpolate within a region of sequence space it has already explored. Even at 30% sequence identity, the so-called “twilight zone” of homology detection (rost_twilight_1999?), proteins often share structural and functional similarities that can be exploited by sufficiently powerful models.\nVariant-level data presents analogous challenges. Variants within the same gene share genomic context, and variants affecting the same protein domain share structural environment. Variants from the same individual share haplotype background. Variants from the same population share allele frequency distributions shaped by demographic history. Each of these relationships creates opportunities for models to learn shortcuts that generalize within the training distribution but fail on genuinely novel examples.\n\n\n\n\n\n\nKey Insight: The Independence Assumption\n\n\n\nThe fundamental issue is that genomic data points are not independent. Random splits assume independence; when this assumption is violated, the test set no longer provides an unbiased estimate of generalization. The consequence is systematic overestimation of performance. A model that achieves 0.90 auROC with random splitting might achieve only 0.75 auROC when evaluated on truly held-out examples, with the gap reflecting how much the model learned about biology versus how much it learned about the structure of the training data.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-homology-aware-splitting",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-homology-aware-splitting",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.2 Homology-Aware Splitting",
    "text": "12.2 Homology-Aware Splitting\nThe solution to homology-driven leakage is to explicitly account for sequence similarity when constructing data splits. Rather than random assignment, examples are clustered by sequence identity, and entire clusters are assigned to training, validation, or test sets. This ensures that no test example is “too similar” to any training example, forcing the model to demonstrate genuine generalization.\n\n\n\n\n\n\nHomology-aware splitting workflow\n\n\n\n\nFigure 12.2: Homology-aware splitting prevents sequence similarity leakage. Step 1: Start with all sequences in the dataset. Step 2: Cluster sequences by similarity using tools like CD-HIT or MMseqs2 at an appropriate threshold (30% identity typical for diverse protein tasks). Step 3: Visualize clusters showing which sequences are related. Step 4: Assign entire clusters to single splits—no cluster is divided across train/validation/test. Step 5: Validate that no test sequence exceeds the identity threshold with any training sequence. The threshold determines evaluation stringency: 30% identity tests distant generalization; 50% tests moderate generalization; higher thresholds permit more similarity but provide weaker generalization evidence.\n\n\n\n\n12.2.1 Clustering Tools and Workflows\nTwo tools dominate homology-aware splitting in practice. CD-HIT clusters sequences by greedy incremental clustering, assigning each sequence to an existing cluster if it exceeds a similarity threshold to the cluster representative, or creating a new cluster otherwise (li_cd-hit_2006?). Why greedy incremental rather than optimal clustering? Computing all-versus-all pairwise similarities for millions of sequences would require billions of comparisons, making optimal clustering computationally infeasible. The greedy approach processes sequences one at a time, comparing each new sequence only to existing cluster representatives rather than to all sequences. This reduces complexity from quadratic to roughly linear in the number of sequences, enabling practical application to genomic-scale datasets. The tradeoff is that cluster assignments depend on input order and may not be globally optimal, but for splitting purposes this approximation suffices. The algorithm is fast and scales to millions of sequences. For proteins, a typical workflow clusters at 40% sequence identity for stringent splitting or 70% for moderate splitting. For nucleotide sequences, thresholds are typically higher (80-95%) due to different evolutionary rates.\nMMseqs2 offers faster clustering with similar sensitivity, becoming essential for large-scale analyses (steinegger_mmseqs2_2017?). The tool supports multiple clustering modes and can handle databases with hundreds of millions of sequences. For foundation model pretraining where deduplication affects billions of sequences, MMseqs2 is often the only practical option.\n\n\n\n\n\n\nPractical Guidance: Choosing Identity Thresholds\n\n\n\nThe choice of identity threshold involves trade-offs:\n\n\n\n\n\n\n\n\n\nThreshold\nProteins\nNucleotides\nTrade-off\n\n\n\n\nStringent\n30-40%\n80-85%\nHardest test; may lack training data\n\n\nModerate\n50-70%\n85-90%\nBalanced; typical benchmark choice\n\n\nPermissive\n80-90%\n95%+\nRetains data but allows some leakage\n\n\n\nRule of thumb for proteins: Use 40% identity for variant effect prediction, 30% for structure prediction (where even distant homologs share structure).\nRule of thumb for DNA: Use 80% for regulatory prediction, but consider gene-family splits instead of sequence identity alone.\n\n\n\n\n12.2.2 Practical Considerations\nSeveral subtleties affect the quality of homology-aware splits. When one cluster contains half the data and is assigned to training, the remaining clusters may be too small or too biased to serve as representative test sets. This cluster size distribution problem can be mitigated through stratified sampling within clusters or careful balancing across splits, ensuring that test sets contain sufficient examples across the label distribution.\nPairwise clustering can miss hidden relationships that arise through transitive homology. Protein A may share 35% identity with protein B, and protein B may share 35% identity with protein C, yet A and C share only 20% identity. If A is in training and C is in testing, B serves as an indirect bridge that allows information to leak between splits despite no direct high-identity pair spanning them. Why does this transitive leakage matter when A and C are dissimilar? The model does not need to memorize specific sequences; it needs only to learn patterns. Information about C-like sequences can flow through B: patterns learned from B (which is similar enough to A to share features) may transfer to C (which is similar enough to B to benefit from those features). This chain of similarity creates a gradient of information flow even when the endpoints share little direct similarity. Connected component analysis or multi-step clustering can address these transitive relationships, though at increased computational cost.\nMulti-domain proteins complicate whole-protein clustering because different domains may have different evolutionary histories. A protein may share one domain with training proteins and another domain with test proteins. Whether this represents leakage depends on the prediction task: if predicting whole-protein function, shared domains matter; if predicting domain-specific properties, they matter more acutely. Domain-aware splitting assigns domains rather than whole proteins to clusters, though this requires domain annotation that may not always be available.\nFor genomic (non-protein) sequences, repeat elements and transposable elements create analogous challenges. A model trained to predict chromatin state may learn features of LINE elements that recur throughout the genome. Excluding repetitive regions from evaluation or explicitly accounting for repeat content can clarify what the model has actually learned about regulatory sequences versus repetitive element patterns.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-splitting-biological-axis",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-splitting-biological-axis",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.3 Splitting by Biological Axis",
    "text": "12.3 Splitting by Biological Axis\nBeyond sequence homology, genomic data admits multiple axes along which splits can be constructed. The choice of axis determines what kind of generalization is being tested.\n\n\n\n\n\n\nStop and Think\n\n\n\nYou’re building a variant pathogenicity predictor for clinical use. You could split your data by: (A) random 80/20, (B) by chromosome, (C) by gene family, or (D) by patient ancestry. Each tests different generalization. Which splitting strategy would best simulate real clinical deployment? Why might you want to try multiple strategies?\nConsider: In the clinic, which of these splits most closely resembles encountering a truly novel variant?\n\n\n\n\n\n\n\n\nSplitting strategies test different aspects of generalization\n\n\n\n\nFigure 12.3: Splitting strategies test different aspects of generalization. Each strategy (rows) addresses specific dependencies (columns): random splits address none; individual-aware prevents sample-level leakage; family-aware accounts for relatedness; chromosome holdout tests cross-genome transfer; gene/protein family prevents homology leakage; cohort/site holdout tests deployment robustness; temporal splits simulate prospective use; ancestry stratification reveals population biases. No single strategy addresses all dependencies; rigorous evaluation combines multiple strategies. Stricter splits produce lower but more realistic performance estimates that better predict clinical deployment success.\n\n\n\n\n12.3.1 Splitting by Individual\nFor tasks involving human genetic variation, ensuring that data from the same individual (or related individuals) does not appear in both training and test sets is essential. A variant effect predictor trained on variants from person A and tested on other variants from person A may learn individual-specific patterns, such as haplotype structure or ancestry-correlated allele frequencies, that do not generalize to new individuals.\nFamily structure creates subtler leakage. First-degree relatives share approximately 50% of their genomes identical by descent. Even distant relatives share genomic segments that can be exploited by sufficiently powerful models. Best practice involves computing kinship coefficients across all individuals and either excluding one member of each related pair or assigning entire family clusters to the same split. The UK Biobank provides pre-computed relatedness estimates; other cohorts may require explicit calculation using tools like KING or PLINK. [Citation Needed]\n\n\n12.3.2 Splitting by Genomic Region\nChromosome-based splits assign entire chromosomes to training or testing. This approach is common in regulatory genomics, where models trained on chromosomes 1-16 are tested on chromosomes 17-22 (or similar partitions). The advantage is simplicity and reproducibility; the disadvantage is that chromosomes are not independent. Chromosome 6 contains the HLA region with its unusual patterns of variation and selection; chromosome 21 is small and gene-poor; sex chromosomes have distinct biology. Results may vary substantially depending on which chromosomes are held out.\nRegion-based splits hold out contiguous segments (e.g., 1 Mb windows) distributed across the genome. This provides more uniform coverage than chromosome splits but requires careful attention to boundary effects. If a regulatory element spans the boundary between training and test regions, parts of its context may leak into training.\n\n\n12.3.3 Splitting by Gene or Protein Family\nFor variant effect prediction, holding out entire genes or protein families tests whether models learn general principles versus gene-specific patterns. A model that achieves high accuracy by memorizing that TP53 variants are often pathogenic has not demonstrated understanding of mutational mechanisms. Gene-level splits force models to generalize to genes they have never seen, providing stronger evidence of biological insight.\nFamily-level splits extend this logic to groups of related genes. Holding out all kinases or all GPCRs tests whether models can generalize across evolutionary families. This is particularly stringent for protein structure and function prediction, where family membership strongly predicts properties.\n\n\n\n\n\n\nStop and Think: Choosing the Right Split\n\n\n\nConsider a project to predict whether coding variants cause loss of protein function. You have variants from 1000 genes, with 50-100 variants per gene. Which splitting strategy would you choose, and why?\nA. Random split (80/10/10) B. Chromosome-based (train on chr1-18, test on chr19-22) C. Gene-based (train on 800 genes, test on 200 held-out genes) D. Individual-based (split by patient ID)\nConsider: What would each split actually test? Which shortcuts could models exploit?\n\n\n\n\n12.3.4 Splitting by Experimental Context\nMulti-task models that predict chromatin marks across cell types can be split by cell type rather than genomic position. Training on liver, lung, and brain while testing on heart and kidney assesses whether learned regulatory logic transfers across tissues. This matters because cell-type-specific factors drive much of regulatory variation; a model that has simply learned which regions are accessible in the training cell types may fail on novel cell types even when sequence features should transfer.\nSimilarly, models can be split by assay type (e.g., training on ATAC-seq, testing on DNase-seq), laboratory (to assess batch effects), or time point (for longitudinal data). Each split tests a different axis of generalization.\n\n\n12.3.5 Splitting by Ancestry\nFor human genomic applications, ancestry-stratified evaluation has become essential. Models trained predominantly on European ancestry cohorts often show degraded performance in African, East Asian, South Asian, and admixed populations. This degradation reflects both differences in allele frequency spectra and differences in linkage disequilibrium patterns that affect which variants are informative.\nBest practice reports performance separately for each major ancestry group represented in the data. When held-out ancestry groups are available (e.g., training on Europeans and testing on Africans), this provides the strongest test of cross-population generalization. When only European data are available, this limitation should be explicitly acknowledged, and claims about generalization should be appropriately modest. The confounding effects of ancestry on genomic predictions are detailed in Chapter 12.\n\n\n12.3.6 Splitting by Time\nTemporal splits assign data to training and test sets based on when observations were collected, annotations were created, or variants were classified. This strategy tests whether models generalize forward in time, the actual deployment scenario for any predictive system.\nFor variant pathogenicity prediction, temporal splits are particularly revealing. ClinVar (Section 2.8.1) provides submission dates enabling clean temporal partitioning. Training on ClinVar annotations from 2018 and testing on variants first classified in 2022 asks whether the model can predict labels that did not yet exist during training. This avoids the circularity that arises when training and test labels were assigned by similar processes at similar times. Variants classified more recently may reflect updated curation standards, new functional evidence, or reclassifications of previously uncertain variants; a model that performs well on these genuinely new classifications demonstrates predictive validity rather than recapitulation of historical curation patterns.\nImplementing temporal splits requires metadata that many datasets lack. ClinVar provides submission dates, enabling clean temporal partitioning. UniProt tracks annotation dates for functional assignments. Clinical cohorts with longitudinal follow-up naturally admit temporal splits based on diagnosis dates. When temporal metadata is unavailable, publication dates of source literature can serve as proxies, though these may not perfectly reflect when information became available to model developers.\nThe key limitation of temporal splits is non-stationarity. The distribution of variants classified in 2022 may differ systematically from those classified in 2018, not because biology changed but because research priorities, sequencing technologies, and ascertainment patterns evolved. Performance degradation on temporally held-out data may reflect distribution shift rather than genuine failure to generalize. Combining temporal splits with stratified analysis (performance by variant type, gene category, or evidence strength) helps disentangle these factors.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-leakage-detection",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-leakage-detection",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.4 Leakage Taxonomy and Detection",
    "text": "12.4 Leakage Taxonomy and Detection\nEven with careful splitting, leakage can enter evaluations through multiple pathways. A variant effect predictor that achieves 0.95 auROC on held-out test data may be exploiting information that would never exist for truly novel variants, rendering the performance estimate meaningless for clinical deployment. Understanding common leakage patterns helps practitioners design cleaner evaluations and critically assess published results.\nGenomic machine learning faces four distinct leakage types, each creating different pathways for inflated performance: label leakage, feature leakage, temporal leakage, and benchmark leakage. These categories are not mutually exclusive; a single evaluation may suffer from multiple forms simultaneously, with compounding effects on apparent performance.\n\n\n\nTable 12.1: The four major leakage types in genomic machine learning, with detection strategies for each.\n\n\n\n\n\n\n\n\n\n\n\nLeakage Type\nDefinition\nExample\nDetection Strategy\n\n\n\n\nLabel leakage\nTarget labels derived from features the model can access\nClinVar classifications informed by SIFT/PolyPhen scores\nCompare to baseline using only those features\n\n\nFeature leakage\nInput features encode future or target information\nConservation scores for pathogenicity prediction\nAblate suspicious features; measure degradation\n\n\nTemporal leakage\nUsing future information to predict past\nTraining on 2023 labels to predict 2020 classifications\nStrict temporal splits with date metadata\n\n\nBenchmark leakage\nTest set construction influenced by evaluated methods\nBenchmark selected proteins with good sequence coverage\nCheck benchmark construction procedure\n\n\n\n\n\n\n\n12.4.1 Label Leakage\nLabel leakage occurs when target labels are derived from information that the model can access through its features. The classic example is training pathogenicity predictors on ClinVar annotations while using sequence features that contributed to those annotations. If ClinVar curators used SIFT and PolyPhen scores when classifying variants, and the new model uses similar sequence features, high performance may reflect recapitulation of curation criteria rather than independent predictive power.\nThe ClinVar circularity problem represents a particularly insidious form of label leakage. When computational predictions contributed to the pathogenicity classifications that later become training labels, new models learn to replicate their predecessors rather than discover independent signal. This circularity propagates through generations of models, each inheriting and reinforcing the biases of earlier predictors. The circularity problem for classical variant effect predictors is examined in Section 4.5, with broader treatment of how such label contamination creates confounded evaluations in ?sec-ch22-label-bias.\nExpression models face analogous challenges when trained on features derived from the same samples used to define expression labels. The information flow becomes circular: labels inform features, which predict labels, creating apparent performance that would not generalize to independent samples.\n\n\n12.4.2 Feature Leakage\nFeature leakage occurs when input features encode information about the target that would not be available at prediction time. In genomics, conservation scores are a common source. If a model uses PhyloP scores as features and the target is pathogenicity, the model may learn that conserved positions are more likely pathogenic without learning anything about variant-specific biology. This would be appropriate if conservation scores are intended to be part of the prediction pipeline, but problematic if the goal is to develop a model that predicts pathogenicity from sequence alone.\nSimilarly, population allele frequency encodes selection pressure. A model that learns “rare variants are more likely pathogenic” has discovered a useful heuristic but not necessarily mechanistic understanding. Whether this counts as leakage depends on the intended use case. For clinical variant interpretation where allele frequency is always available, exploiting this feature is appropriate. For understanding variant biology, it may mask whether the model has learned anything beyond frequency-based priors.\nFeature leakage also arises when features encode information about data partitions or batch structure rather than biology. Coverage patterns that differ systematically between cases and controls, quality metrics that correlate with sequencing center, or variant density profiles that reflect caller-specific behavior all constitute feature leakage of this form.\n\n\n12.4.3 Temporal Leakage\nTemporal leakage violates the causal structure of prediction by using future information to predict past events. A model trained on ClinVar annotations from 2023 and tested on annotations that were uncertain in 2020 may perform well because new annotations were informed by model-like predictions. The apparent validation is circular: the model predicts labels that were partially derived from model-like reasoning applied after the prediction timepoint.\nClinical outcome prediction faces similar risks when laboratory values, imaging results, or clinical notes recorded after the prediction timepoint enter the feature set. A model predicting 30-day mortality that includes vital signs from day 15 has trivial access to outcome-correlated information. Proper temporal splits must respect not only when samples were collected but when each feature became available.\nTraining on variants classified in 2023 to predict classifications that were uncertain in 2020 allows models to learn from reclassification patterns rather than intrinsic variant properties. The model exploits the trajectory of scientific knowledge rather than the underlying biology.\n\n\n12.4.4 Benchmark Leakage\nBenchmark leakage occurs when test set construction was influenced by methods similar to those being evaluated. If a protein function benchmark was created by selecting proteins with high-confidence annotations, and those annotations were partly derived from sequence similarity searches, sequence-based models may perform well by exploiting the same similarity that guided benchmark construction.\nFoundation models face particular challenges with benchmark leakage. If a DNA language model is pretrained on all publicly available genomic sequence including ENCODE data, and then evaluated on ENCODE-derived benchmarks, the pretraining has exposed the model to information about the test distribution even if specific test examples were held out. The model may have learned statistical patterns in ENCODE data that transfer to ENCODE benchmarks without reflecting genuine biological understanding.\nThis form of leakage is especially difficult to detect because it operates at the level of distributional overlap rather than specific example memorization. A model that has never seen a particular test sequence may still have learned the statistical regularities that make that sequence predictable within the benchmark distribution.\n\n\n12.4.5 Detecting Leakage\nSeveral strategies help detect leakage, though none provides definitive proof of its absence. These approaches complement each other; rigorous evaluation employs multiple strategies, recognizing that each catches different leakage pathways while remaining blind to others.\n\n\n\n\n\n\nPractical Guidance: Leakage Detection Checklist\n\n\n\nWhen evaluating your own model or reviewing published results, work through these detection strategies:\n\nBaseline analysis: Does a simple model using only potentially leaky features (allele frequency, conservation) achieve similar performance?\nFeature ablation: Does removing suspicious features cause dramatic performance drops?\nConfounder analysis: Does performance remain after conditioning on potential confounders (gene, ancestry, batch)?\nTemporal validation: Does performance hold on prospectively collected data?\nOverlap audit: Has the overlap between pretraining data and benchmark test sets been documented and checked?\n\n\n\nSimple models that could not plausibly have learned biology provide an essential baseline analysis. If a linear model using only allele frequency achieves 0.80 auROC on a pathogenicity benchmark, and a sophisticated deep model achieves 0.82, the marginal improvement may not justify claims of biological insight. The deep model’s performance is bounded by what simple confounders already explain.\nSystematic feature ablation removes potentially leaky features and measures performance degradation. If removing conservation scores causes a 20-point drop in auROC, the model was heavily dependent on conservation rather than learning independent predictors. This approach identifies which features drive performance but cannot distinguish legitimate signal from leakage without domain knowledge about what information should be available at prediction time.\nExplicit confounder analysis models potential confounders and tests whether model predictions remain informative after conditioning. If a variant effect predictor’s scores become non-predictive after controlling for gene length and expression level, the model may have learned gene-level confounders rather than variant-level effects. Chapter 12 examines how leakage relates to these broader confounding structures.\nTemporal validation evaluates models on data collected after the training data was frozen. If performance degrades substantially on newer data, the model may have been fitted to temporal artifacts in the original dataset. This approach is particularly valuable for detecting temporal leakage but requires access to prospectively collected data.\nFinally, overlap auditing explicitly checks for sequence or sample overlap between pretraining corpora and evaluation benchmarks. For foundation models, this requires documenting pretraining data composition and comparing against benchmark construction procedures. The audit may reveal that apparent generalization is actually interpolation within seen distributions.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-metrics-genomic-tasks",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-metrics-genomic-tasks",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.5 Metrics for Genomic Tasks",
    "text": "12.5 Metrics for Genomic Tasks\nMetrics quantify model performance but different metrics answer different questions. Choosing appropriate metrics requires clarity about what aspect of performance matters for the intended application.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading about specific metrics, consider: You’re evaluating a variant pathogenicity predictor where only 0.5% of variants are truly pathogenic. Your model achieves auROC = 0.92. Is this good? What other information do you need to decide whether to use this model in practice?\nHint: Think about what happens when you apply a threshold to actually flag variants for follow-up.\n\n\n\n\n\n\n\n\nMetric selection flowchart\n\n\n\n\nFigure 12.4: Metric selection depends on task characteristics and deployment requirements. Binary classification requires different metrics depending on class balance: auROC provides stable ranking assessment for balanced data, while auPRC is essential for imbalanced settings where auROC can mislead. Probability calibration matters for deployment: a model may rank correctly while producing systematically overconfident probabilities. Clinical decision-making requires metrics like net benefit that account for the costs of different error types. Continuous predictions use correlation and error metrics, but high correlation does not guarantee clinical utility—a model may track relative differences while failing to capture absolute effect sizes. Selecting appropriate metrics prevents optimizing for misleading targets.\n\n\n\n\n12.5.1 Discrimination Metrics\nFor binary outcomes (pathogenic versus benign, bound versus unbound, accessible versus closed), discrimination metrics assess how well the model separates classes. The auROC measures the probability that a randomly selected positive example is ranked above a randomly selected negative example. auROC is threshold-independent and widely reported but can be misleading when classes are highly imbalanced.\nThe auPRC better reflects performance when positives are rare. For variant pathogenicity prediction, where perhaps 1% of variants are truly pathogenic, a model achieving 0.95 auROC might still have poor precision at useful recall levels. auPRC directly captures the precision-recall trade-off that matters for applications requiring both high sensitivity and manageable false positive rates.\n\n\n\n\n\n\nKey Insight: Why auROC Can Mislead\n\n\n\nThe distinction between auROC and auPRC reflects a mathematical property with practical consequences:\n\nauROC is invariant to class imbalance: A model’s auROC remains identical whether 1% or 50% of examples are positive, because it measures pairwise ranking between one positive and one negative.\nThis invariance becomes a liability in deployment: A model with 0.95 auROC applied to a dataset where 0.1% of variants are pathogenic might flag 100 false positives for every true positive at a threshold capturing 80% of positives.\n\nRule of thumb: Report both auROC (for comparison across datasets) and auPRC (for realistic assessment of deployment utility). When in doubt, auPRC is the more honest metric for imbalanced problems.\n\n\nThis same invariance becomes a liability when evaluating for deployment. A model with 0.95 auROC applied to a dataset where 0.1% of variants are pathogenic might flag 100 false positives for every true pathogenic variant at a threshold capturing 80% of positives. The auROC provides no warning of this behavior because it treats a positive-to-negative pair the same regardless of how many negatives exist. For any application where false positives carry real costs (manual curation, clinical follow-up, unnecessary patient anxiety), auROC presents an optimistic picture that collapses upon deployment.\nauPRC explicitly accounts for the negative class size. When positives are rare, achieving high precision requires a model that scores the vast majority of negatives lower than positives, not just a typical negative. This makes auPRC sensitive to class imbalance in exactly the way deployment is sensitive to class imbalance. A model moving from a balanced benchmark to a 1000:1 imbalanced application will show stable auROC but declining auPRC, mirroring the actual increase in false discovery rate users will experience. For this reason, auPRC (or equivalently, average precision) should be the primary metric when the deployment class distribution is known and imbalanced.\nThreshold-dependent metrics including sensitivity, specificity, positive predictive value, and negative predictive value require specifying a decision threshold. These metrics are more interpretable for specific use cases (e.g., “the model identifies 90% of pathogenic variants while flagging only 5% of benign variants as false positives”) but require choosing thresholds that may not generalize across settings.\n\n\n12.5.2 Regression and Correlation Metrics\nFor continuous predictions (expression levels, effect sizes, binding affinities), correlation metrics assess agreement between predicted and observed values. Pearson correlation measures linear association; Spearman correlation measures rank association and is robust to nonlinear relationships. The coefficient of determination (\\(R^2\\)) measures variance explained, though interpretation requires care when baseline performance is near zero.\nFor predictions at genomic scale (e.g., predicted versus observed expression across thousands of genes), these metrics may obscure important patterns. A model might achieve high genome-wide correlation by correctly predicting which genes are highly expressed while failing on the genes where predictions matter most. Task-specific stratification, such as correlation within expression quantiles or among disease-relevant genes, provides more actionable information.\n\n\n12.5.3 Ranking and Prioritization Metrics\nMany genomic workflows care about ranking rather than absolute prediction. Variant prioritization pipelines rank candidates for follow-up; gene prioritization ranks targets for experimental validation. Top-k recall measures the fraction of true positives captured in the top \\(k\\) predictions. Enrichment at k compares the true positive rate in the top \\(k\\) to the background rate. Normalized discounted cumulative gain (NDCG) weights ranking quality by position, penalizing relevant items placed lower in the list more than those placed near the top. Why penalize by position rather than treating all rankings equally? The cost of ranking a true positive lower depends on where it falls in the list. A pathogenic variant ranked 5th instead of 1st will still be found quickly; the same variant ranked 500th instead of 495th has already been effectively lost in the noise. NDCG captures this intuition through logarithmic discounting: moving an item from position 10 to position 2 improves the score more than moving it from position 100 to position 92, because early ranks matter more for workflows with finite follow-up capacity.\nThese metrics align with how predictions are actually used. If experimental capacity permits validating only 20 variants per locus, top-20 recall matters more than global auROC. Reporting both global metrics and rank-aware metrics at relevant cutoffs provides a complete picture.\n\n\n12.5.4 Clinical Utility Metrics\nFor clinical applications, discrimination and calibration are necessary but not sufficient. Decision curves plot net benefit across decision thresholds, where net benefit weighs the value of true positives against the cost of false positives at each threshold. A model may achieve high auROC but offer no net benefit at clinically relevant thresholds if it fails to discriminate in the region where decisions are actually made.\nNet reclassification improvement (NRI) measures how often adding genomic features to a clinical model changes risk classifications in the correct direction. This directly addresses whether genomics adds clinical value beyond existing predictors. Chapter 27 provides detailed treatment of clinical evaluation frameworks.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-baseline-selection",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-baseline-selection",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.6 Baseline Selection",
    "text": "12.6 Baseline Selection\nBaseline comparisons determine the meaning of reported performance. A model achieving 0.85 auROC might represent a major advance if the best prior method achieved 0.70, or a trivial improvement if simple heuristics achieve 0.83. Choosing appropriate baselines is as important as choosing appropriate metrics.\n\n12.6.1 Strong Baselines, Not Straw Men\nThe temptation to compare against weak baselines inflates apparent contributions. A deep learning model compared against a naive prior or a deliberately crippled baseline will appear impressive regardless of whether it offers genuine value. Strong baselines force honest assessment of improvement.\nFor sequence-based predictions, position weight matrices (PWMs) and k-mer logistic regression provide classical baselines that capture sequence composition without deep learning. If a convolutional model barely outperforms logistic regression on k-mer counts, the convolutional architecture may not be contributing as much as claimed.\nFor variant effect prediction, simple features like allele frequency, conservation scores, and amino acid properties provide baselines that any sophisticated model should substantially exceed. CADD (Section 4.3) serves as a well-calibrated baseline that combines many hand-crafted features; outperforming CADD demonstrates that learning provides value beyond feature engineering.\nFor foundation models, comparisons should include both randomly initialized models of similar architecture (to isolate the value of pretraining) and simpler pretrained models (to isolate the value of scale or architectural innovations). Claiming that pretraining helps requires demonstrating improvement over training from scratch on the same downstream data.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nFor each scenario, identify the appropriate baseline:\n\nA new DNA language model claims to predict TF binding sites better than previous approaches. What baselines should it beat?\nA variant pathogenicity predictor claims state-of-the-art performance. What would a “straw man” comparison look like, and what would a rigorous comparison include?\nA foundation model claims that pretraining improves downstream performance. What comparison demonstrates the value of pretraining specifically?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nIt should beat: PWM/k-mer baselines, the best current deep learning model (like Enformer or DNABERT-2), and a randomly initialized model of the same architecture to show pretraining value. (2) A straw man would compare only to outdated methods or use different data; rigorous comparison includes current best tools (CADD, REVEL, AlphaMissense), simple baselines (conservation scores, allele frequency), and ablations testing each component’s contribution. (3) Compare against a randomly initialized model with identical architecture trained from scratch on the same downstream task data - this isolates whether gains come from pretraining rather than just model size or architecture.\n\n\n\n\n\n\n\n\n12.6.2 Historical Baselines and Progress Tracking\nComparing to methods from five years ago may demonstrate progress but overstates the contribution of any single method. Comparisons should include the best currently available alternatives, not just historically important ones. When prior work is not directly comparable (different data, different splits, different metrics), reimplementing baselines on common benchmarks provides fairer comparison.\nField-wide progress tracking benefits from persistent benchmarks with frozen test sets. Once test set results for a benchmark are published, that benchmark becomes less useful for future model development because the test set is no longer truly held out. Periodic benchmark refresh with new held-out data helps maintain evaluation integrity.\n\n\n12.6.3 Non-Deep-Learning Baselines\nDeep learning models should be compared against strong non-deep alternatives. Gradient-boosted trees, random forests, and regularized linear models often achieve competitive performance with far less computation. If a 100-million-parameter transformer barely outperforms XGBoost on tabular features, the complexity may not be justified.\nThis comparison is especially important for clinical deployment, where simpler models may be preferred for interpretability, computational efficiency, or regulatory approval. Demonstrating that deep learning provides substantial gains over strong non-deep baselines strengthens the case for adoption.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-ablation-studies",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-ablation-studies",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.7 Ablation Studies",
    "text": "12.7 Ablation Studies\nAblation studies systematically remove or modify model components to understand their contributions. Where baselines compare across methods, ablations investigate within a method, revealing which design choices actually matter.\n\n12.7.1 Component Isolation\nStandard ablations remove individual components: attention layers, skip connections, normalization schemes, specific input features. If removing attention heads causes minimal performance degradation, the model may not be exploiting long-range dependencies as claimed. If removing a particular input modality has no effect, that modality may not be contributing useful information.\nAblations should be designed to test specific hypotheses. If the claim is that a foundation model learns biologically meaningful representations, ablating pretraining (comparing to random initialization) directly tests this claim. If the claim is that cross-attention between modalities enables integration, ablating cross-attention while retaining separate encoders tests whether integration provides value.\n\n\n12.7.2 Hyperparameter Sensitivity\nReporting performance across hyperparameter ranges reveals robustness. A model that achieves state-of-the-art performance only at a narrow learning rate range with specific regularization may be overfit to the evaluation setup. Consistent performance across reasonable hyperparameter variations provides stronger evidence of genuine capability.\n\n\n12.7.3 Architecture Search Confounds\nWhen model development involves extensive architecture search, reported performance conflates the value of the final architecture with the value of search on the validation set. The validation set is no longer truly held out; it has been used to select among architectures. Final evaluation on a completely untouched test set, with the architecture fixed before test set examination, provides cleaner assessment.\n\n\n12.7.4 Reporting Standards\nAblation tables should clearly indicate what was changed in each condition, the number of random seeds or runs, and measures of variance. Single-run ablations can produce misleading results due to training stochasticity. Reporting means and standard deviations across multiple runs reveals whether observed differences exceed random variation.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-statistical-rigor",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-statistical-rigor",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.8 Statistical Rigor",
    "text": "12.8 Statistical Rigor\nPerformance differences between models may reflect genuine capability differences or random variation in training and evaluation. Statistical analysis distinguishes signal from noise.\n\n\n\n\n\n\nTwo Cultures: Inference vs. Prediction\n\n\n\nReaders from biostatistics may find this chapter’s evaluation paradigm unfamiliar. The distinction traces to what Leo Breiman called the “two cultures” of statistical modeling (Breiman 2001).\n\n\n\nTable 12.2: The two cultures of statistical modeling applied to genomics.\n\n\n\n\n\n\n\n\n\n\nAspect\nInferential (Classical Statistics)\nPredictive (Machine Learning)\n\n\n\n\nPrimary goal\nEstimate parameters, test hypotheses\nGeneralize to new data\n\n\nKey question\n“Is this effect significant?”\n“How well does this predict?”\n\n\nValidation\np-values, confidence intervals\nHeld-out test sets, cross-validation\n\n\nModel preference\nInterpretable (linear, logistic)\nWhatever predicts best\n\n\nTypical application\nGWAS effect sizes, clinical trials\nFoundation models, risk prediction\n\n\n\n\n\n\nInferential modeling asks whether an observed relationship is “real” (unlikely under the null hypothesis). A GWAS reports effect sizes with p-values because the goal is understanding which variants associate with disease and estimating their effects. Model complexity is constrained to enable interpretation: a coefficient in logistic regression has meaning; a weight in a neural network does not.\nPredictive modeling asks whether a model generalizes beyond training data. A variant effect predictor reports auROC on held-out variants because the goal is accurate classification, not mechanistic understanding. Model complexity is constrained only by overfitting: if a billion-parameter transformer predicts better, use it.\nFoundation models are fundamentally predictive. They optimize for generalization, not inference. This explains why this chapter emphasizes held-out test performance rather than hypothesis testing—and why readers trained in classical biostatistics may need to shift their evaluation intuitions.\nThe cultures are not opposed. Mendelian randomization (Section 25.2.1) uses predictive models (genetic instruments) to answer inferential questions (causal effects). Polygenic risk scores use inferential results (GWAS effect sizes) for predictive applications. But understanding which culture a method belongs to clarifies what its evaluation metrics actually measure.\n\n\n\n12.8.1 Significance Testing\nFor classification metrics, significance tests ask whether observed differences exceed what would be expected from sampling variation. Bootstrap confidence intervals resample the test set with replacement, recompute metrics on each resample, and report the distribution of metric values. Non-overlapping 95% confidence intervals suggest significant differences. Permutation tests shuffle predictions between models and measure how often shuffled differences exceed observed differences.\nFor comparing multiple models across multiple benchmarks, correction for multiple testing becomes important. Without correction, 20 pairwise comparisons will produce an expected one false positive at the 0.05 level even when all models perform equally. The Bonferroni correction divides the significance threshold by the number of tests; the Benjamini-Hochberg procedure controls false discovery rate with more power than Bonferroni. [Citation Needed]\n\n\n12.8.2 Effect Sizes\nStatistical significance does not imply practical significance. A difference of 0.001 auROC might be statistically significant with millions of test examples while being practically meaningless. Effect sizes quantify the magnitude of differences independent of sample size. Cohen’s d for continuous outcomes and odds ratios for binary outcomes provide standardized measures of effect magnitude.\nReporting both significance tests and effect sizes provides complete information. A result that is statistically significant with a tiny effect size warrants different interpretation than one that is significant with a large effect size.\n\n\n12.8.3 Confidence Intervals on Metrics\nPoint estimates of auROC or correlation should be accompanied by confidence intervals. DeLong’s method provides analytical confidence intervals for auROC (delong_comparing_1988?); bootstrap methods provide distribution-free intervals for any metric. Reporting “auROC = \\(0.85\\) (95% CI: \\(0.82\\)–\\(0.88\\))” is more informative than “auROC = \\(0.85\\)” alone.\n\n\n12.8.4 Variance Across Random Seeds\nDeep learning models are sensitive to initialization and optimization stochasticity. Training the same architecture with different random seeds can produce substantially different results. Best practice trains multiple runs and reports means and standard deviations. If the standard deviation across runs exceeds the difference between methods, claimed improvements may not be reproducible.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-evaluating-fm",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-evaluating-fm",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.9 Evaluating Foundation Models",
    "text": "12.9 Evaluating Foundation Models\nGenomic foundation models (Chapter 13) admit multiple evaluation paradigms, each testing different aspects of learned representations.\n\n\n\n\n\n\n\n\nZero-shot evaluation\n\n\n\n\n\n\n\nLinear probing\n\n\n\n\n\n\n\nFine-tuning\n\n\n\n\n\n\nFigure 12.5: Foundation model evaluation paradigms test different capabilities. (A) Zero-shot: frozen model predictions test whether pretraining captured task-relevant patterns; requires no task data but limited to aligned tasks. (B) Linear probing: frozen embeddings with learned linear classifier test whether task features are linearly accessible in representations; requires minimal data and isolates representation quality. (C) Fine-tuning: full gradient updates test total potential but conflate representation and adaptation; achieves best performance but requires substantial data. The value of pretraining is best measured by the gap between few-shot and from-scratch performance—large gaps indicate that pretraining provides transferable features beyond what task data alone would support.\n\n\n\n\n12.9.1 Zero-Shot Evaluation\nIn zero-shot evaluation, the pretrained model is applied without any task-specific training. For masked language models, this typically means using token probabilities to score variants or classify sequences. A variant that disrupts a position the model predicts with high confidence may indicate functional importance.\nZero-shot performance tests whether pretraining captures task-relevant structure without explicit supervision. Strong zero-shot performance suggests the pretraining objective aligned with the evaluation task; weak zero-shot performance suggests misalignment. Comparing zero-shot performance to simple baselines (e.g., conservation scores for variant effects) calibrates whether the foundation model provides value beyond what simpler approaches achieve.\n\n\n12.9.2 Linear Probing\nLinear probing freezes the foundation model and trains only a linear classifier on extracted embeddings. This isolates representation quality from fine-tuning capacity. If a linear probe on foundation model embeddings substantially outperforms a linear probe on random embeddings, the foundation model has learned useful features.\nLayer-wise probing reveals where information is encoded. Early layers may capture local sequence features while later layers capture more abstract patterns. If the information needed for a task is extractable from early layers, the model may not require the full depth of the architecture for that application.\n\n\n12.9.3 Fine-Tuning Evaluation\nFull fine-tuning adapts all model parameters to the downstream task. This provides the best performance but conflates representation quality with adaptation capacity. A foundation model might achieve high fine-tuned performance through the capacity of its architecture rather than the quality of its pretrained representations.\nComparing fine-tuned foundation models to equivalently architected models trained from scratch isolates the value of pretraining. If both approaches converge to similar performance given sufficient downstream data, pretraining provides label efficiency (less data needed to reach a given performance level) rather than improved final performance. Data efficiency curves, plotting performance against downstream training set size, reveal this trade-off.\n\n\n12.9.4 Transfer Across Tasks\nFoundation models justify their “foundation” designation by transferring to diverse downstream tasks. Evaluating on a single task, however well-designed, cannot assess breadth of transfer. Multi-task evaluation across regulatory prediction, variant effects, protein properties, and other applications reveals whether foundation models provide general-purpose representations or excel only on tasks similar to their pretraining objective.\nTransfer across species, tissues, and experimental modalities provides additional evidence of generalization. A DNA language model that transfers from human to mouse, or from blood cells to neurons, demonstrates that its representations capture biological principles rather than species-specific or tissue-specific patterns.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-calibration",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-calibration",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.10 Calibration Essentials",
    "text": "12.10 Calibration Essentials\nStrong discrimination does not guarantee useful probability estimates. A model achieving 0.95 auROC might assign probability 0.99 to all positive examples and 0.98 to all negatives, ranking perfectly while providing meaningless confidence values. Clinical decision-making requires both: accurate ranking to identify high-risk variants and accurate probabilities to inform the weight of computational evidence. Calibration assesses whether predicted probabilities match observed frequencies, a property essential for rational integration of model outputs into diagnostic workflows.\n\n12.10.1 Assessing Calibration\nThe most intuitive assessment comes from reliability diagrams, which plot predicted probabilities against observed frequencies. The construction bins predictions into intervals (commonly ten bins spanning 0 to 0.1, 0.1 to 0.2, and so forth), computes the mean predicted probability within each bin, computes the fraction of positive examples within each bin, and plots these quantities against each other. Perfect calibration produces points along the diagonal; systematic deviations reveal overconfidence (points below the diagonal) or underconfidence (points above).\nA single summary statistic, the expected calibration error (ECE), captures miscalibration as the weighted average absolute difference between predicted and observed probabilities across bins. Lower ECE indicates better calibration. The metric depends on binning choices; equal-width bins may place most examples in a few bins for models with concentrated predictions, while equal-mass bins ensure each bin contains the same number of examples but may span wide probability ranges. ECE should be reported alongside reliability diagrams for interpretability.\nAggregate calibration metrics can mask important heterogeneity. A model might achieve low aggregate ECE while being systematically overconfident for rare variant classes and underconfident for common ones, with opposite errors canceling in the aggregate statistic. Stratified calibration analysis across ancestry groups, variant classes, and gene categories identifies these disparities. For genomic models intended for diverse populations, subgroup-stratified calibration is not optional; aggregate metrics can mask clinically significant differential performance.\n\n\n12.10.2 Recalibration Methods\nPost-hoc recalibration adjusts predicted probabilities without retraining the underlying model. Methods range from single-parameter approaches like temperature scaling (Guo et al. 2017), which divides logits by a learned constant to compress overconfident distributions, to non-parametric transformations like isotonic regression, which fits a monotonic function mapping raw scores to calibrated probabilities. Platt scaling (platt_probabilistic_1999?) fits a logistic regression from model outputs to true labels, providing intermediate flexibility. Each method makes different assumptions about the structure of miscalibration and requires different amounts of calibration data. The mathematical details, theoretical foundations, and guidance for method selection are developed in Section 23.3.\nAll recalibration methods require held-out calibration data distinct from both training and test sets. Calibrating on test data and then evaluating calibration on the same test data produces overoptimistic estimates. For foundation models, the calibration set should be drawn from the deployment distribution; calibrating on ClinVar expert-reviewed variants may not transfer to variants in less-studied genes or underrepresented populations.\n\n\n12.10.3 Calibration in Model Comparison\nWhen comparing models, calibration metrics complement discrimination metrics. Two models with identical auROC may have dramatically different calibration, and the better-calibrated model will produce more reliable clinical evidence even though its ranking performance is equivalent. Reporting both discrimination (auROC, auPRC) and calibration (ECE, reliability diagrams) provides a complete picture of model performance.\nCalibration can often be improved post-hoc without sacrificing discrimination. Temperature scaling preserves ranking while adjusting probability magnitudes, meaning a model can be recalibrated to improve ECE without changing auROC. This observation suggests that raw discrimination metrics may be more fundamental indicators of model quality, with calibration treated as an adjustable property. The comprehensive treatment of calibration theory is developed in Section 23.2, including its relationship to uncertainty quantification (Section 23.1) and methods for quantifying different sources of prediction uncertainty. Clinical deployment requires additional calibration considerations examined in Section 27.6.2.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-putting-together",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-putting-together",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.11 Putting It All Together",
    "text": "12.11 Putting It All Together\nWhen designing or evaluating a genomic model assessment, working through a systematic checklist helps identify gaps and potential problems. The following questions organize this review, though the specific considerations will vary by application.\n\n\n\n\n\n\nPractical Guidance: Evaluation Design Checklist\n\n\n\nUse this checklist when designing an evaluation or reviewing published work:\nData Splitting\n\nHave individuals, genomic regions, gene families, and ancestries been appropriately separated?\nHas homology-aware clustering been applied with appropriate identity thresholds?\nIs there any plausible pathway for leakage or circularity?\n\nBaselines\n\nAre comparisons made against the best available alternatives?\nDo non-deep-learning baselines establish floors that justify complexity?\nDoes improvement over baselines warrant additional computational costs?\n\nMetrics\n\nAre multiple metrics reported (discrimination, calibration, ranking)?\nAre confidence intervals provided?\nAre subgroup-stratified metrics reported for clinically relevant populations?\n\nAblations\n\nHave systematic ablations demonstrated which design choices drive performance?\nIs performance robust across hyperparameter ranges and random seeds?\n\nStatistical Rigor\n\nAre significance tests applied with appropriate correction?\nAre effect sizes reported alongside p-values?\n\nFor Foundation Models\n\nIs performance reported across zero-shot, probing, and fine-tuning regimes?\nDo data efficiency curves reveal where pretraining provides value?\nHas transfer been tested across diverse tasks?\n\n\n\nFirst, consider the level of decision the model is intended to support. A model intended for molecular prediction faces different evaluation requirements than one designed for variant prioritization, patient risk stratification, or clinical action. Metrics should align with the actual decision context: enrichment metrics suit variant ranking, while net benefit matters for clinical decisions.\nSecond, examine whether data splits adequately prevent leakage. Are individuals, genomic regions, gene families, and ancestries appropriately separated? Has homology-aware clustering been applied with appropriate identity thresholds? Is there any plausible pathway for leakage or circularity through shared labels, features, or distributional overlap?\nThird, assess the baseline comparisons. Are comparisons made against the best available alternatives, not just historical or deliberately weak baselines? Do non-deep-learning baselines establish floors that justify architectural complexity? Does the improvement over baselines warrant the additional computational and interpretability costs?\nFourth, evaluate metric selection. Are multiple metrics reported to capture discrimination, calibration, and ranking quality? Are metrics computed with confidence intervals that convey uncertainty? Are subgroup-stratified metrics reported to assess whether performance varies across clinically relevant populations?\nFifth, examine whether ablation studies isolate component contributions. Have systematic ablations demonstrated which design choices drive performance? Is performance robust across hyperparameter ranges and random seeds, or does it depend on specific configurations?\nSixth, consider statistical rigor. Are significance tests applied with appropriate correction for multiple comparisons? Are effect sizes reported alongside p-values to distinguish statistical from practical significance? Are confidence intervals provided for key metrics?\nFor foundation models specifically, additional considerations apply. Is performance reported across zero-shot, probing, and fine-tuning regimes? Do data efficiency curves reveal where pretraining provides value? Has transfer been tested across diverse tasks to justify the “foundation” designation?\nFinally, assess robustness to deployment conditions. How does performance vary across cohorts, platforms, and ancestries? How does the model behave under distribution shift, missing data, or label noise? Would the evaluation translate to realistic deployment scenarios?\nThis checklist is not exhaustive but covers the most common evaluation pitfalls. Working through it systematically at the design stage can prevent problems that are difficult to fix retrospectively. Reviewers and readers can use the same checklist to critically assess published work.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch11-benchmarks.html#sec-ch11-question-behind-metric",
    "href": "part_2/p2-ch11-benchmarks.html#sec-ch11-question-behind-metric",
    "title": "11  Benchmarks and Evaluation",
    "section": "12.12 The Question Behind the Metric",
    "text": "12.12 The Question Behind the Metric\nThe question is never simply “what is the auROC?” but rather “what has been demonstrated, and how much should we trust it?” A reported metric summarizes one aspect of model behavior on one dataset under one evaluation protocol. Whether that metric predicts performance in deployment depends on details that standard reporting obscures: how data were split, whether leakage occurred, which subgroups were evaluated, what baselines were compared, and whether statistical conclusions account for multiple comparisons and estimation uncertainty.\nThe shortcuts that accelerate research in other machine learning domains produce misleading conclusions when applied to genomic data. Random train-test splits ignore homology that creates pseudo-replication. Single-metric comparisons miss failure modes in clinically relevant subgroups. Significance tests without effect sizes conflate statistical and practical importance. Benchmark evaluation without temporal awareness allows indirect leakage through shared community resources. Homology, population structure, batch effects, and label circularity create countless opportunities for self-deception, and genomic data exhibit all of these in abundance.\nRigorous evaluation requires sustained effort at every stage, from experimental design through statistical analysis. Confounding and leakage (Chapter 12) examines how population stratification, batch effects, and ascertainment bias produce results that evaporate under deployment, with specific attention to ancestry-stratified evaluation in ?sec-ch22-ancestry-confounding and batch effect detection in Section 22.7.1. Uncertainty quantification (Chapter 23) extends calibration assessment to epistemic versus aleatoric uncertainty (Section 23.1) and selective prediction (Section 23.7). Interpretability (Chapter 24) addresses whether models have learned genuine biology or exploited confounded patterns, with attribution methods in Section 24.1 providing specific diagnostic tools. For clinical applications specifically, risk prediction frameworks (Chapter 27) develop evaluation approaches tailored to decision-making, where net benefit and decision curves supplement discrimination metrics. Together, these perspectives provide the critical apparatus for engaging with genomic foundation model claims.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat is homology leakage, and why does it make random train-test splits inadequate for protein and DNA sequence benchmarks?\nExplain the four types of data leakage (label, feature, temporal, benchmark) and give a concrete genomic example of each.\nA model achieves 0.85 Spearman correlation on a DMS benchmark with random splits, but only 0.60 with contiguous region splits. What does this performance gap reveal?\nWhy does auROC alone fail to capture calibration quality, and when would a well-calibrated model matter more than high discrimination?\nHow does label circularity in ClinVar (where computational predictions influence annotations) compromise benchmark validity for new variant effect predictors?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nHomology leakage occurs when evolutionarily related sequences appear in both training and test sets, allowing models to succeed through memorization of family-specific patterns rather than learning general biological principles. Random splits fail because they ignore sequence similarity—a test protein at 80% identity to a training protein provides minimal generalization evidence, yet random splits routinely create such pairs. Homology-aware splitting (using tools like CD-HIT at 30-40% identity thresholds) ensures test sequences are evolutionarily distant from training sequences.\nFour leakage types:\n\nLabel leakage: Target labels derived from model features. Example: ClinVar pathogenicity classifications that incorporated SIFT/PolyPhen predictions, then used to evaluate new predictors using similar features—the model learns to replicate curation criteria rather than discover independent signal.\nFeature leakage: Input features encode unavailable future information. Example: Using population allele frequency as a feature for pathogenicity prediction captures selection pressure (rare = likely pathogenic) without learning variant biology; appropriate for clinical use but problematic for mechanistic understanding.\nTemporal leakage: Using future information to predict past events. Example: Training on 2023 ClinVar annotations to predict variants that were uncertain in 2020, when those 2023 annotations may have used model-like predictions made after 2020.\nBenchmark leakage: Test set construction influenced by similar methods. Example: A protein benchmark selecting well-annotated proteins via sequence similarity searches, then evaluating sequence-based models that exploit the same similarity used in benchmark construction.\n\nThe 0.85 → 0.60 performance drop reveals the model relies heavily on local sequence context rather than learning positional effects or long-range constraints. Random splits allow nearby positions to appear in both training and test sets; the model learns that “positions near training variants tend to have similar effects.” Contiguous splits remove this crutch by placing entire sequence regions in test, forcing genuine spatial generalization. The 0.25 correlation gap represents how much performance came from interpolating between nearby measured positions versus understanding the protein’s functional landscape.\nauROC measures ranking (whether positives score higher than negatives) but is invariant to probability calibration (whether a score of 0.8 actually means 80% probability). A model can achieve 0.95 auROC by assigning 0.99 to all pathogenic variants and 0.98 to all benign variants—perfect ranking, useless probabilities. Well-calibrated models matter most when:\n\nMaking threshold-based decisions (reporting variants above some probability cutoff)\nIntegrating model evidence with other information (Bayesian updating requires calibrated likelihoods)\nCommunicating uncertainty to clinicians (a stated “80% probability” should be reliable)\nComparing risks across different variant types or populations\n\nLabel circularity creates a feedback loop: if ClinVar curators used computational predictions (SIFT, PolyPhen, conservation scores) to classify variants, and those classifications become training labels for new predictors using similar features, the new model learns to replicate curation criteria rather than provide independent evidence. This inflates benchmark performance without improving biological insight—the model predicts what previous models predicted, not what biology determines. Detecting circularity requires: (1) comparing against baselines using only the potentially circular features, (2) checking whether performance exceeds what those features alone achieve, (3) temporal validation on prospectively classified variants, and (4) examining whether the model adds value beyond existing predictors in clinical adjudication.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nThis chapter covered the dual challenge of benchmarks (what to measure) and evaluation methodology (how to measure it properly).\nKey Takeaways on Benchmarks:\n\nProtein benchmarks (TAPE, FLIP, ProteinGym) are most mature, with standardized transfer learning evaluation and homology-aware splitting\nDNA/regulatory benchmarks (Genomic Benchmarks, BEND) are rapidly developing but face challenges with quantitative targets and long-range dependencies\nVariant effect benchmarks span molecular (DMS) to clinical (ClinVar) levels, with critical circularity concerns for database-derived labels\nTrait-level benchmarks (TraitGym, EmbedGEM) assess whether foundation models add value beyond classical PGS methods\n\nKey Takeaways on Methodology:\n\nRandom splits fail for genomic data because sequences share homology, individuals share ancestry, and samples share batch effects\nHomology-aware splitting (CD-HIT/MMseqs2 at appropriate thresholds) prevents the most common leakage pathway\nFour leakage types (label, feature, temporal, benchmark) require different detection strategies\nMetric selection must match deployment objectives: auPRC for imbalanced data, calibration for probability estimates, ranking metrics for prioritization\nStrong baselines and proper ablations distinguish genuine advances from benchmark-specific tuning\n\nLooking Ahead: The next chapter (Chapter 12) examines how confounding and leakage structures beyond homology create spurious performance claims, including population stratification, batch effects, and ascertainment bias.\nConnections:\n\nApply evaluation principles when assessing claims in later chapters on foundation models (Chapter 13 through Chapter 17)\nCalibration concepts developed here connect to uncertainty quantification (Chapter 23)\nClinical utility metrics introduced here are expanded for clinical risk prediction (Chapter 27)\n\n\n\n\n\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726.\n\n\nDallago, Christian, Jody Mou, Kadina E. Johnston, Bruce J. Wittmann, Nicholas Bhattacharya, Samuel Goldman, Ali Madani, and Kevin K. Yang. 2022. “FLIP: Benchmark Tasks in Fitness Landscape Inference for Proteins.” bioRxiv. https://doi.org/10.1101/2021.11.09.467890.\n\n\nGrešová, Katarína, Vlastimil Martinek, David Čechák, Petr Šimeček, and Panagiotis Alexiou. 2023. “Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification.” BMC Genomic Data 24 (1): 25. https://doi.org/10.1186/s12863-023-01123-8.\n\n\nGuo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. “On Calibration of Modern Neural Networks.” In Proceedings of the 34th International Conference on Machine Learning, 1321–30. PMLR. https://proceedings.mlr.press/v70/guo17a.html.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. “BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks.” arXiv. https://doi.org/10.48550/arXiv.2311.12570.\n\n\nMukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom Soare, Raghav Tandon, David Amar, et al. 2024. “EmbedGEM: A Framework to Evaluate the Utility of Embeddings for Genetic Discovery.” Bioinformatics Advances 4 (1). https://doi.org/10.1093/bioadv/vbae135.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. “ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design.” Advances in Neural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nRao, Roshan, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, and Yun S. Song. 2019. “Evaluating Protein Transfer Learning with TAPE.” arXiv. https://doi.org/10.48550/arXiv.1906.08230.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarks and Evaluation</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html",
    "href": "part_2/p2-ch12-confounding.html",
    "title": "12  Confounding and Data Leakage",
    "section": "",
    "text": "12.1 Confounding, Bias, and Leakage\nThe terminology of confounding, bias, and leakage describes distinct phenomena that often co-occur and reinforce each other. Precision in language helps clarify what has gone wrong when a model fails.\nA confounder is a variable that influences both the input features and the label. Ancestry provides a canonical example: it affects allele frequencies across the genome (the features) and disease risk through environmental, socioeconomic, and healthcare pathways (the labels). If ancestry is not explicitly modeled or controlled, a model trained to predict disease may learn to identify ancestry rather than disease biology. The prediction appears accurate because ancestry correlates with outcome, but the model has captured correlation rather than mechanism.\nBias refers to systematic deviation from the quantity we intend to estimate or predict. Bias can result from confounding, but also arises from measurement error, label definitions, sampling procedures, or deployment differences. A case-control study with 50% disease prevalence will train models that systematically over-predict risk when deployed in populations where true prevalence is 5%. The model may be perfectly calibrated for the training distribution yet dangerously miscalibrated for clinical use.\nData leakage occurs when information about the test set inadvertently influences model training or selection. Leakage pathways include overlapping individuals or variants between training and evaluation, shared family members across splits, duplicated samples under different identifiers, and indirect channels such as pretraining on resources that later serve as benchmarks. The circularity between computational predictors and ClinVar annotations discussed in Section 4.5 exemplifies this last category: CADD-like scores influence which variants receive pathogenic annotations, and those annotations then become training labels for the next generation of predictors.\nDistribution shift describes mismatch between training and deployment data distributions. Shift can be driven by changes in ancestry composition, sequencing technology, clinical coding practices, or temporal trends in care. A model that learns hospital-specific coding patterns will fail when deployed at a different institution, not because the biology differs but because the label generation process does.\nThe following table clarifies the distinctions between these related but distinct concepts:\nFor foundation models, these risks are magnified. Genomes encode ancestry, relatedness, and assay conditions in thousands of subtle features, even when those labels are never explicitly provided. Large transformers find shortcuts that smaller models would miss if those shortcuts improve the training objective. Complex training regimes involving pretraining on biobank-scale data, fine-tuning on curated labels, and evaluation on community benchmarks create many opportunities for direct and indirect leakage.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-terminology",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-terminology",
    "title": "12  Confounding and Data Leakage",
    "section": "",
    "text": "Predict Before You Look\n\n\n\nBefore examining the table below, test your understanding: For each scenario, identify whether it represents confounding, bias, data leakage, or distribution shift:\n\nA model trained on variants from 2018-2020 ClinVar is tested on variants added to ClinVar in 2023\nA polygenic score calibrated on 50% case prevalence is deployed in a population with 5% prevalence\nAncestry affects both allele frequencies and disease risk through healthcare access pathways\nThe same individual’s genome appears in both training and test sets under different identifiers\n\nCheck your predictions against the table definitions below.\n\n\n\n\n\n\nTable 12.1: Distinguishing confounding, bias, leakage, and distribution shift. Each phenomenon has different causes, manifestations, and solutions.\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExample\nDetection\nPrimary Solution\n\n\n\n\nConfounding\nVariable affects both features and labels\nAncestry affects genotype frequencies and disease risk\nConfounder-only baselines match model performance\nMatching, adjustment, or invariance learning\n\n\nBias\nSystematic deviation from target\nTraining at 50% prevalence, deploying at 5%\nCalibration analysis across settings\nDesign matching, recalibration\n\n\nData leakage\nTest information influences training\nSame variant in train and test sets\nPerformance collapse under strict splits\nRigorous deduplication, temporal splits\n\n\nDistribution shift\nTrain/deploy distributions differ\nModel trained on one hospital, deployed at another\nPerformance degradation on new cohorts\nDomain adaptation, multi-site training\n\n\n\n\n\n\n\n\n\n\n\n\nKey Insight: The Confounder-Mechanism Distinction\n\n\n\nThe critical question for any genomic model is: Does the association between features and labels flow through the biological mechanism I care about, or through a confounding pathway? A model predicting variant pathogenicity from sequence might learn that certain haplotype backgrounds correlate with pathogenic labels, but if that correlation exists because of ancestry-biased ascertainment rather than biological causation, the model has learned a shortcut that will fail when applied to differently ascertained populations.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-sources",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-sources",
    "title": "12  Confounding and Data Leakage",
    "section": "12.2 Sources of Confounding in Genomic Data",
    "text": "12.2 Sources of Confounding in Genomic Data\nConfounders in genomic modeling cluster into several categories, though the same underlying variable (such as recruitment site) may simultaneously induce ancestry differences, batch effects, and label bias. These categories are not mutually exclusive; batch effects in single-cell data (Section 19.6.1) and multi-omic integration (Section 22.7.1) represent domain-specific manifestations of the same underlying challenge.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading about specific confounding sources, consider: if you were designing a study to train a variant pathogenicity predictor, what variables might affect both the variants you observe (your features) and the pathogenicity labels you collect (your outcomes)? List three potential confounders and how they might create spurious associations.\n\n\n\n12.2.1 Population Structure and Relatedness\nAncestry creates perhaps the most pervasive confounders. Continental and sub-continental population structure affects both genomic features and many phenotypes of interest, creating classic confounding. The portability failures of polygenic scores across ancestry groups (Section 3.7) represent one clinically consequential manifestation of this confounding. Family relationships (siblings, parent-offspring pairs, cryptic relatedness detectable only through genotype similarity) and founder effects that create local haplotype structure compound these issues. Relatedness creates a more subtle problem than population stratification: when close relatives appear in both training and test sets, models can memorize shared haplotype segments rather than learning generalizable patterns, producing inflated performance estimates that collapse for unrelated individuals.\n\n\n12.2.2 Technical Batch Effects\nSequencing and analysis pipelines introduce their own systematic differences. Different instruments produce distinct error profiles. Library preparation protocols vary in GC bias, coverage uniformity, and adapter content. Capture kits determine which genomic regions receive adequate coverage. Alignment algorithms and variant callers make different decisions at ambiguous positions. When samples from a particular batch disproportionately represent a specific label class (cases sequenced at one center, controls at another), models learn to distinguish batches rather than biology.\n\n\n12.2.3 Institutional and Recruitment Confounding\nThe institutions where patients receive care introduce additional confounding layers. Hospital systems use distinct coding practices, diagnostic thresholds, and follow-up schedules. The phenotype quality issues that result are examined in Section 2.7, with implications for how models learn from systematically biased labels. Population-based biobanks differ from referral-center cohorts in disease severity, comorbidity patterns, and demographic composition. Individuals who receive genomic testing may be more severely affected, more affluent, or preferentially drawn from particular ancestry groups, introducing selection bias that distorts apparent variant-phenotype relationships.\nThese sources of confounding trace back to data collection and curation processes. Training data inherit the biases present in the databases from which they derive: ClinVar’s overrepresentation of European ancestry variants (Section 2.8.1), gnomAD’s population composition (Section 2.2.3), and the tissue coverage decisions of consortia like ENCODE and GTEx (Section 2.4.1). Understanding data provenance is prerequisite to anticipating which confounders a model may have learned.\n\n\n12.2.4 Label Generation Bias\nThe process of generating ground truth annotations itself creates biases. Clinical labels derived from billing codes or problem lists reflect documentation practices as much as underlying disease. Variant pathogenicity databases exhibit the systematic biases detailed in Section 2.8: ClinVar annotations over-represent European ancestry, well-studied genes, and variants submitted by high-volume clinical laboratories (Landrum et al. 2018). Expression, regulatory, or splicing labels derived from specific tissues or cell lines may not generalize to other biological contexts. The circularity problem identified in Section 4.5 persists into the foundation model era: when model predictions influence which variants receive expert review, and expert classifications become training labels, feedback loops amplify historical biases.\n\n\n12.2.5 Temporal Drift\nClinical practice, diagnostic criteria, and coding conventions evolve over time. Sequencing technologies and quality control pipelines also change. A model trained on 2015 data may fail on 2024 data not because biology changed but because documentation practices, coding standards, and available treatments all evolved. This temporal drift affects both the features models learn and the labels they predict.\n\n\n12.2.6 Resource Overlap and Indirect Leakage\nEven the resources used for training and evaluation create leakage pathways. When databases like gnomAD or UK Biobank appear in both model training and evaluation, indirect information flows compromise apparent generalization. A foundation model pretrained on gnomAD allele frequencies, then evaluated on a benchmark that uses gnomAD for population filtering, faces indirect leakage even if specific variants do not overlap. Community benchmarks that reuse widely available variant sets across multiple publications create additional leakage pathways that accumulate over time as the field iterates.\nThe following table summarizes the major confounding sources, their mechanisms, and detection approaches:\n\n\n\nTable 12.2: Major sources of confounding in genomic modeling. Each source creates distinct pathways from features to labels that bypass the biological mechanisms of interest.\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nAffects Features Via\nAffects Labels Via\nDetection Signal\nMitigation Approach\n\n\n\n\nAncestry\nAllele frequencies, haplotypes, LD patterns\nHealthcare access, environmental exposure\nPerformance stratified by ancestry; PCA-only baseline\nMatching, PCs as covariates, invariance training\n\n\nRelatedness\nShared haplotype segments\nShared environmental factors, ascertainment\nKinship matrix analysis; family-aware split sensitivity\nFamily-aware splitting\n\n\nBatch effects\nCoverage, error profiles, variant calling\nCase/control imbalance across batches\nBatch predicts phenotype; embedding clusters by batch\nBatch covariates, harmonization, domain adaptation\n\n\nInstitution\nSequencing protocols, capture kits\nCoding practices, diagnostic criteria\nPerformance varies by site\nMulti-site training, cohort holdouts\n\n\nLabel generation\nFeatures used in curation decisions\nCircular dependency with prior predictions\nAblating predictive features degrades performance\nTemporal splits, independent validation\n\n\nTemporal drift\nTechnology evolution\nPractice guideline changes\nPerformance degrades on newer data\nTime-based splits, continuous monitoring",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-population-shortcut",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-population-shortcut",
    "title": "12  Confounding and Data Leakage",
    "section": "12.3 Population Structure as a Shortcut",
    "text": "12.3 Population Structure as a Shortcut\nPopulation structure represents one of the most pervasive confounders in genomic modeling. The core issue is that ancestry simultaneously affects genomic features and many phenotypes through pathways that have nothing to do with direct genetic causation.\nHuman genetic variation is structured by ancestry: allele frequencies, haplotype blocks, and linkage disequilibrium patterns differ across populations in ways that reflect demographic history. Principal components computed from genome-wide genotypes provide a low-dimensional summary of this structure and have become standard in genome-wide association studies (GWAS) to correct for stratification (Patterson, Price, and Reich 2006; Price et al. 2006). Yet ancestry is not merely a statistical nuisance. It is intertwined with geography, environment, socioeconomic status, and access to healthcare, factors that directly impact disease risk, likelihood of receiving genetic testing, and the quality of phenotyping when testing occurs.\nThe statistical genetics community developed these corrections precisely because early genome-wide association studies produced spurious signals driven by ancestry differences between cases and controls rather than causal variant effects (see Section 3.1.4 for detailed treatment of population stratification in association testing). Foundation models face the same fundamental problem in a different guise: ancestry structure that confounded linear regression in GWAS now confounds neural network predictions, and the solutions require similar conceptual foundations even when the technical implementations differ.\n\n\n\n\n\n\n\n\nPopulation structure in genomic data\n\n\n\n\n\n\n\nAncestry encoded in local sequence composition\n\n\n\n\n\n\n\n\n\nAncestry creates shortcut pathways\n\n\n\n\n\n\n\nShortcuts fail when ancestry-label relationships change\n\n\n\n\n\n\nFigure 12.2: Population structure creates exploitable shortcuts in genomic prediction. (A) PCA reveals clear ancestry clustering in genetic data. (B) Even local k-mer frequencies differ by ancestry, meaning foundation models have access to ancestry signal from raw sequences. (C) Ancestry creates dual pathways to both features (through allele frequencies) and labels (through healthcare access and annotation practices), enabling models to learn ancestry as a proxy for the target outcome. (D) The consequence: polygenic scores show 40-75% performance reduction when applied to non-European populations because ancestry-based shortcuts do not generalize. Higher model capacity does not solve confounding—it makes it worse by enabling detection of ever more subtle ancestry-linked features.\n\n\n\nConsider a rare disease clinic serving primarily individuals of European ancestry. This clinic contributes most pathogenic variant submissions to ClinVar, while variants observed predominantly in other ancestries remain classified as variants of uncertain significance (Landrum et al. 2018).. A model trained on ClinVar may learn that European-enriched variants tend to have pathogenic labels and non-European-enriched variants tend to have uncertain or benign labels, not because of any biological difference in pathogenicity but because of differential clinical characterization. The model appears to predict pathogenicity while actually predicting ancestry-correlated ascertainment.\nFoundation models trained on nucleotide sequences see ancestry information directly: the distribution of k-mers and haplotypes differs by population. When such models are fine-tuned to predict disease risk or variant effects, they may leverage ancestry as a shortcut. Increasing model capacity does not solve this problem; it often makes it worse by enabling detection of increasingly subtle ancestry-linked features. The polygenic score portability literature provides stark evidence: risk scores derived from European ancestry cohorts show 40-75% reductions in prediction accuracy when applied to African ancestry individuals (Duncan et al. 2019). Similar patterns emerge for variant effect predictors and regulatory models, though they are often less thoroughly documented due to limited cross-ancestry evaluation.\n\n\n\n\n\n\nKey Insight: Capacity Amplifies Confounding\n\n\n\nA common misconception is that larger, more powerful models will “see through” confounding to the underlying biology. In reality, the opposite often occurs. A linear model might capture only the strongest ancestry-outcome correlations; a transformer with billions of parameters will find every ancestry-linked feature that improves the training objective, no matter how subtle. Model expressiveness is not a defense against confounding; it is an amplifier.\n\n\nThis mismatch between the populations used for model development and the populations that would benefit from genomic medicine creates a fundamental tension between current practice and equitable healthcare. Models that work primarily for European ancestry individuals perpetuate existing health disparities, regardless of their benchmark performance. The fairness implications are examined further in Section 12.10.\n\n\n\n\n\n\nKnowledge Check: Recall Key Concepts\n\n\n\nBefore moving to technical artifacts, test your retention from the earlier sections:\n\nWhat is the difference between a confounder and data leakage? Give a concrete example of each.\nWhy does ancestry act as a confounding variable in genomic prediction?\nWhat diagnostic would reveal that your model’s 0.85 auROC primarily reflects ancestry rather than biological mechanism?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nA confounder is a variable that affects both features and labels (e.g., ancestry affects allele frequencies and disease risk through healthcare access), while data leakage occurs when test information influences training (e.g., the same variant appearing in both train and test sets). Both inflate performance, but leakage involves information that shouldn’t exist at prediction time, while confounding involves real but non-causal associations. (2) Ancestry affects genomic features through population-specific allele frequencies and haplotype structure, while simultaneously affecting disease labels through environmental factors, healthcare access, socioeconomic status, and clinical ascertainment practices—creating a spurious association pathway the model can exploit. (3) Train a confounder-only baseline using just ancestry principal components with no genomic features. If this baseline achieves performance close to your full model (e.g., 0.80 vs 0.85 auROC), ancestry confounding drives most of the signal.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-technical-artifacts",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-technical-artifacts",
    "title": "12  Confounding and Data Leakage",
    "section": "12.4 Technical Artifacts as Biological Signal",
    "text": "12.4 Technical Artifacts as Biological Signal\nTechnical pipelines are complex, and each step from sample collection through final variant calls can introduce systematic differences that models may learn.\nSequencing centers differ in instruments, reagents, and quality control thresholds. Library preparation protocols produce distinct coverage profiles and GC bias patterns. Capture kits determine which genomic regions are well-covered and which have systematic dropout. Read length affects the ability to span repetitive regions and call structural variants. Alignment and variant calling algorithms make different decisions at ambiguous genomic positions.\n\n\n\n\n\n\n\n\nEmbeddings cluster by batch rather than phenotype\n\n\n\n\n\n\n\nCoverage patterns vary systematically by center\n\n\n\n\n\n\n\nBatch-phenotype confounding enables trivial prediction\n\n\n\n\n\n\nFigure 12.3: Technical batch effects masquerade as biological signal. (A) Model embeddings cluster by sequencing center rather than by case/control status, revealing that learned representations capture technical rather than biological variation. (B) Coverage patterns at the same genomic region differ systematically by center, providing features models can exploit. (C) Batch-phenotype confounding occurs when cases and controls are imbalanced across centers—predicting batch becomes equivalent to predicting disease. Together, these panels illustrate why models can achieve high apparent performance while learning nothing about biology. Detection requires visualizing embeddings, examining coverage patterns, and testing whether batch identity predicts the outcome.\n\n\n\nWhen samples from a particular batch or platform are disproportionately drawn from a specific phenotype class, models learn to distinguish batches. Why does batch-phenotype correlation arise in real studies? Practical constraints drive the pattern: case samples are often collected at specialized disease centers with particular sequencing infrastructure, while controls come from population biobanks using different platforms. Temporal factors compound this—if cases were sequenced earlier when certain technologies dominated, and controls were added later with newer platforms, technology-phenotype correlation becomes embedded in the data. Studies rarely randomize case-control status across batches because retrospective collection is cheaper than prospective design, creating systematic confounding that standard quality control cannot detect.\nIn high-dimensional feature spaces, even subtle batch-specific artifacts (coverage dips at particular loci, variant density patterns reflecting caller behavior, residual adapter sequences) can become predictive. Foundation models that process raw reads, coverage tracks, or variant streams are particularly vulnerable because batch signatures may be encoded in features that preprocessing would typically remove.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA model achieves 0.88 auROC predicting disease status from whole-genome sequences. You discover that case samples were sequenced at Center A using Illumina NovaSeq, while control samples were sequenced at Center B using HiSeq. What would you predict about performance on a new cohort where cases and controls are equally distributed across both centers? What diagnostic would you run to test for batch confounding?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nPerformance would likely collapse to near-chance levels (close to 0.50 auROC) because the model learned to distinguish sequencing centers rather than disease biology. When cases and controls are equally distributed across both centers, the batch-disease correlation disappears and the learned shortcut becomes useless. Diagnostics to run: (1) Train a classifier to predict sequencing center from the model’s learned embeddings - high accuracy confirms batch encoding. (2) Visualize embeddings colored by center and by disease status - clustering by center rather than disease reveals the problem. (3) Evaluate performance stratified by center - if within-center performance is poor, the model relies on between-center differences.\n\n\n\n\n\nCommon patterns suggesting batch confounding include embedding spaces where samples cluster by sequencing center rather than phenotype, strong predictive performance that collapses when evaluated on data from a new platform, and models that can accurately predict batch identity (sequencing center, capture kit, processing date) from inputs that should be batch-independent. When a model designed to predict disease can also predict which laboratory processed the sample, something has gone wrong.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-label-circularity",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-label-circularity",
    "title": "12  Confounding and Data Leakage",
    "section": "12.5 Label Bias and Circularity",
    "text": "12.5 Label Bias and Circularity\nLabels in genomic applications rarely represent ground truth in any absolute sense. They represent the outputs of complex processes involving clinical documentation, expert review, computational prediction, and database curation. These processes introduce biases that models absorb and may amplify.\nClinical phenotypes derived from electronic health records inherit the limitations of medical documentation. Billing codes capture what was reimbursable, not necessarily what was present. Problem lists reflect what clinicians chose to document, which varies by specialty, institution, and individual practice patterns. Diagnostic criteria change over time, creating apparent temporal trends in disease prevalence that reflect evolving definitions rather than changing biology.\n\n\n\n\n\n\nLabel circularity inflates apparent validation\n\n\n\n\nFigure 12.4: Label circularity inflates apparent validation. Clinical laboratories submit variants to ClinVar using computational predictions (CADD, REVEL) as supporting evidence. ClinVar aggregates these submissions, creating labels that reflect historical computational predictions. New models trained on ClinVar learn to replicate these patterns, achieving high apparent accuracy through agreement with previous predictors rather than independent biological insight. When these new models are deployed, they influence the next generation of submissions, perpetuating the cycle. Breaking circularity requires prospective validation on newly discovered variants, temporal splits that train on historical data and test on recent annotations, or independent functional assay validation that bypasses computational predictions entirely.\n\n\n\nVariant pathogenicity labels illustrate the problem of circularity. ClinVar aggregates submissions from clinical laboratories, research groups, and expert panels (Landrum et al. 2018). The evidence underlying these submissions often includes computational predictions: a laboratory may cite CADD, REVEL, or other predictors as supporting evidence for a pathogenic classification. When the next generation of predictors trains on ClinVar, it learns to replicate the computational predictions that contributed to those labels. Performance on ClinVar-derived benchmarks thus reflects, in part, agreement with previous predictors rather than independent biological insight.\nWhy does circularity inflate validation metrics specifically? The mechanism is statistical: the new model’s task becomes predicting what previous models predicted, not predicting true pathogenicity. If CADD influenced 30% of pathogenic labels, and the new model learns to approximate CADD, it automatically achieves high agreement on those labels—regardless of whether the underlying biology was correctly captured. The inflation is proportional to the previous model’s influence on labeling: more circularity means more inflated benchmarks. Critically, this inflation is invisible within the circular ecosystem; only prospective validation on genuinely novel variants or independent functional assays reveals the gap between apparent and true performance.\nThis circularity extends across the ecosystem of genomic resources. gnomAD allele frequencies inform variant filtering in clinical pipelines. UK Biobank genotype-phenotype associations shape which variants receive functional follow-up. Structural annotations from ENCODE and Roadmap Epigenomics influence which regulatory regions are considered biologically important. Foundation models pretrained on these resources, then evaluated against benchmarks derived from the same resources, may achieve impressive scores while learning to reproduce the assumptions and biases of existing annotations rather than discovering new biology.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-data-splitting",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-data-splitting",
    "title": "12  Confounding and Data Leakage",
    "section": "12.6 Data Splitting",
    "text": "12.6 Data Splitting\nData splitting is among the primary tools for assessing generalization, yet naive splits can silently permit leakage that inflates apparent performance.\n\n\n\n\n\n\nMathematical Foundations\n\n\n\nThe following section introduces formal concepts about data splitting. The core intuition is simple: different splitting strategies test different types of generalization. Random splits test interpolation; structured splits test extrapolation to genuinely new contexts.\n\n\n\n12.6.1 Random Individual-Level Splits\nRandom individual-level splits assign samples randomly to training, validation, and test sets. This approach fails when samples are not independent: family members may appear on both sides of a split, allowing models to memorize shared haplotypes. Rare variant analysis is particularly vulnerable because disease-causing variants may be private to specific families, and memorizing which families have which variants is far easier than learning generalizable sequence-function relationships.\n\n\n12.6.2 Family-Aware Splits\nFamily-aware splits address relatedness by ensuring that all members of a family appear in the same split. This prevents direct memorization of family-specific variants but does not address population structure (ancestry groups may remain imbalanced across splits) or other confounders.\n\n\n12.6.3 Locus-Level Splits\nLocus-level splits hold out entire genomic positions, ensuring that no variant at a test position appears during training. This stringent approach prevents models from memorizing site-specific patterns and is essential for variant effect prediction where the goal is to score novel variants at positions the model has never seen.\nWhy do models memorize positions when batch effects or ascertainment biases exist? Gradient descent discovers whatever pathway most efficiently reduces loss. When certain genomic positions systematically appear in training with particular labels—because well-studied genes are overrepresented, or because sequencing centers focused on specific regions—the model can reduce loss by learning “position X tends to be pathogenic” rather than “variants disrupting this motif tend to be pathogenic.” In high-dimensional feature spaces, both pathways are viable; position memorization is often easier. Locus-level splits force the model to succeed on positions it has never seen, eliminating the memorization shortcut entirely.\nMany published benchmarks fail to implement locus-level splitting, allowing models to achieve high scores by recognizing familiar positions rather than learning generalizable effects. The evaluation considerations in Section 12.4 address these issues in detail.\n\n\n12.6.4 Region and Chromosome Splits\nRegion or chromosome splits hold out entire genomic regions, testing whether models learn biology that transfers across the genome rather than region-specific patterns. This is particularly relevant for regulatory prediction, where local chromatin context may differ between regions.\n\n\n12.6.5 Cohort and Site Splits\nCohort or site splits hold out entire institutions, sequencing centers, or biobanks, directly testing robustness to the batch and cohort effects discussed above. Models that perform well only within their training cohort but fail on held-out cohorts have learned institution-specific patterns.\n\n\n12.6.6 Temporal Splits\nTime-based splits use temporal ordering, training on earlier data and evaluating on later data. This approach simulates prospective deployment and tests robustness to temporal drift. A model trained on 2018 data and evaluated on 2023 data faces realistic distribution shift that random splits would obscure.\n\n\n12.6.7 Indirect Leakage Across Resources\nBeyond explicit split design, indirect leakage remains a concern. A variant that appears in ClinVar may also appear in gnomAD (with population frequency information), in functional assay datasets (with splicing or expression effects), and in literature-derived databases (with disease associations). Pretraining on any of these resources while evaluating on another creates indirect information flow that standard deduplication would miss.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore examining the table, consider these scenarios and predict which splitting strategy is most appropriate:\n\nYou’re building a variant effect predictor that must score novel variants at genomic positions never seen before\nYour model showed excellent performance in development but you need to test if it will work at other hospitals\nYou’re training on genotypes from families with rare diseases\nYou want to simulate how your model would perform if deployed next year\n\nMatch each scenario to the splitting strategy it requires. Then check the table to see if your predictions align with the “When to Use” column.\n\n\nThe following table compares splitting strategies and their properties:\n\n\n\nTable 12.3: Data splitting strategies for genomic models. Each strategy addresses different leakage risks and tests different generalization capabilities. The choice depends on the deployment context and evaluation goals.\n\n\n\n\n\n\n\n\n\n\n\n\nStrategy\nWhat It Holds Out\nLeakage Addressed\nGeneralization Tested\nWhen to Use\n\n\n\n\nRandom\nRandom samples\nNone\nInterpolation only\nNever for final evaluation\n\n\nFamily-aware\nFamily groups\nRelatedness memorization\nAcross unrelated individuals\nWhen pedigree structure exists\n\n\nLocus-level\nGenomic positions\nPosition memorization\nNovel genomic positions\nVariant effect prediction\n\n\nChromosome\nEntire chromosomes\nRegional patterns\nCross-genome transfer\nRegulatory prediction\n\n\nCohort/Site\nInstitutions\nBatch effects, coding practices\nCross-institution deployment\nClinical deployment validation\n\n\nTemporal\nTime periods\nFuture information\nProspective performance\nSimulating real deployment",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-leakage-confounding",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-leakage-confounding",
    "title": "12  Confounding and Data Leakage",
    "section": "12.7 Data Leakage as Confounding",
    "text": "12.7 Data Leakage as Confounding\nData leakage can be understood as a special case of confounding where the confounder is information that should not exist at prediction time. This framing clarifies why leakage inflates performance estimates and why leaked models fail in deployment: they have learned associations with variables that are unavailable when predictions must actually be made.\nThe detailed taxonomy of leakage types (label, feature, temporal, and benchmark leakage) along with detection strategies is provided in Section 12.4. Here we examine how each leakage type creates confounding structures that distort model evaluation.\n\n12.7.1 Causal Structure of Leakage\nIn causal terms, leakage introduces a backdoor path between features and labels that does not represent the relationship we intend to model. Consider a pathogenicity predictor trained with conservation scores that were computed using alignments incorporating known pathogenic variants. The causal structure includes: (1) the intended path from sequence features through biological mechanism to pathogenicity, and (2) a leaked path from pathogenicity labels through their influence on conservation databases back to conservation features. The model cannot distinguish signal flowing through these two paths, and performance estimates reflect both.\nLabel leakage creates confounding when the process that generated labels also influenced feature construction. The confounder is the shared information source: ClinVar curators who used computational predictions created a dependency between those predictions and subsequent labels. Feature leakage creates confounding when features correlate with labels through non-causal pathways, such as batch effects that happen to align with case-control status. Temporal leakage creates confounding through time-dependent information flow: future knowledge that influenced either features or labels introduces associations that would not exist in prospective application.\n\n\n12.7.2 Compounding Effects\nThese leakage types interact and compound. A model suffering from multiple forms may achieve extraordinary benchmark performance while learning nothing transferable to prospective clinical use. The apparent signal is real within the leaked evaluation framework but spurious for the intended application.\n\n\n\n\n\n\nStop and Think: Compound Leakage\n\n\n\nConsider a variant effect predictor that: (1) uses conservation scores computed from databases that include known pathogenic variants, (2) was trained on ClinVar labels that were influenced by earlier predictors, and (3) is evaluated on a benchmark constructed using similar computational filtering methods.\nHow many distinct leakage pathways can you identify? For each pathway, what would the model learn that would inflate its apparent performance but fail in prospective deployment?\n\n\nConsider a variant effect predictor that uses conservation scores (feature leakage), was trained on ClinVar labels influenced by earlier predictors (label leakage), and is evaluated on a benchmark constructed using similar computational methods (benchmark leakage). Each leakage type independently inflates performance; together, they create an evaluation that measures something entirely different from prospective predictive ability.\n\n\n12.7.3 Implications for Confounding Analysis\nThe confounding framework suggests that leakage detection methods (described in Section 12.4) can be understood as strategies for identifying and blocking backdoor paths. Feature ablation removes variables that may carry leaked signal. Temporal validation eliminates paths that depend on future information. Baseline analysis reveals when simple confounders explain most of the apparent performance.\nThis perspective also clarifies why some apparent leakage may be acceptable. If conservation scores will always be available at prediction time, the path through conservation represents legitimate signal rather than confounding. The distinction depends on the deployment context: what information will actually be available when the model must make predictions? Leakage is confounding by information that exists in evaluation but not in application.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-detection",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-detection",
    "title": "12  Confounding and Data Leakage",
    "section": "12.8 Detecting Confounding",
    "text": "12.8 Detecting Confounding\nConfounding is often subtle, requiring systematic diagnostics rather than reliance on aggregate performance metrics.\n\n\n\n\n\n\nDiagnostic toolkit for detecting confounding\n\n\n\n\nFigure 12.5: Diagnostic toolkit for detecting confounding. (1) Confounder-only baselines: if ancestry PCs alone approach full model performance, ancestry confounding is likely. (2) Stratified analysis: large performance gaps between ancestry groups suggest the model exploits group-specific shortcuts. (3) Residual association: predictions should not correlate with ancestry after controlling for the true label; residual correlation indicates ancestry encoding beyond what the task requires. (4) Split sensitivity: performance should not depend dramatically on splitting strategy; large drops under locus-level or temporal splits indicate memorization. (5) Negative controls: models should not predict outcomes unrelated to genetics (insurance type, administrative codes); above-chance prediction indicates learned confounders. Apply all diagnostics; confounding may manifest in only some.\n\n\n\n\n12.8.1 Confounder-Only Baselines\nThe most direct diagnostic trains simple models using only potential confounders: ancestry principal components, batch indicators, sequencing center identifiers, recruitment site. If these confounder-only baselines approach the performance of complex genomic models, confounding likely drives a substantial portion of the signal. Reporting confounder-only baselines alongside genomic model results makes hidden shortcuts visible.\n\n\n\n\n\n\nKey Insight: The Baseline Diagnostic\n\n\n\nIf a model using only ancestry principal components (no genomic features) achieves 0.75 auROC, and your full genomic model achieves 0.82 auROC, how much of that 0.82 reflects biology versus ancestry confounding? The baseline provides a floor: any performance attributable to your genomic features is the delta above this floor. Always compute and report confounder-only baselines.\n\n\nThe choice of baseline fundamentally shapes conclusions about model performance. A particularly insidious form of baseline weakness occurs when polygenic prediction studies use only clumping-and-thresholding (C+T) methods as comparators rather than LD-aware Bayesian approaches. C+T aggressively discards genetic signal by pruning correlated variants, creating an artificially weak baseline that inflates apparent deep learning gains by 16-60% compared to properly tuned alternatives (Ge et al. 2019; Vilhjálmsson et al. 2015).\nStudies reporting neural network “improvements” over C+T baselines may be demonstrating only that neural networks implicitly model linkage disequilibrium—which LD-aware Bayesian methods like LDpred2 and PRS-CS already capture more efficiently. When compared against these stronger baselines, apparent neural network advantages often disappear or reverse. A 2025 Nature Communications analysis found that neural networks performed only 93-95% as well as properly tuned linear regression for polygenic prediction when appropriate baselines were used, with apparent “nonlinear advantages” reflecting joint-tagging effects rather than genuine epistasis detection.\n\n\n\n\n\n\nBaseline Selection as Hidden Confounder\n\n\n\nWeak baselines function as hidden confounders in model evaluation: they create spurious associations between model complexity and performance improvement that do not reflect genuine capability gains. Always verify that published comparisons include LD-aware methods (LDpred2-auto, PRS-CS, SBayesR) rather than only C+T. Claims of substantial improvement over “state-of-the-art” warrant skepticism until baseline strength is confirmed.\n\n\n\n\n12.8.2 Stratified Performance Analysis\nPerformance stratified by ancestry group, sequencing platform, institution, and time period reveals whether aggregate metrics mask heterogeneity. Both discrimination (auROC, area under the precision-recall curve (auPRC)) and calibration diagnostics should be computed for each subgroup. Models may achieve high overall auROC while being poorly calibrated or nearly useless for specific subpopulations. Performance that varies dramatically across subgroups suggests confounding or distribution shift even when overall metrics appear strong.\n\n\n12.8.3 Residual Confounder Associations\nAssociations between model outputs and potential confounders can reveal encoding of ancestry or batch information beyond what the label requires. Plotting predictions against ancestry principal components, adjusting for true label status, shows residual confounding. Comparing mean predicted risk across batches or time periods within the same true label class identifies systematic biases. Formal association tests (regression, mutual information) between predictions and confounders that show strong residual associations indicate the model has learned confounder-related features that go beyond predicting the label itself.\n\n\n12.8.4 Split Sensitivity Analysis\nVarying the splitting strategy probes for leakage. Re-evaluating performance under locus-level splits, cohort holdouts, or temporal splits reveals whether initial results depended on memorization. A model that achieves 0.90 auROC with random splits but only 0.75 auROC with locus-level splits has likely memorized site-specific patterns. Large drops in performance under stricter splitting indicate inflated initial results.\n\n\n12.8.5 Negative Control Outcomes\nUsing outcomes known to be unrelated to genomics as negative controls provides powerful confirmation of confounding. If a model trained to predict disease from genotypes can also predict administrative outcomes (insurance type, documentation completeness) with similar accuracy, it has learned confounders. Shuffling labels within batch or ancestry strata should eliminate predictive signal; if it does not, the model exploits structure that transcends any specific outcome.\n\n\n\n\n\n\nKnowledge Check: Apply Detection Methods\n\n\n\nYou’ve trained a disease prediction model that achieves 0.88 auROC. Apply what you’ve learned about detection:\n\nName three diagnostic tests you would run to detect confounding\nFor each test, describe what result would indicate a problem\nIf your confounder-only baseline achieves 0.82 auROC, what does this tell you about your model?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nThree key diagnostics: (a) Confounder-only baseline using ancestry PCs and batch indicators, (b) Stratified performance analysis across ancestry groups and sequencing centers, (c) Split sensitivity comparing random vs. locus-level vs. temporal splits. (2) Problem indicators: (a) Baseline approaches full model performance (e.g., 0.82 vs 0.88), indicating confounding drives most signal; (b) Performance varies dramatically across subgroups (e.g., 0.90 in European ancestry but 0.60 in African ancestry), suggesting shortcuts; (c) Performance drops substantially under stricter splits (e.g., from 0.88 to 0.70 with locus-level), indicating memorization. (3) If the confounder-only baseline achieves 0.82 auROC while your full model achieves 0.88, then 0.82 of your performance comes from ancestry/batch shortcuts, and only 0.06 (the delta) comes from actual genomic features—your model has learned primarily confounders, not biology.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-mitigation",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-mitigation",
    "title": "12  Confounding and Data Leakage",
    "section": "12.9 Mitigation Strategies",
    "text": "12.9 Mitigation Strategies\nNo mitigation strategy eliminates confounding entirely, and each involves trade-offs between bias, variance, and coverage. The approaches described here are complementary: design-based methods constrain confounding before modeling begins, statistical adjustments handle residual confounding, invariance learning provides protection when confounders are incompletely measured, and rigorous benchmark construction ensures that evaluation reflects generalization rather than shortcut learning.\n\n\n\n\n\n\nMitigation strategies address confounding at different stages\n\n\n\n\nFigure 12.6: Mitigation strategies address confounding at different stages. Study design approaches (matching, balanced sampling) prevent confounding before data collection but reduce sample size. Covariate adjustment during training explicitly models known confounders but may inadvertently remove genuine biological signal. Residualization removes confounded variance in preprocessing but assumes linear relationships. Adversarial invariance learning trains representations that do not encode confounders but requires knowing which groups to enforce invariance over. Group DRO optimizes for worst-group performance at the cost of average performance. Multi-site training diversifies data sources. Temporal splits evaluate prospective performance. No single strategy eliminates all confounding; effective practice combines multiple approaches targeting different confounding sources.\n\n\n\nThe following table provides a decision framework for selecting mitigation strategies based on your specific situation:\n\n\n\nTable 12.4: Mitigation strategy selection guide. Choose based on when intervention is possible, what confounders are measured, and acceptable trade-offs.\n\n\n\n\n\n\n\n\n\n\n\n\nStrategy\nWhen Applied\nConfounder Requirement\nMain Trade-off\nBest For\n\n\n\n\nMatching\nStudy design\nKnown, measurable\nReduced sample size\nKnown major confounders\n\n\nCovariate adjustment\nTraining\nKnown, measurable\nMay remove real signal\nAncestry, batch correction\n\n\nResidualization\nPreprocessing\nKnown, measurable\nInformation loss\nStrong linear confounding\n\n\nAdversarial invariance\nTraining\nKnown groups\nReduced accuracy\nUnknown within-group variation\n\n\nGroup DRO\nTraining\nKnown groups\nWorse average performance\nFairness-critical applications\n\n\nMulti-site training\nData collection\nNone\nLogistical complexity\nInstitution effects\n\n\nTemporal splits\nEvaluation\nTime stamps\nSmaller test set\nProspective deployment\n\n\n\n\n\n\n\n12.9.1 Study Design and Cohort Construction\nDesign-based approaches provide the most robust protection against confounding because they prevent the problem rather than attempting to correct it statistically. When cases and controls are matched on potential confounders before data collection, those variables cannot drive spurious associations regardless of model complexity.\nMatching strategies balance cases and controls on age, sex, ancestry, recruitment site, and sequencing platform. For ancestry, matching can use self-reported categories, genetic principal components, or fine-scale population assignments depending on the granularity required. Why does matching work when statistical adjustment could handle the same variables? Matching eliminates confounding by design rather than by assumption. Statistical adjustment assumes the functional form relating confounders to outcomes is correctly specified; if the true relationship is nonlinear or involves interactions the model does not include, residual confounding persists. Matching makes no such assumptions: when cases and controls have identical confounder distributions, no functional form is needed because there is no confounder-outcome variation to model. Exact matching (requiring identical values) provides the strongest protection but may be infeasible when confounders are continuous or when the pool of potential controls is limited. Propensity score matching or coarsened exact matching offer practical alternatives that achieve approximate balance across multiple confounders simultaneously.\nBalanced sampling during training prevents models from optimizing primarily for majority patterns. When one ancestry group comprises 80% of training data, gradient updates predominantly reflect that group’s patterns, and minority group performance suffers. Down-sampling the majority group or up-sampling minority groups within mini-batches ensures that all groups contribute meaningfully to parameter updates. The trade-off is reduced effective sample size: discarding majority group samples wastes information, while up-sampling minority groups risks overfitting to limited examples.\nProspective collection with diversity targets ensures that training data represent the populations where models will be deployed. Retrospective matching can balance existing cohorts but cannot address variants or patterns that are absent from the original collection. The All of Us Research Program, Million Veteran Program, and similar initiatives that prioritize ancestral diversity from inception provide data that enable genuinely generalizable models, though the genomic AI field has yet to fully leverage these resources.\nThe limitation of design-based approaches is that they must anticipate which variables will confound. Unknown or unmeasured confounders cannot be matched, and over-matching (matching on variables that are consequences of the exposure) can introduce bias rather than remove it. Design and analysis approaches work best in combination: match on known confounders, then adjust for residual imbalances that matching did not eliminate.\n\n\n12.9.2 Covariate Adjustment\nCovariate adjustment explicitly models confounders rather than ignoring them, allowing estimation of outcome effects that account for confounding variables. The approach is familiar from genome-wide association studies, where including ancestry principal components as covariates in regression models reduces spurious associations driven by population structure.\nFor foundation models, covariate adjustment takes several forms. The simplest approach includes confounder variables (ancestry PCs, batch indicators, sequencing platform) as additional input features alongside genomic data. The model learns to use confounder information when predicting outcomes, and the genomic feature coefficients or attention weights reflect associations that remain after accounting for confounders. This approach assumes the model can learn the appropriate adjustment; for complex confounding patterns, explicit modeling may be preferable to implicit learning.\nResidualization removes confounder-associated variance before training genomic models. Regressing features or phenotypes on confounders and retaining only the residuals ensures that subsequent models cannot exploit confounder-outcome associations. The risk is removing genuine biological signal when confounders correlate with causal variants. Ancestry principal components, for instance, capture population structure that includes both confounding (differential ascertainment) and biology (population-specific genetic architecture). Aggressive residualization may discard the latter along with the former.\nMixed models and hierarchical structures treat institution, batch, or ancestry group as random effects, estimating genomic associations while accounting for clustering within groups. This approach is standard in genetic epidemiology and translates naturally to deep learning through hierarchical Bayesian frameworks or explicit modeling of group-level parameters. The key advantage is borrowing strength across groups while allowing group-specific intercepts or slopes, though computational costs increase substantially for large datasets with many groups.\nThe fundamental limitation of covariate adjustment is that it requires measuring and correctly specifying confounders. Unmeasured confounders remain uncontrolled. Conditioning on colliders (variables caused by both exposure and outcome) introduces bias rather than removing it. Careful causal reasoning, often formalized through directed acyclic graphs, is essential for determining which variables should be adjusted and which should not.\n\n\n12.9.3 Domain Adaptation and Invariance Learning\nDomain adaptation methods aim to learn representations that do not encode confounders, achieving predictions that generalize across batches, institutions, or populations without explicitly modeling each source of variation. These approaches are particularly valuable when confounders are numerous, incompletely measured, or difficult to specify.\nAdversarial training adds a discriminator network that attempts to predict batch identity, ancestry, or other confounders from learned representations. The feature extractor is trained with two competing objectives: maximize prediction accuracy for the primary task while minimizing the discriminator’s ability to recover confounder labels. Why does this adversarial setup produce invariant representations? The gradient reversal trick provides the key insight: during backpropagation, gradients from the discriminator are negated before flowing to the feature extractor. Instead of learning features that help predict the confounder (which normal backprop would produce), the feature extractor learns features that actively hurt confounder prediction. The feature extractor faces a two-player game where the discriminator tries to extract confounder information and the feature extractor tries to hide it. At equilibrium, representations contain information useful for the primary task but encode confounders no better than random chance. When successful, the learned representations retain information useful for prediction while discarding information that distinguishes confounded groups. Domain adversarial neural networks and gradient reversal layers implement this approach efficiently within standard deep learning frameworks.\nThe theoretical limitation is that perfect invariance and maximum accuracy cannot be achieved simultaneously when confounders correlate with the outcome through both causal and non-causal pathways. Enforcing strict invariance to ancestry, for instance, may remove genuine population-specific genetic effects along with confounding. Practitioners must balance the degree of invariance against task performance, typically through hyperparameters controlling the adversarial loss weight.\nGroup distributionally robust optimization (group DRO) targets worst-group performance rather than average performance, encouraging models that work for all subgroups rather than optimizing for the majority. The training objective minimizes the maximum loss across predefined groups (ancestry categories, sequencing platforms, institutions), ensuring that no group is systematically disadvantaged. This approach requires group labels during training and may sacrifice some average performance to improve worst-case outcomes.\nImportance weighting and distribution matching align feature distributions across domains without explicit adversarial training. Samples from underrepresented domains receive higher weights during training, or feature distributions are explicitly matched through maximum mean discrepancy or optimal transport objectives. These methods can be combined with other approaches and are particularly useful when the target deployment distribution is known but differs from training data.\n\n\n12.9.4 Data Curation and Benchmark Design\nThe signals available for learning depend entirely on how data are curated and how benchmarks are constructed. Careful attention to data provenance and evaluation design prevents many confounding problems that would otherwise require complex modeling solutions.\nDeduplication across training and evaluation sets prevents direct memorization. For genomic data, deduplication must operate at multiple levels: individual samples (the same person appearing under different identifiers), family groups (relatives sharing haplotype segments), and genomic loci (the same variant position appearing in both training and test sets). Variant effect prediction requires particularly stringent locus-level deduplication; a model that has seen any variant at position chr1:12345 during training cannot be fairly evaluated on novel variants at that position.\nSplitting strategies determine what generalization is actually tested. Random splits assess interpolation within the training distribution. Locus-level splits test generalization to novel genomic positions. Chromosome holdouts test transfer across genomic regions. Cohort splits test robustness to institutional and demographic differences. Temporal splits simulate prospective deployment. Each strategy answers a different question, and benchmark performance under one splitting regime does not guarantee performance under others. Reporting results across multiple splitting strategies reveals which aspects of generalization a model has achieved. The comprehensive treatment of benchmark design in Chapter 11 addresses these considerations in detail.\nBenchmark diversity ensures that evaluation reflects the full range of deployment contexts. Benchmarks constructed from a single ancestry group, institution, or sequencing platform test only narrow generalization. Explicitly including diverse ancestries, multiple institutions, and varied technical platforms in evaluation sets reveals performance heterogeneity that homogeneous benchmarks would hide. The ProteinGym and CASP benchmarks in protein modeling demonstrate how thoughtfully constructed evaluation resources can drive genuine progress; genomic variant interpretation would benefit from similar community efforts.\nDocumentation of overlaps between training resources and benchmarks enables readers to assess potential leakage. When a foundation model is pretrained on gnomAD, fine-tuned on ClinVar, and evaluated on a benchmark that filters variants using gnomAD frequencies, the information flow is complex and potentially circular. Explicit documentation of which resources contributed to which stages of model development allows appropriate skepticism about performance claims. Benchmark papers should catalog known overlaps with major training resources; model papers should acknowledge which benchmarks may be compromised by their pretraining choices.\n\n\n12.9.5 Causal Inference Approaches\nWhen observational confounding cannot be eliminated through design or statistical adjustment, causal inference frameworks offer principled alternatives that leverage the structure of genetic inheritance itself.\nThe random assortment of alleles at meiosis creates natural experiments that Mendelian randomization exploits (Davey Smith and Ebrahim 2003). Because genotypes are assigned before birth and cannot be influenced by most environmental confounders, genetic variants that affect an exposure (such as a biomarker level or gene expression) can serve as instrumental variables for estimating causal effects on downstream outcomes. Why does this random assortment matter for causal inference? Consider the confounding that plagues observational studies: people with high LDL cholesterol may also smoke, exercise less, and have poorer diets, confounding any association between LDL and heart disease. But the genetic variants affecting LDL levels were randomly assigned at conception, before any lifestyle choices occurred. These variants cannot be confounded by lifestyle because they were fixed before lifestyle existed. By using genetic variants as instruments, Mendelian randomization asks: “Do people who were randomly assigned higher LDL (through genetic lottery) have higher heart disease risk?” This isolates the causal effect of LDL itself. A foundation model trained to predict expression levels can be evaluated for causal relevance by testing whether its predictions, instrumented through genetic variants, associate with disease outcomes in ways that survive Mendelian randomization assumptions. This approach has revealed that many observational biomarker-disease associations reflect confounding rather than causation, and similar logic applies to model-derived predictions.\nDirected acyclic graphs (DAGs) formalize assumptions about causal structure and clarify which variables should be adjusted, which should be left unadjusted, and which adjustments would introduce bias rather than remove it (Pearl 2009). Conditioning on a collider (a variable caused by both exposure and outcome) induces spurious associations; conditioning on a mediator blocks causal pathways of interest. Explicit DAG construction forces researchers to articulate their causal assumptions, making hidden confounding visible and enabling principled variable selection. For genomic models, DAGs clarify the relationships among ancestry, technical factors, biological mechanisms, and phenotypic outcomes, revealing which adjustment strategies address confounding versus which inadvertently condition on consequences of the outcome.\nOutcomes and exposures known to be unrelated to the prediction target provide empirical tests of residual confounding without requiring complete causal knowledge (Lipsitch, Tchetgen Tchetgen, and Cohen 2010). A negative control outcome is one that should not be causally affected by the exposure of interest; if the model predicts it anyway, confounding is present. A negative control exposure is one that should not causally affect the outcome; association with the outcome again indicates confounding. For a variant effect predictor, administrative outcomes (insurance status, documentation completeness) serve as negative control outcomes that genotypes should not predict. Synonymous variants in non-conserved regions can serve as negative control exposures that should not affect protein function. Strong predictions for negative controls reveal that the model has learned confounders rather than biology.\nThese causal approaches do not replace careful study design and rigorous splitting, but they provide additional tools for distinguishing genuine biological signal from confounded associations, particularly when the same observational data must serve both training and evaluation purposes.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-fairness",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-fairness",
    "title": "12  Confounding and Data Leakage",
    "section": "12.10 Fairness and External Validity",
    "text": "12.10 Fairness and External Validity\nConfounding connects directly to fairness and health equity. Models that achieve high average performance while failing for specific populations may appear successful while exacerbating existing disparities.\nPolygenic risk scores illustrate this tension. European ancestry-derived scores predict cardiovascular disease, diabetes, and breast cancer risk reasonably well within European ancestry populations. Applied to African or Asian ancestry individuals, the same scores show substantially worse discrimination and calibration (Duncan et al. 2019). Healthcare systems that deploy these scores without ancestry-specific validation risk providing inferior risk stratification to already underserved populations. The portability analysis framework in Section 3.7 quantifies these degradations, while clinical deployment frameworks (Section 27.8) address operational responses.\nVariant interpretation exhibits similar patterns. ClinVar contains many more pathogenic variant classifications for European ancestry individuals than for other populations (Landrum et al. 2018). The data composition issues underlying this imbalance are examined in Section 2.8.1. Predictors trained on ClinVar inherit this imbalance, performing better for variants common in European populations and worse for variants enriched in other ancestries. Clinical deployment of such predictors may reduce diagnostic yield for non-European patients.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA hospital system proposes deploying a polygenic risk score for breast cancer screening prioritization. The score was developed and validated in a European ancestry cohort. The hospital serves a population that is 40% African ancestry and 25% Hispanic/Latino.\n\nWhat fairness concerns should be raised before deployment?\nWhat validation studies would you require?\nWhat monitoring should be implemented post-deployment?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nMajor fairness concerns: the score may show 40-75% reduced accuracy in non-European populations, potentially providing inferior risk stratification to already underserved groups; differential performance could lead to missed diagnoses in minority populations or inappropriate screening recommendations. (2) Required validation: stratified performance analysis by ancestry group in the target population; calibration assessment for each ancestry group (not just discrimination); comparison to ancestry-matched baselines; sensitivity analysis to understand performance degradation mechanisms. (3) Post-deployment monitoring: track screening recommendations and cancer detection rates stratified by ancestry; monitor for disparities in false positive/negative rates; assess whether the tool improves or worsens existing outcome disparities; implement thresholds for stopping use if equity metrics deteriorate.\n\n\n\n\n\n\nThe uncertainty quantification approaches discussed in Chapter 23 provide partial mitigation: models that report high uncertainty for under-represented populations at least flag predictions that should not be trusted. Out-of-distribution detection methods (Section 23.6) specifically address when inputs fall outside the training distribution. The interpretability methods in Chapter 24 can reveal when models rely on ancestry-correlated features, with attribution analysis (Section 24.1) identifying which input features drive ancestry-dependent predictions. Yet technical solutions alone are insufficient. Addressing fairness requires intentional data collection that prioritizes under-represented populations, evaluation protocols that mandate subgroup analysis, and deployment decisions that consider equity alongside aggregate accuracy.\nExternal validity asks whether a model’s performance in one setting predicts its performance in another. Confounding and distribution shift often cause dramatic external validity failures. A model that achieves excellent metrics in the development cohort may fail when deployed at a different institution, in a different healthcare system, or in a different country. The clinical risk prediction frameworks in Section 27.9 emphasize multi-site validation precisely because single-site performance frequently fails to generalize.\nThe fairness implications of confounding extend beyond technical model performance into questions of justice in healthcare resource allocation, diagnostic equity, and the distribution of benefits from genomic medicine. Governance frameworks for addressing these structural challenges are examined in Section 26.1.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-checklist",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-checklist",
    "title": "12  Confounding and Data Leakage",
    "section": "12.11 A Practical Checklist",
    "text": "12.11 A Practical Checklist\nThe following checklist synthesizes the diagnostics and mitigations discussed above. Systematic application during model development and evaluation surfaces confounding that would otherwise remain hidden.\n\n\n\n\n\n\nPractical Guidance: Using This Checklist\n\n\n\nThis checklist should be applied at three stages: (1) during study design, to prevent confounding through matching and balanced sampling; (2) during model development, to detect and mitigate confounding through diagnostics and training modifications; and (3) during evaluation, to ensure that performance estimates reflect genuine generalization rather than shortcut learning. Document your responses to each item in your methods section.\n\n\nPopulation structure and relatedness: Quantify ancestry via principal components and relatedness via kinship coefficients. Decide explicitly whether to match, stratify, or adjust for these factors, and document the justification. Report performance stratified by ancestry group. When family structure exists in the data, verify that relatives do not appear across train-test boundaries.\nData splits and leakage: Ensure individuals, families, and genomic loci do not cross the train-validation-test boundaries for target tasks. Implement stricter splits (locus-level, chromosome-level, cohort-based, time-based) and report the performance differences. Check for overlap with external databases or benchmarks used in evaluation and document any shared resources.\nBatch, platform, and cohort effects: Catalog technical variables (sequencing center, instrument, protocol, assay) and cohort identifiers. Test whether these variables predict labels or align with subgroups of interest. Use embedding visualizations, principal components, or simple classifiers to detect batch signatures. Apply mitigation (design matching, covariate adjustment, domain adaptation) when batch effects are detected.\nLabel quality and curation bias: Document how labels were defined and what processes (billing codes, expert review, computational prediction, registry inclusion) produced them. Quantify label noise where possible. Consider robust training strategies when labels are noisy. Assess how curated resources like ClinVar reflect historical biases and whether those biases affect evaluation validity.\nCross-group performance and fairness: Report metrics for each major subgroup (ancestry, sex, age, cohort, platform) rather than only aggregate performance. Examine calibration across groups, not just discrimination. Discuss clinical implications of residual performance gaps and whether deployment might worsen existing disparities.\nReproducibility and transparency: Document dataset construction, inclusion criteria, and splitting strategies completely. Release preprocessing, training, and evaluation code when feasible. Describe which confounders were measured, how they were handled, and what limitations remain.\nModels that pass this checklist provide more reliable evidence of genuine biological learning. Models that fail at multiple points may achieve benchmark success while learning shortcuts that will not transfer to new settings.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_2/p2-ch12-confounding.html#sec-ch12-rigor",
    "href": "part_2/p2-ch12-confounding.html#sec-ch12-rigor",
    "title": "12  Confounding and Data Leakage",
    "section": "12.12 Rigor as Response",
    "text": "12.12 Rigor as Response\nThese confounding and bias problems are not reasons for despair. They are reasons for rigor. The same expressive capacity that enables foundation models to discover subtle shortcuts also enables them to learn complex biological patterns when training data and evaluation protocols are designed appropriately. The goal is not to abandon powerful models but to create conditions under which their power serves biological discovery rather than benchmark gaming.\nSeveral trends support progress. Multi-ancestry biobanks and international collaborations expand the diversity of available training data. Benchmark developers implement stricter splitting protocols and require subgroup analyses. Pretraining strategies that explicitly promote invariance to technical factors are emerging. Uncertainty quantification methods (Chapter 23) provide mechanisms for models to express appropriate caution when inputs fall outside their training distribution. The problem of confounding is tractable with sustained attention to data provenance, evaluation design, and deployment monitoring. The benchmark catalog in Chapter 11 identifies which evaluation resources are most susceptible to particular confounders, while the evaluation methodology in Chapter 12 provides protocols for detecting leakage before it inflates reported performance.\nYet vigilance remains essential. New datasets bring new confounders. Novel architectures create new opportunities for shortcut learning. Community benchmarks accumulate indirect leakage as resources are reused across studies. Treating confounding as a first-order concern throughout model development, rather than an afterthought addressed only when reviewers complain, distinguishes models that actually work from models that merely perform well on convenient benchmarks. The interpretability methods in Chapter 24 provide tools for distinguishing genuine regulatory insight from sophisticated pattern matching, with mechanistic interpretability (Section 24.7) offering the strongest evidence about what models have actually learned. The uncertainty quantification approaches in Chapter 23 enable models to communicate when their predictions should not be trusted, with selective prediction (Section 23.7) providing operational frameworks for routing uncertain cases to human review. Together with rigorous evaluation, these capabilities move the field toward models that reveal genuine biology and behave reliably across the diverse clinical and scientific settings where they will be deployed.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nDistinguish between confounding, bias, data leakage, and distribution shift. Give a concrete genomic example of each.\nHow does population structure create shortcuts that foundation models exploit, and why does increasing model capacity amplify rather than mitigate this problem?\nA confounder-only baseline using ancestry PCs achieves 0.75 auROC, while your full genomic model achieves 0.82 auROC. What does this reveal about your model’s learned features?\nWhy does label circularity in ClinVar (where computational predictions influence pathogenicity annotations) inflate validation metrics for new predictors trained on ClinVar?\nWhat splitting strategies address different types of leakage (individual overlap, family structure, position memorization, temporal drift)?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nConfounding, bias, leakage, and distribution shift: Confounding occurs when a variable affects both features and labels (example: ancestry influences allele frequencies through population history and disease risk through healthcare access pathways). Bias is systematic deviation from the target quantity (example: training on 50% disease prevalence but deploying at 5% prevalence causes systematic over-prediction). Data leakage occurs when test information influences training (example: the same variant appearing in both training and test sets, or ClinVar labels being influenced by CADD scores that later become training features). Distribution shift is mismatch between training and deployment distributions (example: a model trained on one hospital’s coding practices failing at a different institution with different documentation standards).\nPopulation structure as exploitable shortcut: Population structure creates dual pathways where ancestry affects genetic features (through population-specific allele frequencies, haplotypes, and linkage disequilibrium patterns) and simultaneously affects disease labels through non-biological pathways (healthcare access, environmental exposures, clinical ascertainment practices). Foundation models can detect ancestry from local k-mer frequencies and haplotype patterns even in raw sequences, then exploit ancestry-label correlations as shortcuts. Increasing model capacity amplifies this problem because larger transformers with billions of parameters can discover increasingly subtle ancestry-linked features that smaller models would miss—model expressiveness is an amplifier of confounding, not a defense against it.\nInterpreting confounder-only baseline performance: The confounder-only baseline achieving 0.75 auROC while the full model achieves 0.82 auROC reveals that the vast majority of predictive performance (0.75 out of 0.82) comes from ancestry/batch shortcuts rather than genomic biology. Only the 0.07 delta represents signal genuinely attributable to genomic features beyond what ancestry alone provides. This indicates the model has primarily learned to exploit confounders rather than biological mechanisms, and would likely fail when deployed in settings where ancestry-outcome relationships differ from the training distribution.\nLabel circularity inflates validation metrics: When clinical laboratories cite computational predictions like CADD or REVEL as supporting evidence for pathogenic classifications, and those classifications become ClinVar labels, new predictors trained on ClinVar face a circular task: they are learning to replicate what previous predictors said, not learning true pathogenicity. The inflation occurs because the new model achieves high agreement on labels that were influenced by computational predictions—essentially predicting what old models predicted rather than independent biological truth. This circularity is invisible within the circular ecosystem but becomes apparent during prospective validation on genuinely novel variants or independent functional assays where the feedback loop does not exist.\nSplitting strategies for different leakage types: Individual overlap requires random individual-level splits at minimum, but this only works when samples are truly independent. Family structure requires family-aware splits that keep all relatives together in the same partition to prevent haplotype memorization. Position memorization requires locus-level splits that hold out entire genomic positions, ensuring the model has never seen any variant at test positions during training. Temporal drift requires time-based splits that train on earlier data and test on later data, simulating prospective deployment and capturing evolution in sequencing technology, coding practices, and diagnostic criteria. Each splitting strategy tests a different aspect of generalization, and models should be evaluated under all relevant strategies to demonstrate genuine robustness.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nCore Concepts:\n\nConfounding occurs when a variable affects both features and labels, creating spurious associations that models exploit as shortcuts\nPopulation structure is the most pervasive confounder in genomics, affecting both genetic features and phenotypes through non-biological pathways\nBatch effects from sequencing centers, capture kits, and analysis pipelines can become predictive signals that fail at deployment\nLabel circularity occurs when computational predictions influence training labels, creating apparent validation that reflects agreement rather than insight\nData leakage can be understood as confounding by information unavailable at prediction time\n\nKey Diagnostics:\n\nConfounder-only baselines reveal how much signal comes from shortcuts versus biology\nStratified performance analysis exposes hidden heterogeneity across subgroups\nSplit sensitivity analysis (random vs. locus-level vs. temporal) tests for memorization\nNegative control outcomes confirm whether models learn confounders\n\nMitigation Hierarchy:\n\nPrevention through design: Matching, balanced sampling, prospective diverse collection\nAdjustment during training: Covariate inclusion, residualization, mixed models\nInvariance learning: Adversarial training, group DRO\nRigorous evaluation: Locus-level splits, cohort holdouts, temporal validation\n\nConnection to Other Chapters:\n\nEvaluation methodology (Chapter 12) provides detailed leakage detection protocols\nUncertainty quantification (Chapter 23) flags predictions on out-of-distribution inputs\nInterpretability (Chapter 24) reveals what features models actually use\nClinical deployment (Section 27.9) addresses operational fairness monitoring\n\nTake-Home Message: High benchmark performance is not evidence of biological learning. Only rigorous evaluation design, systematic confounding diagnostics, and stratified subgroup analysis can distinguish models that have learned biology from models that have learned shortcuts. Treat confounding as a first-order concern throughout model development.\n\n\n\n\n\n\nDavey Smith, George, and Shah Ebrahim. 2003. “‘Mendelian Randomization’: Can Genetic Epidemiology Contribute to Understanding Environmental Determinants of Disease?*.” International Journal of Epidemiology 32 (1): 1–22. https://doi.org/10.1093/ije/dyg070.\n\n\nDuncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R. Peterson, and B. Domingue. 2019. “Analysis of Polygenic Risk Score Usage and Performance in Diverse Human Populations.” Nature Communications 10 (1): 3328. https://doi.org/10.1038/s41467-019-11112-0.\n\n\nGe, Tian, Chia-Yen Chen, Yang Ni, Yen-Chen Anne Feng, and Jordan W. Smoller. 2019. “Polygenic Prediction via Bayesian Regression and Continuous Shrinkage Priors.” Nature Communications 10 (1): 1776. https://doi.org/10.1038/s41467-019-09718-5.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLipsitch, Marc, Eric Tchetgen Tchetgen, and Ted Cohen. 2010. “Negative Controls: A Tool for Detecting Confounding and Bias in Observational Studies.” Epidemiology 21 (3): 383. https://doi.org/10.1097/EDE.0b013e3181d61eeb.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006. “Population Structure and Eigenanalysis.” PLOS Genetics 2 (12): e190. https://doi.org/10.1371/journal.pgen.0020190.\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\nPrice, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E. Weinblatt, Nancy A. Shadick, and David Reich. 2006. “Principal Components Analysis Corrects for Stratification in Genome-Wide Association Studies.” Nature Genetics 38 (8): 904–9. https://doi.org/10.1038/ng1847.\n\n\nVilhjálmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev, Sara Lindström, Stephan Ripke, Giulio Genovese, et al. 2015. “Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores.” American Journal of Human Genetics 97 (4): 576–92. https://doi.org/10.1016/j.ajhg.2015.09.001.",
    "crumbs": [
      "Part II: Sequence Architectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Confounding and Data Leakage</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html",
    "href": "part_3/p3-ch13-fm-principles.html",
    "title": "13  Foundation Model Paradigm",
    "section": "",
    "text": "13.1 From Task-Specific Models to Foundation Models\nThe history of computational genomics reveals a consistent pattern: models become more general while maintaining or improving task-specific performance. Hand-crafted scores such as CADD and SIFT established that integration of diverse genomic annotations could improve variant pathogenicity prediction (Rentzsch et al. 2019; Schubach et al. 2024) (Chapter 4). These approaches faced a ceiling imposed by the features available for engineering, a limitation examined in Section 4.6.4. These approaches relied on expert feature engineering, combining conservation scores, functional annotations, and population frequency data through ensemble methods or logistic regression.\nTask-specific deep learning models demonstrated that neural networks could learn relevant features directly from sequence. DeepSEA predicted chromatin accessibility and transcription factor binding from 1 kb sequences using convolutional architectures (J. Zhou and Troyanskaya 2015). ExPecto extended this approach to gene expression prediction by modeling regulatory elements across multiple cell types (J. Zhou et al. 2018). Sei organized regulatory predictions into interpretable sequence classes through unsupervised clustering (Chen et al. 2022). SpliceAI achieved near-perfect splice site prediction through dilated convolutions over 10 kb contexts, though its architecture was purpose-built for this specific task and could not generalize to other regulatory prediction problems (Chapter 6). Enformer scaled sequence-to-function modeling to 200 kb windows and thousands of chromatin tracks through transformer architectures (Avsec et al. 2021).\nThese models succeeded within their specific domains but remained difficult to repurpose. Training a DeepSEA model required chromatin accessibility data. Using SpliceAI for regulatory prediction would require complete retraining on different labels. Each application domain needed its own model, trained from scratch on task-specific data. The fundamental limitation was not model capacity but training paradigm: supervised learning on narrow tasks produced narrow capabilities.\nSequence language models introduced the self-supervised pretraining paradigm (Chapter 8) to genomics. DNABERT applied masked language modeling to DNA sequences, demonstrating that general representations could be learned without task-specific labels (Ji et al. 2021) (Section 14.2). ESM and ESM-2 showed that protein language models pretrained on sequence alone could transfer effectively to structure prediction, variant effect prediction, and protein design (Rives et al. 2021; Lin et al. 2022) (Section 15.1). The Nucleotide Transformer family scaled DNA language modeling to cross-species training corpora (Dalla-Torre et al. 2023) (?sec-ch11-nucleotide-transformer). HyenaDNA used implicit convolutions to reach million-token contexts at single-nucleotide resolution (Nguyen et al. 2023) (?sec-ch11-hyenadna).\nThe transition from task-specific to foundation models changes the relationship between model developers and users. Task-specific models deliver predictions as their primary product. Foundation models deliver representations that users adapt to their own tasks through the transfer learning techniques examined in Chapter 9. This distinction affects everything from model architecture design to evaluation strategies to deployment infrastructure.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-task-specific",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-task-specific",
    "title": "13  Foundation Model Paradigm",
    "section": "",
    "text": "Predict Before You Look\n\n\n\nBefore viewing the figure below, make a prediction: In the task-specific paradigm, if a researcher needs to solve five different genomic prediction problems (e.g., splice site prediction, enhancer identification, transcription factor binding, chromatin accessibility, and gene expression), how many separate models would they need to train? In the foundation model paradigm, how many models would be trained from scratch? What is the key difference in how knowledge is reused between these two approaches?\n\n\n\n\n\n\n\n\n\n\nTask-specific paradigm: isolated models for isolated tasks\n\n\n\n\n\n\n\nFoundation model paradigm: shared representations, efficient adaptation\n\n\n\n\n\n\nFigure 13.1: The paradigm shift from task-specific to foundation models. (A) The task-specific paradigm trains separate models from scratch for each application. Knowledge about sequence patterns cannot transfer between tasks, requiring substantial labeled data for each new application. (B) The foundation model paradigm pretrains a single large model on diverse unlabeled sequences, capturing general biological knowledge in reusable representations. Small task-specific adapters enable efficient transfer to diverse downstream tasks. This paradigm shift mirrors developments in natural language processing, where pretrained language models revolutionized the efficiency and capability of text-based AI systems.\n\n\n\n\n\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading about the formal definition of foundation models, consider: what properties would you require for a model to qualify as a “foundation” for multiple downstream tasks? Think about training data, output types, and how users would interact with the model.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-defining",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-defining",
    "title": "13  Foundation Model Paradigm",
    "section": "13.2 Defining Genomic Foundation Models",
    "text": "13.2 Defining Genomic Foundation Models\nThe term “foundation model” appears frequently in genomics literature, sometimes applied to any large neural network trained on biological sequences. Establishing clear criteria helps separate true genomic foundation models from ordinary deep learning approaches that happen to operate on DNA or protein sequences.\n\n13.2.1 Essential Properties\nThe defining characteristic of genomic foundation models is their capacity to serve purposes far beyond their original training objectives. This generality emerges from several interconnected properties.\nFoundation models train on entire genomes, pan-genomic sequence collections, or large assay compendia with minimal supervision. Their pretraining objectives include masked language modeling, next-token prediction, denoising, or multi-task sequence-to-function prediction. Critically, these objectives do not require dense task-specific labels for every training example. A model that requires annotated enhancers or curated pathogenic variants for every training instance does not qualify as a foundation model under this criterion.\nThe representations these models produce must prove useful across many downstream tasks. Embeddings can be extracted through forward passes and reused with simple linear probes or lightweight adapter modules rather than requiring full model retraining. The representations should encode biological information at multiple scales, from local sequence motifs to long-range regulatory grammar.\nTransfer capability extends across multiple dimensions: different assays (from chromatin accessibility to gene expression), different tissues and cell types, different species, and different variant types (from SNVs to structural variants). Evidence of broad transfer requires evaluation across multiple benchmarks rather than demonstration of performance on a single task (Chapter 11).\nFoundation models operate at a scale that would be impractical for task-specific training. Some scale context length, as HyenaDNA scales to million-token windows at single-nucleotide resolution. Others scale parameter count, as the ESM and Nucleotide Transformer families reach billions of parameters. Still others scale data diversity through pan-genomic pretraining across hundreds of species or integration of many assays and cell types. The scaling dimension chosen reflects the model’s intended applications and architectural constraints.\nFinally, foundation models typically expose consistent APIs for common operations. These include embedding extraction for sequences or variants, sequence probability scoring, and mask-based in silico mutagenesis for variant effect prediction. Models distributed through repositories such as Hugging Face often include documented recipes for downstream fine-tuning and example notebooks demonstrating common use cases.\nThe following table summarizes the essential properties that distinguish foundation models from task-specific alternatives.\n\n\n\nTable 13.1: Distinguishing properties of foundation models versus task-specific models.\n\n\n\n\n\n\n\n\n\n\nProperty\nFoundation Model\nTask-Specific Model\n\n\n\n\nTraining data\nMinimal/no labels; self-supervised\nDense task-specific labels required\n\n\nTransfer capability\nMany downstream tasks\nSingle task\n\n\nScale dimension\nParameters, context, or data diversity\nOptimized for specific task\n\n\nUser interaction\nEmbeddings + adaptation\nEnd-to-end predictions\n\n\nEvaluation\nMulti-task benchmarks\nSingle-task metrics\n\n\n\n\n\n\nWhy does self-supervised pretraining enable transfer while supervised training does not? The difference lies in what the training objective encourages the model to learn. A model trained to predict splice sites learns features specifically useful for splice site prediction—the GT-AG consensus, branch point motifs, exonic splicing enhancers. These features may be irrelevant or even misleading for other tasks. In contrast, a model trained to predict masked nucleotides across the entire genome must learn features useful for reconstructing any genomic context. To succeed at this broad objective, the model must discover general principles: how motifs combine, how sequence composition varies across regions, what patterns distinguish functional from non-functional sequence. These general features transfer because they capture the underlying organization of the genome rather than task-specific shortcuts.\n\n\n13.2.2 What does not Count\nMany excellent genomic models fail one or more of these criteria and should not be classified as foundation models. Early versions of DeepSEA trained specifically on chromatin accessibility data from a limited set of cell types lack the generality and standardized interface of foundation models, though later iterations that integrate many assays begin to approach foundation model territory (J. Zhou and Troyanskaya 2015). SpliceAI predicts splicing outcomes exceptionally well but was designed for that specific task and provides neither general-purpose embeddings nor easy transfer to other genomic prediction problems (Jaganathan et al. 2019). Even a very large Enformer-like model trained solely on human chromatin tracks remains bound to its specific prediction interface despite its scale and sophistication (Avsec et al. 2021).\nThe distinction matters for several reasons. It affects evaluation strategy, since foundation models must be assessed across families of tasks rather than single benchmarks (Chapter 12). It affects integration into existing pipelines, since foundation models serve as feature extractors while task-specific models typically provide end-to-end predictions. It affects how we think about model development, since foundation model training requires different infrastructure and data curation than task-specific supervised learning.\n\n\n13.2.3 Limitations of the Foundation Model Concept\nThe term “foundation” carries implications worth examining. Architectural foundations are static, load-bearing, and invisible once construction proceeds. Genomic foundation models share only the load-bearing property: they support downstream applications that would otherwise require independent construction. Yet unlike architectural foundations, these models remain visible and modifiable throughout their use. Fine-tuning adjusts the foundation itself rather than building atop an immutable base. The metaphor also implies that foundations precede and enable all subsequent work, but genomic foundation models often coexist with task-specific alternatives that outperform them on narrow benchmarks.\nA more accurate metaphor might be “foundation” in the educational sense: a broad base of knowledge that enables specialized learning but continues to develop alongside it. The pretraining phase establishes general competence; adaptation refines that competence for specific purposes without abandoning the original learning. This framing better captures the dynamic relationship between pretrained representations and downstream tasks, though the architectural metaphor has become standard terminology.\n\n\n\n\n\n\nKey Insight: The Foundation Model Criterion\n\n\n\nA model qualifies as a foundation model not by its size or training cost, but by its demonstrated ability to transfer to diverse downstream tasks without full retraining. The critical test is: can users extract embeddings or apply lightweight adaptation to solve problems the original developers never anticipated? If the answer is yes across multiple task families, you have a foundation model. If the model only produces predictions for its original task, it remains task-specific regardless of scale.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-scaling",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-scaling",
    "title": "13  Foundation Model Paradigm",
    "section": "13.3 Scaling Laws and Compute-Optimal Training",
    "text": "13.3 Scaling Laws and Compute-Optimal Training\nThe success of foundation models in natural language processing rests partly on empirical scaling laws: predictable relationships between model size, training data, computational budget, and performance. Understanding these relationships guides resource allocation and model development decisions.\n\n13.3.1 Chinchilla Framework and Genomic Constraints\n\n\n\n\n\n\nMathematical Content Ahead\n\n\n\nThe following subsection presents the mathematical formulation of scaling laws. The key intuition is that performance improves predictably with more parameters and more data, but the rate of improvement follows specific power laws. If you find the equations challenging, focus on the practical implications summarized after the mathematical derivation.\n\n\nHoffmann et al. formalized the relationship between model performance and scaling factors through a power law decomposition (Hoffmann et al. 2022):\n\\[L(N, D) = E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta}\\]\nwhere \\(L\\) represents cross-entropy loss on held-out data, \\(N\\) denotes model parameters, \\(D\\) indicates training tokens, and \\(E\\), \\(A\\), \\(B\\), \\(\\alpha\\), \\(\\beta\\) are empirically fitted constants. For language models, the exponents \\(\\alpha\\) and \\(\\beta\\) both approximate 0.3, meaning doubling parameters or data reduces loss by roughly 20%. The constant \\(E\\) represents irreducible loss (the entropy remaining even with infinite resources), while the parameter term \\(A/N^\\alpha\\) quantifies gains from greater model capacity and the data term \\(B/D^\\beta\\) captures gains from additional training examples.\nWhy does this decomposition take this particular form? The power law structure reflects the diminishing returns inherent in learning: the first million parameters capture the most common patterns, while each subsequent million captures progressively rarer regularities. The exponents near 0.3 indicate that performance improvements slow considerably as scale increases—you need roughly eight times the resources to halve the gap to optimal performance. The additive structure separates distinct bottlenecks: insufficient model capacity to represent learned patterns (the \\(N\\) term) versus insufficient data diversity to learn all relevant patterns (the \\(D\\) term). This decomposition explains why over-parameterized models trained on limited data, or under-parameterized models trained on vast data, both underperform balanced allocations.\nTraining cost constrains both parameters and data simultaneously through the compute budget \\(C\\) (measured in FLOPs):\n\\[C \\approx 6ND\\]\nThis approximation holds because each training token requires roughly 6 FLOPs per parameter (one forward pass and one backward pass through the network). Optimizing this tradeoff by minimizing loss subject to the budget constraint yields:\n\\[N_{\\text{opt}} \\propto C^{0.49}, \\quad D_{\\text{opt}} \\propto C^{0.51}\\]\nThese exponents, both near 0.5, encode the Chinchilla insight: model size and training data should scale approximately equally. Practical implementations target roughly 20 tokens per parameter for compute-optimal training.\n\n\n\n\n\n\nWorked Example: Applying the Chinchilla Framework\n\n\n\nScenario: Your lab has a compute budget of \\(10^{20}\\) FLOPs for training a DNA language model. How should you allocate between model size and training data?\nStep 1: Apply the 20-tokens-per-parameter heuristic. Using \\(C \\approx 6ND\\) and \\(D \\approx 20N\\):\n\\[10^{20} \\approx 6 \\times N \\times 20N = 120N^2\\]\n\\[N \\approx \\sqrt{10^{20}/120} \\approx 2.9 \\times 10^8 \\text{ parameters}\\]\nStep 2: Calculate corresponding data requirement:\n\\[D \\approx 20 \\times 2.9 \\times 10^8 \\approx 5.8 \\times 10^9 \\text{ tokens}\\]\nInterpretation: With this budget, you should target approximately 300 million parameters trained on roughly 6 billion tokens. Training a 3 billion parameter model on the same budget would leave it severely undertrained (only 600 million tokens), likely underperforming the smaller compute-optimal model.\nCaveat for genomics: These ratios were derived for natural language. Genomic sequence may require different ratios due to its simpler alphabet and different statistical structure. When in doubt, empirically validate on held-out data.\n\n\nThe constants fitted for language models should not be assumed to hold for genomic tasks. DNA lacks the hierarchical compositional structure of natural language; regulatory grammar does not build meaning through recursive phrase structure the way sentences do. More critically, the 20-tokens-per-parameter guidance reflects optimization for next-token prediction loss on held-out text. Genomic foundation models often aim to learn representations that transfer to diverse downstream tasks (variant effect prediction, chromatin state inference, evolutionary constraint estimation), none of which were objectives during language model pretraining. The optimal balance between parameters and data may shift when the goal is representation learning rather than task-specific loss minimization.\n\n\n13.3.2 Empirical Scaling in Genomic Models\nSeveral genomic foundation model families have reported scaling experiments, though systematic scaling laws comparable to NLP remain elusive. The Nucleotide Transformer family provides perhaps the clearest genomic scaling data (Dalla-Torre et al. 2023). Performance on downstream benchmarks improves consistently with parameter count across models from 50 million to 2.5 billion parameters. The largest models (trained on multi-species data) outperform smaller models trained on human sequences alone, suggesting that cross-species data provides effective scaling even when human-specific performance is the target. Training compute scaled from approximately \\(10^{19}\\) to \\(10^{21}\\) FLOPs across the model family.\n\n\n\n\n\n\n\n\nLoss vs. model size\n\n\n\n\n\n\n\nDownstream performance vs. model size\n\n\n\n\n\n\n\nOptimal allocation of compute\n\n\n\n\n\n\nFigure 13.2: Scaling laws for genomic foundation models. (A) Pretraining loss decreases predictably with model parameters following a power law, enabling informed decisions about resource allocation. (B) Downstream task performance scales consistently across diverse tasks including contact prediction, secondary structure, and variant effects, demonstrating that larger models capture more transferable biological knowledge. (C) Compute-optimal scaling reveals the trade-off between model size and training data: for fixed compute budget, optimal performance requires balancing parameter count with training tokens. These scaling relationships, first established in natural language processing, extend to biological sequence models and guide foundation model development.\n\n\n\nESM-2 demonstrated similar scaling for protein language models, with performance on structure prediction and variant effect tasks improving smoothly from 8 million to 15 billion parameters (Lin et al. 2022). The largest ESM-2 models approach the structure prediction accuracy of AlphaFold2 using only single-sequence input, a capability entirely absent in smaller models. HyenaDNA focused on context length scaling rather than parameter scaling, demonstrating that million-token contexts at single-nucleotide resolution could be achieved through sub-quadratic architectures (Nguyen et al. 2023).\nThe scaling law framework has direct implications for model development decisions in genomics, though the constraints differ fundamentally from natural language processing. Unlike natural language, where text data is effectively unlimited, genomic sequence data faces hard constraints. Reference genomes for well-studied species total perhaps \\(10^{11}\\) to \\(10^{12}\\) nucleotides. Population-level variant data can expand this somewhat, but the effective diversity may be lower than raw counts suggest. In such data-constrained regimes, smaller models trained to convergence may outperform larger models that are undertrained.\nAcademic groups typically face stricter compute constraints than industry labs. Given fixed compute budgets, the Chinchilla framework suggests allocating resources toward longer training of smaller models rather than abbreviated training of larger models. A 500 million parameter model trained for 10 epochs on diverse genomic data may outperform a 5 billion parameter model trained for 1 epoch on the same data. Cross-species data offers a potential path around genomic data limitations. The Nucleotide Transformer and Evo families exploit this strategy, learning evolutionary patterns from diverse genomes that improve human-specific predictions.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider the genomic data constraint: reference genomes contain roughly \\(10^{11}\\) to \\(10^{12}\\) nucleotides total. How does this compare to the trillions of tokens used to train large language models? What strategies might help overcome this data limitation for genomic foundation models?\n\n\n\n\n13.3.3 Emergent Capabilities\nPerhaps the most intriguing aspect of foundation model scaling is the emergence of qualitatively new capabilities at sufficient scale. Emergence refers to abilities that are absent or negligible in smaller models but appear discontinuously as models grow. Think of it like learning to ride a bicycle: you cannot ride “a little bit”—you wobble and fall until suddenly, with enough practice, you can balance. The capability emerges all at once rather than improving gradually. Similarly, certain model capabilities remain effectively zero until the model crosses a threshold, then appear seemingly from nowhere.\nIn large language models, emergent capabilities include multi-step reasoning, code generation, and in-context learning. These capabilities appear at model scales of roughly \\(10^{10}\\) parameters and above, with no clear precursor in smaller models (wei_emergent_2022?).\nGenomic foundation models exhibit analogous emergence, though the capability thresholds are less well characterized. The most striking example involves structural understanding from sequence: ESM-2 at sufficient scale produces contact maps and secondary structure predictions from single sequences with accuracy approaching multiple sequence alignment methods like trRosetta (Lin et al. 2022). Smaller ESM models show no meaningful structural understanding. This capability emerges at approximately 650 million parameters and continues improving with scale.\nLarger Nucleotide Transformer models transfer more effectively to novel species not seen during training (Dalla-Torre et al. 2023). The ability to generalize beyond training species appears to require sufficient model capacity to learn abstract regulatory principles rather than memorizing species-specific patterns. Similarly, foundation models at sufficient scale can predict variant effects without task-specific fine-tuning, using only the difference in likelihood between reference and alternative sequences. For example, ESM-1v computes log-likelihood ratios to predict protein variant pathogenicity in a zero-shot manner. This zero-shot capability requires models large enough to capture subtle sequence dependencies\nFew-shot approaches include task examples in the input context, allowing in-context learning without parameter updates. HyenaDNA demonstrated this capability for genomic tasks, suggesting that sufficiently large models with long context can adapt through prompts rather than training (Nguyen et al. 2023).\nThe practical implication is that capability thresholds exist: models below certain scales may be fundamentally incapable of certain tasks regardless of fine-tuning. Identifying these thresholds helps guide model selection and prevents wasted effort fine-tuning models that lack necessary capacity.\n\n\n\n\n\n\nKey Insight: Emergence Creates Capability Thresholds\n\n\n\nCertain capabilities only appear above specific scale thresholds. A 100 million parameter protein language model cannot perform zero-shot structure prediction regardless of how it is fine-tuned—the capability simply does not exist at that scale. Before attempting to adapt a foundation model for a challenging task, verify that models of similar scale have demonstrated the required capability. Attempting to fine-tune a model below the capability threshold is wasted effort.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-taxonomy",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-taxonomy",
    "title": "13  Foundation Model Paradigm",
    "section": "13.4 A Taxonomy of Genomic Foundation Models",
    "text": "13.4 A Taxonomy of Genomic Foundation Models\nThe landscape of genomic foundation models can be organized into four broad families. Each family exhibits distinct characteristics, strengths, limitations, and typical application domains.\n\n\n\n\n\n\nTaxonomy of genomic foundation models\n\n\n\n\nFigure 13.3: Taxonomy of genomic foundation models organized by modality and approach. DNA language models (blue) process nucleotide sequences with emphasis on long context and single-nucleotide resolution. Protein language models (green) encode evolutionary knowledge from protein sequences with increasing integration of structural information. Regulatory sequence models (orange) combine sequence processing with multi-task prediction of chromatin and expression tracks. Multi-modal and emerging models (purple) integrate across modalities, combining sequence with structure (AlphaFold2) or leveraging multiple information sources simultaneously. Arrows indicate connections between families where models build on each other’s capabilities.\n\n\n\nThe following table provides a quick reference for comparing the four foundation model families across key dimensions.\n\n\n\nTable 13.2: Comparison of genomic foundation model families.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFamily\nInput\nOutput\nPretraining\nStrength\nLimitation\n\n\n\n\nDNA LMs\nNucleotide sequence\nEmbeddings, probabilities\nMLM, autoregressive\nGeneral, scalable\nNo functional grounding\n\n\nSeq-to-Function\nSequence windows\nAssay predictions\nSupervised multi-task\nMechanistic\nTied to training assays\n\n\nVEP Models\nVariant + context\nEffect scores\nMixed supervision\nClinical relevance\nNarrow task focus\n\n\nMulti-Omic\nMultiple modalities\nCross-modal embeddings\nContrastive, joint\nHolistic\nData engineering complexity\n\n\n\n\n\n\n\n13.4.1 DNA Language Models\nDNA language models treat genomic sequence as a language to be modeled, learning representations from raw nucleotide strings through self-supervised objectives. Without explicit functional labels, these models discover patterns through statistical regularities in genomic sequence.\nThe pretraining objectives typically involve masked language modeling or autoregressive next-token prediction. Training draws from reference genomes or pan-genomic sequence collections spanning multiple species. The resulting models produce per-position or pooled sequence embeddings that can be extracted and used for downstream tasks. Critically, these embeddings are not tied to specific assays or cell types, making them applicable to any task that benefits from general sequence context.\nDNABERT and DNABERT-2 apply BERT-style masked language modeling to DNA sequences, using overlapping k-mers as tokens (Ji et al. 2021; Z. Zhou et al. 2024) (Section 14.2). The Nucleotide Transformer family scales this approach to larger parameter counts and cross-species training (Dalla-Torre et al. 2023) (?sec-ch11-nucleotide-transformer). HyenaDNA achieves subquadratic complexity through implicit convolutions, enabling context lengths up to one million nucleotides (Nguyen et al. 2023) (?sec-ch11-hyenadna). Caduceus incorporates bidirectional processing and reverse-complement equivariance as architectural inductive biases (?sec-ch11-caduceus). Evo 2 combines long-range attention with biological tokenization strategies (?sec-ch11-evo2). GROVER integrates learned BPE-style tokenization with training on regulatory tracks in addition to raw sequence (Sanabria et al. 2024). These models and their architectural innovations are examined in detail in Chapter 14.\nThe primary strength of DNA language models lies in their generality: representations not bound to specific assays, cell types, or experimental conditions, capable of processing novel sequences absent from reference genomes. Their self-supervised training requires only genome sequences, making them scalable to massive corpora. The corresponding limitation is that without explicit functional grounding, they may not capture subtle regulatory patterns that manifest only under specific cellular conditions. Performance on tasks requiring fine-grained functional discrimination may lag models trained with functional supervision.\nApplications span sequence classification (promoters, enhancers, transposons), motif discovery, variant effect prediction through embedding perturbation, sequence generation for synthetic biology, and transfer learning to new species with limited labeled data.\n\n\n13.4.2 Sequence-to-Function Foundation Models\nSequence-to-function models predict molecular readouts directly from sequence through supervised or semi-supervised training on assay compendia. These models blur into foundation model territory when their output space is sufficiently broad and their internal representations prove useful for tasks beyond the original assay set.\nThese models map DNA sequences to high-dimensional vectors of molecular measurements, including chromatin accessibility, histone modifications, transcription factor binding, and gene expression levels. Training uses large collections of functional genomics assays spanning many cell types, enabling the models to learn regulatory grammar through supervised prediction of molecular phenotypes.\nEnformer predicts thousands of chromatin and expression tracks from 200 kb sequence windows through transformer attention (Avsec et al. 2021) (Section 16.2). Borzoi extends this with refined architectures and expanded RNA-seq coverage (?sec-ch13-borzoi). Sei organizes predictions into interpretable sequence classes through unsupervised clustering (Chen et al. 2022) (?sec-ch13-sei). Earlier models including DeepSEA and Basset established the paradigm at smaller scales (Chapter 6).\nThe explicit functional supervision in these models provides mechanistic grounding that pure language models lack. Predictions can be interpreted through comparison to experiments. The models naturally support variant effect prediction by computing reference-alternative differences. The tradeoff is that models remain tied to training assays and cell types; extension to new contexts typically requires retraining or new data collection.\nApplications center on regulatory variant interpretation in well-studied cell types, eQTL fine-mapping, enhancer identification, transcription factor binding prediction, and regulatory mechanism discovery.\n\n\n13.4.3 Variant Effect Prediction Models\nThe clinical need to interpret genetic variants has driven development of models optimized specifically for predicting functional or clinical consequences. These take a variant and predict its effect on molecular phenotypes, organismal fitness, or disease risk.\nVariant effect prediction models integrate sequence context with evolutionary information, population genetics signals, and sometimes structural or functional annotations. They output pathogenicity scores, effect size estimates, or functional consequence predictions. Training combines multiple data sources: clinical labels from ClinVar, population frequency from gnomAD, functional assays such as deep mutational scanning, and evolutionary constraint metrics.\nAlphaMissense applies protein language models to predict pathogenicity of missense variants (Cheng et al. 2023) (Section 17.2.3). ESM-1v uses evolutionary context for protein variant effect prediction (?sec-ch14-zeroshot-plm). EVE combines evolutionary and structural information (?sec-ch14-alignment-models). Genomic foundation models like DNABERT and Enformer provide variant effect predictions through in silico mutagenesis (?sec-ch14-dna-vep). The architecture, training, evaluation, and clinical deployment of variant effect predictors are covered comprehensively in Chapter 17, with integration into clinical workflows detailed in Chapter 28.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nAt this point, you should be able to distinguish between the three model families covered so far. Without looking back, try to answer: What is the key difference between DNA language models and sequence-to-function models in terms of their training objectives? Which family would you choose if you needed to predict enhancer activity in a novel cell type not represented in existing training data?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nDNA language models use self-supervised objectives (masked language modeling or next-token prediction) on raw sequence without functional labels, while sequence-to-function models train with supervised multi-task prediction on thousands of chromatin and expression assays. For a novel cell type, DNA language models would be preferable because they learn general sequence patterns that transfer across contexts, whereas sequence-to-function models are tied to the specific cell types and assays in their training data.\n\n\n\n\n\n\n\n13.4.4 Multi-Omic Foundation Models\nThe most ambitious foundation models natively integrate multiple molecular modalities, jointly processing DNA sequence, chromatin state, gene expression, protein abundance, 3D genome structure, or phenotypic descriptions.\nMulti-omic models employ architectures designed for heterogeneous input types: transformer variants with cross-attention, graph neural networks, or modality-specific encoders with fusion layers (Chapter 7, Chapter 21). Training objectives encourage cross-modal alignment through contrastive learning, joint prediction, or generative modeling of multiple data types.\nOmni-DNA uses transformer-based autoregressive models with vocabulary expansion and multi-task finetuning, unifying diverse genomic tasks under an instruction-response paradigm (Li et al. 2025). Models integrating Hi-C data capture 3D genome organization (Chapter 20). Cross-modal architectures align DNA embeddings with chromatin or expression predictions (Chapter 22).\nThe unified representations these models produce enable cross-modal queries, and joint training can improve performance through multi-task effects. Data engineering becomes substantially more complex, however, with different modalities requiring different measurement technologies and quality control. The field is early, with few models reaching production maturity.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-design-dimensions",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-design-dimensions",
    "title": "13  Foundation Model Paradigm",
    "section": "13.5 Design Dimensions",
    "text": "13.5 Design Dimensions\nWithin and across families, individual models differ along orthogonal design dimensions that affect suitability for specific tasks.\n\n13.5.1 Data Composition\nThe choice of training data shapes what patterns a model can learn. Training on human sequences alone focuses on clinically relevant patterns but limits exposure to evolutionary diversity. Cross-species training encourages learning of conserved elements and evolutionary constraints, potentially improving generalization but risking dilution of human-specific signals.\nSequence diversity presents a similar tradeoff. Training on reference genomes alone provides clean sequences but limited exposure to population variation. Incorporating variant data improves robustness but requires careful design to avoid learning spurious associations. Models may also train on raw sequence alone or incorporate functional annotations, trading generality against functional grounding. The implications of training data choices for model bias are examined in Chapter 12.\n\n\n13.5.2 Architecture Choices\nArchitectural decisions determine both computational characteristics and inductive biases. Among transformer variants, encoder-only models (DNABERT, Nucleotide Transformer) excel at classification and embedding tasks, while decoder-only models (GROVER) support generative applications (Chapter 7). Full and sparse attention patterns, linear approximations, and Flash attention implementations affect computational efficiency.\nHyena-based models and state space models achieve subquadratic scaling, enabling longer contexts than standard transformers with comparable parameters. Hybrid approaches combine local convolutions with global attention, as in Enformer, processing sequences at multiple resolutions.\n\n\n13.5.3 Context Length\nThe context window determines what genomic relationships a model can capture. Short context (under 1 kb) captures local patterns: motifs, splice sites, promoter elements. Medium context (1 to 10 kb) spans complete genes with proximal regulatory regions. Long context (10 to 200 kb) represents enhancer-promoter interactions and TAD-scale organization. Ultra-long context (over 200 kb) enables chromosomal domain modeling and complex structural variant interpretation. The effective use of long context requires appropriate tokenization and positional encoding strategies discussed in Chapter 5, with specific implementations examined in Section 14.2 (k-mer tokenization), ?sec-ch11-nucleotide-transformer (BPE variants), and Section 7.2 (position embeddings).\nThe following table summarizes the relationship between context length and the biological phenomena that can be captured.\n\n\n\nTable 13.3: Context length determines what genomic relationships a model can capture.\n\n\n\n\n\n\n\n\n\n\n\nContext Length\nRange\nBiological Scope\nExample Applications\n\n\n\n\nShort\n&lt; 1 kb\nMotifs, splice sites\nTF binding, splice prediction\n\n\nMedium\n1-10 kb\nGenes, proximal regulation\nPromoter analysis, UTR effects\n\n\nLong\n10-200 kb\nEnhancer-promoter, TADs\nRegulatory variants, eQTL\n\n\nUltra-long\n&gt; 200 kb\nChromosomal domains\nStructural variants, 3D genome\n\n\n\n\n\n\n\n\n13.5.4 Tokenization\nThe representation of nucleotides as model inputs affects both computational efficiency and biological resolution. Character-level tokenization maintains single-base resolution but imposes longest sequence lengths. K-mer tokenization reduces length by a factor approaching \\(k\\), with vocabulary reaching 4,096 for 6-mers. Learned tokenization (BPE-style) discovers schemes from data, potentially allocating vocabulary more efficiently (Medvedev et al. 2025). The choice should align with both computational constraints and biological resolution requirements. Detailed discussion of tokenization strategies appears in Chapter 5.\n\n\n\n\n\n\nDesign dimensions for genomic foundation models\n\n\n\n\nFigure 13.4: Design dimensions for genomic foundation models. Radar chart positions representative models across six key dimensions: context length (how much sequence the model processes), parameter count (model capacity), training compute (resources required), architecture type (encoder vs. decoder), tokenization strategy (k-mer vs. single-nucleotide), and pretraining objective (masked vs. autoregressive). Different models make different trade-offs: ESM-2 emphasizes parameter scale within protein-length contexts; Enformer balances long context with multi-task supervision; HyenaDNA pushes context length to megabases using sub-quadratic architectures; Evo combines massive scale with autoregressive generation. These trade-offs determine which applications each model best serves.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-build-vs-use",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-build-vs-use",
    "title": "13  Foundation Model Paradigm",
    "section": "13.6 Build Versus Use Decisions",
    "text": "13.6 Build Versus Use Decisions\nThe availability of pretrained foundation models creates strategic choices about when to use existing models, when to adapt them, and when to train from scratch.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading the detailed guidance, consider your own research context: Do you have unique proprietary data? What computational resources are available? How specific is your target task? Based on these factors, would you expect to use, adapt, or build a foundation model?\n\n\n\n13.6.1 When to Use Existing Models\nExisting foundation models provide immediate utility when the target application aligns with model capabilities, labeled data is limited, and computational resources are constrained.\nFor tasks where general sequence representations suffice, frozen foundation model embeddings with simple downstream classifiers often perform competitively with fine-tuned alternatives. This approach requires minimal compute (single forward passes), no gradient computation through large models, and modest labeled data (hundreds to thousands of examples). Applications include sequence classification, clustering, and similarity search.\nSome foundation models support zero-shot variant effect prediction through likelihood ratio scoring. This requires no task-specific training and produces calibrated scores for novel variants immediately. Zero-shot approaches work well when the pretraining objective aligns with the target task and when fine-tuning data is unavailable or unreliable.\nFoundation model APIs also enable rapid prototyping, allowing quick assessment of whether a modeling approach is viable before committing resources to custom development. Testing variant effect prediction with ESM-1v takes hours rather than the weeks required to train a custom model.\n\n\n13.6.2 When to Adapt Existing Models\nAdaptation through fine-tuning or lightweight methods (LoRA, adapters, prefix tuning) makes sense when downstream tasks require specialized behavior beyond what frozen embeddings provide, sufficient labeled data exists (typically thousands to tens of thousands of examples), and the target domain falls within the pretraining distribution.\nParameter-efficient methods like LoRA update a small fraction of model parameters (often under 1%) while keeping the foundation model frozen (Hu et al. 2021). This preserves general knowledge while allowing task-specific adaptation. Compute requirements are modest: a few GPU-hours for most genomic tasks. The approach works well when the foundation model’s representations are largely appropriate but need refinement for specific applications. Details on parameter-efficient adaptation appear in Chapter 9.\nUpdating all parameters typically achieves the best single-task performance but requires more data (tens of thousands of examples), more compute (GPU-days to weeks), and careful regularization to prevent overfitting. Full fine-tuning makes sense for high-stakes applications where maximum accuracy justifies the investment.\n\n\n13.6.3 When to Train from Scratch\nBuilding custom foundation models requires substantial justification given the resources involved.\nNovel domains present the clearest case for custom pretraining. When target sequences differ fundamentally from existing model pretraining data (novel species, synthetic sequences, non-standard nucleotides), existing models may provide poor transfer. Applications requiring architectural features absent from existing models (specific attention patterns, custom tokenization, multi-modal inputs) similarly demand building from scratch.\nOrganizations with unique large-scale datasets (clinical biobanks, pharmaceutical screening data) may achieve better performance through custom pretraining than public models allow, though the data advantage must be substantial to justify training costs. Applications requiring larger models or longer contexts than available options face similar calculus.\n\n\n13.6.4 Cost-Benefit Analysis\nThe decision framework involves comparing expected performance against resource requirements.\nTraining a foundation model from scratch requires \\(10^{20}\\) to \\(10^{22}\\) FLOPs, translating to thousands of GPU-hours and tens of thousands of dollars at current cloud prices. Fine-tuning requires \\(10^{16}\\) to \\(10^{18}\\) FLOPs, often achievable in hours on single GPUs. Inference with frozen embeddings requires only forward passes.\nFoundation model pretraining requires billions of tokens. Fine-tuning requires thousands to tens of thousands of labeled examples. Zero-shot and embedding approaches require only evaluation data.\nFor well-studied tasks with abundant labeled data, fine-tuned models typically outperform frozen embeddings by 5 to 15% on standard metrics. Zero-shot approaches often achieve 70 to 90% of fine-tuned performance. Custom foundation models rarely outperform existing options by large margins unless the application involves genuinely novel domains.\nThe following table summarizes the resource requirements and expected performance for each approach.\n\n\n\nTable 13.4: Resource requirements and expected performance for different foundation model approaches.\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nCompute\nData Required\nTime\nExpected Performance\n\n\n\n\nFrozen embeddings\n\\(10^{14}\\) FLOPs\n100s-1000s labels\nHours\n70-90% of fine-tuned\n\n\nLoRA/Adapters\n\\(10^{16}\\) FLOPs\n1000s labels\nHours-Days\n95% of full fine-tuning\n\n\nFull fine-tuning\n\\(10^{18}\\) FLOPs\n10Ks labels\nDays-Weeks\nBest single-task\n\n\nTrain from scratch\n\\(10^{20}\\)+ FLOPs\nBillions tokens\nWeeks-Months\nBest if novel domain\n\n\n\n\n\n\n\n\n\n\n\n\nDecision framework for using vs. building foundation models\n\n\n\n\nFigure 13.5: Decision framework for using vs. building foundation models. Entry point: a new genomic prediction task. First decision: does a suitable pretrained model exist? If yes, assess task alignment. For high alignment, USE frozen embeddings (hours of work, ~$10 compute, achieving 70-90% of fine-tuned performance)—this serves most applications. For moderate alignment, ADAPT using LoRA or light fine-tuning (days of work, $100-1000 compute, ~95% of full fine-tuning). Only when existing models fundamentally lack required capabilities should practitioners BUILD custom foundation models (months of work, $100K+ compute). The vast majority of applications are best served by using or adapting existing models rather than building from scratch.\n\n\n\nTime costs often dominate: using existing models takes hours to days, fine-tuning takes days to weeks, training from scratch takes weeks to months. For time-sensitive applications, using existing models often dominates even if custom training would eventually yield better results.\n\n\n\n\n\n\nPractical Guidance: The Build-vs-Use Decision\n\n\n\nFor most genomic applications, follow this decision sequence:\n\nStart with frozen embeddings from the most appropriate existing foundation model. Evaluate on held-out data before investing more resources.\nTry parameter-efficient fine-tuning (LoRA or adapters) if frozen embeddings underperform by more than 10% versus published baselines.\nConsider full fine-tuning only for high-stakes applications where the 5% improvement over LoRA justifies GPU-days of compute.\nTrain from scratch only when all of the following hold: (a) target domain differs fundamentally from existing pretraining data, (b) you have access to unique large-scale data, (c) timeline permits months of development, and (d) budget permits $100K+ in compute.\n\nMost researchers will never need to train a foundation model from scratch. The efficiency of using pretrained models is precisely their value proposition.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-evaluation",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-evaluation",
    "title": "13  Foundation Model Paradigm",
    "section": "13.7 Evaluation Principles",
    "text": "13.7 Evaluation Principles\nFoundation models resist evaluation on single tasks. Their value lies in transfer across many applications, making comprehensive evaluation substantially more complex than benchmarking task-specific models.\n\n13.7.1 Multi-Task Assessment\nA genomic foundation model should be evaluated across families of related tasks rather than isolated benchmarks. For DNA language models, this includes sequence classification tasks, variant effect prediction across multiple variant types, motif discovery, and cross-species transfer. For sequence-to-function models, evaluation should span prediction of held-out assays, transfer to novel cell types, and consistency with experimental measurements.\nThe diversity of evaluation tasks complicates comparison across models. A model excelling at promoter classification may underperform on eQTL fine-mapping. Direct comparisons require controlling for differences in training data, model scale, and evaluation protocols. Standardized benchmark suites are examined in Chapter 11.\n\n\n13.7.2 Transfer Versus Pretraining Performance\nFoundation models are intended for transfer, making pretraining loss only moderately predictive of downstream utility. A model with slightly worse masked language modeling loss may produce better embeddings if its training objective better aligns with useful representations. Evaluation should explicitly test transfer through zero-shot performance, few-shot learning, cross-domain transfer, and robustness to distribution shift.\nDetailed discussion of benchmark suites, evaluation protocols, and methodological best practices appears in Chapter 11 and Chapter 12.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-ecosystem",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-ecosystem",
    "title": "13  Foundation Model Paradigm",
    "section": "13.8 Foundation Model Ecosystem",
    "text": "13.8 Foundation Model Ecosystem\nGenomic foundation models exist within a broader ecosystem of infrastructure, community resources, and shared practices.\n\n13.8.1 Model Distribution\nMost models are distributed through centralized repositories. Hugging Face hosts many DNA and protein language models with documented APIs. GitHub repositories accompany publications with weights, code, and examples. Standardized formats reduce friction in adoption, enabling rapid benchmarking and experimentation.\n\n\n13.8.2 Documentation Requirements\nResponsible distribution requires comprehensive documentation: training data provenance, preprocessing procedures, architecture details, hyperparameters, evaluation protocols, and known limitations. Data provenance is particularly important given population-specific biases and use restrictions in genomic datasets (Chapter 12).\n\n\n13.8.3 Industry and Academic Contributions\nBoth academic and industry groups develop genomic foundation models. Academic models emphasize reproducibility and open access. Industry models may offer superior performance through proprietary data or compute but with limited transparency. Notable industry contributions include NVIDIA’s BioNeMo platform and Microsoft’s Azure genomics integration. Users should review license terms before clinical or commercial deployment.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-open-questions",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-open-questions",
    "title": "13  Foundation Model Paradigm",
    "section": "13.9 Open Questions",
    "text": "13.9 Open Questions\nDespite rapid progress, fundamental challenges remain unsolved, and the field’s trajectory remains uncertain.\nWhether genomic foundation models converge toward unified architectures or maintain specialized families is unclear. The diversity of genomic scales, resolution requirements, and functional contexts may preclude the convergence seen in NLP, where transformers now dominate across most tasks.\nExisting models learn correlations without distinguishing causal from spurious relationships. Integrating causal structure could improve robustness and enable counterfactual reasoning, but current architectures provide no principled mechanism for causal inference (Chapter 12).\nModels trained on reference genomes and common variants may not calibrate well for ultra-rare or de novo variants, precisely the variants most likely to be clinically actionable (Chapter 28). Improved integration of structural and evolutionary constraints could strengthen rare variant interpretation.\nTranslation to clinical use requires robust cross-population performance, calibrated uncertainty (Chapter 23), interpretability for clinicians (Chapter 24), prospective validation, and regulatory approval. These requirements extend well beyond benchmark performance, and the path from research model to clinical deployment remains poorly charted.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch13-fm-principles.html#sec-ch13-convergence",
    "href": "part_3/p3-ch13-fm-principles.html#sec-ch13-convergence",
    "title": "13  Foundation Model Paradigm",
    "section": "13.10 Convergence Without Consolidation",
    "text": "13.10 Convergence Without Consolidation\nFoundation models for genomics divide into families serving different needs. DNA language models learn general sequence representations from self-supervised pretraining, capturing evolutionary constraints and regulatory patterns without explicit functional labels (Chapter 14). Sequence-to-function models predict molecular phenotypes from sequence, providing quantitative outputs (expression levels, chromatin states, splice probabilities) that DNA language models alone cannot produce (Chapter 16). Variant effect models integrate sequence representations with evolutionary information to score the functional impact of genetic variants (Chapter 17). Multi-omic models combine sequence with additional data modalities to capture regulatory relationships that sequence alone cannot resolve (Chapter 22). No single family dominates; effective genomic AI requires matching model capabilities to application requirements.\nScale introduces both opportunities and constraints. Scaling laws describe predictable relationships between parameters, data, compute, and performance, enabling principled resource allocation. Some capabilities appear only at sufficient scale, creating thresholds that cannot be crossed through fine-tuning alone. The practical implication is that certain applications require institutional-scale investment, while others can leverage existing pretrained models with modest adaptation. The build-versus-use framework guides this decision: use existing models when they suffice, adapt through fine-tuning or feature extraction when needed, train from scratch only when unique data or requirements justify the investment.\nThis framework instantiates across specific domains. DNA language models (Chapter 14) and protein language models (Chapter 15) exemplify self-supervised pretraining on biological sequence. Regulatory models (Chapter 16) demonstrate sequence-to-function prediction at long-range scales. Variant effect prediction (Chapter 17) integrates multiple model families for clinical interpretation. Throughout, these principles guide model selection: what does this application require, which model family provides it, and what scale is necessary to achieve it?\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat distinguishes a foundation model from a task-specific deep learning model? Why does self-supervised pretraining enable transfer to multiple downstream tasks while supervised training does not?\nAccording to the Chinchilla scaling laws, what is the relationship between model parameters, training data, and compute budget? If you have a fixed compute budget, should you train a larger model on less data or a smaller model on more data?\nWhat are the four major families of genomic foundation models and what is the key strength and limitation of each family?\nExplain the concept of emergent capabilities in foundation models. Why does this matter when selecting a model for adaptation to a new task?\nWhen should you build a foundation model from scratch versus adapting an existing one? What are the key decision factors in the build-versus-use hierarchy?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nFoundation vs. task-specific models: Foundation models are distinguished by their ability to transfer to diverse downstream tasks through embeddings or lightweight adaptation, not by their size alone. Self-supervised pretraining enables transfer because the model must learn general features useful for reconstructing any genomic context (e.g., how motifs combine, sequence composition patterns, functional vs. non-functional sequence). In contrast, supervised training on narrow tasks produces features specifically optimized for that task (e.g., splice site prediction learns GT-AG consensus and branch points), which may be irrelevant or misleading for other applications.\nChinchilla scaling laws: The framework shows that loss \\(L(N, D) = E + A/N^\\alpha + B/D^\\beta\\) where \\(N\\) is parameters, \\(D\\) is training tokens, and compute \\(C \\approx 6ND\\). The key insight is that model parameters and training data should scale approximately equally—the optimal ratio is roughly 20 tokens per parameter. For a fixed compute budget, you should train a smaller model on more data rather than a larger model on less data, as undertrained large models typically underperform smaller compute-optimal models.\nFour foundation model families: (1) DNA language models use self-supervised pretraining for general sequence representations but lack functional grounding for subtle regulatory patterns. (2) Sequence-to-function models predict molecular phenotypes with mechanistic grounding but are tied to training assays and cell types. (3) Variant effect prediction models integrate multiple information sources for clinical relevance but focus narrowly on variant interpretation. (4) Multi-omic models integrate across modalities for holistic understanding but face data engineering complexity and are still early in development.\nEmergent capabilities: These are abilities that appear discontinuously at certain scale thresholds and are absent in smaller models (e.g., ESM-2’s structural understanding emerges at ~650M parameters, zero-shot variant effect prediction requires sufficient scale). This matters because attempting to fine-tune a model below the capability threshold is wasted effort—if a 100M parameter model fundamentally cannot perform a task, no amount of fine-tuning will enable it. You must verify that models of similar scale have demonstrated the required capability before attempting adaptation.\nBuild vs. use hierarchy: Start with frozen embeddings from existing models (hours of work, ~$10 compute, 70-90% of fine-tuned performance). Escalate to parameter-efficient adaptation like LoRA if embeddings underperform (days of work, $100-1000 compute, ~95% of full fine-tuning). Consider full fine-tuning only for high-stakes applications (weeks, $1000+). Train from scratch only when: (a) target domain differs fundamentally from existing pretraining data, (b) you have unique large-scale data, (c) timeline permits months, and (d) budget permits $100K+ compute. Most researchers never need to train from scratch.\n\n\n\n\n\n\nCore Concepts:\n\nFoundation models are distinguished from task-specific models by their ability to transfer to diverse downstream tasks through embeddings or lightweight adaptation, not by size alone.\nScaling laws (Chinchilla framework) describe predictable relationships between parameters, data, compute, and performance. For genomics, data constraints often matter more than compute constraints.\nEmergent capabilities appear at scale thresholds—models below these thresholds cannot achieve certain capabilities regardless of fine-tuning.\nFour model families serve different needs: DNA language models (general embeddings), sequence-to-function models (assay predictions), variant effect models (clinical interpretation), and multi-omic models (cross-modal integration).\nBuild-vs-use decisions follow a clear hierarchy: start with frozen embeddings, escalate to adaptation if needed, train from scratch only for genuinely novel domains.\n\nKey Takeaways:\n\nThe paradigm shift from task-specific to foundation models changes how researchers interact with models—from training practitioners to adaptation specialists.\nFor most applications, using or adapting existing foundation models is more efficient than training from scratch.\nEvaluation must span multiple tasks; single-benchmark performance does not capture foundation model value.\nThe path from research to clinical deployment requires addressing uncertainty, interpretability, and regulatory requirements beyond benchmark performance.\n\nLooking Ahead: The next chapters examine each foundation model family in depth: DNA language models (Chapter 14), protein language models (Chapter 15), regulatory sequence-to-function models (Chapter 16), and variant effect prediction (Chapter 17).\n\n\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2203.15556.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2106.09685.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLi, Zehui, Vallijah Subasri, Yifei Shen, Dongsheng Li, Yiren Zhao, Guy-Bart Stan, and Caihua Shan. 2025. “Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning.” arXiv. https://doi.org/10.48550/arXiv.2502.03499.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA] Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model.” Nature Methods 12 (10): 931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Foundation Model Paradigm</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html",
    "href": "part_3/p3-ch14-dna-lm.html",
    "title": "14  DNA Language Models",
    "section": "",
    "text": "14.1 From Task-Specific CNNs to General-Purpose Language Models\nThe convolutional neural networks examined in Chapter 6 achieved strong performance on specific genomic prediction tasks. They faced, however, the feature ceiling limitation discussed in Section 4.6.4: performance bounded by what architectural choices and training data could capture. DeepSEA predicted chromatin marks from sequence; SpliceAI identified splice junctions with clinical utility; ExPecto estimated expression effects of variants. Each model was engineered for its particular application, with architectural choices (filter sizes, dilation patterns, pooling strategies) optimized for the task at hand.\nThis paradigm succeeded but imposed three constraints that limited scalability. Every new assay, cell type, or phenotype required fresh labeled data; a model trained on ENCODE chromatin data could not predict histone modifications in a new cell type without additional labeled examples. Model architecture was bound to specific prediction problems: SpliceAI’s dilated convolutions were tailored for splice junction detection, and ExPecto’s spatial transformation was designed for the distance-dependent relationship between regulatory elements and transcription start sites. These architectural choices, while effective, did not transfer naturally to other problems. Features learned for one task could not easily support others; a model that learned to recognize transcription factor binding sites during chromatin accessibility training could not directly apply those representations to variant effect prediction without substantial re-engineering.\nProtein language models demonstrated an alternative. ESM and related models trained on massive corpora of protein sequences using masked language modeling (predicting held-out amino acids from context) or autoregressive objectives (predicting the next amino acid). The resulting representations transferred to structure prediction, function annotation, and variant effect scoring without architecture changes (Chapter 15). DNA language models import this recipe: pretrain on large collections of genomic sequences using self-supervised objectives, then adapt the learned representations to downstream tasks through probing, fine-tuning, or zero-shot scoring.\nThe practical workflow begins with training a language model on unlabeled genomic sequences to predict masked or subsequent nucleotides. From the trained model, embeddings are extracted for sequences of interest (windows around variants, regulatory elements, or entire genes). These embeddings then support downstream tasks through probing with lightweight classifiers, fine-tuning for specific applications, or zero-shot scoring via probability comparisons. Once a sufficiently powerful backbone exists, it becomes the default starting point for nearly any DNA-level prediction problem.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-task-specific-to-general",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-task-specific-to-general",
    "title": "14  DNA Language Models",
    "section": "",
    "text": "Stop and Think\n\n\n\nBefore reading about the limitations of task-specific CNNs, consider: if you trained a model specifically to predict chromatin accessibility in one cell type, what challenges might you face when applying it to a new cell type or a different prediction task?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nYou would face three key challenges: (1) Data dependency - you’d need new labeled chromatin accessibility data for the new cell type, which is expensive and may not be available; (2) Architecture mismatch - the model’s architecture was optimized for chromatin accessibility, so adapting it to a different task (like splice prediction) would require substantial re-engineering; (3) Feature transfer failure - patterns learned for one task (chromatin marks) don’t directly transfer to unrelated tasks without retraining. This is precisely why the foundation model paradigm emerged: learn general representations once, then adapt to many tasks.\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of DNA language models from 2021-2025\n\n\n\n\nFigure 14.1: Evolution of DNA language models from 2021-2025. The timeline traces key milestones in architectural capability. DNABERT (2021) demonstrated proof-of-concept with 512-token contexts using k-mer tokenization. Nucleotide Transformer (2023) scaled to 2.5 billion parameters with 6kb context and multi-species pretraining. HyenaDNA (2023) broke through the quadratic attention barrier, achieving 1 megabase context through sub-quadratic Hyena operators. Caduceus (2024) introduced reverse-complement equivariance through bidirectional Mamba architectures. Evo 2 (2024-2025) scaled to 40 billion parameters with million-base contexts, enabling pan-genomic understanding and sequence generation. Upper track shows exponential growth in context length; lower track highlights architectural innovations enabling each advance.\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive: Self-Supervised Learning\n\n\n\nFor biology readers: Self-supervised learning creates training labels from the data itself, without manual annotation:\nThe insight: Instead of requiring expensive labeled data (e.g., “this variant is pathogenic”), self-supervised learning generates labels automatically from raw data.\nTwo main strategies for sequence models:\nMasked Language Modeling (MLM): Hide some tokens, predict them from context.\n\nInput: “The CTCF motif [MASK] gene expression”\nTarget: predict the masked word\nGenomic version: mask nucleotides, predict from surrounding sequence\n\nAutoregressive (Next-Token Prediction): Predict each token from all previous tokens.\n\nGiven: “ACGT”\nPredict: the next nucleotide\nUsed by GPT-style models\n\nWhy it works:\n\nCreates unlimited training data from unlabeled sequences\nForces model to learn statistical patterns that capture biological structure\nPositions with strong predictions = evolutionarily constrained positions\nPatterns useful for masked prediction transfer to other tasks\n\nThe key insight: Predicting masked nucleotides requires understanding what patterns are “allowed” in genomic sequence—which is exactly what determines variant effects.\n\n\n\n\n\n\n\n\nKey Insight: The Foundation Model Paradigm Shift\n\n\n\nThe core shift from task-specific CNNs to DNA language models is this: instead of building specialized architectures for each task, train a single model to understand DNA sequence through self-supervision, then adapt that understanding to any downstream application. This inverts the traditional workflow from task-first (design architecture for task, train from scratch) to representation-first (learn general representations, adapt to tasks).",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-dnabert",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-dnabert",
    "title": "14  DNA Language Models",
    "section": "14.2 DNABERT: The First DNA Language Model",
    "text": "14.2 DNABERT: The First DNA Language Model\nDNABERT applied the BERT masked language modeling framework to genomic sequences, establishing proof of concept for DNA self-supervision (Ji et al. 2021). The model used overlapping k-mers (typically 6-mers) as tokens, creating a vocabulary of 4,096 tokens from the \\(4^6\\) possible hexamers. This tokenization strategy, detailed in Section 5.2, provided computational efficiency at the cost of positional ambiguity for variants. Training on the human reference genome, DNABERT learned to predict masked tokens from surrounding context using the standard BERT architecture.\nThe design choices reflected computational constraints of the time. The \\(k\\)-mer tokenization provided some sequence compression compared to single-nucleotide representations, but the overlapping nature (each nucleotide participates in multiple adjacent k-mers) meant the compression was modest and created ambiguity about precise variant positions. Context windows were limited to 512 tokens, corresponding to a few hundred base pairs of genomic sequence. The standard transformer architecture with quadratic attention complexity made longer contexts computationally prohibitive, a limitation examined in Chapter 7 and resolved by the architectural innovations in Section 14.5.1 and Section 14.5.2.\nDespite these limitations, DNABERT demonstrated several important principles. Fine-tuning on downstream tasks (promoter classification, splice site prediction, transcription factor binding site identification) achieved competitive performance with task-specific models trained from scratch. Learned embeddings captured biologically meaningful patterns, with similar sequences clustering together in embedding space even when trained only on the reference genome. The BERT-style architecture could be reused across multiple tasks with modest adaptation.\nDNABERT-2 addressed the tokenization limitations through improved approaches including BPE-style token merging that better compressed repetitive sequences (Zhou et al. 2024). The resulting model could represent longer genomic contexts within the same number of tokens, improving computational efficiency. On standardized benchmarks spanning sequence classification, regulatory element prediction, and variant effect scoring (Chapter 11), DNABERT-2 achieved consistent gains over both the original DNABERT and non-pretrained baselines. These improvements validated the importance of thoughtful tokenization design for genomic applications (see Chapter 5 for detailed discussion of tokenization strategies).\nThe DNABERT family collectively established that self-supervision on DNA works, that tokenization choices substantially affect performance, and that masked language model training produces reusable representations for diverse sequence tasks. The foundation model paradigm transfers effectively from natural language to genomic sequence.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-nucleotide-transformer",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-nucleotide-transformer",
    "title": "14  DNA Language Models",
    "section": "14.3 Nucleotide Transformer: Scaling Data and Model Diversity",
    "text": "14.3 Nucleotide Transformer: Scaling Data and Model Diversity\nDNABERT demonstrated feasibility but operated at modest scale relative to the size of genomes. The Nucleotide Transformer family pushed substantially further, emphasizing diversity in both training data and model architecture (Dalla-Torre et al. 2023).\nThe training corpus spanned genomic data from multiple species and human populations, exposing models to diverse sequence patterns, different regulatory architectures, and evolutionary constraints recurring across lineages. This cross-species pretraining mirrors the use of large multi-species alignments in protein language models but operates directly on raw DNA without explicit alignment. Context length expanded to approximately 6 kb per input sequence, representing an order-of-magnitude increase over DNABERT while still using dense transformer attention. The training objective remained masked language modeling on subsequences sampled from genomes.\nThe Nucleotide Transformer project introduced a benchmark panel that has become a standard yardstick for evaluating DNA language models. Tasks include promoter and enhancer classification, histone mark and chromatin accessibility prediction, splice site identification, and regulatory element type classification. Models are evaluated through linear probes or light fine-tuning on standardized train/validation/test splits. This benchmark infrastructure enabled systematic comparison across models and established the evaluation protocols now used throughout the field (see Chapter 11 for comprehensive discussion of genomic benchmarks).\nScaling experiments revealed predictable relationships between model size, training data, and performance. Larger models with more pretraining data and longer context windows achieved better downstream performance, following patterns observed in natural language and protein modeling. These scaling trends suggest that continued investment in larger genomic language models will yield further improvements, though the optimal allocation between parameters, data, and compute remains an active research question (Chapter 13).",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-gpn",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-gpn",
    "title": "14  DNA Language Models",
    "section": "14.4 GPN: Cross-Species Pretraining for Variant Effect Prediction",
    "text": "14.4 GPN: Cross-Species Pretraining for Variant Effect Prediction\nWhile the Nucleotide Transformer demonstrated the value of scaling, the Genomic Pretrained Network (GPN) explored a complementary direction: what can be learned from cross-species pretraining on relatively small, well-annotated genomes (Benegas, Batra, and Song 2023). Rather than scaling to maximum size, GPN asked whether self-supervision could yield useful variant effect predictors even in constrained settings.\nGPN was trained on unaligned reference genomes from Arabidopsis thaliana and seven related species within the Brassicales order using masked language modeling. Despite this modest training corpus, analysis revealed emergent encoding of gene structure (exon-intron boundaries, splice sites) and DNA sequence motifs (transcription factor binding patterns) without explicit supervision. The model discovered these patterns purely from statistical regularities of genomic sequence across related species.\n\n\n\n\n\n\nStop and Think\n\n\n\nGPN learns from only eight plant genomes, yet it outperforms conservation scores derived from alignments across dozens of species. What might explain this surprising result? Consider what information is preserved versus lost in multiple sequence alignment.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nMultiple sequence alignment preserves only positions where sequences can be reliably aligned, discarding information from insertion/deletion-rich regions and structural variants. GPN learns from unaligned sequences, capturing patterns in these regions that alignment-based methods miss. Additionally, GPN learns higher-order sequence patterns (motif combinations, spacing constraints, regulatory grammar) that phyloP and phastCons cannot represent—they only measure single-position conservation. The self-supervised objective forces GPN to predict nucleotides from context, requiring it to learn the sequence rules that determine “allowed” versus “disallowed” patterns. This is precisely the information needed for variant effect prediction.\n\n\n\n\n\nFor variant effect prediction, GPN used a likelihood ratio approach. Given reference and alternate alleles at a position, the model computes the log-likelihood of each under the learned sequence distribution. Variants that substantially reduce sequence likelihood (relative to the reference) are inferred to be more disruptive. This scoring strategy exploits the fact that constrained positions should have confident predictions for the reference allele, while unconstrained positions allow more flexibility.\n\n\n\n\n\n\nWorked Example: Zero-Shot Variant Scoring\n\n\n\nConsider scoring a variant at position 1000 in a gene. The workflow:\n\nExtract context window: Take the sequence from positions 500-1500 (1 kb centered on variant)\nCompute reference likelihood: Feed sequence with reference allele (e.g., A) to model; record log-probability \\(\\log P(\\text{seq}|\\text{A})\\)\nCompute alternate likelihood: Feed sequence with alternate allele (e.g., G) to model; record log-probability \\(\\log P(\\text{seq}|\\text{G})\\)\nCalculate likelihood ratio: \\(\\Delta = \\log P(\\text{seq}|\\text{ref}) - \\log P(\\text{seq}|\\text{alt})\\)\nInterpret: Positive \\(\\Delta\\) means alternate reduces sequence likelihood, suggesting disruption. Larger values indicate stronger constraint violation.\n\nThis approach requires no variant-specific training data and works for any position the model can process.\n\n\nEvaluated on A. thaliana variants using allele frequencies from the 1001 Genomes Project, GPN outperformed traditional conservation scores including phyloP and phastCons (Benegas, Batra, and Song 2023). This was notable because phyloP and phastCons require explicit multiple sequence alignments and evolutionary models, while GPN learned its representations from unaligned sequences through self-supervision alone. The later GPN-MSA extended this approach to mammalian genomes by incorporating multi-species alignments, achieving strong performance on human variant benchmarks (?sec-ch14-dna-lm-vep). The success of this approach informed subsequent development of zero-shot variant scoring methods for clinical applications (?sec-ch26-fm-scoring).\nGPN established several important principles. Cross-species pretraining captures evolutionary constraints transferable to variant effect prediction. Relatively small models trained on focused phylogenetic groups can outperform larger generic conservation measures within that group. The masked language modeling objective naturally produces representations suitable for variant scoring via likelihood comparisons.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-long-context",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-long-context",
    "title": "14  DNA Language Models",
    "section": "14.5 Long-Context Revolution",
    "text": "14.5 Long-Context Revolution\n\n\n\n\n\n\nMathematical Content Ahead\n\n\n\nThis section discusses computational complexity notation (O notation) for comparing algorithms. If you are not familiar with this notation: \\(O(L^2)\\) means computational cost grows with the square of sequence length \\(L\\), while \\(O(L)\\) or \\(O(L \\log L)\\) means cost grows linearly or near-linearly. The key takeaway is that quadratic scaling makes long sequences impractical, while linear scaling enables megabase processing.\n\n\nQuadratic attention complexity limits transformer context to tens of kilobases at best. Processing a 100 kb sequence with dense attention requires on the order of \\(10^{10}\\) computations per layer. Yet regulatory phenomena routinely span larger distances: enhancer-promoter interactions extend 50-200 kb, topologically associating domains organize chromatin at the megabase scale, and some gene regulation involves even longer-range dependencies. The three-dimensional organization of chromatin that enables these long-range contacts is examined in Chapter 20; here we focus on how linear sequence models can capture information about these interactions. The mismatch between biological context and computational context represented a fundamental architectural limitation.\n\n\n\n\n\n\n\n\nThe quadratic attention bottleneck limits standard transformers\n\n\n\n\n\n\n\nSub-quadratic architectures enable million-base contexts\n\n\n\n\n\n\nFigure 14.2: Breaking the attention bottleneck for genomic modeling. (A) Standard self-attention requires computing all pairwise interactions between positions, creating quadratic O(L²) memory and compute requirements. This limits practical contexts to approximately 10-50 kilobases, falling short of enhancer-promoter distances (50-100kb) and topologically associating domain scales (~1Mb). (B) Sub-quadratic architectures achieve longer contexts through different strategies: Hyena uses learnable long convolutions (O(L log L)); Mamba employs selective state-space models with linear scaling (O(L)); linear attention approximates the attention mechanism with kernel methods (O(L)). These innovations enabled DNA language models to process million-base contexts, accessing biological relationships invisible to shorter-context models.\n\n\n\n\n14.5.1 HyenaDNA: Megabase Context via Implicit Convolutions\nHyenaDNA addressed this limitation by replacing attention with implicit convolutions that scale sub-quadratically (Nguyen et al. 2023). The Hyena architecture parameterizes long convolutional filters through neural networks rather than storing explicit filter weights, achieving \\(O(L \\log L)\\) complexity compared to \\(O(L^2)\\) for standard attention. The result was a 500-fold increase in context length: HyenaDNA processes sequences up to 1 Mb while maintaining single-nucleotide resolution.\nWhy does this architectural change achieve sub-quadratic complexity? Standard attention requires computing and storing an \\(L \\times L\\) matrix because every position’s output explicitly depends on pairwise interactions with every other position. Hyena sidesteps this requirement entirely. Instead of computing explicit pairwise interactions, Hyena learns a continuous filter function parameterized by a small neural network that can be evaluated at any relative position. This filter is applied to the sequence via convolution, which can be computed efficiently using Fast Fourier Transform (FFT) in \\(O(L \\log L)\\) time—the same mathematical trick that enables efficient signal processing and image filtering. The key insight is that convolution in the spatial domain equals multiplication in the frequency domain, so we transform to frequency space, multiply, and transform back, avoiding the quadratic pairwise computation altogether. The tradeoff is expressiveness: attention’s content-dependent routing (where which positions interact depends on what the sequence contains) is replaced by position-dependent filtering (where interactions depend on relative distance but not content). For many genomic applications where positional relationships matter as much as content similarity, this tradeoff proves acceptable.\nProcessing megabase-scale windows allows the model to capture entire gene bodies plus flanking regulatory regions, long-range enhancer-promoter interactions, and topologically associating domain structure. Despite the long context, single-nucleotide tokens preserve maximum resolution for variant effect prediction. Each nucleotide is independently represented without the ambiguity introduced by \\(k\\)-mer tokenization.\nOn Nucleotide Transformer benchmarks, HyenaDNA achieved state-of-the-art results on the majority of tasks with orders of magnitude fewer parameters. On GenomicBenchmarks, it surpassed prior state-of-the-art on seven of eight datasets (Nguyen et al. 2023). HyenaDNA also demonstrated in-context learning in genomics: performance improved when examples were included in the input context without updating model weights. This capability, familiar from large language models, had not previously been observed for genomic sequences and suggests that sufficient context length combined with appropriate architecture enables qualitatively new forms of biological reasoning.\n\n\n14.5.2 Caduceus: Bidirectional Processing with Reverse-Complement Equivariance\nDNA is double-stranded, and any sequence can be read from either strand. The reverse complement of a sequence encodes the same information from the opposite strand’s perspective. For many biological processes, predictions should be identical or related consistently regardless of which strand is presented. Standard neural networks can produce divergent predictions for a sequence and its reverse complement, even with data augmentation during training.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nConsider a model predicting transcription factor binding. A binding site on the forward strand (5’-GAATTC-3’) has reverse complement (5’-GAATTC-3’ on the reverse strand). Should a model’s prediction differ based on which strand you query? Why or why not? What about for genes, which have defined orientations?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nFor transcription factor binding sites, predictions should be identical for forward and reverse complement sequences because the binding site represents the same functional element on either strand. However, for genes with defined orientations (coding sequences, promoters), strand matters biologically, so predictions can differ. The key insight is that strand symmetry applies to motifs without inherent directionality, but not to oriented features like genes where 5’ to 3’ direction carries functional meaning.\n\n\n\n\n\nCaduceus addressed this challenge by building reverse-complement equivariance directly into the architecture (Schiff et al. 2024). The model extends the Mamba state space architecture (which achieves O(L) complexity) to support both bidirectional processing and strand equivariance. The BiMamba component enables information flow in both directions along the sequence, while the MambaDNA block ensures mathematically related predictions for sequences and their reverse complements.\nOn downstream benchmarks, Caduceus outperformed previous long-range models. On challenging long-range variant effect prediction tasks, it exceeded models with ten times as many parameters that lacked bidirectionality or equivariance (Schiff et al. 2024). The key insight was that incorporating appropriate biological inductive biases can substitute for raw scale. Strand symmetry is a known property of DNA; building it into the architecture avoids wasting model capacity learning what could be specified directly.\n\n\n\n\n\n\nKey Insight: Inductive Biases vs. Scale\n\n\n\nCaduceus demonstrates a fundamental principle: when you know something about your domain, encode it in the architecture rather than hoping the model learns it from data. Strand symmetry is mathematically specifiable; building it in yields better performance with fewer parameters than training a larger model to approximate it. This principle applies broadly: whenever you have domain knowledge that can be expressed architecturally, doing so typically improves efficiency and generalization.\n\n\n\n\n14.5.3 Evo 2: Genome-Scale Modeling Across the Tree of Life\nThe original Evo model demonstrated that DNA language models could operate at unprecedented scale (Nguyen et al. 2024). Trained on 2.7 million prokaryotic and phage genomes comprising 300 billion nucleotides, Evo processed sequences up to 131 kilobases using the StripedHyena architecture, a hybrid design combining state-space models with attention mechanisms. The 7 billion parameter model exhibited emergent biological understanding: predicting gene essentiality, identifying functional elements, and generating synthetic sequences with plausible biological properties. Evo demonstrated that training on raw DNA sequence alone, without annotation, could yield models that captured fundamental aspects of genome organization.\nEvo 2 extends this foundation to the entire tree of life (Brixi et al. 2025). Where Evo focused primarily on prokaryotes and phages, Evo 2 incorporates eukaryotic genomes with their dramatically different organization: extensive noncoding regions, complex regulatory architectures, and intronic sequences that comprise the majority of gene length in many species. This expansion required both larger models and longer context windows to capture the sprawling regulatory landscapes characteristic of eukaryotic genomes.\n\n\n\n\n\n\n\n\nDNA reverse complement equivalence\n\n\n\n\n\n\n\nStandard models break RC equivalence\n\n\n\n\n\n\n\nCaduceus architecture guarantees equivariance\n\n\n\n\n\n\nFigure 14.3: Reverse complement equivariance as architectural constraint. (A) DNA’s double-stranded nature creates biological equivalence: a regulatory element on one strand has the same function when read as its reverse complement from the other strand. (B) Standard models violate this equivalence, producing different predictions for forward and reverse complement sequences despite their biological identity. (C) Caduceus uses bidirectional Mamba state-space models with equivariant architecture to guarantee identical predictions for equivalent sequences. This architectural constraint eliminates strand bias without data augmentation, improving both biological correctness and data efficiency.\n\n\n\nThe training corpus draws from the OpenGenome2 dataset comprising 9.3 trillion DNA tokens across all domains of life, a 30-fold increase over the original Evo training data. This massive scale exposes the model to the full spectrum of genomic organization: compact prokaryotic gene arrangements, sprawling eukaryotic regulatory landscapes with extensive noncoding sequence, viral genomes with overlapping reading frames, and the diversity of regulatory architectures across evolution. The model comes in 7 billion and 40 billion parameter variants, with the larger model extending well beyond the original Evo’s scale.\nThe architecture builds on StripedHyena 2, refining the hybrid design that proved effective in the original Evo. The combination of convolutional operations with selective attention mechanisms enables processing of sequences up to 1 million nucleotides, nearly an order of magnitude beyond Evo’s 131 kilobase context. Like its predecessor, Evo 2 uses an autoregressive training objective (predicting the next base given all previous bases), which differs from the masked language modeling used in DNABERT and related models. Autoregressive training may provide complementary strengths for sequence generation and likelihood-based scoring, since the model learns to generate plausible sequences in addition to discriminating between them (Chapter 8).\nEvo 2 exhibits several forms of emergent biological knowledge despite training only on raw sequence, extending the capabilities first observed in Evo. The model learns to identify exon-intron boundaries without explicit annotation, identifies transcription factor binding site patterns matching known motifs, captures aspects of protein secondary and tertiary structure when processing coding sequences, and identifies prophage insertion regions in bacterial genomes. Where Evo demonstrated these capabilities primarily in prokaryotic contexts, Evo 2 generalizes them across eukaryotic genomes with their more complex gene structures.\nFor variant effect prediction, Evo 2 enables zero-shot scoring through likelihood ratios. Variants can be scored for consistency with learned genomic patterns by comparing model probabilities for reference versus alternate sequences. On benchmarks of pathogenic versus benign variants, zero-shot scores achieve competitive performance with specialized supervised methods, though calibration remains necessary before clinical application (?sec-ch14-dna-lm-vep). The calibration methods required for clinical deployment are examined in Section 23.2, and integration into diagnostic workflows in ?sec-ch26-fm-scoring. The model also supports classification of variants of uncertain significance through simple classifiers trained on its embeddings.\nThe pan-species training enables cross-species applications that extend Evo’s prokaryotic focus to the full breadth of biology. Variant interpretation extends naturally to non-model organisms, supporting conservation genomics and agricultural breeding where labeled training data is scarce. Model representations cluster sequences by phylogenetic relationships even without explicit evolutionary modeling. Beyond discriminative tasks, Evo 2 demonstrates generative capabilities building on Evo’s initial demonstrations: synthesizing plausible mitochondrial genomes, prokaryotic operons, and eukaryotic regulatory regions with coherence across kilobase to megabase scales.\n\n\n\n\n\n\nKnowledge Check: Predict Before You Look\n\n\n\nBefore viewing Table 14.1 below, test your understanding of the architectural tradeoffs:\n\nWhich models do you predict handle the longest context lengths?\nWhat architectural features enable processing beyond the quadratic attention bottleneck?\nWhich training objective (masked LM vs. next-token prediction) do you think is more common in long-context models?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nExpected predictions based on earlier discussion:\n\nLongest contexts: HyenaDNA and Evo 2 should handle 1 Mb contexts, since they use sub-quadratic architectures (Hyena operators and StripedHyena). Caduceus achieves 131 kb with linear-complexity Mamba. Standard transformers (DNABERT, Nucleotide Transformer) are limited to ≤6 kb due to quadratic attention.\nEnabling features: Sub-quadratic complexity is key. HyenaDNA uses implicit convolutions with O(L log L) complexity via FFT. Caduceus uses Mamba state-space models with O(L) complexity. Evo 2 uses hybrid StripedHyena combining both approaches.\nTraining objective: Long-context models tend to use next-token (autoregressive) prediction rather than masked LM. This is because autoregressive training naturally supports generation and may provide better gradients for learning long-range dependencies. Notice in the table: all sub-quadratic models use next-token prediction.\n\n\n\n\n\n\nThe following table highlights five landmark DNA language models representing distinct paradigms. The chapter discusses additional models including DNABERT-2 (BPE tokenization), GPN (cross-species VEP), and Evo (prokaryotic scale).\n\n\n\nTable 14.1: Landmark DNA language models representing distinct architectural paradigms.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nYear\nContext\nParameters\nArchitecture\nKey Innovation\n\n\n\n\nDNABERT\n2021\n~500 bp\n110M\nTransformer\nFirst DNA LM, masked pretraining\n\n\nNucleotide Transformer\n2023\n6 kb\n500M-2.5B\nTransformer\nMulti-species scaling\n\n\nHyenaDNA\n2023\n1 Mb\n1.6M-6.6M\nHyena\nSub-quadratic long context\n\n\nCaduceus\n2024\n131 kb\n1.6M-6.6M\nMamba\nRC-equivariance\n\n\nEvo 2\n2025\n1 Mb\n7B-40B\nStripedHyena 2\nPan-genomic, largest scale",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-training-data",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-training-data",
    "title": "14  DNA Language Models",
    "section": "14.6 Training Data and What Models Learn",
    "text": "14.6 Training Data and What Models Learn\nDNA language models are trained on diverse corpora ranging from single reference genomes to pan-genomic collections spanning the tree of life. Understanding what training data is used and what models learn from it is essential for anticipating model capabilities and limitations.\n\n14.6.1 Training Corpus Composition\nEarly models like DNABERT trained primarily on the human reference genome (GRCh38), providing exposure to approximately 3 billion nucleotides from a single individual. The Nucleotide Transformer expanded to include multiple species and human population variation from resources like the 1000 Genomes Project (Chapter 2). Evo 2 scaled to 9.3 trillion tokens spanning all domains of life, including complete bacterial chromosomes, eukaryotic genomes, viral sequences, and metagenomic assemblies.\n\n\n\n\n\n\n\n\nPan-genomic training data for Evo 2\n\n\n\n\n\n\n\nContext length curriculum for stable training\n\n\n\n\n\n\n\nEvo 2 generates biologically coherent sequences\n\n\n\n\n\n\nFigure 14.4: Evo 2 training regime for pan-genomic understanding and generation. (A) Training data spans 2.7 trillion nucleotides across all domains of life, enabling learning of universal and lineage-specific sequence patterns. (B) Context length curriculum progressively extends from 8k to 1M tokens, maintaining stable optimization through each transition. (C) The resulting model generates novel sequences with emergent biological coherence: appropriate gene structure, regulatory elements, and statistical properties matching natural genomes. This combination of massive scale, architectural innovation, and careful training regime enables both understanding and generation of genomic sequence.\n\n\n\nThe composition of training data shapes what models learn. Reference-only training captures the genome’s architecture but not population variation. Multi-individual training exposes models to common polymorphisms but may underrepresent rare variants. Cross-species training provides evolutionary context (constrained regions are conserved, variable regions diverge) but may not capture species-specific regulatory patterns. Training on functional genomics data (GROVER-style approaches) teaches regulatory activity patterns but ties models to specific assays and cell types.\nA tension exists between generality and specificity. Models trained on broader corpora learn more general representations that transfer across species and contexts, but may underperform narrower models on specific applications. Models trained on focused datasets may capture task-relevant patterns more effectively but transfer less well. The optimal training strategy depends on intended applications.\n\n\n14.6.2 Probing What Models Learn\n\n\n\n\n\n\nStop and Think\n\n\n\nIf a model was trained only to predict masked nucleotides (never seeing any labels), how could we determine whether it learned to recognize splice sites, transcription factor binding sites, or gene boundaries? What experiments would reveal this?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nUse linear probing: freeze the pretrained model and extract embeddings for sequences with known annotations (e.g., windows around splice sites versus random genomic positions). Train a simple linear classifier (logistic regression) on these frozen embeddings to predict the annotation. If the classifier achieves high accuracy, the embeddings must encode information distinguishing that feature—meaning the model learned to recognize it during pretraining. This works because if splice sites have different statistical patterns than non-splice sites, and the model learned to predict nucleotides accurately, it must have internalized what makes splice sites distinctive. The probing methodology is detailed in Section 9.3.3.\n\n\n\n\n\nLinear probing experiments reveal what information is encoded in model representations without task-specific fine-tuning. By training simple classifiers (logistic regression, single-layer perceptrons) on frozen embeddings to predict known annotations, researchers can assess whether models have learned biologically meaningful patterns. The methodology for such probing experiments is detailed in Section 9.3.3, with implications for interpretability examined in Section 24.4.\nDNA language models consistently learn to recognize several categories of genomic features. Models learn patterns corresponding to known transcription factor binding sites, splice signals, and other sequence motifs without explicit supervision; probing for specific motif presence shows that model embeddings can distinguish sequences containing binding sites from those lacking them. Representations also encode gene structure: models distinguish coding from noncoding regions, identify exon-intron boundaries, and recognize splice donor and acceptor sites. This knowledge emerges from sequence statistics alone, suggesting that the compositional and structural differences between genomic region types are learnable from DNA sequence.\nEvolutionary constraints are implicitly captured, particularly in models trained on multi-species data. Positions under purifying selection (constrained across evolution) show different embedding patterns than neutral positions. This provides a self-supervised analog to traditional conservation scoring, though the relationship between model-learned and alignment-based conservation measures varies across genomic contexts.\nMore complex patterns like regulatory grammar (the syntax governing how transcription factors combine to specify expression) show mixed evidence. Models capture some aspects of regulatory logic, such as the spacing preferences between binding sites, but may not fully represent the combinatorial complexity of enhancer function. Similarly, long-range dependencies (enhancer-promoter interactions across tens of kilobases) are accessible to long-context models but require extensive probing to assess whether they are actually leveraged.\n\n\n14.6.3 What Models Do Not Learn\n\n\n\n\n\n\nKnowledge Check: Connect to Earlier Material\n\n\n\nRecall the feature ceiling discussed in Section 4.6.4. How do the limitations of sequence-only DNA language models relate to that concept? Think about what information is available versus what is needed for complete predictions.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe feature ceiling occurs when model performance is bounded by what the input features can represent, not by model architecture or training data. DNA language models face a fundamental feature ceiling: they can only learn from sequence, but gene regulation depends on epigenetic state, 3D chromatin structure, and cellular context that sequence doesn’t encode. Even with infinite parameters and perfect optimization, a sequence-only model cannot predict cell-type-specific chromatin accessibility because the information simply isn’t present in DNA sequence alone. This is analogous to trying to predict splice patterns without knowing which tissues a gene is expressed in—the necessary information is missing. Breaking this ceiling requires multi-modal integration (Part IV) that incorporates functional genomics data beyond sequence.\n\n\n\n\n\nEqually important is recognizing what current DNA language models struggle to represent. Sequence-only models cannot capture epigenetic context: DNA methylation, histone modifications, and chromatin accessibility all affect gene regulation but are not encoded in primary sequence. Some models (like GROVER) address this by incorporating functional genomics data, but this ties them to specific cell types and experimental conditions.\nThe three-dimensional structure of chromatin affects which regulatory elements can physically interact, but linear sequence models cannot represent folding (Chapter 20). Cell-type specificity of gene regulation depends on transcription factor expression levels and chromatin state, not just sequence; models trained on sequence alone can predict potential regulatory activity but not its realization in specific contexts.\nComplex variant patterns beyond single nucleotide changes remain challenging. Indels, structural variants, repeat expansions, and epistatic interactions between distant loci are either not representable (depending on tokenization) or poorly predicted. Most benchmark tasks focus on SNVs, leaving multi-nucleotide effects underexplored.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore viewing Table 14.2 below, make predictions based on what you’ve learned:\n\nWhat categories of biological features can sequence-only models learn?\nWhat requires information beyond DNA sequence?\nWhere do you think the fundamental boundary lies between “learnable from sequence” and “requires additional context”?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nLearnable from sequence: Features encoded in DNA sequence itself—motifs (TF binding sites, splice signals), gene structure (exon boundaries, coding regions), and evolutionary constraint (patterns conserved across species). These emerge from statistical regularities in genomic sequence.\nRequires additional context: Features dependent on cellular state—epigenetic modifications (methylation, histone marks), 3D chromatin organization (which enhancers contact which promoters), and cell-type-specific activity (whether a motif is actually bound by TFs present in that cell).\nFundamental boundary: Sequence encodes potential (what could happen), but realization depends on context (what does happen in specific cells). A promoter sequence contains information about where transcription could initiate, but whether it actually does depends on transcription factor availability, chromatin accessibility, and regulatory signals—none of which are in the DNA sequence alone.\n\n\n\n\n\nThe following table summarizes what DNA language models can and cannot learn from sequence alone:\n\n\n\nTable 14.2: What DNA language models can and cannot learn from sequence alone. The fundamental limitation is that sequence encodes potential, not realization in specific cellular contexts.\n\n\n\n\n\n\n\n\n\n\nCategory\nCan Learn\nCannot Learn\n\n\n\n\nSequence motifs\nTF binding sites, splice signals, promoter elements\nCell-type-specific activity of motifs\n\n\nGene structure\nExon-intron boundaries, coding vs. noncoding\nAlternative splicing patterns in specific tissues\n\n\nEvolutionary constraint\nConservation patterns from cross-species training\nRecent selection not captured in training data\n\n\nRegulatory grammar\nSpacing preferences between motifs\nFull combinatorial logic of enhancers\n\n\nEpigenetic state\n—\nDNA methylation, histone modifications, chromatin accessibility\n\n\n3D structure\n—\nChromatin folding, enhancer-promoter contacts\n\n\nComplex variants\nSNVs (with limitations)\nIndels, structural variants, repeat expansions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDNA LMs encode genomic element identity\n\n\n\n\n\n\n\nModel attention correlates with conservation\n\n\n\n\n\n\n\n\n\nDNA LMs learn regulatory grammar\n\n\n\n\n\n\n\nProbing reveals fundamental limitations\n\n\n\n\n\n\nFigure 14.5: Probing analysis reveals what DNA language models learn—and what they cannot. (A) Frozen embeddings support high-accuracy linear classification of genomic element types including promoters, enhancers, and splice sites. (B) Model attention correlates with evolutionary conservation, suggesting learned focus on functionally important positions. (C) Attention patterns capture long-range regulatory relationships between enhancers and promoters. (D) However, probing fails for context-dependent properties: tissue-specific activity, environmental responses, and 3D chromatin organization cannot be predicted from sequence representations alone. DNA language models learn sequence potential; realizing that potential requires cellular context they cannot access.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-benchmarks",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-benchmarks",
    "title": "14  DNA Language Models",
    "section": "14.7 Benchmark Performance and Evaluation",
    "text": "14.7 Benchmark Performance and Evaluation\n\n\n\n\n\n\nSpaced Retrieval: Test Your Memory\n\n\n\nWithout looking back, try to recall from Section 14.1:\n\nWhat were the three key limitations of task-specific CNN models that motivated the shift to foundation models?\nWhat is the core difference between the old “task-first” workflow and the new “representation-first” paradigm?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThree limitations of task-specific CNNs:\n\nData dependency: Every new task required fresh labeled data; models couldn’t transfer knowledge\nArchitecture rigidity: Model architectures were tailored to specific tasks and didn’t generalize\nFeature isolation: Learned features didn’t transfer across tasks without re-engineering\n\nWorkflow shift:\n\nOld (task-first): Design specialized architecture for task → collect labeled data → train from scratch → repeat for each new task\nNew (representation-first): Pretrain general-purpose model on unlabeled sequences → extract reusable representations → adapt to any downstream task with minimal data\n\nThis paradigm shift is why DNA language models can support diverse applications (regulatory classification, variant scoring, splice prediction) without task-specific retraining.\n\n\n\n\n\nStandardized benchmarks enable systematic comparison across DNA language models, though each benchmark captures only part of what we care about. Understanding benchmark construction and limitations is essential for interpreting performance claims.\n\n14.7.1 Major Benchmark Suites\nThe BEND (Benchmark for Nucleotide Deep learning) suite provides a unified framework with tasks including gene finding, enhancer annotation, chromatin state prediction, and variant effect scoring (Marin et al. 2024). Standardized splits and metrics enable fair comparison. BEND specifically evaluates whether models capture biologically meaningful features at different resolution scales.\nGenomic Benchmarks focus on regulatory element classification tasks: distinguishing promoters from nonpromoters, identifying active enhancers, predicting histone mark presence (gresova_genomic-benchmarks_2023?). These tasks test whether model representations encode basic genomic annotations. Most current DNA language models achieve high accuracy on these tasks, suggesting benchmark saturation for simpler classification problems.\nThe Long Range Benchmark (LRB) and DNALongBench evaluate long-context modeling capabilities (Cheng et al. 2024). Tasks include predicting distal enhancer-promoter interactions, modeling chromatin structure across hundreds of kilobases, and integrating information over extended genomic windows. These benchmarks specifically test whether long-context architectures provide meaningful advantages over shorter-context models.\nComparative evaluations across model families reveal that no single architecture dominates all tasks (Manzo, Borkowski, and Ovcharenko 2025). Performance varies substantially depending on task characteristics (local motif recognition versus long-range integration), training data composition, and architectural choices. HyenaDNA and Caduceus excel on long-range tasks where their architectural innovations matter; DNABERT-2 and Nucleotide Transformer perform well on shorter-range regulatory classification; Evo 2 shows advantages on cross-species tasks and variant effect prediction.\n\n\n14.7.2 Benchmark Limitations\nSeveral systematic issues affect benchmark interpretation. Many benchmarks have reached saturation, where multiple models achieve near-perfect performance and discriminative power disappears. This has happened for simpler classification tasks in Genomic Benchmarks. Data leakage arises when training and test sequences share homology, allowing models to succeed through memorization rather than generalization; the homology-aware splitting strategies required to prevent this are detailed in Section 12.2. Careful sequence clustering (using tools like MMseqs2 or CD-HIT) is required, but many older benchmarks lack rigorous split design. The comprehensive treatment of benchmark construction and evaluation methodology appears in Chapter 11 and Chapter 12.\nDistribution shift between benchmark data and real-world applications means strong benchmark performance may not predict deployment success. Most benchmarks derive from well-studied regions of well-characterized genomes; performance on understudied regions, rare variants, or non-European populations may differ substantially. The systematic treatment of such confounding factors appears in Chapter 12, with specific attention to ancestry-related performance disparities in ?sec-ch22-ancestry-confounding.\nThe choice of evaluation metric affects what gets optimized. auROC favors discrimination regardless of calibration; Spearman correlation measures rank ordering but not absolute effect size prediction. Clinical applications may require well-calibrated probability estimates or accurate quantitative predictions, neither of which standard metrics directly assess (Chapter 23). The gap between benchmark performance and deployment utility remains substantial for most genomic applications.\n\n\n\n\n\n\nBenchmark comparison across DNA language models\n\n\n\n\nFigure 14.6: Benchmark comparison across DNA language models. Heatmap shows relative performance on standardized genomic prediction tasks. Larger, newer models generally achieve higher performance, with particularly strong gains on tasks requiring long-range context (enhancer activity, gene expression) where architectural innovations enabling million-base contexts provide clear advantages. Performance on local tasks (splice prediction, promoter classification) is more saturated, with smaller models approaching larger model performance. These benchmarks primarily test frozen embedding quality; fine-tuning would yield different relative rankings.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-annotation-aware",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-annotation-aware",
    "title": "14  DNA Language Models",
    "section": "14.8 Annotation-Aware Extensions",
    "text": "14.8 Annotation-Aware Extensions\nRecent work explores enriching DNA language models with explicit biological structure beyond raw sequence. These approaches represent early steps toward multi-modal genomic foundation models.\nLife-Code proposes central-dogma-informed tokenization, treating coding and noncoding regions differently (Liu et al. 2025). Coding regions use codon tokens (three-nucleotide units specifying amino acids), respecting the genetic code’s fundamental structure. Noncoding regions use learned subword units optimized during training. Knowledge distillation from protein language models imports protein-level structural knowledge into DNA representations. Life-Code achieves competitive results across DNA, RNA, and protein tasks, suggesting that encoding biological structure into tokenization provides useful inductive bias (Chapter 5).\nBioToken extends tokenization to include explicit genomic annotations (Medvedev et al. 2025). Rather than representing regions purely as nucleotide strings, BioToken creates composite tokens encoding sequence content, variant presence, structural annotations (exon, intron, UTR), and functional context. The associated BioFM model achieves state-of-the-art performance across genomic benchmarks with substantially fewer parameters (265M), suggesting that annotation-aware representations improve parameter efficiency.\nThese approaches foreshadow the multi-modal foundation models discussed in Part IV (Chapter 22), where sequence is only one of many integrated information streams.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-practical-use",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-practical-use",
    "title": "14  DNA Language Models",
    "section": "14.9 Using DNA Language Models in Practice",
    "text": "14.9 Using DNA Language Models in Practice\n\n\n\n\n\n\nPractical Guidance: Choosing a Usage Pattern\n\n\n\nThe right approach depends on your data and computational constraints:\n\nLittle/no labeled data: Use zero-shot scoring or few-shot in-context learning\nModerate labeled data (hundreds to thousands of examples): Extract embeddings, train lightweight downstream classifier\nSubstantial labeled data (thousands+): Fine-tune with LoRA or full fine-tuning\nComputational constraints: Prefer embedding extraction (frozen model, one forward pass per sequence)\nMaximum performance: Full fine-tuning with sufficient data\n\n\n\nDNA language models support multiple usage patterns for different applications.\n\n14.9.1 Embeddings as Universal Features\nThe simplest approach extracts embeddings from a pretrained model and uses them as features for downstream classifiers. The workflow involves extracting embeddings for windows around loci of interest, pooling or selecting positions relevant to the task, and training lightweight downstream models (linear layers, shallow MLPs, gradient boosting) on the extracted features.\nThis approach supports diverse applications. Regulatory element classification distinguishes promoters, enhancers, silencers, and insulators based on learned representations. Chromatin state prediction uses sequence embeddings to predict ATAC-seq or histone mark presence. Variant effect scoring replaces or augments hand-crafted features in frameworks like CADD with language model features (analogous to CADD v1.7’s incorporation of protein language model features, as discussed in Chapter 4). The integration of these features into comprehensive variant effect prediction workflows is detailed in ?sec-ch14-combining-evidence. Splicing analysis combines embeddings with specialized architectures.\nBecause the language model remains frozen, this approach is computationally efficient and avoids catastrophic forgetting when new tasks are added. The pretrained model serves as a general-purpose feature extractor supporting many downstream applications.\n\n\n14.9.2 Fine-Tuning and Adaptation\nWhen sufficient labeled data exists, fine-tuning typically outperforms frozen embedding approaches (Chapter 9). Updating all language model parameters for a specific task allows representations to specialize, achieving highest performance but requiring more compute and risking catastrophic forgetting of general knowledge.\nParameter-efficient methods like LoRA (Low-Rank Adaptation) offer a middle path, inserting small trainable modules into each layer while keeping the backbone mostly frozen (Hu et al. 2021). These approaches achieve most of the performance gains of full fine-tuning while maintaining computational efficiency and preserving general capabilities. Adapter-based methods similarly add small bottleneck modules tuned for specific tasks.\n\n\n14.9.3 Zero-Shot and Few-Shot Scoring\nFor variant interpretation, language models enable zero-shot scoring based on sequence likelihood. Compute the model’s probability for a sequence containing the reference allele, compare to probability for the sequence with the alternative allele, and interpret variants reducing probability as more disruptive. This approach requires no variant-specific training and can score any single-nucleotide variant the model can represent.\nZero-shot scoring quality depends on how well the model’s learned distribution captures biological constraints. Performance tends to improve with model scale and training data diversity (Chapter 13). Few-shot approaches include task examples in the input context, allowing in-context learning without parameter updates. HyenaDNA demonstrated this capability for genomic tasks, suggesting that sufficiently large models with long context can adapt through prompts rather than training.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-open-challenges",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-open-challenges",
    "title": "14  DNA Language Models",
    "section": "14.10 Limitations and Open Challenges",
    "text": "14.10 Limitations and Open Challenges\nDespite substantial progress, DNA language models face several fundamental limitations.\nThe tradeoff between context length and representational fidelity persists. Long-context models like HyenaDNA and Evo 2 can process megabase sequences but require efficient architectures that may not capture all the relationships dense attention would learn. Whether these architectural tradeoffs matter for specific applications remains task-dependent.\nMost tokenization schemes represent insertions and deletions awkwardly or not at all. Structural variants spanning kilobases, repeat expansions, and complex rearrangements fall outside what current models can process (Chapter 5). Epistatic interactions between variants at distant loci are not captured even by long-context models trained solely on single sequences.\nTraining data composition shapes model capabilities in underexplored ways. Models trained primarily on European-ancestry genomes may perform poorly on variants common in other populations (Chapter 12). Ascertainment bias in training databases (enrichment for coding regions, well-studied genes, specific diseases) propagates to learned representations. The field lacks systematic evaluation of performance disparities across populations.\nInterpretability remains limited (Chapter 24). While probing studies reveal what models encode, explaining why a specific variant receives a particular score in terms connecting to biological mechanism is difficult. Attention patterns and gradient-based attribution provide some insight but often fail to identify the specific sequence features driving predictions.\nIntegration with other modalities is nascent. DNA sequence provides necessary but insufficient information for predicting gene regulation. Epigenomic state, three-dimensional chromatin structure, transcription factor concentrations, and cellular context all matter. Current DNA language models cannot represent these factors; multi-modal approaches (discussed in Part IV) aim to address this limitation.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch14-dna-lm.html#sec-ch14-soft-landing",
    "href": "part_3/p3-ch14-dna-lm.html#sec-ch14-soft-landing",
    "title": "14  DNA Language Models",
    "section": "14.11 Representations Without Predictions",
    "text": "14.11 Representations Without Predictions\nDNA language models capture sequence patterns, regulatory motifs, and evolutionary constraints through self-supervised pretraining on genomic sequence. The progression from early proof-of-concept models through architectural innovations enabling megabase context demonstrates that the paradigm works: models trained to predict masked nucleotides learn representations that transfer across diverse downstream tasks. Biological inductive biases (strand symmetry, codon structure, cross-species training) can substitute for raw scale on appropriate tasks, creating opportunities for efficient models that encode domain knowledge.\nYet DNA language models have inherent limitations. They produce representations, not predictions. A language model can embed a sequence in a space where similar regulatory elements cluster together, but it cannot directly output the expression level that sequence will produce or the chromatin accessibility it will exhibit. The models capture what patterns exist in genomic sequence but not what those patterns do in cellular context. They cannot represent epigenomic state, three-dimensional chromatin organization, or cell-type-specific regulation without additional inputs beyond sequence.\nThese limitations define the complementary relationship between language models and sequence-to-function models. Where DNA language models learn representations from sequence statistics, regulatory models like Enformer and Borzoi predict molecular phenotypes from sequence context (Chapter 16). The regulatory models provide quantitative outputs (expression levels, chromatin tracks, splice probabilities) that language models alone cannot produce. For variant effect prediction (Chapter 17), both representation quality and phenotypic prediction matter: language model embeddings capture evolutionary constraint while regulatory models predict functional consequences. Understanding what each model family provides is prerequisite to combining them effectively.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat was the key innovation of DNABERT that proved self-supervised pretraining could work for DNA sequences? What were its main limitations regarding tokenization and context length?\nExplain why standard transformer attention has quadratic complexity O(L²) and how this limits biological applications. How do HyenaDNA and Caduceus overcome this limitation?\nWhat is reverse-complement equivariance and why does it matter for DNA language models? Give an example of when this property should hold versus when it should not.\nDescribe how DNA language models perform zero-shot variant scoring using likelihood ratios. What makes this approach work without pathogenicity training data?\nDNA language models learn representations from sequence alone. What can they capture (provide three examples) and what fundamental limitations prevent them from learning (provide three examples)?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nDNABERT innovation and limitations:\n\nInnovation: Applied BERT masked language modeling to DNA using k-mer tokenization (6-mers), demonstrating that self-supervised pretraining on genomic sequence learns transferable representations for downstream tasks\nTokenization limitation: Overlapping k-mers create positional ambiguity for variants (each nucleotide appears in multiple adjacent tokens)\nContext limitation: Limited to 512 tokens (~few hundred bp) due to quadratic attention complexity\n\nQuadratic complexity and solutions:\n\nWhy quadratic: Standard attention computes all pairwise interactions between positions, requiring an L×L matrix. For 100kb sequence: ~10¹⁰ computations per layer\nBiological limitation: Most regulatory phenomena span &gt;10kb (enhancer-promoter interactions: 50-200kb; TADs: ~1Mb), exceeding what quadratic attention can process\nHyenaDNA solution: Implicit convolutions with O(L log L) complexity via FFT, achieving 1 Mb context\nCaduceus solution: Mamba state-space models with O(L) linear complexity, plus reverse-complement equivariance\n\nReverse-complement equivariance:\n\nDefinition: Model predictions should be mathematically related for a sequence and its reverse complement\nWhy it matters: DNA is double-stranded; many features (TF binding sites, motifs) are functionally identical regardless of which strand is read\nShould hold: Transcription factor binding sites (e.g., GAATTC has same binding activity on either strand)\nShould not hold: Genes with defined orientation (5’→3’ direction matters for transcription; strand determines sense vs. antisense)\nImpact: Building this constraint into architecture (Caduceus) improves efficiency and correctness\n\nZero-shot variant scoring:\n\nMethod: (1) Compute model’s log-likelihood for sequence with reference allele: log P(seq|ref); (2) Compute log-likelihood for sequence with alternate allele: log P(seq|alt); (3) Calculate likelihood ratio: Δ = log P(seq|ref) - log P(seq|alt); (4) Positive Δ means alternate reduces likelihood → inferred to be disruptive\nWhy it works: Pretraining forces the model to learn what sequence patterns are “allowed” in genomes. Constrained positions have confident predictions; variants violating learned constraints get low probability. This captures evolutionary constraint without explicit pathogenicity labels\n\nWhat DNA-LMs can and cannot learn:\n\nCan learn from sequence: (a) Sequence motifs (TF binding sites, splice signals, promoter elements); (b) Gene structure (exon-intron boundaries, coding vs. noncoding regions); (c) Evolutionary constraints (conservation patterns from cross-species training)\nCannot learn (missing from sequence): (a) Epigenetic state (DNA methylation, histone modifications, chromatin accessibility); (b) 3D chromatin structure (which enhancers contact which promoters); (c) Cell-type specificity (whether a regulatory element is active in specific tissues)\nFundamental limitation: Sequence encodes potential, not realization in cellular context\n\n\n\n\n\n\n\nWhat we covered:\n\nThe paradigm shift from task-specific CNNs to general-purpose DNA language models that learn reusable representations from self-supervised pretraining\nModel evolution from DNABERT (512 tokens, proof of concept) through Evo 2 (1 Mb context, 40B parameters, pan-genomic)\nArchitectural innovations that enable long contexts: implicit convolutions (HyenaDNA), state-space models (Caduceus), and hybrid designs (Evo 2)\nBiological inductive biases like reverse-complement equivariance that can substitute for raw scale\nWhat models learn (motifs, gene structure, evolutionary constraint) and what they cannot learn (epigenetic state, 3D structure, cell-type specificity)\nPractical usage patterns: embeddings as features, fine-tuning, and zero-shot variant scoring\n\nKey takeaways:\n\nDNA language models produce representations, not predictions. They capture sequence patterns but not cellular context.\nNo single model dominates all tasks; model choice depends on context length requirements and available training data.\nBenchmark performance may not predict real-world deployment success due to distribution shift and metric limitations.\n\nLooking ahead:\n\nProtein language models (Chapter 15) apply similar principles to amino acid sequences\nRegulatory models (Chapter 16) predict molecular phenotypes that DNA-LMs cannot\nVariant effect prediction (Chapter 17) combines both representation and prediction\n\n\n\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “[GPN] DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nCheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu Yang, Lei Li, and Jian Ma. 2024. “DNALONGBENCH: A Benchmark Suite For Long-Range DNA Prediction Tasks,” October. https://openreview.net/forum?id=opv67PpqLS.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2106.09685.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nManzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025. “Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory Variants.” bioRxiv: The Preprint Server for Biology, June, 2025.05.19.654920. https://doi.org/10.1101/2025.05.19.654920.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. “BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks.” arXiv. https://doi.org/10.48550/arXiv.2311.12570.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. “BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Foundation Models.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence Modeling and Design from Molecular to Genome Scale with Evo.” Science 386 (6723): eado9336. https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>DNA Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html",
    "href": "part_3/p3-ch15-protein-lm.html",
    "title": "15  Protein Language Models",
    "section": "",
    "text": "15.1 ESM Model Family\nThe ESM (Evolutionary Scale Modeling) family developed at Meta AI Research represents the most influential protein language model lineage, progressing from an initial proof-of-concept to models capable of predicting three-dimensional structure from sequence alone. The progression from ESM-1b through ESM-2 illustrates how scaling transformer architectures yields systematic improvements in biological knowledge extraction, while revealing what self-supervised learning on protein sequences can and cannot achieve.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-esm-family",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-esm-family",
    "title": "15  Protein Language Models",
    "section": "",
    "text": "15.1.1 ESM-1b: Establishing the Paradigm\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading on, consider: if you wanted to teach a neural network about protein biology without providing any labels (no structure annotations, no function labels, no conservation scores), what training objective might you use? What information is implicitly present in the sequences themselves?\n\n\nThe Evolutionary Scale Modeling project demonstrated that transformer language models trained on protein sequences learn biologically meaningful representations without explicit supervision (Rives et al. 2021). The approach was strikingly simple: take the BERT architecture that had revolutionized natural language processing, replace words with amino acids, and train on protein sequence databases. The resulting models learned far more than anyone expected.\nESM-1b was trained on UniRef50, a clustered database of approximately 33 million protein sequences covering the known diversity of protein families. The construction and characteristics of protein sequence databases are detailed in Section 2.6, with implications for training data curation in Chapter 2. UniRef50 clusters sequences at 50% identity, providing broad coverage while reducing redundancy that would otherwise bias the model toward overrepresented families (Suzek et al. 2007). Why 50% specifically? This threshold represents a structural inflection point: proteins sharing &gt;50% sequence identity almost always share the same fold, while proteins below 30% identity may have completely different structures despite some sequence similarity. Clustering at 50% thus preserves one representative per structural family while collapsing near-identical orthologs that would otherwise dominate training. This curation strategy ensures the model encounters diverse evolutionary solutions to protein function rather than memorizing common motifs.\nThe architecture follows the BERT-style bidirectional transformer design with 650 million parameters distributed across 33 layers, a hidden dimension of 1,280, and 20 attention heads. The maximum sequence length of 1,024 amino acids accommodates most individual protein domains and many complete proteins. The training objective is masked language modeling, the self-supervised strategy introduced in Section 8.1: randomly mask 15% of amino acids in each sequence, and train the model to predict the masked positions given surrounding context. This objective contains no information about structure, function, or evolution beyond what is implicit in the sequences themselves.\nWhy does masked language modeling teach the model about protein biology? The key is that proteins are not random sequences—they are products of evolution, shaped by physical and functional constraints. To accurately predict a masked amino acid, the model must learn what makes a sequence “protein-like.” At a buried core position, hydrophobic amino acids are overwhelmingly favored because charged residues would destabilize the fold. At an active site, specific catalytic residues are nearly invariant because alternatives abolish function. At the interface between two domains, co-evolved residues must fit together geometrically. The model cannot distinguish these cases without learning the underlying principles. Predicting masked tokens thus becomes a proxy for understanding protein constraints.\n\n\n15.1.2 Emergent Biological Knowledge\nThe surprise was not that ESM-1b learned to predict masked amino acids accurately, but what else it learned in the process. Despite never seeing structural or functional labels during training, ESM-1b’s internal representations encode information about protein biology at multiple levels of organization.\nSecondary structure emerges in the attention patterns. When researchers analyzed which sequence positions the model attends to when making predictions, they found that attention concentrates along patterns corresponding to alpha helices and beta sheets. The model implicitly learns that certain amino acid sequences form specific structural elements, encoding this knowledge without ever being told what secondary structure is.\nMore remarkably, ESM-1b captures residue-residue contacts. Amino acids that are distant in the linear sequence but close in three-dimensional space attend to each other in the model’s attention matrices. This emergent capability suggests the model learns aspects of protein folding purely from sequence statistics. When attention weights were converted to contact predictions, they achieved accuracy approaching dedicated contact prediction methods that were explicitly trained for that task.\n\n\n\n\n\n\nKey Insight\n\n\n\nThe model was never told about protein structure, yet structure emerged. This is the central surprise of protein language models: the training objective (predict masked amino acids) is purely about sequence statistics, but the solution the model finds encodes structural relationships. Evolution has embedded so much physical information in sequence constraints that learning sequence patterns is, implicitly, learning structure.\n\n\nThe model’s masked token predictions correlate strongly with position-specific conservation scores derived from multiple sequence alignments. ESM effectively learns which positions tolerate variation and which are evolutionarily constrained, extracting this information from the statistical patterns across 33 million sequences rather than from explicit conservation annotations. Positions where the model confidently predicts specific amino acids correspond to positions that are conserved across protein families.\nPerhaps most striking, attention concentrates on functionally important positions. Catalytic residues, binding sites, and other sites of biological importance receive elevated attention even without explicit functional annotation in the training data. The model identifies that certain sequence positions are more informative about surrounding context, and these positions frequently correspond to sites where nature has constrained variation because they perform essential functions.\n\n\n\n\n\n\n\n\nAttention captures residue contacts\n\n\n\n\n\n\n\nSecondary structure emerges in embedding space\n\n\n\n\n\n\n\n\n\nAttention concentrates on functional sites\n\n\n\n\n\n\n\nEvolutionary relationships encoded in embeddings\n\n\n\n\n\n\nFigure 15.1: Emergent properties of protein language models. (A) Specific attention heads capture residue-residue contacts with high correlation to experimental structures. (B) Secondary structure emerges as distinct clusters in embedding space. (C) Attention concentrates on functionally important positions including catalytic sites and binding pockets. (D) Evolutionary relationships are encoded, with proteins clustering by taxonomic origin. All properties emerge from masked language modeling on protein sequences without explicit structural or functional supervision, demonstrating that evolutionary sequence patterns encode rich biological information.\n\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nWithout looking back, can you articulate what biological knowledge emerges from ESM-1b despite the model never receiving explicit labels for these properties? List at least three types of emergent knowledge.\n\n\nCheck your answer\n\nThe model learns: (1) secondary structure patterns (alpha helices, beta sheets) visible in attention matrices, (2) residue-residue contacts between positions distant in sequence but close in 3D space, (3) evolutionary conservation patterns that identify constrained positions, and (4) functional site locations where catalytic residues and binding sites receive elevated attention.\n\n\n\n\n\n15.1.3 ESM-2: Scaling Up\nESM-2 extended the ESM approach across a range of model scales, from 8 million to 15 billion parameters, enabling systematic study of how biological knowledge scales with model capacity (Lin et al. 2022). The results confirmed a pattern familiar from natural language processing: bigger models learn more.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore viewing the table below, make a prediction: How do you expect model performance to scale as parameters increase from 8 million to 15 billion (a ~2000-fold increase)? Will performance gains be linear, accelerating, or diminishing? At what point would you expect performance to plateau?\n\n\n\nESM-2 model family spanning four orders of magnitude in parameter count, with architecture details and relative performance on structure-related tasks.\n\n\nModel\nParameters\nLayers\nHidden Dim\nPerformance Gain\n\n\n\n\nESM-2 (8M)\n8M\n6\n320\nBaseline\n\n\nESM-2 (35M)\n35M\n12\n480\nModest\n\n\nESM-2 (150M)\n150M\n30\n640\nSubstantial\n\n\nESM-2 (650M)\n650M\n33\n1280\nLarge\n\n\nESM-2 (3B)\n3B\n36\n2560\nNear-optimal\n\n\nESM-2 (15B)\n15B\n48\n5120\nState-of-the-art\n\n\n\nPerformance scales smoothly with model size across structure prediction, contact prediction, and variant effect tasks. The scaling relationship is not linear: doubling parameters does not double accuracy. But gains remain consistent through even the largest models, suggesting that the 15-billion parameter ceiling reflects computational constraints rather than fundamental limits on what sequence statistics can teach.\nWhy does performance continue improving with scale? The answer lies in the complexity of the patterns the model must capture. Small models can learn common motifs and obvious constraints—the GT dinucleotide at splice donors, the hydrophobic core of globular proteins. But subtle patterns require more capacity: the coordinated positions of an allosteric network, the epistatic relationships between distant residues, the statistical signatures of rare folds. Each increase in model size allows the model to represent more of these subtle dependencies. The consistent gains across very different tasks (structure, contacts, variant effects) suggest that larger models learn more general biological principles, not just task-specific shortcuts.\nThe scaling behavior mirrors observations in natural language processing, where larger models consistently capture more nuanced patterns. This predictable relationship between scale and capability provides a roadmap for model development: if more biological knowledge is needed, train a larger model on more data. The practical implications shaped how the field approached subsequent genomic foundation models, with the scaling law framework and its implications discussed in Section 13.3.\n\n\n\n\n\n\n\n\nLoss vs. model size\n\n\n\n\n\n\n\nDownstream performance scales consistently\n\n\n\n\n\n\n\nESM-2 follows compute-optimal scaling\n\n\n\n\n\n\nFigure 15.2: Scaling laws for protein language models demonstrated by ESM-2. (A) Perplexity decreases as a power law of parameter count from 8M to 15B parameters. (B) Downstream task performance scales consistently across diverse applications including contact prediction, structure modeling, and variant effect scoring. (C) ESM-2 development followed near-optimal compute scaling, maximizing capability for given resources. These scaling relationships, mirroring observations in natural language models, suggest that continued scaling would yield further improvements in protein understanding.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-alternative-architectures",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-alternative-architectures",
    "title": "15  Protein Language Models",
    "section": "15.2 Alternative Architectures",
    "text": "15.2 Alternative Architectures\nThe success of ESM raised a natural question: how much depends on the specific BERT architecture versus the general approach of self-supervised learning on protein sequences? The ProtTrans family explored this question by applying multiple transformer architectures to protein modeling (Elnaggar et al. 2021).\nProtBERT applies the bidirectional encoder to protein sequences, trained on the Big Fantastic Database (BFD) comprising approximately 2.1 billion protein sequences. This training corpus, substantially larger than UniRef50, provides broader coverage at the cost of including more redundant and potentially lower-quality sequences. The architectural choices match ESM closely, enabling direct comparison of training data effects.\nProtT5 adapts the encoder-decoder architecture from T5, enabling both understanding and generation tasks (Raffel et al. 2023). The encoder processes input sequences to produce contextual representations, while the decoder can generate output sequences conditioned on those representations. This architecture proved valuable for tasks requiring sequence generation, such as structure-conditioned design or sequence completion, though the encoder-only architecture remains dominant for embedding and classification tasks.\nProtXLNet explores permutation language modeling, capturing bidirectional context without the artificial [MASK] token that BERT-style models require during training (Yang et al. 2020). By training on all possible token orderings, XLNet-style models learn to predict each token from any subset of context tokens, potentially capturing richer dependencies at the cost of more complex training.\n\nComparison of protein language model architectures. All learn meaningful representations when trained on sufficient data, but architectural differences affect downstream task suitability.\n\n\n\n\n\n\n\n\n\nArchitecture\nModel\nKey Feature\nStrengths\nBest Use Cases\n\n\n\n\nEncoder-only (BERT)\nESM, ProtBERT\nBidirectional attention, MLM objective\nExcellent embeddings, efficient inference\nClassification, embedding extraction, variant scoring\n\n\nEncoder-decoder (T5)\nProtT5\nSeparate encoder/decoder, seq2seq capable\nGeneration tasks, flexible output\nSequence design, structure-conditioned generation\n\n\nAutoregressive (XLNet)\nProtXLNet\nPermutation language modeling\nRicher dependencies, no [MASK] mismatch\nTasks sensitive to pretraining objective\n\n\n\nThese architectural variants demonstrate that the protein language modeling paradigm generalizes beyond specific design choices. All architectures learn meaningful representations when trained on sufficient data, though performance differences emerge for specific downstream tasks. Encoder-only models excel at classification and embedding tasks where the entire sequence is available. Encoder-decoder models enable generation tasks where outputs must be produced token by token.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-attention-coupling",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-attention-coupling",
    "title": "15  Protein Language Models",
    "section": "15.3 Attention and Evolutionary Coupling",
    "text": "15.3 Attention and Evolutionary Coupling\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider two amino acid positions in a protein that must physically contact each other for the protein to fold correctly. If a mutation at position A disrupts the contact, what evolutionary pressure would exist at position B? How might this leave a signature in sequence databases that a language model could detect?\n\n\nThe emergence of contact information in ESM’s attention patterns connects to a deeper principle: evolutionary coupling. When two residues must maintain physical contact for a protein to function, mutations at one position create selective pressure for compensatory mutations at the other. Over evolutionary time, these correlated mutations leave statistical signatures in protein families that can be detected through covariance analysis of multiple sequence alignments.\nDirect Coupling Analysis (DCA) and related methods extract these coevolutionary signals to predict residue-residue contacts (Morcos et al. 2011). The approach requires constructing multiple sequence alignments, computing covariance matrices, and applying statistical corrections to distinguish direct from indirect correlations. The resulting contact predictions enabled the first accurate structure predictions for proteins lacking homologs in structural databases.\nProtein language models learn to extract similar information through a different route. Rather than computing covariance explicitly, transformers learn attention patterns that capture which positions inform predictions at other positions. When position i strongly attends to position j during masked prediction, the model has learned that knowing the amino acid at j helps predict the amino acid at i. This is precisely the signature of evolutionary coupling: positions that covary because they must maintain physical contact.\n\n\n\n\n\n\nKey Insight\n\n\n\nTraditional coevolution analysis (DCA) requires computing alignments and covariance matrices for each protein family. PLMs learn to perform this analysis implicitly during training on millions of sequences, compressing the evolutionary logic into attention patterns that apply immediately to any new sequence. The attention mechanism rediscovers coevolution as the optimal strategy for predicting masked tokens.\n\n\nThe attention-based approach offers several advantages over traditional covariance analysis. Language models generalize across protein families, learning shared principles that transfer to proteins with sparse evolutionary sampling. They handle the statistical challenge of distinguishing direct from indirect correlations implicitly through deep architecture rather than requiring explicit correction. And they provide rich representations beyond binary contact predictions, encoding information about the strength and nature of residue relationships.\nRao and colleagues demonstrated this connection directly by extracting attention weights from ESM and converting them to contact predictions (Rao et al. 2020). The resulting predictions approached the accuracy of dedicated contact prediction methods, despite the model never being trained to predict contacts. The attention mechanism, optimized purely for masked token prediction, discovers the coevolutionary structure of protein sequences as a byproduct.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-esmfold",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-esmfold",
    "title": "15  Protein Language Models",
    "section": "15.4 ESMFold: Structure from Sequence",
    "text": "15.4 ESMFold: Structure from Sequence\nStructure prediction has traditionally required multiple sequence alignments (MSAs) that search protein databases for evolutionary relatives, a process that can take hours per protein and fails entirely for sequences lacking detectable homologs. ESMFold demonstrated that the representations learned by ESM-2 contain sufficient evolutionary information to predict three-dimensional structure directly, eliminating the alignment requirement while maintaining competitive accuracy.\n\n15.4.1 Alignment-Free Prediction\nThe most dramatic demonstration of protein language model capabilities came with ESMFold, which predicts protein 3D structure directly from ESM-2 embeddings without requiring multiple sequence alignments (Lin et al. 2022). Traditional structure prediction, including AlphaFold2, relies heavily on MSAs constructed through computationally expensive searches against sequence databases. These searches can take hours per protein, and prediction quality depends critically on finding informative homologs.\nESMFold eliminates this requirement entirely. The architecture couples ESM-2 (using the 15-billion parameter variant) with a structure module adapted from AlphaFold2’s Evoformer and structure module. The language model embeddings replace MSA-derived features, providing the evolutionary context that the structure module needs to predict atomic coordinates. The model takes a single sequence as input and outputs predicted 3D coordinates for all atoms.\nThe computational speedup is substantial: approximately 60-fold faster than AlphaFold2 for typical proteins. This speed advantage makes it feasible to predict structures for the millions of protein sequences emerging from environmental sequencing projects, where computing MSAs would be prohibitively expensive. Metagenomic proteins, often lacking close homologs in existing databases, represent exactly the cases where MSA-based methods struggle and where single-sequence predictions become essential.\nESMFold achieves atomic-level accuracy for many proteins, though slightly below AlphaFold2 for proteins that benefit strongly from MSA information. The accuracy gap is largest for proteins with sparse evolutionary sampling, where explicit alignments provide information that single-sequence analysis cannot fully recover. For well-represented protein families, ESMFold approaches AlphaFold2 accuracy at a fraction of the computational cost.\n\nComparison of structure prediction approaches. The choice depends on computational constraints and the evolutionary context of target proteins.\n\n\n\n\n\n\n\n\n\nMethod\nInput\nTime per Protein\nBest Accuracy Cases\nLimitation Cases\n\n\n\n\nAlphaFold2\nSequence + MSA\nHours\nDeep MSA available\nOrphan proteins\n\n\nESMFold\nSequence only\nMinutes\nWell-represented families\nSparse evolutionary sampling\n\n\n\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore continuing, pause and reflect: ESMFold predicts structure from single sequences in minutes while AlphaFold2 requires hours of MSA construction. What information does ESM-2 encode that makes this possible? Where did that information come from, given that the model was only trained to predict masked amino acids?\n\n\n\n\n\n\n\n\n\n\nESMFold eliminates MSA requirement\n\n\n\n\n\n\n\nSpeed vs. accuracy trade-off\n\n\n\n\n\n\n\n\n\nESMFold fails for specific protein categories\n\n\n\n\n\n\n\nESMFold enables proteome-scale prediction\n\n\n\n\n\n\nFigure 15.3: ESMFold structure prediction from single sequences. (A) Architecture comparison: ESMFold uses pretrained ESM-2 representations from single sequences, eliminating AlphaFold2’s MSA requirement and achieving 60× speedup. (B) Speed-accuracy trade-off: ESMFold achieves ~85% of AlphaFold2’s accuracy at 50× the speed. (C) Failure modes: orphan proteins, novel folds, and dynamic proteins remain challenging without evolutionary context. (D) Scale application: the ESM Metagenomic Atlas predicted structures for 617 million proteins, demonstrating capabilities impossible with MSA-based methods.\n\n\n\n\n\n15.4.2 What ESMFold Reveals About PLMs\nESMFold’s success demonstrates that ESM-2’s internal representations encode sufficient information to determine 3D structure. The language model has learned not merely local sequence patterns but global folding principles, capturing what makes a sequence fold into a particular three-dimensional shape.\nThis has profound implications for understanding what protein language models learn. The attention patterns that emerge from masked prediction are, in some sense, learning the physics of protein folding. Residues that need to be close in 3D space to maintain stability attend to each other in the transformer’s attention matrices. The statistical patterns in protein sequences, shaped by billions of years of evolution under physical constraints, encode structural information that sufficiently powerful language models can decode.\nThe fundamental insight is that evolution has already solved the structure prediction problem, millions of times over, and recorded the solutions in sequence databases. Language models learn to read those solutions, extracting the implicit structural knowledge that selection has embedded in surviving sequences. The analogy to human language is illuminating: just as you learned grammar by reading millions of sentences without formal rules, PLMs learn protein “grammar” by reading millions of sequences without explicit physics lessons. The rules of protein folding emerge from pattern recognition, not from being taught what a hydrogen bond is.\n\n\n\n\n\n\nAlphaFold: Structure Prediction Without Foundation Models\n\n\n\nAlphaFold2’s performance at CASP14 in 2020 solved a 50-year grand challenge, predicting protein structures with accuracy competitive with experimental determination (Jumper et al. 2021). The achievement transformed structural biology and earned its creators the 2024 Nobel Prize in Chemistry. Yet AlphaFold is not a foundation model in the sense this book uses the term (see Chapter 13). Understanding why illuminates what makes PLM-based approaches distinctive.\nAlphaFold requires multiple sequence alignments as input. The Evoformer architecture processes MSA features alongside the query sequence, using attention mechanisms that operate over both the sequence dimension and the alignment dimension. Evolutionary information enters the model explicitly through database search rather than being learned implicitly from sequence data. This design choice has computational consequences: MSA construction can take hours per protein, and prediction quality depends critically on finding informative homologs. For orphan proteins lacking close relatives in sequence databases, AlphaFold’s accuracy degrades substantially.\nThe architectural innovations that enabled AlphaFold’s success differ fundamentally from the foundation model paradigm. Evoformer’s attention over MSA rows and columns, iterative recycling through the network, and the structure module’s SE(3)-equivariant operations represent expert-designed inductive biases encoding protein physics. These components were engineered specifically for structure prediction, not learned from self-supervised objectives on broad sequence data. The model excels at its designed task but does not produce general-purpose representations transferable to other problems.\nESMFold inverts this design philosophy. Rather than requiring explicit evolutionary input, ESMFold couples ESM-2 embeddings with a structure module adapted from AlphaFold’s architecture. The language model provides the evolutionary context that the structure module needs, context learned implicitly through masked token prediction on millions of protein sequences. A single sequence goes in; predicted coordinates come out. No MSA construction, no database search, no hours of preprocessing.\nThe comparison reveals what protein language models have and have not learned. ESMFold approaches AlphaFold accuracy for well-represented protein families where the language model’s training data provided dense evolutionary sampling. The gap widens for proteins where deep MSAs provide information that single-sequence analysis cannot fully recover. ESMFold runs approximately 60-fold faster than AlphaFold, enabling structure prediction at metagenomic scale for the millions of protein sequences emerging from environmental sequencing projects. The two approaches exhibit different failure modes: AlphaFold struggles with orphan proteins that lack homologs; ESMFold struggles with sequences the language model finds surprising (high perplexity), even when homologs exist.\nAlphaFold3 complicates this dichotomy (Abramson et al. 2024). The updated architecture uses diffusion-based structure generation and handles protein-ligand, protein-nucleic acid, and multi-chain complexes within a unified framework. MSA dependency is reduced in some contexts, and the model moves toward general biomolecular structure prediction rather than single-chain protein folding. Whether this represents convergence between task-specific and foundation model approaches remains an open question.\nAlphaFold demonstrated that protein structure prediction was computationally tractable; ESMFold demonstrated that foundation models had learned enough biology to solve it differently. Both insights matter. For this book’s purposes, ESMFold illustrates the foundation model paradigm: self-supervised pretraining produces representations that transfer to downstream tasks, including tasks (like structure prediction) that were not part of the training objective. AlphaFold’s success through architectural engineering rather than learned representations represents an alternative path, one that achieved the goal first but may prove less generalizable as the field matures. The AlphaMissense model discussed in Chapter 17 repurposes AlphaFold’s structure module for variant effect prediction, suggesting that even task-specific architectures can seed broader applications when their components prove useful beyond their original context.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-function-prediction",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-function-prediction",
    "title": "15  Protein Language Models",
    "section": "15.5 Function Prediction",
    "text": "15.5 Function Prediction\nBeyond structure, protein language models enable prediction of protein function directly from sequence. Function prediction encompasses multiple tasks: predicting Gene Ontology terms that describe molecular function, biological process, and cellular component; classifying enzyme activity; identifying binding sites and interaction partners; and predicting subcellular localization.\nTraditional function prediction relied on homology: proteins similar in sequence are assumed to share function. This approach fails for orphan proteins lacking characterized homologs and cannot distinguish functional differences between closely related sequences. PLM-based approaches address both limitations by learning representations that capture functional signatures beyond simple sequence similarity.\nFor Gene Ontology term prediction, PLM embeddings serve as input features to classification models that predict which GO terms apply to each protein. The embeddings capture evolutionary and structural information relevant to function, enabling accurate predictions even for proteins with limited homology to characterized sequences (kulmanov_deepgo-se_2024?). Performance improves with embedding quality, suggesting that larger language models capture more functionally relevant information. These embeddings can also serve as node features in biological network analyses (?sec-ch18-fm-embeddings), and the function predictions inform drug target identification workflows (?sec-ch27-variant-to-gene).\nEnzyme classification benefits similarly from PLM representations. The Enzyme Commission hierarchy categorizes enzymes by the reactions they catalyze, from broad classes (oxidoreductases, transferases) to specific substrate preferences. PLM embeddings distinguish these categories effectively, capturing the sequence features that determine catalytic activity without requiring explicit structural analysis (sanderson_deepectransformer_2023?).\nBinding site prediction applies attention analysis to identify which residues participate in ligand binding, protein-protein interactions, or nucleic acid recognition. Positions that the model identifies as important for contextual prediction often correspond to functionally important sites, including binding pockets and catalytic residues (fang_deepprosite_2023?). This capability enables rapid identification of functional sites in newly sequenced proteins.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA researcher has a novel protein sequence from an environmental sample with no detectable homologs in existing databases. Which PLM-based predictions would you expect to be most reliable, and which would be most uncertain? Consider: structure prediction, function prediction, and variant effect prediction.\n\n\nConsider before revealing\n\nFor orphan proteins, variant effect prediction may be most uncertain because the model has limited evolutionary context for what is “normal” at each position. Function prediction via GO terms will also struggle if the protein represents a truly novel functional category. Structure prediction via ESMFold may perform surprisingly well if the protein uses common structural motifs even without close homologs, though unusual folds will be challenging. The fundamental limitation is that all PLM capabilities ultimately derive from patterns learned across evolutionary data, making truly novel proteins inherently difficult.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-variant-effects",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-variant-effects",
    "title": "15  Protein Language Models",
    "section": "15.6 Variant Effect Prediction",
    "text": "15.6 Variant Effect Prediction\nA critical clinical application of protein language models is predicting the effects of amino acid substitutions. Missense variants are the most common type of protein-coding mutation, and clinical genetics pipelines must routinely assess whether specific substitutions are likely pathogenic or benign. The traditional approach required either direct experimental characterization or computational methods trained on labeled pathogenicity data, both of which scale poorly to the millions of possible variants in each human genome (see Chapter 4 for discussion of classical approaches).\n\n\n\n\n\n\nMathematical Detail\n\n\n\nThe following section uses log-likelihood ratios from the language model to score variants. If you are unfamiliar with log probabilities, the intuition is: the model assigns a probability to each possible amino acid at each position. If the model thinks the mutant amino acid is less likely than the wild-type, the variant receives a negative score (predicted deleterious). If the mutant is equally or more likely, the variant receives a zero or positive score (predicted benign).\n\n\nESM-1v demonstrated that PLMs can predict variant effects without any training on variant labels (Meier et al. 2021). The approach exploits the masked language modeling objective directly: for a variant at position \\(i\\) changing amino acid \\(a\\) to amino acid \\(b\\), compute the log-likelihood ratio:\n\\[\n\\Delta \\text{score} = \\log P(b \\mid \\text{context}) - \\log P(a \\mid \\text{context})\n\\]\nIf the model assigns higher probability to the mutant amino acid than the wild-type, the variant is predicted benign; if lower, deleterious. This zero-shot prediction requires no labeled training data. The model’s evolutionary knowledge, learned from sequence databases, directly informs variant interpretation.\nThe intuition is straightforward. Evolution has shaped protein sequences such that certain positions strongly prefer certain amino acids. Substitutions that violate these preferences are more likely to disrupt function. The language model captures these preferences through training on millions of evolutionarily successful sequences. Variants that the model finds surprising are more likely to be functionally disruptive.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider a protein position where the wild-type amino acid is glycine (small, flexible) and a variant introduces tryptophan (large, bulky). Without knowing anything about the protein’s function, why might a protein language model assign low probability to this substitution? What evolutionary patterns would the model have learned during training that make this substitution “surprising”?\n\n\n\n\n\n\n\n\nKey Insight\n\n\n\nZero-shot variant prediction inverts the typical ML workflow. Instead of training on labeled variant data (benign vs. pathogenic), the model learns what “normal” looks like from evolutionary data. Abnormal variants are detected as deviations from learned expectations, requiring no pathogenicity labels whatsoever. This explains why PLM scores generalize across proteins: they measure violation of evolutionary constraint rather than matching patterns of known pathogenic variants.\n\n\n\n\n\n\n\n\n\n\nLog-likelihood ratio scoring mechanism\n\n\n\n\n\n\n\nEvolutionary intuition behind zero-shot scoring\n\n\n\n\n\n\n\n\n\nDeep mutational scanning correlation\n\n\n\n\n\n\n\nClinVar pathogenicity discrimination\n\n\n\n\n\n\nFigure 15.4: Zero-shot variant effect prediction from protein language models. (A) The scoring mechanism computes log-likelihood ratio between variant and reference amino acids given sequence context. More negative scores indicate variants the model finds surprising—evolutionarily disfavored substitutions. (B) The evolutionary intuition: billions of years of natural selection have tested trillions of mutations. Low-probability variants violate learned evolutionary constraints, suggesting functional disruption. (C) Correlation with deep mutational scanning experiments validates that PLM scores capture functional effects. (D) Discrimination of ClinVar pathogenic vs. benign variants approaches methods trained with supervised labels, demonstrating that evolutionary constraint encodes pathogenicity information.\n\n\n\nBrandes and colleagues applied ESM-1b to predict effects for all approximately 450 million possible missense variants in the human genome, providing a precomputed resource for clinical variant interpretation (Brandes et al. 2023). On ClinVar benchmarks, ESM-1b outperformed existing methods in classifying variants as pathogenic or benign.\nAlphaMissense extended this approach by combining PLM representations with structural context from predicted protein structures (Cheng et al. 2023). The integration of sequence-based and structure-based signals improves accuracy, particularly for variants affecting protein stability or buried residues. AlphaMissense provides predictions for all approximately 71 million possible single amino acid substitutions in the human proteome.\nThe detailed comparison of variant effect prediction methods, including how PLM-based scores integrate with clinical classification frameworks, is covered in Section 17.2.3. The calibration of these scores to ACMG criteria appears in ?sec-ch14-acmg-mapping, and integration into rare disease diagnostic workflows in ?sec-ch26-fm-scoring. Here, the key point is that protein language models provide the foundational representations that make accurate zero-shot variant prediction possible.\n\n\n\n\n\n\nPractical Guidance: Choosing a PLM for Variant Effect Prediction\n\n\n\n\nFor quick screening: Use precomputed scores from ESM-1b (450M variants) or AlphaMissense (71M proteome-wide). Available through databases and APIs.\nFor novel proteins: Run ESM-1v or ESM-2 inference directly. Larger models (650M+) generally perform better.\nFor structural context: Consider AlphaMissense when protein structure is relevant to the variant mechanism (buried residues, stability effects).\nFor integration with clinical pipelines: PLM scores are one line of evidence among many. See Chapter 17 for integration with ACMG criteria and other computational evidence.\nLimitations to remember: Zero-shot scores reflect evolutionary constraint, not direct pathogenicity. A constrained position is likely functionally important, but not all variants at important positions cause disease.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-structure-integration",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-structure-integration",
    "title": "15  Protein Language Models",
    "section": "15.7 Integration with Structure Prediction",
    "text": "15.7 Integration with Structure Prediction\nProtein language models exist within a broader ecosystem of computational methods for protein analysis. Understanding how PLMs relate to structure prediction systems clarifies their role and capabilities.\nAlphaFold2 achieved breakthrough accuracy in structure prediction by combining learned representations with explicit geometric modeling (Jumper et al. 2021). The architecture processes both sequence information through embeddings and evolutionary information through multiple sequence alignments, using an attention-based module (Evoformer) to integrate these signals before predicting atomic coordinates. AlphaFold2’s success depended critically on MSA quality: proteins with many homologs could be predicted accurately, while orphan proteins remained challenging.\nESMFold demonstrated that PLM embeddings can replace MSA-derived features, achieving competitive accuracy without the alignment bottleneck. This finding clarified the relationship between language models and structure prediction: PLMs learn to compress evolutionary information into representations that are functionally equivalent to explicit alignments, at least for proteins with sufficient representation in training databases.\nAlphaFold3 extended structure prediction to protein complexes, nucleic acids, and small molecules (Abramson et al. 2024). The architecture incorporates diffusion-based generation, enabling prediction of binding poses and complex assemblies. These capabilities complement PLM-based function prediction by providing structural context for interpreting functional predictions.\nGenerative protein design methods including RFDiffusion and ProteinMPNN leverage both structural and sequence information (Watson et al. 2023; Dauparas et al. 2022). RFDiffusion generates novel protein backbones through diffusion processes conditioned on design objectives. ProteinMPNN designs sequences likely to fold into specified structures. Both methods benefit from PLM representations when designing sequences with desired functional properties, demonstrating how language models integrate into the broader protein engineering pipeline (see Chapter 30 for detailed treatment of sequence design methods).\nThe trajectory from ESM to ESMFold to integration with design tools illustrates how PLMs serve as a foundation for diverse downstream applications. The representations learned through self-supervised training transfer across tasks, providing a common language for structure prediction, function annotation, variant interpretation, and protein engineering. This pattern of foundation models enabling diverse applications recurs throughout genomic AI, as discussed in Chapter 13.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-limitations",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-limitations",
    "title": "15  Protein Language Models",
    "section": "15.8 Limitations",
    "text": "15.8 Limitations\nDespite their success, protein language models face several limitations that inform the development of genomic models and guide appropriate application.\n\n\n\n\n\n\nLimitations and failure modes of protein language models\n\n\n\n\nFigure 15.5: Fundamental limitations of protein language models. Orphan proteins lacking evolutionary context produce unreliable predictions. Novel folds designed computationally lie outside the training distribution. Intrinsically disordered proteins and dynamic conformations cannot be captured in single embeddings. Epistatic interactions between mutations are not modeled by independent position scoring. Post-translational modifications that alter function are invisible to sequence-only models. These limitations guide appropriate application: PLMs excel for well-characterized protein families but require caution for edge cases where evolutionary learning provides insufficient guidance.\n\n\n\n\n15.8.1 Orphan and Dark Proteins\nPLMs learn from evolutionary statistics, performing best for proteins with rich representation in training databases. Orphan proteins, those unique to specific lineages without detectable homologs, lack the evolutionary context that PLMs exploit. For these proteins, the model has no basis for distinguishing likely from unlikely amino acids at each position, and predictions degrade accordingly.\nThe problem extends to “dark” proteins that are poorly characterized despite having homologs. If an entire protein family has escaped experimental characterization, PLMs may learn statistical patterns without capturing functional relevance. The model cannot distinguish constraint imposed by function from constraint imposed by historical accident.\n\n\n15.8.2 Novel Folds\nTraining data shapes what models can predict. PLMs trained on natural protein databases learn the statistical patterns of naturally occurring folds, potentially struggling with designed proteins or hypothetical folds outside the training distribution. When researchers design proteins with novel topologies not found in nature, PLM predictions become less reliable because the relevant sequence patterns were never encountered during training (verkuil_language_2022?).\n\n\n15.8.3 Conformational Flexibility\nMost PLM representations assume a single static structure, but many proteins adopt multiple conformations relevant to function. Allosteric proteins, intrinsically disordered regions, and proteins that undergo conformational changes upon binding present challenges for methods that embed each sequence into a single representation. The language model learns the average properties of sequences but may not capture the dynamic range that determines biological behavior.\n\n\n15.8.4 Epistasis\nMost variant effect predictions assume independence: the effect of mutation A does not depend on whether mutation B is present. Real proteins exhibit epistasis, where variant effects depend on sequence context. Two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. Current PLM-based predictors model marginal effects at each position but do not explicitly capture higher-order interactions, though the contextual embeddings may represent some epistatic relationships implicitly.\nWhy is epistasis so difficult to model? The challenge is combinatorial: with 20 amino acids at each position, predicting all pairwise interactions for a 300-residue protein requires assessing \\(20^2 \\times \\binom{300}{2} \\approx 18\\) million combinations. Higher-order interactions grow exponentially worse. PLMs sidestep this by learning correlations from data rather than enumerating possibilities, but the training data rarely includes systematic measurements of multi-mutant effects. Deep mutational scanning typically measures single-mutant effects; double and higher-order mutants are exponentially rarer in experimental datasets. The model thus learns what it can observe, and complex epistatic interactions remain largely unobserved.\n\n\n15.8.5 Interpretability\nWhile attention patterns correlate with biological features, understanding exactly what PLMs learn remains challenging. The field is developing interpretation methods, including attention pattern analysis (Section 24.5) and probing studies (Section 24.4), but PLMs remain partially opaque. The distinction between plausible and faithful explanations, critical for clinical applications, is examined in Chapter 24. For clinical applications where explanations matter, this interpretability gap limits adoption. A prediction that a variant is pathogenic is more useful when accompanied by mechanistic insight into why the variant disrupts function.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider applying a PLM to predict variant effects in two scenarios: (1) a well-characterized human enzyme with thousands of known homologs, and (2) a viral protein from a newly emerged pathogen with only a few known relatives. Which PLM limitations are most relevant in each case? How might you adjust your interpretation of the predictions accordingly?\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nBefore moving forward, test your understanding of PLM limitations:\n\nWhy do orphan proteins pose challenges for protein language models?\nIf you designed a completely novel protein fold not found in nature, why might ESMFold struggle to predict its structure accurately?\nHow does the independence assumption in variant effect prediction fail to capture epistatic interactions?\n\n\n\nCheck your understanding\n\n\nOrphan proteins lack evolutionary context. PLMs learn from statistical patterns across protein families—they identify which amino acids are likely at each position based on what evolution has “tried” and preserved. For proteins unique to specific lineages without homologs, the model has no evolutionary data to learn from, making predictions unreliable.\nNovel folds lie outside the training distribution. The model learned sequence-structure relationships from natural proteins in its training data. A computationally designed fold with topology not found in nature represents patterns the model never encountered during training, reducing prediction reliability.\nEpistatic interactions occur when variant effects depend on sequence context. The standard scoring approach treats each position independently, computing the effect of mutation A without considering whether mutation B is present. But two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. The combinatorial explosion of pairwise and higher-order interactions makes explicit modeling intractable, and training data rarely includes systematic multi-mutant measurements.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-lessons",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-lessons",
    "title": "15  Protein Language Models",
    "section": "15.9 Lessons for Genomic Foundation Models",
    "text": "15.9 Lessons for Genomic Foundation Models\nThe success of protein language models established principles that guided subsequent development of genomic foundation models. These lessons transfer with appropriate modifications to DNA and RNA modeling.\n\n15.9.1 Self-Supervised Biological Knowledge\nPLMs demonstrated that massive amounts of biological knowledge can be learned from unlabeled sequences. The same evolutionary pressures that shape proteins also shape DNA. Purifying selection removes deleterious variants, leaving statistical signatures in sequence databases that self-supervised models can exploit. This principle underlies the entire foundation model paradigm: sufficiently large models trained on sufficiently large datasets with appropriate objectives will learn representations that capture biological function.\n\n\n15.9.2 Scaling Benefits\nPerformance improves predictably with model size through the range currently explored. The progression from 8 million to 15 billion parameters in ESM-2 showed consistent gains across structure prediction, contact prediction, and variant effect tasks. While scaling cannot continue indefinitely, current models remain in a regime where additional capacity yields reliable improvements. This relationship justified the computational investment in large genomic foundation models (see Chapter 13 for discussion of scaling laws in genomic contexts).\n\n\n15.9.3 Effective Transfer Learning\nRepresentations learned for one task (masked token prediction) transfer to other tasks (structure prediction, variant effects, function annotation). This suggests that self-supervised pretraining captures fundamental biological knowledge rather than task-specific shortcuts. A model trained to predict masked amino acids simultaneously learns about protein structure, function, evolutionary constraint, and disease relevance. The same principle motivates genomic language models: models trained to predict masked nucleotides may simultaneously learn about regulatory elements, evolutionary conservation, and variant effects. Transfer learning strategies, including fine-tuning approaches and parameter-efficient adaptation, are discussed in detail in Chapter 9, with specific guidance on choosing between these strategies in ?sec-ch09-choosing-strategy.\n\n\n15.9.4 Architecture-Sequence Matching\nThe BERT-style bidirectional encoder proved effective for proteins, where entire sequences are typically available and lengths rarely exceed a thousand residues. Genomic sequences present different challenges: much longer lengths spanning kilobases to megabases, different information density with coding regions being dense while intergenic regions are sparser, and structural features including reverse-complement relationships absent in proteins. These differences motivate architectural adaptations in genomic language models, as explored in Chapter 14.\n\n\n15.9.5 Integration Benefits\nAlphaMissense demonstrated that PLM embeddings combine effectively with structural and population genetics information, achieving accuracy beyond what any single information source provides. The most powerful methods integrate multiple signals, using PLMs as one component of larger systems. This principle extends to genomic foundation models, where sequence-based representations complement rather than replace functional annotations, chromatin data, and clinical information (see Chapter 17 for variant effect prediction integration strategies).",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch15-protein-lm.html#sec-ch15-conclusion",
    "href": "part_3/p3-ch15-protein-lm.html#sec-ch15-conclusion",
    "title": "15  Protein Language Models",
    "section": "15.10 Paradigm That Generalized",
    "text": "15.10 Paradigm That Generalized\nProtein language models established that transformer architectures can learn deep biological knowledge from sequence alone. ESM’s ability to predict structure, function, and variant effects without explicit labels demonstrated the power of self-supervised learning on evolutionary data. The framework validated a paradigm: treat biological sequences as language, train large models to predict masked tokens, and extract functional knowledge from learned representations. Attention patterns in these models capture evolutionary constraint, contact prediction, and structural relationships without requiring multiple sequence alignments or explicit structural supervision.\nThis success directly motivated genomic language models. If proteins constitute a language that transformers can learn, perhaps DNA does too. The DNA language models examined in Chapter 14 adapt protein language model architectures and training strategies to the distinct challenges of genomic sequences: longer contexts, different alphabets, ambiguous tokenization, and the full complexity of gene regulation beyond protein coding. RNA language models occupy an intermediate position, sharing features with both protein and DNA modeling while addressing the unique challenges of RNA structure and processing.\nThe integration path extends beyond sequence modeling. Just as protein language model representations feed into structure prediction (ESMFold) and variant effect prediction (AlphaMissense), genomic language model embeddings integrate into regulatory models (Chapter 16) and clinical applications (Chapter 28, Chapter 27). Protein design methods (Chapter 30) demonstrate how generative modeling builds on the representations that language models provide. Throughout this progression, the principle that ESM established remains: self-supervised learning on biological sequences captures knowledge that transfers across diverse applications.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nHow does masked language modeling on protein sequences lead to emergent knowledge about protein structure and function? What specific types of biological information emerge without explicit supervision?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nMasked language modeling requires the model to predict hidden amino acids from surrounding context. To succeed, the model must learn what makes sequences “protein-like”—the constraints imposed by evolution. Emergent knowledge includes: (1) secondary structure (alpha helices, beta sheets visible in attention patterns), (2) residue-residue contacts (spatially close amino acids attend to each other), (3) evolutionary conservation (constrained positions receive confident predictions), and (4) functional sites (catalytic residues, binding sites receive elevated attention). These properties emerge because evolutionary and physical constraints shape which amino acid combinations survive, embedding structural and functional information in sequence statistics.\n\n\n\n\nExplain the connection between attention patterns in protein language models and evolutionary coupling. Why do attention weights correlate with residue-residue contacts in 3D structure?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nEvolutionary coupling occurs when two residues must maintain physical contact for protein function. If mutation at position A disrupts the contact, selection favors compensatory mutations at position B. This creates correlated mutations across evolutionary time. Protein language models learn that knowing the amino acid at position B helps predict the amino acid at position A—precisely the signature of evolutionary coupling. The model learns to attend to positions that provide predictive information, and these often correspond to spatially proximate residues that coevolve. The attention mechanism thus rediscovers coevolution as the optimal strategy for masked token prediction.\n\n\n\n\nHow does ESMFold predict protein structure without multiple sequence alignments? What enables this, and what are the performance tradeoffs compared to AlphaFold2?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nESMFold uses ESM-2 embeddings as input to a structure prediction module adapted from AlphaFold2’s architecture. The language model embeddings, learned from masked token prediction on millions of sequences, encode evolutionary information that replaces explicit MSA-derived features. This works because the model has compressed evolutionary patterns into its parameters during pretraining. Tradeoffs: ESMFold is ~60× faster (minutes vs. hours) and handles orphan proteins better, but achieves slightly lower accuracy for proteins where deep MSAs provide information that single-sequence analysis cannot fully recover. The gap is largest for proteins with sparse evolutionary sampling.\n\n\n\n\nDescribe how protein language models perform zero-shot variant effect prediction using log-likelihood ratios. Why does this approach work for distinguishing pathogenic from benign variants?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nZero-shot scoring computes: Δscore = log P(mutant | context) - log P(wild-type | context). If the model assigns lower probability to the mutant amino acid than the wild-type, the variant is predicted deleterious. This works because the model learned evolutionary preferences from millions of successful sequences. Evolution eliminates functionally disruptive variants, leaving statistical signatures that the model captures. Variants the model finds “surprising” (low probability) often violate evolutionary constraints, indicating likely functional disruption. The approach requires no pathogenicity labels—it measures deviation from learned evolutionary expectations rather than matching patterns of known pathogenic variants.\n\n\n\n\nWhat are three fundamental limitations of protein language models? For each limitation, explain what type of information the model lacks and why self-supervised sequence training cannot provide it.\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n(1) Orphan proteins: Proteins without homologs lack evolutionary context. PLMs learn from statistical patterns across families, but orphan proteins provide no family data to learn from. (2) Novel folds: Designed proteins with topologies not found in nature lie outside the training distribution. The model learned sequence-structure relationships from natural proteins; completely novel folds represent patterns never encountered. (3) Epistatic interactions: Standard scoring assumes variant effects are independent, but real proteins show epistasis where effects depend on sequence context. The combinatorial explosion makes explicit modeling intractable, and training data rarely includes systematic multi-mutant measurements to learn higher-order interactions.\n\n\n\n\n\nKey Concepts Covered:\n\nESM model family: Transformer-based protein language models trained on masked language modeling, from ESM-1b (650M parameters) through ESM-2 (up to 15B)\nEmergent knowledge: Structure, contacts, conservation, and functional sites emerge from sequence training without explicit labels\nEvolutionary coupling: Attention patterns learn coevolutionary relationships between residues that must maintain physical contact\nESMFold: Single-sequence structure prediction using PLM embeddings, 60x faster than AlphaFold2\nZero-shot variant prediction: Log-likelihood ratios from PLMs score variants without pathogenicity training data\nLimitations: Orphan proteins, novel folds, conformational flexibility, epistasis, and interpretability\n\nMain Takeaways:\n\nEvolution has encoded protein structure and function in sequence statistics. PLMs learn to decode this information through self-supervised training.\nThe training objective (predict masked amino acids) is simple, but what the model learns to achieve this objective is profound: structural relationships, evolutionary constraints, and functional importance.\nPLM capabilities scale predictably with model size, providing a roadmap for development.\nZero-shot variant prediction works because the model has learned what “normal” looks like from evolutionary data. Abnormal variants are detected as deviations from expectations.\nPLMs face fundamental limitations for sequences outside evolutionary training distributions (orphan proteins, novel folds) and struggle with dynamic properties (conformational flexibility, epistasis).\n\nConnections to Other Chapters:\n\nDNA language models (Chapter 14) adapt this paradigm to genomic sequences\nVariant effect prediction methods (Chapter 17) integrate PLM scores with other evidence\nProtein design (Chapter 30) builds on PLM representations for generative tasks\nInterpretability methods (Chapter 24) probe what PLMs have learned\n\n\n\n\n\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. “[AlphaFold3] Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nDauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. “Robust Deep Learning–Based Protein Sequence Design Using ProteinMPNN.” Science 378 (6615): 49–56. https://doi.org/10.1126/science.add2187.\n\n\nElnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, et al. 2021. “ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Deep Learning and High Performance Computing.” arXiv. https://doi.org/10.48550/arXiv.2007.06225.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “[AlphaFold2] Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nMorcos, Faruck, Andrea Pagnani, Bryan Lunt, Arianna Bertolino, Debora S. Marks, Chris Sander, Riccardo Zecchina, José N. Onuchic, Terence Hwa, and Martin Weigt. 2011. “Direct-Coupling Analysis of Residue Coevolution Captures Native Contacts Across Many Protein Families.” Proceedings of the National Academy of Sciences 108 (49): E1293–1301. https://doi.org/10.1073/pnas.1111471108.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” arXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nRao, Roshan, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. 2020. “Transformer Protein Language Models Are Unsupervised Structure Learners.” bioRxiv. https://doi.org/10.1101/2020.12.15.422761.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “[ESM-1b] Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences of the United States of America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. “UniRef: Comprehensive and Non-Redundant UniProt Reference Clusters.” Bioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. “De Novo Design of Protein Structure and Function with RFdiffusion.” Nature 620 (7976): 1089–1100. https://doi.org/10.1038/s41586-023-06415-8.\n\n\nYang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. “XLNet: Generalized Autoregressive Pretraining for Language Understanding.” arXiv. https://doi.org/10.48550/arXiv.1906.08237.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Protein Language Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html",
    "href": "part_3/p3-ch16-regulatory.html",
    "title": "16  Regulatory Models",
    "section": "",
    "text": "16.1 Long-Range Regulation Problem\nConsider a canonical mammalian gene with complex tissue-specific expression. The promoter sits at the transcription start site, but the sequences that determine when and where the gene is expressed may be scattered across a 200 kilobase neighborhood. Think of it like a marionette puppet: the puppet itself (the gene) sits in one location, but the strings controlling its movement (enhancers, silencers) may attach from positions far above, each string pulled by a different puppeteer (transcription factor) to produce coordinated motion. Multiple enhancers drive expression in different tissues; silencers suppress expression in inappropriate contexts; insulators demarcate regulatory domains. Chromatin looping brings these distal elements into physical proximity with the promoter, but the loops themselves are dynamic and cell-type-specific.\nShort-context models face an information-theoretic barrier in this setting. A model with a 2 kilobase receptive field cannot distinguish a variant in an enhancer 50 kilobases upstream from a variant in neutral sequence at the same distance. Both fall outside the model’s effective context. Stacking more convolutional layers or using dilated convolutions can expand the receptive field, but the computational path between distant positions grows long, and gradients attenuate over many layers. Models like Basenji2 pushed convolutional receptive fields to tens of kilobases through aggressive pooling, but purely convolutional architectures struggle to propagate information across hundreds of kilobases without impractical depth, a limitation examined in Section 6.6. [Citation Needed]\nThe scale of the problem becomes concrete when examining enhancer-promoter distances in the human genome. Median enhancer-promoter distances in many tissues span 20 to 50 kilobases, with substantial fractions exceeding 100 kilobases (Gasperini et al. 2019). Topologically associating domains (TADs), which define the neighborhoods within which regulatory elements typically interact, range from hundreds of kilobases to several megabases. A model that cannot span these distances cannot fully capture the regulatory grammar of the genome.\nAttention mechanisms offer a direct solution: by computing pairwise interactions between all positions, attention can model dependencies across arbitrary distances in a single layer. The cost is quadratic scaling with sequence length. A naive transformer operating on 200,000 base pairs at single-nucleotide resolution would require attention matrices with 40 billion entries, far exceeding practical memory limits. Hybrid architectures sidestep this constraint by using convolutions to compress the sequence before attention, reducing the effective sequence length to a few thousand tokens while preserving the information needed for long-range modeling.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html#sec-ch16-long-range",
    "href": "part_3/p3-ch16-regulatory.html#sec-ch16-long-range",
    "title": "16  Regulatory Models",
    "section": "",
    "text": "Stop and Think\n\n\n\nBefore reading further, consider: if you wanted to predict whether a gene is expressed in liver versus brain, what genomic features beyond the promoter sequence might you need to examine? How far from the gene might relevant information lie?\n\n\n\n\n\n\n\n\n\n\nKey Insight\n\n\n\nThe scale mismatch between model context and regulatory biology is not a minor inconvenience–it is a fundamental barrier. A 2kb context window can see only 1% of a 200kb regulatory neighborhood. Models that work well for detecting individual motifs systematically fail for tasks requiring integration of distal information.\n\n\n\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore viewing the table below, make a prediction: Which architectural approach do you expect will achieve the best balance between long-range modeling capability and computational efficiency? Consider the trade-offs between context window size, computational scaling, and the ability to capture distant regulatory interactions. What constraints might limit pure CNNs versus pure transformers, and how might hybrid approaches address these limitations?\n\n\n\n\n\nTable 16.1: Comparison of architectural approaches for regulatory sequence modeling. Hybrid architectures achieve the best trade-off between context length and computational tractability.\n\n\n\n\n\n\n\n\n\n\n\n\nArchitecture Type\nContext Window\nEffective Resolution\nComputational Scaling\nLong-Range Modeling\n\n\n\n\nPure CNN (DeepSEA, Basset)\n1-2 kb\nSingle nucleotide\nLinear\nNone (outside receptive field)\n\n\nDilated CNN (Basenji2)\n40-130 kb\n~128 bp bins\nLinear\nIndirect (many layers)\n\n\nPure Transformer\nLimited by memory\nSingle nucleotide\nQuadratic\nDirect (single layer)\n\n\nHybrid CNN-Transformer (Enformer)\n200 kb\n~128 bp bins\nQuadratic on compressed\nDirect (attention)\n\n\nEfficient Attention (AlphaGenome)\n~1 Mb\nVariable\nSub-quadratic\nDirect (optimized)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegulatory elements span large genomic distances\n\n\n\n\n\n\n\nModel context limits what can be captured\n\n\n\n\n\n\n\n\n\nExpression prediction requires long-range integration\n\n\n\n\n\n\n\nDistribution of enhancer-promoter distances\n\n\n\n\n\n\nFigure 16.1: Long-range regulation demands long-context models. (A) A typical gene locus with promoter at TSS, enhancers 20-100kb away, and CTCF boundary elements defining the regulatory domain. (B) Different models see different portions of this landscape: DeepSEA (1kb) captures only the promoter; Basenji2 (40kb) includes proximal regulation; Enformer (200kb) spans most functional elements; AlphaGenome (1Mb) captures entire topologically associating domains. (C) Expression prediction requires integrating signals across the full regulatory landscape—variants in distant enhancers can have effects as large as promoter mutations. (D) Distribution of enhancer-promoter distances: median 20-50kb with substantial fraction exceeding 100kb.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html#sec-ch16-enformer",
    "href": "part_3/p3-ch16-regulatory.html#sec-ch16-enformer",
    "title": "16  Regulatory Models",
    "section": "16.2 Enformer: Attention Meets Regulatory Genomics",
    "text": "16.2 Enformer: Attention Meets Regulatory Genomics\nEnformer (Ž. Avsec et al. 2021) demonstrated that combining convolutional compression with transformer attention could dramatically improve expression prediction from sequence. The model processes 200 kilobase windows of DNA and predicts thousands of chromatin and transcription tracks across cell types and species, establishing a template that subsequent models have extended and refined.\n\n16.2.1 Architecture\nThe Enformer architecture consists of three stages that progressively transform raw sequence into multi-task predictions.\n\n\n\n\n\n\nComputational Detail\n\n\n\nThe following section describes Enformer’s architecture in detail. Understanding the precise mechanics helps when interpreting predictions and troubleshooting unexpected results, but the core concept is straightforward: compress with convolutions, then model long-range interactions with attention.\n\n\nThe convolutional stem takes one-hot encoded DNA (four channels for A, C, G, T) and applies a series of convolutional blocks with residual connections. Each block includes convolutions that detect local patterns, batch normalization and nonlinearities, and pooling operations that reduce sequence length while increasing channel depth. By the end of the stem, a 200 kilobase input has been compressed to roughly 1,500 tokens, each representing approximately 128 base pairs of underlying sequence. This compression strategy resembles how a city planner might study traffic patterns: rather than tracking every individual car (each nucleotide), they aggregate traffic into neighborhood-level summaries (128 bp bins), preserving the essential flow information while making the analysis tractable. This compression is crucial: it reduces the attention computation from quadratic in 200,000 to quadratic in 1,500, a reduction of roughly 17,000-fold in memory requirements.\nWhy is this particular compression strategy effective? The convolutional stem preserves local motif information while discarding positional precision that regulatory biology does not require. A transcription factor binding site is roughly 6-12 base pairs; whether it occurs at position 50,127 or 50,135 rarely matters for function. The 128 bp bins are large enough to contain complete motifs while small enough to preserve the spatial relationships between regulatory elements—whether an enhancer is 50 kb or 51 kb from a promoter makes little functional difference, but whether it is 10 kb or 100 kb away matters greatly. The hierarchical compression also allows the model to learn features at multiple scales: early convolutional layers detect individual motifs, while later layers combine motifs into composite regulatory patterns before attention integrates across long distances.\nThe transformer trunk operates on the compressed sequence through a stack of self-attention layers. Each layer computes attention scores between all pairs of positions, allowing information to flow directly between any two locations in the 200 kilobase window. Relative positional encodings preserve information about the distances between elements, which matters for regulatory biology where the spacing between motifs often carries functional significance. The combination of multi-head attention and feed-forward layers enables the model to learn complex, position-dependent relationships across the full window.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nWhy does Enformer use relative positional encodings in the transformer rather than absolute positional encodings? Consider what information about regulatory elements matters for their function.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nRelative positional encodings preserve information about the distance between regulatory elements (e.g., an enhancer is 50kb from a promoter) rather than their absolute positions in the genome. This matters because regulatory function depends on spacing relationships—whether two motifs are 100bp apart versus 10kb apart affects their interaction—but not on whether they’re at genomic coordinate 10,000 or 10,000,000. This design choice reflects biological reality: evolution preserves regulatory spacing more than absolute positions.\n\n\n\n\n\nTask-specific output heads branch from the shared transformer backbone. Separate heads predict different types of outputs: DNase accessibility and ATAC-seq signal (chromatin openness), histone modifications including H3K4me3, H3K27ac, and other marks, CAGE signal reflecting transcription initiation, and additional functional genomics readouts where training data is available. Each head consists of convolutional and linear layers that transform the shared representation into track-specific predictions.\nThe multi-task design serves multiple purposes. Different assays provide complementary supervision: chromatin accessibility reflects regulatory potential, histone marks indicate active enhancers and promoters, and CAGE captures transcriptional output. Training on all assays jointly encourages the backbone to learn representations that capture the full regulatory cascade from accessible chromatin through enhancer activation to transcription initiation.\n\n\n\n\n\n\nEnformer hybrid CNN-transformer architecture\n\n\n\n\nFigure 16.2: Enformer architecture combining convolutional compression with transformer attention. Stage 1 (Convolutional Stem): compresses 200kb input to ~1,500 tokens through pooling, achieving 100× reduction while preserving local motif features. Stage 2 (Transformer Trunk): 8-head self-attention with relative positional encodings enables direct information flow between any positions, modeling long-range regulatory relationships. Stage 3 (Task-Specific Heads): shared representations branch to ~5,000 output tracks spanning chromatin accessibility, histone modifications, and transcription signals across cell types. The hybrid design makes attention computationally tractable (quadratic on 1,500 rather than 200,000 positions) while preserving the ability to model enhancer-promoter interactions spanning the full 200kb window.\n\n\n\n\n\n16.2.2 Training Data and Cross-Species Learning\nWhere does a model learn what an enhancer looks like, or that H3K27ac marks active regulatory regions? The answer shapes everything the model can and cannot do. Enformer trains on functional genomics data from both human and mouse, spanning hundreds of assays and cell types. The chromatin accessibility, histone modification, and transcription initiation assays introduced in Section 2.4.1 and Section 2.4 provide the supervision signals: DNase-seq and ATAC-seq measure regulatory potential, ChIP-seq for histone marks identifies active enhancers and promoters, and CAGE captures where transcription begins. Human training data derives largely from ENCODE and Roadmap Epigenomics consortia, supplemented by CAGE data from FANTOM and additional chromatin profiling studies [Citation Needed]. Mouse data from analogous consortia provides complementary supervision.\n\n\n\n\n\n\nRegulatory Genomics Assays: What They Measure\n\n\n\nSequence-to-function models learn from experimental measurements of regulatory activity. Understanding what each assay captures clarifies what models can and cannot learn.\nChromatin accessibility assays identify regions where DNA is not tightly wrapped around nucleosomes, making it available for transcription factor binding:\n\nDNase-seq uses DNase I enzyme to cut accessible DNA; sequencing reads pile up at open chromatin regions\nATAC-seq uses Tn5 transposase, which preferentially inserts into accessible regions; faster and requires fewer cells than DNase-seq\n\nTranscription factor binding assays identify where specific proteins bind DNA:\n\nChIP-seq (Chromatin Immunoprecipitation followed by sequencing) uses antibodies to pull down DNA bound by a target protein; widely used for transcription factors and histone modifications\nCUT&RUN and CUT&Tag are newer methods that require fewer cells and produce less background noise\n\nHistone modification assays reveal chromatin states associated with regulatory function:\n\nH3K4me3 marks active promoters\nH3K27ac marks active enhancers and promoters\nH3K27me3 marks repressed regions (Polycomb silencing)\nH3K36me3 marks transcribed gene bodies\n\nTranscription assays measure where and how much transcription occurs:\n\nCAGE (Cap Analysis of Gene Expression) captures transcript 5’ ends, precisely mapping transcription start sites\nRNA-seq measures steady-state RNA levels across exons and introns\nPRO-seq and GRO-seq measure nascent transcription by capturing RNA polymerase-associated RNA\n\nRegulatory activity assays directly measure enhancer function:\n\nSTARR-seq (Self-Transcribing Active Regulatory Region sequencing) tests thousands of sequences for enhancer activity by measuring their ability to drive transcription of themselves\nMPRA (Massively Parallel Reporter Assays) tests sequences in reporter gene constructs\n\nChromatin conformation assays reveal 3D genome organization:\n\nHi-C captures all chromatin contacts genome-wide\nMicro-C provides higher resolution contact maps\nHiChIP and PLAC-seq enrich for contacts involving specific proteins\n\nEach assay type provides different supervision signal. Accessibility assays indicate regulatory potential; binding assays identify specific factors; activity assays measure functional output. Models trained on combinations learn richer regulatory representations than those trained on any single assay type.\n\n\n\n\n\n\nRegulatory genomics assays and their signals\n\n\n\n\nFigure 16.3: Regulatory genomics assays provide complementary supervision for sequence-to-function models. Left column: Assay types organized by what they measure. Chromatin accessibility (DNase-seq, ATAC-seq) identifies open regions available for TF binding. Histone ChIP-seq reveals active promoters (H3K4me3), active enhancers (H3K27ac), and repressed regions (H3K27me3). CAGE captures transcription initiation sites. Right column: How signals appear in data. Each assay generates read pileups at specific genomic features, which models learn to predict from sequence. Multi-task training on diverse assays encourages learning of generalizable regulatory representations rather than assay-specific artifacts.\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nConsider Enformer’s training setup. If the model is trained on data primarily from ENCODE and Roadmap Epigenomics consortia, which cell types do you expect it will predict well for, and which might it struggle with? Think about what cell types these consortia focused on.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nEnformer will predict well for common cell lines like K562, HepG2, GM12878, and major tissue types extensively profiled by these consortia. It will likely struggle with rare cell types, disease-specific cell states, developmental stages not represented in the data, and tissue types that are difficult to culture or obtain. Predictions for underrepresented contexts should be interpreted with appropriate caution.\n\n\n\n\n\nCross-species training confers several advantages. Regulatory sequences that are functionally constrained evolve more slowly than neutral sequence, so mouse and human share many regulatory motifs and principles despite 80 million years of divergence. Training on both species helps the model distinguish conserved regulatory logic from species-specific noise, reduces overfitting to idiosyncrasies of human data, expands the effective training set without requiring additional human samples, and implicitly emphasizes evolutionarily conserved patterns that are more likely to be functionally important.\nThe training objective combines losses across all tracks, positions, and species. Count-based likelihoods (Poisson or negative binomial) handle sequencing-derived signals, while correlation-based objectives ensure the model captures the overall shape of coverage profiles. Per-track weighting prevents abundant assays from dominating gradients.\n\n\n16.2.3 Variant Effect Prediction\nThe clinical and scientific value of Enformer lies substantially in its ability to predict how sequence variants alter regulatory activity. The procedure follows a straightforward logic: extract a 200 kilobase window containing the variant, compute predictions for the reference allele, compute predictions for the alternative allele, and compare the outputs across all tracks and positions.\nWorked Example: Scoring a Regulatory Variant\nConsider an intergenic variant 45 kilobases upstream of gene X, identified in a GWAS for liver enzyme levels. To interpret this variant with Enformer:\n\nExtract window: Center a 200kb window on the variant position, ensuring both the variant and the X promoter fall within the window\nGenerate sequences: Create reference and alternative versions differing only at the variant position\nForward pass: Run both sequences through Enformer, obtaining predictions for ~5,000 tracks at ~1,500 positions each\nCompute delta: Subtract reference predictions from alternative predictions\nInterpret: Focus on relevant tracks (liver cell types) and positions (enhancer region around variant, promoter region of gene X)\nQuantify: If CAGE signal at the X promoter decreases by 0.3 log-fold in HepG2 but not in other cell types, this suggests a liver-specific regulatory effect consistent with the GWAS phenotype\n\nThe resulting variant effect scores span thousands of dimensions, one for each assay and cell type. A variant might increase predicted DNase accessibility in one cell type while decreasing predicted CAGE signal in another, suggesting context-dependent regulatory effects. By aggregating predictions around gene promoters, researchers can estimate variant effects on gene expression in specific tissues.\nValidation against GTEx expression quantitative trait loci (eQTLs) demonstrated that Enformer’s predictions correlate with observed genetic effects on expression (Ž. Avsec et al. 2021). Variants with large predicted effects on promoter-proximal CAGE signal were enriched among significant eQTLs. Notably, this correlation extended to distal variants: sequence changes 50 kilobases or more from a gene’s transcription start site still showed predictive power when they fell in regions of predicted regulatory activity. This long-range predictive capacity distinguishes Enformer from short-context models and validates the architectural investment in extended context windows. These predictions integrate with classical variant effect methods (Chapter 4) and foundation model approaches (?sec-ch14-enformer-vep) to provide comprehensive variant interpretation, with clinical workflow integration detailed in ?sec-ch26-fm-scoring.\n\n\n\n\n\n\nEnformer variant effect prediction workflow\n\n\n\n\nFigure 16.4: Enformer variant effect prediction workflow. Step 1: Center a 200kb window on the variant and construct reference and alternative sequences. Step 2: Run parallel forward passes to obtain track predictions for both sequences across all 5,000+ assay tracks. Step 3: Compute delta between alternative and reference predictions to identify affected tracks and positions. Step 4: Interpret cell-type-specific effects—reduced DNase in liver suggests tissue-specific regulatory disruption; H3K27ac decrease at enhancer indicates weakened enhancer activity; CAGE decrease at promoter predicts reduced expression. Validation against GTEx eQTLs confirms that predicted expression effects correlate with observed genetic associations, including for variants 50+ kb from their target genes.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html#sec-ch16-borzoi",
    "href": "part_3/p3-ch16-regulatory.html#sec-ch16-borzoi",
    "title": "16  Regulatory Models",
    "section": "16.3 Borzoi: From Chromatin to Transcriptome",
    "text": "16.3 Borzoi: From Chromatin to Transcriptome\nWhile Enformer predicts transcription initiation through CAGE, RNA-seq captures a richer picture of gene expression: not just where transcription begins, but how the transcript is spliced, which isoforms dominate, where transcription terminates, and how stable the resulting mRNA is. Borzoi (Linder et al. 2025) extends the hybrid architecture paradigm to predict full RNA-seq coverage profiles, enabling a unified view of how sequence variation affects the entire transcriptional program.\n\n16.3.1 Beyond Transcription Initiation\n\n\n\n\n\n\nStop and Think\n\n\n\nCAGE measures where transcription begins, but what aspects of gene expression does it miss? Consider what happens to an RNA molecule after transcription initiates. What regulatory events occur post-transcriptionally that could affect protein output?\n\n\nA single gene can produce multiple transcript isoforms through alternative promoter usage, alternative splicing, and alternative polyadenylation. These isoforms may have different stabilities, different translation efficiencies, and different functions. A variant that shifts isoform ratios without changing total expression could have substantial phenotypic consequences: a switch from a cytoplasmic to a nuclear isoform, for instance, or inclusion of a premature stop codon in the predominant transcript.\nCAGE and chromatin assays cannot capture these complexities. They measure where transcription might begin and what the chromatin environment looks like, but they do not reveal how RNA polymerase traverses the gene body, where splicing occurs, or which 3’ end is selected. RNA-seq coverage profiles encode all of this information: exon boundaries appear as coverage drops at intron junctions, alternative splicing manifests as variable junction usage, and polyadenylation site choice appears in the coverage pattern near gene 3’ ends.\n\n\n16.3.2 Predicting Coverage at Nucleotide Resolution\nTeaching a model to predict where reads pile up sounds straightforward, but RNA-seq coverage varies by orders of magnitude—from thousands of reads on highly expressed exons to near-zero in introns—and the sharp transitions at splice junctions encode critical information. How do you train a model to capture both the forest and the trees? Borzoi builds on an Enformer-style backbone with modifications tailored to RNA-seq prediction. The convolutional stem and transformer trunk follow similar principles, compressing long input windows and propagating information through attention. Output heads predict stranded RNA-seq coverage across the window, with additional heads for complementary signals like PRO-seq (nascent transcription), CAGE, and other assays when available.\nTraining on RNA-seq coverage imposes different demands than training on chromatin marks. Coverage varies over orders of magnitude between introns and exons; the model must capture both the overall expression level and the fine structure of the coverage profile. Junction reads that span splice sites provide particularly informative supervision, as they directly constrain the model to learn splicing patterns. The loss function balances accurate prediction of coverage levels with faithful reproduction of the coverage shape, including sharp transitions at exon boundaries.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nWhy do junction reads (reads spanning splice sites) provide particularly informative training signal for Borzoi? What would happen if the model were trained only on exonic coverage without junction information?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nJunction reads directly constrain the model to learn splicing patterns by providing explicit supervision about which exons are connected in mature transcripts. Without junction information, the model could learn to predict high coverage on exons and low coverage on introns, but it wouldn’t learn which exons are spliced together, alternative splicing patterns, or splice site recognition sequences. Junction reads force the model to understand the combinatorial logic of exon inclusion and exclusion, not just expression levels.\n\n\n\n\n\n\n\n16.3.3 Applications Beyond Expression Level\nA variant that causes exon skipping doesn’t change how much gene expression you see—it changes which transcript you get. If your model only predicts total expression levels, this pathogenic variant looks harmless. What can we do with full coverage predictions that simple expression quantification misses? By predicting full RNA-seq coverage, Borzoi enables analyses that go beyond simple expression quantification. Splicing variant effects can be assessed by comparing predicted coverage at exons and junctions under reference and alternative alleles. A variant that reduces predicted junction reads for a particular exon suggests exon skipping; increased junction reads to a cryptic splice site suggests aberrant splicing. These predictions complement specialized splicing models like SpliceAI (Section 6.5), providing additional context about how splicing changes fit within the broader transcriptional program. The integration of Borzoi splicing predictions with SpliceAI scores is examined in ?sec-ch14-spliceai.\nAlternative promoter usage becomes visible through coverage patterns near transcription start sites. A variant that increases coverage downstream of one TSS while decreasing it downstream of another suggests a shift in promoter preference. Such shifts can alter the 5’ UTR of the resulting transcript, affecting translation efficiency and regulatory motif content.\nPolyadenylation site choice affects 3’ UTR length and content. Shorter 3’ UTRs may escape microRNA-mediated repression; longer ones may include additional regulatory elements. Borzoi’s coverage predictions around annotated polyadenylation sites can reveal variants that shift site usage, potentially explaining effects on mRNA stability and translation that would be invisible to chromatin-based models.\n\n\n\n\n\n\nKey Insight\n\n\n\nBorzoi shifts the paradigm from predicting regulatory potential (chromatin state) to predicting regulatory outcome (RNA processing). This matters clinically because many disease-causing variants act through splicing or other post-transcriptional mechanisms that chromatin-focused models cannot detect.\n\n\n\n\n\n\n\n\n\n\nRNA-seq captures post-transcriptional processing\n\n\n\n\n\n\n\nBorzoi predicts coverage at nucleotide resolution\n\n\n\n\n\n\n\n\n\nVariant effects on RNA processing\n\n\n\n\n\n\n\nIntegration with specialized splicing models\n\n\n\n\n\n\nFigure 16.5: Borzoi extends regulatory prediction from chromatin to transcriptome. (A) While chromatin assays capture regulatory potential, RNA-seq reveals regulatory outcome: exon structure, splice junctions, and polyadenylation site usage. (B) Borzoi predicts stranded RNA-seq coverage profiles, reproducing sharp exon boundaries and junction structure from sequence alone. (C) Variant effect prediction extends to post-transcriptional mechanisms: altered splicing, shifted promoter usage, and changed polyadenylation patterns that would be invisible to chromatin-focused models. (D) Borzoi predictions integrate with specialized tools like SpliceAI, providing complementary context about how splicing changes fit within broader transcriptional programs.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html#sec-ch16-sei",
    "href": "part_3/p3-ch16-regulatory.html#sec-ch16-sei",
    "title": "16  Regulatory Models",
    "section": "16.4 Sei: A Regulatory Vocabulary from Sequence",
    "text": "16.4 Sei: A Regulatory Vocabulary from Sequence\nWhile Enformer and Borzoi predict continuous coverage tracks, Sei (Chen et al. 2022) takes a complementary approach: learning a discrete vocabulary of sequence classes that capture distinct regulatory activities. Rather than predicting thousands of individual assays, Sei maps sequences to a reduced set of regulatory states, each associated with characteristic chromatin and transcription patterns.\n\n16.4.1 Discrete Regulatory States\nA promoter that’s active in liver but silent in brain isn’t just “partially open”—it’s in fundamentally different regulatory states. But how many distinct states exist, and what sequence features define them? Tracking thousands of individual assay predictions makes these questions hard to answer. Sei builds on observations that chromatin states cluster into interpretable categories: active promoters, strong enhancers, poised enhancers, heterochromatin, and so forth. Previous methods like ChromHMM defined such states from observed chromatin marks in specific cell types [Citation Needed]. Sei learns to predict sequence class membership directly from DNA, asking what regulatory identity a sequence carries based on its intrinsic properties.\nThe model predicts 40 sequence classes derived from clustering patterns across chromatin accessibility, histone modifications, and transcription factor binding. Each class corresponds to a recognizable regulatory state: promoter-like sequences, enhancer-like sequences, CTCF binding sites, repressed regions, and various intermediate states. The output is not a single class assignment but a probability distribution over classes, reflecting the observation that many sequences have context-dependent regulatory potential.\n\n\n16.4.2 Complementary to Track Prediction\nWhen a clinician asks “what kind of regulatory element did this variant hit?”, answering “it reduced H3K27ac by 0.3 log-fold in HepG2 cells” is technically precise but practically opaque. What they want to know is: did the variant disrupt an enhancer, a promoter, or something else entirely? Sei and Enformer-style models serve complementary purposes. Enformer provides detailed, quantitative predictions across specific assays and cell types; Sei provides a compressed, interpretable summary of regulatory identity. For variant interpretation, both perspectives can be valuable. Enformer might reveal that a variant reduces predicted H3K27ac signal in liver but not heart; Sei might reveal that the same variant shifts sequence class membership from “strong enhancer” toward “weak enhancer,” a more immediately interpretable characterization.\nThe regulatory vocabulary approach also facilitates systematic analysis across many variants. Rather than tracking changes in thousands of individual tracks, researchers can ask how a set of variants affects the distribution of regulatory classes, identifying patterns that might be obscured in high-dimensional track space.\n\n\n\n\n\n\nSei regulatory vocabulary classes\n\n\n\n\nFigure 16.6: Sei discrete regulatory vocabulary. Instead of predicting thousands of continuous tracks, Sei classifies sequences into 40 regulatory states derived from clustering chromatin accessibility, histone modifications, and transcription factor binding patterns. Top: Example sequence classes organized by function—active promoters, strong/weak enhancers, CTCF-bound insulators, poised elements, and repressed regions. Bottom: Variant interpretation using the vocabulary. A variant shifts probability mass from “Strong Enhancer” toward “Weak Enhancer” class—an interpretable statement that complements Enformer’s quantitative track predictions.\n\n\n\n\n\n\nTable 16.2: Comparison of regulatory model outputs and their appropriate use cases. Different models excel at different aspects of regulatory prediction.\n\n\n\n\n\n\n\n\n\n\n\nModel\nOutput Type\nNumber of Outputs\nBest For\n\n\n\n\nEnformer\nContinuous tracks\n~5,000 (cell type x assay)\nQuantitative, cell-type-specific predictions\n\n\nBorzoi\nRNA-seq coverage\nPer-position, stranded\nSplicing, isoform, RNA processing effects\n\n\nSei\nDiscrete classes\n40 probability scores\nInterpretable regulatory state changes\n\n\nAlphaGenome\nMulti-modal\nChromatin + RNA + contacts\nComprehensive, unified predictions",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html#sec-ch16-alphagenome",
    "href": "part_3/p3-ch16-regulatory.html#sec-ch16-alphagenome",
    "title": "16  Regulatory Models",
    "section": "16.5 AlphaGenome: Unifying Modalities at Megabase Scale",
    "text": "16.5 AlphaGenome: Unifying Modalities at Megabase Scale\nAlphaGenome (Z. Avsec, Latysheva, and Cheng 2025) extends the hybrid modeling paradigm in two directions: longer context windows (approximately one megabase) and broader output modalities spanning chromatin, expression, splicing, and three-dimensional contacts. The goal is a single model that provides a comprehensive view of how sequence determines regulatory state.\n\n16.5.1 From 200kb to One Megabase\n\n\n\n\n\n\nStop and Think\n\n\n\nEnformer processes 200kb windows. AlphaGenome extends to approximately 1Mb. What additional biological features could a model capture with this extended context? Think about the sizes of topologically associating domains (TADs) and super-enhancers.\n\n\nThe megabase context window pushes against computational limits even with hybrid architectures. AlphaGenome addresses this through efficient attention mechanisms that reduce the quadratic cost, hierarchical processing that handles different output modalities at appropriate resolutions, and architectural refinements accumulated from Enformer and Borzoi development.\nThe output repertoire spans chromatin accessibility and histone modifications (following Enformer), gene expression and RNA coverage (following Borzoi), splicing predictions including exon inclusion and junction usage, and contact predictions reflecting three-dimensional chromatin organization.\nUnifying these modalities in a single model offers several advantages. The backbone representation must capture information relevant to all outputs, encouraging learning of features that connect chromatin state to transcription to RNA processing. Variant effect predictions become coherent across modalities: a single forward pass reveals how a variant affects chromatin, expression, splicing, and contacts, rather than requiring separate runs through independent models.\n\n\n16.5.2 Closed Weights, Open Questions\nThe most capable model is useless if you cannot use it on your data. When patient genomic sequences cannot leave your institution’s servers, API-only access becomes a barrier, not a convenience. Who controls the model matters as much as what the model can do. AlphaGenome is primarily available through an API interface rather than as a downloadable model. This arrangement simplifies use for many applications: researchers can score variants without managing large model weights or specialized hardware. It also introduces constraints around data privacy, customization, and integration with local pipelines. Clinical applications that cannot send patient sequence data to external services may be unable to use API-only models directly, motivating interest in openly available alternatives.\n\n\n\n\n\n\nPractical Guidance: Choosing a Regulatory Model\n\n\n\nUse Enformer when:\n\nYou need open-source, locally runnable predictions\nChromatin state and transcription initiation are your primary interests\nYou want to fine-tune or adapt the model for custom tasks\nYou require reproducible, version-controlled predictions\n\nUse Borzoi when:\n\nSplicing effects are important for your variants\nYou need RNA-level predictions beyond transcription initiation\nYou are integrating with other splicing predictors like SpliceAI\nPost-transcriptional regulation is clinically relevant\n\nUse Sei when:\n\nYou need interpretable regulatory state classifications\nYou are analyzing many variants and want compressed summaries\nCommunicating results to non-specialists is important\nYou want to identify variants that shift regulatory identity\n\nUse AlphaGenome when:\n\nYou need the most comprehensive multi-modal predictions\nData privacy constraints allow API usage\n3D contact predictions are relevant to your question\nYou want unified predictions across chromatin, RNA, and structure\n\n\n\nFrom the perspective of variant interpretation workflows, AlphaGenome provides a comprehensive set of predictions from a single query. A variant can be assessed for effects on local chromatin state, expression of nearby genes, splicing of overlapping transcripts, and potential disruption of chromatin contacts, all from the same underlying model. The challenge lies in synthesizing these multiple outputs into actionable conclusions, a topic addressed in Section 17.3.4, with practical workflow integration in ?sec-ch14-workflow-design.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html#sec-ch16-accomplishments",
    "href": "part_3/p3-ch16-regulatory.html#sec-ch16-accomplishments",
    "title": "16  Regulatory Models",
    "section": "16.6 What Hybrid Architectures Accomplish",
    "text": "16.6 What Hybrid Architectures Accomplish\nThe progression from DeepSEA through Enformer, Borzoi, and AlphaGenome reflects accumulating solutions to specific limitations. Each model addresses constraints that bounded its predecessor’s utility.\n\n16.6.1 Spanning Enhancer-Promoter Distances\nAn enhancer 80 kilobases from a promoter was invisible to earlier models—not uncertain, not weakly detected, but completely absent from the prediction. What do we gain when we can finally see it? The most direct contribution is enabling long-range interaction modeling. A 200 kilobase context window encompasses the distances over which most cis-regulatory interactions occur. Attention mechanisms allow the model to learn direct relationships between enhancers and promoters without requiring information to propagate through many intermediate layers. Empirically, this translates to improved prediction of expression and better correlation with eQTLs, particularly for variants in distal regulatory elements.\n\n\n16.6.2 Multi-Task Regularization\nWhy train one model on thousands of different assays instead of training specialized models for each? The naive expectation might be that a model focused solely on predicting gene expression would outperform one distracted by chromatin accessibility and histone modifications. Yet the opposite proves true. Training on hundreds of assays jointly constrains the model to learn representations that generalize across regulatory modalities. A feature useful only for predicting H3K4me3 in one cell type provides less gradient signal than a feature useful across chromatin, transcription, and accessibility. This multi-task pressure steers the model toward learning fundamental regulatory logic rather than assay-specific artifacts.\nWhy does multi-task training produce better representations than single-task training? The answer lies in what the shared backbone must learn to succeed across all tasks. Predicting chromatin accessibility requires recognizing transcription factor binding motifs. Predicting gene expression requires recognizing how those motifs combine into functional enhancers. Predicting histone modifications requires recognizing the sequence features that recruit chromatin modifiers. No single feature set suffices for all tasks, so the backbone must learn a rich vocabulary of regulatory features—precisely the vocabulary needed for robust variant effect prediction. In contrast, a model trained only on one assay might exploit assay-specific artifacts (batch effects, mapping biases, cell-line idiosyncrasies) that happen to correlate with the training signal but do not reflect genuine regulatory biology.\n\n\n16.6.3 Cross-Species Constraints\nIf human and mouse regulatory sequences have diverged over 80 million years of evolution, why would training on both species together help rather than hurt? The answer reveals something fundamental about what we want these models to learn. Training on human and mouse together further regularizes the model. Species-specific binding site variants, repetitive elements, and technical artifacts in training data affect one species but not the other. Features that generalize across species are more likely to reflect conserved regulatory mechanisms. This provides a form of evolutionary validation built into the training process.\nWhy does cross-species training work when human and mouse regulatory sequences have diverged substantially? The key insight is that core regulatory logic is more conserved than specific sequences. The transcription factors that drive liver expression in humans are largely the same as those in mice, even if the exact positions and sequences of binding sites have shifted. A model trained on both species must learn what HNF4A binding looks like in general, not just where HNF4A binds in one particular genome. This abstraction makes the model robust to sequence variation while preserving sensitivity to functional features—exactly the properties needed for predicting effects of human genetic variants, which by definition differ from the reference sequence the model was trained on.\n\n\n16.6.4 Unified Variant Effect Prediction\nRunning a variant through five different models and reconciling contradictory outputs is tedious at best and misleading at worst. What if one model could give you chromatin effects, expression changes, and splicing consequences in a single coherent prediction? Perhaps most practically valuable, hybrid models provide a unified framework for variant effect prediction on expression and related phenotypes. Rather than assembling scores from multiple specialized models, researchers can query a single model for comprehensive predictions. The outputs span cell types and assays, enabling tissue-specific interpretation of regulatory variants. This capability integrates naturally with the variant interpretation workflows described in ?sec-ch14-combining-evidence and the clinical applications examined in Chapter 28. The calibration of these multi-track predictions for clinical use is addressed in ?sec-ch14-calibration.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html#sec-ch16-limitations",
    "href": "part_3/p3-ch16-regulatory.html#sec-ch16-limitations",
    "title": "16  Regulatory Models",
    "section": "16.7 Limitations and Open Challenges",
    "text": "16.7 Limitations and Open Challenges\nDespite their power, long-context regulatory models face fundamental limitations that bound their current utility and define directions for future development.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nBefore reading about limitations, reflect: based on what you know about how these models are trained, what categories of failure would you expect? Consider training data, model architecture, and the biology of gene regulation.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nExpected failure categories include: training data bias (poor performance on underrepresented cell types and populations), finite context windows missing trans-acting factors and distant regulatory elements, inability to model 3D chromatin structure from linear sequence, and correlation-based learning that may not capture true causal mechanisms. The models learn associations between sequence and functional readouts but cannot distinguish which patterns are causally important versus merely correlated.\n\n\n\n\n\n\n16.7.1 Training Data Constraints\nYour patient has a variant in a regulatory element active in pancreatic beta cells. Will the model’s prediction be reliable? That depends entirely on whether beta cells were well-represented in training—and the model won’t tell you if they weren’t. Functional genomics data is biased in coverage, overrepresenting well-studied cell types (embryonic stem cells, K562, HepG2, lymphoblastoid cell lines) while leaving many tissue types and disease-relevant cell states poorly covered. Models trained on available data will perform better in represented contexts and may fail silently in underrepresented ones. Ancestry bias compounds the problem: most functional genomics studies derive from individuals of European descent, limiting the diversity of haplotypes and regulatory variants represented in training data. These data gaps are examined more comprehensively in Section 2.4.1 and Section 2.4.\nThese biases propagate to variant effect predictions. A variant in a regulatory element active primarily in pancreatic beta cells may receive poor predictions if beta cell data is sparse in training. A variant on a haplotype common in African populations but rare in Europeans may fall outside the model’s effective training distribution. Users must recognize that prediction confidence varies with representation in training data, a consideration that current models do not explicitly communicate. Chapter 12 examines how such biases can compromise model validity.\n\n\n16.7.2 Finite Context\nA transcription factor binding 50 megabases away can determine whether your gene of interest is expressed. A structural variant duplicating a distant super-enhancer can drive oncogene activation. No fixed window, however large, can see everything that matters. Even megabase-scale windows capture only local regulation. Trans-acting factors, three-dimensional contacts spanning multiple megabases, and whole-chromosome organization fall outside model context. Structural variants that rearrange large genomic segments, duplicate enhancers, or create novel fusion genes cannot be modeled within fixed-window architectures. The reference genome assumption underlying these models further limits their applicability to complex haplotypes and populations with substantial structural variation relative to the reference.\n\n\n16.7.3 Missing Three-Dimensional Context\nTwo enhancers equidistant from a promoter in linear sequence may have completely different effects—one brought into contact through a chromatin loop, the other sequestered in a different nuclear compartment. The genome is folded, but these models read it flat. Linear sequence models treat DNA as a one-dimensional string, but gene regulation occurs in three-dimensional nuclear space. Chromatin loops bring distal elements into proximity; nuclear compartmentalization segregates active and repressed regions; phase-separated condensates concentrate regulatory factors. While AlphaGenome predicts some contact features, current hybrid models do not fully integrate three-dimensional chromatin organization. The relationship between linear sequence, three-dimensional structure, and regulatory output remains incompletely captured. Chapter 20 examines models that explicitly address chromatin architecture.\n\n\n16.7.4 Correlation Versus Causation\nA model that perfectly predicts expression levels might still be completely wrong about why genes are expressed. It could be learning batch effects, GC content, or any signal correlated with the training labels without understanding regulatory mechanism. How do we know when a model has learned biology versus artifacts? Hybrid models learn correlations between sequence and functional readouts, not causal mechanisms. A variant might receive a high predicted effect score because it disrupts a motif correlated with expression in training data, not because the motif causally drives expression. Attribution methods can identify which sequence features contribute to predictions, but attribution is not validation. High-confidence predictions require experimental confirmation through approaches like massively parallel reporter assays, CRISPR perturbation, or allelic series analysis.\n\n\n\n\n\n\nKey Insight\n\n\n\nPrediction accuracy and mechanistic understanding are different things. A model can achieve high correlation with experimental measurements by learning any predictive signal, even confounded ones. The fact that a model accurately predicts expression does not mean it has learned the correct causal mechanism.\n\n\n\n\n16.7.5 Interpretability Challenges\nWhen Enformer predicts that a variant disrupts gene expression, can we ask why? What motif was disrupted? What regulatory logic was broken? With hundreds of millions of parameters, the answer is rarely straightforward. The scale of these models (hundreds of millions of parameters) makes mechanistic interpretation difficult. Attention patterns provide some insight into which positions the model considers related, but attention weights are not guaranteed to reflect the model’s actual computational strategy. Attribution methods (saliency maps, integrated gradients) can highlight important input positions, but the features the model constructs from those positions remain opaque. Chapter 24 examines these interpretability methods and their limitations in detail.\n\n\n\n\n\n\n\n\nTraining data bias limits generalization\n\n\n\n\n\n\n\nLinear models miss 3D chromatin context\n\n\n\n\n\n\n\n\n\nCorrelation vs. causation in predictions\n\n\n\n\n\n\n\nContext limitations miss trans-acting effects\n\n\n\n\n\n\nFigure 16.7: Limitations of long-context regulatory models. (A) Training data bias: cell lines like K562 and HepG2 dominate ENCODE; disease-relevant cell states are underrepresented. Predictions may be unreliable for untrained contexts. (B) Missing 3D context: models process linear sequence, but chromatin folds into loops and compartments that bring distant elements into physical proximity. Long-range effects may occur through 3D contacts that linear context misses. (C) Correlation vs. causation: high predicted effect may reflect training correlations rather than true causal relationships. Validation requires experimental perturbation. (D) Finite context: even 1Mb windows cannot capture trans-acting factors encoded elsewhere in the genome, nor structural variants spanning megabases.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html#sec-ch16-foundation-models",
    "href": "part_3/p3-ch16-regulatory.html#sec-ch16-foundation-models",
    "title": "16  Regulatory Models",
    "section": "16.8 Relationship to Foundation Models",
    "text": "16.8 Relationship to Foundation Models\nLong-context regulatory models occupy an interesting position in the genomic foundation model landscape. They share key characteristics with foundation models: large scale, broad training data, strong performance across tasks, and utility as feature extractors for downstream applications. Yet they differ from self-supervised DNA language models (Chapter 14) in their heavy reliance on supervised, task-specific training signals.\nEnformer and its descendants can be viewed as highly specialized foundation models, pretrained on the specific task of regulatory prediction and adaptable to related applications. Their representations encode regulatory logic learned from functional genomics supervision, complementing the sequence patterns learned by self-supervised models from raw DNA. In practice, the two approaches may prove most powerful in combination: self-supervised models provide sequence representations from evolutionary context, while supervised regulatory models provide representations from functional genomics context. Integrating these representations for tasks like variant effect prediction is an active area of development, explored further in Chapter 17.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider the difference between Enformer (supervised on functional genomics data) and DNA language models like Nucleotide Transformer (self-supervised on sequence alone). What types of regulatory patterns might each approach learn well? What might each miss?\n\n\nFrom a practical standpoint, hybrid regulatory models remain among the most directly useful genomic deep learning systems for variant interpretation. They provide quantitative, tissue-specific predictions for regulatory variants, outperform short-context alternatives on distal regulatory elements, and integrate naturally into variant prioritization workflows. Their limitations are real but understood; their strengths are substantial and empirically validated.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch16-regulatory.html#sec-ch16-prediction-explanation",
    "href": "part_3/p3-ch16-regulatory.html#sec-ch16-prediction-explanation",
    "title": "16  Regulatory Models",
    "section": "16.9 Prediction Without Explanation",
    "text": "16.9 Prediction Without Explanation\nLong-range regulatory prediction from sequence is tractable. Enformer established that hybrid convolutional neural network (CNN)-transformer architectures could span 200 kilobases and predict expression-related chromatin features. Borzoi extended coverage to the full transcriptome with improved quantitative accuracy. AlphaGenome unified multiple regulatory modalities at megabase scale, predicting chromatin accessibility, histone modifications, transcription factor binding, and gene expression from a single architecture. Each generation captures more of the regulatory landscape with greater fidelity to experimental measurements.\nYet these models predict regulatory outcomes without explaining regulatory mechanism. They learn that certain sequence patterns associate with certain expression levels, but they do not represent enhancer-promoter contacts, transcription factor cascades, or the causal chain from sequence to phenotype. The attention patterns that span long distances may correspond to genuine regulatory interactions or may reflect confounded sequence features that happen to predict expression. Interpretability methods (Chapter 24) can probe what patterns models have learned, but high prediction accuracy does not guarantee mechanistic insight.\nThis distinction shapes how regulatory model predictions should be used. For variant effect prediction (Chapter 17), regulatory models provide one input among several: they predict whether a variant alters chromatin accessibility or expression, while protein language models (Chapter 15) assess coding consequences and evolutionary models quantify constraint. The clinical integration of these signals (Chapter 28) requires understanding what each model contributes and where each is likely to fail. Regulatory models excel at predicting noncoding variant effects when the relevant cell type is represented in training data; they struggle with cell types absent from training and with variants acting through mechanisms not captured by the output tracks they predict.\n\n\n\n\n\n\nApply Your Knowledge\n\n\n\nYou need to predict the regulatory impact of a variant located 50kb from the nearest gene. Based on what you’ve learned in this chapter, which model architecture would you choose and why? What input features would be most important?\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhy do short-context models fundamentally fail at modeling mammalian gene regulation? What is the scale of the problem (provide specific distance ranges)?\nExplain the hybrid CNN-transformer architecture strategy. How does Enformer achieve a 200kb context window without requiring quadratic attention over 200,000 positions?\nCompare Enformer, Borzoi, and Sei in terms of what they predict and what applications they are best suited for. What are the key differences in their outputs?\nWhat does it mean that regulatory models provide “prediction without explanation”? Why is high accuracy at predicting expression not the same as understanding regulatory mechanism?\nIdentify three critical limitations of current regulatory models that prevent them from capturing the full complexity of gene regulation. For each, explain what biological information is missing.\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nScale mismatch: Short-context models have receptive fields of 1-2 kb or at most tens of kb through dilated convolutions, but mammalian enhancers are typically 20-50 kb from their target promoters, with substantial fractions exceeding 100 kb. Topologically associating domains (TADs) range from hundreds of kb to several megabases. A model with a 2kb window can see only 1% of a 200kb regulatory neighborhood, treating distant enhancers as if they don’t exist.\nHybrid architecture efficiency: Enformer uses a convolutional stem to compress 200kb of sequence down to ~1,500 tokens (each representing ~128bp bins), reducing the sequence length by approximately 100×. This makes attention computationally tractable because attention scales quadratically with sequence length—operating on 1,500 positions instead of 200,000 reduces memory requirements by roughly 17,000-fold. The convolutions preserve local motif information while discarding positional precision that biology doesn’t require, then the transformer attention enables direct information flow between any positions in the compressed representation.\nModel output comparison: Enformer predicts ~5,000 continuous chromatin and transcription tracks (DNase, histone marks, CAGE) across cell types, best for quantitative tissue-specific predictions. Borzoi predicts full RNA-seq coverage profiles at nucleotide resolution, best for splicing effects and post-transcriptional regulation. Sei predicts probabilities across 40 discrete regulatory state classes (promoter-like, enhancer-like, etc.), best for interpretable regulatory identity changes and compressed variant summaries. AlphaGenome unifies chromatin, RNA, and 3D contact predictions at megabase scale via API.\nPrediction vs. explanation: High accuracy means the model has learned sequence patterns that correlate with expression measurements, but correlation doesn’t imply the model understands causal mechanisms. A variant might score high because it disrupts a motif correlated with expression in training data rather than because that motif causally drives expression. The model doesn’t represent enhancer-promoter contacts, transcription factor cascades, or the mechanistic chain from sequence to phenotype—it learns statistical associations that predict outcomes without explaining how regulation actually works.\nThree critical limitations: (1) Training data bias—models overrepresent well-studied cell types (K562, HepG2, lymphoblastoid lines) and European ancestry, causing poor performance on underrepresented tissues and populations where prediction confidence is unknown. (2) Finite context windows—even 1Mb contexts cannot capture trans-acting factors encoded elsewhere in the genome, structural variants spanning multiple megabases, or whole-chromosome organization effects. (3) Missing 3D chromatin structure—models treat DNA as linear sequence but regulation occurs in 3D nuclear space where chromatin loops bring distant elements into physical proximity; current models don’t fully integrate how linear sequence determines 3D organization and how that structure mediates regulatory function.\n\n\n\n\n\n\nKey Concepts:\n\nLong-range regulation problem: Mammalian gene regulation operates across 20-100+ kb, beyond the reach of short-context models\nHybrid architectures: CNN compression + transformer attention balances computational efficiency with long-range modeling\nMulti-task learning: Training on diverse functional genomics assays encourages learning of generalizable regulatory features\nCross-species training: Human and mouse data together emphasize evolutionarily conserved patterns\n\nModel Comparison:\n\n\n\nModel\nContext\nKey Outputs\nAccess\n\n\n\n\nEnformer\n200 kb\nChromatin, CAGE (~5,000 tracks)\nOpen weights\n\n\nBorzoi\n200+ kb\nRNA-seq coverage, splicing\nOpen weights\n\n\nSei\n~4 kb\n40 regulatory class probabilities\nOpen weights\n\n\nAlphaGenome\n~1 Mb\nChromatin + RNA + contacts\nAPI only\n\n\n\nCritical Limitations:\n\nTraining data bias toward well-studied cell types and European ancestry\nFinite context misses trans-acting factors and structural variants\nLinear sequence representation ignores 3D chromatin organization\nCorrelation-based learning does not guarantee causal understanding\nModel scale (hundreds of millions of parameters) impedes interpretation\n\nClinical Applications:\n\nVariant effect prediction for noncoding/regulatory variants\nTissue-specific expression predictions\nIntegration with classical VEP methods and protein language models\nSee Chapter 17 for detailed variant interpretation workflows\n\nLooking Forward:\n\nChapter 17: How to combine regulatory model predictions with other evidence\nChapter 20: Models that explicitly address chromatin architecture\nChapter 24: Methods for probing what these models learn\nChapter 28: Clinical application in diagnostic workflows\n\n\n\n\n\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nGasperini, Molly, Andrew J. Hill, José L. McFaline-Figueroa, Beth Martin, Seungsoo Kim, Melissa D. Zhang, Dana Jackson, et al. 2019. “A Genome-Wide Framework for Mapping Gene Regulation via Cellular Genetic Screens.” Cell 176 (1): 377–390.e19. https://doi.org/10.1016/j.cell.2018.11.029.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regulatory Models</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html",
    "href": "part_3/p3-ch17-vep-fm.html",
    "title": "17  Variant Effect Prediction",
    "section": "",
    "text": "17.1 Foundation Model Paradigm for Variant Interpretation\nClassical variant effect predictors operate by aggregating hand-crafted features: conservation scores computed from multiple sequence alignments, amino acid property changes, protein domain annotations, and regulatory marks at genomic loci (Chapter 4). Methods like CADD train machine learning models to distinguish pathogenic from benign variants using these features, achieving useful discrimination but ultimately limited by what features the developers chose to include. When a variant falls in a region poorly covered by existing annotations, classical methods have little to offer.\nFoundation models invert this relationship. Rather than engineering features, they learn representations from raw sequence data during pretraining, then apply those representations to variant interpretation. A protein language model trained to predict masked amino acids implicitly learns which substitutions violate evolutionary constraints. A DNA language model trained to predict nucleotides in genomic context learns which changes disrupt sequence grammar. The representations encode information about structure, function, and constraint that was never explicitly labeled during training.\nThis paradigm shift has practical consequences. Coverage extends to any variant in any gene, not just those with extensive prior annotation. Representations capture subtle patterns (co-evolution between distant residues, context-dependent motif strength) that resist manual feature engineering. Transfer learning enables rapid adaptation to new tasks and variant classes, with the specific strategies detailed in Chapter 9. The cost is interpretability: understanding why a foundation model assigns a particular score requires specialized analysis techniques rather than simple inspection of feature weights (Chapter 24).\nThree architectural families dominate current VEP applications. Protein language models (Chapter 15) encode amino acid sequences and score missense variants by measuring likelihood changes. DNA language models (Chapter 14) operate on nucleotide sequences and can score variants of any type. Regulatory models (Chapter 16) predict molecular phenotypes (chromatin accessibility, gene expression, splicing) and score variants by their predicted impact on these phenotypes. The strongest-performing systems combine elements from multiple families.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-fm-paradigm",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-fm-paradigm",
    "title": "17  Variant Effect Prediction",
    "section": "",
    "text": "Key Insight: Evolution as a Massive Experiment\n\n\n\nFoundation models exploit a profound insight: evolution has already conducted billions of “experiments” testing which sequence variants are compatible with life. Protein sequences that survived natural selection represent the “passing” experiments; variants never observed in homologous proteins failed these tests. By learning to predict which amino acids or nucleotides are probable at each position, foundation models implicitly learn what evolution has permitted and, by contrast, what it has rejected. This is why a model trained only on masked token prediction can score variant pathogenicity without ever seeing disease labels.\n\n\n\n\n\n\n\nTable 17.1: Comparison of foundation model approaches for variant effect prediction. Each approach offers distinct advantages depending on the variant type and desired output.\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nInput\nVariant Types\nKey Methods\nStrengths\nLimitations\n\n\n\n\nProtein LM\nAmino acid sequence\nMissense\nESM-1v, EVE, AlphaMissense\nCaptures evolutionary constraint, structural context\nCoding only, requires translation\n\n\nDNA LM\nNucleotide sequence\nAll (SNV, indel, noncoding)\nGPN-MSA, Evo 2\nGenome-wide coverage\nLess protein-specific information\n\n\nRegulatory\nDNA + epigenomic context\nNoncoding, regulatory\nEnformer, Sei, AlphaGenome\nMechanistic predictions\nCell-type specificity, calibration\n\n\nStructure-aware\nSequence + 3D structure\nMissense\nAlphaMissense\nExplains why constraint exists\nRequires structure prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero-shot scoring from PLM likelihoods\n\n\n\n\n\n\n\nLinear probing with frozen embeddings\n\n\n\n\n\n\n\n\n\nFull fine-tuning for maximum performance\n\n\n\n\n\n\n\nMulti-modal integration of sequence and structure\n\n\n\n\n\n\nFigure 17.1: Foundation model paradigms for variant effect prediction. (A) Zero-shot scoring uses pretrained likelihoods directly, requiring no task-specific data but limited to what pretraining captured. (B) Linear probing adds a simple classifier on frozen embeddings, requiring minimal labeled data. (C) Full fine-tuning updates all parameters, achieving best performance but requiring substantial labeled data. (D) Multi-modal integration combines sequence (evolutionary) and structure information for comprehensive variant assessment. The appropriate paradigm depends on available labeled data and whether structure contributes to the variant’s effect mechanism.\n\n\n\n\n17.1.1 Zero-Shot and Supervised Approaches\nFoundation model VEP methods divide into two paradigms. Zero-shot approaches apply pretrained models directly without task-specific training: ESM-1v scores variants by comparing amino acid likelihoods, requiring no pathogenicity labels. Think of it like a spell-checker that flags words it has never seen in well-edited text—it does not need a dictionary of “misspellings” because any word that looks unusual compared to normal usage stands out as potentially wrong. Similarly, the model’s pretraining objective (masked token prediction) implicitly teaches which substitutions violate evolutionary constraints by learning what “well-edited” (evolutionarily tested) sequences look like. Supervised approaches like AlphaMissense add task-specific training layers and optimize explicitly for pathogenicity prediction using labeled examples.\n\n\n\n\n\n\nStop and Think: Zero-Shot vs. Supervised Trade-offs\n\n\n\nBefore reading further, consider: If zero-shot methods avoid the biases present in labeled training data, why would anyone use supervised approaches at all? What advantages might supervised fine-tuning provide, and when might those advantages outweigh the risk of inheriting label biases?\n\n\nThe choice involves tradeoffs. Zero-shot methods avoid label bias entirely; they cannot learn to recapitulate existing predictor scores because they never see those scores during training. Supervised methods achieve stronger discrimination when high-quality labels exist but risk inheriting biases from training data. Zero-shot approaches generalize more reliably to novel proteins outside training distributions; supervised methods may overfit to well-studied gene families. In practice, the strongest current systems (AlphaMissense, popEVE) combine foundation model representations with some supervised adaptation, attempting to capture benefits of both paradigms.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-protein-vep",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-protein-vep",
    "title": "17  Variant Effect Prediction",
    "section": "17.2 Protein-Based Variant Effect Prediction",
    "text": "17.2 Protein-Based Variant Effect Prediction\nMissense variants (single amino acid substitutions) account for approximately half of known pathogenic variants in ClinVar, making protein-level prediction a central challenge [Citation Needed]. Foundation model approaches exploit a simple insight: evolution has already tested billions of amino acid substitutions across millions of years; variants that repeatedly survive natural selection are likely tolerable, while those never observed in homologous proteins likely disrupt function.\n\n17.2.1 Zero-Shot Scoring with Protein Language Models\nThe simplest foundation model approach to missense VEP requires no task-specific training. A protein language model trained on masked token prediction assigns probabilities to each amino acid at each position given surrounding context. Variant effect scores emerge from comparing the probability of the reference amino acid to the probability of the variant amino acid.\nESM-1v operationalizes this approach using the ESM-2 architecture fine-tuned for single-sequence variant effect prediction (Meier et al. 2021). For a variant substituting amino acid \\(a_\\text{ref}\\) with \\(a_\\text{var}\\) at position i, the score is computed as:\n\\[\\Delta \\text{LLR} = \\log P(a_\\text{var} | \\text{context}) - \\log P(a_\\text{ref} | \\text{context})\\]\n\n\n\n\n\n\nWorked Example: Computing LLR Variant Scores\n\n\n\nConsider a missense variant in BRCA1 that substitutes leucine (L) with proline (P) at position 1780.\nStep 1: The protein language model examines the sequence context around position 1780 and predicts probabilities for each amino acid at that position:\n\n\n\nAmino Acid\nProbability\n\n\n\n\nLeucine (L, reference)\n0.35\n\n\nProline (P, variant)\n0.02\n\n\nIsoleucine (I)\n0.28\n\n\nOther residues\n0.35\n\n\n\nStep 2: Compute the log-likelihood ratio: \\[\\Delta \\text{LLR} = \\log(0.02) - \\log(0.35) = -3.91 - (-1.05) = -2.86\\]\nStep 3: Interpret the score: A score of -2.86 indicates the variant amino acid (proline) is substantially less probable than the reference (leucine) given the evolutionary context. The model has learned that this position strongly favors aliphatic residues (L, I, V), and the rigid proline is unexpected—consistent with a functionally important position where proline would disrupt structure.\n\n\nNegative scores indicate that the variant amino acid is less probable than reference in learned evolutionary context, suggesting potential deleteriousness. The model sees only the single query sequence, not multiple sequence alignments, yet achieves discrimination competitive with alignment-based methods on deep mutational scanning benchmarks. The emergence of this capability from masked token prediction, without explicit training on variant effects, exemplifies the emergent biological knowledge discussed in ?sec-ch12-emergent-knowledge.\n\n\n\n\n\n\nComputing Variant Likelihoods from Language Models\n\n\n\nLanguage models assign probabilities to sequences, but extracting variant effect scores requires specific computational strategies. Different approaches trade off between computational cost, theoretical justification, and empirical performance.\nMasked marginal likelihood (used by ESM-1v): Mask the position of interest and compute the probability of each amino acid given the unmasked context. The variant score is the log probability ratio of variant versus reference amino acid:\n\\[\\text{Score} = \\log P(a_\\text{var} | \\mathbf{x}_{-i}) - \\log P(a_\\text{ref} | \\mathbf{x}_{-i})\\]\nwhere \\(\\mathbf{x}_{-i}\\) denotes the sequence with position i masked. This approach requires a single forward pass per variant position. It directly measures how surprising each amino acid is given the local and global context the model learned during pretraining.\nPseudo-likelihood: Sum the masked marginal log-probabilities across all positions in the sequence:\n\\[\\text{PLL}(\\mathbf{x}) = \\sum_{i=1}^{L} \\log P(x_i | \\mathbf{x}_{-i})\\]\nThe variant effect is the difference in pseudo-likelihood between mutant and wild-type sequences. This captures how the mutation affects sequence probability globally, not just at the mutated position, and may detect compensatory effects, but requires L forward passes (one per position).\nAutoregressive likelihood (for GPT-style models): Compute the probability of generating the sequence left-to-right:\n\\[\\text{LL}(\\mathbf{x}) = \\sum_{i=1}^{L} \\log P(x_i | x_1, \\ldots, x_{i-1})\\]\nThis provides a proper generative probability but creates asymmetry: mutations early in the sequence affect more downstream predictions than mutations late in the sequence. Bidirectional models avoid this issue.\nMasked language model pseudo-perplexity: Average negative log-probability across positions, measuring how “surprising” the sequence appears to the model. Lower perplexity indicates more natural sequences.\nPractical considerations:\n\nMasked marginal is computationally cheapest (one forward pass) and works well for local effects\nPseudo-likelihood is more thorough but expensive; often approximated by sampling positions\nAutoregressive likelihood provides proper probabilities but with positional asymmetry\nMultiple scoring strategies often correlate; choice depends on computational budget and task\n\nMost protein language model variant scoring uses masked marginal likelihood due to its efficiency and strong empirical performance. DNA language models use analogous strategies adapted for nucleotide sequences and longer contexts.\n\n\nThis zero-shot capability reflects what protein language models learn during pretraining: structural constraints (buried positions are hydrophobic), functional constraints (active sites are conserved), and co-evolutionary patterns (compensating mutations at contacting residues). The model has never seen pathogenicity labels, yet its predictions correlate with disease association because evolution and disease share underlying biology.\n\n\n17.2.2 Alignment-Based Models: EVE and popEVE\nAn alternative approach explicitly models multiple sequence alignments rather than relying on implicit evolutionary information in single-sequence representations. EVE (Evolutionary Model of Variant Effect) fits a variational autoencoder to the MSA for each protein, learning a generative model that captures position-specific and pairwise constraints (Frazer et al. 2021). Variant scores derive from the change in sequence probability under this model.\nThe EVE architecture consists of an encoder that maps sequences to a latent space and a decoder that reconstructs sequences from latent representations. Training maximizes a lower bound on sequence likelihood across the MSA. For variant scoring, EVE computes the log-likelihood ratio between mutant and wild-type sequences, capturing how surprising the substitution appears given the evolutionary record for that specific protein.\npopEVE extends this framework with improved training procedures and explicit modeling of population allele frequencies (Orenbuch et al. 2025). By incorporating frequency information, popEVE better separates rare deleterious variants from common benign polymorphisms. The model achieves strong performance on ClinVar classification while providing uncertainty estimates through ensemble disagreement.\nThe tradeoff between single-sequence and MSA-based approaches involves coverage versus depth. ESM-1v scores any protein sequence without requiring alignment construction. EVE provides stronger performance when high-quality MSAs are available but cannot score proteins lacking sufficient homologs. For well-studied protein families with deep evolutionary sampling, MSA-based methods remain competitive; for orphan proteins or rapidly evolving sequences, single-sequence models offer the only foundation model option.\n\n\n\n\n\n\nKnowledge Check: Single-Sequence vs. MSA-Based Models\n\n\n\nA patient carries a missense variant in OBSCN, an extremely large gene encoding a protein with few characterized homologs. The gene has only 12 sequences in its MSA, most from closely related mammals.\n\nWould ESM-1v or EVE be more appropriate for scoring this variant? Why?\nWhat fundamental limitation does this example illustrate about alignment-based approaches?\nIf the variant were in BRCA1 instead (which has hundreds of homologs), would your answer change?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nESM-1v would be more appropriate because it works on single sequences and doesn’t require a high-quality MSA. This illustrates that alignment-based methods fail for orphan proteins or those with sparse evolutionary sampling, where MSA-based approaches lack sufficient data. For BRCA1 with hundreds of homologs, EVE might perform better because the deep evolutionary sampling provides rich coevolution signals that EVE can exploit, though ESM-1v would still provide reliable predictions.\n\n\n\n\n\n\n\n17.2.3 AlphaMissense: Structure-Informed Pathogenicity Prediction\nAlphaMissense represents the current state of the art for proteome-wide missense pathogenicity prediction, combining protein language model representations with structural information from AlphaFold2 (Cheng et al. 2023). The system provides precomputed scores for 71 million possible missense variants across the human proteome, enabling instant lookup for any variant in any protein-coding gene.\nThe architecture integrates multiple information sources. Sequence representations come from a protein language model encoding the wild-type sequence and mutation position. Structural representations derive from AlphaFold2 predictions, capturing local geometry (secondary structure, solvent accessibility, packing density) and longer-range contacts. A neural network combines these representations to produce a pathogenicity probability between 0 and 1.\nWhy does combining sequence and structure information improve predictions beyond what either provides alone? Sequence-based evolutionary constraint tells you that a position is important, but structure explains why. Consider two equally conserved positions: one is buried in the hydrophobic core where any polar substitution destabilizes the fold, while the other forms a catalytic residue where even conservative substitutions abolish enzyme activity. A charge-preserving substitution (Asp to Glu) might be tolerable at a structural position but devastating at the catalytic one. Structure reveals these mechanistic differences that sequence conservation alone cannot distinguish. Similarly, two surface-exposed positions might show identical evolutionary constraint, but one forms a protein-protein interaction interface while the other faces solvent. Structural context disambiguates cases where sequence statistics are similar but mechanisms—and therefore tolerance for variation—differ substantially.\nTraining uses a carefully constructed dataset that avoids the circularity plaguing earlier predictors. Rather than training on ClinVar labels (which themselves derive from computational predictions), AlphaMissense uses population frequency as a proxy for pathogenicity: variants common in gnomAD are likely benign, while variants absent from large population samples and observed in disease contexts are likely pathogenic. This approach reduces the risk of learning features that simply recapitulate existing predictor scores.\n\n\n\n\n\n\nKey Insight: Why Structure Matters for Variant Interpretation\n\n\n\nProtein language models capture that a position is evolutionarily constrained, but structural information explains why. A buried hydrophobic residue is constrained because substituting a charged amino acid would destabilize the fold. An active site residue is constrained because even subtle changes disrupt catalysis. An interface residue is constrained because substitutions abolish protein-protein interactions. By incorporating AlphaFold2 structural features, AlphaMissense can distinguish variants at constrained positions where the constraint arises from different mechanisms, improving prediction for cases where sequence constraint alone is ambiguous.\n\n\nCalibration receives explicit attention. Raw model outputs undergo isotonic regression calibration against held-out ClinVar variants, ensuring that predicted probabilities correspond to observed pathogenic proportions (Section 23.2). A score of 0.8 should mean that 80% of variants with similar scores are pathogenic, enabling meaningful clinical interpretation. AlphaMissense reports calibrated scores along with discrete classifications (likely pathogenic, likely benign, uncertain) at thresholds chosen to achieve specific precision targets.\nPerformance on independent benchmarks substantially exceeds classical predictors. On deep mutational scanning datasets (where experimental fitness measurements provide ground truth independent of clinical labels), AlphaMissense achieves correlations of 0.5 to 0.7 depending on the assay, compared to 0.3 to 0.5 for CADD or PolyPhen-2 [Citation Needed]. On ClinVar expert-reviewed variants held out from training, AlphaMissense achieves auROC values above 0.9, representing a meaningful improvement over the 0.85 to 0.88 typical of classical methods [Citation Needed].\nThe structural component proves essential for this performance. Ablation experiments removing AlphaFold2 features degrade performance substantially, particularly for variants at protein-protein interfaces and buried core positions where local geometry determines functional impact. The protein language model captures evolutionary constraint; structural information explains why that constraint exists.\n\n\n\n\n\n\n\n\nAlphaMissense integrates evolution and structure\n\n\n\n\n\n\n\nTraining strategy avoids circularity\n\n\n\n\n\n\n\n\n\nStructural context determines pathogenicity\n\n\n\n\n\n\n\nProteome-wide classification of 71 million variants\n\n\n\n\n\n\nFigure 17.2: AlphaMissense: integrating evolution and structure for variant pathogenicity. (A) Architecture combines ESM evolutionary embeddings with AlphaFold2 structural features. (B) Training avoids ClinVar circularity by using population frequency as a pathogenicity proxy—common gnomAD variants are presumed benign, rare variants in disease-associated contexts are presumed pathogenic. (C) Pathogenicity scores vary appropriately by structural context: core and active site variants score highest, surface variants score lowest. (D) Proteome-wide application classifies 71 million possible human missense variants, creating the largest pathogenicity prediction resource.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-dna-vep",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-dna-vep",
    "title": "17  Variant Effect Prediction",
    "section": "17.3 DNA-Based Variant Effect Prediction",
    "text": "17.3 DNA-Based Variant Effect Prediction\nApproximately 98% of the human genome lies outside protein-coding regions, yet noncoding variants contribute substantially to disease risk through effects on gene regulation, splicing, and genome stability [Citation Needed]. Predicting the impact of these variants requires models that operate directly on DNA sequence rather than translated protein.\n\n17.3.1 Splice Variant Prediction with SpliceAI\nSplicing variants illustrate both the promise and current limitations of deep learning for noncoding VEP. Approximately 10% of pathogenic variants in ClinVar act through splicing mechanisms, disrupting the precise excision of introns from pre-mRNA [Citation Needed]. Classical approaches relied on position weight matrices matching consensus splice site sequences, achieving limited sensitivity for variants outside the core GT-AG dinucleotides.\nSpliceAI applies the dilated convolutional architecture introduced in Chapter 6 to predict splice site usage from raw DNA sequence (Jaganathan et al. 2019). The architecture processes 10,000 nucleotides of context through 32 residual blocks with dilated convolutions (dilation rates increasing from 1 to 128), enabling the receptive field to span several kilobases while maintaining nucleotide resolution. Output heads predict splice donor probability, splice acceptor probability, and junction usage at each position.\nFor variant effect prediction, SpliceAI compares predictions between reference and alternate sequences. The delta score quantifies the change in splice site probability, with positive values indicating gained splice sites and negative values indicating lost sites. Scores exceeding 0.2 correlate with experimentally validated splicing changes; scores above 0.5 have high specificity for pathogenic splicing variants [Citation Needed].\nClinical deployment has validated SpliceAI’s utility. Illumina integrated the model into their clinical interpretation pipeline, and multiple diagnostic laboratories use SpliceAI scores as supporting evidence for ACMG classification. The architectural innovations that enable this performance, including the dilated convolution strategy for expanding receptive fields, are detailed in Section 6.5. The model identifies pathogenic splicing variants missed by classical methods, particularly deep intronic variants that create novel splice sites through cryptic activation.\nLimitations reflect the model’s training data. SpliceAI learned from annotated transcripts representing major isoforms in common tissues. Tissue-specific alternative splicing, rare isoforms, and developmental stage-specific patterns fall outside the training distribution. The model also does not capture downstream consequences: whether a predicted splicing change produces a functional protein, triggers nonsense-mediated decay, or has no phenotypic effect requires additional analysis.\n\n\n17.3.2 Regulatory Variant Prediction with Enformer\nWhile SpliceAI addresses one specific noncoding mechanism, regulatory variants that alter enhancer activity, promoter function, or chromatin organization require different approaches. Enformer (Chapter 16) predicts multiple molecular phenotypes (histone modifications, transcription factor binding, chromatin accessibility, gene expression) from 196,608 base pairs of DNA sequence, providing a substrate for regulatory VEP (Ž. Avsec et al. 2021).\nVariant effect prediction with Enformer compares predicted tracks between reference and alternate sequences. For a variant in an enhancer, the model might predict reduced H3K27ac signal and decreased CAGE expression at the target promoter. These molecular predictions can be aggregated into variant effect scores, with larger predicted changes indicating greater functional impact.\nSeveral challenges complicate Enformer-based VEP. The model predicts relative effects (fold changes in predicted signal) rather than absolute deleteriousness. Calibrating these predictions against pathogenicity labels requires additional supervised training. Cell-type specificity adds complexity: a variant might strongly affect predictions in cardiac tissue while showing no effect in liver, requiring prior knowledge of relevant tissues for clinical interpretation.\nSei extends this approach by learning a regulatory vocabulary: clusters of predicted effects that correspond to interpretable categories like “active promoter,” “strong enhancer,” or “CTCF binding site” (Chen et al. 2022). Variant scores reflect shifts between these categories, providing more interpretable outputs than raw track changes. A variant that converts an enhancer prediction to a quiescent state has clearer implications than one that reduces H3K27ac by 0.3 log-fold.\n\n\n\n\n\n\nStop and Think: Mechanism vs. Pathogenicity\n\n\n\nEnformer can predict that a variant reduces enhancer activity, and SpliceAI can predict that a variant creates a cryptic splice site. But neither model directly predicts whether these molecular changes cause disease.\nConsider: A variant in an enhancer might reduce expression of the target gene by 30%. Under what circumstances would this reduction be: - Clearly pathogenic? - Clearly benign? - Uncertain in its clinical significance?\nThis exercise illustrates a fundamental gap between mechanistic prediction (what the variant does molecularly) and clinical prediction (whether it causes disease).\n\n\n\n\n17.3.3 DNA Language Models: GPN-MSA and Evo 2\nDNA language models provide an alternative to phenotype prediction: scoring variants by how unexpected they appear in learned sequence context, analogous to protein language model approaches for missense variants.\nGPN-MSA combines DNA language modeling with multi-species sequence alignments (Benegas et al. 2024). Building on the GPN approach introduced in ?sec-ch11-gpn, the model processes aligned sequences from dozens of vertebrate species, learning which positions are conserved and which tolerate variation. Variant scores derive from likelihood ratios: how much less probable is the variant allele compared to reference given the alignment context? This approach captures deep evolutionary constraint missed by simple conservation scores while providing genome-wide coverage including noncoding regions.\nEvo 2 pushes context length to approximately one megabase, enabling single models to capture local motifs and long-range dependencies simultaneously (Brixi et al. 2025). The StripedHyena architecture provides computational efficiency at this scale through state-space-based sequence modeling rather than quadratic attention, as detailed in ?sec-ch11-evo2 and Chapter 7. Training on diverse genomes across the tree of life teaches general principles of sequence organization that transfer to human variant interpretation.\nZero-shot variant scoring with Evo 2 follows the standard likelihood ratio approach. Initial benchmarks show performance competitive with conservation-based scores for coding variants and potentially superior performance for noncoding variants where local sequence context matters more than position-specific conservation. The extremely long context enables modeling of effects mediated by distal elements, though whether this theoretical capability translates to improved VEP remains under investigation.\n\n\n17.3.4 AlphaGenome: Unified Multi-Omic Variant Effect Prediction\nAlphaGenome (?sec-ch13-alphagenome) represents the most ambitious current attempt at comprehensive VEP, predicting multiple molecular phenotypes from megabase-scale DNA sequence and using those predictions to assess variant effects across modalities (Z. Avsec, Latysheva, and Cheng 2025).\nVariant effect prediction with AlphaGenome provides mechanistically interpretable outputs. A promoter variant might show reduced accessibility and decreased expression prediction. An enhancer variant might show weakened contact with its target promoter in addition to reduced local histone acetylation. A splicing variant triggers SpliceAI-like splice site changes while also affecting regulatory track predictions near the affected exon.\nThe multi-omic approach enables variant prioritization that considers multiple mechanisms simultaneously. A variant in a regulatory element that affects accessibility, expression, and chromatin contacts represents stronger evidence than one affecting only a single predicted phenotype. Conversely, variants with no predicted effect across modalities can be deprioritized despite proximity to disease genes.\nPractical deployment involves tradeoffs. Evaluating a single variant requires forward passes through the full model, incurring substantial computational cost compared to lookup-based approaches like AlphaMissense. The model may exhibit overconfidence when extrapolating beyond training cell types. Calibrating multi-dimensional predictions into single pathogenicity scores remains an open problem. These constraints position AlphaGenome as a tool for detailed mechanistic investigation of prioritized variants rather than genome-wide screening.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-combining-evidence",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-combining-evidence",
    "title": "17  Variant Effect Prediction",
    "section": "17.4 Combining Evidence Across Modalities",
    "text": "17.4 Combining Evidence Across Modalities\nNo single model addresses all variant types and mechanisms. Missense variants in protein-coding regions call for protein-level predictors; splicing variants require splice-specific models; regulatory variants benefit from long-context DNA models. Practical VEP workflows combine multiple predictors to achieve comprehensive coverage.\n\n17.4.1 Integration Strategies\nThe simplest integration approach applies different models to different variant classes. Missense variants receive AlphaMissense scores; synonymous and intronic variants near splice sites receive SpliceAI scores; promoter and enhancer variants receive Enformer or AlphaGenome predictions. This modular strategy ensures that each variant type receives predictions from an appropriate model.\n\n\n\nTable 17.2: Model selection guidance by variant type. The choice depends on variant location and suspected mechanism.\n\n\n\n\n\n\n\n\n\n\n\nVariant Type\nPrimary Model(s)\nSecondary/Supporting\nKey Considerations\n\n\n\n\nMissense\nAlphaMissense\nESM-1v, EVE\nCheck for splicing effects if near exon boundary\n\n\nSynonymous\nSpliceAI\nEnformer (regulatory)\nOften dismissed but can affect splicing\n\n\nSplice site (canonical)\nSpliceAI\n\nVery high pathogenicity for loss of function genes\n\n\nDeep intronic\nSpliceAI\nEnformer, GPN-MSA\nLook for cryptic splice site activation\n\n\nPromoter/5’UTR\nEnformer, AlphaGenome\nGPN-MSA\nConsider tissue specificity\n\n\nEnhancer\nEnformer, Sei\nAlphaGenome\nNeed prior knowledge of target genes\n\n\nStructural variants\nLimited coverage\n\nGap in current foundation model capabilities\n\n\n\n\n\n\nMore sophisticated integration aggregates scores across models for the same variant. A missense variant might receive both AlphaMissense (protein impact) and Enformer (regulatory impact, relevant if the codon overlaps a regulatory element) predictions. Combining these requires decisions about weighting and potential double-counting of shared information.\nBayesian approaches offer principled integration. Priors encode beliefs about variant mechanism proportions; likelihoods incorporate model predictions given mechanism; posteriors combine evidence across models while respecting uncertainty. REVEL (Rare Exome Variant Ensemble Learner) demonstrated this approach for classical predictors [Citation Needed]; extending it to foundation model outputs requires careful calibration of each component score.\n\n\n17.4.2 Avoiding Double-Counting\nFoundation models trained on overlapping data risk capturing correlated rather than independent information. AlphaMissense and ESM-1v both encode evolutionary constraint; combining their scores as independent evidence overweights evolutionary signal. Similarly, conservation-based DNA models like GPN-MSA share information with phyloP scores already incorporated in classical predictors.\nCorrelation analysis helps quantify redundancy. If two model scores correlate above 0.8 across a benchmark dataset, they likely provide similar information and should not be counted as independent evidence. Residual analysis can identify what unique signal each model contributes beyond shared components.\n\n\n\n\n\n\nPractical Guidance: Evidence Independence in ACMG Classification\n\n\n\nFor ACMG classification, guidelines specifically address computational evidence weighting. The PP3 (computational evidence supporting pathogenicity) and BP4 (computational evidence supporting benignity) criteria apply when multiple tools agree. However, using five correlated predictors that all derive from evolutionary conservation should not count as five independent pieces of evidence.\nRecommended practice: 1. Group predictors by primary information source (evolutionary constraint, structural features, regulatory marks) 2. Select one representative tool from each group for ACMG evidence 3. Document tool correlations in your laboratory’s validation records 4. Update tool selection as new methods emerge and correlations are characterized\nClinical laboratories should develop local policies for which tools to consult and how to weight their outputs, ideally based on validation against known variants in their patient population.\n\n\n\n\n17.4.3 Practical Workflow Design\nAn effective VEP workflow balances comprehensiveness against efficiency. Genome-wide screening might use fast, zero-shot models (DNA language model likelihood scores) to identify variants deviating from expected sequence patterns. Prioritized variants then receive detailed evaluation with computationally expensive models (AlphaGenome multi-omic predictions). Final interpretation combines computational scores with population frequency, gene-level constraint metrics, segregation data, and clinical phenotype.\nThe ordering matters for efficiency. Filtering the majority of variants with fast models before applying expensive models reduces computational cost by orders of magnitude. The choice of filtering threshold trades sensitivity against specificity: strict thresholds miss true pathogenic variants; lenient thresholds burden downstream analysis with false positives. Threshold selection should match intended use: diagnostic applications prioritize sensitivity while research screening may prioritize specificity.\nWhy does the tiered approach work biologically as well as computationally? Most variants in a genome are benign—purifying selection has eliminated severely deleterious variants from the population. A fast initial filter that correctly classifies the majority of benign variants reduces the candidate set to a manageable number for detailed analysis. The variants that pass the filter are enriched for those where predictions are uncertain or where multiple mechanisms may be at play, justifying the computational investment in multi-model evaluation. This mirrors clinical reasoning: common polymorphisms need not be scrutinized in depth, while rare variants in disease-relevant genes warrant comprehensive assessment.\n\n\n\n\n\n\n\n\nModel selection by variant type\n\n\n\n\n\n\n\nNon-coding variant integration pipeline\n\n\n\n\n\n\n\n\n\nEnsemble scoring combines predictions\n\n\n\n\n\n\n\nUncertainty aggregation across models\n\n\n\n\n\n\nFigure 17.3: Multi-model integration for comprehensive variant assessment. (A) Coding variants are routed to appropriate models based on consequence type: missense to AlphaMissense/ESM, splice-proximal to SpliceAI, synonymous requiring both splicing and regulatory assessment. (B) Non-coding variants use specialized models for each region: promoters and enhancers use Enformer, splice regions use SpliceAI, deep intronic variants combine conservation with regulatory predictions. (C) Ensemble methods combine individual model scores through learned weights, typically outperforming any single model. (D) Uncertainty aggregation identifies variants where models disagree, flagging them for review. When models agree, predictions are confident; when models disagree, elevated uncertainty triggers manual review.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-calibration",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-calibration",
    "title": "17  Variant Effect Prediction",
    "section": "17.5 Calibration and Clinical Categories",
    "text": "17.5 Calibration and Clinical Categories\n\n\n\n\n\n\nDifficulty Warning: Statistical Foundations Required\n\n\n\nThis section introduces calibration concepts that require familiarity with probability, Bayesian reasoning, and classification metrics. If terms like “reliability diagram,” “isotonic regression,” or “odds ratio” are unfamiliar, consider first reviewing the technical foundations in Section 23.2. The practical implications are accessible without deep statistical background, but understanding why these methods work requires the full treatment in Chapter 23.\n\n\nA pathogenicity score of 0.73 means nothing in isolation. If that score reflects a well-calibrated model, approximately 73% of variants receiving similar scores are truly pathogenic, and clinical decisions can proceed accordingly. If the model is miscalibrated, the true pathogenic rate could be 40% or 95%, rendering the score unreliable for clinical interpretation. Model scores become clinically useful only when they map to actionable categories through calibration, the process of ensuring that predicted probabilities match observed frequencies.\n\n17.5.1 Assessing Calibration\nCalibration plots (reliability diagrams) visualize the relationship between predicted probabilities and observed frequencies. Variants are binned by predicted score, and the proportion of pathogenic variants in each bin is plotted against the bin’s mean predicted probability. Perfect calibration falls on the diagonal: a predicted 0.8 pathogenicity corresponds to an 80% observed pathogenic rate. Points below the diagonal indicate overconfidence (predictions exceed reality), while points above indicate underconfidence.\nMost raw model outputs are poorly calibrated. Neural networks trained with cross-entropy loss tend toward overconfidence, predicting probabilities near 0 or 1 more often than warranted. Protein language model likelihood ratios produce unbounded scores requiring transformation before probability interpretation. The theoretical foundations of why deep networks and foundation models exhibit systematic miscalibration, along with formal definitions of calibration metrics including expected calibration error (ECE), are developed in Section 23.2. The specific challenges posed by foundation model miscalibration in clinical settings are examined in Section 23.2.3.\n\n\n17.5.2 Calibration Methods for Variant Effect Prediction\nPost-hoc calibration transforms raw model outputs into probabilities that match observed pathogenicity frequencies. The technical details of these transformations, including temperature scaling, Platt scaling, and isotonic regression, are developed in Section 23.3. Here we focus on their application to variant effect prediction.\nCalibration should use data representative of deployment conditions. Calibrating on ClinVar expert-reviewed variants produces reliable performance on similar variants but may not transfer to novel genes, rare populations, or variant classes underrepresented in ClinVar. A model calibrated on well-studied cancer genes may be systematically overconfident when applied to genes with fewer characterized variants. Stratified calibration by gene function, variant class, or population improves reliability at the cost of increased data requirements.\nThe systematic biases that arise from distribution shift between calibration and deployment present particular challenges for clinical genomics. Foundation models trained predominantly on European-ancestry data may exhibit differential calibration across populations, producing well-calibrated predictions for some patient groups and miscalibrated predictions for others (Chapter 12). These disparities have direct implications for equitable care, as clinical decisions based on miscalibrated predictions will be systematically worse for patients from underrepresented backgrounds. The sources and consequences of such differential calibration are examined in Section 23.2.\n\n\n\n\n\n\n\n\nRaw model outputs require calibration\n\n\n\n\n\n\n\nCalibration methods correct systematic bias\n\n\n\n\n\n\n\n\n\nThreshold selection for clinical use\n\n\n\n\n\n\n\nPrevalence affects score interpretation\n\n\n\n\n\n\nFigure 17.4: Calibration for clinical variant interpretation. (A) Raw model scores are not calibrated probabilities—neural networks exhibit systematic overconfidence, clustering predictions near 0 and 1. (B) Calibration curves reveal and correct probability bias; post-calibration predictions track the diagonal, providing accurate probability estimates. (C) Threshold selection depends on clinical context: screening applications favor high sensitivity, diagnostic applications balance sensitivity and specificity, confirmation applications favor high specificity. (D) Clinical prevalence dramatically affects interpretation—the same score means different things in healthy populations versus rare disease clinics. Proper clinical deployment requires calibration and prevalence-adjusted interpretation.\n\n\n\n\n\n17.5.3 Mapping to ACMG Categories\nThe ACMG-AMP variant classification framework defines five categories: pathogenic, likely pathogenic, uncertain significance, likely benign, and benign (Richards et al. 2015). Computational evidence contributes to classification through specific criteria: PP3 (computational evidence supporting pathogenicity) and BP4 (computational evidence supporting benignity).\n\n\n\n\n\n\nACMG-AMP Variant Classification Framework\n\n\n\nThe American College of Medical Genetics and Genomics (ACMG) and Association for Molecular Pathology (AMP) established a standardized framework for classifying sequence variants in Mendelian disease genes (Richards et al. 2015). Understanding this framework is essential for applying foundation model predictions in clinical settings.\nClassification categories:\n\n\n\nCategory\nAbbreviation\nClinical interpretation\n\n\n\n\nPathogenic\nP\nDisease-causing; actionable\n\n\nLikely pathogenic\nLP\n&gt;90% probability pathogenic; actionable\n\n\nUncertain significance\nVUS\nInsufficient evidence; not actionable\n\n\nLikely benign\nLB\n&gt;90% probability benign\n\n\nBenign\nB\nNot disease-causing\n\n\n\nEvidence types and strengths:\nEvidence for pathogenicity includes: - PVS1 (Very Strong): Null variant in gene where loss-of-function causes disease - PS1-PS4 (Strong): Same amino acid change known pathogenic, functional studies, segregation in families, prevalence in affected vs. controls - PM1-PM6 (Moderate): Functional domain, absent from controls, missense in gene with low benign variation, etc. - PP1-PP5 (Supporting): Cosegregation, missense in gene with mostly missense pathogenic, computational evidence (PP3), patient phenotype match, reputable source\nEvidence for benignity includes: - BA1 (Stand-alone): Allele frequency &gt;5% in population databases - BS1-BS4 (Strong): Frequency higher than expected, functional studies, segregation against, observed in trans with pathogenic - BP1-BP7 (Supporting): Missense in gene with truncating mechanism, silent with no splice impact, computational evidence (BP4), etc.\nCombining evidence:\n\n\n\n\n\n\n\nClassification\nEvidence required\n\n\n\n\nPathogenic\nPVS1 + ≥1 PS; OR 2 PS; OR 1 PS + 3 PM; OR 1 PS + 2 PM + 2 PP; …\n\n\nLikely pathogenic\n1 PVS1 + 1 PM; OR 1 PS + 1-2 PM; OR 1 PS + ≥2 PP; …\n\n\nLikely benign\n1 BS + 1 BP; OR ≥2 BP\n\n\nBenign\nBA1; OR ≥2 BS\n\n\nVUS\nCriteria not met for other categories\n\n\n\nComputational evidence (PP3/BP4):\nFoundation model scores contribute through PP3 (supporting pathogenicity) or BP4 (supporting benignity). These criteria provide supporting-level evidence. To achieve stronger evidence levels, tools must be calibrated to demonstrate odds ratios of pathogenicity meeting specific thresholds: &gt;2.08 for supporting, &gt;4.33 for moderate, &gt;18.7 for strong (Tavtigian et al. 2018). Recent calibration studies have established that several foundation model-based predictors can achieve moderate or strong evidence levels at appropriate thresholds (Pejaver et al. 2022; Bergquist et al. 2025).\n\n\nMapping continuous foundation model scores to these discrete criteria requires threshold selection. Conservative thresholds ensure high precision at the cost of low recall: only variants with very high (or very low) scores receive computational evidence designation. Lenient thresholds increase recall but admit more false positives, potentially inflating pathogenicity classifications. The choice reflects a fundamental trade-off between missing actionable variants and overclassifying benign variants as potentially harmful.\nClinGen sequence variant interpretation working groups have developed model-specific recommendations for computational predictors, specifying score thresholds that correspond to different evidence strengths. Tavtigian and colleagues proposed a Bayesian framework for calibrating computational evidence strength based on odds ratios of pathogenicity at different score thresholds (Tavtigian et al. 2018). Under this framework, thresholds must achieve specific odds ratios (greater than 2.08 for supporting evidence, greater than 4.33 for moderate evidence) to qualify for particular ACMG evidence levels. Pejaver et al. applied this framework to calibrate 13 classical missense predictors, establishing that four tools (BayesDel, MutPred2, REVEL, VEST4) could provide up to Strong evidence for pathogenicity (Pejaver et al. 2022). In 2025, ClinGen extended these calibrations to foundation model-based predictors, demonstrating that AlphaMissense, ESM1b, and VARITY all reach Strong evidence for pathogenicity and Moderate for benignity at appropriate score thresholds (Bergquist et al. 2025). Laboratories should select tools with established calibrations and document threshold choices in variant reports.\n\n\n17.5.4 The Challenge of Uncertain Significance\nThe variant of uncertain significance (VUS) category deserves particular attention. Variants with intermediate foundation model scores genuinely reflect uncertainty: the models cannot confidently distinguish pathogenic from benign. This uncertainty may arise from limited training data for the gene or variant class, conflicting signals in the sequence context, or genuine biological ambiguity where the variant’s effect depends on factors the model cannot observe.\nForcing these variants into discrete categories by applying arbitrary cutoffs misrepresents the actual evidence. A variant scored at 0.55 is not “slightly pathogenic”; it is a variant for which the model has insufficient evidence to discriminate. Reporting calibrated probabilities alongside discrete classifications preserves information for downstream decision-making. Clinicians can then integrate computational evidence with functional studies, segregation data, and clinical presentation, appropriately weighting the computational contribution based on its expressed uncertainty.\nThe broader framework for understanding and quantifying uncertainty in foundation model predictions, including methods for distinguishing uncertainty arising from limited data (epistemic uncertainty) from uncertainty inherent in the prediction task (aleatoric uncertainty), is developed in Chapter 23. Conformal prediction methods that provide finite-sample coverage guarantees for variant classification are examined in Section 23.5.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-uncertainty",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-uncertainty",
    "title": "17  Variant Effect Prediction",
    "section": "17.6 Uncertainty Quantification",
    "text": "17.6 Uncertainty Quantification\nCalibration addresses systematic bias in probability estimates; uncertainty quantification addresses the confidence of individual predictions. A well-calibrated model might correctly estimate that 70% of variants in some category are pathogenic, but for any individual variant, we want to know whether the model’s prediction is reliable or whether the variant falls outside the model’s competence.\n\n17.6.1 Sources of Uncertainty\nEpistemic uncertainty reflects gaps in the model’s knowledge: regions of input space with sparse training data, variant types rarely observed during training, or proteins from understudied families. This uncertainty is reducible in principle by collecting more data and can be estimated by measuring model disagreement across training variations.\nAleatoric uncertainty reflects inherent noise in the prediction target: variants whose pathogenicity genuinely varies across individuals or contexts, or cases where the same score corresponds to both pathogenic and benign variants for biological rather than modeling reasons. This uncertainty is irreducible by additional training and represents fundamental limits on predictability.\nDistinguishing these uncertainty types matters for interpretation. High epistemic uncertainty suggests caution: the model has not seen similar variants and may be extrapolating unreliably. High aleatoric uncertainty suggests that the variant’s effect genuinely depends on factors not captured by sequence alone.\n\n\n\n\n\n\nKnowledge Check: Epistemic vs. Aleatoric Uncertainty\n\n\n\nConsider two variants with identical AlphaMissense scores of 0.65:\nVariant A: A missense change in BRCA1, a gene with thousands of characterized variants in ClinVar and extensive deep mutational scanning data.\nVariant B: A missense change in a recently discovered orphan gene with no characterized variants and only 8 homologous sequences in databases.\n\nWhich variant likely has higher epistemic uncertainty? Why?\nFor which variant might the intermediate score more likely reflect aleatoric uncertainty (genuine biological ambiguity)?\nHow would you communicate the different confidence levels to a clinician?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nVariant B has higher epistemic uncertainty because the model has limited training data for orphan genes with sparse evolutionary sampling. For Variant A in BRCA1, the intermediate score more likely reflects aleatoric uncertainty—genuine biological ambiguity about pathogenicity given extensive characterization data. Communication to clinicians should distinguish these: “For BRCA1, the intermediate score reflects uncertain biological effect; for the orphan gene, the score itself is uncertain due to limited data and should be interpreted cautiously.”\n\n\n\n\n\n\n\n17.6.2 Uncertainty Estimation Methods\nEnsemble methods train multiple models on different data subsets or with different random initializations. Like asking multiple doctors for second opinions on a diagnosis, if all the doctors independently reach the same conclusion, you can be more confident in the answer; if they disagree substantially, that disagreement itself is informative—the case is genuinely ambiguous. Similarly, prediction variance across ensemble members estimates epistemic uncertainty. Large disagreement indicates that the prediction depends strongly on training specifics rather than robust learned patterns. Deep ensembles provide well-calibrated uncertainty estimates but multiply computational cost linearly with ensemble size.\nWhy does ensemble disagreement estimate uncertainty? The intuition is that if multiple models trained on slightly different data or with different random seeds all converge to similar predictions, the prediction is robust to training details and likely reflects genuine patterns in the data. Conversely, if predictions diverge substantially, each model has learned something idiosyncratic that does not generalize. For variant effect prediction, a variant where five ensemble members predict pathogenicity between 0.75 and 0.82 warrants more confidence than one where predictions range from 0.35 to 0.90. The ensemble is effectively asking: “Would I reach the same conclusion if I had seen slightly different training examples?” When the answer is no, that uncertainty should propagate to clinical interpretation.\nMonte Carlo dropout approximates Bayesian inference by applying dropout at test time and averaging predictions across multiple stochastic forward passes. Variance across passes estimates uncertainty without training multiple models. This approach adds modest computational overhead and can be applied to any dropout-containing architecture.\nConformal prediction provides distribution-free uncertainty quantification with coverage guarantees (Angelopoulos and Bates 2023). Given a calibration set, conformal methods construct prediction sets guaranteed to contain the true label with specified probability (e.g., 90%). For variant classification, this might produce sets like {pathogenic, uncertain} or {benign} depending on the variant and desired coverage. Larger prediction sets indicate greater uncertainty; single-element sets indicate confident predictions. Section 23.5 examines conformal methods for genomic applications in detail.\n\n\n17.6.3 Out-of-Distribution Detection\nBeyond quantifying uncertainty for in-distribution predictions, responsible deployment requires detecting when inputs fall outside the model’s training distribution. A protein language model trained on natural proteins may produce confident but unreliable predictions for synthetic sequences or fragments. A regulatory model trained on common cell types may fail on rare developmental stages.\nLikelihood-based detection uses the model’s own representations to identify unfamiliar inputs. Sequences with low embedding density or anomalous attention patterns may fall outside the training distribution regardless of predicted scores. Flagging these inputs for manual review prevents automated classification of cases the model cannot reliably assess.\nDistance-based methods compare new inputs to training examples in representation space. Variants far from any training example in embedding space warrant skepticism even if the model produces confident predictions. Maintaining summary statistics of training representations enables efficient distance computation at deployment.\n\n\n\n\n\n\n\n\nAleatoric vs. epistemic uncertainty sources\n\n\n\n\n\n\n\nEnsemble disagreement quantifies uncertainty\n\n\n\n\n\n\n\n\n\nDistance-based out-of-distribution detection\n\n\n\n\n\n\n\nSelective prediction improves reliability\n\n\n\n\n\n\nFigure 17.5: Uncertainty quantification for variant effect prediction. (A) Aleatoric uncertainty (inherent data ambiguity) reflects context-dependent effects that cannot be reduced. Epistemic uncertainty (model limitations) can be reduced with more training data. (B) Ensemble disagreement provides empirical uncertainty estimates—variance across model runs indicates prediction reliability. (C) Distance from training distribution flags out-of-distribution variants requiring caution. (D) Selective prediction uses uncertainty to abstain on difficult cases, achieving higher accuracy by routing uncertain variants to expert review rather than providing unreliable predictions.\n\n\n\nChapter 23 develops uncertainty quantification methods in detail, including practical implementation guidance and evaluation metrics. For VEP applications, the key insight is that uncertainty estimates complement point predictions: high-confidence predictions can inform clinical decisions; low-confidence predictions should prompt additional evidence gathering rather than blind acceptance of model outputs.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-fm-gains",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-fm-gains",
    "title": "17  Variant Effect Prediction",
    "section": "17.7 What Foundation Models Add",
    "text": "17.7 What Foundation Models Add\nHaving surveyed current foundation model approaches, we can now directly address what they contribute beyond classical methods (Chapter 4). The answer is nuanced: substantial improvements in some domains, modest gains in others, and persistent blind spots that new architectures have not yet resolved.\n\n17.7.1 Improved Discrimination\nOn standard benchmarks, foundation model VEP methods consistently outperform classical predictors. AlphaMissense achieves auROC of 0.91 on held-out ClinVar missense variants compared to 0.85 for CADD [Citation Needed]. SpliceAI detects pathogenic splicing variants with sensitivity of 0.90 compared to 0.60 for MaxEntScan [Citation Needed]. GPN-MSA scores correlate more strongly with deep mutational scanning measurements than phyloP or GERP [Citation Needed].\nThese improvements reflect richer representations. Classical methods aggregate independent features (conservation, amino acid properties, domain annotations); foundation models learn nonlinear interactions among positions and capture patterns too subtle for manual feature engineering. The gap is largest for variants where context matters: buried core missense variants where structural environment determines impact, splice variants where cryptic site activation depends on flanking sequence, regulatory variants where motif disruption interacts with chromatin context.\n\n\n17.7.2 Extended Coverage\nClassical methods often fail silently on understudied genes, rare variant classes, or poorly annotated regions. SIFT and PolyPhen require protein alignments; variants in singleton genes without homologs receive no prediction. CADD depends on annotation features; variants in regions lacking regulatory marks receive uninformative scores.\nFoundation models degrade more gracefully. Protein language models score any amino acid sequence regardless of available homologs. DNA language models score any genomic position regardless of existing annotation. This extended coverage matters for clinical sequencing of rare diseases, where pathogenic variants often reside in less-studied genes precisely because their severe effects are incompatible with population frequency.\n\n\n17.7.3 Mechanistic Interpretability\nAlphaGenome and similar multi-output models provide predictions about mechanism rather than bare pathogenicity scores. A variant flagged as deleterious might also show predicted effects on chromatin accessibility, contact frequency, and downstream gene expression. These mechanistic predictions enable hypothesis generation and targeted experimental validation (Chapter 24).\nClassical methods offer limited mechanistic insight. CADD provides a single score without indicating whether it derives from conservation, protein impact, regulatory disruption, or other features. Decomposing the score into component contributions requires separate analysis. Foundation models that predict molecular phenotypes naturally provide this decomposition.\n\n\n17.7.4 Persistent Limitations\nFoundation models have not solved several fundamental challenges. Ancestry bias persists because training data remain skewed toward European populations; performance degrades for variants common in African or Asian populations but rare in training sets. The systematic analysis of ancestry-related confounding appears in ?sec-ch22-ancestry-confounding, with broader confounding detection methods in ?sec-ch22-detection. Calibration requires substantial labeled data that inherit existing biases. Rare variant classes (structural variants, complex indels, repeat expansions) lack sufficient training examples for reliable prediction.\nThe comparison to classical methods reveals diminishing returns on certain axes. For well-conserved active site variants in thoroughly studied proteins, PolyPhen-2 already achieves near-optimal performance; AlphaMissense improves marginally. The largest foundation model gains appear for difficult cases where classical features are uninformative or misleading.\n\n\n\n\n\n\nKey Insight: Where Foundation Models Help Most\n\n\n\nFoundation models provide the largest improvements over classical methods in precisely the situations where clinical need is greatest:\n\nNovel genes: Orphan proteins lacking homologs where alignment-based methods fail\nRare variants: Never-before-seen substitutions where population frequency provides no information\nNoncoding regions: Regulatory variants where classical annotation is sparse\nContext-dependent effects: Buried core variants, cryptic splice sites, motif-chromatin interactions\n\nFor common variants in well-studied genes with extensive clinical annotation, the marginal improvement may be modest. But these are not the variants causing diagnostic uncertainty. The value of foundation models lies in extending reliable prediction to the long tail of rare variants in less-characterized genes, exactly where clinical interpretation struggles most.\n\n\n\n\n\n\n\n\n\n\nPerformance varies by protein family\n\n\n\n\n\n\n\nRare variants remain challenging\n\n\n\n\n\n\n\n\n\nClinical impact assessment\n\n\n\n\n\n\n\nRemaining gaps require new approaches\n\n\n\n\n\n\nFigure 17.6: Foundation model VEP: gains and remaining gaps. (A) Performance varies by protein family—well-characterized families achieve high accuracy while poorly-studied families show degraded performance. (B) Rare variants are harder to predict, particularly for recently evolved or population-specific variants. (C) Clinical impact assessment: FM predictions could reclassify significant fractions of VUS, but require appropriate validation. (D) Remaining gaps include complex variants (multi-allelic, structural), modifier effects, and tissue-specific pathogenicity. Foundation models have advanced VEP substantially but do not eliminate the need for functional validation.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-clinical-integration",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-clinical-integration",
    "title": "17  Variant Effect Prediction",
    "section": "17.8 Clinical Integration Considerations",
    "text": "17.8 Clinical Integration Considerations\nFoundation model VEP tools require thoughtful integration into clinical workflows. Their benchmark performance does not automatically translate without attention to deployment context, validation requirements, and human factors.\n\n17.8.1 Laboratory Validation\nBefore clinical use, laboratories should validate foundation model tools against local truth sets representing their patient population. Published benchmark performance on ClinVar may not generalize to a laboratory’s specific case mix. Validation should assess discrimination (can the tool distinguish pathogenic from benign?), calibration (do probability estimates match observed frequencies?), and utility (does incorporating the tool improve variant classification compared to existing workflows?).\nValidation requires variants with known pathogenicity independent of the computational predictions being tested. Using ClinVar variants whose classifications already incorporated CADD scores to validate CADD creates circular reasoning, a form of label circularity examined in ?sec-ch22-label-circularity. Gold-standard variants from functional studies, segregation data, or expert review provide cleaner validation targets, with detailed evaluation methodology in Chapter 12.\n\n\n\n\n\n\nPractical Guidance: Laboratory Validation Checklist\n\n\n\nBefore deploying a foundation model VEP tool clinically, ensure you have:\n\nIdentified an appropriate validation set independent of tool training data\nAssessed discrimination metrics (auROC, auPRC) on your patient population\nGenerated calibration plots and calculated expected calibration error\nCompared performance to your current workflow (utility analysis)\nEstablished thresholds for PP3/BP4 evidence based on local calibration\nDocumented ancestry composition of validation set and any stratified analyses\nTrained analysts on appropriate interpretation and known limitations\nEstablished a process for monitoring ongoing performance and updating thresholds\n\nThis validation should be repeated when model versions change or when significant shifts in your patient population occur.\n\n\n\n\n17.8.2 Workflow Integration\nFoundation model predictions represent one evidence type among many. ACMG guidelines specify how computational evidence combines with population frequency, functional data, segregation, and clinical phenotype. Computational evidence alone rarely suffices for pathogenic or benign classification; it supports or weakens classifications established by other evidence types.\nLaboratory information systems require modification to display and store foundation model outputs alongside existing annotations. Analyst training ensures appropriate interpretation: understanding that high scores indicate deleteriousness without establishing causation, recognizing when scores fall outside validated ranges, and knowing when to request additional evidence for uncertain cases.\n\n\n17.8.3 Communication to Clinicians\nVariant reports communicated to ordering clinicians should present foundation model evidence appropriately. Reporting raw scores without context confuses non-specialist clinicians. Reporting discrete classifications without uncertainty may convey false confidence. Effective reporting might state: “Computational tools (AlphaMissense, SpliceAI) concordantly predict this variant is likely to affect protein function, supporting the PP3 criterion for pathogenicity classification.”\nWhen foundation model predictions conflict with other evidence, reports should acknowledge the discrepancy rather than suppressing inconvenient results. A variant segregating with disease in a family but receiving a benign computational prediction warrants explicit discussion, not quiet exclusion of the computational evidence.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-open-challenges",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-open-challenges",
    "title": "17  Variant Effect Prediction",
    "section": "17.9 Open Challenges",
    "text": "17.9 Open Challenges\nCurrent foundation model approaches leave substantial problems unsolved. These open challenges define directions for future research and areas where clinical caution remains warranted.\n\n17.9.1 Complex Variant Types\nMost current models address single nucleotide variants and small indels. Structural variants (deletions, duplications, inversions spanning kilobases to megabases) remain largely outside foundation model capabilities. Copy number variation, repeat expansions, and complex rearrangements alter genome architecture in ways current sequence models cannot represent. Extending foundation model paradigms to these variant classes requires architectural innovations beyond current approaches.\n\n\n17.9.2 Long-Read Sequencing and Variant Effect Prediction\nThe emergence of long-read sequencing technologies fundamentally changes the landscape of variant detection and interpretation. Pacific Biosciences (PacBio) high-fidelity (HiFi) reads and Oxford Nanopore Technologies (ONT) ultra-long reads routinely span tens of kilobases, far exceeding the 150-300 base pair fragments of short-read platforms (logsdon_long-read_2020?). This extended read length enables detection of variant classes invisible to short-read analysis while creating both opportunities and challenges for foundation model-based interpretation.\nStructural variant detection benefits most dramatically from long-read sequencing. Short reads struggle to resolve breakpoints of large deletions, duplications, and inversions, often missing structural variants entirely or mischaracterizing their boundaries. Long reads spanning structural variant breakpoints enable precise localization and accurate genotyping. Tools like pbsv, Sniffles2, and SVIM detect structural variants with sensitivity and specificity far exceeding short-read methods (Smolka et al. 2024). The resulting catalogs reveal that each human genome harbors thousands of structural variants affecting millions of base pairs, a substantial source of genetic variation previously obscured by technological limitations.\nInterpreting these structural variants presents challenges that current foundation models do not address. A deletion removing an entire exon has qualitatively different consequences than a single nucleotide substitution, yet protein language models score point mutations without mechanisms for evaluating larger-scale changes. Regulatory models like Enformer operate on fixed-length sequence windows and cannot naturally represent the genomic rearrangements that structural variants introduce. Extending foundation model approaches to structural variant interpretation requires new architectures that explicitly model genome organization rather than treating sequence as a linear string.\nPhasing and haplotype-aware analysis represent a second area where long-read data transforms variant interpretation. Human genomes are diploid, with variants distributed across two homologous chromosomes. The functional consequences of multiple variants depend critically on whether they reside on the same haplotype (cis) or opposite haplotypes (trans). Two loss-of-function variants in cis leave one functional copy, while the same variants in trans may completely abolish gene function (Section 1.4; ?sec-ch26-compound-het).\nShort-read sequencing typically produces unphased genotypes: variants are detected without determining their chromosomal assignment. Statistical phasing using population reference panels infers likely haplotypes but introduces errors, particularly for rare variants where population frequencies provide limited information. Long reads spanning multiple variant sites provide direct physical phasing, resolving haplotype structure from the sequencing data itself. With HiFi reads averaging 15-20 kilobases, most nearby variants can be directly phased, while ultra-long ONT reads exceeding 100 kilobases can phase variants separated by substantial genomic distances.\nFoundation models have not yet incorporated haplotype information systematically. Protein language models score individual variants without considering whether they occur together on the same protein copy. DNA language models process single reference sequences rather than diploid genotypes with phased variants. Developing haplotype-aware variant effect prediction remains an open challenge: models must learn how variant combinations interact, distinguishing compensatory mutations that restore function from compound effects that amplify disruption.\n\n\n\n\n\n\nLong-read sequencing expands variant detection scope\n\n\n\n\nFigure 17.7: Long-read sequencing expands the scope of variant effect prediction. Short-read sequencing (top) detects primarily SNVs and small indels for which current FM-VEP approaches are well-suited. Long-read sequencing (bottom) reveals structural variants, repeat expansions, and complex haplotypes that current models cannot process. Realizing the clinical potential of long-read sequencing requires developing FM-VEP approaches for these new variant classes—a major open challenge.\n\n\n\nLong-read-specific variant calling increasingly incorporates deep learning approaches. DeepVariant and Clair3 use convolutional and recurrent architectures to call variants from pileup images, with versions trained specifically on long-read data achieving accuracy that approaches or exceeds short-read calling for SNVs and small indels (Zheng et al. 2022). PEPPER-Margin-DeepVariant pipelines combine multiple neural network components to handle the higher error rates and distinct error profiles of nanopore sequencing. These tools demonstrate that deep learning can extract accurate variant calls from long-read data despite its different characteristics.\nThe error profiles of long-read platforms create both challenges and opportunities for foundation model training. ONT sequencing exhibits systematic errors in homopolymer regions and certain sequence contexts that differ from short-read error patterns. PacBio HiFi reads achieve per-read accuracy exceeding 99.9% through circular consensus sequencing, approaching short-read quality while retaining long-read advantages. Foundation models trained on long-read data must learn these error profiles to distinguish true variants from sequencing artifacts.\nTraining foundation models on long-read data remains largely unexplored. Current DNA language models train on reference genomes and short-read assemblies, rarely incorporating the raw signal or base-called sequences from long-read platforms. Models trained directly on long-read data might learn different patterns: the extended context could enable modeling of longer-range dependencies, while exposure to structural variants during training could improve representation of genome architecture.\nSeveral technical challenges complicate long-read foundation model development. Training data volumes are smaller: long-read datasets remain orders of magnitude smaller than short-read repositories. The computational cost of processing longer sequences scales unfavorably for attention-based architectures, though state-space models like those underlying Evo 2 (Section 7.7.2) partially address this limitation. Representing structural variants requires architectural innovations beyond sequence modeling, potentially incorporating graph representations or hierarchical encodings of genome structure.\nThe integration of long-read variant detection with foundation model interpretation represents a frontier for the field. As long-read sequencing costs decline and data volumes grow, training foundation models that leverage the unique advantages of long reads (extended context, structural variant representation, direct phasing) becomes increasingly feasible. Models that combine the pattern-recognition capabilities of foundation models with the variant classes revealed by long-read sequencing could substantially expand the scope of computational variant interpretation.\n\n\n17.9.3 Combinatorial Effects\nGenomes contain multiple variants that may interact. Compound heterozygosity (two variants affecting both copies of a gene) creates pathogenic states from individually tolerable variants, a clinical scenario examined in Section 1.4.1 and ?sec-ch26-compound-het. Modifier variants in other genes modulate penetrance. Haplotype effects mean variants on the same chromosome have different consequences than variants on opposite chromosomes, with phasing methods to distinguish these scenarios detailed in Section 1.4. Current models score variants independently, ignoring these interactions that determine clinical presentation.\n\n\n17.9.4 Phenotype Specificity\nA variant pathogenic for one phenotype may be benign for another. SCN5A variants cause distinct cardiac arrhythmia syndromes depending on their specific functional effects [Citation Needed]. Foundation models trained on pathogenic/benign labels average across phenotypes, potentially obscuring clinically relevant specificity. Phenotype-specific training requires much larger datasets than currently available.\n\n\n17.9.5 Temporal and Environmental Context\nVariant effects often depend on age, environmental exposures, or physiological state. A variant pathogenic under metabolic stress may be tolerable at baseline. Foundation models capture sequence context but not the dynamic biological context determining phenotypic expression. Integrating longitudinal clinical data with sequence-level predictions remains an unsolved challenge.\n\n\n17.9.6 Equity and Access\nState-of-the-art foundation models require substantial computational resources for training and sometimes for inference. Laboratories in resource-limited settings may lack access to cutting-edge tools, creating a two-tiered system where well-funded institutions deploy sophisticated variant interpretation while others rely on simpler methods. Precomputed scores (like AlphaMissense’s proteome-wide release) partially address computational barriers, but equity concerns extend far beyond compute access.\nTraining data composition determines which patients foundation models serve well. ClinVar contains many more pathogenic variant classifications for European-ancestry individuals than for other populations (Landrum et al. 2018). Protein language models trained predominantly on sequences from well-studied organisms may capture evolutionary constraints less accurately for proteins divergent from training distributions. The consequence is systematic: variant interpretation performs best for patients who already benefit most from biomedical research, and worst for those historically excluded. A diagnostic laboratory serving a diverse urban population will encounter variants where foundation model predictions are less reliable precisely because those variants come from underrepresented ancestries.\nValidation cohorts exhibit similar biases. When foundation models are evaluated on ClinVar or gnomAD-derived benchmarks, performance metrics reflect accuracy for the populations overrepresented in those resources. A model achieving 0.95 auROC on standard benchmarks may achieve substantially lower discrimination for African-ancestry variants simply because the benchmark itself undersamples that population. Equitable deployment requires ancestry-stratified evaluation that explicitly reports performance gaps, not aggregate metrics that obscure disparities (Chapter 12). The broader implications of these biases, and governance frameworks for addressing them, receive comprehensive treatment in Section 26.1.\n\n\n\n\n\n\nStop and Think: Equity in Model Development\n\n\n\nConsider a clinical genetics laboratory that serves a diverse patient population where 40% of patients have non-European ancestry. The laboratory is evaluating AlphaMissense for clinical use.\n\nWhat specific validation analyses should the laboratory conduct before deployment?\nIf the model performs well overall (auROC = 0.92) but substantially worse for African-ancestry variants (auROC = 0.81), what are the ethical implications of deploying it?\nHow might the laboratory communicate differential performance to ordering clinicians?\n\nThere are no simple answers to these questions, but responsible deployment requires explicitly grappling with them.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_3/p3-ch17-vep-fm.html#sec-ch17-conclusion",
    "href": "part_3/p3-ch17-vep-fm.html#sec-ch17-conclusion",
    "title": "17  Variant Effect Prediction",
    "section": "17.10 Tools for Interpretation, Not Oracles",
    "text": "17.10 Tools for Interpretation, Not Oracles\nFoundation models have transformed variant effect prediction from feature engineering to representation learning. Protein language models capture evolutionary constraint at resolution that multiple sequence alignments cannot match. DNA language models and regulatory models extend coverage to noncoding variants across the genome. Multi-omic architectures provide mechanistic predictions enabling hypothesis generation beyond bare deleteriousness scores. The best current methods substantially outperform classical approaches on established benchmarks, particularly for rare variants and novel genes where training data are sparse.\nYet benchmark performance does not automatically translate to clinical utility. Calibration requires careful attention: a model may discriminate pathogenic from benign variants while systematically overestimating or underestimating probabilities. Uncertainty quantification remains immature; models often produce confident predictions for inputs that fall outside their training distribution. Population bias persists despite foundation model advances; improvements over classical methods are smallest for ancestry groups underrepresented in training data. Complex variant types, combinatorial effects, and tissue-specific consequences remain beyond current capabilities.\nClinical deployment demands humility alongside enthusiasm. Foundation model VEP tools are aids to human interpretation, not autonomous classifiers. Their predictions inform rather than determine variant classification, complementing population frequency data, functional assay evidence, segregation analysis, and clinical judgment. Used appropriately, they accelerate diagnosis and reduce missed findings. Used as oracles, they create false confidence and may perpetuate existing inequities in genomic medicine. Clinical workflows (Chapter 28, Chapter 27) integrate these predictions alongside uncertainty quantification (Chapter 23) and interpretability methods that probe what foundation models have learned (Chapter 24). Variant effect prediction sits at the center of genomic medicine; foundation models have raised its ceiling while the work of achieving its potential continues.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nExplain the paradigm shift from classical feature-based VEP to foundation model-based VEP. How do foundation models perform zero-shot variant scoring without pathogenicity labels?\nCompare protein-based VEP approaches (ESM-1v, EVE, AlphaMissense) with DNA-based approaches (SpliceAI, Enformer, GPN-MSA). For each approach, identify what variant types it handles best and what biological information it leverages.\nWhat is model calibration and why does it matter for clinical variant interpretation? How do reliability diagrams help assess calibration quality?\nDescribe three strategies for combining evidence across multiple foundation models. What is the double-counting problem and how can it be avoided when applying ACMG criteria?\nIdentify three persistent challenges or limitations in foundation model-based VEP that require human judgment. For each, explain why current models struggle and what types of variants are most affected.\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nParadigm shift from classical to foundation model VEP: Classical methods manually engineer features (conservation scores, amino acid properties, domain annotations) and train classifiers on labeled pathogenic/benign variants. Foundation models invert this: they learn representations from unlabeled sequences during pretraining (masked token prediction), discovering evolutionary constraints implicitly. Zero-shot scoring compares the likelihood of reference versus variant alleles given learned sequence context—variants that are evolutionarily unexpected (low likelihood) tend to be deleterious. This approach requires no pathogenicity labels because evolution has already conducted billions of experiments testing which variants are compatible with life.\nProtein vs. DNA approaches comparison: Protein approaches (ESM-1v, EVE, AlphaMissense) operate on amino acid sequences and excel at missense variants. ESM-1v uses zero-shot likelihood ratios from protein language models. EVE fits variational autoencoders to MSAs, capturing co-evolution. AlphaMissense combines sequence with AlphaFold2 structural features for state-of-the-art missense prediction. DNA approaches (SpliceAI, Enformer, GPN-MSA, Evo 2) work on nucleotide sequences and cover all variant types. SpliceAI specializes in splice variants using dilated convolutions. Enformer predicts regulatory impacts. GPN-MSA and Evo 2 provide DNA language model scores genome-wide, including noncoding regions where protein models cannot operate.\nCalibration importance and reliability diagrams: Calibration ensures that predicted probabilities match observed frequencies—a score of 0.8 should mean 80% of variants with that score are truly pathogenic. Without calibration, scores are arbitrary and cannot guide clinical decisions. Reliability diagrams plot predicted probabilities (x-axis) against observed pathogenic proportions (y-axis). Perfect calibration falls on the diagonal. Points below indicate overconfidence (model predicts higher probabilities than reality); points above indicate underconfidence. Neural networks typically overpredict extreme probabilities and require post-hoc calibration (temperature scaling, isotonic regression) before clinical use.\nCombining evidence strategies and double-counting: Three strategies: (1) Modular routing—apply different models to different variant types (AlphaMissense for missense, SpliceAI for splice sites). (2) Score aggregation—combine predictions from multiple models using ensemble methods or Bayesian integration with careful weighting. (3) Tiered filtering—use fast models for initial screening, expensive models for prioritized variants. The double-counting problem arises when multiple predictors share information sources (e.g., AlphaMissense and ESM-1v both encode evolutionary constraint). Treating correlated predictions as independent evidence overweights shared signals. Solution: assess correlations, group predictors by information source, select one representative from each group for ACMG evidence, and document tool correlations in laboratory validation records.\nThree persistent challenges: (1) Structural variants (deletions, duplications, inversions)—current sequence models cannot represent architectural rearrangements; they are trained on SNVs and small indels. (2) Combinatorial/compound effects—models score variants independently, missing compound heterozygosity where two individually tolerable variants together cause disease, and haplotype effects where phasing determines pathogenicity. (3) Population bias—training data overrepresent European ancestry; models perform worse for African and Asian populations where variants are undersampled in ClinVar/gnomAD, creating systematic disparities in prediction accuracy for underrepresented groups. These limitations require human judgment to integrate additional evidence (segregation, functional assays, ancestry-appropriate interpretation).\n\n\n\n\n\n\nCore concepts covered:\n\nParadigm shift: Foundation models learn representations from unlabeled sequences, enabling variant scoring without pathogenicity labels. Evolution has already encoded functional constraints; models learn to detect when variants violate these patterns.\nProtein-based methods: Zero-shot scoring with protein language models (ESM-1v) compares amino acid likelihoods. Alignment-based models (EVE, popEVE) explicitly capture evolutionary constraints from MSAs. AlphaMissense combines sequence and structural information for state-of-the-art missense prediction.\nDNA-based methods: SpliceAI predicts splicing changes using dilated convolutions over 10kb context. Enformer and Sei predict regulatory effects. GPN-MSA and Evo 2 provide DNA language model scoring for all variant types.\nIntegration strategies: Different models address different variant types. Combining evidence requires avoiding double-counting from correlated predictors. Practical workflows tier models by computational cost.\nCalibration: Raw model outputs require calibration before clinical use. Reliability diagrams assess calibration quality. ACMG criteria (PP3/BP4) map continuous scores to evidence levels through calibrated thresholds.\nUncertainty: Epistemic uncertainty (model knowledge gaps) differs from aleatoric uncertainty (inherent unpredictability). Ensemble methods, conformal prediction, and out-of-distribution detection provide complementary approaches.\nPersistent challenges: Structural variants, combinatorial effects, phenotype specificity, and population bias remain unsolved. Foundation models are tools for interpretation, not oracles.\n\nKey connections:\n\nChapter 4 provides the classical VEP baseline that foundation models improve upon\nChapter 15 develops the protein language model architectures underlying ESM-1v and AlphaMissense\nChapter 16 covers the regulatory models (Enformer, Sei) used for noncoding VEP\nChapter 23 presents the full technical treatment of calibration and uncertainty quantification\nChapters 27-28 show how VEP tools integrate into clinical workflows\n\nLooking ahead: Having established how foundation models predict variant effects, Chapter 18 extends these concepts to RNA-level predictions, while Chapter 23 provides the complete framework for responsible uncertainty quantification in clinical deployment.\n\n\n\n\n\n\nAngelopoulos, Anastasios N., and Stephen Bates. 2023. “Conformal Prediction: A Gentle Introduction.” Foundations and Trends® in Machine Learning 16 (4): 494–591. https://doi.org/10.1561/2200000101.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBergquist, Timothy, Sarah L. Stenton, Emily A. W. Nadeau, Alicia B. Byrne, Marc S. Greenblatt, Steven M. Harrison, Sean V. Tavtigian, et al. 2025. “Calibration of Additional Computational Tools Expands ClinGen Recommendation Options for Variant Classification with PP3/BP4 Criteria.” Genetics in Medicine 27 (6): 101402. https://doi.org/10.1016/j.gim.2025.101402.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. 2022. “[DeepSEA Sei] A Sequence-Based Global Map of Regulatory Activity for Deciphering Human Genetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nFrazer, Jonathan, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin Gal, and Debora S. Marks. 2021. “[EVE] Disease Variant Prediction with Deep Generative Models of Evolutionary Data.” Nature 599 (7883): 91–95. https://doi.org/10.1038/s41586-021-04043-8.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nOrenbuch, Rose, Courtney A. Shearer, Aaron W. Kollasch, Aviv D. Spinner, Thomas Hopf, Lood van Niekerk, Dinko Franceschi, Mafalda Dias, Jonathan Frazer, and Debora S. Marks. 2025. “[popEVE] Proteome-Wide Model for Human Disease Genetics.” Nature Genetics, November, 1–10. https://doi.org/10.1038/s41588-025-02400-1.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel, Sean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022. “Calibration of Computational Tools for Missense Variant Pathogenicity Classification and ClinGen Recommendations for PP3/BP4 Criteria.” American Journal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie Gastier-Foster, Wayne W. Grody, et al. 2015. “Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology.” Genetics in Medicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nSmolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W. Horner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024. “Detection of Mosaic and Population-Level Structural Variants with Sniffles2.” Nature Biotechnology 42 (10): 1571–80. https://doi.org/10.1038/s41587-023-02024-y.\n\n\nTavtigian, Sean V., Marc S. Greenblatt, Steven M. Harrison, Robert L. Nussbaum, Snehit A. Prabhu, Kenneth M. Boucher, and Leslie G. Biesecker. 2018. “Modeling the ACMG/AMP Variant Classification Guidelines as a Bayesian Classification Framework.” Genetics in Medicine 20 (9): 1054–60. https://doi.org/10.1038/gim.2017.210.\n\n\nZheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam, and Ruibang Luo. 2022. “Symphonizing Pileup and Full-Alignment for Deep Learning-Based Long-Read Variant Calling.” Nature Computational Science 2 (12): 797–803. https://doi.org/10.1038/s43588-022-00387-x.",
    "crumbs": [
      "Part III: Foundation Model Families",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variant Effect Prediction</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html",
    "href": "part_4/p4-ch18-rna.html",
    "title": "18  RNA Structure and Function",
    "section": "",
    "text": "18.1 RNA as Molecule Versus Transcriptome Readout\nTwo complementary perspectives frame computational approaches to RNA. The molecular view treats RNA as a physical object with primary sequence, secondary structure through base pairing, tertiary organization in three-dimensional space, and chemical modifications that alter its properties. In this view, modeling goals include predicting which bases pair with which, how the molecule folds, which proteins bind to it, and how synthetic RNAs might be designed with desired properties. The transcriptomic view treats RNA as a cellular readout: coverage profiles along the genome, splice junction usage, isoform abundances, expression levels that vary across cell types and conditions. Here the goal is explaining how genomic sequence and chromatin state give rise to these measurements.\nModels that predict transcriptomic signals from DNA sequence (Enformer, Borzoi, and related architectures covered in Chapter 16) operate in the second paradigm. They take genomic sequence as input and output RNA-seq or CAGE profiles as predictions. These models never see RNA sequence directly; they learn the mapping from DNA context to transcriptional output. The molecular perspective treats RNA sequence as input and predicts structure, function, or design properties.\nThe distinction parallels the difference between protein language models and proteomics prediction models. ESM takes amino acid sequences and learns structural representations (Chapter 15). A model predicting protein abundance from genomic features would be solving a different problem. Both perspectives are valuable, and both ultimately concern RNA, but they operate at different levels of the biological hierarchy and require different architectures and training strategies.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-perspectives",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-perspectives",
    "title": "18  RNA Structure and Function",
    "section": "",
    "text": "Table 18.1: Two perspectives on RNA modeling, distinguished by input modality and biological question.\n\n\n\n\n\n\n\n\n\n\n\n\nPerspective\nInput\nOutput\nExample Models\nKey Questions\n\n\n\n\nMolecular\nRNA sequence\nStructure, binding, design\nRNA-FM, SPOT-RNA, structure probing models\nHow does this RNA fold? What binds it?\n\n\nTranscriptomic\nDNA sequence + context\nExpression, splicing, coverage\nEnformer, Borzoi, SpliceAI\nHow much transcript is produced? Where is it spliced?",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-structure-challenge",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-structure-challenge",
    "title": "18  RNA Structure and Function",
    "section": "18.2 Why Secondary Structure Creates a Distinct Modeling Challenge",
    "text": "18.2 Why Secondary Structure Creates a Distinct Modeling Challenge\n\n\n\n\n\n\nStop and Think\n\n\n\nProteins fold into stable three-dimensional structures, and protein language models have learned to predict these folds with remarkable accuracy. Before reading on, consider: why might RNA structure prediction be fundamentally harder than protein structure prediction, despite RNA having only 4 bases compared to 20 amino acids?\n\n\nRNA secondary structure prediction differs fundamentally from protein structure prediction in ways that shape every modeling choice. Where DNA language models learn from linear sequence patterns (Chapter 14) and protein models exploit evolutionary constraints across homologs (Chapter 15), RNA models must contend with conformational dynamics that neither domain faces at comparable scale. Three interrelated challenges define the problem: thermodynamic landscapes with multiple competing minima, base-pairing interactions that span hundreds of nucleotides, and pseudoknot structures (configurations where bases in a loop pair with bases outside that loop, creating interleaved patterns that standard algorithms cannot efficiently handle) that violate the assumptions of efficient algorithms. Understanding these challenges clarifies why RNA structure remains harder to predict than protein structure despite the apparent simplicity of four bases versus twenty amino acids.\n\n18.2.1 Flat Energy Landscape Problem\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading about RNA’s energy landscape, predict: if an RNA sequence can fold into multiple different structures with similar energies, what would this mean for predicting “the” structure of that RNA? How does this differ from protein folding?\n\n\nRNA’s defining computational challenge emerges from thermodynamics. Proteins fold into stable three-dimensional structures because their energy landscapes contain deep minima: the native state sits in a pronounced funnel that guides the folding process. RNA energy landscapes are remarkably flatter—like a marble on a dinner plate with multiple shallow dents rather than a deep bowl. The marble settles in one dent but can easily roll to another with a small nudge. Multiple conformations compete for occupancy, with free energy differences often smaller than thermal fluctuations at cellular temperatures. A given RNA sequence may adopt several alternative structures with similar stabilities, and the dominant conformation can shift in response to ion concentrations, temperature, protein binding, or chemical modifications.\n\n\n\n\n\n\nKey Insight\n\n\n\nRNA’s flat energy landscape means that the same sequence can adopt multiple different structures with similar stabilities. This creates a fundamental many-to-many relationship between sequence and structure that protein modeling largely avoids—most proteins have a single dominant fold. RNA models must somehow represent or accommodate this conformational heterogeneity.\n\n\nThis conformational plasticity has biological functions (riboswitches that change structure in response to ligand binding, RNA thermometers that regulate translation at different temperatures) but creates modeling difficulties. Minimum free energy (MFE) predictions, which identify the single lowest-energy structure, may miss functionally relevant alternative conformations. Partition function calculations that consider the full ensemble are more complete but computationally expensive and harder to interpret. Deep learning models that predict structure from sequence must somehow capture this many-to-many relationship between sequence and conformation, a challenge that protein structure prediction largely avoided because the sequence-to-structure mapping for most proteins is effectively one-to-one.\n\n\n\n\n\n\n\n\nProtein folding shows deep funnel topology\n\n\n\n\n\n\n\nRNA folding shows flat landscape with multiple minima\n\n\n\n\n\n\nFigure 18.1: Energy landscape comparison between protein and RNA folding. (A) Proteins fold into deep energy minima, typically reaching a single stable native structure. (B) RNA energy landscapes are flatter, with multiple conformations competing at similar free energies—explaining why RNA can adopt alternative structures under different conditions.\n\n\n\n\n\n18.2.2 Base Pairing and Long-Range Dependencies\nSecondary structure arises from Watson-Crick base pairing (A-U, G-C) and wobble pairs (G-U) that create stems, loops, bulges, and internal loops. Unlike protein secondary structure, where alpha helices and beta sheets are local motifs determined by nearby residues, RNA secondary structure involves long-range contacts. A base at position i may pair with a base at position j hundreds of nucleotides away. The intervening sequence must accommodate this pairing without introducing steric clashes or thermodynamically unfavorable arrangements.\nThis long-range dependency structure differs fundamentally from protein contact prediction, where most important contacts occur between residues close in primary sequence. RNA structure prediction must consider all possible pairings across the entire sequence, evaluate their compatibility, and identify the globally optimal (or near-optimal) arrangement. The number of possible secondary structures grows exponentially with sequence length, making exhaustive enumeration intractable for long RNAs.\nWorked Example: Consider a 100-nucleotide RNA. Position 10 might pair with position 90, creating a stem. But positions 10-15 might alternatively pair with positions 45-50, creating a different stem. These alternatives are mutually exclusive—a base can only pair with one partner—but both might have similar free energies. The algorithm must evaluate all such possibilities and find the globally best combination, not just locally optimal stems.\n\n\n18.2.3 Pseudoknots and Tertiary Complexity\n\n\n\n\n\n\nStop and Think\n\n\n\nStandard RNA structure prediction algorithms use dynamic programming with O(n³) complexity by assuming nested base pairs. What happens to this assumption when bases in a loop pair with bases outside that loop? Why would this dramatically increase computational complexity?\n\n\nPseudoknots occur when bases in a loop pair with bases outside that loop, creating interleaved base-pairing patterns that violate the nested structure assumed by standard secondary structure algorithms. A typical pseudoknot involves two stem regions whose base pairs cross each other when drawn in standard notation. These structures are functionally important (the telomerase RNA catalytic core contains a pseudoknot essential for activity) but algorithmically challenging. Standard dynamic programming approaches for secondary structure prediction exclude pseudoknots because their inclusion increases computational complexity from \\(O(n^3)\\) to \\(O(n^6)\\) or worse. [Citation Needed] The complexity increase arises because nested base pairs can be solved by recursive decomposition—if positions \\(i\\) and \\(j\\) pair, the structure between them is independent of the structure outside—but pseudoknots violate this independence by creating dependencies between the “inside” and “outside” regions. The algorithm must therefore consider all possible ways the crossing pairs might interleave, leading to exponentially more states.\n\n\n\n\n\n\n\n\nBasic structural elements: stems, loops, bulges\n\n\n\n\n\n\n\nLong-range base pairing spans hundreds of nucleotides\n\n\n\n\n\n\n\n\n\nPseudoknots increase computational complexity\n\n\n\n\n\n\n\nCommon notation systems for secondary structure\n\n\n\n\n\n\nFigure 18.2: RNA secondary structure vocabulary. (A) Basic structural elements: stems, hairpin loops, internal loops, bulges, and multi-loop junctions. (B) Long-range base pairing: RNA pairs can span hundreds of nucleotides, unlike protein secondary structure. (C) Pseudoknots: interleaved base pairs that increase prediction complexity from O(n³) to O(n⁶). (D) Common notation systems for representing secondary structure.\n\n\n\nTertiary structure involves the three-dimensional arrangement of secondary structure elements in space, including long-range interactions mediated by non-Watson-Crick base pairs, metal ion coordination, and RNA-RNA kissing loops. Predicting RNA tertiary structure remains far less developed than protein tertiary structure prediction. No RNA equivalent of AlphaFold exists, and the training data situation is dire: the Protein Data Bank contains over 200,000 protein structures but fewer than 2,000 RNA structures, many of which are ribosomal RNA fragments or tRNA variants from the same structural families. [Citation Needed]",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-classical",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-classical",
    "title": "18  RNA Structure and Function",
    "section": "18.3 Classical Approaches to Structure Prediction",
    "text": "18.3 Classical Approaches to Structure Prediction\nBefore deep learning entered the field, two complementary paradigms dominated RNA structure prediction. Thermodynamic approaches compute minimum free energy structures from experimentally calibrated energy parameters, while comparative methods infer structure from patterns of compensatory mutations across homologous sequences. Both approaches remain valuable, and understanding their strengths and limitations illuminates what deep learning models must learn to surpass them.\n\n18.3.1 Thermodynamic Folding Models\nThe dominant classical paradigm for RNA secondary structure prediction relies on nearest-neighbor thermodynamic models. These approaches assign free energy contributions to each base pair and structural element (loops, bulges, internal loops, multiloops) based on experimentally calibrated parameters. Given these parameters, dynamic programming algorithms identify the minimum free energy structure or compute the partition function over all possible structures.\nMfold and the ViennaRNA package represent the most widely used implementations. [Citation Needed] These programs use dynamic programming to fill a table of optimal substructure scores: for each possible base pair \\((i,j)\\), they compute the minimum free energy achievable for the subsequence between \\(i\\) and \\(j\\), building up from small subsequences to the full molecule. The algorithm works because RNA secondary structure is “nested”—if \\((i,j)\\) pairs, then any other pair \\((k,l)\\) either lies entirely within \\((i,j)\\) or entirely outside it, never crossing. This nesting property enables the \\(O(n^3)\\) dynamic programming solution.\nThese methods achieve reasonable accuracy for short, well-behaved RNAs where the thermodynamic parameters are most reliable. Limitations emerge for longer RNAs where the flat energy landscape means many structures have similar energies, for RNAs in complex cellular environments where proteins and other factors alter folding, and for RNAs with modifications or non-canonical interactions not captured by standard parameter sets. These methods also assume equilibrium conditions that may not hold for co-transcriptional folding or kinetically trapped states.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nThermodynamic folding methods find the minimum free energy structure. Given what you’ve learned about RNA’s flat energy landscape, what is the main limitation of this approach? (Hint: think about alternative conformations.)\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe main limitation is that MFE methods predict only the single lowest-energy structure, missing functionally relevant alternative conformations that may have similar free energies. RNA’s flat energy landscape means multiple structures can coexist with comparable stability, so the global minimum may not be the only biologically important state.\n\n\n\n\n\n\n\n18.3.2 Comparative and Covariation Methods\nFor RNAs with sufficient homologous sequences, comparative approaches provide an orthogonal route to structure inference. If two positions exhibit compensatory mutations (G-C changing to A-U while maintaining complementarity), those positions likely base-pair. Databases like Rfam curate consensus secondary structures for RNA families based on these covariation signals. [Citation Needed]\nComparative methods are powerful but require multiple sequence alignments of homologous RNAs. Novel RNAs, rapidly evolving regulatory elements, or species-specific transcripts may lack sufficient homologs for reliable inference. The approach also assumes that structure is conserved across the aligned sequences, which breaks down for RNAs that have diverged in function or that adopt condition-specific alternative structures.\n\n\n\nTable 18.2: Comparison of RNA structure prediction approaches. Each has complementary strengths.\n\n\n\n\n\n\n\n\n\n\n\nApproach\nStrengths\nLimitations\nBest Use Case\n\n\n\n\nThermodynamic (MFE)\nNo homologs needed; well-understood parameters\nMisses alternative structures; struggles with long RNAs\nShort RNAs, initial prediction\n\n\nPartition function\nConsiders ensemble; provides base-pair probabilities\nComputationally expensive; hard to interpret\nAssessing structural confidence\n\n\nComparative\nHigh accuracy when homologs available; reveals conserved structure\nRequires homologs; assumes structure conservation\nWell-characterized RNA families\n\n\nDeep learning\nLearns complex patterns; can handle pseudoknots\nRequires training data; may not generalize\nStructure probing integration",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-dl-structure",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-dl-structure",
    "title": "18  RNA Structure and Function",
    "section": "18.4 Deep Learning for Secondary Structure Prediction",
    "text": "18.4 Deep Learning for Secondary Structure Prediction\nDeep learning reframes secondary structure prediction as sequence-to-structure mapping, learning the relationship directly from data rather than encoding it through thermodynamic parameters. These models can capture patterns that classical approaches miss, particularly for complex structures and pseudoknots, though they require training data that remains limited compared to protein structure prediction. Two complementary training strategies have emerged: supervised learning from experimentally determined structures and semi-supervised approaches using structure probing data.\n\n18.4.1 From Thermodynamics to Learned Patterns\nDeep learning models for RNA structure prediction frame the task as sequence-to-structure mapping, analogous to protein contact prediction (Chapter 15). Given an RNA sequence, the model predicts base-pairing probabilities for all position pairs, contact maps indicating which bases interact, or per-nucleotide structural states (paired, unpaired, in loop, in stem).\nModels like SPOT-RNA use convolutional or attention-based architectures to capture long-range dependencies in sequence. [Citation Needed] Some approaches directly predict pairing matrices as dense outputs; others output per-position classifications that are post-processed into structures. Training typically uses experimentally determined structures from databases like RNAstralign or bpRNA, supplemented by computationally predicted structures from thermodynamic models.\nPerformance on benchmark datasets often exceeds classical thermodynamic methods, particularly for RNAs with complex structures or pseudoknots where dynamic programming approaches struggle. The learned models can capture patterns beyond nearest-neighbor rules, potentially encoding longer-range sequence dependencies that contribute to folding but were not parameterized in classical approaches.\n\n\n18.4.2 Structure Probing as Supervision\nHigh-throughput structure probing experiments provide an alternative source of supervision. SHAPE (selective 2’-hydroxyl acylation analyzed by primer extension), DMS-seq, and icSHAPE measure nucleotide accessibility or flexibility across entire transcriptomes. Positions that are base-paired or buried in tertiary structure show lower reactivity than exposed positions. [Citation Needed]\nThese data offer several advantages for model training. They cover far more RNAs than crystal structures, extending beyond well-characterized families to regulatory elements and novel transcripts. They capture structure in cellular context, reflecting the influence of proteins, modifications, and physiological conditions. And they provide soft constraints rather than binary pairing assignments, potentially better matching the conformational heterogeneity of real RNA populations.\nModels trained on structure probing data learn to predict accessibility profiles from sequence. These predictions can be integrated with thermodynamic models (using predicted accessibility as constraints) or used directly for downstream tasks like predicting RNA-protein binding or designing stable constructs.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-foundation",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-foundation",
    "title": "18  RNA Structure and Function",
    "section": "18.5 RNA Foundation Models",
    "text": "18.5 RNA Foundation Models\n\n\n\n\n\n\nStop and Think\n\n\n\nProtein language models like ESM-2 trained on 65 million sequences and learned to predict protein structure from sequence alone. Before reading on, consider: what would an RNA foundation model need to achieve a similar breakthrough? What resources or data might be missing?\n\n\nThe success of protein language models naturally prompted attempts to apply the same paradigm to RNA. Train large transformers on massive sequence corpora following the scaling principles examined in Section 13.3, learn representations through self-supervised objectives (Chapter 8), then transfer to downstream tasks. RNA foundation models exist and show promise, but they have not yet achieved the transformative impact of their protein counterparts. The reasons illuminate fundamental differences between protein and RNA modeling.\n\n18.5.1 Scale Gap with Protein Language Models\n\n\n\n\n\n\nStop and Think\n\n\n\nESM-2 learned to predict protein structure from sequence alone, with attention patterns that correspond to 3D contacts—an emergent capability that surprised even its creators. Why haven’t RNA foundation models achieved a similar breakthrough? Before reading on, consider what differences between protein and RNA data might explain this gap.\n\n\nRNA foundation models attempt to replicate the protein language model paradigm: train large transformers on massive sequence corpora using self-supervised objectives, then transfer learned representations to downstream tasks. The approach has produced working models, but the results lag substantially behind protein language models in both scale and demonstrated capabilities.\nThe comparison with ESM illustrates the gap. ESM-2 trained on over 65 million protein sequences from UniRef, spanning the known diversity of protein families (Chapter 15). RNA-FM, one of the more successful RNA foundation models, trained on approximately 23 million noncoding RNA sequences from RNAcentral (Chen et al. 2022). While not a trivial corpus, this represents an order of magnitude fewer sequences, and the RNA sequences span a narrower range of structural and functional diversity than proteins. The consequences appear in downstream performance: RNA-FM improves over baselines on secondary structure prediction and family classification, but shows nothing like the emergent structure prediction that made ESM-2’s attention patterns predict contact maps without supervision.\n\n\n\n\n\n\nKey Insight\n\n\n\nThe gap between RNA and protein foundation models is not primarily about architecture or training objectives—it’s about data. Proteins have accumulated over 4 billion years of evolution across all domains of life, with rich structural annotations from crystallography and cryo-EM. RNA databases are smaller, biased toward well-characterized families, and lack equivalent structural training sets. Until this data gap closes, RNA foundation models will struggle to achieve protein-like breakthroughs.\n\n\nSeveral factors explain the disparity. Protein sequences have accumulated over 4 billion years of evolution across all domains of life, with each functional protein family represented by thousands of homologs. RNA databases are biased toward well-characterized structural families (tRNAs, rRNAs, ribozymes) with sparser coverage of regulatory ncRNAs and lineage-specific transcripts. The epitranscriptomic modifications that alter RNA function are invisible in sequence databases, unlike protein post-translational modifications that at least occur at predictable sequence motifs.\n\n\n\n\n\n\n\n\nKey metrics comparison between protein and RNA FMs\n\n\n\n\n\n\n\nTraining data composition differs dramatically\n\n\n\n\n\n\n\n\n\nEmergent capabilities comparison\n\n\n\n\n\n\n\nThe fundamental structural data challenge\n\n\n\n\n\n\nFigure 18.3: The scale gap between protein and RNA foundation models. (A) Key metrics comparison showing protein models train on 3× more sequences with much greater diversity. (B) Training data composition: protein data span diverse families while RNA databases are dominated by well-characterized classes. (C) Emergent capabilities comparison showing protein models achieve structure prediction that RNA models lack. (D) The fundamental data challenge: fewer than 2,000 RNA structures versus over 200,000 protein structures in the PDB.\n\n\n\n\n\n18.5.2 Architectures and Objectives\nMost RNA foundation models follow the masked language modeling (MLM) paradigm established by BERT (Chapter 5). RNA-FM uses a transformer encoder with nucleotide-level tokenization, predicting masked bases from surrounding context. The learned embeddings show some correspondence to secondary structure when probed with downstream tasks, though the correspondence is weaker than the structure-function relationship learned by protein language models.\nAlternative architectures explore different design choices. Some models incorporate explicit structure tokens or operate on sequence-structure graphs, learning joint representations over both modalities. Others use codon-level tokenization for coding RNAs or explore state-space models and other efficient attention variants to handle longer sequences. RNAErnie and related models experiment with multi-task objectives that combine MLM with auxiliary predictions for structural features or family classification. [Citation Needed]\nThe field remains in active development, with no clear consensus on optimal architecture, tokenization, or training strategy. Unlike protein modeling, where ESM established a dominant paradigm that subsequent work has refined, RNA modeling still explores fundamental design choices.\n\n\n18.5.3 Downstream Applications\nRNA foundation model embeddings support various downstream tasks. Secondary structure prediction fine-tunes the model to output pairing probabilities or SHAPE reactivity profiles. RNA-protein binding prediction uses CLIP-seq data to predict interactions with RNA-binding proteins. Family classification assigns sequences to Rfam families or functional categories (tRNA, rRNA, miRNA, lncRNA). Expression and stability tasks predict transcript half-life or steady-state levels from UTR sequences.\nPerformance varies substantially across tasks. For structurally constrained RNAs like tRNAs and rRNAs, where sequence motifs strongly determine structure and function, foundation model embeddings provide useful features. For regulatory lncRNAs that often lack stable secondary structures and conserved motifs, improvement over baseline methods is more modest. The diversity of RNA types and tasks complicates benchmarking (Chapter 12), and models that excel on one task may struggle on others.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-codon",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-codon",
    "title": "18  RNA Structure and Function",
    "section": "18.6 Codon-Level Models for Coding RNA",
    "text": "18.6 Codon-Level Models for Coding RNA\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider two mRNA sequences that encode the exact same protein but differ in their codon choices (synonymous codons). A protein language model sees identical sequences. What biological information exists in the mRNA that the protein model completely misses?\n\n\nCoding sequences present a modeling opportunity that neither DNA nor protein foundation models fully exploit. The genetic code’s synonymous redundancy means that mRNA sequence carries information beyond amino acid identity: codon choice affects translation speed, mRNA stability, and co-translational folding. Codon-level foundation models tokenize mRNA into three-nucleotide units, learning representations that capture these codon-specific signals invisible to protein language models.\n\n18.6.1 Beyond Nucleotide Tokenization\nCoding sequences occupy a special niche where protein and nucleic acid constraints intersect. The genetic code assigns 61 sense codons to 20 amino acids, creating synonymous redundancy where multiple codons encode the same amino acid. This redundancy is not functionally neutral: synonymous codons differ in tRNA availability, translation speed, co-translational folding effects, and mRNA stability. Protein language models, which operate on amino acid sequences, cannot capture these codon-level signals.\n\n\n\n\n\n\nStop and Think\n\n\n\nA protein language model like ESM sees only amino acid sequence. A DNA language model like DNABERT sees nucleotide sequence but treats all positions equally. What information is lost by each approach when modeling a coding sequence? What could a codon-level model capture that neither can?\n\n\nCodon-level foundation models address this gap by tokenizing mRNA into codons rather than nucleotides. Models like cdsFM, EnCodon, and DeCodon treat each three-nucleotide codon as a single token, training on masked codon prediction and related objectives (Naghipourfar et al. 2024). This tokenization encodes a biological prior: codons are the fundamental units of translation, and mutations at the codon level determine amino acid changes while mutations within synonymous codons affect expression without changing protein sequence.\nThe codon vocabulary contains 61 tokens (excluding stop codons) plus special tokens for noncoding regions and boundaries. This intermediate vocabulary size (between character-level nucleotide tokenization and typical BPE vocabularies of thousands of tokens) balances resolution with context length (Chapter 5). A 300-amino-acid protein corresponds to 900 nucleotides or 300 codons, making whole-gene modeling tractable within standard transformer context windows. The therapeutic implications of codon-level modeling are examined in ?sec-ch28-mrna-design, where these representations guide mRNA vaccine and protein replacement therapy design.\n\n\n\n\n\n\n\n\nMultiple mRNA sequences can encode the same protein\n\n\n\n\n\n\n\nCodon selection affects multiple biological properties\n\n\n\n\n\n\n\n\n\nCodon tokenization reduces sequence length\n\n\n\n\n\n\n\nDifferent model types capture different information\n\n\n\n\n\n\nFigure 18.4: Codon-level modeling of mRNA. (A) Multiple mRNA sequences can encode the same protein through synonymous codon choices. (B) Codon selection affects tRNA availability, translation speed, mRNA structure, and stability. (C) Codon tokenization reduces sequence length while encoding biological priors about translation units. (D) Comparison of what protein, DNA, and codon language models can capture from coding sequences.\n\n\n\n\n\n18.6.2 What Codon Models Add\n\n\n\n\n\n\nStop and Think\n\n\n\nYou’re designing a therapeutic mRNA to express a protein in human cells. The amino acid sequence is fixed (you need that exact protein). But you have roughly 3^300 possible mRNA sequences to choose from for a 300-amino-acid protein. What criteria would guide your choice among synonymous alternatives?\n\n\nCompared to protein language models, codon-level models enable direct modeling of mRNA design problems where amino acid sequence is fixed but codon choice is variable. They capture codon usage bias and its relationship to expression, model translation elongation dynamics that affect co-translational folding, and distinguish synonymous variants that are neutral at the protein level but affect mRNA properties.\nLife-Code extends this approach into a central-dogma-wide framework, linking DNA, RNA, and protein representations through shared or aligned embedding spaces (Liu et al. 2025). CodonBERT specifically targets mRNA design for vaccines and therapeutics, training on over 10 million mRNA sequences to learn representations that predict expression, stability, and immunogenicity (Li et al. 2023).\nCodon models typically ignore mRNA secondary structure and modifications. Local structure affects ribosome access and translation rate; modifications like m6A influence stability and localization. Combining codon-aware tokenization with structure-aware representations remains an open direction, less mature than the parallel integration of sequence and structure in protein modeling (Chapter 15).\n\n\n\nTable 18.3: Comparison of model types for coding sequences. Codon models fill a gap between DNA and protein approaches.\n\n\n\n\n\n\n\n\n\n\n\n\nModel Type\nInput\nCaptures Codon Bias\nCaptures Structure\nUse Case\n\n\n\n\nProtein LM\nAmino acids\nNo\nProtein structure\nVariant effects, function\n\n\nDNA LM\nNucleotides\nPartially (no boundaries)\nNo\nRegulatory sequence\n\n\nCodon LM\nCodons\nYes\nNo (typically)\nmRNA design, translation\n\n\nHybrid\nCodon + structure\nYes\nYes\nFuture direction",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-utr",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-utr",
    "title": "18  RNA Structure and Function",
    "section": "18.7 UTR Models and Translation Regulation",
    "text": "18.7 UTR Models and Translation Regulation\n\n\n\n\n\n\nStop and Think\n\n\n\nTwo mRNAs encode identical proteins with identical codon sequences, but one produces 100-fold more protein than the other. Where would you look to explain this difference, and what mechanisms might be responsible?\n\n\nThe untranslated regions flanking a coding sequence determine how much protein an mRNA produces and how long the message survives in the cell. These regulatory effects operate through distinct mechanisms in the 5’ and 3’ UTRs, creating opportunities for both understanding endogenous regulation and engineering synthetic mRNAs with desired expression properties.\n\n18.7.1 Why UTRs Dominate Expression Control\nThe protein output of an mRNA depends as much on its untranslated regions as on its coding sequence. A transcript’s 5’ UTR determines whether ribosomes find and engage the start codon; its 3’ UTR controls how long the message survives and where in the cell it localizes—like an expiration date stamped on perishable goods, except here the “date” is encoded in sequence features that cellular machinery reads to decide when to degrade the transcript. Two mRNAs encoding identical proteins can differ by orders of magnitude in expression if their UTRs differ. This regulatory leverage makes UTR modeling essential for both understanding endogenous gene regulation and designing synthetic mRNAs for therapeutic applications.\nThe 5’ UTR spans from the transcription start site to the start codon, typically 50 to 200 nucleotides in human mRNAs. Within this region, secondary structure can occlude the start codon and impede ribosome scanning, upstream open reading frames (uORFs) can capture ribosomes before they reach the main coding sequence, and internal ribosome entry sites (IRES) can enable cap-independent translation under stress conditions. The Kozak consensus sequence surrounding the start codon influences initiation efficiency, but context extending dozens of nucleotides in either direction modulates this effect. Predicting translation efficiency from 5’ UTR sequence requires integrating these overlapping signals.\nThe 3’ UTR extends from the stop codon to the poly-A tail, ranging from under 100 nucleotides to over 10 kilobases. This region harbors binding sites for RNA-binding proteins and microRNAs that collectively determine mRNA half-life, localization, and translational status. AU-rich elements (AREs) recruit decay machinery in response to cellular signals. Pumilio and other RNA-binding proteins recognize specific motifs to repress or activate translation. The density and arrangement of miRNA binding sites create combinatorial regulatory logic that varies across cell types depending on which miRNAs are expressed.\n\n\n\n\n\n\nPractical Guidance: UTR Design Considerations\n\n\n\nWhen designing or optimizing mRNAs for expression:\n\n5’ UTR: Avoid strong secondary structure near start codon; eliminate upstream AUGs that create uORFs; ensure Kozak consensus (GCCRCCAUGG); consider GC content for stability vs. structure trade-offs\n3’ UTR: Consider borrowing UTRs from highly expressed endogenous genes (alpha-globin, beta-globin are common choices); avoid sequences resembling miRNA binding sites for target tissues; balance length (longer = more regulatory sites) against manufacturing constraints\nIntegration: Remember that 5’ and 3’ effects interact—optimize iteratively, not independently\n\n\n\n\n\n18.7.2 Sequence-to-Expression Models\nHigh-throughput reporter assays have enabled systematic modeling of UTR function. Massively parallel reporter assays (MPRAs) measure expression driven by thousands of UTR variants in a single experiment, providing training data at scales previously unavailable. Sample et al. used such data to train Optimus 5-Prime, a convolutional model that predicts ribosome load from 5’ UTR sequence with accuracy sufficient to guide synthetic UTR design (Sample et al. 2019). The model learned interpretable features corresponding to known regulatory elements (uORF presence, Kozak strength, secondary structure) while also capturing context-dependent interactions invisible to element-counting approaches.\nFor 3’ UTRs, models must contend with greater length and combinatorial complexity. A 2-kilobase 3’ UTR may contain dozens of potential regulatory sites whose effects depend on spacing, secondary structure context, and the expression levels of cognate binding proteins. Approaches range from motif-based models that score individual elements and sum contributions, to deep learning architectures that process entire UTR sequences and learn nonlinear interactions. Agarwal and Kelley trained models on endogenous mRNA stability measurements, demonstrating that 3’ UTR sequence features explain substantial variance in half-life across the transcriptome (Agarwal and Shendure 2020).\nTransfer learning from RNA foundation models offers a complementary approach. Rather than training UTR-specific models from scratch, pretrained representations from RNA-FM or similar models can be fine-tuned on expression prediction tasks (Chapter 5). The pretrained embeddings encode sequence context and potential structural features that may transfer to UTR function prediction, though systematic comparisons between foundation model transfer and task-specific training remain limited.\n\n\n18.7.3 Integration with mRNA Design\nUTR optimization represents a distinct component of therapeutic mRNA design, complementing codon optimization. For a vaccine or protein replacement therapy, the coding sequence determines what protein is made while the UTRs determine how much protein is made and for how long. Current mRNA therapeutics typically use UTRs borrowed from highly expressed endogenous genes (human alpha-globin and beta-globin UTRs are common choices) rather than computationally optimized sequences. [Citation Needed]\nModel-guided UTR design could improve on this approach by optimizing for specific objectives: maximizing expression in target tissues, extending mRNA half-life to reduce dosing frequency, or minimizing immunogenicity by avoiding sequences that trigger innate immune sensors. The challenge lies in the combinatorial interaction between UTRs and coding sequence. Secondary structures can span the UTR-CDS boundary, and the optimal 5’ UTR for one coding sequence may perform poorly for another. Integrated models that jointly optimize UTRs and coding sequence represent an active research direction, though experimental validation of computationally designed UTRs remains limited compared to the extensive optimization of coding sequences. The design principles and optimization strategies for therapeutic mRNAs, including COVID-19 vaccine development, are detailed in ?sec-ch28-utr-design.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-mrna-design",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-mrna-design",
    "title": "18  RNA Structure and Function",
    "section": "18.8 mRNA Design and Optimization",
    "text": "18.8 mRNA Design and Optimization\nTherapeutic mRNA design requires navigating a vast sequence space where multiple objectives compete. Expression, stability, immunogenicity, and manufacturability all depend on sequence choices that interact in complex ways. The COVID-19 vaccines demonstrated that rational mRNA design can achieve clinical efficacy, while also revealing how much of current practice remains empirical rather than model-driven.\n\n18.8.1 Design Objectives and Trade-offs\n\n\n\n\n\n\nMathematical Note\n\n\n\nThe following section discusses the vast combinatorial space of possible mRNA sequences. The key intuition is that for any protein, there are astronomically many different mRNA sequences that could encode it (because multiple codons encode each amino acid). This space is far too large for exhaustive search, motivating model-guided optimization.\n\n\nmRNA sequence design selects nucleotide sequences that encode a desired protein while optimizing expression, stability, safety, and manufacturability. For a 300-amino-acid protein, there are approximately \\(3^{300}\\) possible synonymous mRNA sequences (roughly the number of synonymous codons raised to the protein length). This astronomical space cannot be exhaustively searched, motivating both classical heuristics and modern machine learning approaches.\nKey objectives include high protein expression in target tissues, mRNA stability during manufacturing and in vivo, controlled translation kinetics that influence co-translational folding, and low immunogenicity for therapeutic applications. These objectives often conflict: increasing GC content may improve stability but introduce unwanted secondary structure, while avoiding rare codons may reduce expression if tRNA pools are limiting. The conflict between stability and expression illustrates a broader principle: GC-rich sequences form more stable base pairs (G-C has three hydrogen bonds versus two for A-U), which extends mRNA half-life but can also create secondary structures that impede ribosome scanning and translation initiation. The design challenge is finding the Pareto frontier where no objective can be improved without sacrificing another.\nWorked Example: Consider designing an mRNA to express a therapeutic enzyme in liver cells. You might:\n\nStart with the wild-type human codon usage (baseline)\nReplace rare codons with more frequent synonymous alternatives (increases expression)\nCheck for unwanted secondary structure in 5’ UTR region (may impede translation)\nVerify absence of cryptic splice sites or premature polyadenylation signals\nAssess GC content (&gt;60% may cause aggregation; &lt;40% may reduce stability)\nUse models to predict expression and iterate\n\nEach choice affects multiple objectives—there is no single “optimal” sequence, only trade-offs.\n\n\n18.8.2 Lessons from COVID-19 Vaccines\nThe COVID-19 mRNA vaccines provided a high-profile demonstration of mRNA design principles at unprecedented scale. The Pfizer-BioNTech and Moderna vaccines incorporated several design elements: N1-methylpseudouridine modification throughout the sequence to reduce innate immune activation, codon optimization to enhance expression in human cells, optimized 5’ and 3’ UTRs from highly expressed genes, and sequence modifications to stabilize the prefusion spike conformation. These choices drew on decades of basic research but were refined through empirical optimization rather than systematic model-based design. [Citation Needed]\nThe vaccines’ success demonstrated that rationally designed mRNAs can achieve therapeutic efficacy at scale. It also revealed limitations in current understanding: the optimal combination of modifications, codons, and UTRs for a given protein target remains partly empirical, and transferring designs across proteins or therapeutic applications requires substantial optimization.\n\n\n18.8.3 Model-Based Design Strategies\nRNA and codon foundation models enable several approaches to systematic design. Scoring and screening use pretrained models to evaluate large candidate sets for predicted expression or stability, selecting top designs for experimental validation. When models are differentiable with respect to input embeddings, gradient-based methods can guide sequence optimization toward desired objectives. Generative approaches sample diverse high-scoring sequences subject to constraints like fixed amino acid sequence or avoided motifs.\nEmpirical results suggest that deep models trained on high-throughput reporter assays or ribosome profiling can outperform classical codon adaptation indices like CAI or tAI, particularly for context-specific expression prediction. Classical indices rely on genome-wide codon frequencies that may not reflect the relevant cellular context, while deep models can learn local effects of codon pairs, mRNA structure, and regulatory elements. These models require substantial training data and may not generalize across organisms or synthetic constructs far from natural sequences.\n\n\n\n\n\n\nTherapeutic mRNA design pipeline from target to optimized construct\n\n\n\n\nFigure 18.5: Therapeutic mRNA design pipeline. Starting from a target protein, the design process optimizes codon usage (selecting from ~3300 possible synonymous sequences), engineers 5’ UTR elements for translation initiation, designs 3’ UTR for stability and localization, and selects chemical modifications (such as N1-methylpseudouridine) to reduce immunogenicity. Inset shows key design choices made for COVID-19 mRNA vaccines.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-ncrna",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-ncrna",
    "title": "18  RNA Structure and Function",
    "section": "18.9 Noncoding RNA Classification and Function",
    "text": "18.9 Noncoding RNA Classification and Function\nRNA that does not encode protein encompasses an extraordinary range of structures, functions, and regulatory mechanisms. Classifying these transcripts and predicting their functions presents challenges that differ from coding sequence analysis: the relevant features vary across RNA classes, functional annotations remain incomplete, and the boundary between functional ncRNA and transcriptional noise is often unclear.\n\n18.9.1 Diversity of Noncoding RNA\nRNA that does not encode protein spans an enormous functional and structural range. Housekeeping RNAs (tRNAs, rRNAs, snRNAs, snoRNAs) perform essential cellular functions with well-characterized structures. Regulatory RNAs (miRNAs, siRNAs, piRNAs, lncRNAs) control gene expression through diverse mechanisms. Structural and catalytic RNAs (ribozymes, riboswitches) adopt complex folds that enable enzymatic activity or ligand sensing. Circular RNAs (circRNAs) and other noncanonical species continue to expand the catalog of RNA diversity.\nEach class has characteristic lengths, structural motifs, genomic contexts, and functional mechanisms. tRNAs are approximately 76 nucleotides with a conserved cloverleaf structure. miRNAs are approximately 22 nucleotides processed from longer hairpin precursors. lncRNAs span thousands of nucleotides with poorly conserved sequence and often no stable secondary structure. Unifying these classes under a single modeling framework is challenging, and models that excel on one class may fail on others.\n\n\n\nTable 18.4: Diversity of noncoding RNA classes. Each presents distinct modeling challenges.\n\n\n\n\n\n\n\n\n\n\n\n\n\nncRNA Class\nTypical Length\nStructure\nConservation\nFunction\nModeling Challenge\n\n\n\n\ntRNA\n~76 nt\nCloverleaf (conserved)\nHigh\nAmino acid delivery\nWell-characterized; good models exist\n\n\nmiRNA\n~22 nt\nProcessed from hairpin\nModerate\nPost-transcriptional silencing\nTarget prediction remains noisy\n\n\nlncRNA\n200 - &gt;10,000 nt\nVariable/none\nLow\nDiverse\nFunctional annotation sparse\n\n\ncircRNA\nVariable\nCircular backbone\nVariable\nmiRNA sponge, other\nDetection and quantification\n\n\nrRNA\n~1,500-5,000 nt\nComplex, conserved\nVery high\nRibosome structure\nWell-characterized\n\n\n\n\n\n\n\n\n18.9.2 From Handcrafted Features to Learned Representations\nClassical ncRNA classification relied on engineered features: k-mer frequencies, GC content, minimum free energy of predicted secondary structure, structural motif counts, and genomic context features like proximity to coding genes or chromatin marks. These features fed conventional classifiers (SVMs, random forests, shallow neural networks) that achieved reasonable performance for well-studied classes with strong sequence and structure signatures.\nThe limits of handcrafted features emerge most clearly for lncRNAs. These transcripts are defined partly by what they lack (no long open reading frame) rather than what they possess. Many lncRNAs show poor conservation, lack stable secondary structures, and have diverse, poorly characterized functions. Distinguishing functional lncRNAs from transcriptional noise remains difficult, and classical feature sets often collapse to generic statistics like length and GC content.\nFoundation model embeddings offer a more flexible approach. Per-nucleotide representations can be pooled into fixed-dimensional vectors that support classification with simple downstream heads. For ncRNAs without strong sequence motifs, the pretrained embeddings may capture subtle distributional patterns learned during self-supervised training. Few-shot learning becomes possible: given a handful of newly characterized RNAs, their embeddings can seed new clusters in representation space, guiding annotation of related sequences.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-mirna",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-mirna",
    "title": "18  RNA Structure and Function",
    "section": "18.10 miRNA Target Prediction",
    "text": "18.10 miRNA Target Prediction\nMicroRNAs regulate gene expression by guiding the RNA-induced silencing complex (RISC) to complementary sites in target mRNAs, typically in the 3’ UTR. A single miRNA can regulate hundreds of transcripts, and a single transcript can harbor binding sites for dozens of miRNAs. This regulatory network influences virtually every cellular process, and dysregulation of miRNA-target interactions contributes to cancer, cardiovascular disease, and neurodegeneration. Predicting which transcripts a given miRNA targets (and vice versa) has been a persistent computational challenge since the discovery of miRNA-mediated regulation.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nmiRNA target prediction relies heavily on “seed complementarity”—perfect base pairing between nucleotides 2-7 of the miRNA and the target site. Why might focusing only on seed matches miss many functional targets? What other factors could influence whether a site is actually regulated?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nSeed-only approaches miss functional targets because actual regulation depends on additional context: local RNA secondary structure may occlude seed matches, competing RNA-binding proteins may block access, miRNA and target abundance vary across cell types, and non-canonical binding modes exist without perfect seed pairing. The cellular context determines whether a seed match produces functional regulation.\n\n\n\n\n\nThe dominant paradigm centers on seed complementarity. Nucleotides 2 through 7 of the miRNA (the seed region) typically form perfect Watson-Crick pairs with target sites, while the remaining nucleotides contribute variably to binding affinity and regulatory effect. Classical algorithms like TargetScan identify conserved seed matches in 3’ UTRs and rank targets by evolutionary conservation, site type (8mer, 7mer-m8, 7mer-A1), and local sequence context (Agarwal and Shendure 2020). Additional features including AU content flanking the site, position within the UTR, and proximity to other miRNA sites improve prediction accuracy.\nDespite decades of refinement, target prediction remains noisy. Experimental validation rates for top predictions rarely exceed 50%, and many functional targets lack canonical seed matches. [Citation Needed] The disconnect arises partly from context dependence: a site may be accessible in one cell type but occluded by RNA structure or competing protein binding in another. It arises partly from the limitations of reporter assays that measure binding in artificial contexts rather than endogenous regulatory effects. And it arises from the biology itself, where weak individual sites combine additively and miRNA-target interactions are probabilistic rather than deterministic.\nDeep learning approaches attempt to improve on seed-based methods by learning complex sequence features from high-throughput binding data. Models trained on CLIP-seq experiments (which crosslink miRNA-target complexes and identify bound sites transcriptome-wide) can capture non-canonical binding modes and context effects invisible to seed-matching algorithms. These models often overfit to cell-type-specific binding patterns and generalize poorly across contexts (Chapter 12). The fundamental challenge is that miRNA targeting depends on factors beyond sequence: miRNA and target abundance, competition among targets for limiting RISC, and cellular state variables that no sequence-based model can capture.\nFor clinical applications, target prediction informs both the mechanism of disease-associated miRNAs and the design of therapeutic interventions. AntimiR oligonucleotides that sequester specific miRNAs have entered clinical trials for hepatitis C (targeting miR-122) and other indications. Predicting off-target effects of such therapeutics requires understanding the full network of targets that will be derepressed when a miRNA is inhibited. Similarly, miRNA mimics designed to replace lost tumor-suppressor miRNAs must be evaluated for potential regulation of unintended targets. In both cases, computational target prediction provides a starting point that experimental validation must refine.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-splicing",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-splicing",
    "title": "18  RNA Structure and Function",
    "section": "18.11 Splicing and Transcript Processing Models",
    "text": "18.11 Splicing and Transcript Processing Models\nSplicing models predict how pre-mRNA is processed into mature transcripts, a problem intimately connected to RNA biology even when the models operate on genomic DNA sequence. SpliceAI established the paradigm, but extensions address tissue specificity, branchpoint prediction, and quantitative splicing outcomes that the original model does not capture.\n\n18.11.1 Beyond SpliceAI\nSpliceAI demonstrated that deep convolutional networks could predict splice sites with near-spliceosomal precision (Section 6.5). The model’s success in identifying cryptic splice variants has made it a standard tool in clinical variant interpretation (Chapter 17). Splicing involves more than splice site recognition, and several extensions address aspects that SpliceAI does not fully capture.\nTissue-specific splicing patterns vary substantially across cell types and developmental stages. A splice site may be used in brain but skipped in liver due to differential expression of splicing factors. Models like Pangolin extend splice prediction by training on tissue-specific RNA-seq data, learning to predict not just whether a site is splice-competent but whether it is used in specific cellular contexts. [Citation Needed] These models enable variant interpretation that accounts for tissue-relevant splicing patterns rather than generic predictions. The integration of tissue-specific splice predictions into clinical variant interpretation workflows is addressed in Chapter 28.\nBranchpoint prediction identifies the adenosine residue where the lariat intermediate forms during splicing. While SpliceAI focuses on donor and acceptor sites, branchpoint recognition involves distinct sequence features (typically a degenerate YURAY motif 18-40 nucleotides upstream of the acceptor) that specialized models can capture. Combined analysis of donor, acceptor, and branchpoint predictions provides more complete characterization of splice-altering variants.\nAlternative splicing prediction moves beyond binary splice site identification to model exon inclusion rates and isoform usage. Models in this space attempt to predict not just whether an exon can be included but quantitative measures of inclusion across conditions, enabling analysis of splicing quantitative trait loci (sQTLs) and their effects on transcript diversity.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-limitations",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-limitations",
    "title": "18  RNA Structure and Function",
    "section": "18.12 Limitations and Open Challenges",
    "text": "18.12 Limitations and Open Challenges\nRNA modeling faces constraints that do not apply to protein or DNA foundation models. Data scarcity limits what can be learned from self-supervised training, functional annotations remain incomplete for most ncRNA classes, and the field has not yet achieved the breakthrough moment that AlphaFold represented for proteins. These limitations define the current frontier and point toward the advances needed for RNA foundation models to mature.\n\n18.12.1 Sparse Structural Data\nThe fundamental limitation of RNA modeling is data scarcity. Protein structure prediction benefits from over 200,000 experimentally determined structures; RNA has fewer than 2,000, heavily biased toward ribosomal RNA and tRNA. [Citation Needed] This scarcity limits supervised learning for tertiary structure prediction and constrains the emergence of structural knowledge from self-supervised pretraining. Until high-throughput methods generate RNA structures at scale comparable to protein crystallography and cryo-EM, RNA tertiary structure prediction will remain a frontier problem rather than a solved one.\nSecondary structure data is more abundant but still limited. Experimentally validated structures cover mainly well-characterized families, while computational predictions for novel sequences rely on thermodynamic models whose accuracy degrades for long RNAs and complex folds. Structure probing experiments provide genome-wide coverage but measure accessibility rather than pairing directly, requiring inference to convert reactivity profiles into structural models.\n\n\n18.12.2 Functional Annotation Gaps\nFor many ncRNA classes, function remains poorly characterized. LncRNA annotations often specify only genomic location and expression pattern without mechanistic understanding. Circular RNA functions are emerging but incompletely cataloged. Even for better-characterized classes like miRNAs, target prediction remains noisy and context-dependent.\nThis annotation gap limits supervised learning for function prediction and complicates evaluation (Chapter 12). When ground truth is uncertain, it becomes difficult to assess whether a model’s predictions reflect genuine biological insight or artifacts of incomplete training data. The field needs both experimental advances to characterize ncRNA function and computational approaches that can learn from weak or partial supervision.\n\n\n18.12.3 Maturity Gap\n\n\n\n\n\n\nKey Insight\n\n\n\nThe maturity gap between RNA and protein foundation models represents both a limitation and an opportunity. The protein modeling roadmap—large-scale self-supervised learning, attention mechanisms, scaling laws—exists and has been proven. Applying that roadmap to RNA requires addressing data scarcity through structure probing and synthetic data, developing architectures that handle conformational flexibility, and building comprehensive benchmarks covering RNA’s diversity.\n\n\nRNA foundation models exist but have not achieved the transformative impact of protein language models. ESM-2 enabled ESMFold, providing structure prediction from single sequences that nearly matches AlphaFold. No comparable RNA breakthrough has occurred. The reasons include data scarcity, the conformational complexity of RNA, and the diversity of RNA classes that makes unified modeling difficult.\nThis maturity gap represents both a limitation and an opportunity. The techniques that succeeded for proteins (large-scale self-supervised learning, attention mechanisms, scaling laws) provide a roadmap (Chapter 14). Applying that roadmap to RNA requires addressing the data challenge through structure probing, synthetic data generation, or more efficient use of limited experimental structures. It requires architectural innovations that handle RNA’s long-range base pairing and conformational flexibility. It requires benchmarks and evaluation frameworks that cover the full diversity of RNA types and tasks, following the rigorous evaluation principles established in Chapter 12 and the benchmark construction guidelines in Chapter 11.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch18-rna.html#sec-ch18-bridge",
    "href": "part_4/p4-ch18-rna.html#sec-ch18-bridge",
    "title": "18  RNA Structure and Function",
    "section": "18.13 Bridge Between Sequence and Cell",
    "text": "18.13 Bridge Between Sequence and Cell\nRNA occupies a distinctive position in genomic AI, bridging the sequence-level models of Part III with the cellular perspectives that follow. Splicing models like SpliceAI operate on pre-mRNA and predict transcript processing outcomes (Section 6.5). Codon-level models capture translation dynamics invisible to protein language models. mRNA therapeutic design demonstrates practical value through codon optimization, UTR engineering, and stability prediction. These applications proceed despite the absence of the structure prediction breakthrough that transformed protein modeling; secondary structure prediction has advanced through deep learning, but tertiary structure accuracy lags protein structure by a wide margin.\nThe relationship between RNA models and other modalities reflects RNA’s position in the central dogma. RNA is the product of transcription that regulatory models predict (Chapter 16), the substrate for translation that protein models assume (Chapter 15), and the primary measurement that single-cell models use to represent cellular state (Chapter 22). Foundation models that learn from RNA sequence capture patterns distinct from those in DNA or protein: codon usage biases, secondary structure constraints, and post-transcriptional regulatory elements that neither genomic nor protein models directly represent.\nBeyond sequence, biological understanding requires cellular and tissue context. Single-cell models treat RNA expression as the primary readout of cellular state, learning representations that capture cell type identity and perturbation response (Chapter 22). Three-dimensional genome models add spatial context that influences transcription. Network models integrate gene relationships that transcend individual sequences. RNA models provide sequence-level representations that feed into these higher-level frameworks, completing the molecular arc from DNA through RNA to protein while opening the path to systems-level integration.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhy is RNA secondary structure prediction fundamentally harder than protein structure prediction, despite RNA having only 4 bases compared to 20 amino acids?\nWhat information do codon-level foundation models capture that both DNA language models and protein language models miss?\nExplain why the 5’ UTR and 3’ UTR both matter for therapeutic mRNA design, but through different mechanisms.\nWhat is the primary data limitation preventing RNA foundation models from achieving breakthroughs comparable to protein language models like ESM-2?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nRNA structure is harder because: RNA has flat energy landscapes where multiple conformations have similar free energies (creating many-to-many sequence-structure relationships), long-range base pairing that creates global dependencies, and pseudoknots that violate the nested structure assumptions enabling efficient algorithms. Proteins typically fold to a single stable structure with a deep energy minimum.\nCodon models capture: Synonymous codon usage patterns that affect translation speed, mRNA stability, and co-translational folding. DNA language models see nucleotides but don’t know codon boundaries; protein language models see only amino acids and miss synonymous variation entirely. Codon models operate at the biologically meaningful unit of translation.\nUTR mechanisms differ: The 5’ UTR controls translation initiation through secondary structure accessibility, upstream ORFs, Kozak consensus strength, and ribosome recruitment. The 3’ UTR controls mRNA stability and localization through RBP binding sites, miRNA target sites, and AU-rich elements. Both affect expression but through distinct molecular mechanisms—initiation versus decay/localization.\nPrimary data limitation: RNA has fewer than 2,000 experimentally determined 3D structures (versus 200,000+ for proteins), and sequence databases are 3× smaller with less functional diversity. Without massive structural training data, RNA foundation models cannot learn the sequence-to-structure mappings that enabled ESM-2’s breakthrough.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nWhat we covered:\n\nRNA occupies a unique position between DNA and protein, with distinct modeling challenges stemming from flat energy landscapes, long-range base pairing, and sparse structural data\nClassical RNA structure prediction uses thermodynamic (MFE) or comparative (covariation) approaches; deep learning methods can capture patterns beyond nearest-neighbor rules and handle pseudoknots\nRNA foundation models lag behind protein counterparts primarily due to data scarcity—fewer sequences, less structural diversity, and heavily biased training sets\nCodon-level models fill a gap between DNA and protein approaches by capturing synonymous codon effects on translation and stability\nUTR sequences dominate expression control; model-guided UTR design is an active area with therapeutic applications\nThe COVID-19 vaccines demonstrated mRNA design principles at scale, though much optimization remains empirical\nmiRNA target prediction remains noisy despite decades of work, limited by context dependence and the probabilistic nature of targeting\nSplicing models extend beyond SpliceAI to tissue-specific predictions and quantitative alternative splicing\n\nKey connections:\n\nBackward: RNA foundation models apply the same self-supervised paradigm as protein LMs (Chapter 15) and DNA LMs (Chapter 14), but with different data constraints\nForward: Single-cell models (Chapter 19) use RNA expression as primary cellular readout; therapeutic mRNA design (?sec-ch28-mrna-design) applies the principles covered here\n\nOpen questions:\n\nCan structure probing data compensate for limited crystallographic training sets?\nWhat architectural innovations will enable RNA models to achieve protein-like breakthroughs?\nHow should models handle RNA’s inherent conformational heterogeneity?\n\n\n\n\n\n\n\nAgarwal, Vikram, and Jay Shendure. 2020. “Predicting mRNA Abundance Directly from Genomic Sequence Using Deep Convolutional Neural Networks.” Cell Reports 31 (7): 107663. https://doi.org/10.1016/j.celrep.2020.107663.\n\n\nChen, Jiayang, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze Yu, Licheng Zong, et al. 2022. “[RNA-FM] Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions.” arXiv. https://doi.org/10.48550/arXiv.2204.00300.\n\n\nLi, Sizhen, Saeed Moayedpour, Ruijiang Li, Michael Bailey, Saleh Riahi, Milad Miladi, Jacob Miner, et al. 2023. “CodonBERT: Large Language Models for mRNA Design and Optimization.” bioRxiv. https://doi.org/10.1101/2023.09.09.556981.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. “Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nNaghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B. Macdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow Coyote-Maestas, and Hani Goodarzi. 2024. “[cdsFM - EnCodon/DeCodon] A Suite of Foundation Models Captures the Contextual Interplay Between Codons.” bioRxiv. https://doi.org/10.1101/2024.10.10.617568.\n\n\nSample, Paul J., Ban Wang, David W. Reid, Vlad Presnyak, Iain J. McFadyen, David R. Morris, and Georg Seelig. 2019. “Human 5′ UTR Design and Variant Effect Prediction from a Massively Parallel Translation Assay.” Nature Biotechnology 37 (7): 803–9. https://doi.org/10.1038/s41587-019-0164-5.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RNA Structure and Function</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-single-cell.html",
    "href": "part_4/p4-ch19-single-cell.html",
    "title": "19  Single-Cell Models",
    "section": "",
    "text": "19.1 Single-Cell Data Landscape\nUnderstanding single-cell foundation models requires understanding the data they learn from. Single-cell technologies produce measurements fundamentally different from bulk assays: sparse, noisy, and high-dimensional, yet rich with information about cellular heterogeneity that bulk measurements obscure. The transition from bulk to single-cell resolution created new analytical challenges and new opportunities, while technical artifacts impose constraints that shape how foundation models must be designed.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-single-cell.html#sec-ch19-data",
    "href": "part_4/p4-ch19-single-cell.html#sec-ch19-data",
    "title": "19  Single-Cell Models",
    "section": "",
    "text": "19.1.1 From Bulk to Single-Cell Resolution\nTraditional transcriptomic studies measure gene expression in bulk tissue, producing a single measurement per gene that represents the average across thousands to millions of cells. This averaging is both a strength and a limitation. It provides robust, reproducible measurements that have powered decades of biological discovery. It also fundamentally limits what questions can be asked. If a gene appears moderately expressed in bulk, is it uniformly expressed across all cells, or highly expressed in a rare subpopulation while silent elsewhere? Bulk data cannot distinguish these scenarios.\n\n\n\n\n\n\nStop and Think: The Averaging Problem\n\n\n\nImagine a tumor with 95% of cells expressing gene X at level 0 and 5% of cells (drug-resistant subpopulation) expressing gene X at level 100. What would bulk RNA-seq report as the expression level of gene X? What critical clinical information would be lost? How might this affect treatment decisions?\n\n\nSingle-cell RNA sequencing (scRNA-seq) resolves this ambiguity by measuring gene expression in individual cells. The technology has evolved rapidly since its introduction in 2009 [Citation Needed]. Early methods captured hundreds of cells per experiment; current platforms routinely profile hundreds of thousands of cells, with some studies exceeding a million [Citation Needed]. Public repositories now contain tens of millions of single-cell transcriptomes spanning diverse tissues, developmental stages, disease states, and species. This scale approaches the data volumes that enabled large language models in natural language processing.\nThe analogy between cells and documents runs deeper than dataset size. In language, words combine according to grammatical rules to form sentences that convey meaning. In cells, genes combine according to regulatory programs to form expression profiles that define cellular identity. A hepatocyte expresses genes for drug metabolism, albumin synthesis, and bile production; a neuron expresses genes for synaptic transmission, ion channels, and neurotransmitter receptors. These expression programs are not random: transcription factors activate coherent sets of target genes, signaling pathways coordinate cellular responses, and developmental programs establish cell type identities through cascades of regulatory events. Just as language models learn syntax and semantics by predicting masked words (see Chapter 8), single-cell foundation models might learn regulatory logic by predicting masked genes.\n\n\n\n\n\n\n\n\nLanguage models predict masked words from context\n\n\n\n\n\n\n\nCellular LMs predict masked genes from expression context\n\n\n\n\n\n\n\n\n\nParallel structure between NLP and single-cell domains\n\n\n\n\n\n\n\nComparison of what each model type learns\n\n\n\n\n\n\nFigure 19.1: The cell-as-document analogy. (A) Language models treat sentences as sequences of word tokens, learning grammar through masked prediction. (B) Cellular language models treat cells as sequences of gene tokens, learning regulatory programs through masked gene prediction. (C) Parallel structure between NLP and single-cell domains. (D) Comparison of what each model type learns: language models capture syntax and semantics while cellular models capture co-expression and cell type programs.\n\n\n\n\n\n\n\n\n\nKey Insight: The Cell-as-Document Analogy\n\n\n\nThe mapping between natural language and cellular expression runs deeper than metaphor:\n\n\n\n\n\n\n\nNatural Language\nSingle-Cell Biology\n\n\n\n\nWord\nGene\n\n\nSentence\nCell expression profile\n\n\nGrammar rules\nRegulatory programs\n\n\nVocabulary (~50,000 words)\nGene set (~20,000 genes)\n\n\nDocument corpus\nCell atlas (millions of cells)\n\n\nMasked word prediction\nMasked gene prediction\n\n\nLearned: syntax, semantics\nLearned: co-expression, cell type programs\n\n\n\nThis correspondence explains why transformer architectures and pretraining strategies transfer successfully from NLP to single-cell biology.\n\n\n\n\n19.1.2 Technical Challenges and Data Characteristics\nSingle-cell data present distinctive challenges that shape how foundation models must be designed. Dropout is pervasive: due to inefficiencies in RNA capture and amplification, many genes that are actually expressed in a cell register as zero in the measurement. A gene with true expression may appear as zero in 50% to 90% of cells where it is actually transcribed [Citation Needed]. This zero-inflation means that absence of signal is not absence of expression.\nSparsity compounds the interpretation challenge. A typical single-cell transcriptome measures 20,000 genes, but any individual cell might have detectable expression for only 1,000 to 5,000 of them. The resulting data matrices are more than 90% zeros, requiring specialized computational approaches.\nBatch effects arise because technical variation between experiments often exceeds biological variation within them. Cells processed on different days, by different operators, or with different reagent lots may cluster by batch rather than by biological type. A model that learns batch-specific patterns rather than biological ones will fail to generalize. This challenge parallels the confounding issues examined in Chapter 12, where technical artifacts can masquerade as biological signal.\n\n\n\n\n\n\nDeep Dive: Batch Effects in Single-Cell Data\n\n\n\nFor ML readers: Batch effects are technical artifacts that can dominate biological signal:\nWhat causes batch effects:\n\nDifferent processing dates, operators, reagent lots\nDifferent sequencing platforms or depths\nSample handling and storage conditions\nAmbient RNA contamination varying by experiment\n\nWhy they matter:\nWithout correction, cells cluster by when they were processed rather than what cell type they are. A T cell from batch 1 may appear more similar to a B cell from batch 1 than to a T cell from batch 2.\nVisualization:\n\n\n\nBefore Correction\nAfter Correction\n\n\n\n\nCells cluster by batch\nCells cluster by type\n\n\nTechnical variation &gt; biological\nBiological variation dominates\n\n\nPoor generalization\nBetter cross-study transfer\n\n\n\nML approaches to batch correction:\n\nContrastive learning: Train model to bring same cell types together while pushing batches apart\nAdversarial training: Encoder learns representations from which a discriminator cannot predict batch identity\nIntegration methods: Harmony, scVI explicitly model and remove batch effects\nRank-based encoding: Reduce sensitivity to absolute expression values\n\nKey insight: A foundation model trained on batch-confounded data will learn to distinguish batches, not biology. Batch correction is not optional preprocessing—it determines whether the model captures anything useful.\n\n\nDynamic range spans orders of magnitude, from highly expressed housekeeping genes to rare transcription factors present at a few copies per cell. Normalizing across this range while preserving biologically meaningful variation requires careful preprocessing choices that can affect downstream results.\n\n\n\nTable 19.1: Summary of technical challenges in single-cell data and their implications for foundation model design.\n\n\n\n\n\n\n\n\n\n\n\nChallenge\nDescription\nImpact on Modeling\nMitigation Strategy\n\n\n\n\nDropout\n50-90% of expressed genes appear as zero\nZero does not mean “not expressed”\nRank-based encoding; imputation\n\n\nSparsity\n&gt;90% zeros in gene-cell matrix\nStandard neural networks struggle\nSparse architectures; appropriate objectives\n\n\nBatch effects\nTechnical variation exceeds biological\nModels learn artifacts instead of biology\nContrastive objectives; adversarial alignment\n\n\nDynamic range\nOrders of magnitude variation\nHighly expressed genes dominate\nLog transformation; rank normalization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDropout and sparsity: over 90% zeros in gene-cell matrices\n\n\n\n\n\n\n\nBatch effects can exceed biological variation\n\n\n\n\n\n\n\n\n\nDynamic range spans orders of magnitude\n\n\n\n\n\n\n\nFoundation model strategies for addressing challenges\n\n\n\n\n\n\nFigure 19.2: Technical challenges in single-cell data. (A) Dropout and sparsity: over 90% of values are zero, but zeros may indicate true absence or technical failure to detect. (B) Batch effects: cells often cluster by experimental batch rather than biological type. (C) Dynamic range: expression spans orders of magnitude from rare transcription factors to abundant housekeeping genes. (D) Foundation model strategies for addressing these challenges: rank-based encoding, large-scale pretraining, and contrastive objectives.\n\n\n\nDespite these challenges, the scale of available data creates opportunities. Tens of millions of cells, spanning hundreds of cell types across dozens of tissues and multiple species, provide training corpora large enough to learn general representations. The question is whether foundation model architectures can extract biological signal from noisy, sparse, high-dimensional measurements.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-single-cell.html#sec-ch19-clm",
    "href": "part_4/p4-ch19-single-cell.html#sec-ch19-clm",
    "title": "19  Single-Cell Models",
    "section": "19.2 Cellular Language Models",
    "text": "19.2 Cellular Language Models\nThe analogy between gene expression and language has proven remarkably productive. If cells are sentences and genes are words, then cellular regulatory programs are grammar: the rules governing which genes appear together in which contexts. Several foundation models now operationalize this analogy, treating single-cell transcriptomes as documents and learning to predict masked genes from expression context. These models differ in their tokenization strategies, pretraining objectives, and architectural choices, but share a common premise: that self-supervised learning on millions of cells can capture regulatory logic that transfers to diverse downstream tasks.\n\n\n\nTable 19.2: Comparison of major single-cell foundation models. All use transformer-based architectures with variations in preprocessing and training objectives.\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nTraining Scale\nTokenization\nKey Innovation\nPrimary Applications\n\n\n\n\nGeneformer\n~30M cells\nRank-based\nNetwork hierarchy emerges in attention\nCell annotation, target ID\n\n\nscGPT\n&gt;33M cells\nBinned expression\nMulti-objective (MLM + autoregressive + contrastive)\nMulti-task, perturbation\n\n\nscFoundation\n&gt;50M cells\nMultiple approaches\nSystematic tokenization comparison\nRepresentation learning\n\n\nTranscriptFormer\n&gt;112M cells\nJoint gene-expression\nCross-species (1.5B years)\nZero-shot annotation\n\n\n\n\n\n\nWhy do these innovations matter biologically? Geneformer’s rank-based encoding discards absolute counts (which vary with technical artifacts) while preserving the relative expression patterns that define cell state. scGPT’s multi-objective training forces the model to simultaneously predict masked genes (learning co-expression), predict expression from sequence (learning regulatory logic), and align cells across batches (learning biological identity distinct from technical variation). TranscriptFormer’s cross-species training exploits 1.5 billion years of evolutionary divergence: regulatory programs conserved across such distances must be fundamental, while species-specific patterns reveal how cell types diversify. Each innovation addresses a specific challenge of single-cell data: technical noise, batch effects, or the need to generalize beyond training cell types.\n\n19.2.1 Geneformer: Learning Network Biology\nGeneformer exemplifies the cellular language model approach, treating each cell as a sentence where genes serve as tokens (Theodoris et al. 2023). The model was pretrained on approximately 30 million single-cell transcriptomes to learn context-aware representations that capture how genes function within cellular regulatory networks. The key insight was that during pretraining, the model gained understanding of network dynamics in a completely self-supervised manner, encoding network hierarchy in its attention weights without explicit supervision on network structure.\nRather than using raw expression counts, Geneformer employs rank-based encoding that emphasizes relative expression. For each cell, genes are ranked by their expression level compared to their typical expression across the training corpus. This transformation highlights which genes are unusually active or silent in each cellular context. A gene ranked highly in a given cell is one whose expression deviates from its baseline, potentially indicating context-specific regulatory activation. The representation discards absolute counts, which vary with sequencing depth and capture efficiency, while preserving the relative ordering that reflects cellular state. This tokenization strategy differs fundamentally from the nucleotide-level approaches used in DNA language models (see Chapter 5).\n\n\n\n\n\n\nWorked Example: Rank-Based Encoding\n\n\n\nConsider a cell with these raw expression counts for four genes:\n\n\n\nGene\nRaw Count\nCorpus Mean\nRatio to Mean\nRank\n\n\n\n\nGAPDH (housekeeping)\n5000\n4800\n1.04\n3\n\n\nMYC (oncogene)\n200\n50\n4.00\n1\n\n\nTP53 (tumor suppressor)\n150\n100\n1.50\n2\n\n\nBRCA1\n80\n85\n0.94\n4\n\n\n\nRaw counts would emphasize GAPDH (highest count), but rank-based encoding highlights MYC (most unusually elevated relative to baseline). This captures what is biologically distinctive about this cell: MYC overexpression, a hallmark of many cancers.\nThe “sentence” for this cell becomes: [MYC, TP53, GAPDH, BRCA1, …] ordered by deviation from baseline, not by absolute abundance.\n\n\nPretraining uses a masked gene prediction objective analogous to BERT-style language modeling (see Chapter 8). A fraction of genes are masked in each cell, and the model learns to predict which genes were masked based on the remaining expression context. This forces the model to learn co-expression patterns: which genes tend to appear together at high ranks in the same cells, and which genes predict each other’s presence. The objective implicitly captures regulatory modules, signaling pathways, and cell-type-specific programs.\nAfter pretraining, Geneformer supports diverse downstream applications through fine-tuning or feature extraction. Cell type annotation achieves high accuracy even with limited labeled examples, leveraging general biological knowledge acquired during pretraining. The model identified candidate therapeutic targets for cardiomyopathy by analyzing how disease-associated genes fit within learned network structure, demonstrating potential for accelerating discovery in rare diseases where large disease-specific datasets are unavailable (Theodoris et al. 2023).\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore continuing, pause to consolidate your understanding of Geneformer’s design:\n\nWhy does rank-based encoding make models more robust to technical variation than raw counts?\nHow does masked gene prediction capture regulatory relationships?\nWhat biological knowledge emerges in attention weights without explicit supervision?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nRank-based encoding captures relative expression (which genes are unusually high/low compared to baseline) rather than absolute counts that vary with sequencing depth, capture efficiency, and other technical factors. This preserves biological signal while discarding technical noise.\nMasked gene prediction forces the model to learn co-expression patterns: which genes predict each other’s presence. If masking gene A makes gene B harder to predict, the model learns they co-occur in regulatory modules or pathways.\nAttention weights capture which genes attend to which others during prediction. These relationships often correspond to regulatory interactions (transcription factors attending to their targets) without being explicitly trained on network structure—the regulatory relationships emerge from expression patterns alone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank-based encoding highlights unusually active genes\n\n\n\n\n\n\n\nTransformer encoder architecture with masked gene prediction\n\n\n\n\n\n\n\n\n\nAttention weights correspond to regulatory relationships\n\n\n\n\n\n\n\nTransfer to cell annotation and target identification\n\n\n\n\n\n\nFigure 19.3: Geneformer innovations. (A) Rank-based encoding transforms raw counts into ranks relative to corpus baseline, emphasizing what is unusual about each cell rather than absolute abundance. (B) Transformer encoder architecture using BERT-style masked prediction produces gene and cell embeddings. (C) Attention weights learned during pretraining correspond to regulatory network relationships without explicit network supervision. (D) Transfer applications including cell type annotation, therapeutic target identification, and disease gene prioritization.\n\n\n\n\n\n19.2.2 scGPT: Generative Pretraining for Single-Cell Analysis\nscGPT extends the foundation model paradigm with a generative architecture trained on over 33 million cells (Cui et al. 2024). The model functions as a generalist backbone for single-cell analysis pipelines, supporting applications from cell type annotation to perturbation response prediction within a unified framework.\nThe architecture incorporates several innovations tailored to single-cell data characteristics. Gene tokens combine learnable embeddings with position encodings that can capture genomic location when relevant. Expression values are discretized into bins to handle the wide dynamic range and zero-inflation characteristic of single-cell measurements; rather than predicting continuous values, the model predicts which expression bin a gene falls into. Special tokens mark cell boundaries and indicate modality when multi-omic data are available.\nscGPT uses multiple pretraining objectives simultaneously. Masked gene prediction encourages learning of co-expression patterns, similar to Geneformer. Autoregressive generation predicts expression of one set of genes conditioned on others, enabling the model to generate synthetic expression profiles or impute missing values. Contrastive objectives push cells from the same type to cluster in embedding space while separating different types, providing discriminative signal that complements the generative objectives. This combination of pretraining objectives parallels the hybrid strategies explored in DNA and protein language models (see Chapter 8).\n\n\n\n\n\n\nStop and Think: Multiple Pretraining Objectives\n\n\n\nscGPT uses three different pretraining objectives simultaneously: masked gene prediction, autoregressive generation, and contrastive learning. Why might combining these objectives be beneficial? What might each objective contribute that the others miss? Consider how this relates to the multi-task pretraining discussion in Section 8.6.\n\n\nThe combination of objectives enables scGPT to excel across multiple applications. Cell type annotation benefits from rich pretrained representations, including identification of fine-grained subtypes that might be missed by simpler methods. Multi-batch integration aligns cells from different experiments while preserving genuine biological variation, addressing the pervasive batch effect problem. Perturbation response prediction anticipates how cells will respond to genetic knockouts or drug treatments, providing a foundation for in silico experimentation.\n\n\n19.2.3 scFoundation and Scaling Single-Cell Models\nscFoundation pushes the scale of single-cell foundation models further, training on over 50 million cells with an architecture designed for both representation learning and generation (Hao et al. 2024). The model explores how scaling laws observed in language models translate to cellular data, finding that larger models trained on more diverse data produce embeddings that transfer better across tasks and contexts. This scaling behavior mirrors patterns observed in DNA language models (see Chapter 13).\nThe pretraining corpus spans diverse tissues, developmental stages, and disease states, including both human and mouse data. This diversity proves essential: models trained on narrow datasets (a single tissue or condition) learn representations that capture that specific context but fail to generalize. Models trained on diverse corpora learn more abstract representations of cellular state that transfer across biological contexts.\nscFoundation emphasizes the importance of tokenization and normalization choices for downstream performance. The model systematically compared different approaches to handling zero-inflation, normalization across sequencing depth, and gene vocabulary selection. These preprocessing decisions, often treated as implementation details, significantly affect what biological signals the model can capture. The parallels to tokenization debates in DNA language models (see Chapter 5) are striking: representation choices made before training constrain what patterns can be learned.\n\n\n19.2.4 TranscriptFormer: Cross-Species Cellular Models\nTranscriptFormer extends single-cell foundation models across evolutionary time, training on over 112 million cells spanning 1.5 billion years of evolution across 12 species (Pearce et al. 2025). This cross-species approach tests whether regulatory principles learned from one organism generalize to others.\nThe model uses a novel autoregressive architecture that jointly predicts genes and their expression levels. Rather than treating gene identity and expression as separate prediction problems, TranscriptFormer generates them together, enabling it to produce synthetic cells conditioned on prompts specifying species, tissue, or cell type. Because the vocabulary spans multiple species with ortholog mappings, the model can transfer cell type annotations across evolutionary distances.\nIn zero-shot settings, TranscriptFormer demonstrates strong performance on both in-distribution and out-of-distribution cell type classification. Remarkably, models trained predominantly on mouse and human data can annotate cell types in zebrafish and other species separated by hundreds of millions of years of evolution. This cross-species transfer reveals that core principles of cellular regulation are deeply conserved, and that foundation models can capture these conserved principles when trained on evolutionarily diverse data. The success of cross-species transfer in cellular models parallels similar findings in protein language models, where evolutionary conservation provides a powerful inductive bias (see Chapter 15).\n\n\n\n\n\n\nKey Insight: Cross-Species Transfer Reveals Conserved Regulatory Programs\n\n\n\nTranscriptFormer’s ability to annotate cell types in zebrafish using models trained on mouse and human data demonstrates something profound: the core regulatory programs that define cell types are deeply conserved across 450 million years of vertebrate evolution. A neuron is recognizably a neuron whether in fish, mouse, or human because the transcription factors, signaling pathways, and gene modules that establish neuronal identity have been maintained through evolutionary time.\nThis conservation is precisely why self-supervised learning on expression data works: the “grammar” of cellular regulation is shared across species, just as the grammar of regulatory sequences is shared across the tree of life (see Section 14.4 for parallel cross-species insights in DNA models).",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-single-cell.html#sec-ch19-perturbation",
    "href": "part_4/p4-ch19-single-cell.html#sec-ch19-perturbation",
    "title": "19  Single-Cell Models",
    "section": "19.3 Perturbation Response Prediction",
    "text": "19.3 Perturbation Response Prediction\nThe ultimate test of whether cellular foundation models understand regulatory biology is prediction: can they anticipate how cells will respond to interventions they have never seen? Perturbation prediction moves beyond pattern recognition toward mechanistic understanding. If a model has learned the causal structure of gene regulatory networks, it should predict the downstream consequences of knocking out a transcription factor or activating a signaling pathway. This capability would transform drug discovery and target identification (Chapter 29), enabling in silico screening of perturbations before expensive wet-lab validation. The design-build-test-learn cycles that could exploit such predictions are examined in Section 30.6. Achieving this capability requires models to distinguish causation from correlation in observational data.\n\n19.3.1 In Silico Experiment Promise\nOne of the most compelling applications of cellular foundation models is predicting how cells will respond to perturbations. If a model has learned regulatory logic from expression data, it should be able to anticipate the transcriptional consequences of knocking out a gene, activating a pathway, or treating with a drug. Such predictions could accelerate drug discovery by prioritizing candidates before expensive wet-lab validation, identify synthetic lethal interactions for cancer therapy, and suggest targets for diseases without known interventions.\nThe perturbation prediction task requires more than memorizing co-expression patterns. The model must understand directional relationships: if gene A activates gene B, then knocking out A should reduce B’s expression. It must capture network effects: perturbations propagate through regulatory cascades, producing secondary and tertiary effects beyond direct targets. It must recognize context dependence: the same perturbation may have different effects in different cell types or states.\n\n\n\n\n\n\nStop and Think: From Correlation to Causation\n\n\n\nConsider two genes, A and B, that are always co-expressed: when A is high, B is high. From observational data alone, can you distinguish these scenarios?\n\nA activates B (A causes B’s expression)\nB activates A (B causes A’s expression)\nC activates both A and B (common cause)\nA and B share a regulatory element (co-regulation)\n\nWhat kind of data would you need to distinguish these cases? How does this limitation affect perturbation prediction models trained on observational single-cell data?\n\n\n\n\n19.3.2 Perturb-seq and Foundation Model Training\nPerturb-seq combines CRISPR-based genetic perturbations with single-cell RNA sequencing, measuring the transcriptional consequences of gene knockouts across thousands of cells (Dixit et al. 2016). These functional screens complement the deep mutational scanning approaches covered in Section 2.4.4, providing cellular rather than molecular readouts of perturbation effects. These datasets provide supervised signal for perturbation prediction: given the pre-perturbation state and the identity of the perturbed gene, predict the post-perturbation expression profile.\nFoundation models approach this task through transfer learning (see Chapter 9). A model pretrained on tens of millions of unperturbed cells learns general representations of cellular state and gene-gene relationships. Fine-tuning on Perturb-seq data teaches the model to map these representations to perturbation outcomes. The hope is that general biological knowledge from pretraining will enable accurate predictions for perturbations not seen during fine-tuning, including knockouts of genes never directly perturbed in training data.\nscGPT and Geneformer both demonstrate perturbation prediction capabilities, though performance varies across perturbation types and cellular contexts. Predictions are most accurate for well-characterized genes with many training examples and clear regulatory relationships. Performance degrades for poorly characterized genes, complex combinatorial perturbations, and cell types underrepresented in training data.\n\n\n\n\n\n\n\n\nPredict post-perturbation expression from cell state and perturbation\n\n\n\n\n\n\n\nTraining data from Perturb-seq provides supervised signal\n\n\n\n\n\n\n\n\n\nModels must learn directional relationships and cascades\n\n\n\n\n\n\n\nPerformance correlates with gene characterization level\n\n\n\n\n\n\nFigure 19.4: Perturbation response prediction. (A) The prediction task: given unperturbed cell state and perturbation identity, predict post-perturbation expression profile. (B) Training data from Perturb-seq provides supervised signal across thousands of gene knockouts. (C) What models must learn: directional regulatory relationships, network cascade effects, and cell-type-specific responses. (D) Current limitation: prediction accuracy correlates with gene characterization level—models perform best on well-studied genes where predictions are least needed.\n\n\n\n\n\n19.3.3 Limitations of Current Approaches\nDespite promising results, current perturbation prediction models face fundamental limitations. Most training data come from immortalized cell lines that may not reflect primary tissue biology. Perturbations are typically single-gene knockouts; combinatorial perturbations involving multiple genes remain challenging. The models predict average responses across perturbed cells rather than the heterogeneity of individual responses.\nMore fundamentally, correlation-based learning from expression data cannot reliably distinguish correlation from causation. Why is this limitation fundamental rather than merely technical? Given observational co-expression data alone, the statistical signature of “A causes B” is indistinguishable from “B causes A” or “C causes both A and B.” If genes X and Y always appear together in expression profiles, no amount of observational data can determine whether X activates Y, Y activates X, both are activated by an unmeasured transcription factor Z, or they share regulatory elements that respond to the same signals. This ambiguity is not a limitation of current methods but a mathematical impossibility inherent to observational data.\nPerturbation data provides the interventional signal needed to break this symmetry: when we knock out gene X and observe that gene Y decreases, we have evidence for a causal relationship that no amount of observational co-expression could provide. Training on observational data (unperturbed cells) and interventional data (perturbed cells) provides complementary signals, but even Perturb-seq data have limited coverage of the regulatory network—typically thousands of perturbations across a space of roughly 20,000 protein-coding genes, leaving most pairwise relationships unobserved. Foundation models capture patterns in data; whether those patterns reflect causal regulatory relationships remains an empirical question that requires experimental validation.\n\n\n\n\n\n\nPractical Guidance: When to Trust Perturbation Predictions\n\n\n\nCurrent perturbation prediction models are most reliable when:\n\nThe target gene is well-characterized with many Perturb-seq examples\nThe cell type is well-represented in training data\nThe perturbation is a single-gene knockout (not combinatorial)\nPredictions are used for prioritization, not as definitive answers\n\nTreat predictions with more skepticism when:\n\nThe target gene has few or no training examples\nThe cell type is underrepresented or novel\nYou’re predicting combinatorial effects\nThe prediction contradicts orthogonal evidence\n\nAlways validate high-stakes predictions experimentally. Use model predictions to prioritize which experiments to run, not to replace experiments entirely.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-single-cell.html#sec-ch19-epigenomic",
    "href": "part_4/p4-ch19-single-cell.html#sec-ch19-epigenomic",
    "title": "19  Single-Cell Models",
    "section": "19.4 Epigenomic Foundation Models",
    "text": "19.4 Epigenomic Foundation Models\nGene expression profiles capture one layer of cellular state, but the regulatory machinery determining which genes can be expressed operates through epigenomic modifications. DNA methylation silences genes by blocking transcription factor binding; chromatin accessibility determines which regulatory elements are available for activation. These epigenomic layers sit upstream of expression, establishing the potential for transcription before any RNA is produced. Foundation models that learn from epigenomic data capture this regulatory potential, complementing expression-based models with a mechanistic view of how cellular identity is encoded and maintained.\n\n19.4.1 DNA Methylation and CpGPT\nDNA methylation occupies a privileged position in the regulatory hierarchy, sitting at a junction between genotype, environment, and phenotype. Methylation patterns integrate genetic influences, since sequence context affects which CpG sites (genomic locations where cytosine is followed by guanine, the primary targets of DNA methylation in mammals) can be methylated and polymorphisms can create or destroy CpG dinucleotides. They also integrate developmental programs, since methylation landscapes are extensively remodeled during differentiation and establish cell-type-specific regulatory states. Environmental exposures including diet, smoking, and stress leave lasting methylation signatures that persist long after the exposure ends [Citation Needed].\nBeyond serving as an integrative readout, methylation encodes rich information about cellular identity and state. Epigenetic clocks built from methylation data predict chronological age with striking accuracy, and deviations from predicted age (epigenetic age acceleration) correlate with mortality risk and disease burden [Citation Needed]. Cell types can be distinguished by their methylation profiles, and disease states often manifest as characteristic methylation changes.\nCpGPT (Cytosine-phosphate-Guanine Pretrained Transformer) treats methylation as a sequence-like object amenable to transformer-based pretraining (Camillo et al. 2024). The model was pretrained on over 1,500 DNA methylation datasets encompassing more than 100,000 samples from diverse tissues and conditions. Each sample is tokenized as a sequence of CpG sites with their methylation values (beta values ranging from 0 to 1) and genomic positions. The model learns to predict masked methylation values from surrounding context, capturing both local correlations between neighboring CpG sites and global patterns that distinguish different tissues or conditions.\nAfter pretraining, CpGPT supports several capabilities with minimal additional supervision. The model can impute methylation levels at CpG sites not directly measured on a given array platform, effectively enabling conversion between different array technologies such as EPIC and 450K. For biological age prediction, fine-tuned CpGPT models match or exceed purpose-built epigenetic clocks while using a more general architecture. The learned embeddings cluster by tissue type without explicit supervision during pretraining, suggesting that the model captures biologically meaningful variation. For disease-associated methylation patterns, CpGPT can be adapted to distinguish cases from controls across multiple disease contexts through transfer learning.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore moving to chromatin accessibility, verify your understanding of DNA methylation’s role:\n\nWhat makes methylation an “integrative readout” that combines genetic, developmental, and environmental influences?\nWhy does methylation sit “upstream” of gene expression in the regulatory hierarchy?\nHow does CpGPT’s architecture differ from cellular language models like Geneformer?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nMethylation integrates: (a) genetic influences—sequence context affects which CpG sites can be methylated; (b) developmental programs—methylation landscapes are remodeled during differentiation and establish cell-type identity; (c) environmental exposures—diet, smoking, and stress leave lasting methylation signatures that persist after exposure ends.\nMethylation determines regulatory potential before transcription occurs by blocking transcription factor binding at silenced genes and marking accessible regulatory regions. Expression can only occur at genes where methylation patterns permit it.\nWhile both use transformer architectures with masked prediction, CpGPT tokenizes CpG sites with continuous methylation values (beta values 0-1) and genomic positions, whereas Geneformer tokenizes cells as rank-ordered gene sequences. CpGPT learns local correlations between neighboring CpG sites and global tissue-specific patterns; Geneformer learns gene co-expression and regulatory modules.\n\n\n\n\n\n\n\n\n19.4.2 Chromatin Accessibility Models\nChromatin accessibility, measured by assay for transposase-accessible chromatin sequencing (ATAC-seq) and related assays, provides a complementary view of regulatory state. Accessible chromatin regions mark active regulatory elements: promoters, enhancers, and insulators where transcription factors can bind. The accessibility landscape varies across cell types and conditions, reflecting the regulatory programs that define cellular identity.\nFoundation models for chromatin accessibility face the challenge of representing accessibility peaks, which are genomic intervals of variable width rather than single values at fixed positions. Different approaches tokenize this data differently: some treat peaks as binary features (accessible or not), others use continuous accessibility scores, and some operate directly on the underlying sequence to predict accessibility.\nModels that predict chromatin accessibility from DNA sequence, such as those built on Enformer-style architectures (see Chapter 16), learn how sequence motifs and their arrangements determine accessibility. These models complement single-cell accessibility measurements by providing a mechanistic link between genotype and epigenetic state. Variants that alter predicted accessibility become candidates for regulatory function even when they fall outside coding regions.\nSingle-cell ATAC-seq (scATAC-seq) provides cell-type-resolved accessibility profiles, revealing which regulatory elements are active in which cells. Foundation models for scATAC-seq face similar challenges to scRNA-seq models (sparsity, dropout, batch effects) with the additional complexity that the feature space (accessibility peaks) varies across datasets depending on peak calling procedures. Models that operate on fixed genomic coordinates can integrate across datasets more readily than those that rely on dataset-specific peak sets.\n\n\n\n\n\n\nKnowledge Check: Epigenomic Data Types\n\n\n\nBefore proceeding to cross-modality integration, ensure you can answer:\n\nWhat biological information does DNA methylation encode that expression does not?\nWhy does chromatin accessibility sit “upstream” of gene expression?\nHow does CpGPT’s tokenization strategy differ from Geneformer’s?\nWhat makes scATAC-seq integration across datasets particularly challenging?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nDNA methylation encodes developmental history, environmental exposure signatures, and epigenetic age—stable regulatory states that persist independently of current expression levels. 2. Chromatin accessibility determines which regulatory elements are available for transcription factor binding, establishing regulatory potential before any transcription occurs. 3. CpGPT tokenizes CpG sites with their methylation values and genomic positions, while Geneformer tokenizes cells as rank-ordered sequences of genes by expression level. 4. scATAC-seq peak sets vary across datasets depending on peak calling procedures, creating mismatched feature spaces that complicate integration—unlike scRNA-seq where genes provide a common coordinate system.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-single-cell.html#sec-ch19-integration",
    "href": "part_4/p4-ch19-single-cell.html#sec-ch19-integration",
    "title": "19  Single-Cell Models",
    "section": "19.5 Cross-Modality Integration",
    "text": "19.5 Cross-Modality Integration\nSingle-cell technologies have expanded beyond transcriptomics to profile chromatin accessibility, DNA methylation, protein levels, and spatial position. Each modality captures a different aspect of cellular state: expression reflects current activity, accessibility reflects regulatory potential, methylation reflects developmental history. Integrating these perspectives into unified representations requires solving a fundamental challenge: aligning cells profiled with different assays when the feature spaces share no direct correspondence. Foundation model approaches to this integration problem combine learned embeddings with biological prior knowledge, producing unified atlases that leverage all available modalities. These integration challenges anticipate the broader multi-omics approaches examined in Chapter 22, where the principles of intermediate fusion and shared latent spaces extend beyond single-cell to patient-level integration of genomics, transcriptomics, proteomics, and clinical data.\n\n19.5.1 Unpaired Integration Challenge\nSingle-cell experiments often profile different modalities in different cells. A study might include scRNA-seq data from one set of cells, scATAC-seq data from another set, and perhaps a small subset with both modalities measured simultaneously through multiome protocols. Integrating these data into a unified atlas requires aligning cells across modalities when the feature spaces are entirely different.\nThis problem is harder than standard batch correction because there is no direct correspondence between features. RNA-seq measures expression across roughly 20,000 genes. ATAC-seq measures accessibility across hundreds of thousands of peaks. A gene is not the same object as a peak. Simple approaches assign peaks to nearby genes and use gene-level summaries for alignment, but this conversion loses information about the detailed structure of accessibility within regulatory regions and introduces arbitrary choices about assignment rules.\n\n\n19.5.2 GLUE: Graph-Linked Unified Embedding\nGLUE (Graph-Linked Unified Embedding) addresses unpaired integration by combining modality-specific encoders with a graph of biological prior knowledge linking features across omics (Cao and Gao 2022). Rather than converting features between modalities, GLUE explicitly encodes regulatory relationships into a guidance graph and learns cell embeddings that are consistent with this graph.\nThe architecture has three key components. Modality-specific variational autoencoders provide encoders that map cells to a shared low-dimensional latent space and decoders that reconstruct modality-specific features. Generative distributions are tailored to each modality: negative binomial for count data, appropriate alternatives for accessibility.\nThe feature graph encodes biological prior knowledge about relationships between features across modalities. Nodes represent genes, peaks, and other genomic features. Edges connect ATAC peaks to genes they might regulate based on genomic proximity or chromatin conformation data. Edges connect genes to transcription factors that bind their promoters. This graph is provided as input rather than learned, allowing incorporation of external knowledge from databases and literature.\nA graph variational autoencoder learns feature embeddings from the guidance graph. These embeddings are used in the decoders, tying different modalities to a common regulatory backbone. Biologically related features (a gene and its putative enhancer) have similar representations, helping align the latent spaces.\n\n\n\n\n\n\n\n\nUnpaired integration: different modalities in different cells\n\n\n\n\n\n\n\nVAE encoders feed into shared latent space\n\n\n\n\n\n\n\n\n\nFeature graph encodes biological prior knowledge\n\n\n\n\n\n\n\nApplications include cross-modal prediction and regulatory inference\n\n\n\n\n\n\nFigure 19.5: GLUE (Graph-Linked Unified Embedding) for multi-omics integration. (A) The unpaired integration problem: RNA-seq and ATAC-seq measured in different cells must be aligned into a unified embedding. (B) Architecture: modality-specific VAE encoders feed into a shared latent space with adversarial alignment. (C) Feature graph encodes biological prior knowledge linking genes to regulatory peaks, constraining alignment to biologically plausible solutions. (D) Applications: cross-modal prediction, regulatory inference, and integration of three or more modalities.\n\n\n\nAdversarial alignment ensures that cell embeddings from different modalities are truly integrated. A discriminator tries to distinguish which modality produced each embedding, and encoders are trained to fool the discriminator. This forces the encoders to produce modality-invariant embeddings where cells from different assays occupy a shared manifold reflecting biological rather than technical variation.\n\n\n\n\n\n\nKey Insight: Biological Knowledge as Regularizer\n\n\n\nGLUE demonstrates a powerful principle: biological prior knowledge can regularize machine learning in ways that pure data-driven approaches cannot match. The feature graph encoding gene-peak relationships is not learned from data; it is provided based on existing biological knowledge (genomic proximity, transcription factor binding, chromatin conformation).\nThis external knowledge serves as a constraint that prevents the model from learning biologically implausible alignments. Without it, the model might align cells based on technical artifacts or spurious correlations. With it, alignment must be consistent with known regulatory relationships.\nThis principle extends beyond single-cell integration: whenever domain knowledge provides structural constraints, incorporating that knowledge explicitly can improve both performance and interpretability. See Chapter 21 for how graph-based biological knowledge enhances other foundation model applications.\n\n\n\n\n19.5.3 Applications of Cross-Modal Integration\nGLUE enables several applications beyond basic integration. Triple-omics integration combines gene expression, chromatin accessibility, and DNA methylation measured in different cells, producing unified cell type annotations that leverage all data types. Regulatory inference uses learned feature embeddings to identify candidate enhancer-gene links, providing a principled alternative to simple distance-based assignment.\nCross-modal prediction becomes possible once cells are aligned. The model can predict chromatin accessibility from expression or vice versa, enabling imputation of missing modalities. If a new dataset contains only scRNA-seq, the integrated model can predict which accessibility peaks would likely be active in each cell type based on expression patterns.\nSCGLUE extends the framework with optimizations for single-cell scale and sparsity (Cao and Gao 2022). The adversarial alignment handles batch effects common in single-cell experiments, and the graph structure incorporates tissue-specific regulatory relationships. The model scales to millions of cells while maintaining biological grounding from the guidance graph.\nThe success of graph-guided integration demonstrates that biological prior knowledge can regularize learning and improve alignment. The feature graph constrains what the model learns, ensuring consistency with known regulatory relationships while allowing discovery of new patterns. This combination of learned representations with structured biological knowledge provides a template for integrating foundation model embeddings with domain expertise (see Chapter 21 for further discussion of graph-based approaches).",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-single-cell.html#sec-ch19-limitations",
    "href": "part_4/p4-ch19-single-cell.html#sec-ch19-limitations",
    "title": "19  Single-Cell Models",
    "section": "19.6 Practical Challenges and Limitations",
    "text": "19.6 Practical Challenges and Limitations\nThe promise of single-cell foundation models comes with significant caveats. Evaluation remains difficult when ground truth is uncertain, training corpora reflect biases in what tissues and populations have been studied, and the distinction between learning biology and memorizing artifacts is not always clear. These challenges do not invalidate the approach, but they constrain what claims can be made and what applications are appropriate. Understanding these limitations is essential for responsible deployment of single-cell foundation models in research and clinical settings.\n\n19.6.1 Batch Effects and Technical Artifacts\nBatch effects remain the dominant challenge in single-cell analysis. Technical variation between experiments, protocols, and platforms can exceed biological variation, causing cells to cluster by batch rather than by type. Foundation models pretrained on diverse data may be more robust to batch effects than models trained on narrow datasets, but robustness is not guaranteed.\nThe problem is particularly acute when applying pretrained models to new data from platforms or protocols not represented in pretraining. A model trained predominantly on 10x Genomics data may perform poorly on Smart-seq2 data, not because of biological differences but because of systematic technical differences in capture efficiency, amplification bias, and gene detection. Evaluation must carefully distinguish genuine biological generalization from memorization of technical signatures. These evaluation challenges parallel the broader methodological concerns discussed in Chapter 12, while specific strategies for detecting and mitigating batch-driven confounding appear in Section 22.7.1.\n\n\n19.6.2 Cell Type Imbalance\nTraining corpora overrepresent common cell types while rare populations are poorly captured. Immune cells, particularly from blood, dominate many datasets. Rare cell types that may be disease-relevant, such as specific neuronal subtypes or tissue-resident stem cells, appear infrequently. Models may excel at distinguishing well-represented types while struggling with rare or novel populations.\nThis imbalance has equity implications when certain tissues or conditions are systematically undersampled. Neurological and psychiatric diseases involve cell types less represented in current atlases than blood or epithelial cells. Diseases affecting underrepresented populations may be modeled less accurately if training data come predominantly from European ancestry cohorts. These equity concerns mirror the population stratification issues examined in Chapter 12.\n\n\n19.6.3 Evaluation Complexity\nEvaluating single-cell foundation models is complicated by uncertain ground truth. Cell type labels in training data reflect current annotations that may be incomplete or inconsistent. Different studies use different annotation schemes, different levels of granularity, and different evidence standards. Performance metrics conflate model quality with annotation quality.\nPerturbation predictions face similar challenges. The “correct” transcriptional response to a perturbation depends on cell type, context, and measurement technology. Even well-characterized perturbations produce variable responses across replicates. Evaluation protocols must acknowledge these uncertainties rather than treating benchmarks as definitive ground truth. The broader principles of rigorous evaluation methodology from Chapter 12 apply here, while benchmark construction considerations specific to cellular models are addressed in Section 11.5. Single-cell data introduce domain-specific complications that require careful attention to leakage and distribution shift.\n\n\n\n\n\n\nStop and Think: The Ground Truth Problem\n\n\n\nImagine evaluating a cell type annotation model. Your benchmark labels cells as “CD4+ T cells” or “CD8+ T cells” based on marker gene expression. But your model predicts a third category: “transitional T cells” with intermediate expression.\nIs the model wrong? Or has it discovered real biological heterogeneity that the benchmark annotation missed? How would you distinguish these scenarios? What does this imply about the interpretation of benchmark performance in single-cell models?\n\n\n\n\n19.6.4 Causality and Mechanism\nThe most fundamental limitation is that correlation-based learning cannot establish causation. Foundation models learn patterns of co-occurrence: which genes appear together, which accessibility peaks associate with which expression changes. These patterns may reflect regulatory relationships, but they may also reflect confounding factors, indirect associations, or artifacts of data processing.\nThe perturbation prediction task illustrates this limitation. A model that accurately predicts perturbation outcomes for well-characterized genes may be learning genuine regulatory logic, or it may be exploiting superficial correlations that happen to work for genes with abundant training data. Distinguishing these possibilities requires experimental validation and careful analysis of model behavior on held-out perturbations.\n\n\n\n\n\n\nPractical Guidance: Responsible Use of Single-Cell Foundation Models\n\n\n\nDo:\n\nUse models for hypothesis generation and prioritization\nValidate key findings experimentally\nReport confidence/uncertainty alongside predictions\nAcknowledge known limitations (training data bias, batch effects)\nCompare to appropriate baselines (not just trivial ones)\n\nDon’t:\n\nTreat model predictions as ground truth without validation\nAssume high benchmark performance means the model “understands” biology\nApply models trained on one platform to very different platforms without evaluation\nIgnore batch structure when interpreting results\nClaim causal relationships from observational data alone\n\nQuestions to ask before deploying:\n\nIs my application similar to the model’s training distribution?\nWhat would go wrong if predictions are incorrect?\nCan I validate predictions before acting on them?\nHave I checked for batch/platform confounding?",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch19-single-cell.html#sec-ch19-conclusion",
    "href": "part_4/p4-ch19-single-cell.html#sec-ch19-conclusion",
    "title": "19  Single-Cell Models",
    "section": "19.7 From Sequence to State",
    "text": "19.7 From Sequence to State\nSingle-cell and epigenomic foundation models learn what states cells occupy, complementing the sequence-based models that learn what sequences encode. DNA and protein language models capture the information content of genomic and protein sequence (see Chapter 14, Chapter 15); cellular models capture the configurations that cells assume in development, homeostasis, and disease. These perspectives address different biological questions: sequence determines the possible states a cell can achieve, while cellular state reflects which possibilities are realized in a given context. A complete understanding of gene regulation requires both.\nThe representations learned by cellular foundation models enable integration across scales and modalities. Cell embeddings serve as node features in graph-based reasoning systems (Chapter 21), connecting expression profiles to protein interaction networks and regulatory pathways. Three-dimensional genome organization (Chapter 20) provides spatial context that constrains which regulatory relationships can operate. Multi-omics integration (Chapter 22) extends beyond expression to proteomics, epigenomics, and clinical measurements. In each case, foundation model embeddings provide the representational substrate that downstream methods refine.\nThe ultimate goal extends beyond prediction to explanation: models that identify the regulatory mechanisms underlying cellular state, the variants that perturb those mechanisms, and the interventions that might restore normal function. Current foundation models capture patterns in cellular data with high fidelity, enabling accurate cell type classification, perturbation response prediction, and cross-dataset integration. Whether those patterns reflect the causal structure of biological regulation, or merely correlations useful for prediction, remains open. Resolving this question requires continued integration of computational modeling with experimental validation, connecting the patterns that models learn to the mechanisms that biology employs.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat is the key advantage of rank-based encoding (as used in Geneformer) compared to count-based encoding for single-cell foundation models?\nExplain why batch effects are particularly problematic for single-cell models and how they can exceed biological variation in magnitude.\nWhat is the fundamental limitation that prevents perturbation prediction models from establishing causation, even when they achieve high prediction accuracy?\nHow does cross-modality integration (like GLUE) use biological prior knowledge to align cells profiled with different assays?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nRank-based encoding: Rank-based encoding emphasizes relative expression (which genes are unusually high or low compared to their typical baseline) rather than absolute counts that vary with technical factors like sequencing depth, capture efficiency, and amplification bias. This preserves biological signal (what is distinctive about this cell’s state) while discarding technical noise that would confound count-based representations.\nBatch effects: Batch effects arise when technical variation between experiments (different processing dates, operators, reagent lots, sequencing platforms) exceeds biological variation within them. Without correction, cells cluster by when they were processed rather than what cell type they are—a T cell from batch 1 may appear more similar to a B cell from batch 1 than to a T cell from batch 2. This means models can learn to distinguish batches instead of biology, leading to poor generalization across studies.\nCausation limitation: From observational co-expression data alone, the statistical signature of “A causes B” is indistinguishable from “B causes A” or “C causes both A and B.” No amount of observational data can determine directionality or distinguish direct causation from confounding. Even with Perturb-seq data, coverage is limited (thousands of perturbations across ~20,000 genes), leaving most regulatory relationships unobserved and requiring models to generalize beyond what they have seen experimentally validated.\nCross-modality integration: GLUE uses a feature graph encoding biological prior knowledge (gene-peak relationships based on genomic proximity, transcription factor binding, chromatin conformation) to link features across modalities. This graph serves as a regulatory backbone that constrains alignment—modality-specific encoders must produce cell embeddings consistent with known regulatory relationships, preventing biologically implausible alignments based on technical artifacts while allowing discovery of new patterns within biologically plausible space.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nCore Concepts:\n\nSingle-cell resolution reveals cellular heterogeneity that bulk measurements average away. A tumor’s drug-resistant subpopulation or a tissue’s rare stem cells become visible only at single-cell resolution.\nCellular language models treat genes as tokens and cells as documents. Geneformer, scGPT, scFoundation, and TranscriptFormer apply transformer architectures to learn regulatory “grammar” from millions of cells.\nRank-based encoding (as in Geneformer) emphasizes relative expression rather than absolute counts, making representations robust to technical variation in sequencing depth.\nPerturbation prediction tests whether models understand regulatory biology well enough to predict intervention outcomes. Current models show promise for well-characterized genes but face fundamental limitations distinguishing correlation from causation.\nCross-modality integration (GLUE) aligns cells profiled with different assays by using biological prior knowledge (gene-peak relationships) as a regulatory backbone.\n\nKey Limitations:\n\nBatch effects can exceed biological variation; models may learn artifacts\nTraining data overrepresent common cell types from studied populations\nGround truth for evaluation is uncertain and inconsistent\nCorrelation-based learning cannot establish causation\n\nConnections:\n\nForward: Cell embeddings feed into graph-based reasoning (Chapter 21) and multi-omics integration (Chapter 22)\nForward: Perturbation prediction enables drug discovery workflows (Chapter 29)\nBackward: Single-cell models apply pretraining strategies from Chapter 8 and transfer learning from Chapter 9\nBackward: Technical confounding mirrors batch effect challenges in Chapter 12\n\n\n\n\n\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T. Higgins-Chen, Steve Horvath, and Bo Wang. 2024. “CpGPT: A Foundation Model for DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. 2024. “scGPT: Toward Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI.” Nature Methods 21 (8): 1470–80. https://doi.org/10.1038/s41592-024-02201-0.\n\n\nDixit, Atray, Oren Parnas, Biyu Li, Jenny Chen, Charles P. Fulco, Livnat Jerby-Arnon, Nemanja D. Marjanovic, et al. 2016. “Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens.” Cell 167 (7): 1853–1866.e17. https://doi.org/10.1016/j.cell.2016.11.038.\n\n\nHao, Minsheng, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Xuegong Zhang, and Le Song. 2024. “Large-Scale Foundation Model on Single-Cell Transcriptomics.” Nature Methods 21 (8): 1481–91. https://doi.org/10.1038/s41592-024-02305-7.\n\n\nPearce, James D., Sara E. Simmonds, Gita Mahmoudabadi, Lakshmi Krishnan, Giovanni Palla, Ana-Maria Istrate, Alexander Tarashansky, et al. 2025. “[TranscriptFormer] Cross-Species Generative Cell Atlas Across 1.5 Billion Years of Evolution: The TranscriptFormer Single-Cell Model.” bioRxiv. https://doi.org/10.1101/2025.04.25.650731.\n\n\nTheodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023. “[Geneformer] Transfer Learning Enables Predictions in Network Biology.” Nature 618 (7965): 616–24. https://doi.org/10.1038/s41586-023-06139-9.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single-Cell Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch20-3d-genome.html",
    "href": "part_4/p4-ch20-3d-genome.html",
    "title": "20  3D Genome Organization",
    "section": "",
    "text": "20.1 Chromatin Organization Hierarchy\nThe genome folds through multiple organizational levels, each with distinct functional consequences and arising from different molecular mechanisms. Understanding this hierarchy is essential for interpreting both normal gene regulation and how structural variants cause disease. The levels are not independent; they interact in complex ways that computational models must capture to predict 3D structure accurately.\nWhy did cells evolve multiple organizational levels rather than a single mechanism? Each level solves a different regulatory problem at a different scale. Compartments segregate active from inactive chromatin, creating nuclear microenvironments with distinct biochemical properties—essentially partitioning the nucleus into “transcription factories” and “silencing zones.” TADs constrain enhancer-promoter search: without boundaries, an enhancer might contact any gene within megabases, creating regulatory chaos; TAD boundaries limit this search to a few hundred kilobases. Loops bring specific regulatory elements into direct contact when needed. This hierarchical division of labor allows cells to achieve both genome-scale organization (compartments) and fine-scale precision (loops) using different mechanisms optimized for different tasks.\nThe following table summarizes the key organizational levels, their scales, and the molecular mechanisms underlying each:",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch20-3d-genome.html#sec-ch20-chromatin-hierarchy",
    "href": "part_4/p4-ch20-3d-genome.html#sec-ch20-chromatin-hierarchy",
    "title": "20  3D Genome Organization",
    "section": "",
    "text": "Predict Before You Look\n\n\n\nBefore examining the table below, try to rank the four organizational levels (chromosome territories, compartments, TADs, and loops) from most to least computationally tractable to predict from DNA sequence. What sequence features might make some levels easier to predict than others?\n\n\n\n\n\n\nTable 20.1: Summary of chromatin organization hierarchy showing spatial scales, key features, and prediction difficulty.\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nScale\nKey Features\nMolecular Mechanism\nComputational Tractability\n\n\n\n\nChromosome territories\nNucleus-wide\nGene-rich toward interior\nNuclear organization\nLow (difficult to predict)\n\n\nA/B compartments\n1–10 Mb\nCheckerboard pattern in Hi-C\nPhase separation\nModerate (chromatin state)\n\n\nTADs\n200 kb–2 Mb\nTriangular domains, conserved boundaries\nLoop extrusion\nHigh (CTCF motifs)\n\n\nChromatin loops\n10–500 kb\nFocal Hi-C enrichments\nCTCF/cohesin\nHigh (convergent motifs)\n\n\n\n\n\n\n\n20.1.1 Chromosome Territories and Compartments\nAt the largest scale, chromosomes occupy distinct nuclear volumes called chromosome territories. Gene-rich chromosomes tend toward the nuclear interior while gene-poor chromosomes associate with the nuclear periphery. This territorial organization limits which chromosomes can exchange material during translocations: recurrent cancer-associated translocations occur preferentially between chromosomes that occupy neighboring territories [Citation Needed]. While chromosome territory organization has clear functional implications, most computational models focus on finer-scale structures where sequence determinants are more tractable.\nWithin chromosome territories, chromatin partitions into two major compartment types distinguished by their transcriptional activity and chromatin state. A compartments contain gene-rich, transcriptionally active chromatin with open, accessible structure. B compartments contain gene-poor, transcriptionally silent regions often associated with the nuclear lamina at the nuclear periphery. This compartmentalization is visible in Hi-C contact maps as a characteristic checkerboard pattern: A compartment regions preferentially contact other A regions even when separated by megabases, while B regions contact other B regions [Citation Needed]. Compartment identity correlates strongly with histone modifications (H3K27ac marks active A compartments; H3K9me3 marks repressive B compartments) and changes during cellular differentiation as lineage-specific genes shift between active and inactive states. The molecular mechanism underlying compartmentalization appears to involve phase separation: regions with similar chromatin states aggregate through weak multivalent interactions, creating nuclear microenvironments with distinct biochemical properties [Citation Needed].\n\n\n20.1.2 Topologically Associating Domains\nBelow the megabase scale of compartments, the genome organizes into topologically associating domains (TADs): sub-megabase regions (median approximately 800 kilobases in mammals) within which sequences contact each other more frequently than with sequences outside the domain. TAD boundaries appear as sharp transitions in contact frequency, visible in Hi-C maps as triangular domains along the matrix diagonal. These boundaries show strong conservation across mammalian species and across cell types within a species, suggesting strong selective pressure to maintain domain organization [Citation Needed]. The prevailing model holds that TADs constrain enhancer-promoter interactions: regulatory elements within a TAD can contact genes in the same domain, but boundaries prevent crosstalk with genes in adjacent domains. This insulation function has clear clinical relevance. Deletions that remove TAD boundaries allow enhancers to contact genes they normally cannot reach. In a well-characterized example, deletions removing the boundary between the EPHA4 locus and the WNT6/PAX3 region allow limb enhancers to ectopically activate WNT6, causing brachydactyly and other limb malformations (Lupiáñez et al. 2015).\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider an enhancer located exactly at the center of a TAD. If this TAD contains three genes—one near each boundary and one near the center—which genes would you expect the enhancer to regulate? How would your prediction change if the enhancer were located near one of the TAD boundaries instead?\n\n\n\n\n20.1.3 Loop Extrusion Mechanism\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore reading: CTCF proteins bind to DNA in a specific orientation. What do you predict happens when two CTCF sites face toward each other (convergent) versus away from each other (divergent)? Which orientation would you expect to form stable chromatin loops?\n\n\nThe molecular basis of TAD formation is now well understood through the loop extrusion model. Think of cohesin as a ring sliding along a rope: once threaded, it moves along the rope, gathering slack into a growing loop. The ring slides freely until it hits a knot tied in a specific direction—only knots facing toward the ring block its progress, while knots facing away let it pass. The cohesin protein complex loads onto chromatin and extrudes DNA bidirectionally, progressively enlarging the extruded loop until it encounters an obstacle. The key obstacle is CTCF protein bound to DNA in a specific orientation. When cohesin encounters CTCF sites oriented toward each other (convergent orientation), extrusion halts and a stable loop forms with the convergent CTCF sites at the loop anchors [Citation Needed]. This model explains several key observations: TAD boundaries are enriched for CTCF binding sites; CTCF motif orientation predicts which sites will anchor loops (convergent pairs form loops while divergent pairs do not); and acute degradation of cohesin eliminates TADs within hours while leaving compartments intact [Citation Needed]. The distinction between compartment and TAD formation mechanisms has important implications for prediction. Models that capture CTCF binding and orientation can predict TAD boundaries; predicting compartments requires learning different sequence features associated with chromatin state.\n\n\n\n\n\n\nKey Insight\n\n\n\nThe orientation rule for CTCF is a powerful predictor: convergent CTCF pairs (pointing toward each other: \\(\\rightarrow \\leftarrow\\)) form stable loop anchors, while divergent pairs (\\(\\leftarrow \\rightarrow\\)) and tandem pairs (\\(\\rightarrow \\rightarrow\\)) do not. This simple rule, which arises from the loop extrusion mechanism, allows computational models to predict many TAD boundaries directly from sequence.\n\n\n\n\n20.1.4 Fine-Scale Chromatin Loops\nAt the finest scale, chromatin forms specific loops between defined loci. Enhancer-promoter loops bring distal regulatory elements into physical proximity with their target genes, while structural loops between convergent CTCF sites establish the TAD framework. Most enhancer-promoter contacts span less than 200 kilobases, but some extend over a megabase [Citation Needed]. Detecting these fine-scale contacts requires high-resolution data; the Micro-C method uses micrococcal nuclease digestion to achieve nucleosome-level resolution, revealing contact patterns invisible in standard Hi-C [Citation Needed]. The functional significance of individual loops remains debated. Some loops appear essential for gene activation; others may be structural features without direct regulatory consequences.\n\n\n\n\n\n\n\n\nChromosome territories: gene-rich interior, gene-poor periphery\n\n\n\n\n\n\n\nA/B compartments form checkerboard in Hi-C\n\n\n\n\n\n\n\n\n\nTADs appear as triangular domains\n\n\n\n\n\n\n\nChromatin loops connect enhancers to promoters\n\n\n\n\n\n\nFigure 20.2: Hierarchical organization of the 3D genome. (A) Chromosome territories: each chromosome occupies a distinct nuclear volume with gene-rich chromosomes toward the interior. (B) A/B compartments: active (A) and inactive (B) chromatin form a checkerboard pattern visible at megabase scale. (C) TADs: topologically associating domains appear as triangular enrichments in Hi-C with a median size of ~800 kb. (D) Chromatin loops: focal Hi-C enrichments represent enhancer-promoter contacts and CTCF-anchored structural loops.\n\n\n\n\n\n\n\n\n\n\n\nCohesin loads onto chromatin as a ring\n\n\n\n\n\n\n\nBidirectional extrusion enlarges the loop\n\n\n\n\n\n\n\n\n\nConvergent CTCF sites halt extrusion\n\n\n\n\n\n\n\nOnly convergent CTCF pairs form stable loops\n\n\n\n\n\n\nFigure 20.3: The loop extrusion mechanism. (A) Cohesin loads onto chromatin as a ring complex. (B) Bidirectional extrusion progressively enlarges the DNA loop. (C) Extrusion halts when cohesin encounters convergent CTCF sites, forming stable loop anchors. (D) The orientation rule: only convergent CTCF pairs (→←) form stable loops; divergent (←→) and tandem (→→) orientations do not halt extrusion.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch20-3d-genome.html#sec-ch20-3d-measurement",
    "href": "part_4/p4-ch20-3d-genome.html#sec-ch20-3d-measurement",
    "title": "20  3D Genome Organization",
    "section": "20.2 Measuring the 3D Genome",
    "text": "20.2 Measuring the 3D Genome\nPredicting 3D genome structure requires training data: measurements of which sequences contact which other sequences in real cells. Chromosome conformation capture methods provide these measurements through a common biochemical principle, though the technologies vary in resolution, throughput, and the aspects of 3D organization they reveal.\n\n20.2.1 Hi-C and Contact Matrices\n\n\n\n\n\n\nTechnical Detail\n\n\n\nThis section describes the biochemistry and normalization of Hi-C data. Understanding these technical details helps explain why training data resolution varies and why certain biases must be corrected. Readers primarily interested in computational prediction can focus on the key point: Hi-C produces a symmetric contact matrix where values represent how frequently two genomic regions were spatially close in the cell population.\n\n\nCells are crosslinked with formaldehyde to freeze chromatin contacts in place; DNA is digested with restriction enzymes; free DNA ends are ligated, preferentially joining fragments that were spatially proximate; and the ligated junctions are identified through sequencing. The frequency of junction reads between two genomic regions reflects how often those regions were in physical contact across the cell population.\nHi-C extends this principle genome-wide by incorporating biotinylated nucleotides at ligation junctions, enabling purification of chimeric fragments from the entire genome [Citation Needed]. The output is a contact matrix—similar to a table showing how often people from different neighborhoods meet each other in a city. Just as residents of the same neighborhood run into each other frequently while encounters between distant neighborhoods are rare, genomic regions close together contact often while distant regions rarely meet. Rows and columns represent genomic bins (typically 1 to 50 kilobases depending on sequencing depth) and values represent contact frequencies between bin pairs. Resolution depends directly on sequencing depth: achieving 1 kilobase resolution requires billions of reads, while 10 kilobase resolution requires hundreds of millions. Raw contact frequencies require extensive normalization to correct for biases from GC content, restriction site density, and mappability. The ICE (iterative correction and eigenvector decomposition) method and related approaches remove these technical artifacts while preserving biological signal [Citation Needed]. ICE works by assuming that all genomic regions should have roughly equal total contact frequency (visibility), then iteratively adjusting row and column sums to achieve this balance. The assumption that visibility differences are technical rather than biological is imperfect but works well in practice because the dominant technical biases (GC content affecting PCR amplification, restriction site density affecting fragmentation) would otherwise dwarf genuine biological variation. The training strategies that enable models to learn from these normalized contact matrices follow the multi-task principles introduced in Section 8.6.\nThe contact matrix encodes all levels of chromatin organization. Compartments appear as the checkerboard pattern when viewing megabase-scale interactions; TADs appear as triangular domains of enriched contacts along the diagonal; and loops appear as focal enrichments at specific off-diagonal positions. The matrix is dominated by the polymer effect: sequences that are close in linear distance contact each other frequently regardless of specific 3D structure, creating strong signal along the diagonal that can obscure biologically meaningful contacts at greater distances. Why does genomic distance dominate contact frequency? Chromatin behaves as a polymer: random thermal fluctuations bring nearby sequences into contact simply because they are tethered to the same chain. Two loci 10 kb apart will contact each other frequently by chance, while loci 10 Mb apart rarely meet through random motion. This distance decay follows a power law (contact frequency ~ distance-1), and deviations from this baseline reveal the biologically interesting contacts—TAD boundaries where contact drops faster than expected, and loops where contact is enriched above the polymer baseline.\n\n\n20.2.2 Resolution and Data Resources\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nRecall from Section 2.4 how different sequencing technologies involve tradeoffs between throughput, resolution, and cost. Before examining the table below, predict: which 3D genome technology would you expect to have the highest resolution but lowest throughput? Which would have the opposite characteristics?\n\n\nBeyond standard Hi-C, several technologies address specific limitations:\n\n\n\nTable 20.2: Comparison of 3D genome measurement technologies. The choice of technology involves tradeoffs between resolution, cost, and genome coverage.\n\n\n\n\n\n\n\n\n\n\n\n\nTechnology\nResolution\nThroughput\nKey Advantage\nKey Limitation\n\n\n\n\nHi-C\n1–50 kb\nHigh\nGenome-wide, well-established\nLimited by restriction sites\n\n\nMicro-C\n~100 bp\nModerate\nNucleosome-level resolution\nHigher cost, fewer datasets\n\n\nSingle-cell Hi-C\nVariable\nLow\nCell-to-cell variation\nExtremely sparse matrices\n\n\nDNA FISH\nSingle locus\nLow\nDirect visualization\nLow throughput\n\n\nCapture Hi-C\n1–5 kb\nModerate\nHigh resolution at targets\nLimited to predetermined loci\n\n\n\n\n\n\nMicro-C achieves nucleosome-level resolution by using micrococcal nuclease instead of restriction enzymes, revealing fine-scale contact patterns invisible at standard Hi-C resolution [Citation Needed]. Single-cell Hi-C measures contacts in individual cells, revealing that any two loci contact each other in only 5 to 15 percent of cells, but the resulting matrices are extremely sparse (most possible contacts are unmeasured in any single cell) [Citation Needed]. Why is single-cell Hi-C so sparse? The math is humbling: each chromosome makes at most one contact per restriction fragment per cell (a fragment cannot be ligated to two partners simultaneously). With millions of possible genomic bin pairs but only thousands of ligation events captured per cell, the sampling is fundamentally incomplete. The sparsity is not a technical limitation that better sequencing will solve—it reflects the physical constraint that each genomic region can only contact one partner at a time in any given cell. Imaging methods such as DNA FISH directly visualize genomic loci in the nucleus, providing ground truth for computational predictions but at much lower throughput than sequencing-based approaches.\nTraining data for 3D prediction models comes primarily from a small number of well-characterized cell lines. The lymphoblastoid cell line GM12878 and the leukemia cell line K562 have deep Hi-C coverage across multiple laboratories, making them the default training sets for most models. Primary tissues and rare cell types have sparse coverage, creating a significant gap between where models are trained and where clinical applications require predictions. The 4D Nucleome Data Portal and ENCODE provide the most comprehensive repositories of 3D genome data, though coverage remains heavily biased toward common cell lines and human samples. This data landscape parallels the challenges discussed for functional genomics data more broadly (Chapter 2).\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA researcher wants to study enhancer-promoter contacts at a specific gene locus implicated in a rare disease. The target region spans 500 kb. Given the technologies described above, which approach would you recommend, and why? What would be the key limitation of your recommended approach?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nCapture Hi-C would be the best choice—it provides high resolution (1-5 kb) at the specific target locus, enabling identification of enhancer-promoter contacts within the 500 kb region without the cost of genome-wide sequencing. The key limitation is that it only captures contacts within the predetermined region, potentially missing long-range interactions with sequences outside the targeted 500 kb window.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch20-3d-genome.html#sec-ch20-3d-prediction",
    "href": "part_4/p4-ch20-3d-genome.html#sec-ch20-3d-prediction",
    "title": "20  3D Genome Organization",
    "section": "20.3 Predicting 3D Structure from Sequence",
    "text": "20.3 Predicting 3D Structure from Sequence\nSequence-based prediction of 3D genome structure asks whether DNA sequence alone contains sufficient information to predict chromatin contacts. The success of models like Akita, Orca, and C.Origami demonstrates that sequence encodes substantial 3D information, particularly for TAD boundaries and CTCF-anchored loops. These models share a common challenge: predicting a two-dimensional contact matrix from a one-dimensional sequence input.\n\n20.3.1 Akita and Dilated Convolutions\n\n\n\n\n\n\nRetrieval Practice\n\n\n\nRecall from Chapter 6 how dilated convolutions expand receptive fields. Why is this architectural choice particularly important for 3D genome prediction, where CTCF binding sites may be separated by hundreds of kilobases? What would happen if we used standard convolutions instead?\n\n\nAkita, introduced by Fudenberg et al. in 2020, established the paradigm for sequence-to-contact prediction (Fudenberg, Kelley, and Pollard 2020). The model takes approximately one megabase of DNA sequence as input and predicts Hi-C contact frequencies at 2 kilobase resolution. The architecture uses dilated convolutions to expand the receptive field without proportionally increasing parameters (an approach discussed in detail in Section 6.5.1), enabling the model to integrate information across the full input window. Dilated convolutions are essential here because TAD boundaries and loop anchors depend on sequence features (primarily CTCF motifs) that may be separated by hundreds of kilobases, far beyond what standard convolutions could capture without prohibitive parameter counts.\nThe output is symmetric (contacts between positions i and j equal contacts between j and i), which the architecture enforces through appropriate pooling operations. This symmetry constraint is not merely a convenience but reflects the physical reality of chromatin contacts: if region A touches region B, then by definition B touches A. Enforcing this constraint in the architecture prevents the model from learning spurious asymmetric predictions that would violate physics. Akita achieves correlation coefficients of 0.6 to 0.8 between predicted and observed contact maps in held-out genomic regions, successfully identifying TAD boundaries and major loop anchors.\n\n\n20.3.2 Orca and Multiscale Prediction\nOrca extends sequence-based prediction to multiple resolutions simultaneously (Zhou 2022). Rather than predicting a single-resolution contact map, Orca generates predictions at 4, 8, 16, 32, 64, 128, and 256 kilobase resolutions, capturing both fine-scale loops and large-scale compartment structure. The multiscale approach addresses a fundamental challenge: compartments span megabases while loops span kilobases, and no single resolution optimally captures both. A single-resolution model forced to compromise would either miss fine-scale loop contacts (at coarse resolution) or fail to capture compartment patterns that emerge from aggregating many weak interactions (at fine resolution).\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nThink back to Enformer from Chapter 16, which uses dilated convolutions to expand its receptive field. How might Orca’s multiscale approach differ from simply using more extreme dilation rates? What architectural advantage does explicit multi-resolution prediction provide?\n\n\nOrca’s architecture processes sequence through parallel pathways tuned to different scales, then combines predictions into a coherent multiscale representation. The parallel pathways use different pooling and dilation strategies, effectively asking “what does this sequence predict at the 4kb scale?” independently from “what does it predict at the 256kb scale?” This design enables prediction of structural variants’ effects across organizational levels, from disrupted loops to altered compartment boundaries.\n\n\n20.3.3 C.Origami and Cross-Cell-Type Transfer\nC.Origami addresses the cell-type specificity problem (Tan et al. 2023). While TAD boundaries are largely conserved across cell types, finer-scale contacts vary substantially. C.Origami incorporates CTCF ChIP-seq data alongside sequence, enabling the model to learn how cell-type-specific CTCF binding patterns shape cell-type-specific contact maps. This design enables transfer learning: train on cell types with both Hi-C and CTCF data, then predict contacts in new cell types using only CTCF ChIP-seq. The approach substantially expands the range of cell types where 3D predictions are possible, since CTCF ChIP-seq is available for many more cell types than deep Hi-C. This transfer strategy echoes the broader transfer learning principles discussed in Chapter 9.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore examining the comparison table, consider the tradeoffs each model makes. Which model would you expect to have the best cross-cell-type performance? Which would be most suitable for predicting the effects of a novel structural variant in a cell type with no available Hi-C data?\n\n\nThe following table compares these three foundational 3D prediction models:\n\n\n\nTable 20.3: Comparison of sequence-based 3D structure prediction models. Each addresses different aspects of the prediction challenge.\n\n\n\n\n\n\n\n\n\n\n\nFeature\nAkita\nOrca\nC.Origami\n\n\n\n\nInput\nSequence only\nSequence only\nSequence + CTCF ChIP-seq\n\n\nContext length\n~1 Mb\n~1 Mb\n~1 Mb\n\n\nOutput resolution\n2 kb\n4–256 kb (multiscale)\nVariable\n\n\nArchitecture\nDilated CNN\nMultiscale CNN\nCNN with auxiliary input\n\n\nCell-type transfer\nLimited\nLimited\nEnabled via CTCF data\n\n\nKey strength\nEstablished paradigm\nMultiscale prediction\nCross-cell-type transfer\n\n\nKey limitation\nSingle cell type\nRequires Hi-C training\nRequires CTCF ChIP-seq\n\n\n\n\n\n\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider the tradeoff between Akita’s sequence-only approach and C.Origami’s requirement for CTCF ChIP-seq data. In what scenarios would each approach be preferred? Think about (1) predicting structural variant effects, (2) predicting contacts in a novel cell type, and (3) understanding what sequence features determine 3D structure.\n\n\n\n\n20.3.4 Learned Sequence Determinants\n\n\n\n\n\n\nRetrieval Practice\n\n\n\nBefore reading about what these 3D prediction models learn, recall the loop extrusion mechanism from earlier in this chapter. What sequence feature would you expect to be the strongest predictor of chromatin loop formation? Why would the orientation of this feature matter?\n\n\nInterpretability analysis reveals what these models learn about sequence determinants of 3D structure. Attribution methods (discussed more fully in Chapter 24) consistently identify CTCF motifs as the strongest predictors of contact patterns, with convergent CTCF pairs (motifs oriented toward each other) most strongly associated with loop anchors. Transcription start sites contribute to boundary predictions, consistent with the observation that active promoters often coincide with domain edges. GC content correlates with compartment identity (GC-rich regions tend toward A compartment), and repetitive element composition shows systematic associations (LINE elements with B compartment; Alu elements with A compartment) [Citation Needed]. The orientation rule for CTCF emerges naturally from training: models learn that CTCF motif orientation, not just presence, predicts which sites will anchor loops. This learned relationship matches the mechanistic understanding from the loop extrusion model, providing validation that models capture biologically meaningful features.\nDespite these advances, significant limitations remain. Resolution is constrained by training data; predicting nucleosome-level contacts requires Micro-C training data that exists for few cell types. The single-cell variation problem is fundamental: models trained on bulk Hi-C predict population averages, but gene regulation may depend on the stochastic 3D configurations in individual cells. Causality cannot be established from prediction alone; a model may correctly predict that two regions contact each other without revealing whether that contact causes any functional consequence. Generalization to cell types distant from training data remains uncertain, and the computational cost of processing megabase sequences limits practical applications for genome-wide analysis.\n\n\n\n\n\n\n\n\nAkita uses dilated convolutions for Hi-C prediction\n\n\n\n\n\n\n\nOrca provides multi-scale predictions\n\n\n\n\n\n\n\n\n\nC.Origami incorporates CTCF for cell-type transfer\n\n\n\n\n\n\n\nPredictions capture TAD boundaries and loops\n\n\n\n\n\n\nFigure 20.4: Sequence-based 3D structure prediction models. (A) Akita architecture: dilated convolutions expand the receptive field to process ~1 Mb sequences and predict Hi-C at 2 kb resolution. (B) Orca multi-scale approach: parallel pathways predict contacts from 4 kb to 256 kb resolution simultaneously. (C) C.Origami incorporates CTCF ChIP-seq alongside sequence, enabling cross-cell-type transfer. (D) Prediction versus ground truth comparison showing that sequence-based models capture TAD boundaries and major loop anchors with correlations of 0.6-0.8.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch20-3d-genome.html#sec-ch20-3d-regulation",
    "href": "part_4/p4-ch20-3d-genome.html#sec-ch20-3d-regulation",
    "title": "20  3D Genome Organization",
    "section": "20.4 3D Structure and Gene Regulation",
    "text": "20.4 3D Structure and Gene Regulation\nThe ultimate purpose of 3D genome prediction is understanding gene regulation. Contact maps matter because they reveal which enhancers can reach which genes. Integrating 3D structure with expression prediction addresses limitations that purely one-dimensional models cannot overcome.\n\n20.4.1 Beyond One-Dimensional Models\nEnformer (Section 16.2) predicts gene expression from sequence within a 200 kilobase window, sufficient to capture many enhancer-promoter relationships but fundamentally limited by its treatment of the genome as a one-dimensional string. This representation cannot distinguish an enhancer that loops to a distant gene from one blocked by a TAD boundary, nor can it explain cell-type-specific contacts that activate different genes from the same enhancer in different contexts. The 3D genome provides this missing context: physical proximity through chromatin loops determines which regulatory elements can communicate.\nConsider an enhancer located 300 kilobases from two genes, one upstream and one downstream. Linear models would predict similar regulatory influence on both genes based on comparable distances. But if a TAD boundary lies between the enhancer and the upstream gene, 3D structure predicts that only the downstream gene receives regulatory input. The boundary insulates the upstream gene from enhancer activity regardless of linear proximity. This insulation function explains why TAD boundaries show such strong evolutionary conservation: disrupting boundaries allows regulatory crosstalk that can dysregulate gene expression with pathogenic consequences.\n\n\n\n\n\n\nKey Insight\n\n\n\nLinear distance is not regulatory distance. An enhancer 500 kb away within the same TAD may have stronger regulatory influence than an enhancer 50 kb away across a TAD boundary. This principle explains why structural variants can be pathogenic even when they do not disrupt any coding sequence—they rewire the regulatory topology.\n\n\n\n\n20.4.2 Structural Variant Interpretation\nThe clinical significance is clearest in structural variant interpretation. Deletions that remove TAD boundaries cause enhancer hijacking, where regulatory elements gain access to genes in adjacent domains. The EPHA4 locus provides the canonical example: limb enhancers normally activate EPHA4 expression in developing limbs. When deletions remove the TAD boundary separating EPHA4 from the adjacent WNT6/PAX3 domain, these enhancers ectopically activate WNT6, causing limb malformations including brachydactyly and polydactyly (Lupiáñez et al. 2015). Different deletion sizes produce different phenotypes depending on which boundaries are removed and which new enhancer-gene contacts form. Similar mechanisms operate in cancer, where structural variants create novel enhancer-oncogene contacts that drive tumor growth [Citation Needed]. The diagnostic challenge is substantial: predicting pathogenicity of structural variants requires understanding which 3D contacts will be disrupted and what new contacts will form, predictions that sequence-only models cannot provide. This challenge intersects with the variant prioritization pipelines discussed in Chapter 28, where 3D genome effects represent a systematic blind spot in current foundation model approaches to variant effect prediction (Chapter 17).\n\n\n\n\n\n\nPractical Guidance: Structural Variant Analysis\n\n\n\nWhen analyzing structural variants for potential pathogenicity:\n\nIdentify nearby TAD boundaries using available Hi-C data or boundary predictions from Akita/Orca\nCheck for boundary disruption: Does the structural variant delete, invert, or translocate a boundary?\nInventory regulatory elements: What enhancers exist in the affected region? What genes lie in adjacent TADs?\nConsider tissue specificity: The consequence depends on which cell types express the relevant enhancers and target genes\nCompare to known cases: Databases of pathogenic structural variants with 3D mechanism provide precedent\n\nStructural variants that disrupt boundaries are more likely to be pathogenic than those preserving domain structure, even if the latter remove more sequence.\n\n\nIntegrating 3D predictions with expression models remains technically challenging. Hybrid approaches use predicted contacts to weight enhancer contributions: rather than treating all enhancers within a window equally, weights reflect predicted contact frequency with the target promoter. This activity-by-contact framework (expression proportional to the sum of enhancer activities weighted by contact frequencies) captures some of the regulatory logic that 1D models miss [Citation Needed]. Graph-based representations (Chapter 21) can encode genes and enhancers as nodes with contacts as edges, enabling graph neural networks to reason about regulatory relationships in 3D space. Attribution methods for understanding which contacts drive expression predictions are examined in Section 24.1. End-to-end training of combined 3D and expression models remains difficult; most current approaches train the components separately and combine predictions post hoc.\n\n\n20.4.3 Causality and Permissive Architecture\nThe causality question complicates interpretation. Do enhancer-promoter contacts cause gene activation, or does gene activation cause contacts? Transcription itself can influence chromatin organization: active transcription may stabilize enhancer-promoter contacts that would otherwise be transient [Citation Needed]. Perturbation experiments provide cleaner causal tests than correlational analysis. Acute degradation of cohesin eliminates TADs within hours, yet most genes show minimal expression changes, suggesting that many TAD structures are permissive rather than deterministic for gene regulation [Citation Needed]. CRISPR-based deletion of specific TAD boundaries similarly produces more modest effects than the structural disruption would suggest [Citation Needed]. The emerging view is nuanced: 3D structure constrains which enhancer-promoter interactions are possible, but whether those interactions occur depends on additional factors including transcription factor availability and chromatin state. This distinction between correlation and causation echoes the confounding challenges discussed in Chapter 12 and the causal inference principles explored in Chapter 25.\n\n\n\n\n\n\nStop and Think\n\n\n\nA recent paper reports that deleting a TAD boundary experimentally results in new chromatin contacts between an enhancer and a previously insulated gene, but the gene’s expression does not change. How would you interpret this result? What additional experiments would help distinguish between (a) the contact is non-functional, (b) the contact is functional but opposed by other regulatory mechanisms, and (c) the contact requires additional factors not present in this experimental system?",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch20-3d-genome.html#sec-ch20-spatial-transcriptomics",
    "href": "part_4/p4-ch20-3d-genome.html#sec-ch20-spatial-transcriptomics",
    "title": "20  3D Genome Organization",
    "section": "20.5 Spatial Transcriptomics",
    "text": "20.5 Spatial Transcriptomics\nSingle-cell RNA sequencing (Chapter 19) reveals cellular heterogeneity but discards spatial information: we learn which genes each cell expresses but not where that cell sits within the tissue. For understanding tumor microenvironments, developmental gradients, or tissue architecture, spatial context is essential. A T cell adjacent to a tumor cell experiences a different microenvironment than one in the surrounding stroma, and this spatial context shapes gene expression programs in ways that dissociated single-cell data cannot capture.\n\n20.5.1 Measurement Technologies\nSpatial transcriptomics technologies fall into two broad categories with complementary strengths. Spot-based methods like Visium (10x Genomics) capture polyadenylated RNA at arrayed positions on a slide, providing transcriptome-wide measurement at approximately 55 micrometer resolution (typically 1 to 10 cells per spot). These methods offer comprehensive gene coverage but limited spatial resolution. Imaging-based methods like MERFISH use sequential rounds of fluorescent hybridization to identify RNA molecules in situ, achieving subcellular resolution but limited to pre-selected gene panels (hundreds to thousands of genes rather than transcriptome-wide) [Citation Needed]. Newer technologies like Stereo-seq achieve near-cellular resolution with transcriptome-wide coverage through spatial barcoding, though they remain less validated than established methods [Citation Needed].\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nConsider the fundamental tradeoff between spatial resolution and gene coverage. Why might this tradeoff exist at a technical level? Which approach would you choose for (1) discovering novel spatial patterns in a tissue, versus (2) mapping known cell-cell interactions at high resolution?\n\n\n\n\n\nTable 20.4: Comparison of spatial transcriptomics platforms. The tradeoff between spatial resolution and gene coverage drives technology selection.\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nResolution\nGene Coverage\nExample Technologies\nBest For\n\n\n\n\nSpot-based\n~55 um (multi-cell)\nTranscriptome-wide\nVisium, Slide-seq\nDiscovery, whole-transcriptome\n\n\nImaging-based\nSubcellular\n100–10,000 genes\nMERFISH, Xenium\nTargeted, single-cell resolution\n\n\nNext-generation\nNear-cellular\nTranscriptome-wide\nStereo-seq, Seq-Scope\nEmerging applications\n\n\n\n\n\n\n\n\n20.5.2 Computational Challenges\nComputational challenges in spatial transcriptomics mirror and extend those in single-cell analysis (Chapter 19). Spot deconvolution addresses the multiple-cells-per-spot problem in Visium data: inferring the cell type composition within each spot by comparing spot expression profiles to reference single-cell atlases. Imputation methods predict expression of genes not measured in imaging-based assays, leveraging correlations learned from reference datasets. Integration aligns spatial data with single-cell references, mapping reference cell types onto spatial coordinates. Domain correction handles batch effects that manifest in spatial patterns as well as expression levels. The sparsity problem is even more severe than in standard single-cell RNA sequencing; gene detection rates in spatial methods often fall below 10 percent [Citation Needed]. The missing modality strategies developed for multi-omics integration (Section 22.6) become essential when spatial methods fail to detect genes that single-cell RNA-seq measures reliably.\n\n\n20.5.3 Spatial Foundation Models\nSpatial foundation models remain much less mature than sequence-based models (Chapter 14, Chapter 15). The fundamental challenge is the lack of an equivalent to evolutionary pretraining: DNA and protein models learn from billions of years of evolutionary experiments encoded in sequence databases, but no comparable natural augmentation exists for spatial organization. Current approaches include graph neural networks that encode spatial relationships as edges between neighboring cells or spots, transformer architectures that treat spatial positions as tokens with positional encodings derived from coordinates, and generative models that learn spatial patterns from atlases of reference tissues. Models like Nicheformer apply transformer architectures to spatial niches (local cellular neighborhoods), learning representations that capture cell-cell communication patterns and tissue microenvironment signatures [Citation Needed]. SpaGCN uses graph convolutional networks with spatial graphs, propagating information between spatially adjacent regions to identify spatial domains with coherent expression patterns [Citation Needed].\nOther approaches address different aspects of the spatial modeling problem. CellPLM pretrains on millions of spatial transcriptomics cells, learning representations that transfer across tissue types and experimental platforms [Citation Needed]. STACI combines spatial coordinates with morphological features from histology images, enabling joint reasoning about molecular and visual tissue properties [Citation Needed]. GraphST uses graph attention networks to propagate expression signals across spatial neighborhoods while preserving local heterogeneity [Citation Needed]. These methods remain early in development compared to sequence foundation models; no spatial equivalent of DNABERT or ESM-2 has achieved broad adoption, and benchmark comparisons across methods remain limited by the diversity of spatial platforms and tissue types.\n\n\n\n\n\n\n\n\nVisium spots capture 1-10 cells each\n\n\n\n\n\n\n\nImaging methods achieve single-cell resolution\n\n\n\n\n\n\n\n\n\nDeconvolution infers cell type composition\n\n\n\n\n\n\n\nGraph neural networks model spatial context\n\n\n\n\n\n\nFigure 20.5: Spatial transcriptomics technologies and computational approaches. (A) Spot-based methods (Visium): transcriptome-wide measurement at ~55 μm resolution, capturing 1-10 cells per spot. (B) Imaging-based methods (MERFISH, Xenium): subcellular resolution but limited to pre-selected gene panels. (C) The deconvolution challenge: inferring cell type composition within each spot using single-cell reference atlases. (D) Spatial foundation models: graph neural networks over spatial tissue graphs enable modeling of cell-cell communication and tissue microenvironment.\n\n\n\nThe clinical applications motivating spatial foundation model development center on tumor microenvironment characterization. The spatial organization of immune cells relative to tumor cells predicts treatment response: tumors with immune cells infiltrating the tumor core respond better to immunotherapy than those with immune exclusion at the tumor periphery [Citation Needed]. Spatial models aim to learn these prognostic patterns from training data, enabling prediction of treatment response from spatial organization alone. Similar applications exist in developmental biology (understanding morphogen gradients and cell fate decisions), neuroscience (mapping brain region organization), and pathology (characterizing disease architecture in tissue sections).",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch20-3d-genome.html#sec-ch20-3d-limitations",
    "href": "part_4/p4-ch20-3d-genome.html#sec-ch20-3d-limitations",
    "title": "20  3D Genome Organization",
    "section": "20.6 Limitations and Open Questions",
    "text": "20.6 Limitations and Open Questions\nCurrent 3D genome and spatial models face limitations that constrain their utility for clinical and research applications. Resolution remains a fundamental constraint: most Hi-C prediction models operate at 2 to 10 kilobase resolution, while functionally relevant enhancer-promoter contacts involve specific sequences within those bins. Predicting which specific kilobases within a TAD contact each other requires resolution that exceeds current training data in most cell types. The resolution needed for accurate prediction may exceed the resolution achievable from bulk Hi-C, creating a data ceiling that computational methods cannot overcome.\nThe population averaging problem is more fundamental than a mere technical limitation. Bulk Hi-C measurements average over millions of cells, each with a different 3D configuration. Any two loci contact each other in only a minority of cells at any given time, yet the averaged contact frequency appears as a single value in the training data. Single-cell Hi-C reveals this heterogeneity but produces extremely sparse data (most possible contacts unmeasured in each cell). Models trained on population averages cannot predict single-cell behavior, yet gene regulation may depend on the stochastic dynamics of contact formation in individual cells. Whether the population average or the single-cell distribution matters more for predicting gene expression remains unclear.\nCausality represents the deepest conceptual challenge. Predicting that two regions contact each other does not establish that the contact causes any biological consequence. Many TAD disruptions produce minimal expression changes; many enhancer-promoter contacts may be bystanders rather than drivers of transcription. The loop extrusion machinery that creates TADs operates continuously, but the transcriptional machinery that reads out enhancer-promoter communication operates on different timescales and with different requirements. Computational predictions of 3D structure are correlational; establishing which predicted contacts matter functionally requires experimental validation that computational methods cannot replace.\nFor clinical applications, the sparse training data creates systematic blind spots. Models trained on GM12878 and K562 may not transfer to the primary cells, developmental stages, or disease states where predictions matter most. A structural variant affecting 3D organization in neural progenitor cells cannot be reliably interpreted using models trained only on lymphoblastoid cells. The cell types most relevant for clinical interpretation are often those with the least 3D characterization data available. This challenge parallels the transferability concerns discussed throughout Chapter 11 and Chapter 12.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nSummarize three distinct limitations of current 3D genome prediction models. For each limitation, identify whether it is primarily (a) a data limitation that more sequencing could address, (b) a fundamental biological challenge, or (c) a computational/algorithmic limitation. How do these limitations affect the clinical utility of 3D structure predictions?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThree key limitations: (1) Resolution gap (a—data): models predict at 2-10 kb but enhancer-promoter contacts require finer resolution; more sequencing could help but may hit biological ceilings. (2) Population averaging (b—biological): bulk Hi-C averages over millions of cells with different configurations; single-cell approaches are fundamentally sparse due to physical constraints. (3) Causality (b—biological): predicting contacts doesn’t establish functional consequences; many contacts are permissive rather than deterministic. These severely limit clinical utility because disease-relevant cells lack training data and predicted contacts may not indicate functional effects.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch20-3d-genome.html#sec-ch20-structure-context",
    "href": "part_4/p4-ch20-3d-genome.html#sec-ch20-structure-context",
    "title": "20  3D Genome Organization",
    "section": "20.7 Structure as Context, Not Cause",
    "text": "20.7 Structure as Context, Not Cause\nThe genome’s three-dimensional organization provides context that one-dimensional sequence models cannot capture. Enhancer-promoter contacts explain regulatory relationships spanning hundreds of kilobases; TAD boundaries constrain which elements can interact; tissue architecture determines the cellular neighborhoods where gene expression programs execute. Models like Akita, Orca, and C.Origami demonstrate that sequence contains substantial information about chromatin folding, predicting contact maps from DNA sequence with accuracy sufficient to identify structural variants and disease-associated changes.\nYet the functional role of 3D structure remains more modest than early enthusiasm implied. Experimental perturbation studies show that TAD boundary disruption often has limited expression consequences [Citation Needed]. Many chromatin contacts appear permissive rather than instructive: they establish the possibility of regulatory communication without determining whether that communication occurs. A predicted enhancer-promoter contact indicates that interaction could happen, not that it does happen or that it matters when it does. The 3D genome may constrain the regulatory landscape without specifying regulatory outcomes.\n\n\n\n\n\n\nKey Insight\n\n\n\n3D structure is permissive, not deterministic. Think of TADs as enabling rather than commanding: they create the possibility of enhancer-promoter communication, but the actual communication requires transcription factors, chromatin accessibility, and other regulatory inputs. This is why disrupting a TAD boundary can cause disease (by enabling pathogenic new contacts) without boundary integrity being necessary for normal gene expression in most cases.\n\n\nThis distinction shapes how 3D structure should be integrated with other modalities. Chromatin contacts become edges in gene regulatory networks (Chapter 21), providing structural priors for graph-based reasoning. Spatial expression patterns integrate with multi-omics approaches (Chapter 22), adding tissue architecture alongside genomics and transcriptomics. For interpretability (Chapter 24), 3D structure offers mechanistic hypotheses that require experimental validation. Whether a predicted regulatory effect operates through chromatin proximity, or whether proximity merely correlates with regulation through shared causes, remains a question that computational models can motivate but not answer. The integration of 3D information into genomic AI proceeds with appropriate uncertainty about what that information contributes.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat is the “orientation rule” for CTCF binding sites, and why does it determine which sites will anchor chromatin loops?\nExplain why “linear distance is not regulatory distance” in the context of enhancer-gene regulation.\nHow does the loop extrusion mechanism create TAD boundaries?\nWhy is 3D genome structure described as “permissive rather than deterministic” for gene regulation?\nWhat is the population averaging problem in Hi-C data, and why does it limit our understanding of gene regulation at the single-cell level?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nCTCF orientation rule: Convergent CTCF pairs (→←) form stable loop anchors because the loop extrusion mechanism halts when cohesin encounters CTCF sites oriented toward each other. Divergent (←→) and tandem (→→) orientations do not block extrusion, so no stable loop forms.\nLinear vs. regulatory distance: An enhancer 500 kb away within the same TAD may regulate a gene more strongly than an enhancer 50 kb away across a TAD boundary. The 3D folding determines regulatory proximity—boundaries insulate genes from enhancers despite short linear distances, while loops bring distant elements into contact.\nLoop extrusion and TAD boundaries: Cohesin loads onto chromatin and extrudes DNA bidirectionally, enlarging a loop until encountering convergent CTCF sites. These CTCF-anchored loops define TAD boundaries. Multiple adjacent loops create the triangular TAD structure visible in Hi-C.\nPermissive vs. deterministic: 3D structure establishes which enhancer-promoter interactions can occur, but whether they do occur depends on additional factors (transcription factors, chromatin accessibility). Many TAD disruptions produce minimal expression changes, showing that contacts enable but don’t command gene regulation.\nPopulation averaging problem: Bulk Hi-C averages over millions of cells, each with a different 3D configuration. Any two loci contact each other in only 5-15% of cells at any time, but Hi-C reports the average contact frequency. This obscures single-cell stochastic dynamics that may be critical for gene regulation but cannot be recovered from bulk data.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nKey Concepts Covered:\n\nChromatin organization hierarchy: Chromosome territories, A/B compartments, TADs, and fine-scale loops represent nested organizational levels with distinct mechanisms\nLoop extrusion model: Cohesin extrudes DNA until blocked by convergent CTCF sites, explaining TAD boundary formation\nHi-C and contact matrices: Chromosome conformation capture methods measure 3D contacts; resolution depends on sequencing depth\nSequence-based prediction: Akita, Orca, and C.Origami predict contact maps from sequence, achieving ~0.6-0.8 correlation with experimental data\nStructural variant interpretation: Boundary disruption causes enhancer hijacking with pathogenic consequences\nSpatial transcriptomics: Extends single-cell analysis to include tissue location, enabling microenvironment characterization\n\nCore Takeaways:\n\nThe 3D genome provides regulatory context that 1D models cannot capture—linear distance is not regulatory distance\nCTCF motif orientation is a powerful predictor of loop anchors: convergent pairs form loops, divergent pairs do not\nSequence contains substantial 3D information, but prediction accuracy varies by organizational level (best for TAD boundaries, worst for compartments)\n3D contacts are permissive rather than deterministic—contact predicts that regulation could occur, not that it does\nClinical application to structural variants is limited by training data bias toward a few cell lines\n\nConnections to Other Chapters:\n\nBuilds on: Chapter 6 (dilated convolutions), Chapter 16 (Enformer and 1D models)\nExtends to: Chapter 21 (3D contacts as graph edges), Chapter 22 (spatial integration)\nRelevant evaluation: Chapter 11 (transfer evaluation), Chapter 24 (attribution methods)\nClinical context: Chapter 17 (variant effect prediction gaps), Chapter 28 (structural variant interpretation)\n\n\n\n\n\n\n\nFudenberg, Geoff, David R. Kelley, and Katherine S. Pollard. 2020. “[Akita] Predicting 3D Genome Folding from DNA Sequence with Akita.” Nature Methods 17 (11): 1111–17. https://doi.org/10.1038/s41592-020-0958-x.\n\n\nLupiáñez, Darío G., Katerina Kraft, Verena Heinrich, Peter Krawitz, Francesco Brancati, Eva Klopocki, Denise Horn, et al. 2015. “Disruptions of Topological Chromatin Domains Cause Pathogenic Rewiring of Gene-Enhancer Interactions.” Cell 161 (5): 1012–25. https://doi.org/10.1016/j.cell.2015.04.004.\n\n\nTan, Jimin, Nina Shenker-Tauris, Javier Rodriguez-Hernaez, Eric Wang, Theodore Sakellaropoulos, Francesco Boccalatte, Palaniraja Thandapani, et al. 2023. “Cell-Type-Specific Prediction of 3D Chromatin Organization Enables High-Throughput in Silico Genetic Screening.” Nature Biotechnology 41 (8): 1140–50. https://doi.org/10.1038/s41587-022-01612-8.\n\n\nZhou, Jian. 2022. “Sequence-Based Modeling of Three-Dimensional Genome Architecture from Kilobase to Chromosome Scale.” Nature Genetics 54 (5): 725–34. https://doi.org/10.1038/s41588-022-01065-4.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>3D Genome Organization</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch21-networks.html",
    "href": "part_4/p4-ch21-networks.html",
    "title": "21  Graph and Network Models",
    "section": "",
    "text": "21.1 Biological Networks and Data Resources\nGraph neural networks can only learn from relationships encoded in their input graphs. The choice of network, its source, and its inherent biases determine what a model can discover and what it will miss. Understanding the landscape of available biological networks, their construction methods, and their systematic limitations is therefore prerequisite to effective graph-based modeling.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch21-networks.html#sec-ch21-biological-networks",
    "href": "part_4/p4-ch21-networks.html#sec-ch21-biological-networks",
    "title": "21  Graph and Network Models",
    "section": "",
    "text": "21.1.1 Landscape of Biological Graphs\nBefore examining graph neural network architectures, it is essential to understand what biological networks exist and where they come from. The choice of network fundamentally shapes what a model can learn, and the biases inherent in network construction propagate through all downstream analyses.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nCurrent protein-protein interaction databases are estimated to capture only 20-30% of true human interactions. Before reading further, predict: How might this incompleteness affect which types of proteins have more documented interactions? What factors might determine whether a protein’s interactions get discovered and catalogued?\n\n\nPhysical associations between proteins constitute perhaps the most widely used network type for GNN applications. Major databases include STRING, which integrates experimental data with computational predictions and text mining to assign confidence scores to interactions; BioGRID, which focuses on curated experimental interactions; and IntAct, which provides detailed interaction metadata from direct molecular experiments. These protein-protein interaction networks are incomplete (current estimates suggest only 20-30% of human PPIs are catalogued) and biased toward well-studied proteins in well-characterized pathways (szklarczyk2023string?; oughtred2021biogrid?; orchard2014intact?; venkatesan2009protein?; hart2006completeness?). A gene involved in cancer or a common disease may have hundreds of documented interactions, while an uncharacterized protein in a specialized tissue may have none, not because it lacks interactions but because no one has looked.\nTranscriptional control relationships require a different network structure. Unlike PPIs, gene regulatory networks are inherently directed: a transcription factor activates or represses its targets, not vice versa. Sources include chromatin immunoprecipitation sequencing (ChIP-seq) experiments that identify transcription factor binding sites, chromatin accessibility data (assay for transposase-accessible chromatin sequencing (ATAC-seq), DNase-seq) that reveals active regulatory regions, and chromosome conformation capture (Hi-C) that maps enhancer-promoter contacts (Chapter 20). Databases like ENCODE and the Roadmap Epigenomics Project provide regulatory annotations across cell types, though coverage varies dramatically by tissue (Chapter 2). Computational methods infer regulatory edges from expression correlations or sequence motifs, but such predictions contain substantial false positives and miss context-specific interactions.\nOrganized biochemical knowledge takes yet another form. KEGG, Reactome, and WikiPathways curate reactions, enzymatic steps, and signaling cascades into hierarchical pathway and metabolic networks where nodes can represent genes, proteins, metabolites, or abstract pathway concepts. These networks encode decades of molecular biology knowledge but reflect historical research priorities: metabolism and signal transduction are well-characterized, while more recently discovered processes like autophagy or RNA modification have sparser coverage.\nBeyond molecular interactions, relationships among genes, diseases, drugs, phenotypes, and other biomedical entities require heterogeneous representations. Unlike protein interaction networks, which contain a single node type and edge type, knowledge graphs are inherently heterogeneous: nodes represent diverse entity classes, and edges capture semantically distinct relationship types. This heterogeneity enables richer reasoning but demands architectures capable of handling multiple node and edge embeddings.\nSeveral large-scale biomedical knowledge graphs have become standard resources. Hetionet integrates 47,031 nodes across 11 types (genes, diseases, compounds, anatomies, and others) with 2.25 million edges spanning 24 relationship types, providing a comprehensive substrate for computational drug repurposing (himmelstein2017systematic?). The Unified Medical Language System (UMLS) aggregates over 200 biomedical vocabularies into a metathesaurus linking millions of concepts through hierarchical and associative relationships. PrimeKG consolidates 17 biological databases into a precision medicine knowledge graph with over 4 million relationships connecting diseases, drugs, genes, pathways, and phenotypes, explicitly designed for machine learning applications (chandak2023primekg?).\nDisease-gene association databases provide critical edges for clinical applications. DisGeNET curates over one million gene-disease associations from expert-reviewed sources, GWAS catalogs (Chapter 3), and text mining, assigning evidence scores that enable confidence-based filtering (pinero2020disgenet?). OMIM (Online Mendelian Inheritance in Man) provides authoritative curation of Mendelian disease genes, while OrphaNet focuses on rare diseases with detailed phenotypic annotations (Chapter 28). The Clinical Genome Resource (ClinGen) adds expert-reviewed gene-disease validity assessments using standardized evidence frameworks.\nDrug-centric resources complete the translational picture. DrugBank provides comprehensive drug-target annotations with mechanism and pharmacology details. ChEMBL aggregates bioactivity data from medicinal chemistry literature, linking compounds to protein targets through binding affinity measurements. The Drug Gene Interaction Database (DGIdb) consolidates druggable gene categories and known interactions to support target prioritization (Chapter 29).\n\n\n\nTable 21.1: Major biological network types and their characteristics. The choice of network determines what a GNN can learn and what biases it inherits.\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork Type\nExample Databases\nNode Types\nEdge Semantics\nKey Limitation\n\n\n\n\nProtein-Protein Interaction\nSTRING, BioGRID, IntAct\nProteins\nPhysical binding, co-complex\n20-30% coverage; study bias\n\n\nGene Regulatory\nENCODE, Roadmap, JASPAR\nTFs, genes, enhancers\nActivation/repression (directed)\nCell-type specificity\n\n\nPathway/Metabolic\nKEGG, Reactome, WikiPathways\nGenes, metabolites, reactions\nEnzymatic, signaling\nHistorical research bias\n\n\nKnowledge Graph\nHetionet, PrimeKG, UMLS\nMultiple entity types\nMultiple relationship types\nIntegration quality varies\n\n\nSpatial/Cell-Cell\nSpatial transcriptomics data\nCells, spots\nProximity, communication\nEmerging; sparse coverage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPPI networks: undirected physical binding\n\n\n\n\n\n\n\nGene regulatory networks: directed TF-target relationships\n\n\n\n\n\n\n\n\n\nKnowledge graphs: heterogeneous multi-type nodes and edges\n\n\n\n\n\n\n\nSpatial graphs: cell proximity from spatial transcriptomics\n\n\n\n\n\n\nFigure 21.1: Landscape of biological networks. (A) Protein-protein interaction networks: undirected edges represent physical binding, with only 20-30% of interactions currently catalogued. (B) Gene regulatory networks: directed edges from transcription factors to targets, derived from ChIP-seq and accessibility data. (C) Knowledge graphs: heterogeneous graphs with multiple node types (genes, diseases, drugs, pathways) and relationship types enabling multi-hop reasoning. (D) Spatial and cell-cell interaction graphs: emerging from spatial transcriptomics, encoding tissue architecture and cell communication.\n\n\n\nThe power of knowledge graphs lies in their support for multi-hop reasoning. A query asking whether a drug might treat a disease can traverse multiple edge types: drug inhibits protein A, protein A interacts with protein B, protein B is implicated in disease. Each hop contributes evidence, and the combination of paths provides signal that no single edge contains. Graph neural networks learn to aggregate across such paths, weighting different relationship types and path lengths according to their predictive value for specific tasks.\nSpatially resolved transcriptomics and imaging data give rise to graphs capturing tissue organization invisible to bulk or even single-cell measurements (Chapter 19). In these spatial and cell-cell interaction graphs, nodes represent cells or spatial locations, while edges encode physical proximity or inferred ligand-receptor communication. Such graphs enable questions about how spatial context influences cell behavior.\n\n\n\n\n\n\nKnowledge Check: Network Bias\n\n\n\nConsider a protein that has no documented interactions in STRING or BioGRID. Answer these questions:\n\nDoes this mean it truly has no binding partners?\nWhat factors might explain why some proteins have hundreds of documented interactions while others have none?\nHow might this asymmetry affect a GNN trained on such networks?\n\n\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nNo - The absence of documented interactions likely reflects lack of study rather than biological truth. Most uncharacterized proteins do have binding partners.\nResearch priorities and historical focus - Proteins involved in cancer, common diseases, or conserved pathways attract more experimental attention. High-throughput screens are biased toward abundant, easily expressed proteins. Tissue-specific or condition-specific proteins may be understudied.\nThe GNN will preferentially propagate signals toward well-connected hub genes - This creates a rich-get-richer dynamic where well-studied genes dominate predictions, potentially missing novel biology in peripheral network regions. The model may learn to recapitulate existing knowledge rather than discover new patterns.\n\n\n\n\n\n\n21.1.2 Biases and Limitations\nAll biological networks share systematic biases that affect downstream modeling. Well-studied genes appear as highly connected hubs not necessarily because they have more interactions but because researchers have investigated them more thoroughly. This ascertainment bias means that GNNs trained on network structure may primarily learn to propagate signals toward well-characterized genes, potentially missing novel biology in peripheral network regions.\nNetwork incompleteness creates particular challenges for message passing algorithms. If a critical interaction is missing, information cannot flow across that gap. If a spurious interaction is present, noise propagates where it should not. These issues are especially acute for less-studied organisms, tissues, or disease contexts where network coverage is sparse.\nThe distinction between physical and functional associations matters for interpretation. A protein-protein interaction might represent stable complex membership, transient signaling, or indirect association through shared binding partners. Different edge types may warrant different treatment by graph models, but many databases conflate these categories or provide insufficient metadata to distinguish them.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch21-networks.html#sec-ch21-gnn-fundamentals",
    "href": "part_4/p4-ch21-networks.html#sec-ch21-gnn-fundamentals",
    "title": "21  Graph and Network Models",
    "section": "21.2 Graph Neural Network Fundamentals",
    "text": "21.2 Graph Neural Network Fundamentals\nThe mathematical machinery underlying graph neural networks differs fundamentally from the architectures examined in previous chapters. Where convolutional and transformer models operate on regular structures (sequences, grids), GNNs must handle irregular topology with variable-degree nodes, no inherent ordering, and arbitrary connectivity (Chapter 6, Chapter 7). This section develops the message passing framework that addresses these challenges, then surveys the canonical architectures that have become standard tools for biological applications.\n\n21.2.1 Message Passing Principles\nThe challenge of learning from graph-structured data lies in the irregular topology: unlike images (regular grids) or sequences (linear chains), graphs have variable-degree nodes, no inherent ordering, and complex connectivity patterns. Classical approaches computed hand-crafted features such as degree centrality, clustering coefficients, or shortest path statistics, then fed these to standard machine learning models. Such features capture useful properties but cannot adapt to task-specific patterns.\nMessage passing provides a learnable alternative. The core intuition is local information exchange: each node should update its representation based on what its neighbors know. By iterating this process across multiple layers, information propagates across the graph, allowing nodes to incorporate signals from increasingly distant parts of the network.\n\n\n\n\n\n\nKey Insight: Message Passing as Neural Diffusion\n\n\n\nThink of message passing as a controlled diffusion process. Just as heat diffuses from hot regions to cold ones, information in a GNN flows from nodes to their neighbors. After one layer, each node knows about its immediate neighbors. After two layers, it knows about neighbors of neighbors. After L layers, information has spread across L-hop neighborhoods. The learned weights control how information mixes, not just that it spreads.\n\n\nFormally, at layer \\(\\ell\\), each node \\(i\\) maintains a hidden state \\(\\mathbf{h}_i^{(\\ell)}\\). A message passing layer computes, for each edge from neighbor \\(j\\) to node \\(i\\), a message:\n\\[\n\\mathbf{m}_{ij}^{(\\ell)} = \\phi_m\\left(\\mathbf{h}_i^{(\\ell)}, \\mathbf{h}_j^{(\\ell)}, \\mathbf{e}_{ij}\\right)\n\\]\nwhere \\(\\phi_m\\) is a learned function and \\(\\mathbf{e}_{ij}\\) represents edge features. The node then aggregates messages from all neighbors and updates its state:\n\\[\n\\mathbf{h}_i^{(\\ell+1)} = \\phi_h\\left(\\mathbf{h}_i^{(\\ell)}, \\bigoplus_{j \\in \\mathcal{N}(i)} \\mathbf{m}_{ij}^{(\\ell)}\\right)\n\\]\nwhere \\(\\mathcal{N}(i)\\) denotes neighbors of node \\(i\\) and \\(\\bigoplus\\) is a permutation-invariant aggregation (sum, mean, max, or attention-weighted combination). The aggregation must be permutation-invariant because neighbors have no inherent ordering.\n\n\n\n\n\n\n\n\nInitial node features from foundation model embeddings\n\n\n\n\n\n\n\nMessage computation along each edge\n\n\n\n\n\n\n\n\n\nAggregation combines neighbor information\n\n\n\n\n\n\n\nAfter L layers, nodes capture L-hop neighborhood context\n\n\n\n\n\n\nFigure 21.2: Message passing in graph neural networks. (A) Initial state: each node has feature vectors from foundation model embeddings. (B) Message computation: for each edge, compute message m_ij = φ(h_i, h_j, e_ij). (C) Aggregation: each node aggregates incoming messages using permutation-invariant operations (sum, mean, max, or attention). (D) After L layers: each node’s embedding incorporates information from its L-hop neighborhood, capturing pathway-level context.\n\n\n\nAfter \\(L\\) layers, a node’s representation incorporates information from all nodes within \\(L\\) hops. For biological networks, this means a gene’s learned embedding can reflect not only its own features but signals from interaction partners, their partners, and so on, capturing pathway-level and module-level context.\n\n\n\n\n\n\nWorked Example: One Message Passing Step\n\n\n\nConsider a small PPI network where protein A interacts with proteins B and C.\nInitial embeddings (from a foundation model):\n\n\\(\\mathbf{h}_A = [0.8, 0.2]\\) (kinase signature)\n\\(\\mathbf{h}_B = [0.3, 0.7]\\) (receptor signature)\n\\(\\mathbf{h}_C = [0.5, 0.5]\\) (adapter protein)\n\nStep 1 - Message computation: Using a simple linear transformation \\(W\\):\n\nMessage from B to A: \\(\\mathbf{m}_{BA} = W \\cdot \\mathbf{h}_B = [0.4, 0.6]\\)\nMessage from C to A: \\(\\mathbf{m}_{CA} = W \\cdot \\mathbf{h}_C = [0.5, 0.5]\\)\n\nStep 2 - Aggregation (mean): \\[\\text{aggregated} = \\frac{1}{2}([0.4, 0.6] + [0.5, 0.5]) = [0.45, 0.55]\\]\nStep 3 - Update (residual connection + ReLU): \\[\\mathbf{h}_A' = \\text{ReLU}(\\mathbf{h}_A + \\text{aggregated}) = \\text{ReLU}([1.25, 0.75]) = [1.25, 0.75]\\]\nAfter this layer, protein A’s representation now incorporates information about its interaction partners. If B is a known receptor for a disease-relevant ligand, that signal has begun propagating to A.\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nBefore reading further, test your understanding of message passing:\n\nIf a GNN has 3 message passing layers, how many hops away can information travel from any given node?\nWhy must the aggregation function be permutation-invariant?\nWhat happens to node representations if you stack many message passing layers without any mechanism to prevent it?\n\n\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\n3 hops - Each layer extends the receptive field by one hop, so after L layers, a node has incorporated information from all nodes within L hops.\nBecause graph neighbors have no inherent ordering - Unlike sequence positions or grid locations, the set of neighbors in a graph has no canonical order. The aggregation function must produce the same result regardless of how neighbors are enumerated.\nThey converge (over-smoothing), losing discriminative signal - Repeated averaging causes node representations to become increasingly similar, eventually converging toward the same mean representation within connected components. This is why most practical GNNs use only 2-4 layers.\n\n\n\n\n\n\n21.2.2 Canonical Architectures\nSeveral GNN architectures have become standard tools for biological applications, each with distinct design choices that reflect different tradeoffs between computational efficiency, expressive power, and scalability.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore reading about over-smoothing, consider this question: If you stack 10 graph convolutional layers that average neighbor representations at each step, what do you think happens to the node embeddings in a connected graph? Will they become more diverse and specialized, or more similar? Why?\n\n\nThe simplest approach performs normalized neighborhood averaging followed by linear transformation and nonlinearity. Graph convolutional networks (GCN) are computationally efficient and conceptually straightforward but suffer from over-smoothing when stacked deeply: repeated averaging causes node representations to converge, losing the discriminative signal that distinguishes different network positions (li2018deeper?; oono2019graph?). The mathematical reason is intuitive: averaging is a low-pass filter that removes high-frequency variation. After many rounds of averaging, all nodes in a connected component converge toward the same mean representation, much like repeatedly blurring an image eventually produces uniform gray.\n\n\n\n\n\n\nDifficulty Warning: Over-Smoothing\n\n\n\nThe over-smoothing problem is subtle but critical. Intuitively, if you repeatedly average a node’s representation with its neighbors, eventually all nodes in a connected component converge to similar representations. This means deeper GNNs are not always better. In practice, most GNNs use only 2-4 layers. Understanding when and why to limit depth is essential for effective GNN design.\n\n\nScalability to large graphs requires a different strategy. GraphSAGE learns aggregation functions that operate on sampled neighborhoods rather than the full neighbor set (hamilton2017inductive?). The key insight is that full-batch GCN requires storing all node representations simultaneously, which becomes prohibitive for graphs with millions of nodes. By sampling a fixed number of neighbors (say, 10) at each layer rather than using all neighbors, GraphSAGE bounds memory requirements and enables mini-batch training. The sampling introduces variance but enables scaling to graphs that would be impossible otherwise. Crucially, GraphSAGE also provides inductive capability: the model can generate embeddings for nodes not seen during training by applying learned aggregators to their neighborhoods. For biological networks that grow as new genes are characterized, this generalization is valuable.\nWhen some neighbors matter more than others, attention-weighted aggregation provides a learnable solution. Graph attention networks (GAT) compute attention scores between each node and its neighbors, allowing the model to focus on the most informative interactions (velickovic2018graph?). This is analogous to attention in transformers but operates over graph neighborhoods rather than sequence positions (Chapter 7).\nFinally, the boundary between sequence and graph models blurs when transformer architectures extend to graphs. Graph transformers replace local message passing with structured or global attention. Some variants attend over all node pairs with positional encodings derived from graph structure (shortest paths, Laplacian eigenvectors); others restrict attention to k-hop neighborhoods (ying2021graphormer?; dwivedi2021graph?). These architectures potentially capture long-range dependencies that multi-layer message passing struggles to propagate.\n\n\n\nTable 21.2: Comparison of canonical GNN architectures. The choice depends on graph size, whether inductive inference is needed, and computational budget.\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nAggregation Method\nScalability\nInductive?\nKey Strength\nKey Limitation\n\n\n\n\nGCN\nNormalized mean\nLimited (full-batch)\nNo\nSimple, efficient\nOver-smoothing with depth\n\n\nGraphSAGE\nSampled aggregators\nHigh (mini-batch)\nYes\nScales to large graphs\nSampling variance\n\n\nGAT\nAttention-weighted\nModerate\nYes\nLearns edge importance\nQuadratic in neighbors\n\n\nGraph Transformer\nGlobal/structured attention\nVariable\nYes\nLong-range dependencies\nComputational cost\n\n\n\n\n\n\nThe expressiveness of GNNs is bounded by their ability to distinguish different graph structures. Theoretical analysis connects standard message passing to the Weisfeiler-Lehman graph isomorphism test, revealing that certain graph structures remain indistinguishable regardless of the number of layers (xu2019how?; morris2019weisfeiler?). For most biological applications, this theoretical limitation is less constraining than practical issues of data quality, training efficiency, and interpretability (Chapter 24).",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch21-networks.html#sec-ch21-fm-embeddings",
    "href": "part_4/p4-ch21-networks.html#sec-ch21-fm-embeddings",
    "title": "21  Graph and Network Models",
    "section": "21.3 Foundation Model Embeddings as Node Features",
    "text": "21.3 Foundation Model Embeddings as Node Features\nThe power of combining foundation models with graph neural networks lies in their complementary strengths. Foundation models extract rich biological information from sequence, but they operate on isolated entities without relational context. Graph neural networks reason over relationships, but they require informative node features to propagate meaningful signal. This section examines how to integrate these approaches effectively, from the architectural principle underlying the combination to practical patterns for implementation.\n\n21.3.1 Integration Principle\nThe central architectural insight for genomic graph learning is that foundation models and graph neural networks operate at complementary levels of abstraction. Sequence-based foundation models excel at extracting biological information from linear sequences: ESM-2 learns evolutionary constraints and structural propensities from protein sequences (Chapter 15); DNABERT and its successors capture regulatory motifs and sequence grammar (Chapter 14); single-cell foundation models like scGPT learn cell state representations from expression profiles (Chapter 19). These representations encode rich biological knowledge but operate on individual entities without explicit relational information. The principles of feature extraction from pretrained models, which underpin this integration pattern, are developed in Section 9.3.\n\n\n\n\n\n\n\n\nFoundation models produce rich entity representations\n\n\n\n\n\n\n\nBiological networks encode relational structure\n\n\n\n\n\n\n\n\n\nGNNs integrate embeddings with network structure\n\n\n\n\n\n\n\nCombined capabilities exceed either approach alone\n\n\n\n\n\n\nFigure 21.3: The foundation model + GNN integration paradigm. (A) Foundation models produce rich representations of individual entities: protein embeddings from ESM, DNA embeddings from DNABERT, cell embeddings from scGPT. (B) Biological networks encode relational structure that sequence models cannot capture. (C) GNNs integrate foundation model embeddings with network structure through message passing. (D) This combination enables capabilities neither achieves alone: disease gene prioritization, drug-target prediction, and perturbation response modeling.\n\n\n\nGraph neural networks excel at learning from relational structure but require informative node features to propagate. When node features are uninformative (simple one-hot encodings or scalar expression values), message passing can only learn from network topology. When node features carry substantial biological signal, message passing can refine and contextualize that information based on network position.\nCombining these approaches follows a natural two-stage pattern. First, apply a foundation model to each entity in the graph to generate initial node embeddings. For a protein-protein interaction network, run ESM-2 on each protein sequence; for a gene regulatory network, use DNA embeddings for regulatory elements and protein embeddings for transcription factors; for a cell graph, apply scGPT to generate cell state representations. Second, train a GNN on these embeddings using the biological graph structure, allowing message passing to integrate entity-level representations with relational context.\nThis combination yields capabilities that neither component achieves alone. The foundation model provides rich, transferable features that would require massive labeled datasets to learn from scratch. The GNN provides relational reasoning that sequence models cannot perform. A protein’s druggability depends both on intrinsic properties (binding pocket geometry, expression pattern) that ESM captures and on network context (pathway position, interaction partners) that the GNN integrates.\n\n\n\n\n\n\nStop and Think\n\n\n\nImagine you are building a model to predict which genes are essential for cancer cell survival. You have: - ESM-2 embeddings for each protein - A protein-protein interaction network from STRING - CRISPR knockout screens identifying essential genes in 10 cancer cell lines\nSketch out your approach. Would you freeze the ESM-2 embeddings or fine-tune them? How many GNN layers would you use? What might go wrong if you only used the network structure without the sequence embeddings?\n\n\n\n\n21.3.2 Practical Integration Patterns\nSeveral integration patterns have emerged in practice, each suited to different constraints and objectives. The simplest approach freezes foundation model weights and treats embeddings as fixed features, training only the GNN layers. This is computationally efficient and prevents catastrophic forgetting of pretrained knowledge but limits the model’s ability to adapt representations to the specific task (Chapter 9).\nWhen sufficient task-specific data is available, allowing gradients to flow through both the GNN and (parts of) the foundation model enables end-to-end optimization. This joint fine-tuning typically requires careful learning rate scheduling, with smaller updates to foundation model parameters and larger updates to GNN layers. The approach can improve performance but risks overfitting and requires substantially more computation.\nA middle ground inserts small trainable modules between foundation model layers or at the interface between foundation model outputs and GNN inputs. Adapter-based integration provides task adaptation with modest parameter overhead, avoiding full fine-tuning costs while retaining flexibility (Chapter 9).\nThe granularity of representations also offers flexibility. For proteins, one might extract both per-residue embeddings (capturing local structure) and sequence-level embeddings (capturing global properties), concatenating these as node features. For regulatory networks, one might combine nucleotide-level DNA embeddings with region-level chromatin accessibility predictions. This multi-scale integration uses foundation model representations at multiple granularities to capture different aspects of biological function.\n\n\n\n\n\n\nPractical Guidance: Choosing an Integration Strategy\n\n\n\nStart simple: Begin with frozen embeddings and a 2-layer GNN. This establishes a strong baseline with minimal hyperparameter tuning.\nAdd complexity when justified: If frozen embeddings underperform and you have substantial labeled data (thousands of examples), try adapter-based integration before full fine-tuning.\nMonitor for overfitting: Watch the gap between training and validation performance. Joint fine-tuning on small datasets often memorizes rather than generalizes.\nConsider compute budget: Pre-computing embeddings for all nodes once is much cheaper than computing them on-the-fly during training. For large graphs, this can reduce training time by 10-100x.\n\n\nThe choice of integration pattern depends on data availability, computational resources, and the degree of distribution shift between foundation model pretraining and the target application (Chapter 12). For well-characterized systems with substantial labeled data, joint fine-tuning may be warranted. For novel organisms or rare diseases with limited labels, frozen embeddings with simple GNN layers often generalize better.\n\n\n21.3.3 Evidence for the Integration Benefit\nEmpirical studies consistently demonstrate that foundation model embeddings improve GNN performance on biological tasks. In protein function prediction, ESM embeddings combined with PPI network GNNs substantially outperform either sequence-only or network-only baselines (lin2025gobeacon?). The improvement is particularly pronounced for proteins with few characterized interaction partners, where network structure alone provides limited signal but sequence features carry evolutionary information.\nFor disease gene prioritization, combining DNA and protein foundation model embeddings with multi-relational GNNs over heterogeneous biological networks improves ranking of causal genes from GWAS loci (saadat2024dna?; decarlo2023xgdag?). The foundation model features help distinguish genes with similar network positions based on sequence-level functional signals.\nIn single-cell analysis, scGPT embeddings combined with cell-cell communication graphs enable more accurate prediction of perturbation effects than either component alone (cui2023scgpt?; segceco2024?; cgcom2023?). The cell embeddings capture transcriptional state, while the graph structure encodes spatial and molecular interaction context.\nThese results suggest that the integration principle generalizes across biological domains. The specific foundation models and graph types vary, but the architectural pattern (rich entity embeddings + relational structure + message passing) consistently outperforms simpler alternatives.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch21-networks.html#sec-ch21-applications",
    "href": "part_4/p4-ch21-networks.html#sec-ch21-applications",
    "title": "21  Graph and Network Models",
    "section": "21.4 Applications",
    "text": "21.4 Applications\nThe integration of foundation model embeddings with graph neural networks enables applications across the translational pipeline. Disease gene prioritization leverages network context to identify causal genes from GWAS loci. Drug-target prediction exploits both sequence-derived druggability features and pathway positioning. Knowledge graph reasoning supports multi-hop inference for drug repurposing. Pathway analysis identifies dysregulated modules in patient-specific contexts. Each application follows the same architectural pattern (rich node features from foundation models, relational structure from biological networks, refinement through message passing) while addressing distinct biological questions.\n\n21.4.1 Disease Gene Prioritization\nGenome-wide association studies identify genomic loci associated with disease risk but rarely pinpoint causal genes (Chapter 3). A typical GWAS locus contains dozens of genes, most of which are passengers linked to the true causal variant through linkage disequilibrium. Identifying which gene(s) mediate the association requires integrating functional evidence with genetic signal.\nNetwork-based prioritization leverages the observation that disease genes cluster in biological networks. If a GWAS locus contains genes A, B, and C, and gene B interacts with five known disease genes while A and C interact with none, gene B becomes a stronger causal candidate. Graph neural networks formalize and extend this intuition, learning to propagate disease labels through networks and score candidate genes based on their network context.\n\n\n\n\n\n\nKey Insight: Beyond Guilt by Association\n\n\n\nClassical network analysis uses “guilt by association”—genes near known disease genes are likely disease genes. GNNs go further by learning which associations matter. Not all neighbors are equally informative. A GNN trained with foundation model embeddings can learn that a neighbor sharing functional domains (detected through sequence similarity) is more informative than a neighbor connected only through high-throughput screening artifacts.\n\n\nThe integration with foundation models strengthens this approach. Rather than relying solely on network topology, which favors well-studied hub genes, the model can assess each candidate’s intrinsic functional properties through sequence embeddings. A gene with protein features characteristic of disease-relevant functions (membrane localization, DNA binding, signaling domains) receives higher scores even if its network position is peripheral. This helps mitigate the ascertainment bias toward well-characterized genes that plagues purely topological methods.\n\n\n\n\n\n\n\n\nGWAS locus: multiple genes in LD with lead SNP\n\n\n\n\n\n\n\nNetwork context: one gene connects to disease genes\n\n\n\n\n\n\n\n\n\nGNN scoring prioritizes network-connected candidates\n\n\n\n\n\n\n\nIntegration of sequence and network features\n\n\n\n\n\n\nFigure 21.4: Disease gene prioritization for GWAS follow-up. (A) The GWAS challenge: a lead SNP is in LD with multiple genes—which is causal? (B) Network context: Gene B connects to five known disease genes while A, C, D are peripheral. (C) GNN scoring: foundation model embeddings as node features combined with network propagation prioritize gene B. (D) Sequence-network integration: Gene C has strong protein features while Gene B has network context—combined scoring identifies both as candidates for follow-up.\n\n\n\nClinical applications include rare disease diagnosis, where patient exome sequencing identifies hundreds of candidate variants and network-based scoring helps prioritize which genes to investigate further (Chapter 28). The approach also supports drug target identification by highlighting genes whose network position and functional properties make them amenable to therapeutic modulation (Chapter 29). For rare disease diagnosis, network-based prioritization integrates with the variant filtering pipelines in ?sec-ch26-prioritization-funnel, where foundation model embeddings and network context jointly inform gene ranking.\n\n\n21.4.2 Drug-Target Interaction Prediction\nIdentifying which proteins a drug binds is fundamental to understanding mechanism and predicting side effects. Experimental screening of drug-target pairs is expensive and incomplete; computational prediction can prioritize candidates for validation.\nDrug-target interaction prediction naturally fits a graph framework. Construct a heterogeneous graph with drug nodes, protein nodes, and edges representing known interactions. Node features for proteins come from sequence foundation models; node features for drugs come from molecular encodings (fingerprints, learned representations from molecular graphs). Train a GNN to predict missing edges, learning which drug and protein features, combined with network context, indicate likely binding.\nThe foundation model integration is critical here. Protein embeddings from ESM capture binding pocket characteristics, domain structure, and evolutionary constraint that influence druggability. The graph structure provides context: if a drug binds protein A, and protein A participates in complex with protein B, then the drug may also affect protein B’s function. Multi-relational GNNs can learn different propagation patterns for different edge types (physical binding versus pathway membership versus sequence similarity), improving prediction accuracy.\nThis application connects to broader drug discovery workflows (Chapter 29), where target identification is one component of a multi-stage pipeline. GNN-based predictions provide hypotheses for experimental validation, accelerating the search for novel therapeutic targets.\n\n\n\n\n\n\nStop and Think\n\n\n\nA pharmaceutical company wants to predict off-target effects of a new kinase inhibitor. They have: - The drug’s binding affinity to 50 kinases (experimentally measured) - A kinase family tree based on sequence similarity - ESM-2 embeddings for all human kinases\nHow would you structure this as a graph learning problem? What would be your nodes, edges, and prediction target? What might the model learn that simple sequence similarity would miss?\n\n\n\n\n21.4.3 Knowledge Graph Reasoning and Drug Repurposing\nDrug repurposing seeks new therapeutic applications for existing compounds, exploiting the observation that drugs often affect multiple targets and pathways beyond their original indication. Knowledge graphs provide a natural framework for repurposing by encoding the relationships through which a drug’s effects might propagate to new disease contexts.\nThe repurposing problem can be framed as link prediction in a heterogeneous graph: given a knowledge graph with drugs, diseases, genes, and pathways as nodes, predict missing drug-treats-disease edges. Unlike direct drug-target prediction, this task requires reasoning across multiple relationship types. A candidate repurposing hypothesis might involve a chain such as: drug D binds protein P1, P1 regulates pathway W, pathway W is dysregulated in disease X, therefore D may treat X. Graph neural networks designed for heterogeneous graphs learn to aggregate evidence across such chains, weighting different metapaths (sequences of edge types) according to their predictive reliability. The drug repurposing applications that exploit this reasoning are detailed in ?sec-ch27-repurposing.\nFoundation model embeddings strengthen knowledge graph reasoning in several ways. For gene and protein nodes, ESM embeddings encode functional properties that influence druggability and pathway membership. For disease nodes, embeddings derived from clinical text or phenotype ontologies capture symptom patterns and comorbidity relationships. For drug nodes, molecular representations from chemical language models or graph neural networks over molecular structure encode binding properties and pharmacokinetics. These rich node features allow the GNN to assess not just whether a path exists but whether the entities along that path have compatible functional characteristics.\nEmpirical results demonstrate the value of this integration. Models combining knowledge graph structure with foundation model embeddings outperform both topology-only approaches (which ignore node semantics) and embedding-only approaches (which ignore relational structure) on standard drug repurposing benchmarks (biomedkg2025?; dreamgnn2025?). The improvement is particularly pronounced for drugs and diseases with sparse direct evidence, where multi-hop reasoning through well-characterized intermediate entities provides the primary signal.\nClinical translation of knowledge graph predictions requires careful interpretation. A high-scoring drug-disease prediction indicates that multiple lines of computational evidence converge, not that efficacy has been established. The paths contributing to predictions provide mechanistic hypotheses that can guide experimental validation: if the model relies heavily on a drug-protein-pathway-disease chain, that pathway becomes a candidate biomarker for patient selection or treatment response monitoring. Several repurposing candidates identified through knowledge graph methods have entered clinical trials, though the approach remains most valuable for hypothesis generation rather than definitive target validation (stebbing2020mechanism?; richardson2020baricitinib?).\n\n\n\n\n\n\n\n\nHeterogeneous knowledge graph with multiple node types\n\n\n\n\n\n\n\nMulti-hop reasoning provides mechanistic hypotheses\n\n\n\n\n\n\n\n\n\nLink prediction scores potential drug-disease associations\n\n\n\n\n\n\n\nFM embeddings enhance node representations\n\n\n\n\n\n\nFigure 21.5: Knowledge graph reasoning for drug repurposing. (A) Heterogeneous knowledge graph structure with multiple node types (drugs, proteins, diseases, pathways) and relationship types. (B) Multi-hop reasoning: a drug might treat a disease through a path like Drug → binds → Protein → pathway → Disease. (C) Link prediction: GNNs learn embeddings to score potential new drug-disease associations. (D) Foundation model enhancement: FM embeddings capture functional similarity that database annotations miss.\n\n\n\n\n\n21.4.4 Pathway and Module Analysis\nIndividual genes rarely act alone; biological function emerges from coordinated activity of gene sets organized into pathways and functional modules. Patient-specific pathway analysis identifies which modules show coordinated dysregulation, providing mechanistic insight beyond single-gene associations.\nGNNs enable pathway analysis that respects network structure rather than treating gene sets as independent members. By propagating patient-specific expression or mutation signals through pathway graphs, models can identify which subnetworks show coherent perturbation. This differs from classical gene set enrichment, which tests for overrepresentation without considering internal pathway topology.\nFoundation model features strengthen pathway analysis by providing functional context for each gene. A gene with features indicating chromatin regulation may contribute to pathway dysfunction through different mechanisms than one with features indicating membrane signaling. The GNN learns to weight these contributions based on network position and functional annotation, identifying pathway perturbations that purely topological or purely gene-set methods miss.\n\n\n21.4.5 Cell Type and State Annotation\nSingle-cell foundation models generate rich representations of individual cells (Chapter 19), but many biological questions involve relationships between cells: which cells communicate, how spatial neighborhoods influence behavior, which cell types co-occur in disease states.\nGraph neural networks over cell-cell interaction graphs enable several applications. Cell type annotation propagates labels from well-characterized cells to ambiguous ones based on expression similarity and spatial proximity. Perturbation response prediction models how signals from perturbed cells propagate to neighbors. Tissue region classification identifies coherent spatial domains (tumor, stroma, immune infiltrate) based on local cell compositions.\nThe foundation model integration follows the standard pattern: scGPT or similar models generate cell embeddings, spatial proximity or inferred ligand-receptor interactions define edges, and GNN message passing refines cell representations based on neighborhood context. The resulting embeddings capture both intrinsic cell state and extrinsic spatial/communicative context, enabling predictions that purely expression-based or purely spatial models cannot make.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch21-networks.html#sec-ch21-practical",
    "href": "part_4/p4-ch21-networks.html#sec-ch21-practical",
    "title": "21  Graph and Network Models",
    "section": "21.5 Practical Considerations",
    "text": "21.5 Practical Considerations\nDeploying graph neural networks on biological data requires navigating choices that profoundly affect model behavior. Graph construction determines what relationships the model can exploit. Scalability strategies determine whether training is feasible on large networks. Robustness techniques determine whether predictions generalize beyond well-characterized network regions. Interpretation methods determine whether outputs provide actionable biological insight. The following subsections address each consideration in turn.\n\n21.5.1 Graph Construction Quality\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nGraph construction involves a fundamental tradeoff: curated databases like BioGRID provide high-confidence interactions but limited coverage, while computational predictions from STRING are comprehensive but noisier. Before reading the discussion, predict: For disease gene prioritization, would you prefer a high-precision network (fewer edges, high confidence) or a high-recall network (more edges, lower confidence)? What would change for a safety-critical application like predicting drug side effects?\n\n\nThe impact of graph construction choices cannot be overstated. A GNN can only learn from relationships encoded in its input graph; missing edges prevent information flow, spurious edges introduce noise, and biased edge sets propagate ascertainment artifacts.\nSource selection involves tradeoffs between precision and completeness. Curated databases like BioGRID provide high-confidence interactions but miss most true relationships. Computational predictions from STRING or co-expression analysis are more comprehensive but noisier. The appropriate choice depends on the downstream task: high-precision networks may be preferable when false positives are costly, while high-recall networks enable discovery of novel biology at the risk of chasing artifacts.\nThresholding decisions determine network density. Confidence scores or distance metrics allow continuous edge weights, but many GNN implementations require discrete edges or work better with relatively sparse graphs. Cross-validation over threshold values or principled selection criteria (target edge density, ensure graph connectivity) help navigate this choice.\nFor heterogeneous graphs, schema design (which node types exist, which edge types connect them) encodes strong assumptions about relevant biology. A knowledge graph that separates genes, transcripts, and proteins as distinct node types enables fine-grained reasoning but requires more training data than a simpler gene-only representation.\n\n\n\n\n\n\nPractical Guidance: Graph Construction Checklist\n\n\n\nBefore training a GNN, verify your graph construction:\n\nEdge source quality: What is the expected false positive rate? False negative rate?\nThreshold selection: Did you validate the confidence threshold on held-out data?\nConnectivity: Are there disconnected components? Isolated nodes?\nDegree distribution: Are there extreme hubs that might dominate message passing?\nTemporal consistency: Were edges assembled from data available at the time of your labels?\nSchema alignment: Do your node/edge types match your biological question?\n\nMistakes in graph construction often matter more than model architecture choices.\n\n\n\n\n21.5.2 Scalability and Mini-Batching\nBiological graphs range from thousands of nodes (a single-patient cell graph) to millions (a comprehensive knowledge graph or large spatial transcriptomics dataset). Full-batch training, where the entire graph is processed simultaneously, becomes infeasible at scale due to memory constraints.\nMini-batching strategies partition computation into manageable pieces. Neighborhood sampling (GraphSAGE-style) restricts message passing to a fixed sample of neighbors per node, enabling node-level mini-batches. Subgraph sampling trains on induced subgraphs corresponding to meaningful units (individual pathways, tissue regions, patient subsets). Cluster-based training partitions the graph into communities, processes each independently, and handles cross-cluster edges in a second pass.\nFor foundation model integration, computational cost compounds: generating embeddings for millions of proteins or cells may itself be expensive. Pre-computing and caching embeddings is often practical, decoupling the foundation model forward pass from GNN training. When embeddings must be computed on-the-fly (for dynamic features or joint fine-tuning), careful batching and gradient checkpointing become essential.\n\n\n21.5.3 Robustness to Noise and Missingness\nAll biological networks contain errors. Experimental methods for detecting interactions have false positive and false negative rates; computational predictions rely on imperfect proxies; even curated databases contain mistakes. GNNs must tolerate this noise to be practically useful.\nRandomly masking edges during training forces the model to avoid relying on any single interaction. This edge dropout improves robustness to missing or incorrect edges and serves as a form of regularization. The mechanism works because dropout during training creates an implicit ensemble: the model must learn to make correct predictions across many different subgraphs, which encourages it to rely on redundant signals rather than any single edge. Similarly, masking node features or entire nodes through node dropout prevents overfitting to well-connected hubs by forcing the model to make predictions even when the most informative hub nodes are unavailable.\nEnsemble methods train multiple GNNs on different network subsamples or with different random initializations, aggregating predictions to reduce variance from network noise. Bayesian GNNs provide uncertainty estimates that flag low-confidence predictions for manual review (Chapter 23).\nEvaluation should explicitly assess robustness by testing on held-out edges, nodes from poorly characterized network regions, or networks constructed from different data sources than training. A model that performs well only on hub genes or well-characterized interactions may fail in precisely the scenarios where computational prediction is most needed (Chapter 12).\n\n\n\n\n\n\nKnowledge Check\n\n\n\nYou have trained a GNN for disease gene prioritization and achieved 0.85 AUC on your test set. Before celebrating, what additional evaluations should you perform to assess whether this performance is meaningful?\nConsider: 1. How would you check if the model is simply learning node degree? 2. How would you assess performance on understudied genes? 3. How would you test generalization to a new disease not in training?\n\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nCompare performance to degree-only baseline - Train a simple model using only node degree as a feature. If your GNN performs only marginally better, it may be learning degree rather than biological mechanism. Additionally, stratify test performance by degree quartiles to check if accuracy is uniform across hub and peripheral genes.\nStratify evaluation by publication count or study bias metrics - Split test genes into well-characterized (many publications) versus understudied (few publications) categories. Compute AUC separately for each group. A model that performs well only on well-studied genes is recapitulating existing knowledge, not discovering new biology.\nTemporal holdout or leave-one-disease-out cross-validation - Train on diseases characterized before year X, test on diseases characterized after. Or use cross-validation where each fold holds out a full disease and all its genes. This tests whether the model learns generalizable disease biology rather than memorizing specific disease-gene pairs.\n\n\n\n\n\n\n21.5.4 Interpretation and Validation\nA key advantage of graph models is interpretability: the graph structure itself provides a scaffold for understanding predictions (Chapter 24). Several techniques extract biological insight from trained GNNs.\nAttention weights in GAT and graph transformer models indicate which neighbors most influenced each node’s prediction. Aggregating attention across predictions can highlight critical edges or subgraphs, suggesting which interactions drive model behavior. For cases where some neighbors matter more than others, this attention weight analysis reveals learned priorities.\nComputing how predictions change with respect to node or edge features identifies which parts of the input most affect outputs. Gradient-based attribution methods such as integrated gradients provide smoother, more reliable attributions than raw gradients.\nSystematically removing edges, masking nodes, or perturbing features and observing prediction changes reveals which graph elements are necessary for specific predictions. This counterfactual analysis can identify model vulnerabilities and generate testable hypotheses about which interactions are essential.\nProjecting learned node representations into two dimensions using Uniform Manifold Approximation and Projection (UMAP) or t-distributed stochastic neighbor embedding (t-SNE) reveals clusters that may correspond to functional categories, cell types, or disease subtypes. Comparing embedding visualizations across conditions identifies network regions that show context-specific changes.\nInterpretation is not an afterthought but a central goal. The most impactful applications are those where GNN predictions generate testable hypotheses about biological mechanism, ultimately validated by experiment. Attention weights highlighting a regulatory edge or gradient attribution implicating a signaling pathway should prompt follow-up experiments, not immediate clinical action.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch21-networks.html#sec-ch21-limitations",
    "href": "part_4/p4-ch21-networks.html#sec-ch21-limitations",
    "title": "21  Graph and Network Models",
    "section": "21.6 Limitations and Open Challenges",
    "text": "21.6 Limitations and Open Challenges\nGraph neural networks inherit the biases and limitations of their input networks. Network incompleteness means critical relationships may be absent. Ascertainment bias means well-studied genes dominate predictions. Correlational structure may not reflect causal mechanisms. These limitations do not invalidate the approach but constrain its appropriate use and interpretation.\n\n21.6.1 Study Bias Problem\nNetwork-based methods inherit the biases of their input networks. Well-studied genes appear as hubs; poorly characterized genes are peripheral or disconnected. GNNs trained on such networks learn to propagate signals toward well-characterized genes, effectively recapitulating rather than extending existing knowledge.\nThis creates particular problems for disease gene discovery, where the goal is often to identify previously unrecognized genes. A model that consistently ranks known disease genes highly may simply be exploiting their network prominence rather than learning generalizable disease biology. Careful evaluation on temporal holdouts (genes characterized after training data was assembled) or stratified by network degree can reveal whether models truly generalize (Chapter 12). The systematic approaches for detecting and quantifying such confounding patterns are detailed in ?sec-ch22-detection.\nMitigation strategies include degree-corrected training objectives, explicit modeling of ascertainment bias, or alternative network constructions that reduce dependence on historical research focus. None fully solves the problem, which reflects fundamental data limitations rather than algorithmic shortcomings.\n\n\n\n\n\n\nStudy bias: publication count correlates with network degree\n\n\n\n\nFigure 21.6: Study bias in biological networks. Strong correlation between publication count and network degree means well-characterized genes (TP53, BRCA1, EGFR) appear as highly connected hubs while understudied genes are peripheral. This ascertainment bias causes GNNs to preferentially propagate signal toward already-known genes. Mitigation strategies include degree normalization, edge confidence weighting, and temporal holdout evaluation.\n\n\n\n\n\n21.6.2 Causality Versus Association\nNetwork edges typically represent associations (two proteins bind, two genes correlate) rather than causal relationships (perturbing gene A changes gene B). GNNs learn to exploit correlational patterns, which may not correspond to causal mechanisms.\n\n\n\n\n\n\nCritical Limitation\n\n\n\nFor applications like drug target identification, the causality limitation matters enormously. A gene that correlates with disease through confounding may be a poor target despite high network-based prioritization scores. A GNN might learn that genes in the “inflammation” module are associated with autoimmune disease, but this does not mean that targeting any gene in that module will be therapeutic. Experimental validation remains essential before clinical translation.\n\n\nIntegrating causal inference methods with graph learning is an active research area, but current GNN applications should be interpreted as identifying associations worthy of experimental follow-up rather than establishing causal relationships.\n\n\n21.6.3 Negative Data and Class Imbalance\nMost biological network datasets encode only positive relationships: known interactions, confirmed regulatory edges, documented associations. The absence of an edge may indicate true non-interaction or simply lack of evidence. This creates severe class imbalance for edge prediction tasks and makes negative sampling strategies critical (Chapter 12).\nRandom negative sampling (assuming absent edges represent non-interactions) is common but biologically unrealistic. More sophisticated approaches sample negatives with matched properties (same degree distribution, similar node features) to create harder and more meaningful contrasts. Evaluation should report performance separately on different negative sampling schemes to assess whether models generalize beyond easily discriminated negatives (Chapter 11).\n\n\n21.6.4 Distribution Shift\nA GNN trained on one biological network (human PPI from STRING) may not transfer to another (mouse regulatory network, patient-specific spatial graph). Foundation model embeddings help by providing transferable features, but network structure differences can still break performance.\nApplying models across species is particularly challenging: network topology, edge type distributions, and gene function may all differ between organisms. Cross-tissue or cross-disease transfer poses similar challenges. Explicit domain adaptation methods, multi-task training across related networks, or foundation model fine-tuning on target domains can help but add complexity (Chapter 9).",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch21-networks.html#sec-ch21-conclusion",
    "href": "part_4/p4-ch21-networks.html#sec-ch21-conclusion",
    "title": "21  Graph and Network Models",
    "section": "21.7 Sequence Encodes, Structure Connects",
    "text": "21.7 Sequence Encodes, Structure Connects\nGraph neural networks operate at a complementary level of abstraction to sequence-based foundation models. Foundation models learn rich representations of biological entities from sequence data; graph neural networks learn to reason about relationships between those entities. Combining them follows a natural pattern: generate embeddings with foundation models, then refine them through message passing over graph structure. This integration yields capabilities that neither component achieves alone, propagating information across protein interaction networks, regulatory pathways, and spatial neighborhoods in ways that sequence models cannot represent.\nThe central insight is that biological knowledge exists at multiple scales. Sequence encodes what individual genes and proteins can do; networks encode how they interact to produce cellular function. GNNs translate the relational structure of biological networks into learnable inductive biases, enabling disease gene prioritization through network propagation, drug target prediction through pathway context, and spatial analysis through tissue graphs. The improvements over sequence-only approaches are consistent across applications, demonstrating that relational context adds genuine information beyond what sequence representations capture.\nYet network structure carries its own biases. Protein interaction databases are enriched for well-studied genes and disease-relevant pathways; less-characterized genes have fewer annotated interactions regardless of their biological importance. Correlation between genes does not imply regulatory relationship. Class imbalance between known disease genes and the genome-wide background reflects research history as much as biology. These biases propagate through GNN predictions, creating systematic patterns in what the models emphasize and what they miss. The multi-omics integration examined in Chapter 22 extends graph-based reasoning to additional modalities. Clinical applications in Chapter 27 leverage network-derived features for risk stratification, while Chapter 28 applies network propagation to gene prioritization workflows. Both depend on understanding where network-derived predictions are trustworthy and where they inherit the limitations of their inputs, challenges that the uncertainty quantification methods in Section 23.4 help address.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nExplain the relationship between foundation models and graph neural networks. Do GNNs replace foundation models or complement them?\nWhat is the over-smoothing problem in GNNs, and how does it limit network depth?\nWhy do protein interaction networks inherit study bias, and how does this affect disease gene predictions?\nDescribe how message passing works in GNNs and what happens with each additional layer.\nWhat is the fundamental difference between edges representing association versus causation in biological networks?\n\n\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nGNNs complement foundation models, not replace them - Foundation models extract rich representations from sequence (what a protein can do), while GNNs add relational reasoning (what it does in network context). The division of labor: FMs provide node features, GNNs perform message passing to integrate neighborhood information.\nOver-smoothing occurs when repeated averaging converges node representations - Each GNN layer averages neighbor representations. After many layers, all nodes in a connected component converge toward similar embeddings, losing discriminative power. This is why most practical GNNs use only 2-4 layers rather than the deep stacking common in sequence models.\nWell-studied genes have more documented interactions due to research focus - Cancer genes, disease genes, and conserved pathway members attract experimental attention. This ascertainment bias means GNNs trained on such networks learn to propagate signals toward already-known genes, potentially missing novel biology in peripheral regions.\nMessage passing iteratively updates node representations based on neighbors - At each layer: (1) compute messages from neighbors, (2) aggregate messages using permutation-invariant operations (sum/mean/max/attention), (3) update node representation by combining aggregated messages with current state. After L layers, each node has incorporated information from its L-hop neighborhood.\nAssociation means correlation; causation means perturbation effects - An edge in a PPI network indicates proteins bind (association) but not that perturbing one changes the other (causation). Co-expression correlations may reflect shared regulation rather than direct regulatory relationship. This matters for drug targets: correlation doesn’t guarantee that modulating a gene will have the desired therapeutic effect.\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nCore Concepts:\n\nGraph neural networks consume foundation model embeddings, not replace them. FMs provide rich node features; GNNs add relational reasoning.\nMessage passing iteratively updates node representations based on neighbor information, with each layer extending the receptive field by one hop.\nOver-smoothing limits GNN depth; most practical models use 2-4 layers.\n\nArchitecture Choices:\n\n\n\nNeed\nChoose\n\n\n\n\nSimple baseline\nGCN with frozen FM embeddings\n\n\nLarge graphs\nGraphSAGE with neighborhood sampling\n\n\nVariable edge importance\nGAT for attention-weighted aggregation\n\n\nLong-range dependencies\nGraph transformers (but higher compute)\n\n\n\nKey Applications:\n\nDisease gene prioritization: Propagate disease labels through PPI networks\nDrug-target prediction: Heterogeneous graphs with drug and protein nodes\nDrug repurposing: Multi-hop reasoning through knowledge graphs\nSpatial analysis: Cell graphs with FM embeddings as node features\n\nCritical Limitations to Remember:\n\nNetworks inherit study bias (well-studied genes are over-connected)\nEdges represent association, not causation\nMissing edges block information flow\nPerformance on hub genes may not generalize to understudied genes\n\nConnection to Later Chapters:\n\nChapter 22: Extending GNNs to multi-modal data\nChapter 23: Uncertainty quantification for network predictions\nChapter 24: Interpreting what GNNs learn\nChapter 27: Clinical applications of network features\nChapter 28: Network-based gene prioritization for diagnosis",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Graph and Network Models</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch22-multi-omics.html",
    "href": "part_4/p4-ch22-multi-omics.html",
    "title": "22  Multi-Omics Integration",
    "section": "",
    "text": "22.1 Limits of Single-Modality Models\nEach molecular layer tells an incomplete story. DNA sequence is static; it encodes potential but not state. A variant’s presence says nothing about whether the gene is expressed, whether the protein is active, or whether the pathway is perturbed. Transcriptomic data captures expression state but misses post-transcriptional regulation, protein modifications, and metabolic flux. Proteomic measurements reveal protein abundance but not necessarily activity or localization. Methylation profiles indicate epigenetic state but require expression data to understand functional consequences.\nThe incompleteness becomes concrete when modeling complex traits. Genome-wide association studies explain perhaps 10-20% of heritability for most common diseases through identified variants [Citation Needed]. Adding expression quantitative trait loci (eQTLs) improves fine-mapping by suggesting which variants affect gene expression (see Section 3.4), but many causal mechanisms operate through splicing, translation, or post-translational modification rather than expression level. Single-cell RNA sequencing reveals cellular heterogeneity invisible to bulk measurements, but the same cell cannot simultaneously undergo RNA-seq and assay for transposase-accessible chromatin sequencing (ATAC-seq), forcing computational integration across modalities measured in different cells (see Chapter 19 for approaches to this challenge).\nConsider the challenge of predicting drug response. Germline variants in drug-metabolizing enzymes explain some inter-individual variation (see Section 2.8.4), but tumor-specific somatic mutations, expression programs, and microenvironment all influence therapeutic efficacy. A genomics-only model sees the inherited component; a transcriptomics-only model sees the current expression state; neither captures the full picture. Multi-omics integration promises to bridge these gaps by learning representations that span molecular layers.\nFoundation models address each molecular layer individually: sequence models predict regulatory effects from DNA (see Chapter 16), expression models capture transcriptional programs (see Chapter 18), and protein language models predict structure and function from amino acid sequence (see Chapter 15). Multi-omics integration asks how these modality-specific representations can be combined into unified patient or cell representations.\nThe promise comes with caveats. Adding modalities increases the number of parameters that must be estimated, potentially worsening overfitting when sample sizes are limited. Different modalities have different noise characteristics, batch structures, and missingness patterns. The same patient’s measurements across platforms may not align perfectly due to sample handling, timing, or technical variation. Naive concatenation of features often performs worse than single-modality models because the signal-to-noise ratio degrades when noisy features outnumber informative ones.\nThese challenges motivate careful consideration of integration strategy. The question is not whether to integrate, but how.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch22-multi-omics.html#sec-ch22-limits",
    "href": "part_4/p4-ch22-multi-omics.html#sec-ch22-limits",
    "title": "22  Multi-Omics Integration",
    "section": "",
    "text": "Stop and Think\n\n\n\nBefore reading further, consider a patient with type 2 diabetes. Which molecular layers would you expect to provide independent information about their disease state? Would genomic variants, gene expression, protein levels, and metabolite concentrations all be equally informative, or would some layers be redundant with others?",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch22-multi-omics.html#sec-ch22-strategies",
    "href": "part_4/p4-ch22-multi-omics.html#sec-ch22-strategies",
    "title": "22  Multi-Omics Integration",
    "section": "22.2 Integration Strategies and Their Tradeoffs",
    "text": "22.2 Integration Strategies and Their Tradeoffs\nThree broad strategies have emerged for combining multi-omics data, each with distinct strengths and limitations.\n\n\n\n\n\n\n\n\nEarly fusion: concatenate features before modeling\n\n\n\n\n\n\n\nLate fusion: combine model predictions\n\n\n\n\n\n\n\nIntermediate fusion: shared latent space\n\n\n\n\n\n\nFigure 22.2: Multi-omics fusion strategies. (A) Early fusion: concatenate features before modeling. Enables cross-modal interaction learning but requires complete data and suffers from dimensionality. (B) Late fusion: train separate models and combine predictions. Handles missing modalities but cannot learn feature interactions. (C) Intermediate fusion: modality-specific encoders project to shared latent space. Balances interaction learning with missing data robustness.\n\n\n\n\n22.2.1 Early Fusion\nIf you concatenate genomic, transcriptomic, and epigenomic data into one giant feature vector, you are betting that their interactions matter from the very first layer. When does this bet pay off, and when does it fail?\nEarly fusion concatenates features from multiple modalities before any modeling, creating a single high-dimensional input vector that contains genomic variants, expression values, methylation levels, and any other available measurements. A classifier or regressor then learns directly from this concatenated representation.\nThe appeal of early fusion lies in its simplicity and flexibility. Any downstream model architecture can operate on concatenated features, from linear regression to deep neural networks. The model can learn arbitrary interactions between features from different modalities, since all information is present in the input. Implementation requires only normalization and alignment of features across samples.\nThe limitations become apparent at scale. Dimensionality explodes when combining genome-wide variants (millions of features), gene expression (tens of thousands of genes), methylation (hundreds of thousands of CpG sites), and protein abundance (thousands of proteins). Most samples have far fewer observations than features, creating severe overfitting risk. Why does high dimensionality cause overfitting even with regularization? Consider the geometry: with p features and n samples (p &gt;&gt; n), the training points lie in a low-dimensional subspace of the feature space. The model can perfectly fit training data by assigning any prediction to training points and interpolating arbitrarily between them—countless solutions achieve zero training error, and most generalize poorly. Regularization constrains the solution space but cannot eliminate the fundamental problem that vastly more parameters exist than constraints. When each modality adds millions of features while sample counts remain in the thousands, the curse of dimensionality overwhelms any regularization scheme.\nMissing data creates additional complications. If any modality is missing for a sample, early fusion requires either excluding that sample (reducing effective sample size) or imputing the missing modality (introducing noise and potential bias). Since multi-omics studies often have incomplete overlap between modalities, with some patients having genomics and transcriptomics but not proteomics, early fusion frequently operates on substantially reduced cohorts.\nScale differences between modalities pose another challenge. Expression values span orders of magnitude; methylation beta values range from zero to one; variant encodings are typically binary. Without careful normalization, modalities with larger variance can dominate the learned representation regardless of biological relevance. Batch effects within each modality add further complexity, since batch correction must precede concatenation but may interact with cross-modal relationships.\nDespite these limitations, early fusion remains appropriate when sample sizes are large relative to feature counts, when all modalities are available for all samples, and when the downstream task is well-defined enough to guide feature selection. Biobank-scale studies with thousands of participants and focused feature sets can succeed with early fusion approaches.\n\n\n22.2.2 Late Fusion\nWhat if each modality truly provides independent information, like separate witnesses to the same event? Late fusion bets that combining final verdicts outperforms forcing witnesses to deliberate together from the start. This independence assumption buys you robustness to missing data but costs you the ability to detect when witnesses are describing the same thing from different angles.\nLate fusion trains separate models for each modality and combines their predictions at the output level. A genomics model produces a risk score; a transcriptomics model produces another risk score; an ensemble method or meta-learner combines these modality-specific predictions into a final output.\nThis approach handles missing modalities gracefully. If a patient lacks proteomic data, the proteomics model simply does not contribute to the ensemble. Sample sizes for each modality-specific model can differ, since training requires only samples with that modality rather than complete multi-omics profiles. Each modality can use whatever architecture works best for its data type: deep networks for imaging, gradient boosting for tabular omics, convolutional architectures for sequence.\nLate fusion cannot capture cross-modal interactions at the feature level. If a variant’s effect on disease depends on expression level of a regulatory gene, neither the genomics model nor the transcriptomics model alone can detect this interaction. The ensemble sees only the modality-specific predictions, not the underlying features. This limitation is fundamental: late fusion assumes that each modality provides independent signal that can be additively combined. Mathematically, if the true risk depends on an interaction term like \\(\\text{variant} \\times \\text{expression}\\), late fusion can only approximate this with \\(f_1(\\text{variant}) + f_2(\\text{expression})\\), which cannot capture the non-additive structure regardless of how sophisticated the individual models become.\nThe assumption of independence often fails in biological systems. Gene expression depends on genetic variants through eQTLs. Protein levels depend on both transcription and post-transcriptional regulation. Methylation states influence and are influenced by transcription. The molecular layers are not independent information sources but coupled components of a dynamic system. Late fusion ignores this coupling.\nCalibration presents a practical challenge. For ensemble predictions to be meaningful, the modality-specific models must produce well-calibrated probability estimates (see Section 23.2 for calibration methods). If the genomics model is overconfident and the transcriptomics model is underconfident, naive averaging produces biased predictions. Calibration techniques help but add complexity to the modeling pipeline.\nLate fusion works well when modalities genuinely provide independent signals, when sample sizes for each modality differ substantially, or when interpretability requires understanding each modality’s contribution separately. Clinical deployment often favors late fusion because it gracefully handles the reality that not all patients will have all measurements.\n\n\n22.2.3 Intermediate Fusion\nEarly fusion demands complete data and cannot scale. Late fusion ignores cross-modal interactions entirely. Is there a middle path that learns how modalities relate to each other while gracefully handling the messy reality of incomplete measurements?\n\n\n\n\n\n\nStop and Think\n\n\n\nYou are designing a multi-omics model that must work even when patients are missing some data types. Early fusion requires all modalities; late fusion cannot capture cross-modal interactions. What properties would an ideal intermediate approach have? How might you encode different modalities into a common representation while preserving modality-specific structure?\n\n\nIntermediate fusion learns modality-specific encoders that map each data type into a shared latent space, then operates on the aligned representations for downstream tasks. This approach combines the flexibility of early fusion with the robustness of late fusion.\nEach modality has its own encoder architecture tailored to its characteristics. A variational autoencoder might encode single-cell expression data, handling sparsity and dropout noise. A convolutional network might process methylation profiles along chromosomal coordinates. A graph neural network might encode protein interaction data (see ?sec-ch18-gnn-fundamentals). These diverse architectures share nothing except their output dimensionality: all encoders produce embeddings in a common latent space.\nAlignment between modalities is encouraged through multiple mechanisms. Reconstruction losses require each encoder’s latent representation to support decoding back to the original features, ensuring that the embeddings retain modality-specific information. Contrastive terms pull together representations of the same biological entity across modalities: the expression embedding for a cell should be similar to the ATAC-seq embedding for the same cell. Graph constraints enforce consistency with known biological relationships: genes connected in interaction networks should have similar embeddings.\n\n\n\n\n\n\nKey Insight\n\n\n\nThe power of intermediate fusion lies in the shared latent space. By forcing different modalities to project into the same embedding space, the model learns that expression patterns and chromatin accessibility in the same cell should map to nearby points. This alignment enables cross-modal reasoning: a classifier operating on this shared space can effectively learn from both modalities simultaneously, even when predicting for samples with only one modality available.\n\n\nThe shared latent space enables cross-modal reasoning. A classifier operating on the shared space can learn interactions between genomic and transcriptomic features, since both are present in the same representation. Transfer becomes possible: a model trained on expression data can be applied to samples with only ATAC-seq by encoding through the ATAC-seq encoder into the shared space.\nMissing modalities no longer require imputation or exclusion. If a sample lacks proteomics, only the available encoders fire, producing a partial representation in the shared space. The downstream model operates on whatever representation is available, degrading gracefully as modalities are missing rather than failing entirely.\nGLUE, introduced in ?sec-ch16-glue for single-cell multi-omics integration, exemplifies this approach. Separate variational autoencoders encode RNA-seq and ATAC-seq data into a shared cell embedding space. A feature graph links ATAC-seq peaks to genes based on genomic proximity and transcription factor binding, providing biological constraints on the alignment. The result enables integration of measurements from different cells, not just different modalities in the same cell.\nIntermediate fusion dominates modern multi-omics deep learning because it balances flexibility with robustness. The modality-specific encoders can be pretrained on large single-modality datasets, then fine-tuned for alignment (see Chapter 9 for transfer learning strategies). New modalities can be added by training new encoders without retraining existing components. The shared space provides a natural target for interpretation and visualization.\nThe approach is not without limitations. The quality of alignment depends heavily on the training objective and the availability of paired samples where multiple modalities are measured in the same biological entity. Without sufficient anchoring, the shared space may fail to capture true biological correspondence. Hyperparameter choices for balancing reconstruction against alignment losses require careful tuning.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore viewing the comparison table below, make a prediction: Which fusion strategy (early, late, or intermediate) do you think would handle missing modalities best? Which would be most computationally expensive? Which would be best for learning interactions between genomic variants and gene expression levels? Write down your predictions, then check them against the table.\n\n\n\n\n\nTable 22.1: Comparison of multi-omics fusion strategies. The choice depends on data characteristics, sample sizes, and downstream application requirements.\n\n\n\n\n\n\n\n\n\n\n\n\nStrategy\nCross-Modal Interactions\nMissing Data Handling\nComputational Cost\nBest When\n\n\n\n\nEarly Fusion\nCan learn arbitrary interactions\nPoor: requires complete data\nLow to moderate\nLarge samples, complete data, focused features\n\n\nLate Fusion\nNone: predictions combined only\nExcellent: uses available modalities\nLow: independent models\nIndependent signals, variable coverage, interpretability needed\n\n\nIntermediate Fusion\nLearns in shared space\nGood: graceful degradation\nHigh: alignment training\nCoupled modalities, transfer learning, paired training data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModality-specific encoders tailored to each data type\n\n\n\n\n\n\n\nShared latent space with alignment losses\n\n\n\n\n\n\n\n\n\nDownstream task heads operate on shared representation\n\n\n\n\n\n\n\nMissing modalities enable graceful degradation\n\n\n\n\n\n\nFigure 22.3: Intermediate fusion architecture in detail. (A) Modality-specific encoders tailored to each data type: VAE for expression handling sparsity, CNN for methylation along genomic coordinates, MLP for proteomics. (B) Shared latent space: encoders produce embeddings aligned through reconstruction loss, contrastive terms, and biological graph constraints. (C) Downstream task heads operate on the shared representation, enabling cross-modal reasoning. (D) Missing modality handling: partial samples use only available encoders, enabling graceful degradation rather than sample exclusion.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch22-multi-omics.html#sec-ch22-foundation-models",
    "href": "part_4/p4-ch22-multi-omics.html#sec-ch22-foundation-models",
    "title": "22  Multi-Omics Integration",
    "section": "22.3 Multi-Omics Foundation Models",
    "text": "22.3 Multi-Omics Foundation Models\nThe foundation model paradigm, introduced in Chapter 13, extends naturally to multi-omics settings. Rather than training task-specific models that integrate modalities for a single downstream application, multi-omics foundation models learn general-purpose representations that transfer across tasks.\n\n22.3.1 Factor-Based Integration\nBefore diving into deep learning, consider a simpler question: can we explain the variation across modalities with a small number of shared factors? If inflammation drives coordinated changes in both gene expression and DNA methylation, a single factor might capture both effects. Factor-based methods test whether multi-omics complexity reduces to interpretable dimensions.\nMulti-Omics Factor Analysis (MOFA and its successor MOFA+) provides a probabilistic framework for learning shared and modality-specific factors from multi-omics data [Citation Needed]. The approach assumes that observed measurements across modalities can be explained by a small number of latent factors, some shared across modalities and others specific to individual data types.\nMOFA+ extends this framework to handle multiple sample groups (such as different tissues or conditions), non-Gaussian likelihoods appropriate for count data, and scalable inference for large datasets [Citation Needed]. The factors learned by MOFA+ capture sources of variation that span modalities, enabling biological interpretation: a factor that loads heavily on inflammatory genes in expression data and on hypomethylation at immune loci in methylation data suggests coordinated epigenetic-transcriptional regulation of inflammation.\nWhile MOFA+ is not a deep learning method in the strict sense, its factor-based decomposition provides a foundation for understanding what multi-omics integration should capture. The shared factors correspond to biological processes that manifest across molecular layers; the modality-specific factors capture technical variation or layer-specific biology.\n\n\n22.3.2 Deep Generative Multi-Omics Models\nRNA and protein measurements have fundamentally different noise characteristics: RNA suffers from dropout, proteins from background binding. Should a model treat them identically, or explicitly account for how each modality was generated? Deep generative approaches choose the latter, building the measurement process into the model itself.\ntotalVI (Total Variational Inference) integrates protein abundance from CITE-seq with gene expression in single-cell data through a hierarchical Bayesian model [Citation Needed]. The approach learns a joint latent space that captures cell state while properly modeling the distinct noise characteristics of RNA and protein measurements. Protein abundance follows a negative binomial distribution with technical factors including background binding; RNA counts follow a zero-inflated negative binomial accounting for dropout. The choice of these specific likelihood functions reflects the data-generating process: RNA-seq suffers from technical dropout where some transcripts fail to be captured despite being present (hence zero-inflation), while protein measurements from antibody-based methods have background binding noise but less zero-inflation. Using the wrong likelihood would force the model to distort its latent space to accommodate systematic errors, degrading biological interpretability.\nThe generative model structure enables imputation of missing modalities. Given RNA expression alone, totalVI can predict expected protein abundance by sampling from the learned joint distribution. This imputation is not mere correlation-based prediction but reflects the full posterior distribution over protein levels given expression.\nMultiVI extends this framework to integrate gene expression with chromatin accessibility [Citation Needed]. The model learns to align measurements from different cells, enabling construction of unified cell atlases from studies that measured different modalities. The alignment relies on the biological assumption that gene expression and chromatin state reflect the same underlying cell state, even when measured in different cells.\nThese Bayesian deep generative models exemplify intermediate fusion with principled uncertainty quantification. The posterior distributions over latent variables capture not just point estimates but confidence in the learned representations (see Chapter 23 for uncertainty quantification methods). This property becomes important for clinical applications where prediction uncertainty must inform decision-making.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nConsider totalVI integrating RNA and protein measurements from single-cell CITE-seq data. The model learns a joint latent space where cells map based on both modalities.\n\nWhy does totalVI use different likelihood functions for RNA (zero-inflated negative binomial) versus protein (negative binomial)?\nIf you had a new sample with only RNA measurements, how would the model generate a protein abundance prediction?\nWhat advantage does the Bayesian approach provide over simply training a regression from RNA to protein?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nRNA-seq suffers from technical dropout where transcripts fail to be captured despite being present (requiring zero-inflation), while protein measurements from antibody-based methods have background binding noise but less zero-inflation. Using the correct likelihood prevents the model from distorting its latent space to accommodate systematic errors. 2. The model maps the RNA measurements into the shared latent space, then samples from the learned posterior distribution over protein levels conditioned on that latent representation. 3. The Bayesian approach provides full posterior distributions capturing uncertainty in predictions, not just point estimates—critical for clinical decisions where knowing confidence matters as much as the prediction itself.\n\n\n\n\n\n\n\n\n22.3.3 Contrastive Multi-Modal Learning\nWhat if you do not need to model the full data-generating process, but only need to learn that a cell’s expression profile and its methylation profile should map to nearby points? Contrastive learning takes this shortcut: instead of reconstructing measurements, it simply learns to recognize which observations came from the same biological entity across modalities.\nContrastive learning provides another path to multi-omics integration. The CLIP model for vision-language demonstrated that contrastive objectives can align embeddings from fundamentally different data types (images and text) into a shared space [Citation Needed]. Similar approaches apply to biological modalities.\nThe contrastive objective is straightforward: embeddings of the same biological entity across modalities should be similar, while embeddings of different entities should be dissimilar. A cell’s expression embedding should be close to its methylation embedding and far from other cells’ methylation embeddings. A patient’s genomic embedding should be close to their transcriptomic embedding across the cohort.\nWhy does this objective produce biologically meaningful alignment rather than arbitrary correspondence? The key is that biological state is the common cause underlying both modalities. A cell’s expression profile and its methylation profile are both consequences of the same underlying regulatory state—active enhancers, bound transcription factors, chromatin accessibility. By forcing the encoders to map the same cell to similar embeddings across modalities, the contrastive loss encourages representations that capture this shared biological state rather than modality-specific artifacts. Features that vary randomly between modalities (technical noise, batch effects) cannot satisfy the objective; only features reflecting genuine cellular identity survive the alignment pressure.\nThis objective requires paired samples for training: the same cells or patients measured across modalities. Anchor pairs define the positive examples; negative examples come from non-matching pairs within a batch. The encoders learn to produce embeddings where cross-modal correspondence emerges from training dynamics rather than explicit feature engineering.\nContrastive approaches scale well and can incorporate foundation model encoders pretrained on single modalities. An expression encoder pretrained on millions of cells via masked gene prediction can be fine-tuned with contrastive objectives to align with an ATAC-seq encoder. The pretraining provides rich initial representations; the contrastive fine-tuning establishes cross-modal correspondence (see Section 8.5 for contrastive pretraining strategies).",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch22-multi-omics.html#sec-ch22-clinical-integration",
    "href": "part_4/p4-ch22-multi-omics.html#sec-ch22-clinical-integration",
    "title": "22  Multi-Omics Integration",
    "section": "22.4 Clinical Integration: EHR, Imaging, and Molecular Data",
    "text": "22.4 Clinical Integration: EHR, Imaging, and Molecular Data\nThe ultimate goal of multi-omics modeling for many applications is patient-level prediction: disease risk, treatment response, prognosis. Achieving this goal requires integrating molecular measurements with clinical data that directly captures patient state and outcomes.\n\n22.4.1 Electronic Health Records as a Modality\nMolecular measurements capture mechanism; clinical records capture manifestation. A patient’s genomic risk score tells you their inherited predisposition, but their ten-year history of diagnoses, medications, and lab values tells you what actually happened. When should you trust the molecular signal, and when does the clinical trajectory override it?\nElectronic health records contain decades of longitudinal observations for millions of patients: diagnoses, procedures, medications, laboratory values, vital signs, clinical notes. This wealth of phenotypic information complements molecular data by capturing disease manifestation rather than molecular mechanism.\nIntegrating EHR with genomics poses distinct challenges. The data types differ fundamentally: structured codes, continuous lab values, free-text notes, and time-stamped events versus static or slowly-changing molecular measurements. Temporal structure matters: the sequence of diagnoses and treatments contains prognostic information that static snapshots miss. Missingness is informative: the absence of a laboratory test may indicate that a clinician deemed it unnecessary, which itself conveys information about patient state (Section 2.7.2). The phenotype quality challenges introduced there cascade through multi-omics integration, where EHR-derived labels may introduce systematic biases that ?sec-ch22-label-bias examines in detail.\nFoundation models for EHR data learn representations from the longitudinal event sequences. These models, often based on transformer architectures that process sequences of medical codes (see Chapter 7), capture temporal dependencies and co-occurrence patterns in clinical trajectories. The resulting patient embeddings encode disease state and prognosis in a form amenable to integration with molecular data.\n\n\n\n\n\n\nKey Insight\n\n\n\nCombining EHR embeddings with genomic features requires handling different temporal scales. Genetic variants are constant throughout life; EHR observations accumulate over years. The integration must determine which clinical observations are relevant to a given molecular measurement, accounting for the time between sample collection and clinical events. A patient’s genomic risk for a disease does not change, but their clinical trajectory unfolds over time, and the relevance of past clinical events depends on when the molecular sample was collected.\n\n\n\n\n22.4.2 Imaging Integration\nMolecular assays homogenize tissue into measurements that lose spatial context. Imaging preserves that context: where is the tumor, how heterogeneous is it, what does the tissue architecture reveal? The tradeoff is clear, but how do you align a three-dimensional scan with a bulk RNA-seq measurement that averaged over a small biopsy region?\nMedical imaging provides spatial information that molecular assays lack. A CT scan reveals tumor location, size, and heterogeneity; histopathology slides show cellular morphology and tissue architecture; MRI captures organ structure and function. These spatial data complement molecular measurements that aggregate over dissected tissue regions.\nRadiogenomics links imaging features to genetic and molecular characteristics. Glioblastoma tumors with specific imaging signatures have distinct methylation patterns and expression programs [Citation Needed]. Radiomic features extracted from CT scans correlate with mutational burden and immune infiltration in lung cancer [Citation Needed]. These associations enable prediction of molecular state from non-invasive imaging, potentially guiding treatment decisions when biopsy is impractical.\nFoundation models for medical imaging, pretrained on millions of scans through self-supervised objectives, provide rich representations for downstream tasks [Citation Needed]. Integrating these imaging embeddings with molecular data follows the intermediate fusion paradigm: modality-specific encoders produce representations in a shared latent space where multi-modal classifiers operate.\nThe integration must account for correspondence between imaging regions and molecular samples. A tumor may be molecularly heterogeneous, with different subclones in different spatial locations. A biopsy samples one location; imaging captures the entire lesion. Alignment requires either spatial registration of biopsy location to imaging coordinates or acceptance that the correspondence is imperfect.\n\n\n22.4.3 Multi-Modal Clinical Prediction Models\nIn clinical practice, patients arrive with whatever data they have: complete molecular workups for some, imaging only for others, extensive EHR histories for many. A practical clinical model cannot demand all modalities. How do you build a unified prediction system that improves with more data but still works with less?\nCombining EHR, imaging, and molecular data for clinical prediction follows the intermediate fusion pattern. Each data type has a specialized encoder: a transformer for longitudinal EHR events, a vision encoder for imaging, domain-specific encoders for expression, methylation, and other molecular modalities. All encoders produce embeddings in a common patient representation space.\nThe training objective typically combines modality-specific reconstruction losses with alignment terms that encourage consistency across data types. A patient’s EHR embedding should be predictive of their molecular state; their imaging embedding should be consistent with their clinical trajectory. Downstream classifiers for outcomes like survival, treatment response, or disease progression operate on the combined representation.\nMissing modalities are common in clinical settings. Not all patients have genomic data; imaging may be unavailable for some conditions; the depth of EHR history varies by healthcare system and patient engagement. Multi-modal clinical models must handle this missingness gracefully, producing useful predictions from whatever data are available while leveraging cross-modal information when present.\nThe clinical deployment path for such models requires validation on external cohorts, prospective evaluation, and regulatory clearance. These practical considerations, addressed in Chapter 27, shape model development from the outset. A model that performs well on a research cohort but requires modalities unavailable in clinical workflows provides little value. The practical deployment considerations, including feature availability and model calibration requirements, are examined in Section 27.3.\n\n\n\n\n\n\nPractical Guidance\n\n\n\nWhen designing clinical multi-modal models, start from deployment constraints:\n\nIdentify available modalities: Which data types will be available for the target patient population? Not all patients in a research cohort have all modalities available in routine clinical care.\nDesign for graceful degradation: Choose intermediate fusion architectures that can produce useful predictions even with incomplete data.\nValidate across missingness patterns: Test performance not just on complete cases but on realistic subsets reflecting clinical data availability.\nConsider timing: When is each modality available relative to the clinical decision point? A model requiring data not yet collected provides no clinical value.\n\n\n\n\n\n\n\n\n\n\n\nData modalities: longitudinal EHR, spatial imaging, static molecular\n\n\n\n\n\n\n\nModality-specific encoders for each data type\n\n\n\n\n\n\n\n\n\nPatient representation space unifies all modalities\n\n\n\n\n\n\n\nClinical prediction tasks with missing modality handling\n\n\n\n\n\n\nFigure 22.4: Clinical multi-modal integration. (A) Data modalities: longitudinal EHR (diagnoses, procedures, labs), spatial imaging (CT, MRI, histopathology), and static molecular measurements (genomics, expression, proteomics). (B) Modality-specific encoders: EHR transformer for event sequences, vision encoder for imaging, foundation model embeddings for molecular data. (C) Patient representation space: unified embedding where EHR predicts molecular state and imaging is consistent with clinical trajectory. (D) Clinical prediction tasks: risk stratification, treatment response, and prognosis—with the practical requirement of handling incomplete data.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch22-multi-omics.html#sec-ch22-systems-view",
    "href": "part_4/p4-ch22-multi-omics.html#sec-ch22-systems-view",
    "title": "22  Multi-Omics Integration",
    "section": "22.5 Systems View: From Variant to Phenotype",
    "text": "22.5 Systems View: From Variant to Phenotype\nMulti-omics integration gains conceptual clarity from a systems biology perspective that traces information flow from genetic variation through molecular intermediates to clinical phenotypes. This cascade view organizes the molecular layers into a causal hierarchy and identifies where integration should occur.\n\n22.5.1 Information Cascade\nGenetic variants are the starting point: heritable differences in DNA sequence that perturb downstream molecular processes. Some variants directly alter protein structure through missense or nonsense mutations. Others affect regulation: promoter variants change expression level, splice site variants alter transcript isoforms, enhancer variants modulate tissue-specific expression.\nThese primary effects propagate through molecular layers. Expression changes alter the cellular protein complement. Protein level changes affect enzyme activity, signaling cascades, and transcriptional feedback. Metabolic flux shifts in response to enzyme availability. Cell behavior changes as the integrated molecular state crosses thresholds for proliferation, differentiation, or death.\nTissue-level phenotypes emerge from cellular behavior aggregated across the organ. Tumor growth reflects altered cell proliferation; fibrosis reflects aberrant extracellular matrix deposition; inflammation reflects immune cell recruitment and activation. These tissue phenotypes manifest as clinical symptoms, laboratory abnormalities, and imaging findings.\nThe cascade view suggests where different modalities provide information. Genomics captures the inherited potential and somatic alterations. Transcriptomics and epigenomics capture the current regulatory state. Proteomics and metabolomics capture the functional molecular complement. Clinical data captures the phenotypic consequences.\n\n\n\n\n\n\n\n\nThe causal cascade from variant to phenotype\n\n\n\n\n\n\n\nWhere each modality provides information\n\n\n\n\n\n\n\n\n\nBottleneck modalities vary by variant type\n\n\n\n\n\n\n\nIntegration guided by causal architecture\n\n\n\n\n\n\nFigure 22.5: Systems biology view of multi-omics integration. (A) The causal cascade: genetic variants propagate through molecular layers—expression, protein, metabolite, cellular behavior—to clinical phenotype. (B) Where each modality provides information: genomics captures the starting point, transcriptomics the current state, proteomics the functional complement, metabolomics downstream effects. (C) Bottleneck modalities: coding variants bottleneck at protein level, regulatory variants at expression level—determining which modality is most informative. (D) Integration implications: the causal architecture guides which modalities to prioritize for different biological questions.\n\n\n\n\n\n22.5.2 Bottleneck Modalities\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider two different variants: (1) a missense variant in TP53 that disrupts DNA binding, and (2) an enhancer variant that reduces TP53 expression by 30%. For which variant would expression data provide more information beyond genomic sequence alone? Why?\n\n\nNot all modalities are equally informative for all questions. The concept of bottleneck modalities identifies which molecular layers most directly mediate the relationship between genetic variation and phenotype.\nFor many coding variants, protein structure is the bottleneck. A missense variant’s effect on disease depends primarily on how it alters protein function, which depends on how the amino acid substitution affects folding, stability, and activity. Expression level matters less than structural consequence. Protein language models that predict structural effects from sequence directly address this bottleneck (see Chapter 15).\nFor regulatory variants, expression is closer to the bottleneck. An enhancer variant affects disease through its effect on target gene expression, which affects downstream processes. Chromatin accessibility and transcription factor binding are intermediate steps; expression level is the more proximal readout. Models that predict expression effects from sequence address this bottleneck (see Chapter 16).\nFor some phenotypes, the bottleneck may lie downstream of molecular measurements entirely. Behavioral traits depend on neural circuit function that emerges from complex cellular and network dynamics. Metabolic traits depend on flux through interconnected pathways that may not be apparent from enzyme abundance alone. These cases suggest that molecular measurements provide incomplete information regardless of integration sophistication.\n\n\n\n\n\n\nKey Insight\n\n\n\nThe bottleneck concept provides practical guidance for integration strategy. If you know that a phenotype is driven primarily by coding variants affecting protein structure, investing in proteomics may be less valuable than deploying state-of-the-art protein structure prediction from sequence. Conversely, if regulatory variants dominate, expression or chromatin accessibility measurements add substantial information beyond genomic sequence. Understanding the causal architecture helps prioritize which modalities to measure and integrate.\n\n\n\n\n22.5.3 Causal vs. Correlational Integration\nMulti-omics data are pervasively correlated. Genes in the same pathway have correlated expression. Methylation and expression are anti-correlated at many promoters. Clinical variables cluster by disease category. These correlations can improve prediction even without causal understanding.\nCausal integration seeks to identify the mechanistic relationships between molecular layers. If a variant causes reduced expression, which causes protein deficiency, which causes metabolic dysfunction, this causal chain suggests intervention targets: expression restoration or enzyme supplementation might address the downstream effects. Correlational integration might achieve the same predictive performance without identifying this chain, since all layers correlate with the phenotype.\nDistinguishing causal from correlational relationships requires experimental perturbation or careful causal inference from observational data. Mendelian randomization uses genetic variants as instruments to infer causal effects of expression on outcomes (see Section 3.9 for integration of GWAS with mechanism). CRISPR screens directly perturb gene function and measure consequences. Multi-omics integration methods increasingly incorporate causal assumptions or validation against perturbation data.\nThe distinction matters for interpretation and intervention. A predictive model based on correlations may fail when the data distribution shifts (see Chapter 12) or when interventions alter the causal structure. A causally informed model captures mechanism that persists across contexts.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch22-multi-omics.html#sec-ch22-missing-modalities",
    "href": "part_4/p4-ch22-multi-omics.html#sec-ch22-missing-modalities",
    "title": "22  Multi-Omics Integration",
    "section": "22.6 Handling Missing Modalities",
    "text": "22.6 Handling Missing Modalities\n\n\n\n\n\n\nChallenging Section\n\n\n\nHandling missing modalities requires understanding both the mathematical framework of intermediate fusion and the biological assumptions underlying cross-modal imputation. If the shared latent space concepts from Section 22.2.3 are not yet clear, review that section before proceeding.\n\n\nReal-world multi-omics data are incomplete. Different studies measure different modalities. Within studies, technical failures, sample limitations, and cost constraints create missing data. Clinical deployment must handle patients with incomplete molecular profiles. Robust multi-omics methods must address missingness directly.\n\n22.6.1 Training with Incomplete Data\nIntermediate fusion architectures handle missing modalities naturally during inference: only the available encoders contribute to the shared representation. Training is more complex because alignment terms require paired measurements across modalities.\nOne approach trains on the subset of samples with complete data, then applies the trained encoders to samples with partial data during inference. This wastes information from the samples with incomplete profiles and may learn representations that fail to generalize to the missing-modality setting.\nA better approach incorporates missingness into training. Modality dropout randomly masks modalities during training, forcing the model to learn representations robust to missing inputs. The mechanism works analogously to standard dropout: by training the model to succeed even when some information is unavailable, modality dropout encourages the shared latent space to encode biological state redundantly across modalities rather than relying on any single data type. The reconstruction and alignment losses are computed only for available modalities, so samples with partial data can still contribute to training.\nCurriculum learning strategies may first train with complete data to establish alignment, then gradually increase modality dropout to improve robustness. The curriculum matters because alignment and robustness have opposing requirements: strong alignment needs paired data to learn which expression patterns correspond to which accessibility patterns, but robustness needs the model to practice predicting with missing inputs. Starting with complete data establishes the cross-modal correspondences, then increasing dropout teaches the model to function without them. The balance between alignment quality (which benefits from complete data) and robustness (which requires training on partial data) requires empirical tuning.\n\n\n22.6.2 Cross-Modal Imputation\nIntermediate fusion enables principled imputation of missing modalities. Given a sample’s available modalities encoded into the shared latent space, decoders for missing modalities can predict expected values. If a patient has expression data but not methylation, the expression encoder produces a latent embedding, and the methylation decoder generates predicted methylation values from that embedding.\nThe imputation quality depends on how well the shared space captures the biological factors underlying both modalities. If expression and methylation reflect the same cell state, the imputation may be accurate. If they capture distinct aspects of biology, imputation will smooth over true variation.\nUncertainty in imputation matters for downstream use. Point estimates of missing values provide no indication of confidence. Generative models that produce distributions over missing values enable propagation of uncertainty through downstream analyses (see Section 23.4). A risk prediction that depends heavily on imputed values should have wider confidence intervals than one based entirely on measured data. The selective prediction and uncertainty communication strategies that could implement this appropriate caution are developed in Section 23.7.\n\n\n22.6.3 Zero-Shot Cross-Modal Transfer\nThe most ambitious application of multi-omics integration is zero-shot prediction across modalities: using a model trained on one set of modalities to make predictions for samples measured with entirely different modalities.\nThis transfer relies on the shared latent space capturing biological state independently of measurement modality. If the space truly represents cell state, then a classifier trained on expression-derived embeddings should work on ATAC-seq-derived embeddings, since both encoders map to the same biological meaning. The alignment training enables this transfer by ensuring that the same biological entity maps to the same latent location regardless of which modality was measured.\nZero-shot transfer is rarely perfect. The modalities may capture somewhat different aspects of biology, and the alignment may be imprecise. But partial transfer can still be valuable: a model achieving 80% of supervised performance without any labeled examples in the new modality saves substantial annotation effort (see ?sec-ch09-zero-shot for zero-shot transfer in other contexts).",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch22-multi-omics.html#sec-ch22-practical-challenges",
    "href": "part_4/p4-ch22-multi-omics.html#sec-ch22-practical-challenges",
    "title": "22  Multi-Omics Integration",
    "section": "22.7 Practical Challenges",
    "text": "22.7 Practical Challenges\nThe gap between multi-omics potential and deployed reality reflects obstacles that compound across modalities. Technical variation that is manageable within a single assay type becomes intractable when batch structures differ across genomics, transcriptomics, and proteomics. Sample sizes that support single-modality analysis may be insufficient when the effective dimensionality grows with each added data type. Interpretability, already challenging for deep learning on individual modalities, becomes harder still when attributions must be compared across features with different scales and semantics. These practical challenges determine whether integration improves predictions or merely adds complexity.\n\n22.7.1 Batch Effects Across Modalities\nBatch effects, systematic technical variation between experimental batches, are endemic in high-throughput biology. Multi-omics integration faces compounded batch effects: each modality may have its own batch structure, batches may be correlated or anti-correlated across modalities, and batch correction methods designed for single modalities may not extend to multi-modal settings.\nConsider a study where expression data were generated at three sequencing centers and proteomics data were generated at two mass spectrometry facilities. The batch effects in each modality are independent. Samples from expression batch 1 are spread across proteomics batches. Correcting expression batch effects does not address proteomics batch effects, and vice versa.\nIntegration must either correct batch effects within each modality before combining (risking removal of real biology that correlates with batch) or incorporate batch as a covariate in the integrated model (requiring that batch structure be known and modeled correctly). Domain adaptation techniques treat batches as domains and learn representations invariant to domain while retaining biological signal. The systematic strategies for detecting batch-driven confounding appear in ?sec-ch22-detection, while mitigation approaches including adversarial domain adaptation are detailed in ?sec-ch22-mitigation.\n\n\n22.7.2 Sample Size and Power\nMulti-omics studies typically have smaller sample sizes than single-modality studies due to cost constraints. Each additional modality increases per-sample cost, trading breadth for depth. This tradeoff has implications for statistical power and model complexity.\nThe effective sample size for multi-omics integration may be smaller than for any single modality. If 1000 patients have expression data and 800 have methylation data but only 600 have both, intermediate fusion sees 600 fully informative samples. Late fusion can use all 1000 expression samples and all 800 methylation samples, avoiding the intersection penalty.\nPower analyses for multi-omics studies must account for the specific integration strategy and the expected missingness pattern. A study designed for early fusion needs larger sample sizes (relative to feature count) than one designed for late fusion. Grant applications and study planning should explicitly consider how integration choices affect required sample sizes.\n\n\n22.7.3 Interpretability Across Modalities\nMulti-omics models compound the interpretability challenges inherent in deep learning. When a model predicts disease risk from integrated genomic, transcriptomic, and proteomic features, clinicians need to understand which modalities and which features drive the prediction. A black-box risk score, however accurate, provides little guidance for understanding mechanism or identifying intervention targets.\nAttribution methods that work for single-modality models do not automatically extend to multi-modal settings (see Chapter 24 for attribution methods). Gradient-based attribution can identify important features within each modality, but comparing importance across modalities requires careful normalization. A genomic variant and an expression value operate on different scales with different effect size distributions; raw attribution scores are not directly comparable.\nThe intermediate fusion architecture offers some interpretability advantages. The shared latent space can be visualized to understand how samples cluster and which modalities contribute to separation. Attention weights in cross-modal transformers indicate which features from each modality the model considers when making predictions. Modality ablation studies quantify each data type’s contribution to overall performance.\nBiological interpretability requires connecting learned representations to known biology. Do the latent dimensions correspond to pathways, cell types, or disease processes? Are cross-modal attention patterns consistent with known regulatory relationships? These questions demand validation against external biological knowledge, not just introspection of model parameters.\n\n\n22.7.4 Evaluation Complexity\nEvaluating multi-omics models is more complex than evaluating single-modality models. Multiple dimensions of performance matter: prediction accuracy, calibration, cross-modality transfer, robustness to missing modalities, biological plausibility of learned representations, and clinical utility.\nA model might achieve high prediction accuracy by memorizing batch effects or leveraging shortcuts in the data. Evaluation should include cross-batch and cross-cohort validation to assess generalization (Chapter 12), with particular attention to homology-aware splitting strategies (Section 12.2) that prevent information leakage across data partitions. Ablation studies that remove each modality quantify the contribution of each data type and identify whether the model genuinely integrates information or relies predominantly on one modality.\nBiological validation through comparison to known biology provides another evaluation axis. Do the learned factors correspond to known pathways? Are attention patterns consistent with regulatory relationships? Do imputed values match held-out measurements? These checks assess whether the model captures biological signal rather than technical artifacts.\nClinical evaluation, addressed in Chapter 27, requires prospective validation in real deployment settings. A model that improves prediction in research cohorts may not improve clinical decisions if the predictions do not change management or if the required modalities are unavailable in clinical workflows.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA research team trains a multi-omics model integrating genomics, transcriptomics, and proteomics for cancer prognosis. The model achieves excellent cross-validation performance but fails when tested on data from a different hospital.\n\nWhat are three possible reasons for this failure?\nWhat evaluation strategies should have been included to detect this problem earlier?\nHow might the fusion strategy choice (early, late, intermediate) affect robustness to this kind of distribution shift?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nThree likely reasons: batch effects differing between hospitals (different sequencing platforms, sample processing), population differences (ancestry, age distribution, disease subtypes), or the model memorizing hospital-specific technical artifacts rather than biological signal. 2. Cross-batch validation, cross-cohort validation with held-out external datasets, and ablation studies to verify each modality contributes genuine biological information rather than batch-correlated shortcuts. 3. Late fusion may be more robust because each modality is processed independently before combination, limiting how batch effects in one modality can corrupt others. Early fusion concatenates raw features, allowing batch effects to propagate across modalities. Intermediate fusion balances these—shared latent spaces can either amplify or mitigate batch effects depending on training objectives.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_4/p4-ch22-multi-omics.html#sec-ch22-conclusion",
    "href": "part_4/p4-ch22-multi-omics.html#sec-ch22-conclusion",
    "title": "22  Multi-Omics Integration",
    "section": "22.8 Integration as Means, Not End",
    "text": "22.8 Integration as Means, Not End\nMulti-omics integration is not an end in itself but a means to improved prediction, understanding, and intervention. The integration strategies and foundation models surveyed here produce representations; downstream applications convert those representations to actionable outputs. Risk prediction combines multi-omic embeddings with clinical variables for individualized prognosis. Treatment response models predict which patients will benefit from specific therapies based on their integrated molecular profiles. Drug discovery uses multi-omics to inform target identification and patient stratification for clinical trials (see Chapter 29). In each case, integration provides the substrate; clinical or scientific goals provide the purpose.\nThe systems view that multi-omics enables shapes how predictions should be interpreted. A risk prediction based on integrated features inherits explanatory power from the causal relationships linking molecular layers to phenotype. Understanding which modalities drive predictions, and how those modalities relate to underlying biology, supports clinical reasoning about mechanism and intervention. This explanatory capacity distinguishes multi-omics from single-modality approaches that may predict equally well but provide less insight into why predictions succeed or fail.\nThe path from research models to clinical deployment requires addressing practical challenges that intensify with integration: batch effects across modalities and institutions, missing measurements that differ systematically across patients, sample size limitations that grow with feature dimensionality, and evaluation complexity when outcomes depend on multiple data types. The clinical applications examined in Chapter 27 and Chapter 28 confront these realities. As the field advances toward whole-patient foundation models that jointly encode genomics, transcriptomics, proteomics, imaging, and clinical data, the integration principles established here provide the foundation. The trade-offs between fusion strategies, the importance of shared latent spaces, the challenge of missing modalities, and the systems biology perspective on information flow will remain relevant as scale and scope expand. The interpretability challenges that compound across modalities (Chapter 24) and the calibration requirements for clinical deployment (Section 23.2) add further dimensions that shape how multi-omics models should be developed and evaluated.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat is the “integration paradox” and why can more data sometimes lead to worse predictions in multi-omics?\nCompare the three fusion strategies (early, intermediate, late) and explain when each is most appropriate.\nWhy does intermediate fusion dominate modern multi-omics approaches?\nWhat is meant by “bottleneck modalities” and why should you identify them when designing multi-omics integration?\nExplain the difference between causal and correlational integration, and why this distinction matters for clinical interventions.\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nIntegration Paradox: The paradox is that naive concatenation of multi-omics features often degrades performance relative to single-modality models. This occurs because noise from uninformative features overwhelms signal from informative ones, batch effects between modalities create spurious correlations that models exploit, and the curse of dimensionality intensifies when features from multiple assays are stacked without principled integration—resulting in more parameters to estimate than training samples can constrain.\nFusion Strategy Comparison: Early fusion concatenates features before modeling, enabling arbitrary cross-modal interactions but requiring complete data and suffering from dimensionality issues—best for large samples with complete data. Late fusion trains separate models per modality and combines predictions, handling missing data excellently but unable to learn feature-level interactions—best when modalities provide independent signals. Intermediate fusion uses modality-specific encoders projecting to a shared latent space, balancing interaction learning with missing data robustness—best when modalities are coupled and paired training data exists.\nIntermediate Fusion Dominance: Intermediate fusion dominates because it balances flexibility with robustness by learning modality-specific encoders that can be pretrained on large single-modality datasets, then aligned through shared latent spaces that enable cross-modal reasoning. The architecture handles missing modalities gracefully (encoders fire only for available data), allows new modalities to be added without retraining existing components, and provides a natural target for interpretation and visualization—addressing the key limitations of both early fusion (dimensionality, missing data) and late fusion (no feature-level interactions).\nBottleneck Modalities: Bottleneck modalities are the molecular layers that most directly mediate the relationship between genetic variation and phenotype for a specific question. For coding variants affecting protein structure, the bottleneck is at the protein level (structure matters more than expression). For regulatory variants, expression is closer to the bottleneck (enhancer effects operate through gene expression changes). Identifying bottlenecks guides which modalities to prioritize for measurement and integration—investing in proteomics provides little value if protein structure prediction from sequence already captures the critical information.\nCausal vs. Correlational Integration: Causal integration identifies mechanistic relationships between molecular layers (e.g., variant causes reduced expression, which causes protein deficiency, which causes metabolic dysfunction), suggesting intervention targets like expression restoration or enzyme supplementation. Correlational integration exploits statistical associations to improve prediction without identifying mechanism—achieving similar predictive performance but lacking explanatory power. The distinction matters clinically because causal models capture mechanisms that persist across contexts and guide interventions, while correlational models may fail when data distributions shift or when interventions alter the causal structure.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nCore Concepts:\n\nThe integration paradox: more data can mean worse predictions when noise overwhelms signal, batch effects create spurious correlations, or dimensionality outpaces sample size\nThree fusion strategies (early, intermediate, late) offer different tradeoffs between cross-modal interaction learning and missing data robustness\nIntermediate fusion dominates modern approaches by learning modality-specific encoders projecting to a shared latent space\n\nKey Methods:\n\nMOFA+ for probabilistic factor-based integration\ntotalVI and MultiVI for deep generative single-cell integration\nContrastive multi-modal learning for aligning embeddings across modalities\n\nDesign Principles:\n\nIdentify bottleneck modalities: which molecular layers most directly mediate genotype-phenotype relationships for your question\nDesign for graceful degradation: models should produce useful predictions even with incomplete modality coverage\nDistinguish causal from correlational integration: causal understanding enables intervention, correlation enables prediction\n\nPractical Considerations:\n\nBatch effects compound across modalities and require per-modality correction or domain adaptation\nEffective sample size shrinks with each required modality due to incomplete overlap\nInterpretability requires cross-modal attribution normalization and validation against known biology\n\nLooking Ahead: Chapter 23 addresses how to quantify and communicate uncertainty in these complex models, while Chapter 27 examines the deployment path from research to clinical practice.",
    "crumbs": [
      "Part IV: Cellular Context",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Multi-Omics Integration</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html",
    "href": "part_5/p5-ch23-uncertainty.html",
    "title": "23  Uncertainty Quantification",
    "section": "",
    "text": "23.1 Types of Uncertainty in Genomic Prediction\nUncertainty in genomic prediction arises from two fundamentally different sources that demand different responses. One source reflects limitations in what the model has learned from available data; this uncertainty can, in principle, be reduced by gathering more examples or improving model architecture. The other source reflects genuine randomness in the biological system itself, where identical genotypes produce variable phenotypes through stochastic developmental processes, environmental interactions, or incomplete penetrance. Distinguishing between these sources determines whether additional data collection would help or whether we must accept irreducible limits on predictive confidence.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-types",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-types",
    "title": "23  Uncertainty Quantification",
    "section": "",
    "text": "Deep Dive: Epistemic vs. Aleatoric Uncertainty\n\n\n\nFor biology readers: Two fundamentally different types of uncertainty require different responses:\nEpistemic uncertainty (“knowledge uncertainty”) — what the model doesn’t know:\n\nArises from limited training data or model limitations\nCan be reduced by collecting more data or improving the model\nExample: A variant in an understudied gene has high epistemic uncertainty because few similar variants were in training data\nDetected by: ensemble disagreement, out-of-distribution detection\n\nAleatoric uncertainty (“data uncertainty”) — inherent randomness:\n\nArises from genuine biological variability or measurement noise\nCannot be reduced even with infinite data\nExample: Incomplete penetrance in BRCA1 — the same variant causes cancer in some carriers but not others due to modifier genes and environment\nDetected by: heteroscedastic models, known biology\n\nWhy the distinction matters clinically:\n\n\n\n\n\n\n\n\nUncertainty Type\nResponse\nAction\n\n\n\n\nHigh epistemic\nDefer, investigate\nOrder additional testing, expert consult\n\n\nHigh aleatoric\nAccept, communicate\nExplain inherent limits to patient\n\n\nHigh both\nMaximum caution\nConservative management\n\n\n\nA model that conflates these types cannot guide appropriate action: it cannot tell you whether more data would help or whether you must accept the uncertainty as irreducible.\n\n\n\n23.1.1 Why Uncertainty Matters\nClinical genetics operates under fundamental uncertainty. When a laboratory reports a variant of uncertain significance (VUS), they acknowledge that current evidence cannot confidently classify the variant as pathogenic or benign. ClinVar contains approximately two million VUS compared to roughly 250,000 variants classified as pathogenic (Landrum et al. 2018), reflecting the reality that most genetic variation remains incompletely understood. Foundation models inherit and sometimes amplify this uncertainty: they may produce confident-seeming scores for variants where the underlying biology remains genuinely unknown. The challenges of VUS classification and current interpretation frameworks are examined in detail in Chapter 28.\nThe consequences of ignoring uncertainty extend beyond statistical abstraction. An overconfident pathogenic prediction may trigger unnecessary interventions, from prophylactic surgeries to reproductive decisions that alter family planning. An overconfident benign prediction may provide false reassurance, delaying diagnosis while a treatable condition progresses. In both cases, the harm stems not from prediction error per se but from the mismatch between stated confidence and actual reliability. A model that accurately conveys its uncertainty enables appropriate clinical reasoning even when the prediction itself is imperfect.\n\n\n\n\n\n\nKey Insight: Calibration vs. Accuracy\n\n\n\nA model can be highly accurate on average yet dangerously miscalibrated. If a model achieves 90% accuracy but assigns 95% confidence to all predictions, clinicians will trust predictions that deserve skepticism. Conversely, a model with 80% accuracy that honestly reports its uncertainty enables better decisions than the overconfident 90% model. The goal is not just to be right, but to know when you’re right.\n\n\nDecision theory formalizes this intuition. The expected value of a clinical action depends on the probability of each outcome weighted by its utility. When a model reports 0.73 probability of pathogenicity, downstream decision-making implicitly assumes this probability is accurate. If the true probability is 0.50, actions optimized for 0.73 will systematically err. Uncertainty quantification ensures that the probabilities entering clinical decisions reflect genuine knowledge rather than artifacts of model architecture or training procedure.\n\n\n23.1.2 Epistemic Uncertainty\nA model trained exclusively on European-ancestry data encounters its first genome from an individual of African ancestry. The model’s predictions may be statistically valid within the distribution it has seen, yet unreliable for this new input due to limited exposure to ancestry-specific patterns of variation, linkage disequilibrium, and regulatory architecture. This uncertainty about what the model has learned, as distinct from noise inherent in the prediction task itself, constitutes epistemic uncertainty.\n\n\n\n\n\n\n\n\nAleatoric: irreducible stochastic variability\n\n\n\n\n\n\n\nEpistemic: reducible knowledge-based uncertainty\n\n\n\n\n\n\nFigure 23.1: Two types of uncertainty. (A) Aleatoric uncertainty: irreducible stochasticity from biological variability or measurement noise—cannot be eliminated with more data. (B) Epistemic uncertainty: reducible uncertainty from limited knowledge—decreases as training data grow. Distinguishing these is critical: high epistemic uncertainty should trigger caution, while aleatoric uncertainty sets fundamental prediction limits.\n\n\n\nEpistemic uncertainty arises from limitations in training data that could, in principle, be reduced by gathering more examples. In genomic foundation models, epistemic uncertainty concentrates in predictable regions of biological space. Proteins from poorly characterized families, where training data contained few homologs, exhibit high epistemic uncertainty because the model has limited basis for inference. This manifests concretely in protein benchmarks: ProteinGym performance varies substantially across protein families (Section 11.1.3). Genes with few characterized variants in ClinVar or gnomAD provide sparse supervision, leaving the model uncertain about which sequence features distinguish pathogenic from benign variation (see Section 2.8.1 and Section 2.2.3 for data resource details). Rare variant classes, such as in-frame deletions in specific protein domains, appear infrequently in training data and consequently generate uncertain predictions. Populations under-represented in biobanks contribute fewer training examples, creating systematic epistemic uncertainty for individuals from these backgrounds, a challenge examined in Section 3.7 and with confounding implications discussed in ?sec-ch22-ancestry-confounding.\nMathematically, epistemic uncertainty reflects uncertainty over model parameters or learned representations. A Bayesian perspective treats the trained model as one sample from a posterior distribution over possible models consistent with the training data. Different plausible models may disagree on predictions for inputs far from training examples while agreeing on well-represented inputs. This disagreement manifests as high variance in predictions across model variants, sensitivity to random initialization, or instability under small perturbations to training data.\nFoundation models exhibit epistemic uncertainty through several observable signatures. Embeddings for unfamiliar sequences cluster in sparse regions of representation space, distant from the dense clusters formed by well-represented sequence families. Ensemble members trained with different random seeds produce divergent predictions for novel inputs while converging for familiar ones. Fine-tuning on the same downstream task with different random seeds yields inconsistent results for edge cases. These signatures provide practical diagnostics for identifying when epistemic uncertainty is high.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading the next section, consider: If you have a variant in a gene with only 5 known pathogenic variants in ClinVar (versus thousands for a well-studied gene like BRCA1), what type of uncertainty dominates your prediction? Would collecting more data help? What kind of data would be most valuable?\n\n\n\n\n23.1.3 Aleatoric Uncertainty\nSome variants are genuinely ambiguous regardless of how much data we collect. The same pathogenic variant in BRCA1 causes breast cancer in one carrier but not another due to modifier genes, hormonal exposures, or stochastic developmental processes. Incomplete penetrance, the phenomenon where disease-associated variants do not always produce disease, creates irreducible uncertainty that no amount of training data can eliminate. This inherent randomness in the mapping from genotype to phenotype constitutes aleatoric uncertainty.\nAleatoric uncertainty reflects noise or stochasticity intrinsic to the prediction problem rather than limitations of the model. Variable expressivity means that even when a variant causes disease, the severity and specific manifestations vary across individuals. Measurement noise in functional assays introduces uncertainty into the labels used for training: deep mutational scanning experiments typically exhibit 10 to 20 percent technical variation between replicates (Fowler and Fields 2014; Rubin et al. 2017),, creating a floor below which prediction error cannot decrease regardless of model sophistication (see Section 2.4.4 for a discussion of DMS data characteristics). Stochastic gene expression means that two genetically identical cells may express a gene at different levels due to random fluctuations in transcription and translation. These sources of randomness set fundamental limits on predictive accuracy.\nAleatoric uncertainty often varies with the input, a property termed heteroscedasticity. Coding variants in essential genes may have relatively low aleatoric uncertainty because strong selection pressure produces consistent phenotypic effects. Regulatory variants exhibit higher aleatoric uncertainty because their effects depend on cellular context, developmental timing, and interactions with other genetic and environmental factors. A model that captures this heteroscedasticity can provide more informative uncertainty estimates by conveying that some predictions are inherently more reliable than others.\nThe following table summarizes the key distinctions between epistemic and aleatoric uncertainty:\n\n\n\nTable 23.1: Comparison of epistemic and aleatoric uncertainty in genomic prediction.\n\n\n\n\n\n\n\n\n\n\nProperty\nEpistemic Uncertainty\nAleatoric Uncertainty\n\n\n\n\nSource\nLimited training data, model knowledge gaps\nInherent noise in biological system\n\n\nReducibility\nCan be reduced with more data\nIrreducible, fundamental limit\n\n\nExample causes\nUnder-represented populations, rare genes, novel folds\nIncomplete penetrance, measurement noise, stochastic expression\n\n\nDetection method\nEnsemble disagreement, embedding distance\nHeteroscedastic model predictions\n\n\nResponse\nCollect more data, seek expert review\nAccept limits, communicate uncertainty\n\n\nClinical implication\nDefer decision, request additional evidence\nProceed with acknowledged uncertainty\n\n\n\n\n\n\n\n\n23.1.4 Decomposing Total Uncertainty\nTotal predictive uncertainty combines epistemic and aleatoric components, and distinguishing between them has practical implications for decision-making. High epistemic uncertainty suggests that gathering more data, either through additional training examples or further investigation of the specific case, could reduce uncertainty and improve the prediction. High aleatoric uncertainty indicates that the prediction is as good as it can get given inherent noise in the problem; additional data will not help because the underlying biology is stochastic.\n\n\n\n\n\n\nDifficulty Warning\n\n\n\nThe following mathematical framework for uncertainty decomposition requires comfort with variance and conditional probability. If these concepts are unfamiliar, focus on the intuition: ensemble disagreement measures epistemic uncertainty, while within-model variance measures aleatoric uncertainty. The key insight is that total uncertainty can be partitioned into “uncertainty we can reduce” and “uncertainty we cannot.”\n\n\nThe law of total variance provides a mathematical framework for decomposition. Total variance in predictions equals the sum of variance due to model uncertainty (epistemic) and variance inherent in the data-generating process (aleatoric). In practice, ensemble methods approximate epistemic uncertainty through disagreement between members: if five independently trained models produce predictions of 0.65, 0.68, 0.70, 0.72, and 0.75, the spread reflects epistemic uncertainty, while the residual variance within each model’s predictions reflects aleatoric uncertainty. Heteroscedastic neural networks, which output both a predicted mean and a predicted variance, can estimate aleatoric uncertainty by learning input-dependent noise levels.\nThese decompositions depend on modeling assumptions and provide approximations rather than exact separations. Ensemble disagreement may underestimate epistemic uncertainty if all members share similar biases from common training data. Heteroscedastic models may confound aleatoric and epistemic uncertainty if the training data is too sparse to reliably estimate noise levels. Despite these limitations, approximate decomposition provides actionable information: variants flagged for high epistemic uncertainty warrant additional data collection or expert review, while variants with high aleatoric uncertainty may require acceptance of irreducible limits on predictive confidence.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-calibration",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-calibration",
    "title": "23  Uncertainty Quantification",
    "section": "23.2 Calibration and Confidence Interpretation",
    "text": "23.2 Calibration and Confidence Interpretation\nCalibration determines whether model outputs can be interpreted as probabilities. A score of 0.85 from a pathogenicity predictor should mean that 85% of variants receiving similar scores are truly pathogenic; only then can clinicians rationally weight computational evidence against other diagnostic criteria. This chapter provides the comprehensive treatment of calibration theory and methods. Readers encountering calibration in applied contexts, whether variant effect prediction (?sec-ch14-calibration) or evaluation methodology (Chapter 12), are referred here for the formal foundations and complete methodological catalog.\n\n\n\n\n\n\n\n\nPerfect calibration: predictions match observed frequencies\n\n\n\n\n\n\n\nOverconfident: predictions exceed reality\n\n\n\n\n\n\n\n\n\nUnderconfident: predictions underestimate true probabilities\n\n\n\n\n\n\n\nCalibration metrics: ECE and MCE quantify miscalibration\n\n\n\n\n\n\nFigure 23.2: Model calibration assessment. (A) Perfect calibration: predicted probabilities match observed frequencies on the diagonal. (B) Overconfident model: predictions systematically exceed reality—dangerous for clinical use. (C) Underconfident model: predictions underestimate true probabilities—reduces utility. (D) Calibration metrics: Expected Calibration Error (ECE) averages across bins; Maximum Calibration Error (MCE) captures worst-case performance.\n\n\n\n\n23.2.1 The Calibration Problem\nAlphaMissense outputs a continuous score between 0 and 1 for each possible missense variant in the human proteome. When it reports 0.85 for a particular variant, what does this number mean? If the model is calibrated, collecting all variants scored near 0.85 and checking their true clinical status should reveal that approximately 85% are pathogenic. Perfect calibration means that predicted probabilities match observed frequencies across the entire range of model outputs: among variants scored at 0.30, roughly 30% should be pathogenic; among variants scored at 0.95, roughly 95% should be pathogenic. This alignment between stated confidence and empirical accuracy is calibration, and most foundation models fail to achieve it.\nThink of calibration like weather forecasts. When your weather app says “70% chance of rain,” you expect that across many days with 70% forecasts, it actually rains about 70% of the time. If it only rains 40% of those days, the app is overconfident and you cannot trust its numbers to plan your week. Similarly, a pathogenicity predictor claiming 85% confidence should be right 85% of the time at that confidence level—otherwise, clinicians cannot rationally weigh its predictions against other evidence.\nFormally, a model \\(f\\) mapping inputs \\(X\\) to probability estimates \\(p = f(X)\\) is calibrated if \\(P(Y = 1 \\mid f(X) = p) = p\\) for all \\(p\\) in the interval from \\(0\\) to \\(1\\). The calibration condition requires that the model’s stated confidence equals the true probability of the positive class conditional on that stated confidence. Miscalibration occurs when this equality fails: overconfident models produce predicted probabilities that exceed true frequencies (a variant scored at \\(0.85\\) is pathogenic only \\(60\\%\\) of the time), while underconfident models produce predicted probabilities below true frequencies.\nModern deep neural networks are systematically miscalibrated despite achieving high accuracy. Guo and colleagues demonstrated that contemporary architectures exhibit worse calibration than older, less accurate models (Guo et al. 2017). The phenomenon arises because standard training objectives like cross-entropy loss optimize for discrimination (separating positive from negative examples) rather than calibration (matching predicted probabilities to frequencies). Over-parameterized models with capacity exceeding what the data requires can achieve near-perfect training loss while producing overconfident predictions on held-out data. The softmax temperature in transformer architectures affects the sharpness of probability distributions, and default settings often produce excessively peaked outputs.\nCalibration and discrimination are distinct properties. A model can achieve perfect area under the receiver operating characteristic curve (auROC), correctly ranking all positive examples above all negative examples, while being arbitrarily miscalibrated. If a classifier assigns probability 0.99 to all positive examples and 0.98 to all negative examples, it ranks perfectly but provides useless probability estimates. Conversely, a calibrated model that assigns 0.51 to positives and 0.49 to negatives would be calibrated but nearly useless for discrimination. Clinical applications typically require both: accurate ranking to identify high-risk variants and accurate probabilities to inform decision-making.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA variant effect predictor achieves auROC of 0.95 on a held-out test set. A colleague concludes that “the model’s probability estimates are reliable.” What is wrong with this reasoning? What additional assessment would you recommend?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nauROC measures discrimination (how well the model ranks pathogenic variants above benign ones) but says nothing about calibration (whether predicted probabilities match observed frequencies). A model can achieve perfect auROC while being arbitrarily miscalibrated—for example, by assigning 0.99 to all pathogenic variants and 0.98 to all benign variants. You should assess calibration using reliability diagrams and ECE, stratified by relevant subgroups like ancestry and variant type.\n\n\n\n\n\n\n\n23.2.2 Measuring Calibration\nReliability diagrams provide visual assessment of calibration by plotting predicted probabilities against observed frequencies. Construction involves binning predictions into intervals (commonly ten bins spanning 0 to 0.1, 0.1 to 0.2, and so forth), computing the mean predicted probability within each bin, computing the fraction of positive examples within each bin, and plotting these two quantities against each other. A perfectly calibrated model produces points along the diagonal where predicted probability equals observed frequency. Systematic deviations reveal calibration patterns: points below the diagonal indicate overconfidence (predictions exceed reality), points above indicate underconfidence, and S-shaped curves suggest nonlinear miscalibration requiring more flexible correction.\nExpected calibration error (ECE) provides a scalar summary of calibration quality. ECE computes the weighted average absolute difference between predicted probabilities and observed frequencies across bins:\n\\[\n\\text{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{n} \\left| \\text{acc}(B_m) - \\text{conf}(B_m) \\right|\n\\]\nwhere \\(B_m\\) denotes the set of examples in bin \\(m\\), \\(|B_m|\\) is the number of examples in that bin, \\(n\\) is the total number of examples, \\(\\text{acc}(B_m)\\) is the accuracy (fraction of positives) in bin \\(m\\), and \\(\\text{conf}(B_m)\\) is the mean predicted probability in bin \\(m\\). Lower ECE indicates better calibration, with zero representing perfect calibration. ECE depends on binning strategy; equal-width bins may place most examples in a few bins for models with concentrated predictions, while equal-mass bins ensure each bin contains the same number of examples but may span wide probability ranges.\nMaximum calibration error (MCE) captures worst-case miscalibration by reporting the largest absolute gap between predicted and observed frequencies across all bins. MCE is appropriate when any severe miscalibration is unacceptable, as in high-stakes clinical applications where even rare catastrophic errors carry significant consequences.\nBrier score decomposes into components measuring calibration and discrimination (refinement), providing a single proper scoring rule that rewards both properties. The Brier score equals the mean squared difference between predicted probabilities and binary outcomes, and its decomposition reveals whether poor scores stem from miscalibration, poor discrimination, or both.\nThe following table summarizes these calibration metrics and their appropriate use cases:\n\n\n\nTable 23.2: Calibration metrics for genomic foundation models. ECE and MCE depend on binning choices; report the binning strategy alongside metric values.\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nFormula/Description\nRange\nBest Value\nUse Case\n\n\n\n\nReliability diagram\nVisual: predicted vs. observed frequency\nN/A\nPoints on diagonal\nInitial assessment, pattern identification\n\n\nECE\nWeighted average\npredicted - observed\n\n[0, 1]\n\n\nMCE\nMaximum\npredicted - observed\nacross bins\n[0, 1]\n\n\nBrier score\nMean squared error of probabilities\n[0, 1]\n0\nCombined calibration + discrimination\n\n\n\n\n\n\n\n\n23.2.3 Why Foundation Models Are Often Miscalibrated\nFoundation models face calibration challenges beyond those affecting standard neural networks. Pretraining objectives like masked language modeling optimize for predicting held-out tokens, not for producing calibrated probability distributions over downstream tasks (see Chapter 8 for a detailed discussion of pretraining objectives). The representations learned during pretraining may encode useful information about sequence biology while providing no guarantee that fine-tuned classifiers will be well-calibrated.\nDistribution shift between pretraining and evaluation compounds miscalibration. A protein language model pretrained on UniRef sequences encounters a fine-tuning task using ClinVar variants. The pretraining distribution emphasizes common proteins with many homologs, while clinical variants concentrate in disease-associated genes with different sequence characteristics. Models may be well-calibrated on held-out pretraining data while miscalibrated on clinically relevant evaluation sets. The broader challenges of distribution shift are examined in ?sec-ch09-domain-shift.\nLabel noise in training data propagates to calibration errors. ClinVar annotations reflect the state of knowledge at submission time and may contain errors, particularly for older entries or variants from less-studied genes. Deep mutational scanning experiments provide functional labels but with measurement noise that varies across assays. Models trained on noisy labels may learn the noise distribution, producing predictions that match training labels but not underlying truth.\nZero-shot approaches present particular calibration challenges. ESM-1v log-likelihood ratios measure how surprising a mutation is to the language model, but these ratios are not probabilities and have no inherent calibration. Converting log-likelihood ratios to pathogenicity probabilities requires explicit calibration against external labels, and the resulting calibration depends on the reference dataset used for this conversion. The protein language model family and its variant effect scoring capabilities are discussed in ?sec-ch12-variant-effects.\n\n\n23.2.4 Calibration Across Subgroups\nAggregate calibration metrics can mask severe miscalibration in clinically important subgroups. A model might achieve low ECE overall while being dramatically overconfident for variants in African-ancestry individuals and underconfident for European-ancestry individuals, with opposite errors canceling in aggregate statistics. Subgroup-stratified calibration assessment is essential for any model intended for diverse populations.\n\n\n\n\n\n\nKey Insight: Hidden Calibration Disparities\n\n\n\nAggregate calibration metrics (ECE, Brier score) can appear excellent while masking severe disparities across subgroups. A model that is overconfident for one population and underconfident for another may achieve near-zero aggregate ECE if errors cancel. Always compute calibration metrics stratified by ancestry, gene family, and variant class. This is not just a technical concern; it directly impacts health equity.\n\n\nAncestry-stratified calibration reveals systematic patterns in current foundation models. Training data for protein language models and variant effect predictors derive predominantly from European-ancestry cohorts, creating differential epistemic uncertainty across populations. Calibration curves stratified by ancestry often show that models are better calibrated for populations well-represented in training data and overconfident or underconfident for underrepresented populations. This differential calibration has direct fairness implications: clinical decisions based on miscalibrated predictions will be systematically worse for patients from underrepresented backgrounds. The broader challenges of fairness and health equity are addressed in Section 3.7.2 and Section 26.1.\nCalibration may also vary by variant class, gene constraint level, protein family, or disease category. Missense variants in highly constrained genes may show different calibration patterns than those in tolerant genes. Variants in well-studied protein families with abundant training examples may be better calibrated than variants in orphan proteins. Stratified reliability diagrams across these categories reveal whether a single calibration correction suffices or whether subgroup-specific approaches are necessary.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-post-hoc-calibration",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-post-hoc-calibration",
    "title": "23  Uncertainty Quantification",
    "section": "23.3 Post-Hoc Calibration Methods",
    "text": "23.3 Post-Hoc Calibration Methods\n\n\n\n\n\n\n\n\nTemperature scaling: single parameter, preserves ranking\n\n\n\n\n\n\n\nPlatt scaling: handles asymmetric miscalibration\n\n\n\n\n\n\n\nIsotonic regression: flexible non-parametric fit\n\n\n\n\n\n\nFigure 23.3: Post-hoc calibration methods. (A) Temperature scaling: divides logits by learned temperature T before softmax—simple, preserves ranking, widely effective. (B) Platt scaling: fits logistic regression on model scores—handles asymmetric miscalibration with two parameters. (C) Isotonic regression: non-parametric monotonic fit—most flexible but requires more calibration data and risks overfitting.\n\n\n\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore learning about calibration methods, consider: If a model consistently assigns probabilities that are too high (overconfident), what mathematical operation might correct this? What if the overconfidence varies depending on the predicted probability level?\n\n\n\n23.3.1 Temperature Scaling\nThe simplest calibration fix is often the most effective. Temperature scaling applies a single learned parameter to adjust model confidence, dramatically improving calibration with minimal computational overhead and no change to model predictions’ ranking.\nThe method modifies the softmax function by dividing logits by a temperature parameter T before applying softmax. The intuition is straightforward: overconfident models produce logits that are too large in magnitude, causing softmax probabilities to concentrate near 0 or 1 rather than reflecting true uncertainty. Dividing by T &gt; 1 shrinks these logits toward zero, which “softens” the probability distribution and reduces spurious confidence:\n\\[\\hat{p}_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\\]\nwhere z_i are the logits (pre-softmax outputs) and *p̂_i* are the calibrated probabilities.\n\n\n\n\n\n\nWorked Example: Temperature Scaling\n\n\n\nA variant effect prediction model produces logits for a missense variant:\nBefore calibration (T = 1.0): - Logits: \\(z = [2.5, -1.2]\\) for [pathogenic, benign] - Softmax: \\(p = [\\frac{e^{2.5}}{e^{2.5} + e^{-1.2}}, \\frac{e^{-1.2}}{e^{2.5} + e^{-1.2}}] = [0.976, 0.024]\\) - The model claims 97.6% confidence in pathogenicity\nAfter calibration (T = 2.0, learned from calibration set): - Scaled logits: \\(z/T = [1.25, -0.6]\\) - Softmax: \\(p = [\\frac{e^{1.25}}{e^{1.25} + e^{-0.6}}, \\frac{e^{-0.6}}{e^{1.25} + e^{-0.6}}] = [0.86, 0.14]\\) - Now the model reports 86% confidence—still favoring pathogenic, but acknowledging greater uncertainty\n\n\n\nTemperature\nP(pathogenic)\nP(benign)\nInterpretation\n\n\n\n\nT = 1.0 (uncalibrated)\n0.976\n0.024\nOverconfident\n\n\nT = 2.0 (calibrated)\n0.86\n0.14\nRealistic uncertainty\n\n\nT = 4.0 (over-softened)\n0.71\n0.29\nUnderconfident\n\n\n\nNote that the ranking is preserved: pathogenic remains more likely than benign. Only the magnitude of confidence changes.\n\n\nWhen T &gt; 1, the distribution becomes softer (more uniform), reducing overconfidence. When T &lt; 1, the distribution becomes sharper, increasing confidence. The optimal temperature is learned by minimizing negative log-likelihood on a held-out calibration set, typically yielding T between 1.5 and 3 for overconfident deep networks.\nTemperature scaling preserves the model’s ranking because dividing all logits by the same constant does not change their relative ordering. A variant ranked as more likely pathogenic than another remains more likely after temperature scaling; only the magnitudes of probability estimates change. This preservation of discrimination while improving calibration makes temperature scaling particularly attractive: calibration improves without sacrificing the model’s hard-won ability to distinguish pathogenic from benign variants.\nThe method’s simplicity (one parameter) is both strength and limitation. A single global temperature cannot fix heterogeneous miscalibration where the model is overconfident in some regions of input space and underconfident in others. When reliability diagrams show complex nonlinear patterns, more flexible calibration methods are necessary.\n\n\n23.3.2 Platt Scaling\nPlatt scaling fits a logistic regression model on the original model’s outputs, learning both a slope and intercept to transform scores into calibrated probabilities. For binary classification:\n\\[\\hat{p} = \\sigma(a \\cdot f(x) + b)\\]\nwhere \\(f(x)\\) is the original model’s output, \\(\\sigma\\) is the sigmoid function, and parameters \\(a\\) and \\(b\\) are learned on calibration data. The two parameters provide more flexibility than temperature scaling’s single parameter, allowing correction of both the sharpness and the location of the probability distribution.\nPlatt scaling is appropriate when miscalibration involves systematic bias (predictions consistently too high or too low) in addition to over- or underconfidence. The method assumes that a monotonic logistic transformation suffices to correct miscalibration, which may not hold for models with complex, non-monotonic calibration curves.\n\n\n23.3.3 Isotonic Regression\nIsotonic regression provides a non-parametric approach that fits a monotonically increasing function mapping raw scores to calibrated probabilities. Unlike temperature or Platt scaling, isotonic regression makes no assumptions about the functional form of miscalibration, allowing it to correct arbitrary monotonic patterns.\nThe method works by pooling adjacent bins whose empirical frequencies violate monotonicity, then assigning each bin its pooled frequency. The resulting calibration function is a step function that increases with the original score. This flexibility comes at a cost: with limited calibration data, isotonic regression may overfit to noise in the calibration set, and the step-function output can appear discontinuous. Additionally, isotonic regression provides no uncertainty estimate on the calibration itself; we learn a point estimate of the calibration function without knowing how reliable that estimate is.\nThe following table provides guidance for selecting among post-hoc calibration methods:\n\n\n\nTable 23.3: Selection guide for post-hoc calibration methods. Start with temperature scaling; move to more complex methods only when reliability diagrams show patterns that simpler methods cannot address.\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nParameters\nBest For\nLimitations\nCalibration Data Needed\n\n\n\n\nTemperature scaling\n1\nUniform overconfidence, when ranking must be preserved\nCannot fix heterogeneous or nonlinear miscalibration\n~1,000 examples\n\n\nPlatt scaling\n2\nBias + overconfidence, binary classification\nAssumes logistic correction is sufficient\n~1,000 examples\n\n\nIsotonic regression\nMany\nComplex, nonlinear calibration curves\nOverfits with limited data, discontinuous output\n~5,000+ examples\n\n\nSubgroup-specific\nVaries\nKnown calibration disparities across groups\nRequires labeled data per subgroup\n~1,000 per group\n\n\n\n\n\n\n\n\n23.3.4 Calibrating Foundation Model Outputs\nGenomic foundation models present specific calibration considerations beyond standard classification settings. The choice of calibration approach depends on whether the model produces logits, log-likelihood ratios, or continuous regression outputs, and on whether calibration targets are available for the deployment distribution.\nFor zero-shot variant effect scores like ESM-1v log-likelihood ratios, raw outputs have no inherent probabilistic interpretation. Calibration requires mapping these continuous scores to pathogenicity probabilities using external labels, typically from ClinVar or population frequency data. This mapping should occur on held-out genes or variants not used for any model development, and the resulting calibration reflects the specific label set used; calibration against ClinVar pathogenic/benign labels may not transfer to other clinical contexts. The principles of proper held-out evaluation are discussed in Chapter 12.\nMulti-output models that predict across many tasks (multiple cell types, tissues, or assays) may require separate calibration for each output. A regulatory model predicting expression across 200 cell types is unlikely to be uniformly calibrated across all outputs; cell types with more training data may show better calibration than rare cell types.\nTemporal stability of calibration deserves consideration. As ClinVar annotations evolve with new evidence, the ground truth against which models were calibrated changes. A model calibrated against 2020 ClinVar labels may become miscalibrated relative to 2025 labels as variant classifications are updated. Periodic recalibration against current labels helps maintain clinical relevance.\n\n\n\n\n\n\nPractical Guidance: Calibration Workflow\n\n\n\nWhen deploying a foundation model for clinical variant interpretation:\n\nAssess baseline calibration using reliability diagrams and ECE on held-out data representative of deployment\nStratify by subgroup (ancestry, gene family, variant class) to identify hidden disparities\nStart simple: Apply temperature scaling; check if the calibration curve approaches the diagonal\nEscalate if needed: If temperature scaling leaves nonlinear patterns, try Platt scaling or isotonic regression\nValidate on deployment distribution: Calibration learned on one distribution may not transfer\nMonitor and recalibrate: Track calibration over time as ground truth labels evolve",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-uq-methods",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-uq-methods",
    "title": "23  Uncertainty Quantification",
    "section": "23.4 Uncertainty Quantification Methods for Foundation Models",
    "text": "23.4 Uncertainty Quantification Methods for Foundation Models\nCalibration ensures that stated probabilities match observed frequencies, but even well-calibrated models provide only point estimates. When a model reports 0.70 pathogenicity probability, is that uncertainty reducible with more data, or does it reflect genuine ambiguity in the biological signal? Distinguishing these sources of uncertainty enables more appropriate clinical responses: epistemic uncertainty (arising from limited data) suggests the prediction might change with additional evidence, while aleatoric uncertainty (inherent to the problem) indicates that even perfect models would remain uncertain.\n\n\n\n\n\n\nUncertainty quantification methods overview\n\n\n\n\nFigure 23.4: Uncertainty quantification methods for deep learning. Ensembles train multiple models and use prediction variance; MC Dropout uses stochastic forward passes through a single model; Bayesian neural networks maintain distributions over weights; evidential deep learning predicts distribution parameters directly. Each involves tradeoffs between computational cost, theoretical grounding, and calibration quality.\n\n\n\n\n23.4.1 Deep Ensembles\nIf one model expresses uncertainty about a prediction, querying multiple models reveals whether that uncertainty reflects genuine ambiguity in the data or an artifact of a particular training run. When five independently trained models agree on a prediction, confidence is warranted; when they disagree, the disagreement itself signals uncertainty. Ensemble disagreement provides one of the most reliable uncertainty estimates available in deep learning, at the cost of training and maintaining multiple models.\nDeep ensembles train M models (typically 5 to 10) with different random initializations, data orderings, or minor architectural variations. At inference time, all members produce predictions, and uncertainty is estimated from the variance or entropy of the ensemble distribution. For classification, epistemic uncertainty appears as disagreement in predicted class probabilities across members. For regression, epistemic uncertainty appears as variance in predicted values.\nWhy do ensembles work for uncertainty estimation? The key insight is that ensemble disagreement reveals where the data underdetermines the model. The theoretical basis for ensemble uncertainty estimation rests on the observation that disagreement between models reflects regions of input space where the training data provides insufficient constraint. Where training examples are dense, gradient descent from different initializations converges to similar solutions, producing agreement. Where training examples are sparse or conflicting, different initializations find different local optima, producing disagreement. This interpretation connects ensembles to Bayesian model averaging, where predictions are averaged over the posterior distribution of model parameters.\nFor foundation models with billions of parameters, training full ensembles becomes prohibitively expensive. Training five copies of ESM-2 requires approximately five times the compute of a single model, potentially millions of dollars in cloud computing costs. Several practical alternatives reduce this burden. Last-layer ensembles freeze the pretrained backbone and train only an ensemble of prediction heads, reducing cost by orders of magnitude while still capturing uncertainty from the fine-tuning process. Snapshot ensembles save model checkpoints at various points during optimization and use these snapshots as ensemble members, requiring only single-model training time. Multi-seed fine-tuning trains the same architecture from multiple random seeds on the fine-tuning task, which is far cheaper than multi-seed pretraining. The broader considerations of fine-tuning and adaptation strategies are discussed in Chapter 9.\n\n\n23.4.2 Monte Carlo Dropout\nMonte Carlo (MC) dropout provides uncertainty estimates from a single trained model by treating dropout regularization as approximate Bayesian inference. During standard training with dropout, random subsets of neurons are zeroed at each forward pass. MC dropout keeps dropout active at test time and performs multiple stochastic forward passes, treating the variation across passes as a measure of model uncertainty.\nGal and Ghahramani showed that this procedure approximates variational inference over the model’s weights (Gal and Ghahramani 2016). Each forward pass with dropout samples a different subnetwork, and the distribution of predictions across samples approximates the predictive distribution under a particular prior over weights. High variance across MC samples indicates epistemic uncertainty about the model’s parameters for that input.\nWhy does dropout approximate Bayesian inference? During training, dropout randomly masks neurons to prevent co-adaptation—forcing the network to learn redundant representations. Treating this masking as sampling from a distribution over subnetworks connects to Bayesian inference: each sampled subnetwork is like a draw from a posterior over model architectures. When many subnetworks agree on a prediction, the input lies in a region well-constrained by training data; when subnetworks disagree, the input lies in a region where different architectural variants have learned different solutions—exactly the signature of epistemic uncertainty.\nMC dropout offers the significant advantage of requiring only a single trained model, avoiding the computational overhead of ensembles. Implementation is straightforward: enable dropout during inference and average predictions over 10 to 50 stochastic forward passes. The variance or entropy of these predictions serves as the uncertainty estimate.\nLimitations temper the method’s appeal. Modern transformer architectures often do not use dropout in their standard configurations, or use dropout only in specific locations (attention dropout, residual dropout) where the approximation may be less accurate. The quality of uncertainty estimates depends on the dropout rate and architecture, with higher dropout rates providing better uncertainty estimates but potentially degrading mean predictions. Empirical comparisons often find that MC dropout underestimates uncertainty relative to deep ensembles, particularly in low-data regimes where epistemic uncertainty should be high.\n\n\n\n\n\n\nStop and Think\n\n\n\nYou need to deploy a variant effect predictor with uncertainty quantification, but you have a limited compute budget. Full ensembles of the foundation model are too expensive. What alternatives could you consider, and what trade-offs would each involve? Think about this before reading the heteroscedastic models section.\n\n\n\n\n23.4.3 Heteroscedastic Models\nStandard regression models predict a single output value, implicitly assuming constant noise variance across all inputs. Heteroscedastic models instead predict both a mean and a variance for each input, capturing the intuition that prediction uncertainty varies depending on the input. For genomic applications, this approach naturally handles the observation that some prediction tasks are inherently noisier than others: coding variant effects may be more predictable than regulatory variant effects, constrained genes more predictable than tolerant genes.\nArchitecture modifications are minimal. Instead of outputting a single value, the model outputs two values interpreted as the mean \\(\\mu(x)\\) and variance \\(\\sigma^2(x)\\) of a Gaussian distribution over outputs. Training uses negative log-likelihood loss under this Gaussian, which penalizes both prediction errors and miscalibrated variance estimates:\n\\[\\mathcal{L} = \\frac{1}{2\\sigma^2(x)}(y - \\mu(x))^2 + \\frac{1}{2}\\log \\sigma^2(x)\\]\nWhy does this loss function take this particular form? It derives from the negative log-likelihood of a Gaussian distribution: if the model predicts that the output follows a Gaussian with mean \\(\\mu(x)\\) and variance \\(\\sigma^2(x)\\), then the probability of observing the true value \\(y\\) is higher when \\(y\\) is close to \\(\\mu\\) and when variance is well-matched to actual prediction error. The first term penalizes prediction errors, weighted by inverse variance so that high-variance predictions are penalized less for the same absolute error—this is mathematically necessary because a Gaussian with larger variance assigns non-negligible probability to a wider range of values. The second term prevents the model from simply predicting infinite variance to avoid all penalties; without it, the optimal strategy would be to always predict \\(\\sigma^2 \\to \\infty\\), making any prediction equally likely. The balance between these terms forces the model to predict variance that actually matches the empirical noise level. The result is a model that learns to predict larger variance for inputs where training labels are noisy or inconsistent, capturing aleatoric uncertainty in an input-dependent manner.\nHeteroscedastic models capture aleatoric uncertainty but not epistemic uncertainty. The predicted variance reflects noise inherent in the labels, not uncertainty about model parameters. Combining heteroscedastic outputs with ensemble methods provides estimates of both uncertainty types: ensemble disagreement captures epistemic uncertainty while the predicted variance captures aleatoric uncertainty.\n\n\n23.4.4 Evidential Deep Learning\nEvidential deep learning places a prior distribution over the class probabilities themselves rather than directly predicting probabilities. For classification, the model outputs parameters of a Dirichlet distribution, which serves as a prior over the simplex of class probabilities. The concentration parameters of this Dirichlet encode both the predicted class probabilities (via their relative magnitudes) and the model’s uncertainty (via their absolute magnitudes).\nLow total concentration indicates high uncertainty: the model is unsure which class is correct. High total concentration with one dominant class indicates confident prediction. This framework provides a principled way to separate epistemic uncertainty (low concentration) from confident predictions (high concentration), all from a single forward pass without ensembling or MC sampling.\nCritics have noted that evidential deep learning can produce unreliable uncertainty estimates when the distributional assumptions are violated or when training data is limited [Citation Needed]. Practical experience suggests that ensembles and MC dropout often provide more robust uncertainty estimates, though evidential methods continue to be refined.\n\n\n23.4.5 Selecting Uncertainty Quantification Methods\nThe choice among uncertainty quantification methods depends on computational constraints, the types of uncertainty relevant to the application, and the foundation model architecture.\nFor applications where distinguishing epistemic from aleatoric uncertainty matters, combining ensemble methods with heteroscedastic predictions provides both. Ensemble disagreement identifies variants where more training data might reduce uncertainty, while high predicted variance identifies variants where uncertainty is inherent to the prediction task.\nFor foundation model applications where full ensembles are impractical, last-layer ensembles offer the best trade-off between computational cost and uncertainty quality. The pretrained representations capture most of the model’s knowledge, and ensembling only the prediction heads captures uncertainty arising from the fine-tuning task.\nFor real-time applications requiring single forward passes, evidential deep learning or heteroscedastic models provide uncertainty estimates without inference-time overhead. These methods capture aleatoric uncertainty effectively but may underestimate epistemic uncertainty for out-of-distribution inputs.\nThe following table summarizes uncertainty quantification methods and their trade-offs:\n\n\n\nTable 23.4: Comparison of uncertainty quantification methods for genomic foundation models. M = number of ensemble members (typically 5-10); N = number of MC samples (typically 10-50).\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nEpistemic\nAleatoric\nTraining Cost\nInference Cost\nFoundation Model Compatibility\n\n\n\n\nDeep ensembles\nExcellent\nVia heteroscedastic variant\nM× base\nM× base\nExpensive for large models\n\n\nLast-layer ensembles\nGood\nVia heteroscedastic heads\n1× backbone + M× heads\nM× heads (cheap)\nPractical for any foundation model\n\n\nMC dropout\nModerate\nVia heteroscedastic variant\n1×\nN× forward passes\nRequires dropout in architecture\n\n\nHeteroscedastic\nNone\nExcellent\n1×\n1×\nEasy to add to any model\n\n\nEvidential\nModerate\nModerate\n1×\n1×\nRequires architectural changes",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-conformal",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-conformal",
    "title": "23  Uncertainty Quantification",
    "section": "23.5 Conformal Prediction: Distribution-Free Guarantees",
    "text": "23.5 Conformal Prediction: Distribution-Free Guarantees\nMost uncertainty quantification methods make assumptions about model behavior or data distributions that may not hold in practice. Temperature scaling assumes miscalibration follows a particular functional form. Ensembles assume that disagreement reflects epistemic uncertainty rather than artifacts of training. Bayesian methods assume specific priors over model parameters. When these assumptions fail, uncertainty estimates may be unreliable precisely when reliability matters most.\nConsider how a cautious doctor might communicate diagnostic uncertainty. Rather than saying “I am 73% confident this is condition A,” they might say “Based on your symptoms and test results, I can confidently rule out conditions C, D, and E, but I cannot yet distinguish between A and B—we need more information.” This approach sidesteps the difficulty of assigning precise probabilities by instead specifying which possibilities remain plausible. The size of this “plausible set” communicates uncertainty: a single remaining possibility indicates high confidence, while many remaining possibilities indicate low confidence.\nConformal prediction formalizes this intuition and offers something stronger: finite-sample coverage guarantees that hold under minimal assumptions. Instead of outputting a point prediction, conformal methods produce a prediction set guaranteed to contain the true label with probability at least \\(1 - \\alpha\\), where \\(\\alpha\\) is a user-specified error rate. If we request \\(90\\%\\) coverage (\\(\\alpha = 0.10\\)), the prediction set will contain the true label at least \\(90\\%\\) of the time, regardless of the model’s accuracy or calibration. This guarantee requires only that calibration and test examples are exchangeable (a condition weaker than independent and identically distributed), making conformal prediction robust to model misspecification.\n\n\n\n\n\n\nKey Insight: Prediction Sets Convey Uncertainty Without Probabilities\n\n\n\nConformal prediction sidesteps the need for well-calibrated probabilities. Instead of asking “what is the probability this variant is pathogenic?”, it answers “which classifications can we confidently rule out?” A prediction set of {Pathogenic} means high confidence. A set of {Pathogenic, VUS, Benign} means low confidence. The size of the set is the uncertainty estimate, and the coverage guarantee holds regardless of model calibration.\n\n\n\n\n\n\n\n\nConformal prediction provides coverage-guaranteed prediction sets\n\n\n\n\nFigure 23.5: Conformal prediction for coverage-guaranteed uncertainty. The procedure uses a calibration set to establish a score threshold such that prediction sets cover the true label with probability at least 1−α. Unlike point predictions, conformal sets communicate uncertainty through their size: well-characterized variants receive small sets, while rare variants receive larger sets that honestly reflect limited knowledge.\n\n\n\n\n23.5.1 Split Conformal Prediction\n\n\n\n\n\n\nDifficulty Warning\n\n\n\nThis section introduces the mathematical framework for conformal prediction, including quantile computations and non-conformity scores. The key intuition is simpler than the formalism: use a held-out calibration set to learn what scores are “normal,” then flag test predictions as uncertain if they look unusual relative to calibration. Focus on the algorithm steps if the probability theory is unfamiliar.\n\n\nThe most practical conformal method, split conformal prediction, begins by partitioning labeled data into training and calibration subsets. After training the model exclusively on the training portion, non-conformity scores are computed for each calibration example, where higher scores indicate poorer agreement between prediction and true label. The threshold \\(q\\) is then set at the \\((1-\\alpha)(1+1/n)\\) quantile of these calibration scores. At test time, the prediction set includes all labels whose non-conformity score falls below this threshold.\nNon-conformity scores measure how “strange” a candidate label is given the model’s output. For classification, a common choice is \\(1 - \\hat{p}_y\\), where \\(\\hat{p}_y\\) is the predicted probability of the true class. High predicted probability means low non-conformity (the label conforms to the model’s expectations); low predicted probability means high non-conformity. For regression, absolute residuals \\(|y - \\hat{y}|\\) serve as non-conformity scores.\nThe construction ensures coverage because calibration scores are exchangeable with test scores under the exchangeability assumption. The quantile threshold is set so that a random calibration score exceeds the threshold with probability at most \\(\\alpha\\); by exchangeability, the same holds for test scores. This elegant argument yields exact coverage guarantees without requiring the model to be accurate or well-calibrated.\nThe coverage guarantee is finite-sample: it holds exactly for any sample size, not just asymptotically. For clinical genomics applications where individual predictions carry significant consequences, this finite-sample property provides assurance that cannot be obtained from asymptotic calibration arguments.\n\n\n23.5.2 Conformal Prediction for Variant Classification\nVariant effect prediction, examined in detail in Chapter 17, concentrates the challenges of uncertainty quantification. Instead of reporting a single pathogenicity score, a conformalized variant classifier outputs a prediction set from the possibilities: {pathogenic}, {benign}, {pathogenic, benign}, or the empty set. The set is guaranteed to contain the true label at the specified coverage rate.\nSet size conveys uncertainty without requiring probability interpretation. A singleton prediction set indicates high confidence: the model has enough information to narrow to a single class. A set containing multiple classes indicates uncertainty: the model cannot confidently distinguish between possibilities. The empty set indicates extreme uncertainty where even the most permissive threshold cannot be satisfied.\nThe trade-off between coverage and informativeness shapes practical deployment. At 99% coverage, prediction sets will frequently include multiple classes, providing reliable but uninformative predictions. At 80% coverage, prediction sets will more often be singletons, providing informative but less reliable predictions. Stakeholders must choose coverage levels that match their tolerance for error versus the cost of uninformative predictions.\n\n\n23.5.3 Limitations and Practical Considerations\nConformal prediction provides marginal coverage guarantees: averaged over all inputs, 90% of prediction sets will contain the true label. This does not guarantee conditional coverage for any particular subgroup. A model might achieve 90% coverage overall while providing only 70% coverage for rare variant classes or underrepresented populations. Subgroup-stratified coverage assessment reveals these disparities, though achieving conditional coverage guarantees requires stronger assumptions or larger calibration datasets.\nThe exchangeability assumption can fail in practice. If the calibration set derives from one population and the test set from another, coverage guarantees may not hold. Temporal shifts (calibration on historical data, testing on future data) similarly violate exchangeability. Methods for conformal prediction under distribution shift exist but require additional assumptions about the nature of the shift.\nPrediction set size trades off against informativeness. Larger sets provide more reliable coverage but less useful predictions. A model that produces {pathogenic, benign} for every variant achieves perfect coverage but provides no discrimination. Careful model development to improve underlying accuracy reduces average set size while maintaining coverage guarantees.\n\n\n23.5.4 Integration with Clinical Workflows\nConformal prediction sets integrate naturally with existing variant classification frameworks. The ACMG-AMP guidelines already accommodate uncertainty through categories like “variant of uncertain significance.” Conformal sets provide a principled basis for this categorization: variants receiving singleton sets ({pathogenic} or {benign}) have strong computational evidence, while variants receiving larger sets have uncertain computational evidence. The ACMG-AMP framework and its integration with computational evidence are discussed in Chapter 28.\nThe coverage guarantee provides a quantitative basis for laboratory policies. A laboratory might decide that computational evidence should achieve 95% coverage before contributing to variant classification, using conformal methods to verify this threshold is met. The guarantee holds regardless of which specific variants are encountered, providing assurance that the policy will perform as intended across the laboratory’s case mix.\nConformal methods also enable selective prediction, where the model abstains rather than producing uncertain predictions. By setting coverage requirements appropriately, laboratories can identify variants where computational methods provide reliable evidence and variants where human review is essential. This selective approach focuses expert attention where it is most needed while allowing automated processing of straightforward cases.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-ood-detection",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-ood-detection",
    "title": "23  Uncertainty Quantification",
    "section": "23.6 Out-of-Distribution Detection",
    "text": "23.6 Out-of-Distribution Detection\n\n\n\n\n\n\n\n\nThe OOD problem: models extrapolate confidently into unreliable regions\n\n\n\n\n\n\n\nDetection methods: from softmax thresholds to embedding approaches\n\n\n\n\n\n\n\nGenomic OOD scenarios requiring detection before clinical use\n\n\n\n\n\n\nFigure 23.6: Out-of-distribution detection for safe deployment. (A) The OOD problem: models extrapolate confidently into regions where predictions are unreliable. (B) Detection methods: from simple softmax thresholds to sophisticated embedding-space approaches. (C) Genomic OOD scenarios: novel genes, underrepresented populations, unusual variant types, mismatched tissues, and technical batch effects—all require detection before clinical use.\n\n\n\n\n23.6.1 Out-of-Distribution Problem\nA DNA language model trained on mammalian genomes encounters a novel archaeal sequence. The model’s embedding places this sequence in an unfamiliar region of representation space, far from the clusters formed by training examples. Yet the model still produces a prediction, potentially with high confidence, because standard neural networks are not designed to recognize when inputs lie outside their training distribution. Detecting out-of-distribution (OOD) inputs is essential for safe deployment of foundation models in settings where novel sequences are inevitable.\nOOD detection identifies inputs that differ meaningfully from training data, allowing systems to flag uncertain predictions before they cause harm. Novel pathogens may share little sequence similarity with characterized viruses in training data. Synthetic proteins designed for therapeutic purposes may occupy regions of sequence space unsampled by evolution. Variants in poorly characterized genes may lack the contextual information that models rely on for accurate prediction. In each case, recognizing that the input is unusual enables appropriate caution.\nThe confidence problem compounds OOD challenges. Neural networks often produce high-confidence predictions on OOD inputs because nothing in standard training penalizes confidence on unfamiliar examples. A classifier trained to distinguish pathogenic from benign variants may confidently predict “pathogenic” for a completely random sequence, not because it has evidence for pathogenicity but because it lacks the capacity to say “I do not know.” This failure mode makes OOD detection essential rather than optional.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA protein language model is deployed to score variants in a newly discovered gene with no homologs in UniRef. The model returns high-confidence pathogenicity predictions. What concerns should you have? What additional information would help you assess whether to trust these predictions?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThis gene is out-of-distribution relative to the model’s training data—high epistemic uncertainty is expected. The confident predictions are a red flag: the model may be extrapolating unreliably beyond its training experience. You should check embedding distance to training examples, ensemble disagreement if available, and whether the model’s confidence is calibrated for truly novel protein families. Experimental validation through functional assays would be essential before acting on these predictions.\n\n\n\n\n\n\n\n23.6.2 Likelihood-Based Detection and Its Failures\nThe intuitive approach to OOD detection uses model likelihood: inputs the model finds improbable should be flagged as OOD. Language models assign likelihoods to sequences; surely OOD sequences should receive low likelihood?\nThis intuition fails for deep generative models. Complex models can assign high likelihood to OOD data for reasons unrelated to semantic similarity to training examples. In high-dimensional spaces, typical sets (regions where most probability mass concentrates) do not coincide with high-density regions. A sequence might land in a high-density region of the model’s distribution while being semantically distant from any training example.\nEmpirically, language models assign high likelihood to repetitive sequences, sequences with unusual but consistent patterns, and sequences from different domains that happen to share statistical properties with training data [Citation Needed]. For genomic models, this means likelihood alone cannot reliably distinguish novel biological sequences from sequences within the training distribution.\n\n\n23.6.3 Embedding-Based Detection\nLearned representations provide more reliable OOD detection than raw likelihood. The key insight is that embeddings encode semantic structure: similar sequences cluster together in embedding space, and OOD sequences land in sparse regions distant from training clusters.\nMahalanobis distance measures how far a test embedding lies from training data, accounting for the covariance structure of the embedding space. For each class, compute the mean embedding and covariance matrix from training examples. For a test input, compute its distance to each class centroid in units of standard deviations, accounting for correlations between embedding dimensions. Large Mahalanobis distance indicates OOD inputs.\nWhy Mahalanobis rather than simple Euclidean distance? Euclidean distance treats all embedding dimensions equally, but learned representations often have highly correlated dimensions and vary substantially in scale. A test point that appears close in Euclidean terms might actually be unusual because it deviates along a low-variance direction where training examples are tightly clustered. Mahalanobis distance normalizes by the covariance structure, detecting such deviations by asking “how many standard deviations away is this point along each principal axis of variation?”\nNearest-neighbor methods provide a non-parametric alternative. For a test embedding, find the \\(k\\) nearest neighbors among training embeddings and compute the average distance. Large average distance to neighbors indicates the test input lies in a sparse region of embedding space, suggesting it is OOD. This approach makes no distributional assumptions and scales well with modern approximate nearest-neighbor algorithms.\nFor genomic foundation models, embedding-based OOD detection enables practical deployment safeguards. ESM embeddings place novel protein folds in regions distant from characterized folds, allowing detection of sequences outside the model’s training experience. DNABERT embeddings reveal unusual sequence composition or repeat structures that may confound predictions. Flagging these cases for expert review prevents confident but unreliable predictions from reaching clinical decisions. The properties of DNA and protein language model embeddings are discussed in Chapter 14 and Chapter 15.\n\n\n23.6.4 Practical OOD Detection for Genomic Applications\nDefining what counts as OOD requires domain knowledge. Novel species or clades may share evolutionary history with training examples yet differ enough to warrant caution. Extreme GC content can indicate contamination, unusual biology, or simply under-represented genomic regions. Engineered sequences (designed proteins, synthetic regulatory elements) intentionally explore regions of sequence space not represented in natural sequences.\nCombining multiple OOD signals improves reliability. Embedding distance, likelihood, and prediction confidence each capture different aspects of distributional difference. An input flagged by multiple methods is more reliably OOD than one flagged by a single method. Threshold selection involves trade-offs between false positives (flagging in-distribution examples unnecessarily) and false negatives (missing true OOD examples).\nThe operational response to OOD detection depends on the application. For variant interpretation, OOD inputs might trigger automatic flagging for expert review rather than automated classification. For high-throughput screening, OOD inputs might receive tentative predictions with explicit uncertainty warnings. For safety-critical applications, OOD inputs might trigger rejection with a request for additional information.\n\n\n\n\n\n\nPractical Guidance: OOD Detection Pipeline\n\n\n\nFor deploying foundation models with OOD safeguards:\n\nStore training embeddings or a compressed representation (centroids, covariance) during model development\nCompute embedding distance for each test input using Mahalanobis or k-NN distance\nSet threshold based on desired false positive rate on a held-out validation set\nFlag OOD inputs for human review rather than automated processing\nLog and monitor OOD rates over time; increasing rates may signal distribution shift\nCombine signals (embedding distance, ensemble disagreement, confidence) for robust detection",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-selective-prediction",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-selective-prediction",
    "title": "23  Uncertainty Quantification",
    "section": "23.7 Selective Prediction and Abstention",
    "text": "23.7 Selective Prediction and Abstention\n\n\n\n\n\n\nSelective prediction enables tiered clinical workflows\n\n\n\n\nFigure 23.7: Selective prediction for clinical deployment. Rather than forcing predictions on all cases, models abstain when uncertainty exceeds a threshold, routing uncertain cases to human review. The coverage-accuracy tradeoff determines what fraction of cases are automated: higher accuracy requires accepting fewer cases. This enables tiered clinical workflows where confident predictions proceed automatically while uncertain cases receive expert attention.\n\n\n\n\n23.7.1 When to Abstain\nA variant effect predictor achieving 95% accuracy overall provides more clinical value if it can identify which predictions are reliable. Selective prediction allows models to abstain on difficult cases, concentrating predictions on inputs where confidence is warranted. The trade-off between coverage (fraction of inputs receiving predictions) and accuracy (correctness among predictions made) defines the selective prediction problem.\nThe coverage-accuracy trade-off reflects a fundamental tension. At 100% coverage, the model predicts on all inputs and achieves its baseline accuracy. As coverage decreases (more abstention), accuracy among predictions made typically increases because the model abstains on its most uncertain cases. The shape of this trade-off curve characterizes the model’s ability to identify reliable predictions.\nAbstention is appropriate when the cost of errors exceeds the cost of deferral. In clinical variant interpretation, a confident but incorrect pathogenic prediction may trigger unnecessary medical intervention, while abstention merely defers the decision to expert review. If expert review is available and affordable relative to error costs, abstaining on uncertain cases improves overall decision quality. Conversely, in high-throughput screening where expert review is infeasible, abstention may provide little benefit because all predictions eventually require automated handling.\n\n\n23.7.2 Selective Prediction Methods\nConfidence-based selection abstains when the model’s maximum predicted probability falls below a threshold. For a classifier producing probabilities over classes, if max_c *p̂_c* &lt; τ, the model abstains. This simple approach works well when model confidence correlates with correctness, but fails when models are confidently wrong.\nEnsemble-based selection abstains when ensemble members disagree beyond a threshold. High disagreement indicates epistemic uncertainty about the correct prediction, warranting abstention even if individual members express confidence. This approach captures uncertainty that confidence-based selection misses when models are overconfident.\nConformal selection abstains when prediction sets exceed a size threshold. If the conformal prediction set contains more than one class, the model lacks confidence to make a unique prediction. This approach connects selective prediction to the coverage guarantees of conformal methods: the model makes predictions with guaranteed coverage on the non-abstained cases.\nLearned selection trains a separate model to predict whether the primary model will be correct on each input. This “rejection model” learns to identify failure modes that simple confidence thresholds miss, potentially achieving better coverage-accuracy trade-offs than heuristic methods.\n\n\n23.7.3 Evaluating Selective Prediction\nRisk-coverage curves plot accuracy (or its complement, risk) as a function of coverage, revealing how performance improves as the model becomes more selective. The area under the risk-coverage curve summarizes overall selective prediction quality. Models with better uncertainty estimates produce steeper curves, achieving high accuracy at lower coverage.\nSelective accuracy at fixed coverage specifies a coverage level (e.g., 80%) and reports accuracy among predictions made at that coverage. This metric directly answers practical questions: “If we let the model predict on its 80% most confident cases, how accurate will it be?”\nComparison across methods requires matched coverage levels. A method that achieves 99% accuracy at 50% coverage and 95% accuracy at 90% coverage may be preferable to a method achieving 97% accuracy at both levels, depending on operational requirements. Reporting full risk-coverage curves enables stakeholders to select operating points appropriate to their cost structures.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-genomic-uq",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-genomic-uq",
    "title": "23  Uncertainty Quantification",
    "section": "23.8 Uncertainty for Specific Genomic Tasks",
    "text": "23.8 Uncertainty for Specific Genomic Tasks\nThe general principles of uncertainty quantification apply differently across genomic prediction tasks. Variant effect prediction, regulatory variant interpretation, and cross-population generalization each present distinct challenges for calibration, coverage, and out-of-distribution detection. The sources of uncertainty vary: coding variants benefit from stronger evolutionary constraint and clearer functional readouts, while regulatory variants operate through context-dependent mechanisms that introduce irreducible noise. Population-specific uncertainty reflects training data composition and has direct implications for equitable clinical deployment.\n\n23.8.1 Variant Effect Prediction Uncertainty\nVariant effect prediction concentrates the challenges of uncertainty quantification. Epistemic uncertainty arises from poorly characterized genes, novel protein folds, and under-represented populations in training data. Aleatoric uncertainty stems from incomplete penetrance, variable expressivity, and noise in functional assay labels. Both types of uncertainty must be estimated and communicated for variant predictions to inform clinical decisions appropriately. The technical details of variant effect prediction models are covered in Chapter 17.\nCalibration challenges for VEP include the evolving nature of ground truth labels. ClinVar annotations change as new evidence emerges; variants classified as VUS may be reclassified as pathogenic or benign, and even confident classifications occasionally reverse. A model calibrated against a historical version of ClinVar may appear miscalibrated against current annotations, not because the model changed but because the labels did. Periodic recalibration against current databases maintains alignment between model outputs and contemporary clinical understanding.\nPopulation-specific calibration addresses the reality that training data predominantly derive from European-ancestry cohorts. For patients from other ancestral backgrounds, both epistemic uncertainty (fewer training examples) and calibration (different baseline pathogenicity rates, different patterns of variation) may differ from the aggregate. Stratified reliability diagrams by ancestry reveal these differences; ancestry-conditional calibration may be necessary for equitable performance across populations. The governance and policy dimensions of ensuring equitable uncertainty communication are addressed in Section 26.1.\n\n\n23.8.2 Regulatory Variant Uncertainty\nRegulatory variants present distinct uncertainty challenges. Unlike coding variants where effects can be localized to specific amino acid changes, regulatory variants act through complex mechanisms involving transcription factor binding, chromatin accessibility, and three-dimensional genome organization. This mechanistic complexity translates to higher aleatoric uncertainty: even perfectly characterized regulatory variants may have context-dependent effects that vary across cell types, developmental stages, and genetic backgrounds. A variant that disrupts a transcription factor binding site may have dramatic effects in tissues where that factor is active and negligible effects elsewhere, yet the model must predict across all contexts simultaneously. The architecture and capabilities of regulatory prediction models are discussed in Chapter 16.\nThe context-dependence of regulatory effects creates a calibration challenge distinct from coding variants. A model may be well-calibrated for predicting expression changes in cell types abundant in training data (lymphoblastoid cell lines, common cancer lines) while poorly calibrated for clinically relevant primary tissues rarely profiled at scale. Stratified calibration assessment across tissue types reveals these disparities, but the sparsity of ground truth labels for many tissues limits the precision of tissue-specific calibration estimates.\nExpression prediction models like Enformer and Borzoi provide uncertainty estimates for predicted expression changes through several approaches. Ensemble methods quantify disagreement across model variants trained with different random seeds. Heteroscedastic architectures predict tissue-specific confidence alongside tissue-specific expression, learning that predictions for well-characterized tissues deserve higher confidence than those for rarely profiled contexts. These uncertainties propagate to downstream interpretations: a variant predicted to alter expression with high uncertainty warrants different treatment than one with narrow confidence bounds, and the tissue-specificity of uncertainty may itself be informative about which experimental follow-up would most reduce ambiguity.\n\n\n23.8.3 Uncertainty Across Populations\nDifferential uncertainty across populations has direct implications for health equity. Models trained predominantly on European-ancestry data exhibit higher epistemic uncertainty for other populations, manifesting in several observable ways: larger prediction sets from conformal methods, higher abstention rates from selective prediction, greater ensemble disagreement, and less reliable confidence estimates from calibration. These differences arise from multiple sources. Linkage disequilibrium patterns differ across populations, meaning that variant correlations learned from European data may not transfer. Population-specific variants absent from training data generate pure epistemic uncertainty. Even shared variants may have different effect sizes across populations due to gene-environment interactions or epistatic backgrounds that vary by ancestry.\nQuantifying population-specific uncertainty requires appropriate calibration and evaluation datasets. A model calibrated exclusively on European-ancestry ClinVar submissions may appear well-calibrated on aggregate metrics while being systematically miscalibrated for other populations. The scarcity of diverse calibration data creates a challenging circularity: we cannot assess population-specific calibration without diverse labeled datasets, yet diverse labeled datasets are precisely what current genomic databases lack. Initiatives like the All of Us Research Program and population-specific biobanks (Uganda Genome Resource, Taiwan Biobank, BioBank Japan) are beginning to address this gap, enabling population-stratified uncertainty assessment that was previously impossible. The broader context of biobank resources and their composition is discussed in Section 2.3.\nTransparent reporting of population-stratified uncertainty metrics enables informed decisions about model deployment. If a model abstains on 30% of variants in one population but only 10% in another, users can make informed choices about supplementary analyses for the higher-abstention population. Clinical laboratories might establish ancestry-specific thresholds for automated reporting versus expert review. Research applications might weight predictions by ancestry-specific confidence when aggregating across diverse cohorts. Ignoring these differences risks providing lower-quality predictions to already under-served populations while presenting a false appearance of uniform reliability, compounding existing disparities in genomic medicine.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-communication",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-communication",
    "title": "23  Uncertainty Quantification",
    "section": "23.9 Communicating Uncertainty to End Users",
    "text": "23.9 Communicating Uncertainty to End Users\nStatistical uncertainty estimates serve clinical decisions only when they reach end users in interpretable form. The translation from model outputs to actionable information involves choices about categorical versus continuous reporting, numerical versus visual presentation, and whether to frame results as probabilities or as expected outcomes under alternative decisions. Different stakeholders require different presentations: clinicians need actionable categories, researchers need distributional information, and patients need accessible risk communication.\n\n23.9.1 Communication Challenge\nA pathogenicity score of 0.73 ± 0.15 may be statistically accurate but nearly useless to a clinician deciding whether to order confirmatory testing. The gap between statistical uncertainty and decision-relevant communication presents a persistent challenge for genomic AI deployment. Different users reason differently about probability and risk; effective communication requires understanding these differences.\nCognitive biases complicate probability interpretation. Humans tend toward overconfidence in point estimates, treating 0.73 as more certain than warranted. Prediction intervals are frequently misunderstood: a 90% confidence interval does not mean the true value has a 90% chance of being in that specific interval (a Bayesian interpretation) but rather that 90% of such intervals would contain the true value (a frequentist interpretation). Base rate neglect leads users to interpret variant-level pathogenicity predictions without accounting for prior probability based on clinical presentation, family history, and phenotypic specificity.\nDifferent stakeholders have different needs. Clinicians require actionable categories that map to clinical decision points, not continuous scores requiring interpretation. Researchers may prefer full probability distributions enabling flexible downstream analysis. Patients and families need understandable risk communication that supports informed decision-making without inducing inappropriate anxiety or false reassurance.\n\n\n23.9.2 Categorical Reporting\nClinical genetics has established categorical frameworks for variant interpretation. The ACMG-AMP guidelines define five categories: pathogenic, likely pathogenic, variant of uncertain significance, likely benign, and benign. The complete ACMG-AMP framework, including how computational evidence integrates with other evidence types, is examined in ?sec-ch26-acmg-amp. Mapping continuous model outputs to these categories requires threshold selection that balances sensitivity and specificity at clinically meaningful operating points, with guidance on calibrating model outputs to ACMG evidence strength provided in ?sec-ch14-acmg-mapping.\nUncertainty within categories can be conveyed through confidence qualifiers or numerical confidence scores attached to categorical calls. A “likely pathogenic” call with 95% confidence differs meaningfully from one with 60% confidence, even though both receive the same categorical label. Two-dimensional reporting combining category and confidence enables more nuanced interpretation without abandoning the categorical framework that clinicians expect.\nThreshold selection involves value judgments beyond pure statistics. The consequences of false positive and false negative pathogenic calls differ by clinical context. For a severe, treatable condition, false negatives carry higher cost, warranting lower thresholds for pathogenic classification. For untreatable conditions where pathogenic classification affects reproductive decisions, the calculus differs. Uncertainty quantification enables informed threshold selection by revealing the trade-offs at different operating points.\n\n\n23.9.3 Visual Communication\nProbability bars and confidence intervals provide visual representation of uncertainty, though their interpretation depends on user familiarity with statistical graphics. Icon arrays, which represent probabilities as proportions of colored icons in a grid (e.g., 73 red icons and 27 blue icons out of 100), improve comprehension for users without statistical training. The visual representation of proportion is more intuitive than numerical probability for many audiences.\nRisk ladders place the prediction in context by showing where it falls relative to other risks of varying magnitude. A variant with 0.73 probability of pathogenicity can be placed alongside risks from other genetic conditions, environmental exposures, or common medical procedures, enabling intuitive comparison.\nInteractive visualizations allow users to explore uncertainty in detail, examining how predictions change under different assumptions or how uncertainty varies across related variants. These approaches suit sophisticated users engaged in research or detailed clinical analysis but may overwhelm users seeking simple answers.\n\n\n23.9.4 Decision-Theoretic Framing\nRather than communicating probability alone, decision-theoretic framing presents expected outcomes under different actions. Instead of “this variant has 73% probability of being pathogenic,” the report might state “if we assume this variant is pathogenic and proceed with surveillance, the expected outcomes are X; if we assume it is benign and decline surveillance, the expected outcomes are Y.”\nThis framing integrates uncertainty with action, helping users understand how uncertainty affects what they should do rather than treating probability as an end in itself. The approach requires modeling clinical outcomes, which introduces additional assumptions, but makes explicit the decision-relevant implications of uncertainty rather than leaving users to integrate probability with consequences on their own.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch23-uncertainty.html#sec-ch23-conclusion",
    "href": "part_5/p5-ch23-uncertainty.html#sec-ch23-conclusion",
    "title": "23  Uncertainty Quantification",
    "section": "23.10 Necessary but Insufficient",
    "text": "23.10 Necessary but Insufficient\nUncertainty quantification transforms foundation model outputs from opaque scores into components of rational decision processes. A well-calibrated pathogenicity prediction that honestly communicates its limitations enables appropriate clinical reasoning: high confidence warrants action, low confidence warrants additional testing or expert review. An overconfident score that claims false precision causes harm through both false positives (unnecessary interventions) and false negatives (missed diagnoses). Temperature scaling, conformal prediction, and out-of-distribution detection together provide the technical foundation for trustworthy genomic AI.\nThe path from uncertainty quantification to clinical impact requires integrating these methods into operational workflows. Selective prediction enables triage between automated handling and expert review based on model confidence. Conformal prediction sets provide coverage guarantees that support risk-aware decision-making. Out-of-distribution detection prevents confident predictions on inputs that fall outside the training distribution, a particularly important capability given the confounding issues examined in Chapter 12. Calibration ensures that numerical probabilities mean what they claim to mean. Together, these tools enable foundation models to participate in clinical decisions without overstating their reliability. Clinical risk prediction frameworks (Chapter 27) develop these tools further for deployment contexts, while rare disease workflows (Chapter 28) apply them to diagnostic interpretation.\nYet uncertainty quantification alone is insufficient. A perfectly calibrated black box remains a black box. The clinician who receives an uncertain prediction wants to understand why the model is uncertain: Is it because the variant falls in a poorly characterized gene? Because the model has never encountered this protein fold? Because the underlying biology is genuinely ambiguous? Interpretability, examined in Chapter 24, complements uncertainty by revealing the basis for predictions and their associated confidence. Attribution methods (Section 24.1) identify which input features drive predictions; probing classifiers (Section 24.4) reveal what information representations encode. The conjunction of calibrated uncertainty and mechanistic understanding approaches what trustworthy clinical AI requires. Neither alone suffices; together they provide the foundation for models that clinicians can reason with rather than merely defer to.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat is the difference between epistemic and aleatoric uncertainty, and why does this distinction matter for clinical decision-making?\nWhat is model calibration, and why can a highly accurate model still be dangerously miscalibrated?\nExplain how temperature scaling improves calibration. What does it preserve and what does it change?\nHow do deep ensembles quantify epistemic uncertainty, and what makes them the “gold standard” among uncertainty quantification methods?\nWhat coverage guarantee does conformal prediction provide, and why does marginal coverage differ from conditional coverage?\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nKey Concepts:\n\nEpistemic vs. aleatoric uncertainty: Epistemic uncertainty arises from limited training data and can be reduced with more examples; aleatoric uncertainty reflects inherent biological noise and is irreducible\nCalibration: A model is calibrated when its predicted probabilities match observed frequencies; most foundation models are miscalibrated and require post-hoc correction\nPost-hoc calibration methods: Temperature scaling (simple, preserves ranking), Platt scaling (handles bias), and isotonic regression (flexible, requires more data)\nUncertainty quantification methods: Deep ensembles (gold standard), MC dropout (single model), heteroscedastic models (aleatoric), last-layer ensembles (practical for foundation models)\nConformal prediction: Distribution-free coverage guarantees; prediction set size conveys uncertainty; marginal not conditional coverage\nOOD detection: Embedding-based methods more reliable than likelihood; flag unusual inputs for expert review\nSelective prediction: Abstain on uncertain cases to improve accuracy on retained predictions\n\nClinical Implications:\n\nCalibration must be assessed within subgroups (ancestry, gene family) to avoid hidden disparities\nPopulation-specific uncertainty has direct health equity implications\nUncertainty communication should match stakeholder needs (categorical for clinicians, distributional for researchers)\nUncertainty quantification is necessary but insufficient; interpretability complements it\n\nConnections to Other Chapters:\n\nBuilds on evaluation principles (Chapter 12) and pretraining objectives (Chapter 8)\nPrecedes interpretability (Chapter 24) which explains why models are uncertain\nApplies to clinical risk prediction (Chapter 27) and rare disease diagnosis (Chapter 28)\nIntersects with fairness concerns (Section 3.7.2, Section 26.1)\n\n\n\n\n\n\n\nFowler, Douglas M., and Stanley Fields. 2014. “Deep Mutational Scanning: A New Style of Protein Science.” Nature Methods 11 (8): 801–7. https://doi.org/10.1038/nmeth.3027.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In Proceedings of The 33rd International Conference on Machine Learning, 1050–59. PMLR. https://proceedings.mlr.press/v48/gal16.html.\n\n\nGuo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. “On Calibration of Modern Neural Networks.” In Proceedings of the 34th International Conference on Machine Learning, 1321–30. PMLR. https://proceedings.mlr.press/v70/guo17a.html.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. “ClinVar: Improving Access to Variant Interpretations and Supporting Evidence.” Nucleic Acids Research 46 (D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nRubin, Alan F., Hannah Gelman, Nathan Lucas, Sandra M. Bajjalieh, Anthony T. Papenfuss, Terence P. Speed, and Douglas M. Fowler. 2017. “A Statistical Framework for Analyzing Deep Mutational Scanning Data.” Genome Biology 18 (1): 150. https://doi.org/10.1186/s13059-017-1272-5.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Uncertainty Quantification</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html",
    "href": "part_5/p5-ch24-interpretability.html",
    "title": "24  Interpretability",
    "section": "",
    "text": "24.1 Attribution Methods and Input Importance\nWhen a model predicts that a 200-kilobase genomic region will show high chromatin accessibility in hepatocytes, a natural question arises: which bases within that region drive the prediction? Attribution methods answer this question by assigning importance scores to input positions, identifying where the model focuses its computational attention. These scores can reveal candidate regulatory elements, highlight the sequence features underlying variant effects, and provide the raw material for downstream motif discovery.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-attribution",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-attribution",
    "title": "24  Interpretability",
    "section": "",
    "text": "Attribution methods comparison on the same genomic sequence\n\n\n\n\nFigure 24.1: Attribution methods comparison on a single genomic sequence. From top: Gradient × Input (fast but noisy), Integrated Gradients (principled with theoretical guarantees), DeepLIFT (reference-based attributions), attention weights (model attention ≠ importance), and in silico mutagenesis (ground truth but 3L forward passes). Regions where methods agree provide high-confidence importance; disagreement flags positions for closer investigation.\n\n\n\n\n24.1.1 In Silico Mutagenesis\nThe most direct approach to measuring input importance is simply to change each base and observe what happens to the prediction. In silico mutagenesis (ISM) systematically introduces mutations at every position, computing the difference between mutant and reference predictions. For a sequence of length L, ISM creates three mutant sequences at each position (substituting each non-reference nucleotide), yielding 3L forward passes through the model. The resulting mutation effect matrix captures how sensitive the prediction is to changes at each position and to each alternative base.\n\n\n\n\n\n\n\n\nProcedure: substitute each position to all alternatives\n\n\n\n\n\n\n\nVisualization: position × mutation heatmap\n\n\n\n\n\n\n\n\n\nValidation against deep mutational scanning\n\n\n\n\n\n\n\nMechanistic insights from ISM profiles\n\n\n\n\n\n\nFigure 24.2: In silico mutagenesis (ISM). (A) Procedure: substitute each position to all alternatives, compute prediction difference. (B) Visualization: position × mutation heatmap with functional regions showing large effects. (C) Validation: ISM predictions correlate with experimental deep mutational scanning (r ≈ 0.6-0.8). (D) Mechanistic insights: ISM reveals binding site boundaries, position-specific tolerance, and allele-specific effects.\n\n\n\nISM provides true counterfactual information rather than approximations. When ISM shows that mutating position 47 from A to G reduces the predicted accessibility by 0.3 log-fold, that is a direct observation about model behavior, not an estimate derived from gradients or attention weights. This directness makes ISM the gold standard for faithfulness: if ISM identifies a position as important, perturbing that position genuinely changes the output.\nThe limitation is computational cost. Scoring all single-nucleotide substitutions in a 200-kilobase input requires 600,000 forward passes, which becomes prohibitive for large models or genome-wide analysis. Practical applications often restrict ISM to targeted windows around variants of interest, using faster methods to identify candidate regions for detailed analysis. For variant effect prediction specifically, ISM reduces to comparing reference and alternative allele predictions, requiring only two forward passes per variant. This forms the computational basis for zero-shot variant scoring in foundation models (?sec-ch14-zeroshot-supervised), where the difference between wild-type and mutant log-likelihoods directly measures predicted effect.\n\n\n\n\n\n\nStop and Think: Attribution Method Tradeoffs\n\n\n\nBefore reading the next section on gradient-based methods, consider: if ISM provides the most faithful importance scores, why would we ever use anything else? What properties would an alternative method need to be useful in practice?\nHint: Think about computational cost, but also about what types of patterns each method can and cannot detect.\n\n\n\n\n24.1.2 Gradient-Based Attribution\nGradient-based methods approximate the counterfactual information from ISM using backpropagation. The gradient of the output with respect to each input position measures how much an infinitesimal change at that position would affect the prediction. With one-hot encoded sequence, the gradient at each base indicates the sensitivity to substituting that nucleotide.\nThe simplest approach, often called saliency mapping, computes raw gradients and visualizes their magnitudes across the sequence. A common variant multiplies gradients by inputs (gradient \\(\\times\\) input), focusing on positions where the current nucleotide is both important and present. These methods require only a single backward pass, making them orders of magnitude faster than ISM.\nGradient-based methods suffer from saturation in regions where the model is already confident. If a strong motif drives the prediction into a saturated region of the output nonlinearity, small perturbations produce near-zero gradients even though the motif is functionally critical. DeepLIFT addresses this limitation by comparing activations between an input sequence and a reference, propagating differences through the network using custom rules that avoid gradient saturation. The resulting attributions satisfy a completeness property: contributions sum to the difference between input and reference predictions (Shrikumar, Greenside, and Kundaje 2017).\n\n\n\n\n\n\nKey Insight: The Saturation Problem\n\n\n\nA critical limitation of gradient-based methods is saturation: when a model is highly confident in its prediction, gradients become very small even for positions that are functionally essential. This happens because gradients measure sensitivity to infinitesimal changes, but a saturated sigmoid or softmax barely changes regardless of the perturbation size. A GATA motif that drives 95% of the prediction might show near-zero gradients because the model is already “certain.” This is why ISM (which measures finite perturbation effects) often reveals importance that gradients miss.\n\n\nIntegrated gradients provide theoretical grounding through the path integral of gradients along a linear interpolation from reference to input (Sundararajan, Taly, and Yan 2017):\n\\[\\text{IG}_i(x) = (x_i - x'_i) \\int_{\\alpha=0}^{1} \\frac{\\partial f(x' + \\alpha(x - x'))}{\\partial x_i} \\, d\\alpha\\]\nThis integral, approximated by summing gradients at discrete interpolation steps, satisfies sensitivity (any input that affects the output receives nonzero attribution) and implementation invariance (functionally equivalent networks produce identical attributions). Integrated gradients have become a standard choice for genomic models, balancing computational efficiency with theoretical guarantees.\nAll gradient-based methods require choosing a reference sequence, which substantially affects the resulting attributions. Common choices include dinucleotide-shuffled versions of the input (preserving local composition while disrupting motifs), average non-functional sequence, or simply zeros. The reference defines what counts as informative: attributions highlight features that differ from the reference and contribute to the prediction difference. A shuffled reference emphasizes motif content; a zero reference treats any sequence information as potentially important.\nThe following table summarizes the key properties of attribution methods to help guide method selection:\n\n\n\nTable 24.1: Comparison of attribution methods for genomic sequence models. Faithfulness indicates how accurately the method reflects true model behavior; computational cost scales with sequence length L.\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nForward Passes\nFaithfulness\nLimitations\nBest Use Case\n\n\n\n\nISM\n3L per sequence\nHigh (direct measurement)\nComputationally expensive\nValidating importance in targeted regions\n\n\nGradient \\(\\times\\) Input\n1 backward pass\nLow-Medium\nSaturation, local approximation\nFast initial screening\n\n\nDeepLIFT\n1 pass (custom)\nMedium\nReference-dependent\nAttribution with completeness guarantees\n\n\nIntegrated Gradients\n10-50 passes\nMedium-High\nReference-dependent, slower\nPrincipled attribution with efficiency\n\n\n\n\n\n\n\n\n24.1.3 Reconciling Attribution Methods\nDifferent attribution methods can produce strikingly different importance maps for the same sequence and prediction. A position might show high importance under ISM but near-zero gradients due to saturation, or high gradient magnitude but minimal effect when actually mutated due to redundancy with nearby positions. This disagreement reflects genuine differences in what each method measures: gradients capture local sensitivity, ISM captures counterfactual effects, and DeepLIFT captures contribution relative to a reference.\nPractical workflows often combine multiple methods. Gradient-based approaches efficiently scan long sequences to identify candidate regions, ISM validates importance in targeted windows, and agreement across methods increases confidence that identified features genuinely drive predictions. Disagreement flags positions for closer investigation, potentially revealing saturation effects, redundancy, or artifacts in individual methods.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-cnn-filters",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-cnn-filters",
    "title": "24  Interpretability",
    "section": "24.2 Interpreting Convolutional Filters",
    "text": "24.2 Interpreting Convolutional Filters\nConvolutional neural networks remain central to genomic sequence modeling, as discussed in Chapter 6, and their first-layer filters offer a particularly tractable interpretability target. Each filter slides along the sequence computing dot products with local windows, and high activation indicates that the local sequence matches the filter’s learned pattern. This architecture creates a natural correspondence between filters and sequence motifs.\n\n24.2.1 From Filters to Position Weight Matrices\nConverting learned filters to interpretable motifs follows a standard workflow. The trained model processes a large sequence set, typically training data or genome-wide tiles, recording positions where each filter’s activation exceeds a threshold. The fixed-length windows around high-activation positions are extracted and aligned, and nucleotide frequencies at each position are computed to build a position weight matrix (PWM). This PWM can be visualized as a sequence logo and compared to databases like JASPAR or HOCOMOCO.\nWhen this procedure is applied to models trained on chromatin accessibility or transcription factor binding, first-layer filters frequently match known transcription factor motifs. DeepSEA filters include recognizable matches to CTCF, AP-1, and cell-type-specific factors [Citation Needed: Zhou & Troyanskaya, DeepSEA paper]. This correspondence validates that models discover biologically meaningful patterns rather than arbitrary correlations, and it provides a direct link between model weights and decades of experimental characterization of transcription factor binding preferences.\nSeveral complications affect filter interpretation. DNA is double-stranded, and models may learn forward and reverse-complement versions of the same motif as separate filters. Some filters capture general sequence composition (GC-rich regions, homopolymer runs) rather than specific binding sites. These patterns can be biologically meaningful in contexts like nucleosome positioning or purely artifactual depending on the training task. Distinguishing informative filters from compositional shortcuts requires cross-referencing with known biology and testing whether filter-derived motifs predict binding in held-out data.\n\n\n\n\n\n\nKnowledge Check: CNN Filter Interpretation\n\n\n\nConsider a CNN trained to predict CTCF binding from DNA sequence. You extract the top-activated sequences for one filter and find they all contain the pattern CCGCGNGGNGGCAG.\n\nHow would you determine if this filter is recognizing the forward or reverse-complement CTCF motif?\nIf another filter shows high activation for poly-G stretches, what follow-up analysis would distinguish whether this reflects true CTCF biology versus a training data artifact?\nWhy might the model learn separate filters for the same motif in different orientations, even though CTCF binding is largely orientation-independent at the ChIP-seq level?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nCheck the reverse complement of the discovered pattern against known CTCF motifs in JASPAR. (2) Test whether removing poly-G sequences reduces prediction accuracy for CTCF binding in held-out data, and check whether poly-G enrichment correlates with GC content or mappability artifacts in the training set. (3) CNNs without explicit reverse-complement architecture learn separate filters because the convolution operation treats forward and reverse strands as independent patterns, even when biology treats them equivalently.\n\n\n\n\n\n\n\n\n24.2.2 Deeper Layers and Combinatorial Patterns\nBeyond the first layer, convolutional filters combine lower-level patterns into complex representations. Deeper layers can encode motif pairs that co-occur at characteristic spacing, orientation preferences between binding sites, and contextual dependencies where a motif’s importance varies with surrounding sequence. These combinatorial patterns capture aspects of regulatory grammar that individual motifs cannot represent.\nDirect interpretation of deeper filters becomes increasingly difficult as receptive fields expand and nonlinearities accumulate. The activation of a layer-5 filter depends on intricate combinations of earlier patterns, resisting simple biological annotation. Indirect approaches prove more tractable: analyzing which input regions drive high activation at deeper layers, clustering high-activation sequences to find common themes, or probing whether deeper representations encode specific biological properties.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-motif-discovery",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-motif-discovery",
    "title": "24  Interpretability",
    "section": "24.3 Motif Discovery from Attributions",
    "text": "24.3 Motif Discovery from Attributions\nAttribution maps highlight important positions but do not directly reveal motifs. A DeepLIFT track might show scattered high-importance bases throughout a sequence without indicating that those bases collectively form instances of the same transcription factor binding site. TF-MoDISco (Transcription Factor Motif Discovery from Importance Scores) bridges this gap by discovering motifs from attribution scores rather than raw sequences (Shrikumar et al. 2018).\n\n\n\n\n\n\nKey Insight: Why TF-MoDISco Works Better Than Traditional Motif Finding\n\n\n\nTraditional motif discovery algorithms (like MEME) scan raw sequences and must contend with a fundamental problem: most positions in regulatory sequences do not participate in functional motifs. The algorithm wastes effort on irrelevant positions and may find patterns that happen to be overrepresented but have no functional significance. TF-MoDISco solves this by using attribution scores to weight the search: positions the model actually uses for prediction get prioritized, while unimportant positions contribute minimally. This importance-weighted approach discovers the motifs that drive model predictions, not just patterns that occur frequently.\n\n\nThe insight underlying TF-MoDISco is that importance-weighted sequences focus motif discovery on positions the model actually uses. Traditional motif finders must contend with the fact that most positions in regulatory sequences do not participate in functional motifs. By extracting seqlets (short windows where total importance exceeds a threshold) and clustering them based on both sequence content and importance profiles, TF-MoDISco identifies patterns that drive model predictions.\nThe workflow proceeds through several stages. Base-level importance scores are computed for many sequences using DeepLIFT, ISM, or integrated gradients. Windows where total importance exceeds a threshold are extracted as seqlets, each representing a candidate motif instance. These seqlets are compared using metrics that consider both sequence content and importance profiles, then clustered into groups corresponding to putative motifs. Within each cluster, seqlets are aligned and consolidated into PWMs and importance-weighted logos. The resulting motifs can be matched to known transcription factors or flagged as novel patterns.\nBeyond individual motifs, TF-MoDISco enables grammar inference by analyzing motif co-occurrence. Mapping discovered motif instances back to genomic coordinates reveals characteristic spacing between motif pairs, orientation preferences, and cell-type-specific usage patterns. These grammatical rules can be validated through in silico experiments: inserting or removing motifs in synthetic sequences and checking whether predictions change as expected.\nApplications to models like BPNet trained on ChIP-seq data have recovered known transcription factor motifs, discovered novel sequence variants, and revealed spacing constraints validated through synthetic reporter assays [Citation Needed: BPNet paper]. The same workflow applies to foundation model analysis: use the model to produce base-level attributions for a downstream task, run TF-MoDISco to extract a task-specific motif vocabulary, and analyze how motif usage varies across conditions.\n\n\n\n\n\n\nWorked Example: TF-MoDISco Workflow\n\n\n\nScenario: You have trained a model to predict liver enhancer activity and want to understand what sequence features it uses.\nStep 1: Compute attributions Run integrated gradients on 10,000 predicted enhancer sequences (selecting sequences with high prediction scores). This produces a 4 × L attribution matrix for each sequence, where L is sequence length.\nStep 2: Extract seqlets Scan each attribution profile for windows where summed importance exceeds a threshold (e.g., top 5% of all windows). A typical 500bp enhancer might yield 3-5 seqlets of 15-25bp each.\nStep 3: Cluster seqlets Using both sequence similarity and attribution profile similarity, cluster the ~40,000 extracted seqlets. Suppose this produces 12 distinct clusters.\nStep 4: Generate motifs For each cluster, align seqlets and compute position weight matrices. Results might include:\n\n\n\nCluster\n# Seqlets\nTop JASPAR Match\nMatch Score\n\n\n\n\n1\n8,200\nHNF4A\n0.92\n\n\n2\n5,100\nCEBPA\n0.89\n\n\n3\n3,400\nFOXA1\n0.85\n\n\n4\n2,800\nNovel (no match &gt; 0.7)\n—\n\n\n\nStep 5: Validate The HNF4A and CEBPA motifs are known liver-specific transcription factors, confirming the model learned biologically relevant features. Cluster 4 represents a potential novel regulatory element requiring experimental validation.\nInterpretation: The model relies heavily on canonical liver transcription factor binding sites, consistent with known liver enhancer biology. The novel motif warrants ChIP-seq or MPRA follow-up.\n\n\n\n\n\n\n\n\nTF-MoDISco motif discovery from attribution scores\n\n\n\n\nFigure 24.3: TF-MoDISco motif discovery from attribution scores. Starting from attribution maps for many sequences, the method extracts high-importance windows (seqlets), clusters similar seqlets, consolidates into position weight matrices, and matches to known motif databases. Discovered motifs represent what the model learned—known transcription factors validate biological relevance while novel patterns may indicate previously unknown regulatory features.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-probing",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-probing",
    "title": "24  Interpretability",
    "section": "24.4 Probing Learned Representations",
    "text": "24.4 Probing Learned Representations\nAttribution methods ask which input positions matter; probing asks what information the model’s internal representations encode. The approach resembles asking a student to “show their work” on an exam: if they can correctly answer follow-up questions about intermediate steps, they likely understood the underlying concepts rather than memorizing answers. A probing classifier is a simple supervised model (typically linear) trained to predict some property of interest from the hidden representations of a pretrained model. If a linear probe can accurately predict a property, that property is encoded in an accessible form within the representation—the model “knows” this information in a way that can be easily extracted.\n\n24.4.1 Probing Methodology\nThe standard probing workflow extracts hidden states from a pretrained model for a set of inputs where the property of interest is known. These hidden states, without further transformation, serve as features for training a simple classifier to predict the property. The classifier’s accuracy indicates how well the representation encodes the probed property, while its simplicity (linearity, minimal parameters) ensures that the probe identifies information present in the representation rather than information the probe itself computes.\nFor protein language models like ESM-2, probing has revealed that representations encode secondary structure, solvent accessibility, contact maps, and even 3D coordinates to a surprising degree, as discussed in Chapter 15. These properties emerge despite training on sequence alone, demonstrating that masked language modeling on evolutionary sequences induces representations that capture structural information. For DNA language models (see Chapter 14), probing can assess whether representations encode chromatin state, gene boundaries, promoter versus enhancer identity, or species-specific regulatory signatures.\nProbing provides diagnostic information distinct from downstream task performance. A model might achieve high accuracy on a regulatory prediction task by learning shortcuts (correlations with GC content, distance to annotated genes) rather than encoding genuine regulatory grammar. Probing can detect such shortcuts: if representations strongly encode GC content but weakly encode transcription factor binding site presence, the model may be exploiting composition rather than sequence logic. This diagnostic function complements the confounder analysis discussed in Chapter 12.\n\n\n\n\n\n\nStop and Think: Probing and Confounding\n\n\n\nYou train a DNA language model on human genome sequences, then probe its representations to understand what it has learned. You find:\n\nLinear probe for GC content: 95% accuracy\nLinear probe for promoter vs. enhancer: 78% accuracy\nLinear probe for tissue-specific enhancer activity: 52% accuracy\n\nWhat do these results suggest about the model’s representations? Before reading further, consider:\n\nWhich result is most concerning for downstream variant effect prediction?\nHow would you distinguish whether the promoter/enhancer probe reflects genuine regulatory learning versus correlation with GC content?\nWhat additional probing experiments would you design?\n\n\n\n\n\n24.4.2 Limitations of Probing\nProbing results require careful interpretation. A probe’s failure to predict some property might indicate that the representation does not encode it, or might reflect limitations of the probe architecture, insufficient training data, or mismatch between the probe’s capacity and the complexity of the encoding. Linear probes may miss nonlinearly encoded information; more complex probes risk learning the property themselves rather than reading it from the representation.\n\n\n\n\n\n\nChallenge Alert: The Selectivity-Accessibility Tradeoff\n\n\n\nThis concept is subtle but important. A representation can encode information in two fundamentally different ways:\n\nAccessible encoding: A simple (linear) probe can extract the information. The representation makes the property easy to read.\nSelective encoding: The information is present but requires nonlinear decoding. The property is represented but not prominently exposed.\n\nThe challenge: if you use a more powerful (nonlinear) probe to detect selective encoding, how do you know the probe is reading information from the representation versus computing it from scratch? This is an active area of methodological research with no perfect solution. Best practice: compare probe performance to a control where the same probe is trained on random representations. If performance drops substantially, the original representation genuinely encoded the property.\n\n\nThe selectivity-accessibility tradeoff complicates interpretation. A representation might encode a property accessibly (recoverable by a linear probe) or selectively (encoded but requiring nonlinear decoding). Properties encoded selectively might be present but not easily extracted, while properties encoded accessibly might be incidentally correlated with the training objective rather than causally important. Combining probing with causal interventions (ablating representation components and measuring effects on downstream predictions) provides stronger evidence about which encoded properties actually matter.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-attention",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-attention",
    "title": "24  Interpretability",
    "section": "24.5 Attention Patterns in Transformer Models",
    "text": "24.5 Attention Patterns in Transformer Models\nTransformer-based genomic models use self-attention to aggregate information across long sequence contexts (see Chapter 7 for architectural details), potentially capturing distal regulatory interactions invisible to models with narrow receptive fields. Attention weights indicate which positions each position attends to, creating natural candidates for interpretability: perhaps high attention weights identify functionally related sequence elements.\n\n\n\n\n\n\n\n\nAttention heatmap showing position-position weights\n\n\n\n\n\n\n\nBiological overlay: attention sometimes aligns with regulatory elements\n\n\n\n\n\n\n\nMulti-head specialization: different heads capture different patterns\n\n\n\n\n\n\nFigure 24.4: Attention patterns in genomic transformers. (A) Attention heatmap showing position-position weights with local and long-range patterns. (B) Biological overlay: attention sometimes aligns with regulatory elements—but correlation does not prove the model learned these relationships causally. (C) Multi-head specialization: different heads capture local context, long-range interactions, or specific motifs. Caution: attention weights describe information routing, not importance—always validate with perturbation experiments.\n\n\n\n\n24.5.1 What Attention Patterns Reveal\nWhen attention weights are analyzed in genomic language models, certain heads exhibit strikingly structured patterns. Some heads preferentially connect positions within the same predicted gene or operon, suggesting the model has learned gene boundaries from sequence alone. Other heads show long-range connections that align with known enhancer-promoter relationships or chromatin loop anchors. Still others cluster positions by functional annotation, connecting genes with similar Gene Ontology terms despite lacking explicit functional labels during training.\nIn models like Enformer that predict regulatory outputs from long genomic windows (see Section 16.2), attention can reveal which distal regions influence predictions at a target gene. Contribution scores aggregated across attention heads often peak at known enhancers, insulators, and chromatin domain boundaries. These patterns suggest that the model has learned aspects of regulatory architecture from the correlation between sequence and chromatin output labels.\n\n\n24.5.2 Why Attention Weights Mislead\nRaw attention weights require skeptical interpretation. High attention between two positions indicates information flow in the model’s computation but does not necessarily indicate causal influence on predictions. Attention serves multiple computational roles beyond identifying important features: routing information for intermediate computations, implementing positional reasoning, and satisfying architectural constraints. A position receiving high attention might be used for bookkeeping rather than contributing to the final output.\n\n\n\n\n\n\nKey Insight: Attention Is Not Explanation\n\n\n\nThe most common interpretability mistake with transformers is treating attention weights as importance scores. This is seductive because attention weights are easy to extract and visualize, and high-attention patterns often look biologically plausible. But attention describes information routing, not causal contribution. Consider this analogy: in a complex recipe, you might frequently consult the measurements section (high “attention”) while the actual flavor comes from the spice section (low “attention” but high importance). To know if an attention pattern matters, you must perturb it and measure the prediction change. Attention without perturbation is correlation without causation.\n\n\nSeveral specific issues undermine naive attention interpretation. Attention weights describe information movement before value vectors are applied; positions with high attention but small value vector magnitudes contribute little to the output. Multi-head attention averages across heads with different functions; examining average attention obscures specialized head behavior. Cross-layer effects mean that the importance of early-layer attention depends on what later layers do with the routed information.\nMore robust approaches combine attention analysis with perturbation experiments. If deleting a position that receives high attention changes the prediction substantially, the attention is functionally meaningful. If deletion has minimal effect, the attention may serve computational purposes unrelated to the target output. Attention rollout and attention flow methods propagate attention through layers to better capture information movement across the full network, though these too provide correlational rather than causal evidence.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-global",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-global",
    "title": "24  Interpretability",
    "section": "24.6 Regulatory Vocabularies and Global Interpretability",
    "text": "24.6 Regulatory Vocabularies and Global Interpretability\nLocal interpretability methods explain individual predictions; global interpretability characterizes what a model has learned across its entire training distribution. For genomic models trained to predict thousands of chromatin features, global interpretability asks whether the model has learned a coherent vocabulary of regulatory sequence classes and how those classes map to biological programs.\n\n24.6.1 Sequence Classes from Sei\nSei exemplifies the global interpretability approach by learning a vocabulary of regulatory sequence classes that summarize chromatin profile diversity across the genome (see ?sec-ch13-sei for architectural details). The model predicts tens of thousands of chromatin outputs (transcription factor binding, histone modifications, accessibility across cell types), then compresses this high-dimensional prediction space into approximately 40 sequence classes through dimensionality reduction and clustering.\nEach sequence class corresponds to a characteristic regulatory activity pattern. Some classes show promoter-like signatures (H3K4me3, TSS proximity, broad expression). Others exhibit enhancer patterns (H3K27ac, H3K4me1, cell-type-restricted activity). Repressive classes display H3K27me3 or H3K9me3 enrichment. Cell-type-specific classes capture lineage-restricted regulatory programs (neuronal, immune, hepatic). This vocabulary transforms thousands of raw chromatin predictions into a compact, interpretable representation.\nVariants can be characterized by their effects on sequence class scores, yielding functional descriptions more informative than raw pathogenicity predictions. A variant that shifts a region from enhancer-like to promoter-like class, or from active to repressive, provides mechanistic hypotheses about its functional consequences. Genome-wide association study (GWAS) enrichment analysis can identify which sequence classes are overrepresented among disease-associated variants, revealing the regulatory programs most relevant to specific phenotypes (see Chapter 3 for GWAS foundations).\n\n\n\n\n\n\nKnowledge Check: Local vs. Global Interpretability\n\n\n\nConsider a foundation model that predicts tissue-specific enhancer activity across 100 cell types.\n\nWhat would a local interpretability analysis tell you about a specific variant in a cardiac enhancer?\nWhat would a global interpretability analysis (like Sei’s sequence classes) tell you about the same variant?\nIn what clinical scenario would you prefer local interpretability? In what research scenario would global interpretability be more valuable?\n\nThink about the difference between explaining one prediction versus characterizing the model’s overall regulatory vocabulary.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nLocal analysis (attribution methods) would identify which specific nucleotides the variant disrupts and what motifs are affected—for example, “variant disrupts a GATA4 binding site at position 142.” (2) Global analysis would show how the variant shifts regulatory program membership—for example, “variant shifts sequence class from cardiac-specific enhancer to generic promoter-like.” (3) Prefer local for clinical variant interpretation where you need mechanistic detail for a specific case; prefer global for GWAS follow-up where you want to understand which regulatory programs are disease-relevant across many variants.\n\n\n\n\n\n\n\n\n24.6.2 Embedding Geometry and Regulatory Programs\nBeyond discrete sequence classes, the continuous geometry of learned representations encodes regulatory relationships. Sequences with similar regulatory functions cluster in embedding space; directions in this space correspond to biological axes of variation. Dimensionality reduction techniques (UMAP, t-SNE, principal component analysis) visualize these relationships, revealing how the model organizes regulatory diversity.\nFor foundation models trained on diverse genomic tasks, embedding geometry can capture cross-task relationships. Sequences that function as enhancers in one cell type might cluster near sequences with enhancer function in related cell types, even if trained independently. Variants that disrupt shared regulatory logic should produce similar embedding perturbations. These geometric properties enable transfer of interpretability insights across tasks and provide compact summaries of model knowledge.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-mechanistic",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-mechanistic",
    "title": "24  Interpretability",
    "section": "24.7 Mechanistic Interpretability",
    "text": "24.7 Mechanistic Interpretability\nClassical interpretability methods treat models as input-output functions, probing what they compute without examining how they compute it. Mechanistic interpretability takes a different approach, attempting to reverse-engineer the algorithms implemented by neural network weights. Think of it like the difference between knowing that a car gets you from A to B versus opening the hood to understand how the engine, transmission, and fuel system work together. Classical interpretability tells you the car runs; mechanistic interpretability identifies which piston fires when and how the carburetor mixes fuel. This emerging field, most developed for language models, offers tools increasingly applicable to genomic foundation models.\n\n\n\n\n\n\nChallenge Alert: Frontier Research Area\n\n\n\nMechanistic interpretability represents the frontier of interpretability research. The concepts in this section are powerful but the techniques are still maturing. Current methods require substantial manual analysis, work best for small models, and have been validated primarily in language models rather than genomic models. As you read, focus on understanding the conceptual framework (circuits, features, superposition) rather than expecting turnkey tools. The field is evolving rapidly, and today’s research prototypes may become tomorrow’s standard practices.\n\n\n\n24.7.1 Circuits and Features\nThe central hypothesis of mechanistic interpretability is that neural networks implement interpretable computations through identifiable circuits: connected subnetworks that perform specific functions. A circuit might detect whether a motif is present, compute the distance between two motifs, or integrate evidence across regulatory elements. Identifying circuits requires tracing information flow through the network and characterizing what each component contributes.\nFeatures are the atomic units of this analysis: directions in activation space that correspond to interpretable concepts. In language models, features have been found that activate for specific topics, syntactic structures, or semantic properties. Analogous features in genomic models might activate for transcription factor binding sites, coding versus non-coding sequence, or regulatory element types. Sparse autoencoders trained on model activations can extract interpretable features by encouraging representations where most features are inactive for any given input.\nSuperposition complicates feature identification. Neural networks can represent more features than they have dimensions by using overlapping, nearly orthogonal directions. Why would networks do this? The answer lies in the statistics of natural data: most features are sparse (active for only a small fraction of inputs), so they rarely need to be represented simultaneously. By packing many sparse features into a lower-dimensional space using nearly orthogonal directions, networks can represent far more concepts than their dimensionality would naively suggest. Features active for different inputs can share parameters, enabling high-capacity representations but complicating interpretation—when we observe an activation pattern, multiple overlapping features may contribute. Techniques from compressed sensing and dictionary learning help decompose superposed representations into constituent features.\n\n\n24.7.2 Applications to Genomic Models\nMechanistic interpretability remains nascent for genomic foundation models, but initial applications show promise. Attention head analysis in DNA language models has identified heads specialized for different genomic functions: some attend within genes, others across regulatory regions, still others implement positional computations [Citation Needed]. Probing activations at different layers reveals hierarchical feature construction, from local sequence patterns in early layers to long-range regulatory relationships in later layers.\nCircuit analysis can explain specific model behaviors. If a model predicts that a variant disrupts regulation, mechanistic analysis can trace which features activate differently for reference versus variant sequence, which attention heads route information about the variant to the prediction, and which intermediate computations change. This mechanistic trace provides far richer explanation than attribution scores alone, potentially identifying the regulatory logic the model has learned.\nThe challenge is scalability. Current mechanistic interpretability techniques require substantial manual analysis and work best for small models or specific behaviors. Foundation models with billions of parameters resist exhaustive circuit enumeration. Developing automated tools for circuit discovery and scaling mechanistic analysis to large genomic models represents an active research frontier.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-validation",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-validation",
    "title": "24  Interpretability",
    "section": "24.8 Validation: From Explanations to Experiments",
    "text": "24.8 Validation: From Explanations to Experiments\nInterpretability methods produce explanations, but explanations are only valuable if they accurately reflect model behavior and connect to biological reality. Validation closes the loop by testing whether interpretability-derived hypotheses hold when subjected to experimental scrutiny.\n\n24.8.1 Faithfulness Testing\nAn interpretation is faithful if it accurately describes what the model does. Testing faithfulness requires interventions: changing the features identified as important and verifying that predictions change accordingly. If an attribution method highlights certain positions as driving a prediction, deleting or scrambling those positions should reduce the prediction. If discovered motifs are claimed to be necessary for regulatory activity, removing them from sequences should impair predicted and measured function.\nSanity checks provide baseline validation. When model weights are randomized, attributions should degrade to uninformative noise. When training labels are scrambled, discovered motifs should disappear or lose predictive power. These checks identify methods that produce plausible-looking outputs regardless of model content, revealing explanations that reflect method biases rather than genuine model features.\nCounterfactual experiments go further by testing whether identified features are sufficient as well as necessary. Inserting discovered motifs into neutral sequences should increase predicted regulatory activity if the motifs genuinely encode functional elements. Constructing synthetic sequences that combine motifs according to discovered grammatical rules should produce predictions consistent with those rules. Discrepancies between expected and observed effects indicate gaps in the interpretation.\nThe following table summarizes the hierarchy of validation tests, from weakest to strongest evidence:\n\n\n\nTable 24.2: Validation hierarchy for interpretability claims. Stronger evidence requires more experimental investment but provides greater confidence that model explanations reflect biological reality.\n\n\n\n\n\n\n\n\n\n\n\nValidation Level\nTest\nWhat It Proves\nWhat It Cannot Prove\n\n\n\n\nSanity Check\nRandom weights produce random attributions\nMethod is not trivially broken\nMethod accurately reflects model\n\n\nComputational Necessity\nAblating feature reduces prediction\nFeature is used by model\nFeature is the only cause\n\n\nComputational Sufficiency\nInserting feature increases prediction\nFeature is sufficient in isolation\nFeature is necessary or biologically meaningful\n\n\nBiological Necessity\nExperimental deletion (CRISPR) abolishes activity\nFeature is biologically required\nModel learned it correctly\n\n\nBiological Sufficiency\nSynthetic construct with feature is active\nFeature is biologically sufficient\nModel captured all relevant features\n\n\n\n\n\n\n\n\n\n\n\n\nPlausible versus faithful explanations: validation distinguishes them\n\n\n\n\nFigure 24.5: Plausible versus faithful explanations. Both paths start with attributions highlighting a GATA motif. Left: plausible but unfaithful—the explanation matches intuition, but the model learned GC content; validation tests fail. Right: faithful—the model actually uses the GATA motif; validation tests pass. The distinction determines whether interpretability generates genuine insight or false comfort. Always validate: does ablating the feature change prediction? Does inserting it increase prediction?\n\n\n\n\n\n24.8.2 Experimental Validation\nThe ultimate test of interpretability connects model-derived hypotheses to biological experiments. Motifs discovered through TF-MoDISco can be tested through electrophoretic mobility shift assays, ChIP-qPCR, or reporter constructs. Predicted spacing constraints can be validated by varying distances between motifs in synthetic constructs and measuring activity. Hypothesized enhancer-promoter connections can be tested through CRISPR deletion of predicted enhancers and measurement of target gene expression.\nThis experimental validation distinguishes genuine mechanistic discovery from pattern matching that happens to produce plausible-looking results. A model might learn that certain k-mers correlate with regulatory activity for confounded reasons (batch effects, mappability artifacts) yet produce motif logos resembling real transcription factors. Only experimental testing can determine whether model-derived hypotheses reflect causal regulatory logic.\nHigh-throughput functional assays enable systematic validation at scale. Massively parallel reporter assays (MPRAs) can test thousands of model-predicted regulatory elements simultaneously. Perturb-seq combines CRISPR perturbations with single-cell RNA-seq to measure effects of knocking out predicted regulatory factors (see ?sec-ch16-perturbation). These technologies create opportunities for iterative model improvement: interpretability generates hypotheses, experiments test them, and results refine both model architecture and training.\n\n\n\n\n\n\nStop and Think: Designing Validation Experiments\n\n\n\nYou have used integrated gradients and TF-MoDISco to analyze a model that predicts liver-specific enhancer activity. The analysis reveals that the model relies heavily on HNF4A and CEBP motifs, often appearing within 50bp of each other.\nBefore reading further, design a validation strategy:\n\nWhat computational experiments would test whether these motifs are necessary for the model’s predictions?\nWhat computational experiments would test sufficiency?\nWhat biological experiments would test whether the model’s reliance on these motifs reflects genuine liver regulatory logic?\nIf the biological experiments fail to validate the model’s predictions, what are the possible explanations?\n\n\n\n\n\n\n\n\n\nClosed-loop interpretability validation workflow\n\n\n\n\nFigure 24.6: Closed-loop interpretability validation. Starting from model predictions, interpretability analysis generates hypotheses about important features. Experimental validation tests necessity (CRISPR deletion) and sufficiency (reporter assays). Validated hypotheses advance biological understanding; failures identify model limitations. This cycle progressively improves both mechanistic understanding and model reliability.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-clinical",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-clinical",
    "title": "24  Interpretability",
    "section": "24.9 Interpretability in Clinical Variant Assessment",
    "text": "24.9 Interpretability in Clinical Variant Assessment\nVariant interpretation guidelines require that computational predictions be weighed alongside experimental and clinical evidence, as discussed further in Chapter 28. Interpretability determines whether model predictions can contribute meaningful evidence beyond raw pathogenicity scores.\nCurrent ACMG-AMP criteria allow computational evidence as supporting (PP3) or opposing (BP4) pathogenicity, but the evidence strength depends on understanding what the prediction reflects (Richards et al. 2015). The full ACMG-AMP framework and its integration with computational evidence is examined in ?sec-ch26-acmg-amp. A splice site disruption score from SpliceAI provides interpretable mechanistic evidence: the variant is predicted to alter splicing because it changes the consensus splice site sequence (Section 6.5) (Jaganathan et al. 2019). This prediction can be evaluated against splice site models, tested with minigene assays, and combined with observations of aberrant transcripts in patient samples. The interpretation enables evidence integration.\n\n\n\n\n\n\nPractical Guidance: Communicating Interpretability in Clinical Reports\n\n\n\nWhen preparing computational evidence for clinical variant interpretation:\n\nAlways include the mechanism, not just the score. “Pathogenicity score: 0.92” is less useful than “Predicted to disrupt CTCF binding site (attribution score -0.8), shifting sequence class from insulator to neutral.”\nSpecify what was tested. Did you run ISM to validate the attribution? Did the motif match a known transcription factor? Is the affected sequence class enriched in relevant GWAS?\nAcknowledge limitations explicitly. If the model was not trained on the relevant tissue type, or if the variant type (structural, repeat) was underrepresented in training, say so.\nSuggest validation experiments. “This prediction could be validated by EMSA for CTCF binding or minigene assay for splicing effects.”\nCross-reference related evidence. Does the computational mechanism explain the patient’s phenotype? Is there functional data in ClinVar for nearby variants?\n\n\n\nFoundation model predictions are less immediately interpretable but potentially more informative. A pathogenicity score from ESM-1v (Section 15.1) reflects evolutionary constraint inferred from protein language modeling, but the specific sequence features driving the prediction require attribution analysis to identify. The protein VEP paradigm is examined in ?sec-ch14-protein-vep. An expression effect predicted by Enformer (Section 16.2) might result from disrupted transcription factor binding, altered chromatin accessibility, or changed 3D regulatory contacts; interpretability analysis distinguishes these mechanisms and guides experimental validation. The DNA-based VEP approaches are detailed in ?sec-ch14-dna-vep.\nFor clinical utility, interpretability must be communicated effectively. Genome browsers displaying attribution tracks alongside variant calls help clinicians identify mechanistic hypotheses. Reports that accompany pathogenicity scores with regulatory vocabulary classifications (this variant shifts an enhancer toward a repressive state) provide actionable context. These communication challenges extend interpretability beyond algorithm development to user interface design and clinical workflow integration.\n\n\n\n\n\n\nInterpretability for clinical variant assessment\n\n\n\n\nFigure 24.7: Interpretability for clinical variant assessment. ACMG evidence strength depends on interpretability: a pathogenicity score alone (PP3/BP4 supporting) provides weak evidence; a score with mechanistic explanation (splice site disruption) is moderate; a score with experimentally validated mechanism (ChIP-confirmed binding loss, functional assay) is strong. Clinical reports should communicate the mechanism, not just the score, to enable evidence integration.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-practical",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-practical",
    "title": "24  Interpretability",
    "section": "24.10 Practical Approaches for Foundation Model Analysis",
    "text": "24.10 Practical Approaches for Foundation Model Analysis\nWorking with genomic foundation models requires matching interpretability methods to specific questions. Several complementary strategies address different aspects of model behavior.\nFor understanding variant effects, the primary goal is explaining why a specific variant receives a particular prediction. Attribution methods (ISM for validation, integrated gradients for efficiency) identify which input positions drive the difference between reference and alternative predictions. If the variant falls within a discovered motif, the interpretation is straightforward. If attributions spread across the sequence, the effect may operate through long-range regulatory changes requiring attention analysis or contribution scores from models like Enformer.\nFor characterizing model representations, probing classifiers diagnose what information is encoded and at which layers. Probing for known regulatory features (promoter versus enhancer, tissue specificity, evolutionary conservation) establishes which biological properties the model captures. Probing for potential confounders (GC content, distance to annotated genes, technical artifacts) identifies shortcuts that might inflate benchmark performance without reflecting genuine regulatory understanding (see Section 11.8 for benchmark limitations and ?sec-ch22-detection for confounder detection methods).\nFor discovering regulatory logic, TF-MoDISco applied to high-confidence predictions extracts motif vocabularies specific to prediction tasks or cell types. Grammar analysis of motif co-occurrence reveals combinatorial rules. Sei-style sequence class analysis situates local motifs within global regulatory programs. Comparing discovered vocabularies across models or training conditions reveals shared versus idiosyncratic features.\nFor debugging and auditing, interpretability methods identify what features drive predictions in held-out distributions. If a model fails on a new cell type, attribution analysis can reveal whether it relies on cell-type-specific versus generalizable features. If performance degrades on specific genomic regions, local interpretability can identify confounding patterns or training data gaps.\nFor generating experimental hypotheses, interpretability produces testable predictions. Discovered motifs can be synthesized and tested. Predicted regulatory elements can be perturbed. Hypothesized transcription factor binding can be validated by ChIP. Model-derived predictions that survive experimental testing represent genuine mechanistic insights; predictions that fail point toward model limitations or confounding.\nThe following table provides a decision framework for selecting interpretability methods based on your analysis goal:\n\n\n\nTable 24.3: Decision framework for interpretability analysis of genomic foundation models. Validation requirements increase with the strength of claims being made.\n\n\n\n\n\n\n\n\n\n\n\nGoal\nPrimary Method\nSupporting Methods\nValidation Required\n\n\n\n\nExplain single variant\nIntegrated gradients\nISM for verification\nMotif match, literature\n\n\nFind regulatory motifs\nTF-MoDISco\nFilter visualization\nJASPAR match, MPRA\n\n\nDiagnose model shortcuts\nProbing classifiers\nAttribution for confounders\nHeld-out distribution\n\n\nUnderstand long-range effects\nAttention analysis\nContribution scores\nPerturbation experiment\n\n\nCharacterize model vocabulary\nSei-style clustering\nEmbedding geometry\nGWAS enrichment\n\n\nGenerate hypotheses for experiments\nTF-MoDISco + grammar\nCircuit analysis\nEMSA, reporter, CRISPR",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch24-interpretability.html#sec-ch24-conclusion",
    "href": "part_5/p5-ch24-interpretability.html#sec-ch24-conclusion",
    "title": "24  Interpretability",
    "section": "24.11 Plausibility Is Not Faithfulness",
    "text": "24.11 Plausibility Is Not Faithfulness\nThe distinction between plausibility and faithfulness remains central to interpretability for genomic foundation models. Models can produce compelling motifs, structured attention patterns, and interpretable probing results while operating through mechanisms that do not correspond to biological reality. A model that correctly predicts splice site strength may do so by recognizing confounded sequence features rather than learning splice site grammar. A model that attributes importance to a transcription factor binding site may be exploiting correlation with GC content rather than modeling regulatory mechanism. Plausible explanations that match biological intuition are not the same as faithful explanations that accurately reflect model computation.\nOnly interventional experiments can distinguish genuine regulatory insight from sophisticated pattern matching. Computational interventions (deletion tests, counterfactual sequence generation, circuit analysis) probe whether identified features are necessary and sufficient for model predictions. Biological interventions (reporter assays, CRISPR perturbations, massively parallel experiments) test whether model-derived hypotheses hold in living systems. The sequence design applications in Chapter 30 operationalize this validation loop, using interpretability-derived hypotheses to guide experimental libraries. The conjunction of computational and experimental validation transforms interpretability from rationalization into discovery, generating testable hypotheses that advance biological understanding rather than merely explaining model behavior.\nAs foundation models grow in scale and capability, interpretability becomes simultaneously more important and more challenging. Larger models implement more complex computations, potentially capturing subtler regulatory logic but resisting simple interpretation. Mechanistic interpretability offers a path forward by characterizing model internals directly, though scaling these techniques to billion-parameter genomic models remains an open problem. The evaluation challenges this creates are examined in Section 12.9, while the confounding risks of scale are addressed in Chapter 12. The integration of interpretability with model development points toward a future where understanding and prediction advance together: motifs discovered through interpretation inform architecture design, experimentally validated hypotheses become supervision signals, and interpretability failures that reveal confounding drive improvements in training data and evaluation. In this vision, interpretability is not merely a tool for explaining existing models but a methodology for building models whose predictions we trust because we understand the mechanisms they have learned.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat is the difference between a plausible and a faithful explanation? Why might a model produce attributions that highlight a biologically plausible motif even when that motif doesn’t drive the prediction?\nWhy is in silico mutagenesis (ISM) considered the “gold standard” for attribution faithfulness, and what is its main practical limitation?\nWhat problem does the saturation issue create for gradient-based attribution methods, and how do integrated gradients address this?\nExplain why attention weights are not reliable indicators of input importance. What do they actually measure?\nDescribe the validation hierarchy from sanity checks to biological sufficiency. What distinguishes computational necessity from biological necessity?\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nCore Concepts:\n\nPlausibility vs. Faithfulness: Plausible explanations match human intuition; faithful explanations accurately reflect model computation. Interpretability methods can produce plausible but unfaithful explanations, providing false comfort rather than genuine insight.\nAttribution Methods: Assign importance scores to input positions. ISM provides faithful counterfactual information but is computationally expensive. Gradient-based methods (saliency, DeepLIFT, integrated gradients) are efficient but can miss important features due to saturation.\nTF-MoDISco: Discovers motifs from attribution scores rather than raw sequences, focusing on patterns the model actually uses for prediction. Enables grammar inference through co-occurrence analysis.\nProbing Classifiers: Diagnose what information model representations encode. Simple (linear) probes identify accessible information; probe failure may indicate absence or inaccessible encoding.\nAttention Interpretation: Attention weights describe information routing, not causal importance. High attention does not imply the attended position drives the prediction. Perturbation experiments are required to establish functional relevance.\nGlobal Interpretability: Methods like Sei sequence classes characterize what a model has learned across its training distribution, providing regulatory vocabularies more informative than individual predictions.\nMechanistic Interpretability: Reverse-engineers the algorithms implemented by model weights, identifying circuits and features. Promising but nascent for genomic models.\nValidation Hierarchy: Sanity checks → computational necessity → computational sufficiency → biological necessity → biological sufficiency. Each level provides stronger evidence but requires more experimental investment.\n\nKey Connections:\n\nInterpretability enables clinical utility by providing mechanistic evidence that satisfies ACMG-AMP criteria (?sec-ch26-acmg-amp, Chapter 28)\nConfounder detection (Chapter 12) relies on interpretability to identify shortcuts\nSequence design (Chapter 30) uses interpretability-derived hypotheses to guide experimental validation\n\nLooking Ahead: Chapter 25 extends interpretability to causal inference, examining how to distinguish correlation from causation in model predictions and when interpretable features reflect genuine regulatory mechanisms.\n\n\n\n\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie Gastier-Foster, Wayne W. Grody, et al. 2015. “Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology.” Genetics in Medicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nShrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. 2017. “Learning Important Features Through Propagating Activation Differences.” In Proceedings of the 34th International Conference on Machine Learning, 3145–53. PMLR. https://proceedings.mlr.press/v70/shrikumar17a.html.\n\n\nShrikumar, Avanti, Katherine Tian, Žiga Avsec, Anna Shcherbina, Abhimanyu Banerjee, Mahfuza Sharmin, Surag Nair, and Anshul Kundaje. 2018. “Technical Note on Transcription Factor Motif Discovery from Importance Scores (TF-MoDISco) Version 0.5.6.5.” arXiv. https://doi.org/10.48550/arXiv.1811.00416.\n\n\nSundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. “Axiomatic Attribution for Deep Networks.” In Proceedings of the 34th International Conference on Machine Learning, 3319–28. PMLR. https://proceedings.mlr.press/v70/sundararajan17a.html.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch25-causal.html",
    "href": "part_5/p5-ch25-causal.html",
    "title": "25  Causal Inference with Foundation Models",
    "section": "",
    "text": "25.1 Prediction vs. Causation",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Causal Inference with Foundation Models</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch25-causal.html#sec-ch25-prediction-vs-causation",
    "href": "part_5/p5-ch25-causal.html#sec-ch25-prediction-vs-causation",
    "title": "25  Causal Inference with Foundation Models",
    "section": "",
    "text": "25.1.1 The Ladder of Causation\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nJudea Pearl proposed three levels of causal reasoning. Before reading about them, what do you think distinguishes “seeing” (observation) from “doing” (intervention) from “imagining” (counterfactual)? Why might a model that can answer observational questions fail at interventional ones?\n\n\nJudea Pearl’s “ladder of causation” provides a framework for understanding the gap between prediction and intervention (pearl_book_2018?). The ladder has three rungs, each representing a qualitatively different type of reasoning:\nRung 1: Association answers questions of the form “What does seeing X tell me about Y?” This is the domain of standard predictive modeling. A foundation model that predicts gene expression from sequence operates at this level: given a sequence pattern, what expression level do we expect? Association captures correlation but remains agnostic about mechanism.\nRung 2: Intervention answers questions of the form “What happens to Y if I change X?” This requires understanding not just correlation but causal structure. Intervening on X breaks its correlations with upstream causes while preserving its effects on downstream variables. A model capable of intervention reasoning can predict not just what expression we observe given a sequence, but what expression would result if we edited the sequence.\nRung 3: Counterfactual answers questions of the form “What would Y have been if X had been different, given that we observed specific values?” Counterfactuals require reasoning about alternative histories for specific individuals, not just population-level effects. This is the realm of “What if this patient had received treatment A instead of treatment B?”\nMost machine learning, including foundation models, operates at rung 1. Models learn associations from training data and predict outcomes for new inputs. Moving to rung 2 requires additional structure, typically assumptions about causal relationships encoded in directed acyclic graphs (DAGs) or identified through experimental interventions. Rung 3 remains largely out of reach for current methods outside carefully controlled settings.\n\n\n\n\n\n\nPearl’s ladder of causation applied to genomics\n\n\n\n\nFigure 25.1: Pearl’s ladder of causation applied to genomics. Rung 1 (Association): Foundation models learn P(Y|X)—observing variant V, predict phenotype P. Rung 2 (Intervention): P(Y|do(X))—if we edit V, what happens to P? Requires causal structure, not just correlations. Rung 3 (Counterfactual): For this specific patient, what would have happened under alternative conditions? The gaps are qualitative: predictive accuracy at Rung 1 provides no guarantee of accuracy at Rungs 2-3.\n\n\n\n\n\n\n\n\n\nKey Insight: The Rung Gap is Qualitative, Not Quantitative\n\n\n\nA common misconception is that better predictions (rung 1) eventually enable intervention reasoning (rung 2). This is false. No amount of observational data can, by itself, distinguish correlation from causation. A model trained only on observational data cannot reliably predict intervention effects, regardless of its predictive accuracy. Moving up the ladder requires either (1) experimental intervention data, (2) causal assumptions encoded in the model, or (3) natural experiments like genetic randomization. Scale and accuracy at rung 1 do not substitute for the structural knowledge needed for rung 2.\n\n\n\n\n25.1.2 Why Predictive Accuracy Does Not Equal Causal Understanding\nA model can achieve excellent predictive accuracy while learning entirely non-causal relationships. Consider a gene expression predictor trained on population data. The model might learn that expression of gene A correlates with expression of gene B, and accurately predict B given A. But this prediction may reflect:\n\nDirect causation: A regulates B\nReverse causation: B regulates A, and the model uses A as a proxy\nCommon cause: Both A and B are regulated by an unmeasured factor C\nSelection bias: The training population was selected in a way that induces correlation\nConfounding: Population structure or batch effects create spurious associations (?sec-ch12-population-structure)\n\nThe distinction matters for intervention. If A directly causes B, then editing A will change B. If the correlation reflects a common cause, editing A will have no effect on B. A model that achieves 95% accuracy predicting B from A provides no information about which causal structure generated the correlation.\n\n\n\n\n\n\nStop and Think: Diagnosing Causal Structure\n\n\n\nA foundation model accurately predicts that patients with high expression of gene X have poor cancer prognosis. Before targeting gene X therapeutically, what evidence would you want?\nConsider: (1) Does X drive poor prognosis, or does aggressive cancer drive both X expression and poor prognosis? (2) What experimental or genetic evidence could distinguish these scenarios? (3) How would you test whether X is a driver versus a passenger?\n\n\nFoundation models are particularly susceptible to learning non-causal patterns because they are trained on massive observational datasets that contain all of these correlation sources. The very scale that enables their predictive power also exposes them to more spurious associations. Confounding in genomic data is pervasive (Chapter 12), and foundation models lack architectural mechanisms to distinguish causal from confounded relationships.\n\n\n25.1.3 The Clinical Stakes\nThe distinction between association and causation is not merely philosophical when models inform clinical decisions. A risk prediction model can be useful even if it captures only associations: knowing that a patient is high-risk enables closer monitoring regardless of whether we understand why (Chapter 27). But treatment decisions require causal reasoning.\nConsider drug target selection. A gene whose expression is associated with disease progression might be a target, a biomarker, or neither. If the gene causally drives progression, inhibiting it could slow disease. If it is merely correlated (perhaps because both expression and progression reflect an upstream driver), inhibiting it will not help. If the association is confounded by treatment patterns in the training data, the gene may have no biological relationship to the disease at all.\nThe same logic applies to polygenic risk scores, variant interpretation, and therapeutic recommendations. Association supports screening and stratification. Causation supports intervention. Confusing the two leads to treatments that do not work, resources wasted on non-causal targets, and potential patient harm.\nThe table below summarizes when association versus causation matters for different clinical applications:\n\n\n\nTable 25.1: When causal reasoning is required for clinical applications. Association-based predictions suffice for stratification and prognosis but not for treatment decisions.\n\n\n\n\n\n\n\n\n\n\n\nApplication\nAssociation Sufficient?\nCausation Required?\nExample\n\n\n\n\nRisk stratification\nYes\nNo\nIdentifying high-risk patients for closer monitoring\n\n\nBiomarker for prognosis\nYes\nNo\nPredicting disease progression\n\n\nDiagnostic classification\nYes\nNo\nClassifying tumor subtypes\n\n\nDrug target selection\nNo\nYes\nChoosing which gene to inhibit\n\n\nTreatment recommendation\nNo\nYes\nSelecting therapy for a patient\n\n\nVariant pathogenicity for intervention\nNo\nYes\nGene therapy target selection",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Causal Inference with Foundation Models</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch25-causal.html#sec-ch25-causal-methods",
    "href": "part_5/p5-ch25-causal.html#sec-ch25-causal-methods",
    "title": "25  Causal Inference with Foundation Models",
    "section": "25.2 Causal Methods in Genomics",
    "text": "25.2 Causal Methods in Genomics\nGenomics has developed specialized methods for causal inference that leverage the unique properties of genetic data. Unlike typical observational studies where confounding is ubiquitous, genetic variants are assigned at conception through meiosis—a natural randomization process that makes genetics uniquely suited for certain causal inference approaches.\n\n\n\n\n\n\nRecall and Connect\n\n\n\nFrom Chapter 12: We learned that population structure creates spurious associations in GWAS. Before reading about Mendelian randomization, consider: How might the random assignment of alleles at conception help overcome confounding? What makes genetic variants different from environmental exposures?\n\n\n\n25.2.1 Mendelian Randomization\n\n\n\n\n\n\nConceptual Difficulty\n\n\n\nMendelian randomization involves subtle causal reasoning. If the three core assumptions and their violation modes are unfamiliar, consider reviewing the causal inference literature before proceeding. The key insight is that genetic variants serve as “natural experiments” because their assignment at conception is random with respect to most confounders.\n\n\nMendelian randomization (MR) exploits the random assortment of alleles during meiosis to create natural experiments (Davey Smith and Ebrahim 2003; lawlor_mendelian_2008?). If a genetic variant affects an exposure (e.g., gene expression, protein level, metabolite concentration), and that variant is associated with an outcome, then under certain assumptions we can infer that the exposure causally affects the outcome.\nThink of the genetic variant as a randomly assigned ticket at birth. Imagine a lottery where some people receive tickets that increase their coffee consumption (a variant affecting caffeine metabolism). If ticket holders—who drink more coffee by chance of genetics, not by lifestyle choice—also have different heart disease rates, we have evidence that coffee consumption itself affects heart disease, not just that coffee drinkers happen to live differently in other ways. The “ticket” (genetic variant) was assigned before any confounders could arise, isolating the effect of the exposure.\nThe logic parallels randomized controlled trials. In an RCT, random treatment assignment ensures that treatment groups differ only in treatment received, not in confounders. In MR, random allele assignment at conception ensures that genotype groups differ only in genotype-driven exposure levels, not in confounders. The genetic variant acts as an “instrumental variable” that isolates the causal effect of the exposure.\nMR relies on three core assumptions:\n\nRelevance: The genetic variant must be associated with the exposure\nIndependence: The variant must not be associated with confounders of the exposure-outcome relationship\nExclusion restriction: The variant must affect the outcome only through the exposure, not through other pathways (no horizontal pleiotropy—the situation where a genetic variant affects multiple traits through independent biological mechanisms, rather than through a single causal pathway)\n\nWhy are these three assumptions necessary? Relevance ensures the genetic variant actually affects the exposure of interest; without it, the variant provides no information about the exposure’s causal effect. Independence is why genetics provides a causal inference advantage: because alleles are randomly assigned at conception before environmental confounders arise, genetic instruments satisfy independence naturally for most confounders (though population structure can violate it). The exclusion restriction ensures we measure the exposure’s effect rather than some other effect of the variant—if a variant affects the outcome through multiple pathways, we cannot isolate which pathway matters.\nViolations of these assumptions, particularly pleiotropy, limit MR’s applicability. Modern MR methods address this through multiple instruments, median-based estimators, mode-based estimators, and outlier detection that are robust to some violations (Bowden, Davey Smith, and Burgess 2015; Hartwig, Davey Smith, and Bowden 2017). Integration with foundation models, discussed below, offers new possibilities for instrument selection and pleiotropy detection.\n\n\n25.2.2 Model-X Knockoffs for Controlled Variable Selection\nMendelian randomization exploits random allele assignment to isolate causal effects; fine-mapping exploits LD structure to identify causal variants. A third approach—Model-X knockoffs—provides a complementary framework for causal variable selection that addresses a specific limitation of standard GWAS: the inability to distinguish marginal association from conditional independence (Candès et al. 2018).\nStandard GWAS tests each variant marginally: is this variant associated with the outcome? But in the presence of linkage disequilibrium, marginal p-values conflate direct effects with indirect correlation. A non-causal variant in high LD with a causal variant will show significant marginal association even though it provides no information about the outcome beyond what other variants already provide. This distinction—marginal versus conditional association—is precisely what causal inference requires.\nModel-X knockoffs construct synthetic “knockoff” variables that preserve the correlation structure among features but are conditionally independent of the outcome given the original features. By comparing how strongly each original variant predicts the outcome versus its knockoff, the method identifies variants that provide genuine predictive signal beyond what their LD neighbors explain. The framework provides finite-sample false discovery rate (FDR) control without requiring knowledge of how the outcome depends on the variants—a model-free property that distinguishes it from parametric approaches.\nApplied to Crohn’s disease GWAS data (~400,000 SNPs), knockoffs identified twice as many discoveries as marginal association testing while maintaining FDR control. Several discoveries were subsequently validated in larger meta-analyses, demonstrating that the method identifies true positives that marginal testing misses due to conservative thresholding across correlated variants.\nFor foundation model applications, knockoffs offer several advantages. First, they provide principled feature selection for high-dimensional embeddings where standard regularization may be insufficient—selecting which embedding dimensions carry genuine predictive signal versus which reflect noise that degrades performance through the dimensionality trap (?sec-ch21-noise-accumulation). Second, they can distinguish true epistasis from “phantom epistasis” created by LD confounding: apparent variant interactions that actually reflect joint tagging of single causal variants. Third, the framework extends naturally to testing whether foundation model attention patterns identify conditionally important positions rather than merely correlated ones.\nThe practical limitation is computational: knockoff construction requires estimating or specifying the joint distribution of variants, which scales poorly with variant count. Approximate methods using block-diagonal LD structure make genome-scale application tractable, but the approach is most powerful for targeted analysis of fine-mapped regions or selected feature sets rather than whole-genome screening.\n\n\n\n\n\n\nWorked Example: Mendelian Randomization for Drug Target Validation\n\n\n\nScenario: A foundation model predicts that inhibiting protein P will reduce cardiovascular disease (CVD) risk because P levels correlate with CVD in observational data. How would MR validate this?\nStep 1: Identify genetic instruments. Find variants associated with P levels (pQTLs). Suppose variant rs12345 reduces P levels by 10%.\nStep 2: Test association with outcome. In GWAS data, does rs12345 associate with reduced CVD risk?\nStep 3: Calculate causal estimate. If rs12345 reduces P by 10% and CVD risk by 5%, the implied causal effect is: 5%/10% = 0.5 (50% reduction in CVD per unit reduction in P).\nStep 4: Check assumptions. Is rs12345 pleiotropic? Does it affect CVD through other pathways? MR-Egger and weighted median methods can test for pleiotropy.\nInterpretation: If MR supports causality, P is a promising drug target. If not, the observational correlation may reflect confounding, and inhibiting P may not reduce CVD.\n\n\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nGWAS identifies regions containing multiple correlated variants. Before reading about fine-mapping, predict: What information would help us identify which specific variant is causal? Why might simply choosing the variant with the strongest p-value be misleading?\n\n\n\n\n25.2.3 Fine-Mapping for Causal Variants\nGenome-wide association studies identify genomic loci associated with traits but typically cannot pinpoint causal variants. Most GWAS signals arise from common variants in linkage disequilibrium (LD) with the true causal variant(s), creating “association signals” that span many correlated SNPs (Section 3.3). Fine-mapping aims to identify which variant(s) within an associated locus are causal.\nStatistical fine-mapping methods compute posterior probabilities that each variant is causal given the observed association statistics and LD structure (Maller et al. 2012; Benner et al. 2016). These methods output credible sets: minimal sets of variants that contain the causal variant(s) with high probability (typically 95%). Smaller credible sets indicate more confident localization.\nWhy does fine-mapping work, and why does it sometimes fail? Fine-mapping exploits the fact that causal variants generate stronger association signals than non-causal variants that merely correlate through LD. By modeling the LD structure explicitly, fine-mapping can disentangle which variant is most likely causal. The method fails when LD is too tight (multiple variants are nearly perfectly correlated, making them statistically indistinguishable) or when multiple causal variants exist in the same region (violating the typical single-causal-variant assumption).\nFunctional annotations dramatically improve fine-mapping. Variants in regulatory elements, coding regions, or conserved sequences are more likely to be causal than variants in unconstrained regions. Foundation models can provide these annotations at unprecedented resolution, predicting variant effects on chromatin accessibility, transcription factor binding, splicing, and protein function (Chapter 17). Integrating foundation model predictions with statistical fine-mapping creates more powerful methods for causal variant identification.\n\n\n25.2.4 From GWAS to Causal Genes\nEven after fine-mapping identifies likely causal variants, connecting variants to causal genes remains challenging. Most GWAS signals fall in non-coding regions, often affecting expression of genes other than the nearest gene through long-range enhancer-promoter interactions. The “GWAS-to-gene” problem asks: given a causal variant, which gene(s) does it affect, and how?\nMultiple lines of evidence inform gene assignment:\n\nExpression quantitative trait loci (eQTLs): Variants that affect expression of nearby genes suggest regulatory mechanisms. Colocalization of GWAS and eQTL signals supports a shared causal variant affecting both expression and phenotype (giambartolomei_bayesian_2014?).\nChromatin interaction data: Hi-C and related methods identify physical contacts between enhancers and promoters (Section 20.2.1), enabling annotation of which genes regulatory variants might contact.\nCoding variant enrichment: When fine-mapped variants include coding variants, the affected gene is immediately implicated.\nFoundation model predictions: DNA sequence models can predict effects of non-coding variants on regulatory element activity and gene expression, providing computational support for regulatory mechanisms (Chapter 16, Chapter 17).\n\nIntegrating these evidence types into coherent gene prioritization frameworks remains an active area. No single method provides definitive causal gene assignment; converging evidence across multiple approaches provides the strongest support.\n\n\n\n\n\n\nFrom GWAS to causal gene identification\n\n\n\n\nFigure 25.2: From GWAS to causal gene. Stage 1: GWAS identifies associated locus with multiple variants in linkage disequilibrium. Stage 2: Fine-mapping with foundation model functional annotations narrows to credible set. Stage 3: Multiple evidence integration—eQTL colocalization, chromatin contacts, FM variant predictions—prioritizes target gene. Stage 4: Experimental validation provides causal ground truth. Foundation models accelerate stages 2-3 but cannot substitute for experimental validation.\n\n\n\n\n\n\n\n\n\nRecall and Connect\n\n\n\nFrom Chapter 3: Recall that linkage disequilibrium (LD) causes multiple variants to be correlated. Now consider: How does fine-mapping use LD structure to identify causal variants? Why would tight LD make causal variant identification harder even with perfect statistical power?",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Causal Inference with Foundation Models</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch25-causal.html#sec-ch25-fm-causality",
    "href": "part_5/p5-ch25-causal.html#sec-ch25-fm-causality",
    "title": "25  Causal Inference with Foundation Models",
    "section": "25.3 Foundation Models and Causality",
    "text": "25.3 Foundation Models and Causality\n\n25.3.1 Can Foundation Models Learn Causal Structure?\nFoundation models are trained on observational data through objectives like next-token prediction or masked element reconstruction. These objectives do not distinguish causal from correlational relationships. A DNA language model trained on sequences across species learns patterns that reflect evolutionary conservation, but conservation conflates multiple causal processes: purifying selection against deleterious mutations, hitchhiking of neutral variants with beneficial ones, and mutational biases that vary across the genome.\nRecent theoretical work examines conditions under which causal structure emerges from observational learning. In language models, there is evidence that models trained on sufficiently diverse text corpora learn something resembling causal structure, because text describes causal relationships and generating coherent text requires modeling them (kiciman_causal_2023?). Whether similar arguments apply to genomic sequence is unclear. Genomic sequences do not describe causal relationships; they embody them. A regulatory element’s sequence determines its function, but this determination is mediated by cellular machinery that the sequence model never observes.\nEmpirical evidence suggests foundation models capture aspects of causal structure but do not reliably distinguish causal from correlational patterns. Models trained on expression data learn gene-gene relationships that sometimes reflect regulatory causation and sometimes reflect co-regulation by shared factors. Models trained on perturbation data (e.g., CRISPR screens) show improved ability to predict intervention effects, suggesting that interventional training data is necessary for interventional prediction capability.\n\n\n\n\n\n\nKnowledge Check: Observational vs. Interventional Learning\n\n\n\nConsider two foundation models: Model A is trained on observational gene expression data (measuring expression without perturbations). Model B is trained on Perturb-seq data (expression measured after CRISPR knockouts).\n\nWhich model is more likely to learn causal gene-gene relationships? Why?\nIf Model A learns that genes X and Y are co-expressed, what causal scenarios could explain this?\nIf Model B learns that knocking out X changes Y expression, what does this tell us about causality?\n\nThe key distinction: Model A learns associations that could reflect common causes; Model B learns from interventions that directly test causal relationships.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nModel B is more likely to learn causal relationships because interventional data (CRISPR knockouts) directly tests causation by breaking correlations with confounders. (2) X and Y co-expression in observational data could reflect: X regulates Y, Y regulates X, both are regulated by a common upstream factor, or spurious correlation from batch effects. (3) If knocking out X changes Y expression, this provides strong evidence that X causally influences Y (either directly or through intermediates), because the intervention breaks non-causal correlations.\n\n\n\n\n\n\n\n\n25.3.2 In-Silico Perturbation Prediction\nYou want to know if editing a regulatory element will increase expression of a therapeutic gene. The experiment would take months and cost thousands of dollars. Can you get a useful prediction first? If a model could accurately predict the effect of your edit before you make it, you could prioritize the most promising candidates and avoid wasting resources on variants that will not work.\nIn-silico perturbation prediction uses foundation models to predict effects of genetic or molecular changes without performing experiments. This directly addresses rung 2 of the causal ladder: “What happens if we change X?”\nSeveral approaches exist:\nSequence-level perturbation: DNA and RNA foundation models can predict effects of sequence mutations on molecular phenotypes like chromatin accessibility, transcription factor binding, and splicing (Chapter 17). These predictions are inherently counterfactual: the model compares predicted output for reference vs. alternate allele. When validated against experimental perturbations, such predictions can support causal reasoning about regulatory mechanisms.\nGene-level perturbation: Single-cell foundation models trained on expression data can be prompted with in-silico gene knockouts or overexpression, generating predictions of downstream expression changes (theodoris_transfer_2023?). These predictions extrapolate from patterns learned in observational data, with accuracy depending on whether observational patterns reflect causal regulation.\nEmbedding-space perturbation: Models that embed cells, genes, or sequences in latent spaces enable perturbation by arithmetic operations on embeddings. Subtracting a “disease” direction and adding a “healthy” direction generates predictions of therapeutic effects. Such approaches assume linear structure in embedding space that may not hold.\nAll in-silico perturbation methods face a fundamental limitation: they cannot validate their own causal accuracy. A model that predicts X causes Y might be correct (X causes Y), might be learning reverse causation (Y causes X, so perturbing X in the model disrupts learned correlations), or might be learning confounded correlations (neither causes the other). External validation through experimental perturbation is necessary to establish causal accuracy.\nThe table below compares different in-silico perturbation approaches:\n\n\n\nTable 25.2: Comparison of in-silico perturbation methods. Validation against experimental interventions is essential for all approaches.\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nInput\nOutput\nCausal Validity\nValidation Required\n\n\n\n\nSequence-level (e.g., Enformer)\nReference vs. alternate allele\nPredicted molecular phenotype change\nModerate—predicts direct sequence effects\nMPRA, allelic expression\n\n\nGene-level (e.g., Geneformer)\nIn-silico knockout\nPredicted expression changes\nLow—extrapolates from correlations\nPerturb-seq, CRISPR screens\n\n\nEmbedding arithmetic\nVector operations in latent space\nTransformed cell state\nLow—assumes linear embedding structure\nExperimental perturbation\n\n\n\n\n\n\n\n\n25.3.3 Counterfactual Reasoning Limitations\nA patient received drug A and her tumor progressed. Her oncologist wonders: would drug B have worked better? This is not a question about average treatment effects in a population—it is about what would have happened to this specific patient under a different treatment. No dataset contains this answer because it requires observing the same patient under two mutually exclusive conditions simultaneously. This is the counterfactual problem, and it represents a fundamental barrier even for the most sophisticated models.\nCounterfactual reasoning—rung 3 of the causal ladder—asks what would have happened under alternative circumstances for a specific individual or instance. This is qualitatively harder than intervention reasoning, which asks about population-level effects of interventions.\nFoundation models face fundamental barriers to counterfactual reasoning:\nIdentifiability: Counterfactual quantities are often not identifiable from observational data even with perfect knowledge of the joint distribution. Learning from data cannot overcome this barrier.\nIndividual-level noise: Counterfactuals require reasoning about stochastic processes at the individual level. What would this specific cell’s expression have been if a specific gene were knocked out? The answer depends on molecular noise that models cannot capture from population-level training.\nTemporal specificity: Counterfactuals often involve specific timepoints and histories. “What would this patient’s outcome have been if treatment started earlier?” requires reasoning about patient-specific trajectories that models trained on cross-sectional data cannot address.\nThese limitations suggest that foundation models can at best approximate counterfactual reasoning for population-level interventions but cannot provide reliable individual-level counterfactuals without additional assumptions or data. Clinical applications requiring individual counterfactual reasoning (e.g., precision treatment optimization) must acknowledge this fundamental limitation.\n\n\n\n\n\n\nKey Insight: The Counterfactual Barrier\n\n\n\nIndividual counterfactuals (“What would have happened to this patient if…?”) require information that does not exist in any dataset: the outcome under the path not taken. This is not a data limitation that more training can overcome—it is a logical impossibility. Foundation models can estimate population-average treatment effects (how do patients like this one respond on average?) but cannot answer individual counterfactuals without additional structural assumptions. Clinical systems claiming to provide personalized counterfactual predictions should be scrutinized for what assumptions they make to bridge this logical gap.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Causal Inference with Foundation Models</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch25-causal.html#sec-ch25-intervention-prediction",
    "href": "part_5/p5-ch25-causal.html#sec-ch25-intervention-prediction",
    "title": "25  Causal Inference with Foundation Models",
    "section": "25.4 Intervention Prediction",
    "text": "25.4 Intervention Prediction\nDespite the limitations above, foundation models can contribute substantially to intervention prediction when combined with appropriate experimental data, validation frameworks, and acknowledgment of causal assumptions.\n\n25.4.1 CRISPR Screen Analysis with Foundation Models\nYou have run a genome-wide CRISPR screen and identified 200 genes whose knockout affects cancer cell viability. But which of these are direct drivers versus downstream consequences? Which hits will replicate in patients rather than just in cell lines? And for the thousands of genes you could not include in this screen, can you predict which would have shown effects?\nCRISPR screens provide large-scale interventional data: systematic gene knockouts or knockdowns across cells, with readouts including viability, expression, and phenotype (shalem_genome-scale_2014?; adamson_multiplexed_2016?). This data is inherently causal—it captures effects of interventions rather than mere associations. Foundation models can help interpret, extend, and transfer insights from these screens.\nFoundation models enhance CRISPR screen analysis in several ways:\nScreen design: Models can predict which guide RNAs will effectively perturb target genes, improving screen efficiency. Expression foundation models can predict baseline expression levels that affect perturbation detectability.\nHit interpretation: When screens identify genes whose perturbation affects a phenotype, foundation models help interpret mechanism. Which regulatory networks are affected? What downstream targets change? Integration with interaction networks (Section 21.2.2) contextualizes screen hits.\nTransfer and extrapolation: Foundation models trained on screens in one context (cell type, condition) can predict perturbation effects in new contexts. This transfer capability enables virtual screens that guide experimental prioritization.\nCombination effects: Predicting effects of multi-gene perturbations from single-gene data is a key challenge. Foundation models that learn gene-gene relationships from expression data can model epistatic interactions, though prediction accuracy for combinations remains limited.\nImportantly, foundation models trained on CRISPR screen data acquire interventional rather than merely associational patterns. This provides a path toward causal prediction capability: train on interventional data to learn interventional structure.\n\n\n25.4.2 Drug Response Prediction\nA patient has a tumor with an unusual mutation profile. Several drugs might work, but which one? Testing all options experimentally would take months the patient may not have. If you could predict which drugs the tumor would respond to based on its molecular profile, you could prioritize the most promising option first. But here lies the challenge: observed correlations between tumor features and drug response might reflect the drugs actually used (selection bias), patient characteristics that affect both tumor biology and outcomes (confounding), or genuine causal sensitivity—and only the last matters for choosing therapy.\nPredicting how patients or tumors will respond to drugs is a central challenge in precision oncology and pharmacogenomics. Drug response has clear causal structure: the drug causes the response. Foundation models can contribute to response prediction by learning patterns that generalize across drugs, cell lines, and patients.\nApproaches include:\nChemical-biological foundation models: Models that jointly embed drug structures and biological contexts (gene expression, mutations) can predict response to drugs not seen during training (zitnik_modeling_2018?). Transfer from chemical structure to biological effect leverages foundation model representations of both domains.\nExpression-based response models: Single-cell foundation models can predict expression changes induced by drug treatment, enabling in-silico drug screening. The accuracy of these predictions depends on whether training data captures the relevant drug-gene relationships.\nGenomic response predictors: Foundation models pretrained on DNA sequence can be fine-tuned to predict drug response from tumor genomes, learning patterns of sensitivity-conferring and resistance-conferring mutations.\nDrug response prediction illustrates both the promise and limitations of foundation models for causal tasks. Models can learn generalizable patterns of drug-gene interaction, but validation requires clinical trials that are expensive and slow. The gap between in-silico prediction and validated clinical utility remains substantial (Chapter 29).\n\n\n\n\n\n\nStop and Think: Validating Drug Response Predictions\n\n\n\nA foundation model predicts that tumors with mutation M will respond to drug D. Before using this prediction clinically:\n\nWhat training data would make this prediction more trustworthy? (Hint: observational vs. interventional)\nHow would you distinguish whether M predicts response because M causes sensitivity, or because M correlates with some other feature that causes sensitivity?\nWhat validation hierarchy would you design? Consider in-vitro, in-vivo, and clinical evidence.\n\n\n\n\n\n25.4.3 Closed-Loop Experimental Validation\nA foundation model predicts that knocking out gene X will sensitize cancer cells to drug Y. You test this prediction experimentally and find it is wrong. What now? You could dismiss the model, but a smarter approach is to use this failure to improve future predictions. The model was wrong because its training data lacked the relevant causal relationship. By feeding the experimental result back into training, you give the model interventional ground truth it could never learn from observational data alone.\nThe most powerful paradigm for developing causally accurate foundation models is closed-loop integration of prediction and experiment. Rather than training models on fixed datasets and deploying them for prediction, closed-loop systems iterate between:\n\nPrediction: Model proposes interventions likely to be informative or effective\nExperiment: Proposed interventions are tested in automated assay platforms\nObservation: Experimental outcomes are recorded\nUpdate: Results update model parameters or inform next predictions\n\nThis design-build-test-learn (DBTL) cycle is discussed extensively in Section 30.6 for sequence design applications. The same framework applies to causal learning: by iterating between prediction and experimental validation, models can accumulate interventional data that supports increasingly accurate causal predictions.\nClosed-loop systems face practical challenges: experimental throughput limits iteration speed, costs constrain scale, and defining informative interventions requires balancing exploration and exploitation. But the fundamental advantage—learning from interventional rather than observational data—addresses the core limitation of standard foundation model training.\n\n\n\n\n\n\nClosed-loop causal learning for foundation models\n\n\n\n\nFigure 25.3: Closed-loop causal learning for foundation models. The cycle iterates between: (1) Predicting intervention effects from current knowledge, (2) Executing prioritized interventions on automated platforms, (3) Capturing experimental readouts, and (4) Updating the model with interventional data. Unlike standard training on observational data, closed-loop systems accumulate interventional ground truth that progressively improves causal accuracy—providing a path from Rung 1 (association) toward Rung 2 (intervention) of the causal ladder.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Causal Inference with Foundation Models</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch25-causal.html#sec-ch25-causal-discovery",
    "href": "part_5/p5-ch25-causal.html#sec-ch25-causal-discovery",
    "title": "25  Causal Inference with Foundation Models",
    "section": "25.5 Causal Discovery",
    "text": "25.5 Causal Discovery\nSuppose you measure expression of 20,000 genes across thousands of cells and find that genes A and B are correlated. Does A regulate B? Does B regulate A? Are both controlled by an upstream regulator C that you did not measure? Without perturbation experiments—which are expensive and cannot test all 400 million possible pairwise relationships—how would you even begin to answer this question?\nCausal discovery aims to learn causal structure itself from observational data: which variables cause which others? In genomics, this includes learning regulatory networks, identifying driver mutations, and discovering mechanistic relationships. The challenge is extracting directional, causal information from data that only shows correlation.\n\n25.5.1 Learning Regulatory Networks\nA cancer researcher identifies a transcription factor that is consistently overexpressed in aggressive tumors. Is this factor driving tumor progression, or is it merely a downstream consequence of some other oncogenic program? If you could reconstruct the regulatory network—the web of cause-and-effect relationships among genes—you could trace the chain of causation and identify the true driver. But how do you learn such a network from expression data alone?\nGene regulatory networks describe causal relationships among genes: transcription factors regulate targets, signaling molecules activate pathways, metabolic enzymes control flux. Learning these networks from data is a foundational problem in systems biology—one where foundation models offer new approaches but also face fundamental limitations.\nClassical approaches infer networks from expression correlation, mutual information, or regression-based methods like GENIE3 (huynh-thu_inferring_2010?). These methods identify associations but struggle to distinguish causal direction and are confounded by common regulators.\nFoundation models offer new approaches to regulatory network inference:\nAttention-based structure learning: Transformer foundation models learn attention patterns over genes. These attention weights can be interpreted as soft regulatory relationships, with attention from gene A to gene B suggesting A influences B’s representation. However, attention weights reflect model computation, not necessarily biological causation (jain_attention_2019?).\nPerturbation-guided learning: Training foundation models on perturbation data (e.g., Perturb-seq, which combines CRISPR perturbation with single-cell RNA-seq) enables learning of directed regulatory relationships. If knocking out A changes B, A to B is supported. Foundation models scale this reasoning across thousands of perturbations.\nMulti-task learning: Models trained to simultaneously predict multiple molecular phenotypes (expression, chromatin state, binding) may learn shared structure reflecting underlying regulatory networks.\nNetwork inference from foundation models remains an active research area. Validation against gold-standard regulatory relationships (e.g., ChIP-seq, perturbation experiments) is essential for assessing accuracy.\n\n\n\n\n\n\nRecall and Connect\n\n\n\nFrom Chapter 24: We learned that attention weights in transformers show which inputs the model considers important. Now think critically: If attention from gene A to gene B is high, does this prove A regulates B? What’s the difference between computational attention and biological causation?\n\n\n\n\n25.5.2 Temporal Causality\nYou observe that in differentiating stem cells, transcription factor A rises before gene B is expressed. Does A activate B? Or do both respond to an earlier signal, with A simply responding faster? In cross-sectional snapshots, you cannot tell—both scenarios produce identical correlations. But in time-series data, the ordering matters: if A consistently rises before B changes across many conditions, this temporal precedence provides evidence for causal direction that pure correlation cannot.\nTime provides a strong constraint on causal direction: causes precede effects. Time-series data in genomics—developmental trajectories, drug response time courses, circadian cycles—enable causal inference approaches that exploit temporal structure.\nGranger causality tests whether past values of X improve prediction of future Y beyond what past Y alone provides (granger_investigating_1969?). The underlying logic is that causes must precede their effects—like how dark clouds appear before rain, not after. If you can better predict tomorrow’s rain by knowing today’s cloud patterns than by knowing only yesterday’s rain, clouds are “Granger-causing” rain. In genomics, this approach identifies genes whose expression changes precede changes in other genes: if knowing gene A’s expression at time 1 helps predict gene B’s expression at time 2 (beyond what B’s own history provides), this suggests A may regulate B.\nDynamical foundation models trained on time-series single-cell data (e.g., RNA velocity measurements) learn temporal dynamics and can be queried for causal relationships (la_manno_rna_2018?). By modeling how expression states evolve, these models implicitly learn which genes drive transitions.\nStructural causal models with temporal constraints encode the assumption that causes precede effects, enabling stronger causal conclusions from time-series observational data. Foundation models can be trained with temporal structure as an architectural prior.\nTemporal approaches require appropriate data: longitudinal measurements, developmental time courses, or perturbation time series. Cross-sectional data, which comprises most genomic datasets, cannot support temporal causal inference directly.\n\n\n25.5.3 Multi-Omics Causal Structure\nA genetic variant associates with both mRNA levels of gene X and disease risk. Does the variant cause disease by altering X’s expression? Or does it affect disease through some other pathway entirely, with the mRNA association being a red herring? If you also measure protein levels and find the variant affects mRNA but not protein—perhaps due to post-transcriptional regulation—you have learned something crucial: the mRNA association cannot explain the disease effect, and you should look elsewhere.\nDifferent molecular modalities (DNA, RNA, protein, metabolite) are linked by known causal relationships: DNA encodes RNA, RNA is translated to protein, proteins catalyze metabolic reactions. Multi-omics data that measures multiple modalities simultaneously enables causal inference that leverages this structure.\nFor example, eQTL analysis identifies genetic variants that causally affect expression. Extending to protein quantitative trait loci (pQTLs) and metabolite QTLs (mQTLs) traces causal effects from genome through transcriptome to proteome to metabolome. Discordance between levels (e.g., an eQTL without corresponding pQTL) suggests post-transcriptional regulation.\nFoundation models trained on multi-omic data can learn cross-modality relationships. Whether these relationships are causal depends on training: models trained on QTL data learn causal structure because genetic variation is the instrument; models trained on matched multi-omic profiles learn associations that may reflect common causes.\nMulti-omic integration for causal inference is discussed further in Chapter 22. Foundation models can integrate across modalities but require causal validation as for single-modality models.\n\n\n\n\n\n\nStop and Think: Causal Structure Across Omics Layers\n\n\n\nThe central dogma (DNA to RNA to protein) provides causal direction: genetic variants cause expression changes, which cause protein level changes.\n\nIf a variant associates with both mRNA and protein levels, what additional evidence would distinguish direct transcriptional effects from post-transcriptional regulation?\nIf a variant associates with mRNA but not protein levels, what biological mechanisms might explain this?\nHow could a foundation model leverage this multi-omic structure to improve causal predictions?",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Causal Inference with Foundation Models</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch25-causal.html#sec-ch25-clinical-implications",
    "href": "part_5/p5-ch25-causal.html#sec-ch25-clinical-implications",
    "title": "25  Causal Inference with Foundation Models",
    "section": "25.6 Clinical Implications",
    "text": "25.6 Clinical Implications\n\n25.6.1 Drug Target Validation Evidence Hierarchy\nYour foundation model identifies protein X as strongly associated with disease progression. Should you invest $50 million to develop a drug targeting X? Association alone cannot answer this question. What if X is merely a biomarker of aggressive disease, not a driver? What if inhibiting X has no effect—or makes things worse? Before committing to expensive clinical development, you need to climb from association toward causal evidence.\nDrug development requires confidence that a target is causally involved in disease—that modulating the target will affect disease outcomes. Foundation model predictions contribute to this evidence base but sit within a broader hierarchy of target validation evidence:\nWeakest evidence: Association. The target’s expression or activity correlates with disease in observational data. Foundation models excel at identifying such associations but cannot distinguish causal from confounded relationships.\nModerate evidence: Mendelian randomization. Genetic instruments affecting the target causally affect disease risk. This provides human in-vivo evidence of causation but may reflect effects of lifetime exposure rather than therapeutic intervention.\nStrong evidence: Perturbation experiments. Knocking out or modulating the target in cellular or animal models affects disease-relevant phenotypes. Foundation models trained on perturbation data can predict such effects but require experimental validation.\nStrongest evidence: Clinical intervention. Drugs targeting the mechanism show efficacy in clinical trials. This is the ultimate validation but comes late in development.\nFoundation models can accelerate target validation by integrating across evidence types: identifying associations, predicting perturbation effects, and prioritizing candidates for experimental validation (Chapter 29). But they cannot substitute for experimental and clinical evidence—they can only prioritize which targets receive experimental investment.\n\n\n\n\n\n\nEvidence hierarchy for drug target validation\n\n\n\n\nFigure 25.4: Evidence hierarchy for drug target validation. Weakest: observational association—foundation models excel here but associations may be confounded. Moderate: Mendelian randomization provides human in-vivo causal evidence. Strong: Experimental perturbation directly tests causation in model systems. Strongest: Clinical trial efficacy is definitive but expensive. Foundation models accelerate association discovery, MR instrument selection, and perturbation prioritization, but cannot substitute for the experimental and clinical evidence required for confident target validation.\n\n\n\n\n\n\n\n\n\nPractical Guidance: Communicating Causal Claims\n\n\n\nWhen presenting foundation model predictions to clinical collaborators or in publications:\n\nDistinguish association from causation explicitly: “The model predicts that variant X associates with outcome Y” is different from “inhibiting X will improve Y.”\nState what validation would be needed: “This prediction suggests X as a candidate target; validation would require [MR analysis / perturbation experiment / clinical trial].”\nQuantify confidence appropriately: Predictive confidence (model calibration) is not causal confidence. A well-calibrated association prediction may still reflect confounding.\nAcknowledge the evidence tier: “This is associational evidence; stronger causal evidence would require genetic instruments or experimental perturbation.”\n\n\n\n\n\n25.6.2 Regulatory Requirements for Causal Claims\nYou have built an AI system that predicts which patients will benefit from a particular therapy. Is this a risk stratification tool that flags high-risk patients for clinical review? Or is it a treatment recommendation system that claims to know what intervention will help? The distinction matters enormously for regulatory approval: the first requires demonstration of predictive accuracy, while the second requires evidence that following the recommendations actually improves outcomes.\nRegulatory agencies evaluate medical AI systems based on their intended use. Systems that make causal claims face higher evidentiary standards than purely predictive systems.\nA risk stratification model that identifies high-risk patients without claiming to identify treatable causes requires validation of predictive accuracy: does the model correctly identify who is at risk? This is achievable through retrospective validation on held-out data.\nA treatment recommendation model that suggests interventions based on predicted causal effects requires validation of causal accuracy: do the recommended interventions actually produce the predicted effects? This requires prospective trials comparing outcomes for patients who receive model-guided vs. standard treatment.\nCurrent regulatory frameworks (Section 26.1) do not fully distinguish predictive from causal validation, but the distinction has practical implications. Foundation model predictions deployed as associational tools (risk scores, phenotype predictions) face achievable validation requirements. The same models deployed as causal tools (treatment recommendations, target prioritization) face requirements that may be impractical without substantial prospective evidence.\nDevelopers of foundation model systems should consider intended use carefully. Claiming causal capabilities that cannot be validated creates both regulatory risk and potential patient harm. Limiting claims to predictive performance, while acknowledging causal limitations, provides a more defensible regulatory path while appropriately caveating clinical use.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Causal Inference with Foundation Models</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch25-causal.html#sec-ch25-conclusion",
    "href": "part_5/p5-ch25-causal.html#sec-ch25-conclusion",
    "title": "25  Causal Inference with Foundation Models",
    "section": "25.7 Looking Forward",
    "text": "25.7 Looking Forward\nCausal inference remains one of the deepest challenges in genomic foundation models. The gap between prediction (rung 1) and intervention (rung 2) is not merely a matter of scale or compute—it reflects fundamental differences in what can be learned from observational vs. interventional data. Foundation models trained on observational genomic sequences can achieve remarkable predictive accuracy while remaining unreliable for causal reasoning.\nThree paths forward seem most promising:\nTraining on interventional data: Foundation models trained on CRISPR screens, drug response data, and other perturbation experiments acquire interventional rather than merely associational patterns. As high-throughput perturbation platforms generate more data, foundation models trained on this data will become increasingly capable of causal prediction.\nIntegration with causal inference methods: Combining foundation model predictions with established causal inference frameworks (Mendelian randomization, fine-mapping, structural causal models) leverages the complementary strengths of each approach. Foundation models provide scale and pattern recognition; causal frameworks provide principled reasoning about intervention.\nClosed-loop experimental systems: Iterating between foundation model prediction and experimental validation creates feedback loops that progressively improve causal accuracy. Such systems require infrastructure investment but offer a path to causally validated foundation models.\nThe frontier challenges in causal reasoning are examined further in Section 31.1.3. For now, practitioners should approach foundation model predictions with appropriate epistemic humility: impressive predictive accuracy does not imply causal validity, and clinical interventions require causal rather than merely correlational evidence.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nDescribe Pearl’s three rungs of the ladder of causation. Why can’t you reach Rung 2 (intervention) from Rung 1 (association) through more data or better prediction accuracy alone?\nA foundation model achieves 95% accuracy predicting disease outcome from gene expression. List three different causal structures that could produce this correlation, and explain why they matter for treatment decisions.\nWhat is Mendelian randomization, and what properties of genetic variants make it a valid causal inference tool?\nHow can foundation models be trained to support causal reasoning? What types of training data would enable movement from associational to interventional predictions?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nRung 1 (Association): P(Y|X)—observing X tells us about Y. Rung 2 (Intervention): P(Y|do(X))—changing X affects Y. Rung 3 (Counterfactual): What would Y have been for this individual if X were different? More data at Rung 1 cannot reach Rung 2 because observational data conflates causal and confounded associations; no amount of correlation data distinguishes causation from common causes without interventional data or causal assumptions. (2) Three causal structures: (a) Gene expression directly causes disease—treatment targeting the gene would work; (b) Disease causes gene expression—targeting the gene would not help; (c) Both are caused by an unmeasured factor—gene is a biomarker but not a therapeutic target. These distinctions are critical for choosing whether to develop therapies targeting the gene. (3) Mendelian randomization uses genetic variants as instrumental variables to infer causation. Key properties: alleles are randomly assigned at conception (independence from confounders), variants affect an exposure (relevance), and under the exclusion restriction, variants affect outcomes only through the exposure. This creates a natural experiment analogous to randomized trials. (4) Train on interventional data: CRISPR screens, drug response experiments, and other perturbation data where effects of interventions are directly observed. Models trained on observational data learn associations; models trained on interventional data learn causal effects. Closed-loop systems that iterate between prediction and experimental validation provide the strongest path.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nThis chapter examined the gap between association (what we observe) and causation (what happens if we intervene)—a distinction critical for clinical application of foundation models.\nKey takeaways:\n\nPearl’s ladder of causation distinguishes association (rung 1), intervention (rung 2), and counterfactual reasoning (rung 3). Foundation models trained on observational data operate at rung 1; moving higher requires interventional data or causal assumptions.\nPredictive accuracy does not imply causal validity. A model that perfectly predicts Y from X may be learning direct causation, reverse causation, common causes, or confounding. Only external validation can distinguish these scenarios.\nGenomics offers unique causal inference tools. Mendelian randomization exploits genetic randomization at conception; fine-mapping localizes causal variants; multi-omic QTL analysis traces causal chains across molecular layers.\nFoundation models can contribute to causal inference through in-silico perturbation prediction, CRISPR screen analysis, and drug response modeling—but only when combined with appropriate validation against experimental ground truth.\nClinical stakes are high. Association-based predictions support screening and stratification. Causal claims support intervention. Confusing the two leads to ineffective treatments and potential patient harm.\nThree paths forward: Training on interventional data, integrating with causal inference frameworks, and building closed-loop experimental systems all offer routes toward causally valid foundation models.\n\nLooking ahead: Section 26.1 examines how regulatory frameworks evaluate AI systems, with implications for causal claims. Chapter 29 applies these concepts to drug target identification. Section 31.1.3 explores frontier challenges in causal reasoning for genomics.\n\n\n\n\n\n\nBenner, Christian, Chris C. A. Spencer, Aki S. Havulinna, Veikko Salomaa, Samuli Ripatti, and Matti Pirinen. 2016. “FINEMAP: Efficient Variable Selection Using Summary Data from Genome-Wide Association Studies.” Bioinformatics 32 (10): 1493–1501. https://doi.org/10.1093/bioinformatics/btw018.\n\n\nBowden, Jack, George Davey Smith, and Stephen Burgess. 2015. “Mendelian Randomization with Invalid Instruments: Effect Estimation and Bias Detection Through Egger Regression.” International Journal of Epidemiology 44 (2): 512–25. https://doi.org/10.1093/ije/dyv080.\n\n\nCandès, Emmanuel, Yingying Fan, Lucas Janson, and Jinchi Lv. 2018. “Panning for Gold: ’Model-X’ Knockoffs for High Dimensional Controlled Variable Selection.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 80 (3): 551–77. https://doi.org/10.1111/rssb.12265.\n\n\nDavey Smith, George, and Shah Ebrahim. 2003. “‘Mendelian Randomization’: Can Genetic Epidemiology Contribute to Understanding Environmental Determinants of Disease?*.” International Journal of Epidemiology 32 (1): 1–22. https://doi.org/10.1093/ije/dyg070.\n\n\nHartwig, Fernando Pires, George Davey Smith, and Jack Bowden. 2017. “Robust Inference in Summary Data Mendelian Randomization via the Zero Modal Pleiotropy Assumption.” International Journal of Epidemiology 46 (6): 1985–98. https://doi.org/10.1093/ije/dyx102.\n\n\nMaller, Julian B., Gilean McVean, Jake Byrnes, Damjan Vukcevic, Kimmo Palin, Zhan Su, Joanna M. M. Howson, et al. 2012. “Bayesian Refinement of Association Signals for 14 Loci in 3 Common Diseases.” Nature Genetics 44 (12): 1294–1301. https://doi.org/10.1038/ng.2435.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Causal Inference with Foundation Models</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch26-regulatory.html",
    "href": "part_5/p5-ch26-regulatory.html",
    "title": "26  Regulatory and Governance",
    "section": "",
    "text": "26.1 Regulatory Frameworks for Genomic AI\nGenomic foundation models exist in regulatory limbo. They are clearly software, sometimes medical devices, occasionally laboratory-developed tests, and frequently components of larger systems that span multiple regulatory categories. If you are familiar with drug approval, the parallels help: just as a new pharmaceutical must prove safety in Phase I, efficacy in Phase II/III, and demonstrate manufacturing quality before reaching patients, medical AI must demonstrate analytical validity (does it measure what it claims?), clinical validity (do measurements correlate with outcomes?), and appropriate quality systems. The difference is that drugs have well-worn regulatory pathways developed over a century, while AI-based medical devices navigate frameworks designed for deterministic software with traceable decision logic, not for neural networks that learn from data, evolve through fine-tuning, and produce outputs that even their developers cannot fully predict.\nNavigating this landscape requires understanding how different jurisdictions approach AI-based medical software, what evidence they require, and where genomic applications create novel challenges that existing pathways did not anticipate.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Regulatory and Governance</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch26-regulatory.html#sec-ch26-regulatory",
    "href": "part_5/p5-ch26-regulatory.html#sec-ch26-regulatory",
    "title": "26  Regulatory and Governance",
    "section": "",
    "text": "26.1.1 Software as Medical Device Paradigm\nRegulatory agencies worldwide classify AI-based clinical tools as software as a medical device (SaMD), a category that applies when software itself constitutes the medical device rather than merely controlling hardware. The International Medical Device Regulators Forum defines SaMD risk tiers based on the seriousness of the health condition and the role software plays in clinical decision-making: software that provides information to drive clinical management of a serious condition receives higher scrutiny than software that merely informs decisions about non-serious conditions (“Software as a Medical Device: Possible Framework for Risk Categorization and Corresponding Considerations  International Medical Device Regulators Forum” 2014).\n\n\n\n\n\n\nWorked Example: SaMD Classification for a Variant Classifier\n\n\n\nConsider a foundation model that classifies germline variants as pathogenic or benign for hereditary cancer syndromes. How would it be classified under the SaMD framework?\nStep 1: Assess condition seriousness. Hereditary breast and ovarian cancer syndrome (BRCA1/2) is a serious condition: pathogenic variants confer 45-85% lifetime breast cancer risk and 10-45% ovarian cancer risk. This places the condition in the “serious” category.\nStep 2: Assess decision role. The classifier directly informs diagnostic conclusions and treatment decisions (e.g., risk-reducing surgery, enhanced screening). This is “drive clinical management” rather than merely “inform.”\nStep 3: Determine risk tier. Serious condition + drive management = highest risk category (Class III in FDA terms, Class IIb/III in EU MDR).\nRegulatory consequence: This variant classifier would require a Pre-Market Approval (PMA) submission to FDA (not the faster 510(k) pathway), including clinical validation data demonstrating performance across ancestry groups, comparison to expert panel classifications, and evidence of clinical utility.\nIn contrast, a model that provides supplementary information about variant population frequency (for physician consideration alongside other evidence) might qualify for a lower risk tier and faster regulatory pathway.\n\n\n\n\n\n\n\n\nSaMD risk classification for genomic AI\n\n\n\n\nFigure 26.1: Software as Medical Device (SaMD) risk classification for genomic AI tools. The two-dimensional framework combines condition seriousness (non-serious → serious → critical) with decision role (inform → drive → diagnose/treat). Genomic variant classifiers for hereditary cancer syndromes fall in the highest-risk category (serious condition + drive management), requiring Pre-Market Approval (PMA) with clinical validation across ancestry groups. Tools providing population frequency data for physician consideration fall lower (inform only), potentially qualifying for faster 510(k) pathways. Color gradient indicates evidentiary burden: higher risk requires more extensive clinical validation.\n\n\n\nWhy this two-dimensional risk framework (condition severity times decision role)? The logic is that harm potential increases along both axes: misclassifying a benign skin lesion causes less harm than misclassifying a malignant tumor, and software that merely displays information for physician interpretation poses less risk than software that autonomously makes treatment decisions. By considering both dimensions, regulators can calibrate evidentiary requirements proportionally—demanding rigorous clinical trials for high-risk applications while enabling faster pathways for lower-risk tools.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading further, consider: What makes a genomic variant classifier different from, say, a radiology AI that identifies tumors in CT scans? Both inform clinical decisions about serious conditions. What unique challenges might genomic AI face that imaging AI does not?\n\n\nGenomic foundation models typically fall into higher-risk categories. A tool that classifies variants as pathogenic or benign directly influences diagnostic conclusions for conditions ranging from hereditary cancer syndromes to rare developmental disorders. The consequences of misclassification can be severe: a false negative may delay life-saving interventions, while a false positive may trigger unnecessary prophylactic surgery or cascade into family-wide psychological harm. Regulators accordingly require substantial evidence across the ACCE framework: analytical validity (does the model measure what it claims to measure?), clinical validity (does measurement correlate with the clinical outcome?), and in some cases clinical utility (does using the model improve patient outcomes?) (“ACCE Model Process for Evaluating Genetic Tests  CDC” 2004).\nThe FDA’s approach to AI-enabled devices has evolved considerably since the first autonomous diagnostic AI received FDA clearance in 2018 (Abràmoff et al. 2018). The agency now distinguishes between “locked” algorithms whose behavior is fixed at approval and “adaptive” algorithms that continue learning from new data after deployment (Administration 2021). Most foundation models fall into neither category cleanly: their weights are frozen after pretraining, but their outputs depend on prompts, fine-tuning, or downstream heads that may change across applications. This architectural ambiguity creates regulatory uncertainty. A foundation model serving as the backbone for multiple clinical applications might require separate submissions for each use case, or a single submission might cover the shared backbone while individual fine-tuned heads receive separate clearances.\n\n\n\n\n\n\nKey Insight: The Locked vs. Adaptive Dilemma\n\n\n\nFoundation models challenge the FDA’s locked/adaptive dichotomy. A model with frozen pretrained weights appears “locked,” but fine-tuning for specific clinical applications, prompt engineering, and evolving downstream heads create “adaptive” behavior in practice. This architectural flexibility—a technical strength—becomes a regulatory liability when approval pathways assume static behavior.\n\n\n\n\n26.1.2 European and Global Regulatory Landscapes\nThe European Union’s approach differs from the FDA’s in several respects relevant to genomic AI. The EU Medical Device Regulation (MDR), which fully replaced prior directives in 2021, classifies standalone software according to similar risk principles but places greater emphasis on conformity assessment by notified bodies rather than centralized agency review (“Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on Medical Devices, Amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and Repealing Council Directives 90/385/EEC and 93/42/EEC (Text with EEA Relevance. )” 2017). For high-risk software, manufacturers must demonstrate compliance with essential safety and performance requirements through technical documentation, quality management systems, and post-market surveillance plans. The AI Act, which entered force in 2024, adds another regulatory layer: high-risk AI systems (including those used in medical diagnosis) must meet transparency, robustness, and human oversight requirements that go beyond device-specific regulations (“Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 Laying down Harmonised Rules on Artificial Intelligence and Amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA Relevance)” 2024).\nRegulatory divergence across jurisdictions creates practical challenges for global deployment. A genomic foundation model cleared by the FDA may require separate CE marking for European markets, TGA approval in Australia, and PMDA review in Japan, each with distinct evidentiary standards and submission formats. Harmonization efforts through the International Medical Device Regulators Forum provide common frameworks for definitions and risk classification, but substantive requirements continue to differ (“Software as a Medical Device (SaMD): Clinical Evaluation  International Medical Device Regulators Forum” 2017). Companies developing clinical-grade genomic AI must either design validation programs that satisfy the most stringent jurisdiction or pursue market-by-market strategies that delay access in some regions.\n\n\n\nTable 26.1: Comparison of major regulatory frameworks for genomic AI\n\n\n\n\n\n\n\n\n\n\n\n\nDimension\nFDA (United States)\nEU MDR + AI Act\nPMDA (Japan)\nTGA (Australia)\n\n\n\n\nPrimary pathway\n510(k), De Novo, PMA\nConformity assessment via notified body\nPMDA review with MHLW approval\nConformity assessment\n\n\nRisk classification\nClass I, II, III\nClass I, IIa, IIb, III\nClass I-IV\nClass I, IIa, IIb, III\n\n\nAdaptive AI stance\nPredetermined Change Control Plan\nPost-market surveillance emphasis\nCase-by-case evaluation\nFollows IMDRF guidance\n\n\nLDT oversight\nExpanding (2024 rule)\nIncluded under IVD Regulation\nLaboratory discretion\nLimited\n\n\nAI-specific rules\nFramework guidance\nAI Act (2024)\nSoft law guidance\nFollows international norms\n\n\nTypical timeline\n6-18 months\n12-24 months\n12-18 months\n6-12 months\n\n\n\n\n\n\nThe regulatory landscape for laboratory-developed tests (LDTs) further complicates matters in the United States. Clinical laboratories have historically been able to develop and offer tests under their own validation without FDA premarket review, relying instead on CLIA certification and state licensure. Many clinical genomics laboratories use in-house bioinformatics pipelines, variant callers, and annotation tools that incorporate machine learning components without seeking FDA clearance. Recent FDA guidance signals intent to assert greater oversight over LDTs, particularly those using complex algorithms, but the boundary between regulated devices and unregulated laboratory procedures remains contested (“Medical Devices; Laboratory Developed Tests” 2024).\n\n\n26.1.3 Validation Requirements for Clinical Genomic AI\nRegulatory submissions for genomic AI devices require validation evidence spanning multiple dimensions. Analytical validation typically involves demonstrating that the model performs consistently across different sequencing platforms, library preparation methods, and sample types. For a variant effect predictor, this might include showing that scores remain calibrated when inputs come from whole-genome sequencing versus targeted panels, from fresh blood versus archived FFPE tissue, or from healthy individuals versus cancer patients with complex somatic variation.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nThe chapter on confounding (Chapter 12) introduced a critical validation pitfall. Can you recall what it is? How might this same pitfall affect regulatory submissions for a variant classifier trained on ClinVar data?\nHint: Think about where training labels come from and how evaluation benchmarks are constructed.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe critical pitfall is circular validation or data leakage—when the same data (or closely related data) appears in both training and evaluation. For ClinVar-trained classifiers, this manifests when variants from the same genes, families, or submissions appear in both train and test sets, inflating performance estimates. Regulatory submissions must demonstrate true held-out validation using variants submitted after model training or from independent databases, not just random splits of the same ClinVar version.\n\n\n\n\n\nClinical validation connects model outputs to clinical outcomes. For a variant classifier, clinical validation might assess concordance with expert panel classifications, correlation with functional assay results, or agreement with segregation patterns in affected families. The choice of reference standard is itself contentious: ClinVar classifications, which many models use as training labels, reflect historical expert consensus that may lag behind accumulating evidence, and circular validation using the same database for training and evaluation produces misleadingly optimistic results (see Chapter 12). The deployment realities discussed in Section 27.6.4 and Appendix B illustrate how these validation requirements interact with institutional workflows, reimbursement constraints, and clinician trust; regulatory clearance represents only one barrier among many.\nSome regulators also require evidence of clinical utility, demonstrating that model use improves patient outcomes compared to standard practice. This higher bar is difficult to meet for genomic AI tools that operate as components within larger clinical workflows. A variant effect predictor may improve prioritization efficiency without changing ultimate diagnoses, or may enable earlier diagnosis that only translates to better outcomes when appropriate treatments exist. Designing trials that isolate the model’s contribution from confounding workflow factors requires careful attention to study design and endpoint selection.\n\n\n\n\n\n\nWorked Example: Regulatory Submission for AlphaMissense\n\n\n\nWhat would a regulatory submission look like for a variant effect predictor like AlphaMissense seeking FDA clearance as a Class II medical device?\n510(k) Submission Components:\n\nDevice Description: Software tool that accepts protein-coding variants as input and outputs pathogenicity predictions (0-1 probability) using a deep learning model trained on protein language model embeddings and structural features.\nPredicate Device: Reference prior-cleared variant interpretation tools (e.g., approved clinical decision support software that incorporates CADD, REVEL, or similar scores).\nAnalytical Validation:\n\nReproducibility: Same variant queried 100 times produces identical output (deterministic inference)\nPlatform independence: Performance verified across cloud (AWS, GCP) and on-premise deployments\nInput robustness: Graceful handling of edge cases (novel amino acids, incomplete transcripts)\n\nClinical Validation (the critical section):\n\nPrimary endpoint: Concordance with ClinVar expert-reviewed pathogenic/benign classifications (excluding VUS)\nPerformance metrics: auROC = 0.91 (95% CI: 0.89-0.93) on 25,847 held-out variants submitted to ClinVar after model training freeze\nSubgroup analysis: Performance stratified by gene family, variant type (missense only), and ancestry of submitting laboratories\nComparison to predicate: AlphaMissense auROC vs. CADD (0.85), REVEL (0.88) on identical test set\nTemporal validation: Model predictions for variants classified after training (prospective cohort) show no degradation vs. training-era variants\n\nLimitations and Labeling:\n\n“Not validated for variants in genes without human homologs in training data”\n“Performance may differ for populations underrepresented in ClinVar”\n“Intended as decision support; does not replace expert interpretation”\n\nPost-Market Surveillance Plan: Quarterly performance monitoring against accumulating ClinVar classifications; drift detection triggers.\n\nThis example illustrates the gap between research publication (which reports benchmark performance) and regulatory submission (which requires prospective validation, subgroup analysis, explicit comparison to existing tools, and ongoing monitoring commitments).",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Regulatory and Governance</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch26-regulatory.html#sec-ch26-governance",
    "href": "part_5/p5-ch26-regulatory.html#sec-ch26-governance",
    "title": "26  Regulatory and Governance",
    "section": "26.2 Data Governance and Consent",
    "text": "26.2 Data Governance and Consent\nFoundation models require training data at scales that strain every assumption underlying informed consent. A protein language model draws on sequences from millions of organisms. A human genomic model aggregates variants from biobanks across continents, each governed by different consent frameworks, legal regimes, and cultural expectations about data use. The participants who contributed samples a decade ago could not have anticipated that their sequences might train generative AI systems capable of designing novel proteins or predicting sensitive traits. Governing this data requires frameworks that balance scientific utility against individual rights, present uses against unknown future applications, and open science norms against community concerns about exploitation.\n\n26.2.1 Consent Problem at Scale\n\n\n\n\n\n\nDifficulty Warning\n\n\n\nThis section addresses legal and ethical frameworks that vary significantly across jurisdictions and evolve rapidly. The concepts are not technically complex but require careful attention to nuance. What constitutes valid consent in one jurisdiction may be insufficient in another, and best practices continue to evolve.\n\n\nFoundation model training requires data at scales that challenge traditional consent paradigms. A protein language model trained on UniRef encompasses sequences from millions of organisms, including many species for which consent concepts do not apply and human sequences contributed under varied research protocols. A model trained on human genomic data from multiple biobanks aggregates information collected under different consent frameworks, some permitting broad secondary research use and others restricting use to specific studies.\nThe legal and ethical status of such aggregated training depends on how consent documents were written, how thoroughly participants understood the scope of future use, and how jurisdictions interpret secondary use provisions. GDPR provisions treat genetic data as a special category requiring explicit consent, but may permit research use under legitimate interest or public interest provisions with appropriate safeguards (“Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA Relevance)” 2016). United States regulations under the Common Rule permit secondary research on properly deidentified data, but genomic data resist complete deidentification given the uniqueness of individual genomes (“Federal Policy for the Protection of Human Subjects (’Common Rule” 2009).\nEven when consent technically permits model training, broader ethical questions remain. Participants who consented to genomic research in 2005 could not have anticipated that their data might train AI systems capable of generating novel sequences or predicting sensitive traits. The temporal gap between data collection and model development strains the fiction of informed consent. Dynamic consent systems that allow ongoing engagement and preference updates address some concerns but are difficult to retrofit onto legacy collections and impose burdens on participants and institutions alike (Kaye et al. 2015).\n\n\n\nTable 26.2: Comparison of consent models for genomic data use\n\n\n\n\n\n\n\n\n\n\n\n\nConsent Model\nDescription\nStrengths\nLimitations\nExamples\n\n\n\n\nBroad consent\nAuthorizes future unspecified research\nEnables innovation; low participant burden\nWeak autonomy protection; trust-dependent\nUK Biobank, All of Us\n\n\nSpecific consent\nAuthorizes named studies only\nStrong autonomy protection\nLimits secondary use; recontact burden\nTraditional clinical trials\n\n\nTiered consent\nParticipants choose categories\nBalances autonomy and utility\nComplex to implement; decision fatigue\nSome disease registries\n\n\nDynamic consent\nOngoing digital engagement\nResponsive; respects evolving preferences\nInfrastructure intensive; selection bias\nPersonal Genome Project\n\n\nCommunity consent\nTribal/community approval required\nRespects collective interests\nSlower; may conflict with individual wishes\nIndigenous biobanks\n\n\n\n\n\n\n\n\n\n\n\n\nConsent model spectrum for genomic data\n\n\n\n\nFigure 26.2: Consent model spectrum for genomic foundation model training data. Left: Broad consent—simple to implement, enables innovation, but provides weak autonomy protection; exemplified by UK Biobank and All of Us. Center-left: Tiered consent—participants choose categories of permitted use; balances autonomy and utility but introduces decision fatigue. Center-right: Dynamic consent—ongoing digital engagement allows preference updates over time; responsive but infrastructure-intensive and may introduce selection bias. Right: Community consent—requires tribal or collective approval; respects group interests but may conflict with individual preferences and slows research. Arrow indicates tradeoff between individual control and research utility.\n\n\n\n\n\n26.2.2 Biobank Governance Models\nLarge biobanks have developed varied governance approaches that shape how their data can be used for foundation model development. UK Biobank, which combines genomic data with extensive phenotypic information on approximately 500,000 participants, permits registered researchers to use data for health-related research under terms that explicitly anticipate computational and AI applications (Sudlow et al. 2015; Bycroft et al. 2018). Access requires application review, data security commitments, and agreement to publish results. The model has enabled substantial foundation model research while maintaining participant trust through transparent policies and active communication.\nOther biobanks operate under more restrictive frameworks. Some disease-specific registries limit use to research on particular conditions. Some indigenous and community biobanks require tribal or community approval for research access, reflecting concerns about historical exploitation and the importance of data sovereignty. The tension between open science norms that favor broad data sharing and community governance norms that prioritize local control creates friction for foundation model developers seeking diverse training data.\nFederated learning and other privacy-preserving techniques offer partial solutions by enabling model training without centralizing raw data (Rieke et al. 2020). Under federated approaches, each data custodian trains local models that share only gradients or model updates with a central coordinator.\nWhy does federated learning protect privacy, and why is that protection incomplete? The core insight is that sharing model updates rather than raw data limits exposure: an adversary who intercepts gradient updates cannot directly read patient sequences. However, gradients encode information about the data that produced them—a sufficiently sophisticated attack can reconstruct input data from gradients, particularly when batch sizes are small or models are overparameterized. The approach protects against centralization risks but introduces technical complexity, may reduce model quality compared to centralized training, and does not eliminate all privacy risks. Zhu et al. demonstrated that gradient updates can sometimes reveal individual-level information through reconstruction attacks (Zhu, Liu, and Han 2019). Practical federated training for genomic foundation models remains an active research area with limited deployment experience.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider a scenario: You are developing a genomic foundation model and want to include data from an indigenous biobank that requires community consent. The community’s governance council is concerned about potential misuse but also wants their members to benefit from genomic medicine advances. What governance structures or technical safeguards might address both concerns? There is no single right answer, but think through the tradeoffs before reading on.\n\n\n\n\n26.2.3 Secondary Use and Data Futures\nThe genomic data collected today may be used for applications not yet imagined. A variant database assembled for pharmacogenomic research might later inform ancestry inference tools with implications for immigration enforcement. Chromatin accessibility data generated for cancer biology might reveal aging signatures relevant to insurance underwriting. Foundation models trained on diverse genomic data acquire emergent capabilities that their creators did not anticipate and may not recognize.\nGovernance structures must therefore address not just present uses but future possibilities. Some institutions adopt broad consent models that authorize essentially unlimited research use, relying on institutional review and public benefit assessments rather than individual authorization for each application. Others implement tiered consent allowing participants to authorize some uses while restricting others. Still others propose data trusts or cooperatives that hold data on participants’ behalf and negotiate access terms collectively.\nNo consensus has emerged on optimal governance structures for genomic foundation model development. The field operates within a patchwork of institutional policies, national regulations, and community norms that permit some training configurations while prohibiting others. Researchers building foundation models must navigate this landscape carefully, documenting data provenance, respecting access restrictions, and anticipating how governance norms may evolve as AI capabilities advance.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Regulatory and Governance</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch26-regulatory.html#sec-ch26-privacy",
    "href": "part_5/p5-ch26-regulatory.html#sec-ch26-privacy",
    "title": "26  Regulatory and Governance",
    "section": "26.3 Privacy and Genomic Data",
    "text": "26.3 Privacy and Genomic Data\nGenomes are simultaneously the most personal data and the most shareable. A genome uniquely identifies its owner, reveals information about disease risk and ancestry, and exposes details about biological relatives who never consented to any disclosure. Standard anonymization techniques fail because the genome itself is an identifier. Foundation models compound these challenges by potentially memorizing and recombining information in ways that defeat conventional privacy protections.\nThe technical solutions each have intuitive analogues in everyday privacy. Differential privacy is like adding noise to survey responses: if you want to know the average income in a neighborhood without revealing any individual’s income, you can have each person randomly add or subtract a small amount before reporting. The average is still approximately correct, but no individual response can be recovered. Federated learning is like a teacher who grades homework without ever collecting the papers: students compute their own scores and report only the results. Secure multi-party computation is like sealed-envelope voting: each party contributes information in a way that reveals the aggregate outcome (who won?) without exposing any individual ballot. Each approach involves tradeoffs between utility and protection that genomic applications make particularly acute.\n\n\n\n\n\n\nKey Insight: Genomes Are Their Own Identifiers\n\n\n\nUnlike most personal data, genomic sequences cannot be truly anonymized because the sequence itself is a unique identifier. Removing names and demographics from a genomic dataset provides no protection if the sequence can be matched against any other database containing that individual’s DNA. This fundamental property distinguishes genomic privacy from conventional data privacy and explains why standard anonymization approaches fail.\n\n\n\n\n\n\n\n\n\n\nPrivacy-utility tradeoff: stronger protection reduces utility\n\n\n\n\n\n\n\nConsent models from broad to dynamic\n\n\n\n\n\n\n\n\n\nFederated learning: data stays local, model travels\n\n\n\n\n\n\n\nCross-border regulatory complexity\n\n\n\n\n\n\nFigure 26.3: Data governance challenges for genomic foundation models. (A) Privacy-utility tradeoff: stronger privacy protection (e.g., differential privacy with small ε) reduces model utility—there is no free lunch. (B) Consent complexity: models range from broad (simple, enables innovation) to dynamic (responsive, resource-intensive). (C) Federated learning: data stays local while model updates travel, but gradient attacks remain a risk. (D) Cross-border complexity: different jurisdictions (HIPAA, GDPR, MDR) impose different requirements, complicating global deployment.\n\n\n\n\n26.3.1 Re-identification Challenge\nGenomic data pose fundamental privacy challenges because genomes are simultaneously unique identifiers and richly informative biological records. A person’s genome can be matched against public genealogy databases, research repositories, or forensic databases to establish identity with high confidence. Once identified, the genomic record reveals information about disease predisposition, ancestry, family relationships, and other sensitive attributes that the person may not wish to disclose.\nConventional anonymization techniques that remove names and obvious identifiers provide limited protection. Re-identification of individuals from genomic data combined with surname inference through Y-chromosome analysis has been demonstrated (Gymrek et al. 2013) Subsequent work has shown re-identification from aggregate genomic statistics under certain conditions and through membership inference attacks on genomic databases (Homer et al. 2008; Erlich and Narayanan 2014). Foundation models compound these concerns by potentially extracting and recombining information in ways that defeat simple deidentification. A model trained on sequences from many individuals might, under adversarial prompting, generate outputs that reveal information about specific training examples.\nTechnical safeguards include differential privacy (which adds calibrated noise to training procedures to bound individual-level information leakage), secure multi-party computation (which enables joint computation over distributed data without revealing inputs), and synthetic data generation (which produces training data that preserves statistical properties without corresponding to real individuals).\nWhy do these privacy techniques work, and why do they all impose costs? Differential privacy works by adding noise calibrated so that any individual’s presence or absence in the training data cannot substantially change model outputs—an adversary cannot determine whether a specific person’s data was used. The cost is that noise degrades model accuracy, and genomic applications often require detecting rare variants where noise is particularly harmful. Secure computation works by distributing computation across parties such that no single party can reconstruct inputs—but cryptographic operations are computationally expensive. Synthetic data works by learning and sampling from a statistical model rather than memorizing individuals—but the statistical model may fail to capture the rare events that matter most clinically.\nEach approach involves tradeoffs between privacy protection and model utility. Differential privacy with strong guarantees may degrade model performance substantially. Secure computation adds computational overhead and complexity. Synthetic data may fail to capture rare variants or unusual correlations essential for clinical applications.\n\n\n\nTable 26.3: Privacy-preserving techniques for genomic AI\n\n\n\n\n\n\n\n\n\n\n\n\nTechnique\nHow It Works\nPrivacy Guarantee\nUtility Cost\nMaturity\n\n\n\n\nDifferential privacy\nAdds calibrated noise during training\nMathematical bound on individual information leakage\nModerate to high; depends on epsilon\nProduction-ready\n\n\nFederated learning\nData stays local; only gradients shared\nNo raw data centralization\nLow to moderate; communication overhead\nResearch to early production\n\n\nSecure multi-party computation\nEncrypted computation across parties\nStrong cryptographic guarantees\nHigh computational overhead\nResearch stage\n\n\nSynthetic data generation\nTrain on fake data with real statistics\nNo real individuals in training\nVariable; may miss rare variants\nEarly production\n\n\nTrusted execution environments\nHardware-isolated computation\nHardware-enforced isolation\nLow\nProduction-ready\n\n\n\n\n\n\n\n\n26.3.2 Family and Relational Privacy\nGenomic privacy extends beyond individuals to families and communities. A person’s genome reveals information about biological relatives who may not have consented to any data collection. Identifying a carrier of a hereditary cancer mutation implies elevated risk for parents, siblings, and children. Revealing ancestry information for one family member constrains inferences about relatives. These relational dimensions mean that individual consent cannot fully protect the interests of those affected by genomic disclosure.\nFoundation models trained on family data, or capable of inferring family relationships from population-level patterns, create new relational privacy risks. A model that accurately predicts recessive disease carrier status from sequence alone could identify at-risk couples without explicit testing. A model that infers extended pedigree structure from population genetics signals could reveal family secrets or create legal complications. Governance frameworks must consider not just the rights of data subjects but the interests of biological relatives who cannot meaningfully consent.\nSome jurisdictions have begun addressing relational genomic privacy through legislation. The Genetic Information Nondiscrimination Act (GINA) in the United States prohibits health insurers and employers from using genetic information discriminatorily, providing partial protection for individuals whose relatives have been tested (“Genetic Information Nondiscrimination Act of 2008” n.d.). GDPR provisions on special category data extend some protections to inferred genetic information (“Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA Relevance)” 2016). But legal frameworks lag behind technical capabilities, and enforcement mechanisms remain limited.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Regulatory and Governance</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch26-regulatory.html#sec-ch26-ip",
    "href": "part_5/p5-ch26-regulatory.html#sec-ch26-ip",
    "title": "26  Regulatory and Governance",
    "section": "26.4 Intellectual Property and Ownership",
    "text": "26.4 Intellectual Property and Ownership\nWho owns a genome sequence? Who owns a prediction derived from it? Who owns the model weights that encode patterns learned from millions of sequences? These questions lack clear answers, and the uncertainty shapes every decision about data sharing, model release, and commercial deployment. Legal frameworks designed for physical inventions and traditional software fit poorly with foundation models that blur boundaries between data, algorithm, and output. The genomics community’s historical commitment to open science confronts new tensions when model weights represent millions of dollars in compute investment and potentially enable misuse.\n\n26.4.1 Genomic Data Ownership\nLegal frameworks for sequence data ownership vary across jurisdictions and remain contested. In the United States, the Supreme Court’s 2013 Myriad decision held that naturally occurring DNA sequences cannot be patented, eliminating one barrier to data sharing but leaving property rights in datasets unclear (“Assoc. For Molecular Pathology v. Myriad Genetics, Inc., 569 U.S. 576 (2013)” n.d.). Databases may receive limited copyright protection for their selection and arrangement, but individual sequences typically cannot be copyrighted as facts or natural phenomena. Contractual restrictions, such as data use agreements attached to biobank access, provide the primary mechanism for controlling sequence data use.\nThe situation differs for synthetic or engineered sequences, which may qualify for patent protection if they meet novelty, utility, and non-obviousness requirements. Foundation models that generate novel sequences thus operate in complex IP territory: sequences generated by the model may be patentable if sufficiently innovative, but determining inventorship (human researcher versus AI system) raises unresolved legal questions (LORD JUSTICE ARNOLD&lt;br&gt;LADY JUSTICE ELISABETH LAING&lt;br&gt;and&lt;br&gt;LORD JUSTICE BIRSS 2021). Courts and patent offices are only beginning to address AI-generated inventions, with varying approaches across jurisdictions.\nFor foundation model developers, the key practical questions concern what restrictions apply to training data and what rights attach to model outputs. Training on publicly available sequences may be permissible under database terms of use, research exemptions, or fair use principles depending on jurisdiction and use context. Commercial deployment of models trained on restricted-access data may require additional authorization. Outputs generated by models may be freely usable by the model operator, or may carry through restrictions from training data, depending on legal interpretation and contractual provisions.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nThe 2013 Myriad decision addressed naturally occurring DNA sequences. But what about a novel protein sequence generated by a foundation model? Consider the following:\n\nCan such a sequence be patented?\nWho would be the inventor—the model developer, the user who prompted the generation, or the AI itself?\nHow might your answers change if the generated sequence is 95% identical to a natural protein versus entirely novel?\n\nThese questions remain unsettled, but thinking through them helps clarify the IP landscape.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nA truly novel, non-naturally-occurring protein sequence can potentially be patented if it meets utility, novelty, and non-obviousness requirements. (2) Current law does not recognize AI as an inventor (see Thaler v. USPTO); inventorship would likely be assigned to the human who directed the generation (user or developer), though this remains contested. (3) A sequence 95% identical to a natural protein likely fails the novelty requirement and would not be patentable, while a truly novel sequence could be—but determining what constitutes sufficient novelty in sequence space remains legally unclear.\n\n\n\n\n\n\n\n\n26.4.2 Model Weights as Assets\nFoundation model weights represent substantial investments of compute, data, and expertise, creating obvious commercial value. Companies training large genomic models face decisions about whether to release weights openly, provide API access without weight release, or restrict access entirely. Each approach carries different implications for scientific progress, commercial competition, and safety management.\nOpen release of weights enables independent research, reproduction, and adaptation but forfeits commercial control and complicates responsibility for misuse. API access maintains control while enabling broad use but creates dependencies and may restrict scientific scrutiny. Restricted access protects competitive advantage and may enhance safety oversight but limits beneficial applications and concentrates power.\nThe genomics community has historically favored open data sharing, with major databases and biobanks making data freely available under permissive terms. Whether this norm extends to foundation model weights is contested. Arguments for openness emphasize scientific reproducibility, broad access benefits, and the difficulty of maintaining meaningful restrictions given technical capabilities for weight reconstruction or distillation. Arguments for restriction emphasize dual-use risks from highly capable generative models, commercial incentives necessary to sustain development investment, and the potential for open models to be fine-tuned for harmful purposes.\n\n\n26.4.3 Prediction Ownership and Liability\nWhen a foundation model generates a clinically relevant prediction (this variant is likely pathogenic, this regulatory sequence will increase expression), questions arise about who owns that prediction and who bears responsibility if it proves wrong. The model developer, the clinical laboratory using the model, the health system employing the laboratory, and the clinician acting on results all have potential roles and potential liability.\nCurrent legal frameworks generally hold clinicians responsible for clinical decisions, with laboratories liable for test quality and medical device manufacturers liable for product defects. How these responsibilities apply when decisions incorporate foundation model outputs remains uncertain. If a model developer provides a variant classifier as SaMD, the developer likely bears some responsibility for the classifier’s performance. If a laboratory integrates foundation model embeddings into a proprietary pipeline, the laboratory may assume primary responsibility for overall system performance. If a clinician overrides a model recommendation based on clinical judgment, liability may shift toward the clinician’s decision-making.\nThese liability questions have practical implications for foundation model deployment. Developers may structure their offerings to minimize liability exposure, for instance by providing research-use-only tools that shift responsibility to users, or by limiting outputs to information that falls short of clinical recommendations. Such structuring may impede beneficial clinical applications if it creates uncertainty about appropriate use or fragments responsibility in ways that leave harms uncompensated.\n\n\n\n\n\n\nPractical Guidance: Navigating Liability in Model Deployment\n\n\n\nFor researchers and companies deploying genomic foundation models:\n\nDocument intended use clearly. Specify whether outputs are for research only or clinical decision support, and what validation has been performed.\nUnderstand the liability chain. Map out who bears responsibility at each stage: developer, deployer, laboratory, clinician, institution.\nConsider “research use only” designations carefully. They limit liability but also limit beneficial use and may not hold up if models are predictably used clinically.\nEngage legal counsel early. Liability frameworks for AI are evolving rapidly; what holds today may change.\nMaintain audit trails. Documentation of model versions, validation results, and deployment decisions supports liability defense and regulatory compliance.\n\n\n\n\n\n\n\n\n\nLiability chain for AI-informed clinical decisions\n\n\n\n\nFigure 26.4: Liability distribution across the genomic AI value chain. Flow proceeds from left (model development) to right (clinical decision). At each stage, different parties bear responsibility: Model developers (SaMD manufacturers) are liable for device performance and accuracy claims. Clinical laboratories integrating FM embeddings assume responsibility for overall system validation. Health systems deploying tools manage workflow integration and clinician training. Clinicians making final decisions bear responsibility for clinical judgment. Question marks indicate uncertain boundaries where liability may be contested. Key insight: “research use only” designations shift liability to users but may not hold up legally if clinical use is predictable.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Regulatory and Governance</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch26-regulatory.html#sec-ch26-responsible",
    "href": "part_5/p5-ch26-regulatory.html#sec-ch26-responsible",
    "title": "26  Regulatory and Governance",
    "section": "26.5 Responsible Development Practices",
    "text": "26.5 Responsible Development Practices\nTechnical capability without responsible deployment causes harm. A foundation model that achieves excellent benchmark performance but fails silently on underrepresented populations widens health disparities. A tool that provides confident predictions without communicating uncertainty misleads clinicians. A system deployed without documentation leaves users unable to assess whether its outputs apply to their context. Responsible development encompasses the entire lifecycle from training data selection through deployment and monitoring, requiring attention to transparency, fairness, and human oversight at each stage.\n\n26.5.1 Transparency and Documentation\nResponsible foundation model development requires transparency about training data, model capabilities, limitations, and intended use. Model cards and datasheets provide structured approaches to capturing this information (Mitchell et al. 2019; Gebru et al. 2021). For genomic foundation models, relevant documentation includes:\nTraining data composition encompasses which species are represented, what genomic regions are covered, which populations contribute human data, what functional annotations are included, and how data were filtered or preprocessed. Data provenance documentation traces sources, access conditions, and any restrictions on use or redistribution. Evaluation results cover performance across relevant benchmarks, disaggregated by ancestry, variant type, gene family, and other relevant strata. Limitation disclosure identifies known failure modes, out-of-distribution detection capabilities, and contexts where model outputs should not be trusted.\nThe challenge is ensuring that documentation reaches users who need it and influences their decisions. A detailed model card published alongside model weights may be ignored by users seeking quick results. Clinical deployments may strip away documentation as models are integrated into larger systems. Effective transparency requires not just producing documentation but designing workflows that surface relevant information at decision points and verifying that users understand limitations.\n\n\n26.5.2 Fairness and Performance Equity\nAncestry bias manifests technically at every stage of the genomic AI pipeline. Polygenic scores derived from European-ancestry GWAS show 40 to 75 percent reductions in prediction accuracy for African-ancestry individuals (Section 3.7). Variant effect predictors trained on ClinVar inherit that database’s overrepresentation of European-ancestry variants, performing better for populations already well-served by genomic medicine (?sec-ch14-protein-vep; Section 2.8.1). Models can exploit ancestry as a confounding shortcut, achieving high benchmark performance while systematically underperforming for underrepresented groups (?sec-ch22-ancestry-confounding). Clinical risk models calibrated on single-institution data may provide inferior risk stratification to patients from populations or care settings not represented in development (Section 27.8). These are not independent problems but manifestations of a single structural issue: genomic datasets encode historical inequities in who gets sequenced, which populations are recruited into biobanks, and whose variants receive clinical characterization.\n\n\n\n\n\n\nKey Insight: Bias Is Structural, Not Just Technical\n\n\n\nAncestry bias in genomic AI is not merely a technical problem solvable by better algorithms. It reflects decades of research prioritization that sequenced European populations first, recruited from academic medical centers, and characterized variants common in well-studied groups. Technical mitigations (stratified evaluation, calibration adjustment, uncertainty flagging) can reduce harm but cannot substitute for structural change: diversifying who participates in research, who builds models, and whose health needs drive development priorities.\n\n\nTechnical solutions exist but require deliberate implementation. Ancestry-stratified evaluation mandates reporting performance separately for major population groups, not just aggregate metrics that obscure disparities. Calibration assessment by subgroup reveals when models systematically over- or under-estimate risk for specific populations. Uncertainty quantification can flag predictions for patients from underrepresented ancestries as less reliable, enabling appropriate clinical caution (see Chapter 23). Reweighting training data or applying group-wise calibration adjustments can partially mitigate disparities, though these post-hoc corrections cannot fully compensate for fundamental data gaps.\nYet technical fixes alone are insufficient. Addressing fairness in genomic AI ultimately requires expanding who participates in genomic research, which populations are prioritized for biobank recruitment, and how resources flow to sequencing initiatives in underrepresented communities. Clinical use of polygenic risk scores derived from European-ancestry GWAS may exacerbate rather than reduce health disparities (Martin et al. 2019). A model trained on biased data and corrected post-hoc will always underperform compared to a model trained on representative data. The field’s trajectory depends on whether current disparities are treated as inconvenient technical limitations or as structural problems demanding structural solutions.\nGenomic foundation models inherit biases from their training data. If training corpora over-represent European ancestry populations, models may perform worse on variants common in other populations, on regulatory elements active in non-European tissues, or on genes under different selective pressures across populations. If functional annotations derive primarily from well-funded research programs focused on common diseases, models may underperform on rare diseases or conditions affecting underserved populations.\nFairness assessment requires disaggregated evaluation across relevant population strata, not just aggregate performance metrics. A variant effect predictor achieving 0.92 auROC overall might achieve 0.95 in European populations and 0.82 in African populations, a disparity masked by aggregate reporting. A regulatory model might perform well on cell types common in training data (lymphocytes, hepatocytes) while failing on less-studied cell types (specialized neurons, rare immune subsets) that matter for particular diseases.\nMitigation approaches include diversifying training data, applying reweighting or resampling strategies during training, and developing adaptation techniques that improve performance on underrepresented groups. But data diversification has limits when underlying resources remain skewed, and post-hoc corrections may trade off overall performance for equity gains. The deeper solution involves changing incentive structures to prioritize diverse data collection and equitable performance from the outset.\n\n\n26.5.3 Human Oversight and Decision Support\nEven highly capable foundation models should operate as decision support tools rather than autonomous decision-makers in clinical contexts. Human oversight serves multiple functions: catching model errors that fall outside training distribution, integrating clinical context that models cannot access, navigating value trade-offs where technical optimization is insufficient, and maintaining accountability structures that enable error correction and redress.\nEffective oversight requires that model outputs be interpretable enough for humans to exercise meaningful judgment. If a variant classifier provides only a pathogenic/benign label without supporting evidence, the overseeing clinician has no basis for assessing whether the model’s reasoning applies to the case at hand. If a regulatory effect predictor reports a large effect without indicating uncertainty, the user may not know when skepticism is warranted. Interpretability tools discussed in Section 24.1 and Section 24.7 support oversight by revealing internal model reasoning, but interpreting such explanations requires expertise and time that may not be available in clinical workflows.\nSystem design must also prevent automation bias, the tendency for human operators to defer to automated recommendations even when independent judgment would lead to different conclusions (Parasuraman and Manzey 2010). Training clinicians to use AI tools effectively, designing interfaces that prompt critical evaluation rather than passive acceptance, and monitoring for over-reliance patterns are all components of responsible oversight architecture.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider two interface designs for a variant classification tool:\nDesign A: Displays “Pathogenic” or “Benign” in large text with a confidence percentage.\nDesign B: Displays the classification with uncertainty bars, lists the top 3 evidence sources, shows known limitations for this variant type, and asks “Does this match your clinical assessment?” before proceeding.\nWhich design better supports human oversight? What tradeoffs does each involve? How might busy clinicians respond differently to each?",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Regulatory and Governance</span>"
    ]
  },
  {
    "objectID": "part_5/p5-ch26-regulatory.html#sec-ch26-biosecurity",
    "href": "part_5/p5-ch26-regulatory.html#sec-ch26-biosecurity",
    "title": "26  Regulatory and Governance",
    "section": "26.6 Dual Use and Biosecurity",
    "text": "26.6 Dual Use and Biosecurity\nThe same capabilities that enable therapeutic protein design could, in principle, enable pathogen enhancement. A model that generates functional regulatory sequences could optimize expression in beneficial or harmful contexts. These dual-use concerns are not unique to foundation models, but the combination of generative capability, broad accessibility, and rapid improvement creates genuinely novel considerations. The gap between computational generation and biological realization provides some natural barrier, yet that gap narrows as both computational and wetlab capabilities advance. Balancing open scientific exchange against biosecurity risks requires ongoing assessment as model capabilities evolve.\n\n\n\n\n\n\nDual-use risk assessment for generative genomic models\n\n\n\n\nFigure 26.5: Dual-use risk assessment for generative genomic models. The risk matrix considers model capability and access openness: high-capability models with open access pose the greatest concern. Risk factors include pathogen enhancement, resistance design, and emergent capabilities. Governance mechanisms span the deployment lifecycle: pre-release evaluation, staged release, usage monitoring, and audit trails. The gap between computational generation and biological realization provides some natural protection, but this gap narrows as both computational and wetlab capabilities advance.\n\n\n\n\n26.6.1 Generative Models and Pathogen Enhancement\n\n\n\n\n\n\nDifficulty Warning\n\n\n\nThis section addresses sensitive dual-use concerns that require careful reasoning about risks that are uncertain and contested. Experts disagree about the severity of biosecurity risks from generative genomic models. The goal is not to resolve these debates but to understand the considerations that inform responsible development decisions.\n\n\nFoundation models capable of generating functional biological sequences raise biosecurity concerns distinct from those posed by predictive models. A protein language model trained to generate functional enzymes might, in principle, be prompted to design proteins with enhanced pathogenic properties. A regulatory sequence model might generate promoters optimized for expression in human tissues of concern. A generative DNA model might propose sequences that evade detection by standard diagnostics.\nThe severity of these risks depends on technical factors that remain uncertain. Current generative models often produce sequences that are theoretically functional but fail in experimental validation; the gap between computational generation and biological realization provides a natural barrier (Soice et al. 2023). Specialized knowledge required to translate generated sequences into actual biological threats remains substantial, though it may decrease as wetlab automation advances. Many dangerous sequences are already documented in public databases, making novel generation less marginal than it might appear. The generative architectures examined in ?sec-ch28-protein-design and ?sec-ch28-regulatory-design, which demonstrate increasing capability for producing functional sequences, make these concerns more than hypothetical; the same capabilities that enable therapeutic protein design also lower barriers to misuse.\nNonetheless, responsible development requires attention to dual-use potential. Strategies include capability evaluation (probing models for ability to generate concerning sequences before release), staged deployment (limiting access to highly capable generative models while monitoring for misuse indicators), and output filtering (screening generated sequences against known hazard databases) (Shevlane 2022). The optimal balance between open scientific exchange and biosecurity restriction remains contested, with reasonable experts holding divergent views on where lines should be drawn.\n\n\n26.6.2 Access Controls and Responsible Release\nFoundation model developers must decide how to release models in ways that enable beneficial use while limiting potential for harm. Complete openness maximizes beneficial applications but foregoes control over misuse. Complete restriction limits misuse but also limits beneficial applications and may prove impossible to maintain as model capabilities become reproducible. Graduated access models attempt to balance these considerations by providing broader access to less capable models while restricting access to more capable systems.\nAccess controls can operate at multiple levels: restricting weight access while providing API availability, limiting API capabilities through output filtering, requiring applications and use agreements for access, or monitoring usage patterns for indicators of concerning applications. Each control imposes costs on legitimate users and may prove circumventable by determined malicious actors. The effectiveness of controls depends on the specific model, the capability of concern, and the technical sophistication of potential misusers.\nFor genomic foundation models specifically, the biosecurity risks are generally lower than for models capable of synthesizing pathogen sequences from scratch, but concerns about privacy violations, discriminatory applications, and scientific misconduct remain. A model capable of inferring sensitive traits from genomic data might be misused for unauthorized health prediction. A model capable of generating realistic synthetic genomic data might be used to fabricate research results. Responsible release strategies must consider these diverse risk profiles.\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat is Software as Medical Device (SaMD), and how do regulatory agencies classify risk tiers for genomic AI tools?\nWhy do foundation models create problems for the FDA’s locked vs. adaptive algorithm distinction?\nExplain the consent challenge at foundation model training scales. Why do traditional informed consent models struggle?\nWhat makes genomic data fundamentally different from other personal data in terms of privacy? Can genomes ever be truly anonymized?\nCompare the dual-use concerns for predictive versus generative genomic foundation models. Why might generative models raise biosecurity concerns that predictive models do not?\n\n\n\n\n\n\n\n\n\nChapter Summary\n\n\n\nThis chapter examined the regulatory, governance, and ethical landscape for genomic foundation models. Key takeaways:\nRegulatory Frameworks:\n\nGenomic AI tools are classified as Software as Medical Device (SaMD) with risk-based oversight\nThe FDA’s locked/adaptive algorithm distinction fits poorly with foundation model architectures\nRegulatory requirements differ substantially across jurisdictions, complicating global deployment\nLaboratory-developed tests occupy an uncertain regulatory space in the United States\n\nData Governance:\n\nFoundation model training scales challenge traditional consent paradigms\nConsent models range from broad to dynamic, each with tradeoffs between utility and autonomy\nFederated learning offers partial solutions but does not eliminate privacy risks\nSecondary use concerns require governance structures that anticipate unknown future applications\n\nPrivacy:\n\nGenomes are unique identifiers that cannot be truly anonymized\nTechnical safeguards (differential privacy, secure computation) involve utility-privacy tradeoffs\nRelational privacy extends concerns to biological relatives who did not consent\n\nIntellectual Property:\n\nNaturally occurring sequences cannot be patented (Myriad), but synthetic sequences may be\nModel weights represent substantial assets with contested norms around release\nLiability for AI-informed clinical decisions remains uncertain across the value chain\n\nResponsible Development:\n\nTransparency through model cards and datasheets is necessary but insufficient\nAncestry bias is structural, requiring more than technical fixes\nHuman oversight requires interpretable outputs and designs that prevent automation bias\n\nDual Use:\n\nGenerative models raise biosecurity concerns that predictive models do not\nThe gap between computational generation and biological realization provides some protection\nResponsible release strategies must balance openness against misuse potential\n\nThese challenges are not barriers to be overcome once but ongoing tensions to be managed as capabilities advance and governance frameworks evolve.\n\n\n\n\n\n\nAbràmoff, Michael D., Philip T. Lavin, Michele Birch, Nilay Shah, and James C. Folk. 2018. “Pivotal Trial of an Autonomous AI-Based Diagnostic System for Detection of Diabetic Retinopathy in Primary Care Offices.” Npj Digital Medicine 1 (1): 39. https://doi.org/10.1038/s41746-018-0040-6.\n\n\n“ACCE Model Process for Evaluating Genetic Tests  CDC.” 2004. https://archive.cdc.gov/www_cdc_gov/genomics/gtesting/acce/index.htm.\n\n\nAdministration, U. S. Food and Drug. 2021. “Artificial Intelligence/Machine Learning (‘AI/ML’)-Based Software as a Medical Device (‘SaMD’) Action Plan.” https://www.fda.gov/media/145022/download.\n\n\n“Assoc. For Molecular Pathology v. Myriad Genetics, Inc., 569 U.S. 576 (2013).” n.d. Justia Law. Accessed December 26, 2025. https://supreme.justia.com/cases/federal/us/569/576/.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp, Allan Motyer, et al. 2018. “The UK Biobank Resource with Deep Phenotyping and Genomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nErlich, Yaniv, and Arvind Narayanan. 2014. “Routes for Breaching and Protecting Genetic Privacy.” Nature Reviews Genetics 15 (6): 409–21. https://doi.org/10.1038/nrg3723.\n\n\n“Federal Policy for the Protection of Human Subjects (’Common Rule.” 2009. Page. https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. “Datasheets for Datasets.” Commun. ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\n“Genetic Information Nondiscrimination Act of 2008.” n.d. U.S. Equal Employment Opportunity Commission. Accessed December 26, 2025. https://www.eeoc.gov/statutes/genetic-information-nondiscrimination-act-2008.\n\n\nGymrek, Melissa, Amy L. McGuire, David Golan, Eran Halperin, and Yaniv Erlich. 2013. “Identifying Personal Genomes by Surname Inference.” Science 339 (6117): 321–24. https://doi.org/10.1126/science.1229566.\n\n\nHealth, Center for Devices and Radiological. 2025. “Artificial Intelligence-Enabled Medical Devices.” FDA, December. https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-enabled-medical-devices.\n\n\nHomer, Nils, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V. Pearson, Dietrich A. Stephan, Stanley F. Nelson, and David W. Craig. 2008. “Resolving Individuals Contributing Trace Amounts of DNA to Highly Complex Mixtures Using High-Density SNP Genotyping Microarrays.” PLOS Genetics 4 (8): e1000167. https://doi.org/10.1371/journal.pgen.1000167.\n\n\nKaye, Jane, Edgar A. Whitley, David Lund, Michael Morrison, Harriet Teare, and Karen Melham. 2015. “Dynamic Consent: A Patient Interface for Twenty-First Century Research Networks.” European Journal of Human Genetics 23 (2): 141–46. https://doi.org/10.1038/ejhg.2014.71.\n\n\nLORD JUSTICE ARNOLD&lt;br&gt;LADY JUSTICE ELISABETH LAING&lt;br&gt;and&lt;br&gt;LORD JUSTICE BIRSS. 2021. “Thaler v Comptroller General of Patents Trade Marks And Designs [2021] EWCA Civ 1374.” https://www.bailii.org/ew/cases/EWCA/Civ/2021/1374.html.\n\n\nMartin, Alicia R., Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada, Benjamin M. Neale, and Mark J. Daly. 2019. “Clinical Use of Current Polygenic Risk Scores May Exacerbate Health Disparities.” Nature Genetics 51 (4): 584–91. https://doi.org/10.1038/s41588-019-0379-x.\n\n\n“Medical Devices; Laboratory Developed Tests.” 2024. Federal Register. https://www.federalregister.gov/documents/2024/05/06/2024-08935/medical-devices-laboratory-developed-tests.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. “Model Cards for Model Reporting.” In Proceedings of the Conference on Fairness, Accountability, and Transparency, 220–29. FAT* ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3287560.3287596.\n\n\nParasuraman, Raja, and Dietrich H. Manzey. 2010. “Complacency and Bias in Human Use of Automation: An Attentional Integration.” Human Factors 52 (3): 381–410. https://doi.org/10.1177/0018720810376055.\n\n\n“Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA Relevance).” 2016. http://data.europa.eu/eli/reg/2016/679/oj.\n\n\n“Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on Medical Devices, Amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and Repealing Council Directives 90/385/EEC and 93/42/EEC (Text with EEA Relevance. ).” 2017. http://data.europa.eu/eli/reg/2017/745/oj.\n\n\n“Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 Laying down Harmonised Rules on Artificial Intelligence and Amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA Relevance).” 2024. http://data.europa.eu/eli/reg/2024/1689/oj.\n\n\nRieke, Nicola, Jonny Hancox, Wenqi Li, Fausto Milletarì, Holger R. Roth, Shadi Albarqouni, Spyridon Bakas, et al. 2020. “The Future of Digital Health with Federated Learning.” Npj Digital Medicine 3 (1): 119. https://doi.org/10.1038/s41746-020-00323-1.\n\n\nShevlane, Toby. 2022. “Structured Access: An Emerging Paradigm for Safe AI Deployment.” arXiv. https://doi.org/10.48550/arXiv.2201.05159.\n\n\n“Software as a Medical Device (SaMD): Clinical Evaluation  International Medical Device Regulators Forum.” 2017. https://www.imdrf.org/documents/software-medical-device-samd-clinical-evaluation.\n\n\n“Software as a Medical Device: Possible Framework for Risk Categorization and Corresponding Considerations  International Medical Device Regulators Forum.” 2014. https://www.imdrf.org/documents/software-medical-device-possible-framework-risk-categorization-and-corresponding-considerations.\n\n\nSoice, Emily H., Rafael Rocha, Kimberlee Cordova, Michael Specter, and Kevin M. Esvelt. 2023. “Can Large Language Models Democratize Access to Dual-Use Biotechnology?” arXiv. https://doi.org/10.48550/arXiv.2306.03809.\n\n\nSudlow, Cathie, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey, et al. 2015. “UK Biobank: An Open Access Resource for Identifying the Causes of a Wide Range of Complex Diseases of Middle and Old Age.” PLOS Medicine 12 (3): e1001779. https://doi.org/10.1371/journal.pmed.1001779.\n\n\nZhu, Ligeng, Zhijian Liu, and Song Han. 2019. “Deep Leakage from Gradients.” arXiv. https://doi.org/10.48550/arXiv.1906.08935.",
    "crumbs": [
      "Part V: Evaluation and Trust",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Regulatory and Governance</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html",
    "href": "part_6/p6-ch27-clinical-risk.html",
    "title": "27  Clinical Risk Prediction",
    "section": "",
    "text": "27.1 From Polygenic Scores to Foundation Model Features\nThe limitations of classical polygenic risk scores define the opportunity for foundation model approaches. As discussed in Section 3.5, polygenic scores aggregate the effects of common variants into weighted sums, with weights derived from genome-wide association study effect sizes. This framework has demonstrated that common genetic variation contributes substantially to risk for conditions including coronary artery disease, type 2 diabetes, and breast cancer. A patient in the top percentile of polygenic risk for coronary disease faces roughly threefold higher lifetime risk than one in the bottom percentile, a gradient comparable to traditional risk factors like smoking or hyperlipidemia [Citation Needed].\nSeveral limitations constrain the clinical impact of this approach. The linear additive model cannot capture epistatic interactions where one variant’s effect depends on others, nor can it represent the nonlinear relationships between genetic variation and disease that emerge from regulatory networks and cellular pathways. Polygenic scores derived from European-ancestry genome-wide association studies substantially underperform in other populations, with effect sizes often attenuating by half or more in African or East Asian ancestries due to differences in linkage disequilibrium structure and allele frequencies (?sec-ch22-ancestry-confounding; Section 3.7). Beyond these technical constraints, a single scalar provides no mechanistic insight: a high polygenic score for diabetes does not indicate whether risk stems from impaired insulin secretion, insulin resistance, or altered satiety signaling, information that might guide intervention selection.\nFoundation models address these limitations through richer representations. Instead of treating variants as independent weighted features, models like Delphi and G2PT learn genome-wide embeddings that encode sequence context, regulatory annotations, and cross-variant interactions (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025). These approaches can capture nonlinear structure in genetic risk, leverage functional priors that transfer across ancestries, and provide attention-based attributions that highlight which genomic regions contribute most to predictions. Fine-mapping models like MIFM estimate posterior probabilities for causal variants within association loci, allowing risk models to weight variants by evidence for causality rather than treating all correlated variants equally (Rakowski and Lippert 2025).\nThe practical architecture of a foundation model-enabled risk system typically involves three components: pretrained encoders that transform genomic data into embeddings, aggregation modules that summarize variant-level or region-level representations into patient-level features, and prediction heads that map these features (combined with clinical covariates) to risk estimates. This modular design separates the computationally expensive foundation model inference from the task-specific prediction layer, enabling updates to either component while maintaining clear interfaces for validation.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-pgs-to-fm",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-pgs-to-fm",
    "title": "27  Clinical Risk Prediction",
    "section": "",
    "text": "Stop and Think\n\n\n\nBefore reading further, consider the three main limitations of traditional polygenic scores mentioned in the introduction: lack of mechanistic insight, poor cross-ancestry portability, and disconnection from clinical workflows. For each limitation, what kind of technical capability would be needed to address it? How might richer representations from foundation models help with each?\n\n\n\n\n\n\n\n\n\n\nKey Insight: From Scalar Scores to Rich Representations\n\n\n\nThe core advancement of foundation model approaches is not simply “better prediction” but a fundamental shift in what the model produces. A polygenic score yields a single number; a foundation model yields a high-dimensional embedding that preserves information about which genomic regions contribute, how they interact, and why they matter biologically. This richer output enables downstream tasks (pathway attribution, cross-ancestry transfer, mechanistic interpretation) that scalar scores cannot support.\n\n\n\n\n\n\nTable 27.1: Comparison of polygenic risk approaches: traditional polygenic scores versus foundation model-enhanced methods.\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nRepresentation\nAncestry Transfer\nMechanistic Insight\nComputational Cost\n\n\n\n\nTraditional PRS\nScalar (single number)\nPoor (LD-dependent)\nNone\nLow\n\n\nPRS + annotations\nScalar + categorical\nModerate\nLimited\nLow\n\n\nFM embeddings\nHigh-dimensional vector\nBetter (sequence-based)\nVia attribution\nHigh (precompute)\n\n\nFM + fine-mapping\nWeighted embeddings\nBest (causal priors)\nStrong\nHigh",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-defining-risk",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-defining-risk",
    "title": "27  Clinical Risk Prediction",
    "section": "27.2 Defining Clinical Risk Prediction",
    "text": "27.2 Defining Clinical Risk Prediction\nA risk prediction model is only as useful as the decision it informs. Effective clinical risk prediction requires precise specification of four elements: the outcome being predicted, the time horizon over which prediction applies, the target population for whom the model is intended, and the clinical action the prediction will trigger.\nConsider a 55-year-old woman with moderately elevated cholesterol and a family history of early coronary disease. Her cardiologist must decide whether to initiate statin therapy, a decision traditionally guided by 10-year cardiovascular risk estimates from tools like the Pooled Cohort Equations. A genomic foundation model could augment this decision in several ways. It might refine her absolute risk estimate by incorporating polygenic information that the traditional calculator ignores. It might identify whether her genetic risk concentrates in pathways amenable to specific interventions (LDL metabolism favoring statins versus inflammatory pathways suggesting alternative approaches). It might flag pharmacogenomic variants affecting statin metabolism that influence dose selection or drug choice.\nEach of these applications represents a different prediction task with distinct requirements. The 10-year risk estimate for major adverse cardiovascular events is an individual-level incident risk problem where discrimination and calibration matter most. The pathway-level attribution is an interpretability challenge requiring mechanistic grounding. The pharmacogenomic prediction is a treatment selection problem where the relevant outcome is adverse drug reaction risk conditional on therapy initiation.\n\n\n\n\n\n\nDeep Dive: Clinical Validity vs. Clinical Utility\n\n\n\nFor technical readers: These distinct concepts determine whether a test should enter clinical practice:\nAnalytical validity: Does the test accurately measure what it claims to measure?\n\nExample: Does the genotyping array correctly call the variant genotypes?\nMetrics: Call rate, concordance with sequencing, reproducibility\n\nClinical validity: Is the measurement associated with the clinical outcome of interest?\n\nExample: Does the polygenic score correlate with disease risk?\nMetrics: Discrimination (auROC), calibration, hazard ratios\n\nClinical utility: Does using the test improve patient outcomes?\n\nExample: Does knowing the polygenic score lead to interventions that reduce disease incidence?\nMetrics: Net reclassification, decision curve analysis, clinical trial outcomes\n\nWhy the distinction matters:\n\n\n\n\n\n\n\n\nLevel\nCan Be High While Others Are Low?\nExample\n\n\n\n\nAnalytical validity\nYes\nPerfectly accurate test for a biomarker that doesn’t predict disease\n\n\nClinical validity\nYes\nHighly predictive test that doesn’t change management\n\n\nClinical utility\nRequires others\nTest must be valid and actually improve care\n\n\n\nA model can achieve excellent discrimination (clinical validity) yet provide no clinical utility if the resulting predictions don’t change clinical decisions, if the population tested differs from training, or if interventions triggered by the prediction are ineffective.\n\n\nClinical risk prediction tasks cluster into several archetypes. Incident risk concerns whether a currently disease-free individual will develop disease within a specified window, such as 10-year diabetes risk for prediabetic patients. Progression risk asks which patients with existing disease will develop complications, for instance nephropathy in diabetes or heart failure after myocardial infarction. Survival and prognosis involve time-from-diagnosis to events like death, recurrence, or transplant, often requiring survival models that handle censoring and competing risks. Treatment response and toxicity concerns whether a patient will benefit from one therapy versus another and their probability of experiencing serious adverse effects.\n\n\n\nTable 27.2: Clinical risk prediction task archetypes and their distinct requirements.\n\n\n\n\n\n\n\n\n\n\n\n\nTask Type\nExample Question\nTime Structure\nKey Metrics\nFM Role\n\n\n\n\nIncident risk\nWill this patient develop T2D in 10 years?\nFixed window\nDiscrimination, calibration\nRisk stratification features\n\n\nProgression\nWill this diabetic develop nephropathy?\nConditional on disease\nTime-to-event\nTrajectory modeling\n\n\nSurvival/prognosis\nHow long after cancer diagnosis?\nOpen-ended, censored\nC-index, survival curves\nTumor embeddings\n\n\nTreatment response\nWill this patient respond to drug X?\nConditional on treatment\nRelative benefit\nDrug-gene interactions\n\n\nToxicity\nAdverse event risk with drug X?\nConditional on treatment\nauPRC (rare outcomes)\nPharmacogenomic features\n\n\n\n\n\n\nFoundation models enter these problems as feature generators. They transform raw sequence data into structured representations that downstream prediction models combine with clinical covariates. The architectural choices for this combination, and the evidence required to trust the resulting predictions, constitute the core methodological challenges of clinical translation.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-feature-integration",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-feature-integration",
    "title": "27  Clinical Risk Prediction",
    "section": "27.3 Feature Integration Architectures",
    "text": "27.3 Feature Integration Architectures\nThe features available for clinical risk models draw on multiple foundation model families, each capturing different aspects of genetic and molecular risk.\nDNA-level foundation models provide variant effect predictions without requiring trait-specific training. Systems like Nucleotide Transformer, HyenaDNA, and GPN compute sequence-based deleteriousness scores that reflect how mutations disrupt regulatory grammar, splice sites, or protein-coding sequences (Dalla-Torre et al. 2023; Nguyen et al. 2023; Benegas, Batra, and Song 2023). These zero-shot predictions transfer across traits and ancestries because they derive from sequence properties rather than population-specific association statistics (Chapter 14). Fine-mapping models integrate these functional priors with association evidence to estimate which variants within a locus are likely causal, providing principled weights for aggregation (Section 3.4). Fine-mapping models like MIFM integrate such functional priors with association evidence to estimate which variants within a locus are likely causal, providing principled weights for aggregation (Rakowski and Lippert 2025).\n\n\n\n\n\n\n\n\nDNA foundation models contribute regulatory variant effects\n\n\n\n\n\n\n\nExpression foundation models provide cell state embeddings\n\n\n\n\n\n\n\nProtein foundation models add coding variant pathogenicity\n\n\n\n\n\n\nFigure 27.1: Feature integration for clinical risk prediction. (A) DNA foundation models contribute regulatory variant effects, enhancer predictions, and PRS enhancement. (B) Expression foundation models provide cell state embeddings, pathway activities, and disease signatures. (C) Protein foundation models add coding variant pathogenicity and structural effects. Integration combines features across modalities for unified risk prediction with calibrated uncertainty.\n\n\n\nProtein language models add coding variant interpretation. AlphaMissense and related systems predict pathogenicity for missense mutations based on evolutionary conservation patterns learned from millions of protein sequences, as discussed in Chapter 15. For conditions with strong coding variant contributions (Mendelian cardiomyopathies, cancer predisposition syndromes), these predictions provide crucial signal beyond what noncoding regulatory models capture.\nMulti-omics foundation models extend beyond germline sequence. Cell-type-resolved representations from GLUE, scGLUE, and CpGPT capture regulatory state across chromatin accessibility, methylation, and expression (Chapter 19) (Cao and Gao 2022; Camillo et al. 2024). Rare variant burden scores from DeepRVAT aggregate predicted effects across genes into pathway-level impairment measures (Clarke et al. 2024). For oncology applications, tumor embedding models like SetQuence and graph neural network-based subtypers encode complex somatic mutation landscapes into patient-level representations (Jurenaite et al. 2024; Li et al. 2022).\nElectronic health record features provide the clinical context without which genomic predictions lack meaning. Demographics, vital signs, laboratory values, medication lists, problem codes, and procedure histories characterize the patient’s current state and trajectory. Time-varying biomarker trajectories (estimated glomerular filtration rate trends, hemoglobin A1c patterns, tumor marker dynamics) capture disease evolution that static snapshots miss.\nThe architectural question is how to combine these heterogeneous inputs. Three fusion strategies offer different tradeoffs.\nEarly fusion concatenates all features into a single input vector and trains a unified model (neural network, gradient boosting, survival regression) on the combined representation. This approach allows the model to learn arbitrary interactions between genomic and clinical features but requires all inputs to be present for every patient, handles scale differences between modalities poorly, and can be dominated by whichever input provides the most features or strongest signal.\nIntermediate fusion trains separate encoders for each modality, producing genomic embeddings, clinical embeddings, and multi-omic embeddings that a fusion module then combines. The fusion module might use attention mechanisms to weight modality contributions dynamically, cross-modal transformers that allow features from one modality to attend to features from another, or simpler concatenation with learned combination weights. This approach offers modularity (foundation model encoders can be swapped as new versions become available) while still enabling learned cross-modal interactions.\nLate fusion trains independent models for each modality and combines their predictions through ensemble methods or meta-learning. A polygenic score model, an electronic health record model, and a multi-omic model each produce risk estimates that a final layer integrates. This approach handles missing modalities gracefully and allows modality-specific architectures but may underutilize cross-modal structure since interactions can only be captured at the final combination stage.\nWhy does late fusion handle missing data so effectively while intermediate fusion struggles? In late fusion, each modality-specific model is trained independently and produces valid predictions whether or not other modalities are available. The genomic model outputs a valid risk score using only genomic data; the EHR model outputs a valid risk score using only clinical data. The combination layer learns optimal weighting when all modalities are present but can fall back to available inputs without architectural modification—if EHR data is missing, the combination layer simply uses the genomic and multi-omic scores. In contrast, intermediate fusion architectures learn cross-modal interactions during training that assume all data streams are present. When a patient lacks methylation data, for example, the fusion layer’s learned weights encode expectations about relationships between methylation features and other modalities that cannot be satisfied, potentially producing undefined or degraded outputs.\n\n\n\nTable 27.3: Comparison of fusion architectures for combining genomic and clinical features.\n\n\n\n\n\n\n\n\n\n\n\n\nFusion Strategy\nCross-Modal Interactions\nMissing Data Handling\nModularity\nBest For\n\n\n\n\nEarly\nFull (learned jointly)\nPoor (all required)\nLow\nDense, complete data\n\n\nIntermediate\nModerate (fusion layer)\nModerate (graceful degradation)\nHigh\nEvolving FM ecosystems\n\n\nLate\nLimited (output only)\nExcellent (independent)\nModerate\nHeterogeneous availability\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Guidance: Choosing a Fusion Architecture\n\n\n\nFor clinical deployment, intermediate fusion often provides the best balance. It enables modular updates as foundation models improve, allows graceful degradation when modalities are missing, and captures cross-modal interactions that late fusion misses. The specific fusion mechanism (attention, concatenation, cross-modal transformer) matters less than ensuring the architecture supports the operational requirements of clinical deployment: batch computation, uncertainty quantification, and interpretable feature attribution.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-ehr-integration",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-ehr-integration",
    "title": "27  Clinical Risk Prediction",
    "section": "27.4 EHR Integration and Phenotype Embeddings",
    "text": "27.4 EHR Integration and Phenotype Embeddings\nPolygenic risk scores condense genetic information into scalar predictions, but clinical decision-making occurs in the context of rich electronic health records that capture diagnoses, procedures, medications, laboratory values, and clinical narratives. A PRS for coronary artery disease exists as an isolated number until integrated with a patient’s history of hypertension, diabetes, smoking, and lipid measurements. The question is not merely whether to combine genetic and clinical information, but how to do so in ways that improve prediction, maintain interpretability, and avoid introducing new sources of bias.\nTraditional approaches treat EHR data as additional covariates in regression models that already include the PRS. Age, sex, smoking status, and existing diagnoses enter as predictors alongside the genetic score, with effect sizes learned from training data. This additive framework has clear interpretation but limited capacity: it assumes that genetic risk and clinical risk contribute independently, missing interactions where genetic predisposition matters more or less depending on clinical context. A patient with elevated LDL cholesterol and high coronary disease PRS may face multiplicative risk that additive models underestimate.\n\n27.4.1 EEPRS Framework\nThe EHR-embedding-enhanced PRS (EEPRS) framework addresses these limitations by integrating phenotype embeddings derived from EHR data with GWAS summary statistics to construct improved polygenic scores (Xu et al. 2025). Rather than using expert-defined phenotype covariates, EEPRS learns vector representations of clinical phenotypes from their patterns of co-occurrence in patient records. These embeddings capture relationships among diseases, symptoms, and risk factors that expert definitions may miss.\nThe framework proceeds in stages. Embedding models (Word2Vec trained on ICD-10 code sequences, or GPT-based embeddings of code descriptions) transform each patient’s diagnostic history into a low-dimensional vector representation. GWAS conducted on these embedding dimensions identify genetic variants associated with each dimension of clinical phenotype space. The resulting summary statistics enable construction of embedding-based polygenic scores that capture genetic predisposition to the phenotypic patterns encoded in each dimension. Integration with traditional disease-specific PGS through weighted combination yields final risk predictions.\nValidation in UK Biobank demonstrated consistent improvement over single-trait polygenic scores across 41 clinical traits. Cardiovascular conditions showed the largest gains: ischemic stroke improved by 66%, heart failure by 32%, and peripheral artery disease by 25% [Citation Needed]. These improvements concentrate in traits where related phenotypes share genetic architecture, allowing the embedding-based scores to leverage cross-phenotype genetic correlation. For isolated traits without strong embedding-dimension associations, improvements were modest or absent.\n\n\n27.4.2 Understanding When Embeddings Help\n\n\n\n\n\n\nKnowledge Check\n\n\n\nThe EEPRS framework showed large improvements for cardiovascular conditions but minimal improvement for breast cancer. Before reading the explanation, can you hypothesize why? Consider what phenotype embeddings capture and how genetic architecture might differ between these conditions.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nCardiovascular conditions cluster together in clinical practice and share genetic architecture, allowing embeddings to capture cross-phenotype genetic correlations. Breast cancer has largely distinct genetic architecture from cardiovascular diseases, so embedding-based scores derived from cardiovascular-weighted dimensions provide no additional predictive signal for cancer risk.\n\n\n\n\n\nThe pattern of improvement across traits reveals when EHR embeddings add value to polygenic prediction. Conditions that cluster together in clinical space, co-occurring in patients and sharing risk factors, benefit most. The cardiovascular cluster (coronary artery disease, ischemic stroke, peripheral artery disease, heart failure, angina, type 2 diabetes) forms a coherent group in both clinical practice and genetic architecture. Embeddings trained on EHR data capture this clustering, and GWAS on embedding dimensions identify variants associated with the shared liability across the cluster. These variants provide additional prediction signal beyond what single-trait GWAS can detect.\nConversely, conditions with distinct genetic architectures that do not cluster with other phenotypes show minimal improvement. Breast cancer and coronary artery disease, despite both being common conditions well-represented in biobanks, did not benefit from embedding integration in external validation. Their genetic architectures are largely distinct; embedding-based scores derived from cardiovascular-weighted dimensions provide no additional signal for cancer prediction.\nThis selectivity has important implications for clinical deployment. EEPRS offers greatest value for conditions where conventional polygenic scores remain underpowered despite adequate GWAS sample sizes. Heart failure, peripheral artery disease, and asthma showed substantial improvements precisely because their polygenic scores have historically underperformed relative to heritability estimates. Embedding integration effectively borrows strength across genetically correlated phenotypes, amplifying signal that single-trait analyses struggle to detect.\n\n\n27.4.3 PRS-PheWAS for Clinical Interpretation\nClinical deployment requires interpretability: why does this score predict disease risk, and what biological mechanisms does it capture? PRS-based phenome-wide association studies provide one answer by systematically testing association between the polygenic score and hundreds of clinical phenotypes (Section 3.8). For embedding-enhanced scores, PRS-PheWAS reveals which clinical manifestations the genetic risk predicts.\nThe EEPRS framework’s cardiovascular improvements became interpretable through PRS-PheWAS analysis. Embedding-based scores derived from ICA-transformed dimensions showed strong associations (adjusted \\(p &lt; 10^{-20}\\)) with hypertension, atrial fibrillation, and cardiac dysrhythmias [Citation Needed]. These associations explain the improvement: the embeddings capture genetic variation that influences multiple cardiovascular endpoints, and aggregating across these endpoints provides stronger risk stratification than targeting any single outcome.\nPRS-PheWAS also reveals unexpected associations that warrant clinical attention. Different embedding methods capture different aspects of phenotypic structure, with GPT-based embeddings uniquely identifying associations with infectious diseases and mental disorders that Word2Vec embeddings missed. These method-specific patterns may reflect differences in what the embedding approaches learn from clinical data, or they may indicate opportunities for method combination that leverages complementary signals.\n\n\n27.4.4 Implementation Considerations\nTranslating EEPRS from research demonstration to clinical deployment requires addressing several practical challenges. The embedding models must be trained on EHR data representative of the deployment population; embeddings learned from UK Biobank may not transfer to health systems with different patient populations, coding practices, or documentation patterns. The integration weights that combine embedding-based and single-trait scores require calibration in the target population, not just the discovery cohort.\nComputational requirements are modest once embeddings are pretrained. Scoring new patients requires computing their embedding from available ICD codes (a lookup operation), then calculating weighted sums across precomputed variant weights. The workflow integrates with existing PGS calculation pipelines, adding embedding score computation and integration as additional steps. Summary statistics for embedding-based GWAS can be distributed like conventional GWAS results, enabling score construction without sharing individual-level data.\nThe deeper challenge is population representativeness. EHR-based embeddings inherit the documentation patterns, coding practices, and healthcare access disparities of the health systems where they were trained. An embedding that positions diabetes near cardiovascular disease reflects the co-occurrence pattern in patients who access both cardiology and endocrinology care; patients who lack access to specialty care may show different patterns. Multi-ancestry validation revealed that EEPRS improvements varied across populations, with gains concentrated in conditions where the underlying genetic correlation structure held across ancestries.\n\n\n27.4.5 Integration with Foundation Model Features\nThe EEPRS framework operates on GWAS summary statistics and phenotype embeddings, both derived from classical statistical approaches. Foundation models offer an alternative integration strategy where learned sequence representations replace or augment summary statistics. Rather than weighting variants by GWAS effect sizes, foundation model approaches can score variants by their predicted functional impact, regulatory consequence, or embedding similarity to known pathogenic variants (Chapter 17).\nAttention-based integration, graph neural networks for pathway aggregation (Chapter 21), and transformer encoders for sequence context can all incorporate EHR embeddings as additional input features. A patient’s clinical embedding provides context that may modify interpretation of their genetic variants: a variant of uncertain significance (VUS) in a cardiovascular gene carries different implications for a patient whose clinical embedding places them in the cardiovascular risk cluster versus one with an unremarkable clinical profile. This contextualization moves beyond additive combination toward models that learn interactions between genetic and clinical risk.\nThe combination of phenotype embeddings and foundation model features remains largely unexplored. EEPRS demonstrated that phenotype embeddings capture heritable variation beyond single-trait GWAS; foundation models demonstrate that sequence context improves variant effect prediction beyond simple annotations (?sec-ch14-fm-gains). Whether these approaches provide complementary signal, and whether their combination improves clinical prediction beyond either alone, represents an open research question with substantial clinical implications.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-temporal-modeling",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-temporal-modeling",
    "title": "27  Clinical Risk Prediction",
    "section": "27.5 Temporal Modeling Architectures",
    "text": "27.5 Temporal Modeling Architectures\nClinical risk prediction spans diverse temporal structures, and the choice of modeling framework must match the prediction task. A screening tool estimating whether a patient will develop diabetes within ten years faces different statistical challenges than a monitoring system tracking whether a patient’s kidney function trajectory signals imminent decline. Foundation model features can integrate into each framework, but the integration patterns differ.\n\n\n\n\n\n\nChallenging Material Ahead\n\n\n\nThe following section introduces survival analysis concepts (hazard functions, censoring, proportional hazards assumptions) and longitudinal modeling frameworks (joint models, time-varying coefficients). Readers unfamiliar with these statistical foundations may benefit from reviewing standard biostatistics references before proceeding. The key conceptual distinction is between models that predict whether an event occurs within a fixed window versus models that track how risk evolves over time.\n\n\nSurvival models address time-to-event outcomes where patients are followed until an event occurs or observation ends. The Cox proportional hazards model remains the workhorse of clinical risk prediction, estimating hazard ratios for features while making minimal assumptions about baseline hazard shape. Foundation model embeddings enter as covariates alongside clinical variables, with the proportional hazards assumption requiring that genomic risk effects remain constant over time. When this assumption fails (as when genetic effects on cancer recurrence differ between early and late periods), stratified or time-varying coefficient extensions accommodate the violation.\n\n\n\n\n\n\n\n\nStatic vs dynamic risk over patient lifetime\n\n\n\n\n\n\n\nLongitudinal trajectory modeling with sequential observations\n\n\n\n\n\n\n\nBayesian updating as evidence accumulates\n\n\n\n\n\n\nFigure 27.2: Temporal dynamics in clinical risk prediction. (A) Static versus dynamic risk: genetic risk sets baseline trajectory while environmental factors and interventions modify it over time. (B) Longitudinal trajectory modeling: sequential observations (labs, imaging, symptoms) update risk predictions with widening uncertainty into the future. (C) Prediction updating: Bayesian framework incorporates new evidence, adjusting risk as interventions and events occur.\n\n\n\nDeep survival models extend this framework through neural network architectures that learn nonlinear feature interactions. DeepSurv replaces the linear Cox predictor with a multilayer network while preserving the partial likelihood objective (Katzman et al. 2018). Deep Survival Machines model the survival distribution as a mixture of parametric components, enabling richer distributional assumptions than the semiparametric Cox approach (Nagpal, Li, and Dubrawski 2021). These architectures naturally accommodate the high-dimensional embeddings that foundation models produce, though the risk of overfitting increases and careful regularization becomes essential.\nLongitudinal models address a different challenge: patients observed repeatedly over time, with measurements that evolve and interact. A patient’s hemoglobin A1c trajectory over five years contains information that a single baseline measurement cannot capture. Whether values are stable, rising, or fluctuating conveys prognostic signal beyond their current level. Joint longitudinal-survival models connect these repeated measurements to event outcomes, modeling how biomarker trajectories associate with hazard while accounting for informative dropout when sicker patients are measured more frequently or die before later observations.\nFoundation model features integrate into longitudinal frameworks at multiple levels. Static genomic embeddings (computed once from germline sequence) serve as time-invariant covariates influencing both trajectory shape and event hazard. Time-varying molecular features (expression profiles, methylation states, circulating tumor DNA levels) can be encoded through foundation models at each measurement occasion, producing sequences of embeddings that recurrent or attention-based architectures process into trajectory representations. The computational cost of re-encoding molecular data at each timepoint is substantial, making efficient inference strategies essential for deployment.\nTransformer architectures designed for irregularly sampled time series offer a natural framework for clinical trajectories. Models like STraTS and similar clinical transformers handle the variable timing and missing measurements characteristic of real-world healthcare data (Tipirneni and Reddy 2022). Position encodings based on actual timestamps rather than sequence position accommodate irregular sampling. Attention mechanisms identify which historical measurements most inform current predictions. Foundation model embeddings at each timepoint provide richer input representations than raw laboratory values alone.\n\n\n\n\n\n\nKey Insight: Static Genetics, Dynamic Clinical Context\n\n\n\nThe fundamental design choice in temporal genomic risk modeling is how to combine time-invariant genetic features (germline sequence does not change) with time-varying clinical context (laboratory values, disease progression, treatment response). The most effective architectures treat genetics as a “prior” that sets baseline risk trajectory, then update predictions as clinical observations accumulate. This mirrors clinical reasoning: genetic predisposition establishes susceptibility, but current clinical state determines immediate risk.\n\n\nThe choice between survival and longitudinal frameworks depends on the clinical question and available data. When the goal is baseline risk stratification (identifying high-risk patients at a single decision point), survival models with static genomic features often suffice. When the goal is dynamic monitoring (detecting deterioration as it develops), longitudinal models that update predictions as new measurements arrive become necessary. Hybrid approaches that initialize with genomic risk and update based on clinical trajectory combine the strengths of both paradigms.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-evaluation",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-evaluation",
    "title": "27  Clinical Risk Prediction",
    "section": "27.6 Evaluation for Clinical Deployment",
    "text": "27.6 Evaluation for Clinical Deployment\nHigh performance on held-out test sets is necessary but far from sufficient for clinical deployment. Risk models must satisfy multiple evidence standards that typical machine learning papers do not address, and teams planning translation must understand these requirements from the outset rather than discovering them after development is complete.\n\n\n\n\n\n\nStop and Think\n\n\n\nImagine you have developed a foundation model-based cardiovascular risk predictor that achieves auROC of 0.82 on your test set, substantially better than the 0.76 of the traditional Pooled Cohort Equations. A health system is interested in deploying it. What questions should they ask before integration? What evidence would you need beyond test set performance? Think about this before reading the evaluation framework below.\n\n\n\n27.6.1 Discrimination\nDiscrimination measures how well a model ranks patients by risk, distinguishing those who will experience outcomes from those who will not. For binary endpoints like disease occurrence within a fixed time window, the area under the receiver operating characteristic curve (auROC) summarizes discrimination across all classification thresholds (Section 12.5). When outcomes are rare (severe adverse drug reactions, specific disease subtypes), the area under the precision-recall curve (auPRC) better reflects how well the model identifies true positives among many negatives. For survival tasks with censoring, the concordance index and time-dependent auROC generalize these metrics to the time-to-event setting.\nStrong discrimination is necessary but not sufficient. A model that correctly ranks patients but systematically overestimates or underestimates absolute risk magnitudes will lead to inappropriate clinical decisions. If a model predicts 5% risk for patients who actually experience 15% event rates, physicians using those predictions will undertreat. Conversely, systematically inflated predictions lead to overtreatment with attendant harms and costs.\n\n\n27.6.2 Calibration\nCalibration asks whether predicted probabilities match observed frequencies. If a model assigns 20% risk to a group of patients, approximately 20% should experience the outcome. Well-calibrated predictions can be interpreted at face value and used directly for clinical decision-making; miscalibrated predictions mislead regardless of discrimination quality.\nAssessment involves calibration plots comparing predicted risk deciles to observed event rates, statistical tests like the Hosmer-Lemeshow test, and proper scoring rules like the Brier score that combine calibration and discrimination (Section 12.10). The methodological foundations for these assessments, including temperature scaling and isotonic regression approaches, are detailed in Section 23.3. These assessments must be stratified by clinically relevant subgroups (ancestry, sex, age, comorbidity burden) because a model well-calibrated overall may be systematically miscalibrated for specific populations.\nFor polygenic score-informed models, calibration requires particular attention. Raw polygenic scores are typically centered and scaled rather than calibrated to absolute risk. Why are raw scores uncalibrated? A PGS is constructed by summing effect sizes across variants, producing a relative ranking rather than an absolute probability. The score distribution depends on the training population’s allele frequencies and LD structure; the same score percentile maps to different absolute risks depending on baseline disease incidence, age, sex, and environmental exposures. Without anchoring to an external incidence rate, the score carries no inherent probability interpretation. Mapping a score to an absolute event probability requires post-hoc models incorporating baseline incidence and clinical covariates. Foundation models can shift score distributions as architectures evolve, meaning recalibration may be necessary when updating encoders. The connection to Chapter 23 is direct: calibration is one form of uncertainty quantification, assessing whether model confidence aligns with actual outcome frequencies.\n\n\n27.6.3 Clinical Utility\nBeyond discrimination and calibration, clinical utility asks whether using the model will change decisions beneficially. Net reclassification improvement quantifies how many patients are appropriately moved across risk thresholds compared to a baseline model. Decision curve analysis estimates net benefit across threshold probabilities, accounting for the relative costs of false positives and false negatives in specific clinical contexts.\nFor foundation model-based tools, these analyses must demonstrate incremental value over existing alternatives. If a complex genomic foundation model provides only marginal improvement over a traditional polygenic score plus standard clinical calculator, the additional complexity, cost, and implementation burden may not be justified. The relevant comparison is not “better than nothing” but “better than what clinicians can already access.”\n\n\n\nTable 27.4: The three pillars of clinical evaluation for risk prediction models.\n\n\n\n\n\n\n\n\n\n\n\nEvaluation Dimension\nKey Question\nPrimary Metrics\nSubgroup Requirements\n\n\n\n\nDiscrimination\nDoes the model rank patients correctly?\nauROC, auPRC, C-index\nBy ancestry, sex, age\n\n\nCalibration\nDo predicted probabilities match reality?\nCalibration slope, ECE, Brier\nBy ancestry, comorbidity\n\n\nClinical utility\nDoes using the model improve decisions?\nNRI, decision curves, net benefit\nBy decision threshold\n\n\nIncremental value\nIs it better than existing tools?\nDelta-metrics vs. baseline\nAcross care settings\n\n\n\n\n\n\n\n\n\n\n\n\nBaseline Requirements for Rigorous Evaluation\n\n\n\nClaims that foundation model-based risk tools improve upon “state-of-the-art” polygenic prediction require verification of baseline strength. The minimum baseline battery for rigorous evaluation should include:\n\nLDpred2-auto or LDpred2-grid: LD-aware Bayesian method that estimates polygenicity and heritability directly\nPRS-CS or PRS-CS-auto: Continuous shrinkage prior accommodating highly polygenic architectures\nSBayesR or SBayesRC: Mixture model approach; annotation-integrated version provides additional benchmark\nPublished PGS from PGS Catalog for the specific trait, representing community-validated scores\nXGBoost or random forest: Non-deep-learning ML alternative establishing whether neural network complexity is necessary\n\nUsing only clumping-and-thresholding (C+T) as baseline artificially inflates apparent foundation model gains by 16-60%. When properly tuned linear methods are included, neural networks for polygenic prediction often perform only 93-95% as well—with apparent nonlinear advantages reflecting implicit LD modeling rather than genuine epistasis detection or representation learning (Ge et al. 2019).\nThe diagnostic question: does your foundation model approach outperform the best available linear method, or only weak linear methods? Incremental improvement over strong baselines justifies deployment complexity; improvement only over weak baselines does not.\n\n\n\n\n27.6.4 Validation Hierarchy\nEvidence strength depends critically on validation design. Internal validation through cross-validation or temporal splits within development data is useful but insufficient due to potential overfitting and subtle data leakage issues discussed in Section 12.4.1. External validation across institutions and ancestries tests the same locked model in independent health systems and diverse populations. This step is essential for assessing whether performance reflects genuine biological signal versus idiosyncratic features of the development dataset.\n\n\n\n\n\n\nClinical validation hierarchy from internal holdout to prospective trials\n\n\n\n\nFigure 27.3: Clinical validation hierarchy. From bottom: internal holdout validation is easiest but overestimates deployment performance; external retrospective validation tests generalization across institutions; prospective observational validation monitors real-time performance without influencing care; prospective interventional trials demonstrate clinical utility through randomized comparisons; long-term outcome tracking provides ultimate validation but is rarely achieved. Each level requires greater investment but provides stronger evidence.\n\n\n\nProspective observational validation runs the model silently alongside clinical care without influencing decisions, measuring real-time performance and drift in deployment conditions. Prospective interventional trials use randomized or quasi-experimental designs to assess whether model-guided care actually improves outcomes, equity, and cost-effectiveness compared to usual care.\nFor most foundation model-based tools, regulators and health systems expect robust external validation at minimum. High-stakes applications (cancer prognosis affecting treatment intensity, pharmacogenomic predictions affecting drug choice) may require prospective interventional evidence. The investment required increases at each level of the hierarchy, but so does the confidence that deployment will produce benefit rather than harm.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-uncertainty",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-uncertainty",
    "title": "27  Clinical Risk Prediction",
    "section": "27.7 Uncertainty Quantification",
    "text": "27.7 Uncertainty Quantification\nIn clinical settings, models must know when they do not know. A risk prediction offered with false confidence is more dangerous than one accompanied by appropriate uncertainty bounds, because the former invites unwarranted action while the latter prompts appropriate caution or additional evaluation.\nTwo sources of uncertainty require distinction. Aleatoric uncertainty reflects irreducible noise in the outcome: even with perfect input features, some patients with identical measured characteristics will experience different outcomes due to unmeasured variables, stochastic biology, or measurement error. Epistemic uncertainty reflects model limitations: insufficient training data, architectural constraints, or distributional shift between training and deployment conditions. Aleatoric uncertainty cannot be reduced by collecting more data or improving models; epistemic uncertainty can (Section 23.1).\n\n\n\n\n\n\nCommunicating uncertainty in clinical risk prediction\n\n\n\n\nFigure 27.4: Communicating uncertainty in clinical risk prediction. Single point estimates without context can mislead; intervals with best estimates provide actionable uncertainty. Visual formats include icon arrays (intuitive for patients), risk thermometers (familiar to clinicians), and contextual comparisons. Decision support should integrate uncertainty directly: flagging predictions with high epistemic uncertainty, showing confidence bands around thresholds, and providing sensitivity analysis.\n\n\n\nPractical uncertainty quantification methods include ensemble approaches, where multiple models trained with different random seeds provide prediction intervals based on their disagreement (Section 23.4.1). Monte Carlo dropout approximates Bayesian uncertainty by averaging predictions across stochastic forward passes (Section 23.4.2). Conformal prediction provides principled prediction intervals with guaranteed coverage under exchangeability assumptions, avoiding the distributional assumptions required by parametric methods (Section 23.5). Temperature scaling post-hoc adjusts model outputs to improve calibration without retraining (Section 23.3).\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA foundation model-based risk tool provides a prediction of 35% 10-year cardiovascular risk for a patient of Nigerian ancestry, along with a wide confidence interval of 15-55%. The model was trained primarily on European-ancestry data. Is this wide interval more likely reflecting aleatoric or epistemic uncertainty? What would you recommend the clinician do with this prediction?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThis wide interval primarily reflects epistemic uncertainty due to distributional shift: the model has limited training data from African-ancestry populations and thus low confidence in its predictions. The clinician should interpret this prediction with caution, consider it alongside other risk factors, and potentially order additional testing rather than making treatment decisions based solely on this uncertain estimate.\n\n\n\n\n\nFor foundation model-based systems, uncertainty decomposes into genomic and clinical components. Genomic uncertainty reflects confidence in variant effect predictions, fine-mapping probabilities, or embedding reliability; it increases for variants from underrepresented populations, rare variants with limited training examples, or sequences falling outside the distribution seen during pretraining. Clinical uncertainty reflects extrapolation to new care settings, practice patterns, or patient populations not represented in development data.\nSelective prediction allows models to abstain when uncertainty exceeds thresholds, flagging cases for human review rather than providing potentially misleading predictions (Section 23.7). This is particularly important for patients from rare ancestries underrepresented in training data or with unusual clinical presentations. The tension between coverage (providing predictions for all patients) and reliability (ensuring predictions are trustworthy) must be navigated thoughtfully, ideally with input from the clinicians who will use the system.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-fairness",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-fairness",
    "title": "27  Clinical Risk Prediction",
    "section": "27.8 Fairness and Health Equity",
    "text": "27.8 Fairness and Health Equity\nMany genomic and electronic health record datasets encode historical inequities in who gets genotyped, which populations are recruited into biobanks, and how healthcare is documented and delivered. Risk models trained on such data can amplify disparities if not carefully evaluated and designed.\nThe structural biases that genomic datasets inherit, from sequencing cohort recruitment to biobank composition to ClinVar submission patterns, create cascading effects on model performance. These biases manifest not as random noise but as systematic underperformance for populations historically excluded from genomic research (?sec-ch22-ancestry-confounding).\n\n\n\n\n\n\nKey Insight: Bias Compounds Through the Pipeline\n\n\n\nUnderrepresentation of non-European ancestries compounds at each stage of the genomic AI pipeline. GWAS discovery is underpowered. Fine-mapping resolution is reduced due to different LD patterns. Variant effect predictors have fewer training examples. PRS portability suffers. Foundation model embeddings are less well-calibrated. Each layer of the stack inherits and potentially amplifies the biases of preceding layers, making end-to-end equity evaluation essential rather than optional.\n\n\nThe ancestry bias in genome-wide association studies persists in foundation model applications. As discussed in Section 3.7, polygenic scores derived from European-ancestry data substantially underperform in other populations. Foundation models have the opportunity but not the guarantee to improve portability by leveraging functional priors that transfer across ancestries (sequence-based deleteriousness does not depend on population-specific linkage disequilibrium) and by incorporating multi-ancestry training data. Whether they succeed depends on training data composition, evaluation practices, and explicit attention to cross-ancestry performance throughout development.\n\n\n\n\n\n\n\n\nAncestry-stratified performance reveals masked disparities\n\n\n\n\n\n\n\nCalibration varies by group with overconfidence for underrepresented\n\n\n\n\n\n\n\n\n\nBias accumulates from training data through deployment\n\n\n\n\n\n\n\nMitigation strategies for equitable performance\n\n\n\n\n\n\nFigure 27.5: Fairness assessment in clinical genomic AI. (A) Ancestry-stratified performance reveals disparities masked by aggregate metrics—African ancestry AUC significantly lower than European. (B) Calibration varies by group: models may be overconfident for underrepresented populations. (C) Bias accumulates: European-dominated training data leads to models that underperform on other populations, compounded by EHR biases. (D) Mitigation strategies: stratified evaluation disclosure, training reweighting, group-specific calibration, and uncertainty flagging for underrepresented inputs.\n\n\n\nElectronic health record features introduce additional bias sources. Which patients receive genetic testing, which laboratory tests are ordered, how diagnoses are coded, and how thoroughly clinical notes are documented all differ systematically across patient populations, care settings, and health systems. A model trained on one institution’s data may encode those institutional patterns rather than underlying biology.\nHealth equity evaluation requires disparity metrics measuring performance differences in discrimination, calibration, and clinical utility across subgroups defined by ancestry, sex, socioeconomic proxies, and care site. Access metrics assess whether financial, geographic, or systemic barriers limit which patients can benefit from genomic risk tools. Outcome metrics evaluate whether clinical actions triggered by predictions differ across groups and whether benefits accrue equitably or concentrate among already-advantaged populations.\nTechnical mitigation strategies include reweighting training data to reduce representation disparities, group-wise calibration ensuring equitable performance across subgroups, and localized fine-tuning using deployment-site data. These approaches are discussed further in ?sec-ch22-mitigation. Technical interventions alone cannot overcome structural inequities. Non-technical approaches including expanding sequencing access, subsidizing testing for underserved populations, and designing workflows that accommodate diverse care settings are equally essential.\nThe core principle is that equity cannot be an afterthought addressed during final evaluation. It must inform pretraining data selection, benchmark choice, validation study design, and deployment planning from the outset. A model that appears well-calibrated overall but is miscalibrated for specific populations will exacerbate rather than reduce health disparities.\nThe governance frameworks, regulatory considerations, and responsible development practices for ensuring equitable clinical AI are examined in Section 26.1.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-clinical-integration",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-clinical-integration",
    "title": "27  Clinical Risk Prediction",
    "section": "27.9 Clinical Integration",
    "text": "27.9 Clinical Integration\nEven a comprehensively validated model can fail in practice if it does not integrate into clinical workflows. Genomic risk predictions must reach clinicians at decision points, in formats that support rather than disrupt care delivery, with appropriate interpretability and uncertainty communication.\n\n\n\n\n\n\nClinical workflow integration for genomic risk prediction\n\n\n\n\nFigure 27.6: Clinical workflow integration for genomic risk prediction. From data collection through processing, result generation, clinical review, and action. Foundation models contribute during processing and result generation. Human oversight checkpoints ensure clinician review before action. Adoption barriers include compute infrastructure, reimbursement, liability concerns, and clinician trust. Effective integration requires attention to timing, display, and action triggers at each stage.\n\n\n\n\n27.9.1 Workflow Integration Patterns\nClinical genomics has established pathways for returning results through CLIA-certified laboratories, structured reports, and genetic counseling. Foundation model-based risk tools can augment these pathways in two primary ways. Laboratory interpretation augmentation uses foundation model predictions to prioritize variants for manual review, provide richer functional annotations, and suggest likely disease mechanisms supporting differential diagnosis. Direct risk embedding in electronic health records precomputes risk scores for patients with genomic data, surfaces them in structured fields or clinical dashboards, and triggers alerts when thresholds are crossed.\nDesign choices include batch versus on-demand computation (batch overnight processing is often preferable given foundation model computational costs and the relative stability of genomic data), synchronous alerts at order entry versus asynchronous reports in clinical inboxes, and whether high-impact predictions require human-in-the-loop review before reaching front-line clinicians.\nThe specifics vary by clinical context. Pharmacogenomic alerts might appear synchronously at prescription order entry, providing immediate guidance on drug selection or dosing. Cardiometabolic risk scores might appear in primary care dashboards updated weekly, informing prevention discussions at annual visits. Oncology prognosis estimates might be generated at diagnosis and reviewed in tumor board settings where multidisciplinary teams make treatment decisions.\n\n\n\n\n\n\nPractical Guidance: Integration Patterns by Use Case\n\n\n\nPharmacogenomics: Synchronous alerts at prescription entry. Pre-compute patient drug-gene interaction profiles. Alert must include actionable alternatives, not just warnings.\nPrimary care prevention: Batch scoring with dashboard display. Weekly updates sufficient given slow-changing genomic risk. Integrate with existing cardiovascular risk calculators.\nOncology prognosis: Generate at diagnosis for tumor board review. Include uncertainty bounds and subgroup performance caveats. Human geneticist review before clinical action.\nRare disease diagnosis: On-demand for active diagnostic workups. Prioritize interpretability (which variants, what mechanisms) over pure risk scores.\n\n\n\n\n27.9.2 System Architecture\nFrom an engineering perspective, foundation model-based clinical tools typically require a secure model-serving endpoint handling inference requests, input adapters transforming laboratory and electronic health record data into model-ready formats, output adapters mapping predictions to structured clinical concepts or user-facing text, and logging infrastructure providing audit trails and enabling drift detection.\nRegulated settings impose additional requirements: versioning of models, data pipelines, and reference genomes with complete reproducibility; access controls and network segmentation protecting genomic data; and validation environments separated from production for safe testing of updates. Practical guidance on hardware requirements, deployment patterns, and cost estimation appears in Appendix B.\n\n\n27.9.3 Post-Deployment Monitoring\nClinical deployment begins rather than ends the model lifecycle. Practice patterns evolve as new treatments and guidelines emerge. Patient populations shift as screening programs expand or contract. Laboratory assays and sequencing pipelines change, introducing distributional shifts in input features.\nMonitoring systems should track input distributions (genotype frequencies, electronic health record feature patterns) to detect when current patients differ from training populations. Output distributions (risk score histograms, threshold-crossing rates) reveal whether model behavior is changing. Performance metrics computed via rolling windows or periodic audits detect calibration or discrimination degradation before clinical consequences accumulate.\nWhen drift is detected, responses range from recalibration (adjusting the score-to-probability mapping while preserving ranking behavior) through partial retraining (updating prediction heads while keeping foundation model weights fixed) to full model updates (retraining encoders, requiring renewed validation). The modular separation between foundation model backbones and clinical prediction heads facilitates this maintenance: encoders can be versioned and swapped with compatibility testing while prediction heads adapt to local deployment conditions.\nIncident response processes allow clinicians to report surprising or harmful predictions, triggering root-cause analysis and potential remediation. Governance structures including AI oversight committees review models periodically and establish clear criteria for deprecation when performance degrades below acceptable thresholds.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-regulatory",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-regulatory",
    "title": "27  Clinical Risk Prediction",
    "section": "27.10 Regulatory and Quality Frameworks",
    "text": "27.10 Regulatory and Quality Frameworks\nFoundation model-based clinical tools exist on a spectrum from research-only applications supporting hypothesis generation through clinical decision support tools informing diagnosis or management to regulated medical devices subject to formal oversight. The regulatory classification depends on intended use, risk level, and the claims made for the tool.\nJurisdictions differ in specifics, but common expectations include transparent descriptions of training data and known limitations, quantitative performance evidence across relevant subgroups, plans for post-market surveillance and incident reporting, and change management procedures for model updates. Beyond formal regulation, health systems typically require standard operating procedures for deployment and decommissioning, model cards describing training data and limitations, validation reports documenting evaluation evidence, and governance structures reviewing and approving new tools (Section 26.1).\nFoundation models introduce additional documentation requirements. Descriptions of pretraining corpora must specify which genomes, assays, and populations were included. Fine-tuning datasets and label definitions require detailed documentation. Procedures for updating to new genome builds, reference panels, or assay types must be established and tested. The modular separation between pretrained encoders and clinical prediction heads can ease regulatory management by allowing independent updates to each component, but this requires careful version control and compatibility testing to ensure that updating one component does not degrade performance of the combined system.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-case-studies",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-case-studies",
    "title": "27  Clinical Risk Prediction",
    "section": "27.11 Case Studies",
    "text": "27.11 Case Studies\nThree stylized case studies illustrate how foundation model features integrate into clinical risk prediction across different disease contexts, time horizons, and decision types.\n\n\n\n\n\n\nStop and Think\n\n\n\nAs you read each case study, consider: (1) What specific clinical decision does the prediction inform? (2) What evidence would be required before deployment? (3) What equity considerations apply? These questions connect the abstract principles discussed earlier to concrete clinical scenarios.\n\n\n\n27.11.1 Cardiometabolic Risk Stratification\nA 52-year-old man presents to his primary care physician for an annual wellness visit. His LDL cholesterol is 145 mg/dL, blood pressure is 138/88 mmHg, and hemoglobin A1c is 5.9%, placing him in the prediabetic range. His father had a myocardial infarction at age 58. The standard Pooled Cohort Equations estimate his 10-year atherosclerotic cardiovascular disease risk at 8.2%, just below the threshold where guidelines recommend statin therapy.\nA foundation model-augmented risk system could refine this assessment. Variant effect scores from DNA foundation models annotate variants in cardiometabolic risk loci with predicted regulatory and coding impacts, combining sequence-based scores with fine-mapping probabilities to prioritize likely causal variants (?sec-ch14-dna-vep; ?sec-ch14-combining-evidence). A polygenic embedding model like Delphi or G2PT produces a genome-wide representation capturing nonlinear risk structure beyond simple effect size sums (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025). This genomic embedding combines with electronic health record features through an intermediate fusion architecture, producing an updated 10-year risk estimate of 11.4%, above the treatment threshold.\n\n\n\n\n\n\nFM-enhanced cardiovascular risk prediction case study\n\n\n\n\nFigure 27.7: Case study: FM-enhanced cardiovascular risk prediction. A 45-year-old male with family history integrates genetic PRS (enhanced by regulatory variant effects), LDLR coding variant (scored by AlphaMissense), inflammatory expression signature, and clinical features. FM-integrated prediction (28% 10-year risk) exceeds both traditional Framingham (18%) and PRS-only (24%) approaches. Clinical action: statin therapy with annual monitoring. Caveat: model validated primarily in European populations; additional caution warranted for other ancestries.\n\n\n\nThe clinical value depends on what this refined estimate enables. If genomic foundation model features merely replicate traditional polygenic score information with higher computational cost, the benefit is marginal. But if the embedding captures pathway-level structure that identifies this patient’s risk as concentrating in LDL metabolism pathways rather than inflammatory or thrombotic mechanisms, that information might strengthen the indication for statin therapy specifically. Attention-based attributions highlighting which genomic regions contribute most to the elevated risk could inform counseling about heritability and family screening.\nExternal validation across multiple health systems and ancestries would need to demonstrate that the foundation model approach provides calibrated predictions and meaningful reclassification improvement over traditional tools. Equity analysis would verify that performance holds across the diverse populations the health system serves rather than degrading for non-European ancestries underrepresented in training data.\n\n\n27.11.2 Oncology Prognosis\nA 64-year-old woman has undergone surgical resection for stage II colorectal cancer with microsatellite stable tumor characteristics. Her oncology team must decide whether adjuvant chemotherapy is warranted given the balance between recurrence risk reduction and treatment toxicity. Traditional staging provides prognostic information, but substantial heterogeneity exists within stage categories.\nFoundation models can enrich prognostic assessment through multiple channels. Tumor mutation profiles encoded through models like SetQuence or SetOmic produce embeddings capturing the specific constellation of somatic alterations beyond simple mutation counts (Jurenaite et al. 2024). Transcriptomic profiling integrated through GLUE-style latent spaces adds expression context reflecting tumor microenvironment and pathway activity (Cao and Gao 2022). Graph neural network-based subtyping assigns the tumor to a molecular subtype with characteristic prognosis and treatment response patterns (Li et al. 2022).\nThese tumor-level representations combine with germline pharmacogenomic features (variants affecting fluoropyrimidine metabolism that influence toxicity risk) and clinical features (performance status, comorbidities, patient preferences) in a survival model predicting two-year recurrence hazard. A high-risk prediction might favor more intensive adjuvant therapy, while low-risk predictions might support observation with close surveillance.\nThe validation requirements are stringent. Retrospective analysis of institutional cohorts establishes proof of concept, but prospective validation in cohorts receiving contemporary treatment regimens is necessary given the rapid evolution of oncology care. Interpretability connecting predictions to specific mutations, pathways, or molecular subtypes supports clinical adoption by providing rationale beyond a black-box hazard estimate.\n\n\n27.11.3 Pharmacogenomic Adverse Event Prediction\nA 45-year-old man with newly diagnosed epilepsy requires anticonvulsant therapy. Carbamazepine is a common first-line choice, but it carries risk of severe cutaneous adverse reactions including Stevens-Johnson syndrome and toxic epidermal necrolysis. The HLA-B15:02* allele is strongly associated with carbamazepine hypersensitivity in patients of Asian ancestry, and FDA labeling recommends genetic testing before initiating therapy in at-risk populations [Citation Needed].\nThis established pharmacogenomic association illustrates both the potential and limitations of current approaches. Single-variant associations with high effect sizes enable straightforward clinical implementation, but they cover a small fraction of drug-gene interactions. Many patients who do not carry HLA-B15:02* still experience adverse reactions, suggesting additional genetic (and non-genetic) risk factors that single-variant testing misses.\nFoundation models could extend pharmacogenomic prediction beyond established single-gene associations. Variant effect scores across HLA genes, drug metabolism enzymes, and immune-related loci provide features reflecting the patient’s overall pharmacogenetic landscape (Section 2.8.4). These features aggregate into a polygenic adverse event risk score that captures contributions from many variants rather than relying on individual high-effect alleles. Combined with clinical features (renal function affecting drug clearance, concomitant medications with interaction potential, prior adverse reaction history), the model predicts adverse event probability specific to the proposed drug.\nThe validation challenge is severe. Serious adverse drug reactions are rare, making endpoint ascertainment difficult and underpowered. Case-control designs enriched for adverse events may overestimate model performance compared to prospective deployment. Multi-site validation across healthcare systems with different prescribing patterns and population ancestry compositions is essential.\nClinical implementation requires integration at the point of prescribing, providing actionable information when drug selection decisions are being made. This argues for pre-computed pharmacogenomic profiles that alert at order entry rather than reactive testing after a prescription is written. The interpretability requirement is particularly acute: clinicians must understand why a model flags a patient as high-risk for a specific drug to make informed risk-benefit decisions.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch27-clinical-risk.html#sec-ch27-translation-test",
    "href": "part_6/p6-ch27-clinical-risk.html#sec-ch27-translation-test",
    "title": "27  Clinical Risk Prediction",
    "section": "27.12 Translation as the Test",
    "text": "27.12 Translation as the Test\nSuccess for genomic foundation models in clinical medicine will depend less on model scale and more on rigorous translation. Problem definition, evidence generation, equity evaluation, regulatory compliance, workflow integration, and post-deployment monitoring each introduce opportunities for failure. Models that clear all hurdles are rare; models that skip stages fail in deployment regardless of their technical sophistication.\nThe representational advances that foundation models provide become valuable only when they flow through validated, equitable, well-integrated clinical tools into decisions that improve patient outcomes. A pathogenicity score with state-of-the-art discrimination adds nothing to care if it reaches clinicians at the wrong moment, in the wrong format, without appropriate uncertainty communication. A risk prediction that performs excellently on average but fails systematically for underrepresented populations may widen health disparities rather than narrow them. Technical capability is necessary but not sufficient for clinical impact.\nRare disease diagnosis illustrates these translation principles in a particularly high-stakes context: where risk prediction addresses population-level stratification, variant interpretation addresses individual patients, with different evidence requirements, clinical workflows, and definitions of success (Chapter 28).\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat are the three main limitations of traditional polygenic risk scores that foundation model features aim to address?\nCompare early, intermediate, and late fusion architectures for integrating genomic and clinical features. When would you choose each approach?\nWhy is calibration distinct from discrimination, and why does it matter for clinical deployment?\nHow does bias compound through the genomic AI pipeline, from GWAS to foundation model predictions?\nWhat is the difference between analytical validity, clinical validity, and clinical utility? Give an example where a model has one but not the others.\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nPRS Limitations: Traditional polygenic scores have three main limitations: (1) they lack mechanistic insight by reducing genomes to single numbers without indicating which biological pathways drive risk, (2) they show poor cross-ancestry portability because they are derived from European-dominated GWAS data and depend on population-specific linkage disequilibrium patterns, and (3) they remain disconnected from clinical workflows by existing outside electronic health records where decisions actually happen. Foundation model embeddings address these by preserving information about which genomic regions contribute to risk, leveraging sequence-based functional priors that transfer across ancestries, and enabling integration architectures that combine genomic and clinical data within EHR systems.\nFusion Architectures: Early fusion concatenates all features into a single input and trains a unified model, allowing arbitrary interactions but requiring complete data for all patients and risking dominance by the strongest signal source. Intermediate fusion trains separate encoders for each modality then combines their embeddings through attention or cross-modal transformers, offering modularity and graceful degradation while still capturing cross-modal interactions. Late fusion trains independent models per modality and combines their predictions through ensemble methods, handling missing data excellently but potentially underutilizing cross-modal structure. Choose early fusion for dense complete datasets, intermediate fusion for evolving foundation model ecosystems requiring modularity, and late fusion when data availability varies substantially across patients.\nCalibration vs Discrimination: Discrimination measures how well a model ranks patients by risk (whether those who develop disease score higher than those who do not), typically assessed via auROC. Calibration measures whether predicted probabilities match observed frequencies (whether patients assigned 20% risk actually experience events at 20% rate). A model can have excellent discrimination but poor calibration if it correctly ranks patients but systematically over- or underestimates absolute risk magnitudes. This matters for clinical deployment because treatment decisions depend on absolute risk thresholds: miscalibrated predictions lead to inappropriate treatment (undertreatment if risks are underestimated, overtreatment if inflated) even when patient ranking is correct.\nBias Compounding: Bias accumulates at each stage of the genomic AI pipeline due to European ancestry overrepresentation in genomic datasets. GWAS discovery power is reduced for non-European populations, leading to weaker effect size estimates. Fine-mapping resolution suffers because linkage disequilibrium patterns differ across ancestries. Variant effect predictors have fewer training examples from underrepresented populations. Polygenic scores built from these components show poor portability and reduced accuracy. Foundation model embeddings trained on biased data produce less calibrated representations for non-European ancestries. Each layer inherits and potentially amplifies the biases of preceding layers, making ancestry-stratified evaluation essential at every stage rather than only at final deployment.\nValidity Levels: Analytical validity means the test accurately measures what it claims (e.g., genotyping array correctly calls variants). Clinical validity means the measurement associates with the clinical outcome (e.g., polygenic score correlates with disease risk). Clinical utility means using the test improves patient outcomes (e.g., knowing the score leads to interventions that reduce disease incidence). A model can have high analytical and clinical validity but no clinical utility if the resulting predictions do not change clinical decisions: for example, a perfectly accurate test for a biomarker that strongly predicts disease but for which no effective interventions exist would have excellent validity but provide no utility since knowing the result does not enable actions that improve outcomes.\n\n\n\n\n\n\nThis chapter examined the translation of genomic foundation models into clinical risk prediction tools that can improve patient outcomes.\nKey Concepts:\n\nFrom PRS to embeddings: Traditional polygenic scores are scalar summaries with limited mechanistic insight and poor cross-ancestry portability. Foundation models produce rich embeddings that preserve information about which variants matter, how they interact, and why they contribute to risk.\nFusion architectures: Early, intermediate, and late fusion strategies offer different tradeoffs for combining genomic and clinical features. Intermediate fusion typically provides the best balance of modularity, cross-modal learning, and graceful degradation for missing data.\nEvaluation beyond discrimination: Clinical deployment requires not just good ranking (discrimination) but also accurate probability estimates (calibration) and demonstrated benefit over existing tools (clinical utility). Each must be evaluated across clinically relevant subgroups.\nValidation hierarchy: Evidence strength increases from internal validation through external validation, prospective observational studies, and prospective interventional trials. Most foundation model tools require at minimum robust external validation.\nUncertainty quantification: Distinguishing aleatoric (irreducible) from epistemic (model-limited) uncertainty enables appropriate clinical responses. Selective prediction allows abstention when confidence is low.\nEquity as design principle: Bias compounds through the genomic AI pipeline. Equity evaluation must span discrimination, calibration, and clinical utility across ancestry, sex, and socioeconomic subgroups. Technical mitigation alone cannot overcome structural inequities.\nWorkflow integration: Valid models fail in practice without appropriate integration into clinical workflows, monitoring for drift, and governance structures for oversight and incident response.\n\nLooking Ahead: Chapter 28 applies these translation principles to rare disease diagnosis, where the stakes are individual patients rather than population-level risk stratification, and where foundation models offer particularly compelling advantages over traditional approaches.\n\n\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “[GPN] DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T. Higgins-Chen, Steve Horvath, and Bo Wang. 2024. “CpGPT: A Foundation Model for DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE] Multi-Omics Single-Cell Data Integration and Regulatory Inference with Graph-Linked Embedding.” Nature Biotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024. “[DeepRVAT] Integration of Variant Annotations Using Deep Set Networks Boosts Rare Variant Association Testing.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nGe, Tian, Chia-Yen Chen, Yang Ni, Yen-Chen Anne Feng, and Jordan W. Smoller. 2019. “Polygenic Prediction via Bayesian Regression and Continuous Shrinkage Priors.” Nature Communications 10 (1): 1776. https://doi.org/10.1038/s41467-019-09718-5.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. “Delphi: A Deep-Learning Method for Polygenic Risk Prediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nJurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge, and René Jäkel. 2024. “SetQuence & SetOmic: Deep Set Transformers for Whole Genome and Exome Tumour Analysis.” BioSystems 235 (January): 105095. https://doi.org/10.1016/j.biosystems.2023.105095.\n\n\nKatzman, Jared L., Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval Kluger. 2018. “DeepSurv: Personalized Treatment Recommender System Using a Cox Proportional Hazards Deep Neural Network.” BMC Medical Research Methodology 18 (1): 24. https://doi.org/10.1186/s12874-018-0482-1.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam, Amit R. Majithia, and Trey Ideker. 2025. “[G2PT] A Genotype-Phenotype Transformer to Assess and Explain Polygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and Yunping Zhu. 2022. “MoGCN: A Multi-Omics Integration Method Based on Graph Convolutional Network for Cancer Subtype Analysis.” Frontiers in Genetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nNagpal, Chirag, Xinyu Li, and Artur Dubrawski. 2021. “Deep Survival Machines: Fully Parametric Survival Regression and Representation Learning for Censored Data With Competing Risks.” IEEE Journal of Biomedical and Health Informatics 25 (8): 3163–75. https://doi.org/10.1109/JBHI.2021.3052441.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nTipirneni, Sindhu, and Chandan K. Reddy. 2022. “Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series.” ACM Trans. Knowl. Discov. Data 16 (6): 105:1–17. https://doi.org/10.1145/3516367.\n\n\nXu, Leqi, Wangjie Zheng, Jiaqi Hu, Yingxin Lin, Jia Zhao, Gefei Wang, Tianyu Liu, and Hongyu Zhao. 2025. “Improving Polygenic Risk Prediction Performance by Integrating Electronic Health Records Through Phenotype Embedding.” The American Journal of Human Genetics 112 (12): 3030–45. https://doi.org/10.1016/j.ajhg.2025.11.006.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Clinical Risk Prediction</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-rare-disease.html",
    "href": "part_6/p6-ch28-rare-disease.html",
    "title": "28  Rare Disease Diagnosis",
    "section": "",
    "text": "28.1 Variant Prioritization Funnel\nClinical variant interpretation operates through progressive filtering, narrowing tens of thousands of candidates to a manageable set for expert review. Each filtering step applies different types of evidence, and foundation models contribute at multiple stages.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-rare-disease.html#sec-ch28-prioritization-funnel",
    "href": "part_6/p6-ch28-rare-disease.html#sec-ch28-prioritization-funnel",
    "title": "28  Rare Disease Diagnosis",
    "section": "",
    "text": "Stop and Think\n\n\n\nBefore reading on, consider: if you had 25,000 variants to evaluate and could only deeply analyze 10, what types of filters would you apply first? What information would help you eliminate the largest number of candidates quickly while minimizing the risk of discarding the causal variant?\n\n\n\n\n\n\n\n\nVariant filtering pipeline for rare disease diagnosis\n\n\n\n\nFigure 28.1: Variant filtering pipeline for rare disease diagnosis. Starting with ~25,000 variants from whole-genome sequencing, progressive filters remove low-quality calls, common population variants, and non-consequential changes. Foundation model scoring (~50 candidates) prioritizes variants for expert review (~5-10). The key insight: FMs contribute most effectively after basic filtering removes obvious noise but before expensive expert curation.\n\n\n\n\n28.1.1 Quality and Technical Filters\nThe first filter removes variants that are likely technical artifacts rather than true biological variation. Sequencing depth below 20x, strand bias exceeding established thresholds, and clustering of variants in repetitive regions all raise suspicion of false positives. The 20x depth threshold exists because variant calling requires sufficient reads to distinguish true heterozygous variants (expected ~50% alternate allele frequency) from sequencing errors (typically &lt;1% per position); below this threshold, stochastic sampling fluctuations make reliable genotyping impossible. Strand bias indicates that a variant appears predominantly on reads from one DNA strand, suggesting it arose from damage or amplification artifacts during library preparation rather than existing in the original genomic DNA. Variant clustering in repetitive regions reflects the fundamental challenge of short-read alignment: when a read could map to multiple genomic locations, misalignment creates apparent variants that do not exist in the sample. Variant calling pipelines like GATK and DeepVariant (see Section 1.8) produce quality scores that guide this initial triage. As discussed in Section 23.2, these confidence estimates require careful calibration; systematic miscalibration in specific genomic contexts propagates directly into interpretation, creating blind spots where uncertain calls masquerade as confident ones or vice versa. Variants failing quality thresholds are removed before any biological interpretation begins.\nFor trio analysis (proband plus both parents), Mendelian inheritance consistency provides an additional quality check. A variant called heterozygous in the child should appear in at least one parent unless it arose de novo. Widespread Mendelian inconsistencies indicate sample swaps, contamination, or systematic calling errors that must be resolved before interpretation proceeds.\n\n\n\n\n\n\nDeep Dive: Trio Analysis in Clinical Genetics\n\n\n\nFor ML/computational readers: Trio analysis sequences a patient (proband) plus both parents:\nKey terminology:\n\nProband: The affected individual being evaluated\nTrio: Proband + mother + father\nDe novo variant: Present in proband but absent in both parents (arose as new mutation)\nSegregation: Whether a variant “travels with” disease in a family\n\nWhy trios dramatically improve interpretation:\n\n\n\n\n\n\n\n\nScenario\nWithout Parents\nWith Trio\n\n\n\n\nDe novo identification\nCannot detect\nImmediately identified\n\n\nCompound heterozygosity\nMust infer\nDirectly observed (which parent contributed each allele)\n\n\nPhasing\nRequires population inference\nDirect observation\n\n\nQuality control\nLimited\nMendelian consistency checks\n\n\n\nDe novo mutations are particularly informative:\n\nMost rare disease variants are inherited, but ~1-2 variants per genome are de novo\nFor dominant conditions, de novo variants in relevant genes are strong evidence for causality\nDe novo variants have not been subjected to selection in previous generations\n\nCompound heterozygosity: In recessive disease, affected individuals have two pathogenic alleles. Trio analysis reveals whether two variants are:\n\nIn trans (one from each parent) → both copies disrupted → likely causal\nIn cis (both from same parent) → one functional copy remains → unlikely to cause recessive disease\n\n\n\n\n\n28.1.2 Population Frequency Filters\nVariants common in the general population are unlikely to cause rare, severe disease. If a variant appears in 1% of gnomAD individuals, it cannot plausibly explain a condition affecting one in 100,000 people under a dominant model. Frequency thresholds depend on inheritance mode and disease prevalence: dominant conditions with complete penetrance require extremely rare variants (often absent from population databases), while recessive conditions can tolerate higher carrier frequencies.\n\n\n\n\n\n\nKey Insight: Frequency Thresholds Are Disease-Specific\n\n\n\nA variant with 0.1% population frequency might be far too common for a dominant condition affecting 1 in 100,000 individuals, yet entirely plausible as a carrier allele for a recessive condition affecting 1 in 10,000. The “rare” threshold is not absolute but depends on the disease model, expected penetrance, and population prevalence. Always calculate expected allele frequency from disease prevalence rather than applying uniform cutoffs.\n\n\nThe Genome Aggregation Database (gnomAD) provides allele frequencies across over 800,000 individuals from diverse ancestries (see Section 2.2.3) (Karczewski et al. 2020). Applying a frequency threshold of 0.01% for dominant conditions and 1% for recessive carriers typically removes 95% or more of variants from consideration. Ancestry-matched frequencies matter: a variant rare in European populations may be common in African or East Asian populations, and global frequency alone can be misleading.\n\n\n28.1.3 Consequence and Gene Filters\nPredicted functional consequence shapes prioritization. Loss-of-function variants (frameshift, nonsense, canonical splice site) in genes intolerant to haploinsufficiency receive immediate attention. Missense variants require additional assessment, as most are benign. Intronic and intergenic variants have historically been deprioritized, though foundation models are beginning to identify functional noncoding variants with greater precision (see Section 16.2 for regulatory models and ?sec-ch14-enformer-vep for variant effect prediction in noncoding regions).\nGene-level filters incorporate prior knowledge. Curated gene panels for specific phenotypes (such as the PanelApp epilepsy panel or cardiomyopathy panel) restrict analysis to genes with established disease associations. For undiagnosed cases without clear phenotype match, broader approaches may include all OMIM disease genes or genes with high constraint (low observed/expected loss-of-function ratios in gnomAD).\n\n\n28.1.4 Foundation Model Scoring\nAfter quality, frequency, and consequence filters, foundation model predictions provide quantitative effect estimates for remaining candidates. For missense variants, AlphaMissense scores offer genome-wide pathogenicity estimates derived from protein structure and evolutionary conservation (Cheng et al. 2023). For splice-region variants, SpliceAI predictions quantify the probability and magnitude of splicing disruption (Jaganathan et al. 2019). For regulatory variants, Enformer and related models estimate effects on chromatin accessibility and gene expression in relevant tissues (Section 16.2; ?sec-ch14-enformer-vep) (Avsec et al. 2021).\n\n\n\n\n\n\nStop and Think\n\n\n\nYou have three candidate variants after filtering: (1) a missense variant with AlphaMissense score 0.95, (2) a splice-region variant with SpliceAI score 0.80, and (3) an intronic variant 50kb from any gene with low Enformer impact. All three are rare in gnomAD. Which would you prioritize for expert review, and why? What additional information would help you decide?\n\n\nThese scores do not directly translate to pathogenicity classifications. A high AlphaMissense score indicates that the protein change is likely functionally disruptive, not that it causes a specific disease. The clinical relevance of any functional disruption depends on the gene’s role in the patient’s phenotype, the inheritance pattern, and whether disruption of that gene produces the observed clinical features. Foundation model scores become one input to a structured evidence framework, not a standalone answer.\n\n\n\nTable 28.1: Comparison of Foundation Model Applications in Variant Prioritization\n\n\n\n\n\n\n\n\n\n\n\n\nVariant Type\nPrimary FM Tool\nWhat It Predicts\nClinical Question Answered\nKey Limitation\n\n\n\n\nMissense\nAlphaMissense\nProtein functional disruption\nIs amino acid change damaging?\nDoes not indicate disease specificity\n\n\nSplice region\nSpliceAI\nSplice site creation/disruption\nWill splicing be altered?\nCell-type-specific effects may vary\n\n\nRegulatory\nEnformer\nGene expression change\nWill expression be affected?\nLimited to trained tissues/cell types\n\n\nStructural\nAlphaFold\nProtein structure change\nIs protein fold disrupted?\nStatic structure, not dynamics\n\n\n\n\n\n\n\n\n\n\n\n\nStop and Think: Spaced Retrieval\n\n\n\nYou learned about population frequency filtering earlier (Section 28.1.2). Now that you understand foundation model scoring, consider: Why must frequency filtering happen before FM scoring rather than after? What would happen if you scored all 25,000 variants with AlphaMissense and then filtered by frequency?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nFrequency filtering must happen first for computational efficiency and biological logic. Scoring all 25,000 variants would waste resources on common variants that cannot explain rare disease regardless of predicted effect. More importantly, high FM scores on common variants represent benign variation that happens to alter protein function—the population frequency evidence overrides the functional prediction. The prioritization funnel applies filters in order of both efficiency (removing the most variants earliest) and biological logic (ruling out impossibilities before evaluating functional impact).",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-rare-disease.html#sec-ch28-acmg-amp",
    "href": "part_6/p6-ch28-rare-disease.html#sec-ch28-acmg-amp",
    "title": "28  Rare Disease Diagnosis",
    "section": "28.2 ACMG-AMP Criteria and Computational Evidence",
    "text": "28.2 ACMG-AMP Criteria and Computational Evidence\nThe American College of Medical Genetics and Genomics and Association for Molecular Pathology (ACMG-AMP) framework provides the dominant structure for clinical variant classification (Richards et al. 2015). Published in 2015 and subsequently refined through ClinGen expert panels, this framework assigns variants to five categories: pathogenic, likely pathogenic, variant of uncertain significance (VUS), likely benign, and benign. Classification emerges from combining multiple evidence types, each assigned a strength level (very strong, strong, moderate, supporting) and direction (pathogenic or benign).\n\n\n\n\n\n\nACMG-AMP variant classification framework\n\n\n\n\nFigure 28.2: ACMG-AMP variant classification framework. Variants are classified into five categories based on accumulated evidence. Pathogenic evidence ranges from Very Strong (PVS1: null variants) to Supporting (PP1-5). Benign evidence similarly ranges from Stand-alone (BA1: high population frequency) to Supporting (BP1-7). Foundation model predictions contribute to computational evidence criteria (PP3, BP4), but provide only Supporting-level evidence without additional validation.\n\n\n\n\n28.2.1 Evidence Categories\nACMG-AMP evidence spans several domains. Population data includes allele frequency in controls (BA1, BS1, BS2 for benign; PM2 for pathogenic support when absent). Computational predictions include in silico tools predicting deleterious effects (PP3 for pathogenic support) or benign effects (BP4 for benign support). Functional data includes well-established functional assays demonstrating deleterious (PS3) or no (BS3) effect. Segregation data addresses co-segregation with disease in multiple affected family members (PP1) or lack of segregation (BS4). De novo status assigns strong (PS2) or moderate (PM6) evidence when parental samples are available and the variant is absent in both parents. Clinical information incorporates specific phenotype match (PP4) and prevalence considerations.\nThe framework combines these evidence types through defined rules. Pathogenic classification requires either one very strong criterion plus one strong, or two strong criteria, with additional supporting evidence. Likely pathogenic requires somewhat less evidence. Most variants end up as VUS because available evidence is insufficient for confident classification in either direction.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA missense variant is absent from gnomAD (PM2), has a high AlphaMissense score (PP3), and occurs in a gene associated with the patient’s phenotype (PP4). Under ACMG-AMP rules, what would be the maximum classification? Why can’t this evidence alone achieve “Pathogenic”?\nHint: Consider what evidence strengths these criteria provide and what combinations are required for pathogenic classification.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe maximum classification would be “Likely Pathogenic.” These criteria provide one moderate (PM2) and two supporting (PP3, PP4) levels of evidence. Pathogenic classification requires either one very strong plus one strong criterion, or two strong criteria—this combination falls short of that threshold. The computational evidence (PP3) provides only supporting-level weight unless upgraded by gene-specific calibration.\n\n\n\n\n\n\n\n28.2.2 PP3 and BP4: Computational Evidence\nComputational predictions enter the ACMG-AMP framework primarily through PP3 (pathogenic supporting evidence from computational predictions) and BP4 (benign supporting evidence). These criteria apply when multiple in silico tools agree that a variant is deleterious (PP3) or benign (BP4).\nThe original 2015 guidelines assigned these criteria only “supporting” strength, reflecting appropriate caution about computational predictions available at the time. Tools like SIFT, PolyPhen-2, and CADD had limited accuracy and concerning circularity issues (Section 4.5). The evaluation challenges these tools face, including benchmark contamination and label leakage, are examined in ?sec-ch22-label-circularity. ClinGen sequence variant interpretation working groups have subsequently refined how computational evidence is weighted, in some cases upgrading to moderate strength for well-calibrated predictors in specific genes.\n\n\n\n\n\n\nChallenging Concept: Evidence Strength Calibration\n\n\n\nThe mapping from continuous model scores to discrete evidence strengths requires careful statistical calibration. This section involves likelihood ratios and odds of pathogenicity, concepts that many readers find initially counterintuitive. Take time with the relationship between prediction accuracy and evidence strength; it is foundational for clinical implementation.\n\n\nFoundation models raise new questions about computational evidence strength. AlphaMissense achieves substantially higher accuracy than traditional tools on held-out ClinVar variants and deep mutational scanning data. Should predictions from these models receive greater evidentiary weight? The answer is not straightforward. Higher accuracy on aggregate benchmarks does not guarantee reliability for any individual prediction. Gene-specific calibration matters: a model may perform well across all genes but poorly for genes with unusual structure or function. And the fundamental limitation remains that computational predictions estimate functional impact, not clinical pathogenicity.\nResponsible application of foundation model predictions in ACMG-AMP classification requires gene-specific and variant-type-specific calibration whenever possible, explicit acknowledgment that PP3/BP4 evidence is supporting unless upgraded by expert panel guidance, use of multiple orthogonal predictors rather than reliance on any single model, and clear documentation of which tools were applied and how predictions were interpreted.\n\n\n28.2.3 Calibrating Predictions to Evidence Strength\nMapping continuous foundation model scores to discrete ACMG evidence strengths requires calibration against the odds ratios established by ClinGen (Tavtigian et al. 2018). The calibration framework, detailed in Section 17.5 and Section 23.3, defines thresholds where supporting evidence requires ~2:1 odds, moderate ~4:1, and strong ~18:1. For computational predictors to warrant evidence strength upgrades, their predictions must demonstrably achieve these odds ratios in validation datasets.\n\n\n\nTable 28.2: ACMG-AMP Evidence Strength and Required Odds Ratios\n\n\n\n\n\n\n\n\n\n\n\nEvidence Strength\nOdds Ratio (Pathogenic)\nOdds Ratio (Benign)\nFM Threshold Example (AlphaMissense)\n\n\n\n\nSupporting\n~2:1\n~1:2\nScore 0.5-0.8\n\n\nModerate\n~4:1\n~1:4\nScore 0.8-0.9\n\n\nStrong\n~18:1\n~1:18\nScore &gt;0.9 (gene-specific validation needed)\n\n\nVery Strong\n~350:1\n~1:350\nNot achieved by current predictors alone\n\n\n\n\n\n\nFor AlphaMissense and similar foundation models, published validation shows that the highest-scoring variants (above 0.9) achieve odds ratios exceeding the strong evidence threshold in some gene contexts (Pejaver et al. 2022; Bergquist et al. 2025). ClinGen expert panels have begun incorporating these calibrations for specific genes, allowing upgraded evidence strength when predictions meet defined criteria. Clinicians should follow gene-specific expert panel recommendations when available rather than applying uniform thresholds across all genes.\n\n\n\n\n\n\nStop and Think\n\n\n\nA missense variant has an AlphaMissense score of 0.92. The gene-specific ClinGen recommendation allows upgrading PP3 to moderate strength for scores above 0.85 in this gene. The variant is also absent from gnomAD (PM2). Can this variant be classified as pathogenic? What additional evidence would be needed?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nNo, this cannot yet be classified as pathogenic. The evidence totals one moderate (PM2 for absence in population databases) plus one moderate (upgraded PP3). Pathogenic classification requires either one very strong plus one strong, or two strong criteria. This combination reaches only “Likely Pathogenic” at best. Additional evidence needed might include: de novo status (PS2, strong), strong functional data (PS3), or co-segregation with disease in multiple families (PP1 upgradable to strong).",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-rare-disease.html#sec-ch28-family-analysis",
    "href": "part_6/p6-ch28-rare-disease.html#sec-ch28-family-analysis",
    "title": "28  Rare Disease Diagnosis",
    "section": "28.3 Family-Based Analysis",
    "text": "28.3 Family-Based Analysis\nRare disease interpretation rarely relies on proband sequence alone. Family structure provides substantial additional information through inheritance pattern constraints, de novo status determination, and segregation analysis.\n\n\n\n\n\n\nKey Insight: Family Structure Multiplies Interpretive Power\n\n\n\nA single variant in a proband might be classified as VUS due to insufficient evidence. That same variant, when shown to have arisen de novo in a child with severe early-onset disease, gains strong pathogenic evidence (PS2). Family data transforms interpretation not by changing the variant itself but by providing context that constrains biological possibility. Always consider what family samples, if obtained, might resolve uncertain classifications.\n\n\n\n\n\n\n\n\n\n\nInheritance patterns: autosomal dominant, recessive, X-linked, de novo\n\n\n\n\n\n\n\nTrio analysis confirming de novo variants\n\n\n\n\n\n\n\nSegregation analysis through extended pedigrees\n\n\n\n\n\n\nFigure 28.3: Family-based analysis for rare disease diagnosis. (A) Inheritance patterns: autosomal dominant, autosomal recessive, X-linked, and de novo mutations each produce characteristic pedigree patterns. (B) Trio analysis: comparing proband to parents identifies de novo variants—particularly valuable for developmental disorders where FMs prioritize de novos in constrained genes. (C) Segregation analysis: tracking variants through extended families provides co-segregation evidence (ACMG PP1) when the variant consistently tracks with disease status.\n\n\n\n\n28.3.1 De Novo Variants\nDe novo variants arise newly in the proband and are absent in both parents. For severe, early-onset dominant conditions, de novo mutations are expected: affected individuals rarely reproduce, so the disease-causing allele must arise fresh each generation. Observing a damaging variant as de novo provides strong evidence for pathogenicity under ACMG-AMP (PS2), often sufficient to push a candidate toward likely pathogenic or pathogenic classification.\nThe informativeness of de novo status depends on the mutation rate at that site and the expected de novo rate for the variant class. The human germline mutation rate is approximately 1 to 1.5 new mutations per 100 million base pairs per generation (Kong et al. 2012). For protein-coding exons (approximately 30 million base pairs), each individual carries roughly one new coding variant on average. Finding a damaging de novo variant in a candidate gene is therefore much more suspicious than finding an inherited variant of similar predicted effect.\nFoundation models assist de novo interpretation by providing effect estimates that help prioritize among multiple de novo variants (typical trio sequencing identifies one to three de novo coding variants) and by identifying de novo variants in noncoding regions that might disrupt critical regulatory elements. A de novo variant in a brain-specific enhancer upstream of a known epilepsy gene, predicted by Enformer to substantially reduce gene expression (Section 16.2.1), warrants investigation even though traditional pipelines might overlook noncoding de novo events.\n\n\n28.3.2 Compound Heterozygosity and Phasing\nRecessive diseases require biallelic disruption: both copies of the gene must be affected for disease to manifest. When a proband carries two different heterozygous variants in the same gene, the critical question is whether these variants are in trans (on opposite chromosomes, leading to biallelic disruption) or in cis (on the same chromosome, leaving one copy functional). Think of it like having two copies of a critical instruction manual: if each copy has a different page torn out, you can piece together the complete instructions from the two damaged copies. But if both torn pages are from the same copy, you still have one perfect backup—and one copy missing two pages that you cannot use anyway.\n\n\n\n\n\n\nStop and Think\n\n\n\nA child has a rare recessive metabolic disorder. You identify two heterozygous missense variants in the candidate gene. The mother carries both variants. What does this tell you about whether these variants can explain the disease? What additional information would you need?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nIf the mother carries both variants and is unaffected, the variants are most likely in cis (on the same chromosome). The child inherited this chromosome from the mother, meaning one of the child’s gene copies has both variants, but the other copy (from the father) is likely normal. This configuration cannot explain a recessive disorder because one functional copy remains. You would need to: (1) confirm the father’s genotype at these positions, (2) check for a third variant the child might have inherited from the father, or (3) reconsider whether this gene explains the phenotype.\n\n\n\n\n\nPhasing determines which configuration applies (Section 1.4.1 for clinical stakes; Section 1.4.3 for methodological details). Several approaches are available. Physical phasing through long-read sequencing directly observes which variants occur on the same DNA molecule, providing definitive phase information when reads span both variant positions. Trio phasing infers phase from parental genotypes: if one variant is inherited from the mother and one from the father, they must be in trans. Statistical phasing uses population haplotype patterns to estimate phase, though accuracy decreases for rare variants not well-represented in reference panels.\nFor clinical interpretation, trio phasing is often the most practical approach. If both variants are confirmed in trans and both are predicted damaging, this supports pathogenicity under a recessive model. If both variants were inherited from a single parent (in cis), the gene cannot explain a recessive phenotype unless a third variant exists.\nFoundation models contribute by estimating the functional severity of each variant. A missense variant with marginal AlphaMissense score might not warrant attention alone, but paired in trans with a clear loss-of-function variant, the compound heterozygous combination could produce sufficient functional disruption to cause disease.\n\n\n28.3.3 Segregation Analysis\nIn larger families with multiple affected and unaffected individuals, segregation analysis examines whether candidate variants track with disease status. Under a dominant model, all affected individuals should carry the variant, and penetrance assumptions constrain how many unaffected carriers are expected. Under a recessive model, affected individuals should be homozygous or compound heterozygous, carriers should be heterozygous, and unaffected non-carriers should lack the variant entirely.\nStrong segregation evidence (PP1, upgradable to strong evidence with sufficient meioses) can substantially support pathogenicity classification. Equally important, failure to segregate provides benign evidence (BS4): a variant present in unaffected family members at rates inconsistent with the proposed inheritance model is unlikely to be causal.\nSegregation analysis requires accurate pedigree information, confirmed sample identities, and careful consideration of age-dependent penetrance and phenocopies. A variant might be present in an unaffected young relative who will develop disease later, or an affected relative might have a different etiology (phenocopy). These complexities require clinical judgment that no computational model can replace.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-rare-disease.html#sec-ch28-somatic",
    "href": "part_6/p6-ch28-rare-disease.html#sec-ch28-somatic",
    "title": "28  Rare Disease Diagnosis",
    "section": "28.4 Somatic Variant Interpretation in Cancer",
    "text": "28.4 Somatic Variant Interpretation in Cancer\nCancer genomics presents distinct interpretive challenges. Tumor genomes accumulate mutations throughout malignant evolution, creating a mix of driver mutations (those conferring selective advantage and contributing to cancer development) and passenger mutations (bystanders with no functional consequence). The interpretive task shifts from identifying variants causing inherited disease to identifying variants driving tumor biology and predicting therapeutic response.\n\n\n\n\n\n\n\n\nGermline vs somatic variant origins\n\n\n\n\n\n\n\nDiagnostic implications of germline vs somatic findings\n\n\n\n\n\n\nFigure 28.4: Distinguishing germline from somatic variants. (A) Origins: germline variants are present from conception in all cells and are heritable; somatic variants arise post-conception in specific tissues. (B) Diagnostic implications: germline findings (e.g., BRCA1) affect surveillance and family testing; somatic findings (e.g., KRAS) guide therapy without family implications. Critical warning: germline findings incidentally discovered in tumor testing require blood confirmation and genetic counseling.\n\n\n\n\n28.4.1 Germline versus Somatic Distinction\nCancer sequencing must distinguish germline variants (present in all cells, inherited or de novo) from somatic variants (acquired in the tumor lineage). Tumor-only sequencing cannot make this distinction reliably, as rare germline variants may be mistaken for somatic events. Paired tumor-normal sequencing, comparing tumor to a non-malignant sample from the same patient, enables confident somatic variant identification.\nThis distinction has direct clinical implications. A germline pathogenic variant in BRCA1 indicates hereditary cancer predisposition affecting the patient and potentially their family members, warranting genetic counseling and possibly risk-reducing interventions. A somatic BRCA1 mutation arose in the tumor and has no implications for inherited risk, though it may still predict response to PARP inhibitors.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA 45-year-old woman with breast cancer has tumor sequencing that identifies a BRCA1 frameshift mutation. What additional test would you recommend, and why does it matter for her family members?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nYou would recommend germline testing on a blood sample to determine whether the BRCA1 mutation is germline (inherited) or somatic (tumor-only). If germline, it indicates hereditary cancer predisposition affecting her family members and warrants genetic counseling and cascade testing. If somatic, it arose only in the tumor and has no implications for inherited risk in relatives.\n\n\n\n\n\n\n\n28.4.2 Driver Classification\nAmong somatic mutations, identifying drivers requires different evidence than germline pathogenicity assessment. Recurrence across independent tumors suggests selective advantage: if BRAF V600E appears in 50% of melanomas, this frequency far exceeds what chance would predict, implying functional importance. The logic here is fundamentally evolutionary: tumors arise through clonal expansion, where cells with growth advantages outcompete their neighbors. A mutation that appears independently in thousands of tumors must confer such an advantage, because random chance alone would distribute mutations nearly uniformly across the ~20,000 protein-coding genes. The probability of the same specific mutation arising repeatedly by chance is vanishingly small; recurrence therefore provides strong statistical evidence of positive selection during tumor evolution. Databases like COSMIC catalog somatic mutation frequencies across cancer types, enabling recurrence-based prioritization (Tate et al. 2019).\nFunctional impact predictions from foundation models apply somewhat differently in the somatic context. A missense variant predicted highly damaging by AlphaMissense in a tumor suppressor gene suggests loss of function consistent with a driver role. The same prediction in an oncogene might indicate loss of normal regulation, potentially activating rather than inactivating the protein. Interpretation must consider the gene’s role (oncogene versus tumor suppressor) and the specific functional consequence of the variant.\n\n\n\nTable 28.3: Germline vs. Somatic Variant Interpretation: Key Differences\n\n\n\n\n\n\n\n\n\n\nAspect\nGermline Interpretation\nSomatic Interpretation\n\n\n\n\nPrimary question\nCauses inherited disease?\nDrives tumor? Predicts therapy response?\n\n\nFramework\nACMG-AMP classification\nRecurrence, functional impact, biomarkers\n\n\nPopulation frequency use\nCommon = likely benign\nNot directly applicable\n\n\nFamily implications\nAffects relatives\nTumor-specific, no inheritance\n\n\nFM role\nFunctional impact on protein\nDriver vs. passenger; therapeutic target\n\n\nClinical action\nGenetic counseling, surveillance\nTreatment selection, prognosis\n\n\n\n\n\n\nTumor mutational burden provides context for individual variant interpretation. Hypermutated tumors (from mismatch repair deficiency or POLE mutations) may carry thousands of coding mutations, making it difficult to identify drivers against this noisy background. In such cases, restricting attention to known hotspots, truncating mutations in tumor suppressors, and variants with strong functional predictions helps prioritize the likely relevant events.\n\n\n28.4.3 Therapeutic Biomarkers\nSomatic variant interpretation increasingly focuses on therapeutic implications. Specific variants predict response to targeted therapies: EGFR exon 19 deletions and L858R mutations predict erlotinib response in lung cancer; BRAF V600E predicts vemurafenib response in melanoma; PIK3CA mutations indicate alpelisib benefit in breast cancer (Lynch et al. 2004; Chapman et al. 2011; André et al. 2019). These associations derive from clinical trials demonstrating differential response by mutation status.\nFoundation models do not directly predict therapeutic response, as they lack the clinical outcome data that would be required. Their contribution is in characterizing novel variants in known therapeutic target genes. A patient whose tumor carries an unusual EGFR mutation not previously characterized might be evaluated using structural models and effect predictions to estimate whether the mutation likely preserves the drug-binding site and confers similar dependency as canonical sensitizing mutations. Such analyses are hypothesis-generating rather than definitive but can inform clinical decision-making when direct trial evidence is unavailable.\n\n\n\n\n\n\nPractical Guidance: When FM Predictions Help with Novel Therapeutic Variants\n\n\n\nFor a novel variant in a known drug target gene, foundation models can help assess:\n\nStructural impact: Does AlphaFold predict the variant alters the drug-binding pocket?\nFunctional consequence: Does AlphaMissense predict the variant disrupts protein function?\nSimilarity to known variants: Is the variant in the same domain as established sensitizing or resistance mutations?\n\nThese assessments support but do not replace clinical judgment. Document the reasoning and discuss uncertainty with the patient.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-rare-disease.html#sec-ch28-validation",
    "href": "part_6/p6-ch28-rare-disease.html#sec-ch28-validation",
    "title": "28  Rare Disease Diagnosis",
    "section": "28.5 Laboratory Validation",
    "text": "28.5 Laboratory Validation\nComputational predictions, however accurate, remain predictions. Functional assays provide direct experimental evidence of variant effects, and ACMG-AMP appropriately weights functional data (PS3 for damaging functional effect, BS3 for no functional effect) as strong evidence when assays are well-validated.\n\n\n\n\n\n\nFunctional validation evidence hierarchy\n\n\n\n\nFigure 28.5: Functional validation evidence hierarchy. Computational predictions (Level 1) from foundation models provide only supporting ACMG evidence. In vitro biochemical assays (Level 2) offer direct functional readout but artificial context. Cellular assays (Level 3: reporters, MPRA) provide cellular context at scale. Organismal models (Level 4) enable in vivo validation. Human natural history studies (Level 5) provide ultimate but observational evidence. Foundation models contribute at Level 1 directly and Levels 2-3 by prioritizing variants for experimental validation.\n\n\n\n\n28.5.1 Types of Functional Assays\nDifferent variant types require different assay approaches. For missense variants, protein function assays measure specific biochemical activities of the mutant protein: enzyme activity, DNA binding, protein-protein interactions, or cellular phenotypes in model systems. Deep mutational scanning systematically characterizes all possible amino acid substitutions at each position in a protein, creating comprehensive functional maps (Section 2.4.4 for data resources; ?sec-ch14-integration-strategies for how foundation models leverage this data). These maps enable immediate lookup of functional effects for any observed missense variant, though coverage remains incomplete across the proteome.\nFor splicing variants, minigene assays clone genomic regions containing the variant into expression vectors and measure splicing patterns in cultured cells. RNA sequencing from patient tissue (when accessible) directly observes whether aberrant splicing occurs in vivo. SpliceAI predictions can be validated by these direct measurements, establishing whether computational predictions match experimental reality for specific variants.\nFor regulatory variants, reporter assays measure whether variant-containing regulatory elements drive appropriate expression patterns. Massively parallel reporter assays (MPRAs) enable testing thousands of variants simultaneously, generating the training data that informs foundation model development while also providing direct validation for specific variants of clinical interest. CRISPR-based approaches can introduce variants into endogenous genomic contexts rather than artificial reporter constructs, providing more physiologically relevant readouts.\n\n\n28.5.2 Integrating Functional Evidence\nFunctional data enters ACMG-AMP classification through PS3 (strong pathogenic evidence from functional studies showing deleterious effect) and BS3 (strong benign evidence from functional studies showing no effect) (Brnich et al. 2019). The strength assignment depends on assay validation: well-established assays measuring physiologically relevant endpoints warrant strong evidence, while novel or less-validated assays may warrant only moderate or supporting strength.\nClinGen has developed detailed recommendations for functional evidence evaluation. The specific gene and disease mechanism should guide assay selection. Controls (known pathogenic and known benign variants) should be included to validate assay performance. The biological relevance of the assay endpoint to the disease mechanism must be justified. These requirements reflect appropriate caution: not all functional assays are equally informative, and inappropriate assays can mislead classification.\nFoundation model predictions can prioritize which variants most warrant functional follow-up. When resources limit testing to a subset of VUS, selecting those with discordant computational predictions (high predicted impact but uncertain clinical classification) maximizes the information gained. Variants where functional testing might resolve classification provide greater value than variants where classification is already clear or unlikely to change regardless of functional results.\n\n\n\n\n\n\nStop and Think\n\n\n\nYour laboratory has resources to functionally test 20 VUS per month. You have 200 VUS in a cardiac arrhythmia gene. How would you prioritize which variants to test? What role might foundation model predictions play in this prioritization?\n\n\n\n\n28.5.3 Closing the VUS Loop\nThe accumulation of variants of uncertain significance represents a major challenge in clinical genetics. Patients receive results that cannot be interpreted, creating anxiety and uncertainty. As more individuals undergo sequencing, VUS prevalence grows. Systematic efforts to resolve VUS through functional characterization could dramatically improve the clinical utility of genetic testing.\nHigh-throughput functional approaches offer a path forward. Saturation genome editing applies CRISPR to introduce every possible single-nucleotide variant at clinically important loci, then measures functional consequences through cellular phenotypes or growth selection (Findlay et al. 2018). These experiments generate comprehensive functional maps that can immediately classify any observed variant. The Brotman Baty Institute’s ongoing efforts for BRCA1, mismatch repair genes, and other clinically important loci exemplify this approach.\nFoundation models trained on these functional datasets can generalize beyond directly measured variants, predicting effects for positions or genes not yet characterized experimentally. This creates a productive cycle: functional data improves model training, improved models identify high-priority variants for follow-up, and targeted experiments fill gaps while further improving models.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-rare-disease.html#sec-ch28-workflow",
    "href": "part_6/p6-ch28-rare-disease.html#sec-ch28-workflow",
    "title": "28  Rare Disease Diagnosis",
    "section": "28.6 Practical Workflow Integration",
    "text": "28.6 Practical Workflow Integration\nTranslating foundation model capabilities into clinical practice requires integration with existing laboratory and clinical workflows. The technical and interpretive steps must fit within established regulatory frameworks, electronic health record systems, and clinical team structures.\n\n28.6.1 Laboratory Workflow\nClinical sequencing laboratories operate under regulatory oversight (CLIA certification, state licensure, and potentially CAP accreditation in the United States). Validated pipelines must produce consistent, reproducible results. Introducing new computational tools requires formal validation demonstrating that the tool performs as expected on representative sample types, that outputs are interpretable and actionable by clinical staff, and that results are documented and traceable.\nFor foundation model integration, validation studies should assess performance on variants with known clinical classifications, compare predictions to existing tools to understand concordance and discordance, evaluate performance across variant types (missense, splice, regulatory) and gene categories, and document threshold selection and evidence strength assignment.\nLaboratory information management systems must capture foundation model predictions alongside other variant annotations. Reports to clinicians should clearly indicate the role of computational evidence, the specific tools applied, and the evidence strength assigned. Overreliance on computational predictions, or failure to communicate their limitations, risks inappropriate clinical decisions.\n\n\n\n\n\n\nPractical Guidance: Documenting FM Predictions in Clinical Reports\n\n\n\nClinical reports should include:\n\nTool identification: Which foundation model(s) were used (name, version)\nScore and threshold: Raw prediction score and classification threshold applied\nEvidence strength: How the prediction maps to ACMG evidence (PP3/BP4 and strength level)\nLimitations: Standard disclaimer about computational evidence\nDiscordance: Note if different tools disagree substantially\n\nExample statement: “AlphaMissense v1.0 pathogenicity score: 0.92 (threshold for PP3 moderate: 0.8). This computational evidence is classified as moderate supporting evidence for pathogenicity per ClinGen recommendations for this gene.”\n\n\n\n\n28.6.2 Clinical Decision-Making\nVariant interpretation reports ultimately inform clinical decisions: whether to pursue additional testing, what genetic counseling to provide, whether to adjust medical management, and what surveillance or prevention strategies to recommend. These decisions rest with clinicians and genetic counselors working with patients, not with computational algorithms.\nFoundation model predictions support this process by improving the efficiency and accuracy of variant prioritization, reducing the number of VUS through more informative computational evidence, identifying potentially actionable variants in previously overlooked genomic regions, and enabling rapid assessment of novel variants not previously observed.\nThe interpretive report should convey both what computational predictions indicate and the uncertainty that remains. Clinicians must understand that even highly accurate models make errors, that predictions may be less reliable for underrepresented populations or unusual variant types, and that computational evidence is one component of a comprehensive assessment. Shared decision-making with patients should acknowledge these limitations while conveying the best current understanding.\n\n\n28.6.3 Regulatory and Ethical Considerations\nClinical use of foundation model predictions raises regulatory questions addressed more fully in Section 26.1. In the United States, laboratory-developed tests using computational predictions fall under CLIA oversight, with additional FDA jurisdiction increasingly asserted for software as a medical device. European regulations under IVDR impose their own requirements. Laboratories must navigate this evolving landscape while ensuring that clinical utility keeps pace with regulatory compliance.\n\n\n\n\n\n\nKey Insight: Equity in Computational Predictions\n\n\n\nFoundation models trained predominantly on European ancestry data may systematically provide less accurate predictions for other populations. This means a patient of African ancestry might receive a VUS classification where a European ancestry patient with the same variant receives a clear pathogenic or benign classification. The computational “evidence gap” can widen health disparities. Laboratories should track ancestry-specific performance metrics and communicate uncertainty appropriately across populations.\n\n\nEquity concerns deserve particular attention. Foundation models trained predominantly on data from individuals of European ancestry may perform less well for other populations (?sec-ch22-ancestry-confounding for detailed discussion of ancestry-related performance disparities; Section 26.5.2 for equity implications). If computational predictions systematically provide less informative evidence for underrepresented groups, this could widen existing disparities in diagnostic yield and clinical care. Ongoing efforts to diversify training data and evaluate performance across ancestries are essential for equitable clinical deployment.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch28-rare-disease.html#sec-ch28-partnership",
    "href": "part_6/p6-ch28-rare-disease.html#sec-ch28-partnership",
    "title": "28  Rare Disease Diagnosis",
    "section": "28.7 Interpretive Partnership",
    "text": "28.7 Interpretive Partnership\nFoundation models transform variant interpretation by providing more accurate, comprehensive, and fine-grained predictions than previous computational approaches. Missense pathogenicity can be estimated proteome-wide with unprecedented accuracy. Regulatory variant effects can be predicted across tissues and cell types. Splicing disruption can be quantified with clinical-grade precision. These capabilities accelerate the diagnostic odyssey, enabling faster and more confident resolution for patients who have often waited years for answers.\nYet foundation models do not replace human judgment in clinical genetics. They do not understand phenotypes, family structures, or therapeutic implications. They do not weigh the psychological impact of uncertain results or navigate the ethical complexities of predictive testing in unaffected relatives. They provide evidence that must be integrated within clinical frameworks designed around human decision-making, alongside family history, physical examination, prior testing, and the accumulated wisdom of clinical experience.\n\n\n\n\n\n\nHuman-AI partnership in variant interpretation\n\n\n\n\nFigure 28.6: Human-AI partnership in variant interpretation. Foundation models excel at pattern recognition across thousands of variants, consistent rule application, and quantitative scoring with uncertainty. Human experts provide clinical context integration, phenotype-genotype reasoning, family communication, and ethical judgment. The productive partnership: FMs reduce 25,000 variants to ~50 prioritized candidates, freeing experts to focus their judgment on cases that matter. This is partnership, not replacement—FMs extend human capacity while humans provide the accountability that clinical decisions require.\n\n\n\nThe productive framing positions foundation models as partners in interpretation: computational systems that handle pattern recognition at scales beyond human capacity, freeing clinical experts to focus on integration, communication, and the decisions where human judgment remains essential. This partnership model, rather than replacement or autonomy, defines the path forward for genomic foundation models in rare disease diagnosis.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nDescribe the variant prioritization funnel from ~25,000 variants to a final diagnosis. At which stage do foundation models contribute most effectively?\nHow do computational predictions map to ACMG-AMP evidence categories? What prevents AlphaMissense scores from providing strong evidence without additional validation?\nWhy does trio sequencing dramatically improve diagnostic yield compared to singleton sequencing? Give two specific examples of evidence that trios enable.\nHow does somatic variant interpretation differ from germline variant interpretation? What different questions does each framework answer?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nVariant Prioritization Funnel: The funnel applies progressive filters: quality/technical filters remove sequencing artifacts, population frequency filters remove common variants unlikely to cause rare disease, consequence filters prioritize coding and functional variants, and foundation model scoring ranks remaining candidates by predicted effect. Foundation models contribute most effectively at the scoring stage (~50 candidates remaining), after basic filters eliminate obvious non-candidates but before expensive expert curation. This positioning maximizes efficiency while minimizing the risk of discarding true causal variants.\nACMG-AMP Computational Evidence: Computational predictions enter through PP3 (pathogenic support) and BP4 (benign support) criteria, traditionally assigned only “supporting” strength. AlphaMissense scores provide functional impact predictions but cannot achieve strong evidence without gene-specific calibration because: (1) functional disruption does not equal clinical pathogenicity, (2) accuracy varies across genes and variant types, and (3) strong evidence requires odds ratios of ~18:1, which must be empirically demonstrated through validation against clinical classifications in specific gene contexts.\nTrio Sequencing Power: Trio sequencing (proband plus both parents) dramatically improves diagnostic yield by enabling de novo variant identification and accurate phasing. First, de novo variants observed in the child but absent in both parents receive strong pathogenic evidence (ACMG PS2), particularly valuable for severe early-onset dominant conditions where affected individuals rarely reproduce. Second, trios enable direct phasing of compound heterozygous variants, determining whether two variants in the same gene are in trans (disrupting both copies, consistent with recessive disease) or in cis (one functional copy remains, excluding recessive causation).\nGermline vs. Somatic Interpretation: Germline interpretation asks “Does this variant cause inherited disease?” and applies ACMG-AMP criteria incorporating population frequency, family segregation, and inheritance patterns, with implications for genetic counseling and family testing. Somatic interpretation asks “Is this variant a tumor driver? Does it predict therapy response?” and relies on recurrence across tumors, functional impact in the context of oncogenes vs. tumor suppressors, and associations with targeted therapies. The same variant (e.g., BRCA1 mutation) has completely different implications depending on whether it is germline (hereditary cancer risk affecting family) or somatic (tumor-specific, potential therapy target, no family implications).\n\n\n\n\n\n\nThis chapter examined how foundation models integrate into clinical variant interpretation for rare disease diagnosis.\nKey Takeaways:\n\nThe Prioritization Funnel: Clinical interpretation progressively filters ~25,000 variants to ~5-10 candidates through quality, frequency, consequence, and FM-based scoring stages. Foundation models contribute most at the scoring stage, after basic filters remove obvious non-candidates.\nACMG-AMP Integration: Computational predictions enter the framework through PP3/BP4 criteria, traditionally at supporting strength. Well-calibrated foundation models may warrant upgraded evidence strength for specific genes, but this requires formal validation and gene-specific calibration.\nFamily Analysis Power: Trio sequencing, phasing, and segregation analysis provide orthogonal evidence that dramatically enhances interpretation. De novo status (PS2) provides strong evidence; compound heterozygosity determination requires accurate phasing.\nGermline vs. Somatic Context: The same variant interpretation framework does not apply to both contexts. Germline interpretation asks “Does this cause inherited disease?”; somatic interpretation asks “Is this a driver? Does it predict therapy response?”\nThe VUS Challenge: Most variants remain VUS due to insufficient evidence. High-throughput functional assays and improved computational predictions work together to close this gap.\nHuman-AI Partnership: Foundation models accelerate prioritization and provide quantitative evidence, but clinical judgment remains essential for final classification, communication, and ethical decision-making.\n\nConnections to Other Chapters:\n\nVariant effect prediction models: Chapter 17\nCalibration and uncertainty: Section 23.2\nRegulatory and ethical frameworks: Section 26.1\nAncestry and fairness considerations: ?sec-ch22-ancestry-confounding\n\n\n\n\n\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F. Scott, and Ada Hamosh. 2015. “OMIM.org: Online Mendelian Inheritance in Man (OMIM®), an Online Catalog of Human Genes and Genetic Disorders.” Nucleic Acids Research 43 (D1): D789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAndré, Fabrice, Eva Ciruelos, Gabor Rubovszky, Mario Campone, Sibylle Loibl, Hope S. Rugo, Hiroji Iwata, et al. 2019. “Alpelisib for PIK3CA-Mutated, Hormone Receptor–Positive Advanced Breast Cancer.” New England Journal of Medicine 380 (20): 1929–40. https://doi.org/10.1056/NEJMoa1813904.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nBergquist, Timothy, Sarah L. Stenton, Emily A. W. Nadeau, Alicia B. Byrne, Marc S. Greenblatt, Steven M. Harrison, Sean V. Tavtigian, et al. 2025. “Calibration of Additional Computational Tools Expands ClinGen Recommendation Options for Variant Classification with PP3/BP4 Criteria.” Genetics in Medicine 27 (6): 101402. https://doi.org/10.1016/j.gim.2025.101402.\n\n\nBrnich, Sarah E., Ahmad N. Abou Tayoun, Fergus J. Couch, Garry R. Cutting, Marc S. Greenblatt, Christopher D. Heinen, Dona M. Kanavy, et al. 2019. “Recommendations for Application of the Functional Evidence PS3/BS3 Criterion Using the ACMG/AMP Sequence Variant Interpretation Framework.” Genome Medicine 12 (1): 3. https://doi.org/10.1186/s13073-019-0690-2.\n\n\nChapman, Paul B., Axel Hauschild, Caroline Robert, John B. Haanen, Paolo Ascierto, James Larkin, Reinhard Dummer, et al. 2011. “Improved Survival with Vemurafenib in Melanoma with BRAF V600E Mutation.” New England Journal of Medicine 364 (26): 2507–16. https://doi.org/10.1056/NEJMoa1103782.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nFindlay, Gregory M., Riza M. Daza, Beth Martin, Melissa D. Zhang, Anh P. Leith, Molly Gasperini, Joseph D. Janizek, Xingfan Huang, Lea M. Starita, and Jay Shendure. 2018. “Accurate Classification of BRCA1 Variants with Saturation Genome Editing.” Nature 562 (7726): 217–22. https://doi.org/10.1038/s41586-018-0461-z.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B. Cummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020. “The Mutational Constraint Spectrum Quantified from Variation in 141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKong, Augustine, Michael L. Frigge, Gisli Masson, Soren Besenbacher, Patrick Sulem, Gisli Magnusson, Sigurjon A. Gudjonsson, et al. 2012. “Rate of de Novo Mutations and the Importance of Father’s Age to Disease Risk.” Nature 488 (7412): 471–75. https://doi.org/10.1038/nature11396.\n\n\nLynch, Thomas J., Daphne W. Bell, Raffaella Sordella, Sarada Gurubhagavatula, Ross A. Okimoto, Brian W. Brannigan, Patricia L. Harris, et al. 2004. “Activating Mutations in the Epidermal Growth Factor Receptor Underlying Responsiveness of Non–Small-Cell Lung Cancer to Gefitinib.” New England Journal of Medicine 350 (21): 2129–39. https://doi.org/10.1056/NEJMoa040938.\n\n\nNguengang Wakap, Stéphanie, Deborah M. Lambert, Annie Olry, Charlotte Rodwell, Charlotte Gueydan, Valérie Lanneau, Daniel Murphy, Yann Le Cam, and Ana Rath. 2020. “Estimating Cumulative Point Prevalence of Rare Diseases: Analysis of the Orphanet Database.” European Journal of Human Genetics 28 (2): 165–73. https://doi.org/10.1038/s41431-019-0508-0.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel, Sean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022. “Calibration of Computational Tools for Missense Variant Pathogenicity Classification and ClinGen Recommendations for PP3/BP4 Criteria.” American Journal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie Gastier-Foster, Wayne W. Grody, et al. 2015. “Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology.” Genetics in Medicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nTate, John G, Sally Bamford, Harry C Jubb, Zbyslaw Sondka, David M Beare, Nidhi Bindal, Harry Boutselakis, et al. 2019. “COSMIC: The Catalogue Of Somatic Mutations In Cancer.” Nucleic Acids Research 47 (D1): D941–47. https://doi.org/10.1093/nar/gky1015.\n\n\nTavtigian, Sean V., Marc S. Greenblatt, Steven M. Harrison, Robert L. Nussbaum, Snehit A. Prabhu, Kenneth M. Boucher, and Leslie G. Biesecker. 2018. “Modeling the ACMG/AMP Variant Classification Guidelines as a Bayesian Classification Framework.” Genetics in Medicine 20 (9): 1054–60. https://doi.org/10.1038/gim.2017.210.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Rare Disease Diagnosis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html",
    "href": "part_6/p6-ch29-drug-discovery.html",
    "title": "29  Drug Discovery",
    "section": "",
    "text": "29.1 Genetic Foundation of Target Selection\nHuman genetics provides uniquely causal evidence for target selection. Unlike expression correlations or pathway membership, genetic associations reflect the consequences of lifelong modulation of gene activity in human populations. Multiple analyses over the past decade have demonstrated that genetically supported targets succeed in clinical trials at roughly twice the rate of targets without genetic evidence [Citation Needed]. Targets implicated by Mendelian disease genetics, genome-wide association study (GWAS) hits, or functional variants show higher probabilities of success in phase II and III trials compared to targets selected through other means.\nThis empirical observation motivates building pipelines where genetic architecture serves as a first-class input to target identification. Genomic foundation models extend this logic by providing richer biological context and enabling transfer learning across diseases and modalities. Rather than simple “variants near gene X,” foundation models encode regulatory architecture, chromatin state, three-dimensional genome interactions, cell-type specificity, and perturbation responses. A single model trained on diverse genomic and multi-omic data can be reused for multiple diseases and therapeutic areas, analogous to how language models transfer across domains.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-genetic-foundation",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-genetic-foundation",
    "title": "29  Drug Discovery",
    "section": "",
    "text": "Drug target prioritization pipeline\n\n\n\n\nFigure 29.1: Drug target prioritization pipeline. Starting from disease understanding (GWAS, expression signatures), targets progress through causal inference (Mendelian randomization confirms causation), target property assessment (druggability, safety), and experimental validation (CRISPR, animal models). Foundation models contribute at each stage: integrating multi-omic associations, prioritizing causal variants, predicting druggability, and interpreting validation data. Critical context: only ~5% of targets entering clinical development succeed.\n\n\n\n\n\n29.1.1 From Variant-Level Predictions to Gene-Level Evidence\nDrug discovery teams rarely care about individual variants per se; they care about genes and pathways. The fundamental challenge in target identification is therefore aggregating variant-level information into gene-level evidence that can guide target selection.\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading about the workflow below, consider: if you have thousands of GWAS-significant variants across hundreds of loci, what information would you need to convert these statistical signals into a ranked list of drug targets? What makes this problem difficult?\n\n\nConsider a typical workflow. Starting from GWAS summary statistics (see Section 2.3.2), statistical fine-mapping methods identify credible sets of potentially causal variants at each locus (see Section 3.4). Sequence-based foundation models then score each candidate variant for regulatory or coding impact. Protein-centric variant effect predictors such as AlphaMissense, GPN-MSA, and the missense components of AlphaGenome combine protein language models, structural information, and evolutionary conservation to assess coding variants (Section 15.1; ?sec-ch14-protein-vep) (Cheng et al. 2023; Benegas et al. 2024; Avsec, Latysheva, and Cheng 2025; Brandes et al. 2023). Regulatory foundation models including Enformer, Borzoi, and long-context DNA language models predict the consequences of noncoding variants on chromatin accessibility, transcription factor binding, and gene expression (Section 16.2; ?sec-ch14-enformer-vep).\nThe critical step is connecting variants to genes. For coding variants, this mapping is straightforward: the variant lies within a gene’s coding sequence, and protein-level scores directly inform that gene’s candidacy. For noncoding variants, the mapping requires integrating chromatin conformation data (Hi-C, promoter-capture Hi-C), enhancer-gene predictions from models like Enformer, and expression quantitative trait locus (eQTL) data that empirically links variants to gene expression changes (see Section 2.5). Fine-mapping approaches such as MIFM can help distinguish truly causal regulatory variants from correlated passengers, tightening the map from GWAS locus to variant to target gene (Section 3.4) (Wu et al. 2024; Rakowski and Lippert 2025).\nThe following table summarizes the key evidence types for connecting variants to target genes:\n\n\n\nTable 29.1: Evidence types for variant-to-gene mapping in drug target prioritization. Coding variants provide direct gene mappings, while noncoding variants require integration of multiple data sources.\n\n\n\n\n\n\n\n\n\n\n\n\nEvidence Type\nVariant Class\nData Source\nStrength\nLimitation\n\n\n\n\nDirect coding\nMissense, LoF\nSequence annotation\nUnambiguous gene link\nRare variants, small effect sizes\n\n\neQTL colocalization\nRegulatory\nGTEx, tissue-specific cohorts\nEmpirical gene expression link\nTissue-specific, LD confounding\n\n\nChromatin contact\nRegulatory\nHi-C, pcHi-C\nPhysical enhancer-gene link\nResolution limits, cell-type specific\n\n\nFM enhancer prediction\nRegulatory\nEnformer, Borzoi\nFunctional prediction\nValidation required\n\n\nNetwork proximity\nBoth\nPPI, pathway databases\nPathway context\nIndirect evidence\n\n\n\n\n\n\nGene-level aggregation proceeds by summarizing variant effects across all variants linked to each gene. For a given gene, this summary might include the burden of predicted loss-of-function variants in cases versus controls, the strongest regulatory variant effect sizes predicted by foundation models, constraint metrics indicating the gene’s intolerance to damaging variation (see Section 2.2.3), and pleiotropy scores reflecting associations with other traits that might indicate safety liabilities or broader biological importance. From a foundation model perspective, the core idea is to treat gene-level evidence as an aggregation problem over high-dimensional variant embeddings. Rather than manually defining a handful of summary statistics, variant embeddings and predicted functional profiles can feed into downstream models that learn which patterns matter most for disease.\n\n\n29.1.2 Linking Genetics to Target Safety and Efficacy\nClassical human genetics has established several heuristics for target selection that foundation models can reinforce and extend. Human knockout individuals, people carrying biallelic loss-of-function variants, provide natural experiments on the consequences of gene inactivation. Protective variants that reduce disease risk suggest the directionality of therapeutic intervention: partial inhibition of a protein may be beneficial rather than harmful. Pleiotropy, meaning associations with many unrelated traits, may signal safety liabilities if modulating a target affects multiple physiological systems (see Section 3.8).\nFoundation models sharpen these assessments. Fine-mapping methods combined with regulatory foundation models can distinguish causal variants from those merely in linkage disequilibrium with causal variants (see Section 3.3). Variant effect scores from protein and regulatory models approximate effect sizes, helping differentiate subtle modulators from catastrophic loss-of-function mutations. Multi-task predictions across chromatin marks, transcription factor binding, expression, and splicing provide mechanistic hypotheses for how risk loci affect biology, moving beyond statistical association toward functional understanding.\nThe output of this workflow is a ranked list of candidate targets with structured evidence that can be compared across diseases and programs. Each target comes annotated with the strength of genetic evidence (effect sizes, fine-mapping probabilities), predicted mechanisms (coding versus regulatory, affected tissues), constraint information (tolerance to loss-of-function, essentiality), and druggability features (protein family, structural information, existing ligands).\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA target gene has strong GWAS support (p &lt; 5e-8) but high pleiotropy (associated with 50+ traits in PheWAS) and high constraint (pLI &gt; 0.99). Should this gene be prioritized or deprioritized as a drug target? What additional information would help decide?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThis gene should be approached cautiously and requires deeper investigation. High constraint suggests the gene is essential for viability, indicating potential toxicity from inhibition. High pleiotropy suggests modulating it affects many systems, raising concerns about on-target side effects. Additional information needed includes: which specific traits are associated (are they safety-relevant?), human knockout data from biobanks, tissue expression patterns, and whether the GWAS associations suggest the direction of effect aligns with therapeutic goals.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-network-discovery",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-network-discovery",
    "title": "29  Drug Discovery",
    "section": "29.2 Network-Aware Target Discovery and Repurposing",
    "text": "29.2 Network-Aware Target Discovery and Repurposing\nIndividual genes do not operate in isolation. Proteins interact in complexes, genes participate in pathways, and regulatory networks coordinate cellular responses. Even with excellent variant-to-gene mapping, the biological context of a target shapes its therapeutic potential. Network-aware approaches propagate genetic signals through these relational structures to identify modules, bottleneck nodes, and repurposing opportunities.\n\n29.2.1 Propagating Genetic Signals Through Networks\nThe basic intuition is that GWAS signals concentrated in a pathway or protein interaction module provide stronger evidence than isolated hits. A single gene with modest genetic support but tight functional connections to several strongly implicated genes may be a more attractive target than an isolated hit with stronger statistics but unclear biology.\nNetwork-based methods integrate noncoding GWAS loci, regulatory annotations, and protein-protein interactomes to identify disease genes and evaluate drug repurposing opportunities in complex diseases. Graph neural network architectures (?sec-ch18-canonical-architectures) can learn to propagate genetic evidence through interaction networks, scoring each gene not just by its direct genetic association but by its network context. The key methodological insight is that genes can be embedded jointly with their network neighbors, allowing the model to capture how genetic perturbations in one gene might affect functionally related genes. This propagation reflects biological reality: proteins function in complexes and pathways, so disrupting one component often affects the entire module. A mutation in a kinase may have minimal direct phenotypic effect if compensatory kinases exist, but that same mutation becomes consequential if the compensatory kinases are also compromised elsewhere in the pathway. Network propagation captures these dependencies that single-gene analysis misses.\n\n\n\n\n\n\nKey Insight: Single-Gene vs. Network Evidence\n\n\n\nSingle-gene approaches treat each target independently based on its direct genetic association. Network-aware approaches recognize that an isolated GWAS hit with unclear mechanism may be less actionable than a gene with modest direct evidence but strong connections to well-validated disease biology. The value of network propagation lies in recovering “near-miss” targets that lack genome-wide significance individually but sit at the center of disease-relevant pathways.\n\n\nFoundation model representations enhance these network approaches. Instead of representing each gene by a sparse vector of annotations, genes can be embedded using features derived from protein language models (Section 15.1.3), regulatory foundation models (Section 16.2), and expression-based cell state encoders (?sec-ch16-clm). These embeddings can then be used as node features in graph neural networks, enabling network-aware target prioritization (?sec-ch18-fm-embeddings). These rich representations capture functional similarity beyond what interaction databases alone can provide. Two genes with similar protein language model embeddings likely share functional properties even if no direct interaction has been catalogued.\n\n\n\n\n\n\nNetwork-based drug target discovery\n\n\n\n\nFigure 29.2: Network-based drug target discovery. Known disease genes (red) seed network propagation through protein-protein interactions. Candidate targets (orange) accumulate high scores based on network proximity. Foundation model embeddings (ESM) provide node features that encode functional similarity, improving propagation beyond topology alone. Network context reveals targets missed by GWAS: genes may not harbor risk variants themselves but are functionally connected to disease mechanisms.\n\n\n\n\n\n29.2.2 Drug Repurposing Through Shared Representations\nThe same framework enables systematic drug repurposing. By representing drugs via their targets, gene expression signatures, and phenotypic effects, and representing diseases via their genetic architecture and molecular signatures, models can score drug-disease pairs based on representation similarity. If a drug’s target sits near genetically implicated genes in representation space, or if the drug’s expression signature opposes the disease signature, that drug becomes a repurposing candidate.\nNetwork proximity provides one concrete operationalization: drugs whose targets are enriched near disease-risk genes, as measured by network diffusion or embedding similarity, may have therapeutic potential for that disease. Several retrospective analyses have found that such proximity predicts reduced disease incidence among users of particular drugs, though prospective validation remains limited.\n\n\n\n\n\n\nCase Study: Metformin and Network-Based Repurposing\n\n\n\nMetformin, a first-line type 2 diabetes treatment, illustrates network-based repurposing in action. Network analyses identified that metformin’s targets (primarily mitochondrial complex I and AMPK) have high proximity to genes implicated in cancer susceptibility, including STK11/LKB1 and TP53-related pathways.\nRetrospective epidemiological analyses across multiple cohorts found that diabetic patients taking metformin had 10-40% reduced cancer incidence compared to patients on other diabetes medications, with particularly strong signals for colorectal and liver cancer. The network proximity score (mean shortest path from metformin targets to cancer genes: 1.8 hops) was in the top 5% of all approved drugs.\nThis hypothesis-generating finding led to prospective clinical trials testing metformin in cancer prevention and treatment. Results have been mixed: some trials show benefit in specific populations, others show no effect. The example illustrates both the promise of network-based repurposing (generating testable hypotheses from computational analysis) and its limitations (retrospective associations do not guarantee prospective efficacy).\n\n\n\n\n\n\n\n\nCorrelation vs. Causation in Repurposing\n\n\n\nThe caution here is fundamental: representation similarity is not causation. A drug that appears near disease genes in embedding space might act through that mechanism, or the association might reflect confounding by indication, survivorship bias, or other artifacts of observational data (see Chapter 12). Network-based repurposing generates hypotheses; Mendelian randomization, natural experiments, and prospective trials must test them. Retrospective analyses claiming repurposing success should be viewed skeptically without prospective validation.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-dti-prediction",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-dti-prediction",
    "title": "29  Drug Discovery",
    "section": "29.3 Drug-Target Interaction Prediction",
    "text": "29.3 Drug-Target Interaction Prediction\nBeyond identifying disease-relevant targets, foundation models can predict which molecules might modulate those targets. Drug-target interaction prediction sits at the interface between genomic and chemical foundation models, using biological representations to inform molecular design decisions.\n\n\n\n\n\n\nDrug-target interaction prediction with foundation models\n\n\n\n\nFigure 29.3: Drug-target interaction prediction with foundation models. Drug structures are encoded by chemical language models (ChemBERTa) while protein targets are encoded by protein foundation models (ESM-2). Combined embeddings predict binding probability and affinity. Applications span virtual screening (score targets for new drugs), drug repurposing (score known drugs for new targets), and off-target prediction (identify safety liabilities). Experimental validation remains essential: computational predictions prioritize but cannot substitute for binding assays.\n\n\n\n\n29.3.1 Representing Targets for Binding Prediction\nTraditional drug-target interaction methods rely on sequence similarity, structural docking, or chemical fingerprint matching. Foundation model approaches replace these hand-crafted features with learned representations. Protein language model embeddings from ESM-2 or similar architectures capture evolutionary and structural information that correlates with binding site properties (Section 15.1.3; Section 15.1) (Lin et al. 2022). Ligand representations from chemical foundation models encode molecular properties relevant to binding affinity and selectivity.\n\n\n\n\n\n\nStop and Think\n\n\n\nTraditional drug-target interaction prediction requires experimental protein structures for docking. Why might protein language model embeddings enable binding prediction for targets without solved structures? What information do these embeddings capture that might correlate with binding behavior?\nThe key insight: Protein language model embeddings encode evolutionary constraints on each position—which amino acids are tolerated, which co-evolve with other positions, which are absolutely conserved. These constraints reflect structural and functional requirements: a position buried in a hydrophobic core shows different evolutionary patterns than a solvent-exposed loop, and active site residues show patterns distinct from scaffolding regions. Binding sites are evolutionarily constrained because mutations that disrupt binding are selected against. Even without explicit structure, embeddings capture the statistical signatures of binding site residues: unusual conservation patterns, co-evolutionary relationships with other binding site positions, and amino acid preferences reflecting the chemical environment. Two proteins with similar binding site embeddings likely share binding site properties—enabling affinity prediction from sequence alone.\n\n\nThe prediction task becomes: given a protein embedding (derived from a protein language model) and a molecule embedding (derived from a chemical language model or graph neural network), predict binding affinity or interaction probability. These models can be trained on large databases of known drug-target interactions and binding affinities, then applied to predict interactions for novel targets or molecules.\n\n\n\n\n\n\nWorked Example: DTI Prediction for a Novel Kinase Target\n\n\n\nA drug discovery team identifies MAP4K4 as a target for metabolic disease based on GWAS and human knockout data. The team wants to find starting compounds before solving the crystal structure.\nStep 1: Generate target embedding. Extract ESM-2 embeddings for the MAP4K4 kinase domain (residues 1-320), producing a 1280-dimensional vector that encodes evolutionary constraints, predicted structural features, and sequence context.\nStep 2: Screen compound library. For each compound in a 100,000-compound kinase-focused library, compute ChemBERTa embeddings (768-dimensional molecular representations).\nStep 3: Predict binding. A DTI prediction model trained on ChEMBL binding data takes concatenated [protein; compound] embeddings and predicts IC50. For MAP4K4 against 100,000 compounds, this takes ~2 hours on a single GPU.\nStep 4: Prioritize hits. Top 100 predicted binders (predicted IC50 &lt; 100 nM) are selected for biochemical testing.\nResults: Of 100 computationally predicted binders, 23 showed confirmed binding (IC50 &lt; 1 μM) in biochemical assays—a 23% hit rate compared to ~1% for random screening. The top hit had IC50 = 47 nM, providing a validated starting point for medicinal chemistry without requiring structural data.\nThis example illustrates how protein foundation model embeddings enable structure-free virtual screening, dramatically reducing the compounds requiring experimental testing.\n\n\nFor genomics-focused applications, the protein representation is the critical contribution. A target identified through genetic validation can be immediately embedded using protein foundation models, enabling binding prediction without waiting for experimental structures or extensive biochemical characterization. This acceleration is particularly valuable for understudied targets where structural data is sparse.\n\n\n29.3.2 Selectivity and Off-Target Prediction\nThe same framework extends to selectivity prediction. By comparing a drug’s predicted binding across all proteins in a proteome-scale embedding space, models can flag potential off-target interactions. A compound predicted to bind its intended target but also showing high affinity for kinases with cardiovascular expression, for example, might warrant additional safety characterization before advancement.\nFoundation model representations capture protein family relationships and binding site similarities that inform off-target predictions. Two proteins with similar embeddings likely share structural features that could bind similar molecules. This information, combined with tissue expression data (Section 2.5.1) and phenome-wide association data linking genes to thousands of traits (Section 3.8.2), enables preliminary safety profiling before expensive preclinical experiments.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-toxicity",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-toxicity",
    "title": "29  Drug Discovery",
    "section": "29.4 Toxicity Prediction from Genomic Context",
    "text": "29.4 Toxicity Prediction from Genomic Context\nSafety failures represent a major cause of drug attrition, particularly in late-stage development where failures are most expensive. Genomic information provides several routes to earlier toxicity prediction.\n\n29.4.1 Genetic Evidence of Target Liabilities\nHuman genetic data offers direct evidence of target-related toxicity. If loss-of-function variants in a target gene associate with adverse phenotypes in biobank populations, those phenotypes may emerge as on-target toxicities during therapeutic inhibition. Phenome-wide association studies across biobanks link genes to thousands of traits, from laboratory values to disease diagnoses to imaging features (see Section 3.8). A target strongly associated with QT prolongation, hepatotoxicity markers, or nephrotoxicity phenotypes warrants careful safety evaluation.\nFoundation models enhance this analysis by providing more accurate variant effect predictions (distinguishing true loss-of-function from benign variants) and by integrating across evidence types (see Chapter 17). A gene might show modest individual associations with several safety-relevant traits that, when aggregated using foundation model representations, reveal a concerning pattern.\n\n\n\n\n\n\nPractical Guidance: Genetic Safety Signals\n\n\n\nWhen evaluating genetic safety evidence for a target:\n\nCheck PheWAS associations for the target gene, focusing on cardiac, hepatic, renal, and hematologic phenotypes\nExamine human knockouts in gnomAD and biobank cohorts for homozygous loss-of-function carriers\nAssess constraint (pLI, LOEUF) as a proxy for essential function that may indicate safety risk\nEvaluate pleiotropy breadth and whether off-target associations could become on-target toxicities\nConsider expression patterns in safety-relevant tissues using GTEx or single-cell atlases\n\nRemember: genetic evidence predicts on-target toxicity from the mechanism itself. Off-target toxicity requires separate assessment through binding prediction and selectivity profiling.\n\n\n\n\n29.4.2 Expression-Based Toxicity Prediction\nTissue expression patterns inform toxicity risk. A target expressed highly in hepatocytes poses greater hepatotoxicity risk than one expressed primarily in the target tissue. Single-cell foundation models (?sec-ch18-cell-annotation; ?sec-ch19-imputation) provide cell-type-resolved expression information, enabling predictions about which cell types might be affected by target modulation.\nMore sophisticated approaches use perturbation-response models trained on CRISPR screens and drug treatment data. Given a target knockdown or drug treatment, these models predict transcriptomic responses across cell types. If the predicted response signature resembles known toxicity signatures (mitochondrial stress, DNA damage response, inflammatory activation), that prediction informs safety risk assessment.\n\n\n\n\n\n\nWorked Example: Predicting Hepatotoxicity from Perturbation Signatures\n\n\n\nA candidate target, RIPK1, shows strong genetic support for inflammatory disease. Before advancing to animal studies, the team uses perturbation-response models to assess hepatotoxicity risk.\nStep 1: Predict transcriptomic response. Using a model trained on the Connectivity Map and CRISPR perturbation databases, predict the gene expression changes in hepatocytes following RIPK1 inhibition.\nStep 2: Compare to toxicity signatures. The predicted response shows: - 78% similarity to the “mitochondrial stress” signature (genes like DDIT3, ATF4, HSPA5 upregulated) - 45% similarity to the “apoptosis induction” signature - 12% similarity to the “DNA damage response” signature\nStep 3: Contextualize with genetic data. PheWAS analysis of RIPK1 loss-of-function variant carriers shows elevated liver enzyme levels (ALT, AST) compared to non-carriers (OR = 1.4, p = 0.02).\nInterpretation: The convergent evidence—perturbation signature similarity to mitochondrial stress + genetic association with liver enzyme elevation—suggests hepatotoxicity risk. The team designs the lead optimization campaign to monitor hepatocyte viability assays and prioritizes compounds with improved selectivity over related kinases expressed in liver.\nThis predictive safety signal emerged computationally before any animal testing, enabling risk-aware medicinal chemistry from the program’s outset.\n\n\n\n\n29.4.3 Integrating Genomic Context with Chemical Properties\nUltimate toxicity prediction requires integrating target information with compound properties. The same molecule might be safe or toxic depending on its selectivity profile, metabolism, and tissue distribution. Foundation models provide the biological context (target properties, off-target predictions, expression patterns) that complements chemical property predictions (metabolism, reactivity, distribution) in integrated toxicity models.\nThe field remains early: prospective validation of foundation model toxicity predictions against clinical outcomes is limited. Current utility lies in prioritizing compounds for experimental toxicity testing and in generating hypotheses about liability mechanisms, rather than replacing traditional safety pharmacology.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-functional-screens",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-functional-screens",
    "title": "29  Drug Discovery",
    "section": "29.5 Functional Genomics Screens and Perturbation Models",
    "text": "29.5 Functional Genomics Screens and Perturbation Models\nWhile human genetics offers observational evidence, drug discovery relies heavily on perturbation experiments that directly test hypotheses. CRISPR knockout and knockdown screens, saturation mutagenesis of protein domains, massively parallel reporter assays (MPRAs) of regulatory elements, and Perturb-seq experiments linking genetic perturbations to single-cell transcriptomic responses all generate data that both validates targets and improves models.\n\n\n\n\n\n\nPerturb-seq for mechanism discovery\n\n\n\n\nFigure 29.4: Perturb-seq for mechanism discovery. Pooled CRISPR libraries target hundreds to thousands of genes; single-cell capture identifies both the perturbation (guide barcode) and its effect (expression profile). The resulting matrix directly measures gene-gene regulatory relationships—interventional data that foundation models cannot learn from observation alone. FM integration: predict perturbation effects for virtual screening, interpret expression signatures for mechanism discovery, and identify informative perturbations for active learning.\n\n\n\n\n29.5.1 Designing Informative Perturbation Libraries\nTraditional pooled screens use simple design rules: one guide RNA per exon, or tiling a regulatory region at fixed spacing. Foundation models enable smarter library design by providing priors over which perturbations are likely to be informative.\nVariant effect scores from protein foundation models can prioritize which amino acid positions are most likely to reveal functional differences when mutated. Positions predicted to be highly constrained and structurally important warrant more thorough coverage than positions predicted to be mutationally tolerant (?sec-ch14-protein-vep). Regulatory foundation models can highlight which enhancer or promoter regions are predicted to have the largest expression effects in the cell type of interest, focusing screening effort on high-impact regions (Section 16.2; ?sec-ch14-enformer-vep).\nBeyond prioritization, foundation models can guide combinatorial design. Model uncertainty, the degree to which a model is confident in its predictions, identifies regions where experimental data would be most informative (Section 23.1.2; Section 23.7). Positions where the model makes uncertain predictions are precisely those where experimental measurement adds the most value. Active learning strategies that select perturbations to maximize expected information gain can dramatically improve the efficiency of screening campaigns. The principle underlying active learning is that not all experiments are equally informative: testing a variant where the model already has high confidence provides little new knowledge, while testing uncertain predictions resolves ambiguity and improves future predictions across related sequences. By iteratively selecting the most informative experiments, active learning can achieve the same model accuracy with 10-100x fewer measurements than random sampling, a critical advantage when each perturbation experiment costs significant time and resources.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA foundation model predicts that positions 45-60 in a protein domain are highly constrained, while positions 120-135 have high uncertainty in their functional predictions. Which region should receive more variants in a saturation mutagenesis library, and why?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe high-uncertainty region (positions 120-135) should receive more variants. Active learning principles suggest testing where the model is uncertain provides the most information to improve predictions. The highly constrained region (45-60) will likely show most mutations are deleterious, which confirms existing predictions but teaches us little new. The uncertain region may reveal unexpected functional tolerance or sensitivity that refines the model for future predictions.\n\n\n\n\n\n\n\n29.5.2 Perturb-seq and Transcriptomic Readouts\nPerturb-seq experiments combine pooled genetic screens with single-cell RNA sequencing, linking each perturbation to its transcriptomic consequences. These data are exceptionally rich: rather than a single phenotypic readout (viability, fluorescence), each cell provides a high-dimensional expression profile reflecting how the perturbation affected cellular state.\nFoundation models trained on Perturb-seq data learn to predict transcriptomic responses to genetic perturbations (?sec-ch16-perturbation for architectural details). Given a gene knockdown, these models predict which other genes will be up- or down-regulated, providing a functional signature for each target. Similar signatures suggest similar biology; divergent signatures suggest distinct mechanisms even for targets in the same pathway.\nThe drug discovery application is perturbation matching. Given a disease state characterized by a transcriptomic signature (perhaps derived from patient samples or disease models), foundation models can identify perturbations whose predicted response signature would move the system toward a healthier state. If knocking down gene X reverses the disease signature, X becomes a candidate therapeutic target. If treating with drug Y produces a signature similar to knocking down gene X, Y becomes a candidate molecule for that mechanism [Citation Needed].\n\n\n29.5.3 Closing the Loop: Lab-in-the-Loop Refinement\n\n\n\n\n\n\nAdvanced Topic\n\n\n\nThe following section covers active learning and iterative experimental design. These concepts require familiarity with uncertainty quantification (Section 23.1.2) and transfer learning (?sec-ch09-full-finetuning). Readers may wish to review these topics before proceeding.\n\n\nIterative refinement represents a particularly valuable application of foundation models in functional genomics. Screen outcomes provide labeled examples that can fine-tune sequence-to-function models for the specific biological context of interest (?sec-ch09-full-finetuning for transfer learning strategies).\nConsider an MPRA that assays thousands of enhancer variants for their effects on reporter gene expression in a disease-relevant cell type. These sequence-activity pairs directly supervise expression-prediction foundation models, dramatically improving their accuracy for that locus and tissue. The refined model then makes better predictions for the next round of experiments, suggesting which additional variants would be most informative to test.\n\n\n\n\n\n\nKey Insight: The Bayesian Interpretation of Lab-in-the-Loop\n\n\n\nThis lab-in-the-loop cycle follows a Bayesian logic: foundation models provide the prior (general knowledge about sequence-function relationships learned from millions of sequences); experiments provide the likelihood (specific measurements in the system of interest); and the posterior (fine-tuned model) makes better predictions by combining both. The more informative the experiments (high uncertainty positions), the more the posterior improves over the prior.\n\n\nThis lab-in-the-loop cycle accelerates discovery while improving model accuracy in disease-relevant regions of sequence space. Foundation models provide the prior (general knowledge about sequence-function relationships); experiments provide the likelihood (specific measurements in the system of interest); and the posterior (updated model) makes better predictions for subsequent experiments.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-biomarkers",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-biomarkers",
    "title": "29  Drug Discovery",
    "section": "29.6 Biomarker Development and Patient Stratification",
    "text": "29.6 Biomarker Development and Patient Stratification\nEven when a target is well validated, many programs fail in clinical trials because the right patients were not enrolled, the right endpoints were not measured, or the treatment effect was diluted across a heterogeneous population. Foundation model representations provide new tools for defining and validating biomarkers.\n\n\n\n\n\n\nBiomarker discovery pipeline with foundation models\n\n\n\n\nFigure 29.5: Biomarker discovery pipeline with foundation models. FM embeddings from patient molecular data provide rich features for association discovery with clinical outcomes. Candidate biomarkers emerge from features distinguishing responders from non-responders. Technical validation develops reproducible assays; clinical validation demonstrates utility in prospective trials. Timeline reality: discovery in months, but clinical validation requires years. Foundation models accelerate discovery but cannot bypass the clinical validation bottleneck required for regulatory approval and clinical adoption.\n\n\n\n\n29.6.1 Foundation Model Features for Stratification\nClassical polygenic scores summarize additive effects of common variants on disease risk (see Section 3.5). These scores have proven useful for patient enrichment in cardiovascular and metabolic disease trials, selecting patients at highest genetic risk who might benefit most from intervention [Citation Needed]. The enrichment logic is straightforward: if a drug reduces cardiovascular events by 30% relative risk, enrolling patients with 2x baseline risk doubles the absolute benefit and thus the statistical power to detect efficacy. High-risk patients also have more room for improvement, making treatment effects more detectable. Deep learning methods extend this approach by learning nonlinear genotype-phenotype mappings that capture interactions and nonadditive effects.\nFoundation models enhance polygenic prediction in several ways. Instead of using raw genotypes as inputs, models can use variant effect scores, regulatory predictions, or gene-level embeddings derived from foundation models. This captures biological context that simple genotypes miss. Models trained on variant embeddings rather than binary genotype calls can capture subtle differences between variants at the same position, distinguishing a mildly damaging missense from a severely damaging one even when both are heterozygous.\nTransfer across populations represents a particular strength (see Section 3.7). Foundation models trained on diverse genomes provide representations that may generalize more robustly across ancestries than models trained on individual cohorts. Fine-mapping-aware approaches that use foundation model features can reduce dependence on linkage disequilibrium patterns that vary across populations, potentially improving the portability of genetic risk predictors.\n\n\n\n\n\n\nStop and Think\n\n\n\nTraditional polygenic scores use the same weight for all heterozygous carriers of a given variant. How might foundation model embeddings allow more nuanced scoring? What information could distinguish two patients who are both heterozygous for a missense variant in the same gene?\n\n\n\n\n\n\n\n\nWorked Example: FM-Enhanced Stratification for Cardiovascular Risk\n\n\n\nA Phase III trial for a PCSK9 inhibitor wants to enrich enrollment for high-risk patients. Compare traditional PRS to FM-enhanced stratification:\nTraditional PRS approach: - Sum of ~1 million GWAS effect sizes × genotypes - European-ancestry cohort: AUC = 0.64 for 10-year cardiovascular events - African-ancestry cohort: AUC = 0.58 (lower due to LD pattern differences) - All variants weighted equally regardless of functional impact\nFM-enhanced approach: 1. For each variant, compute AlphaMissense pathogenicity score (coding) or Enformer regulatory impact (noncoding) 2. Weight variants by functional impact: high-impact variants (score &gt; 0.8) receive 3× weight 3. Include FM-derived features: aggregate burden of predicted loss-of-function variants in LDL-related genes\nResults on held-out validation cohort (n = 15,000):\n\n\n\nMetric\nTraditional PRS\nFM-Enhanced\nImprovement\n\n\n\n\nEuropean AUC\n0.64\n0.68\n+6%\n\n\nAfrican AUC\n0.58\n0.65\n+12%\n\n\nTop decile relative risk\n2.8×\n3.4×\n+21%\n\n\nSample size for 80% power\n4,200\n3,100\n-26%\n\n\n\nInterpretation: FM-enhanced stratification provides modest overall improvement but substantially reduces ancestry-dependent performance gaps. The 26% reduction in required sample size translates to millions of dollars in trial cost savings and faster time to results.\nThe improvement comes from FM embeddings capturing functional information that generalizes across populations better than LD-dependent effect sizes. Two patients with the same genotype at a variant may have different FM-predicted impacts based on their full genetic context.\n\n\n\n\n29.6.2 Multi-Omic Biomarker Discovery\nBeyond germline genetics, drug development increasingly leverages somatic genomics, transcriptomics, proteomics, and other molecular readouts. Tumor sequencing combined with expression profiling characterizes the molecular landscape of each patient’s cancer. Single-cell multiome data (RNA plus ATAC) reveal cell-state heterogeneity that bulk assays miss (see Chapter 19).\nFoundation models trained on these data types provide embeddings that capture patient-level molecular profiles. Set-based architectures that treat each patient’s genomic features as a set (rather than assuming fixed feature positions) can handle the heterogeneity of tumor genomes, where different patients have different mutations (?sec-ch19-strategies). Gene regulatory network inference models trained on atlas-scale single-cell data can extract pathway activity scores that serve as mechanistically interpretable biomarkers.\nThe key shift is that biomarkers are no longer limited to a handful of hand-picked variants or expression markers. They become functions over high-dimensional genomic and multi-omic embeddings, learned in a data-driven way yet grounded in biological priors from foundation models. A biomarker might be a region of embedding space corresponding to patients with particular molecular subtypes, defined by the model rather than by manual curation.\n\n\n29.6.3 Trial Design and Endpoint Selection\nFoundation model predictions inform trial design at multiple stages. Patient enrichment uses genetic risk scores or molecular subtypes to select patients most likely to respond, increasing statistical power and reducing required sample sizes. Adaptive designs use intermediate biomarker responses to modify randomization or dosing during the trial. Endpoint selection uses molecular signatures to define pharmacodynamic biomarkers that indicate target engagement, supporting dose selection and early efficacy signals.\nRegulatory agencies increasingly accept genomic biomarkers for patient selection in oncology and are developing frameworks for other therapeutic areas [Citation Needed]. The challenge is validation: demonstrating that foundation model predictions actually stratify patient outcomes requires prospective trials or well-designed retrospective analyses with appropriate controls for confounding (see Chapter 12).",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-industry-workflows",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-industry-workflows",
    "title": "29  Drug Discovery",
    "section": "29.7 Industry Workflows and Infrastructure",
    "text": "29.7 Industry Workflows and Infrastructure\nFor pharmaceutical and biotechnology organizations, the challenge is not whether they can access a foundation model but how to integrate these models into existing data platforms, governance structures, and decision-making processes.\n\n29.7.1 Building Model Infrastructure\nIn mature organizations, foundation models should be treated as shared infrastructure rather than ad hoc scripts developed by individual project teams. A well-organized model catalog contains DNA language models (Nucleotide Transformer, HyenaDNA, GENA-LM), sequence-to-function models (Enformer, Borzoi), and variant effect predictors (AlphaMissense, GPN-MSA, CADD) with documented capabilities, limitations, and appropriate use cases (see Appendix D for model reference).\nFeature services provide centralized APIs that accept variants, genomic intervals, or genes as input and return embeddings, predicted functional profiles, or risk features. Centralization enables consistency across programs and avoids redundant computation. Logging and versioning ensure that analyses can be reproduced even as models and data evolve.\nData governance maintains clear separation between models trained on public data versus sensitive internal cohorts. Internal data, including proprietary clinical trial data, patient samples, and collaborator contributions, requires careful handling. Guardrails define where internal data can be used for fine-tuning and how resulting models can be shared or published.\n\n\n29.7.2 Strategic Choices: Build, Buy, or Fine-Tune\nOrganizations face three strategic options when adopting foundation models.\n\n\n\nTable 29.2: Strategic options for foundation model adoption in drug discovery organizations. The optimal choice depends on internal data assets, computational resources, and use case specificity.\n\n\n\n\n\n\n\n\n\n\n\nStrategy\nAdvantages\nDisadvantages\nBest For\n\n\n\n\nUse external models as-is\nLow cost, community benchmarked, fast deployment\nMay not fit internal populations or assays\nEarly exploration, non-differentiated applications\n\n\nFine-tune open-source FMs\nRetains general knowledge, adapts to internal data\nRequires compute and ML expertise, privacy controls\nCompanies with internal biobanks, proprietary phenotypes\n\n\nTrain bespoke internal models\nMaximum control, aligned with specific use cases\nHighest cost, risk of overfitting, long timelines\nUnique data assets, differentiated therapeutic areas\n\n\n\n\n\n\nUsing external models as-is offers low upfront cost and benefits from community benchmarking, but may not capture organization-specific populations, assays, or therapeutic areas. A model trained primarily on European ancestry populations may perform poorly on a company’s Asian-focused programs; a model trained on common cell lines may miss biology relevant to rare disease indications.\nFine-tuning open-source foundation models on internal data retains general representations while adapting to local data distributions (see Chapter 9 for fine-tuning approaches). This approach requires computational investment and careful privacy controls, but often provides the best balance of generality and specificity. A company with large internal biobank data can fine-tune a general variant effect predictor on that cohort, improving predictions for its patient populations without sacrificing the broad knowledge captured during pretraining.\nTraining bespoke internal models from scratch offers maximum control and allows alignment of pretraining objectives with specific use cases (see Chapter 8). A company focused on rare diseases might pretrain on sequences and phenotypes particularly relevant to that space. The cost is substantial: pretraining requires significant compute, data engineering, and machine learning expertise (see Appendix B). There is also risk of overfitting to narrow internal datasets if the pretraining corpus is not sufficiently diverse.\nIn practice, most organizations adopt hybrid strategies. They start with public foundation models for early exploration and non-sensitive applications, gradually fine-tune on internal data as value becomes clear, and reserve from-scratch training for cases where unique data assets justify the investment. Lightweight model-serving infrastructure handles latency-sensitive applications such as clinical decision support, while heavier offline systems support large-scale research workloads.\n\n\n29.7.3 Industry Context: Timelines and Decision Gates\nAcademic machine learning research optimizes benchmark performance; drug discovery optimizes probability of clinical and commercial success under time and resource constraints. Understanding industry context helps foundation model practitioners contribute effectively.\nDrug discovery programs progress through gates: target validation, candidate selection, investigational new drug filing, and clinical trial phases. Each gate requires specific evidence: biological rationale, efficacy data, safety data, manufacturing feasibility. Foundation model contributions must align with gate requirements. A beautiful embedding space is valueless if it cannot be translated into evidence that advances a program through its next gate.\n\n\n\n\n\n\nPractical Guidance: Aligning FM Work with Drug Development Gates\n\n\n\n\n\n\n\n\n\n\n\nGate\nKey Questions\nFM Contribution\n\n\n\n\nTarget validation\nIs the target disease-relevant and modifiable?\nGenetic evidence aggregation, VEP scoring, network context\n\n\nLead selection\nWhich molecules have best target engagement?\nDTI prediction, selectivity profiling\n\n\nPreclinical\nWhat are safety liabilities?\nPheWAS-informed toxicity prediction, off-target flags\n\n\nClinical Phase I\nIs target engaged at tolerable doses?\nPharmacodynamic biomarkers\n\n\nClinical Phase II/III\nWhich patients respond?\nStratification biomarkers, enrichment scores\n\n\n\nFor each application, ask: “What decision does this enable, and when must that decision be made?”\n\n\nTimelines matter. A prediction that takes six months to validate experimentally may be worthless if the program decision must be made in three months. Foundation models that enable faster experiments (through better library design, prioritization, or interpretation) create more value than models that provide incrementally better predictions but require the same experimental timeline to validate.\nBiotechnology companies and pharmaceutical companies operate differently. Biotechs often focus on single programs with limited resources, prioritizing speed and risk-taking. Pharma companies manage portfolios across therapeutic areas, prioritizing consistency and scalability. Foundation model infrastructure that serves one context may not serve the other. A boutique biotech might prefer lightweight, single-purpose models; a large pharma might invest in comprehensive infrastructure serving dozens of programs.\n\n\n29.7.4 Intellectual Property and Data Considerations\nFoundation models raise new questions around intellectual property, data sharing, and regulatory expectations that organizations must navigate.\nModels trained on proprietary data can be valuable assets but are difficult to patent directly. The model architecture is typically based on published methods; the weights reflect training data that may include public and proprietary components. Downstream discoveries, including specific targets, biomarkers, and therapeutic hypotheses derived from foundation model analyses, are more clearly protectable but require careful documentation of inventive contribution.\nCollaborative model development across institutions may require federated learning or model-to-data paradigms, especially for patient-level data. Genomic data carries re-identification risk (see Section 26.3); sharing raw data, even for model training, requires appropriate consent and data use agreements. Federated approaches that train on local data without centralizing raw information can enable multi-institutional collaboration while respecting privacy constraints.\nFor regulatory submissions, foundation models used in drug development create documentation requirements. If a model informed target selection, patient stratification, or safety assessment, regulators may request information about model training, validation, and performance across subgroups (see Section 26.1). The confounding and interpretability challenges discussed in Chapter 12 and Chapter 24 become acute when models inform pivotal decisions in drug development. Clear documentation trails from model prediction to program decision support regulatory review.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-evaluation",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-evaluation",
    "title": "29  Drug Discovery",
    "section": "29.8 Evaluation and Validation",
    "text": "29.8 Evaluation and Validation\nEvaluating foundation model contributions to drug discovery requires carefully separating model performance from scientific and clinical validity. A model that achieves high benchmark scores may still fail to improve drug discovery outcomes; a model with modest benchmarks may provide actionable insights that advance programs.\n\n29.8.1 Benchmark Limitations\nMany published benchmarks draw targets and drugs from the same databases used to pretrain models, creating risk of leakage that inflates performance estimates (Section 12.4.4; Section 12.4). Repurposing success stories often rely on retrospective data mining with limited prospective validation. The ultimate test of a foundation model for drug discovery is whether it identifies targets that succeed in clinical trials, a test that takes years and confounds model contribution with countless other factors.\nConfounding pervades drug discovery data (see Chapter 12). Models trained on observational clinical and genomic data inherit confounders from those data. Drug-disease associations learned by foundation models may reflect treatment patterns rather than true causal relationships. Confounding by indication (sicker patients receive different treatments), survivorship bias (only patients who survived long enough enter certain analyses), and healthcare access patterns all threaten validity. Genetic instruments and careful epidemiologic designs remain essential for causal claims that foundation model predictions cannot provide alone.\n\n\n29.8.2 From Prediction to Validation\nFoundation model predictions are hypotheses, not conclusions. A target ranked highly by genetic evidence and foundation model scoring still requires experimental validation. The value of foundation models lies in prioritizing which hypotheses to test, not in replacing experimental testing.\nValidation strategies depend on the application. Target predictions can be validated through functional genomics screens that test whether predicted targets affect disease-relevant phenotypes. Biomarker predictions require retrospective validation on held-out cohorts or prospective validation in clinical trials. Repurposing predictions require real-world evidence analyses or prospective trials.\nThe timeline for validation matters. Some predictions can be tested in weeks (cell-based assays for target validation); others require years (clinical outcomes for biomarkers). Foundation model contributions should be assessed on timescales relevant to drug discovery decisions, not just immediate benchmark performance.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-molecular-design",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-molecular-design",
    "title": "29  Drug Discovery",
    "section": "29.9 Connections to Molecular Design",
    "text": "29.9 Connections to Molecular Design\nFoundation model representations connect target identification to downstream molecular design. The bridge between genomic and molecular foundation models typically involves using target context as conditioning signals for molecule generation. Gene-level embeddings from genomic foundation models, reflecting genetic evidence, tissue specificity, and network context, can condition chemistry models that propose molecules targeting that gene.\nMulti-modal foundation models jointly trained on DNA, RNA, proteins, structures, small molecules, and phenotypic readouts learn representations that span these modalities (see Chapter 22). Such models can predict not just whether a molecule binds a target, but how target modulation in a particular genetic context might affect cellular phenotypes. Closed-loop optimization uses genomic foundation models to predict target relevance and liability, chemistry and protein foundation models to propose molecules, and experimental feedback to update both model types in active learning cycles.\nThe detailed treatment of molecular design belongs to Chapter 30. From a target identification perspective, the key point is that genomic foundation models determine whether a target is worth pursuing; downstream models then optimize how to hit it. The investment in accurate target identification and validation pays dividends throughout the drug discovery pipeline by ensuring that optimization efforts focus on targets with the highest probability of clinical success.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch29-drug-discovery.html#sec-ch29-conclusion",
    "href": "part_6/p6-ch29-drug-discovery.html#sec-ch29-conclusion",
    "title": "29  Drug Discovery",
    "section": "29.10 Prioritization, Not Automation",
    "text": "29.10 Prioritization, Not Automation\nFoundation models connect to drug discovery not as replacements for experimental validation but as tools that reduce risk and focus resources. Target discovery workflows aggregate variant-level predictions into gene-level evidence, integrating fine-mapping, variant effect prediction, and regulatory modeling to prioritize candidates with strong genetic and mechanistic support. Network-aware approaches propagate genetic signals through protein and regulatory networks to identify druggable nodes and repurposing opportunities. Drug-target interaction prediction uses foundation model representations to assess binding, selectivity, and safety liabilities before synthesis. Functional genomics screens leverage foundation models for library design and iterative refinement. Biomarker development uses foundation model features for patient stratification and trial enrichment.\nThroughout these applications, the value proposition is acceleration and prioritization, not automation of discovery. Foundation models help identify the most promising targets, design the most informative experiments, and select the patients most likely to benefit. Programs that would have required years of hypothesis testing can reach the right target faster. Screens that would have required exhaustive enumeration can focus on high-priority candidates. The fundamental uncertainties of drug development remain: targets validated genetically may still fail in trials, predictions of binding may not translate to efficacy, and patients selected by biomarkers may still not respond. Foundation models reduce risk without eliminating it.\nSequence design (Chapter 30) extends these ideas from analysis to generation, where foundation models not only evaluate existing sequences but propose new ones optimized for therapeutic function. The transition from interpretation to design represents the frontier where foundation models become engines for creating biology, not just understanding it.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhy do genetically validated drug targets succeed at higher rates in clinical trials? What specific evidence does human genetics provide that other target selection methods lack?\nDescribe the pipeline for aggregating variant-level foundation model predictions into gene-level target prioritization. What are the key challenges in connecting noncoding variants to target genes?\nHow do network-aware approaches differ from single-gene analysis in target discovery? When might a gene with modest GWAS evidence be prioritized over one with stronger statistical association?\nWhat is the difference between exploiting model predictions (choosing high-scoring designs) versus exploring uncertain regions in active learning? When should you prioritize each strategy?\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nGenetic validation advantage: Genetically validated targets succeed at approximately twice the rate of non-validated targets because human genetics provides causal evidence of target-disease relationships through lifelong modulation in real populations. Unlike correlational evidence from expression studies or pathway membership, genetic associations reflect actual consequences of gene function changes, including protective variants that indicate direction of effect and human knockouts that demonstrate the physiological consequences of complete gene inactivation.\nVariant-to-gene aggregation: The pipeline starts with GWAS summary statistics and fine-mapping to identify credible causal variants, then applies foundation models to score regulatory and coding impacts (using models like AlphaMissense for coding variants and Enformer for regulatory effects). The key challenge for noncoding variants is connecting them to target genes, which requires integrating chromatin conformation data (Hi-C), eQTL colocalization, enhancer-gene predictions from regulatory models, and network proximity evidence to determine which gene is actually affected by a regulatory variant.\nNetwork-aware vs. single-gene approaches: Network-aware approaches recognize that proteins function in complexes and pathways, so genetic evidence concentrated in a functional module provides stronger validation than isolated hits. A gene with modest GWAS significance but strong connections to multiple well-validated disease genes may be prioritized over an isolated genome-wide significant hit with unclear mechanism, because network context reveals “near-miss” targets that aggregate signals across functionally related genes rather than requiring each target to independently reach statistical significance.\nExploitation vs. exploration in active learning: Exploitation means testing high-scoring predictions where the model is confident, which validates existing knowledge but provides limited new information. Exploration means testing regions where the model has high uncertainty, which maximally improves future predictions by resolving ambiguity. You should prioritize exploration when building foundational understanding (early screening phases) and shift toward exploitation when refining known mechanisms or selecting final candidates for advancement, as uncertain regions teach the model more and improve predictions across related sequences.\n\n\n\n\n\n\nCore Concepts:\n\nGenetic target validation: Targets with genetic support succeed at ~2x the rate of unsupported targets; foundation models provide mechanistic context for genetic associations\nVariant-to-gene aggregation: Pipelines integrate fine-mapping, VEP scoring, and regulatory predictions to rank candidate targets\nNetwork-aware discovery: Graph neural networks propagate genetic signals through interaction networks, recovering targets missed by single-gene analysis\nDrug-target interaction: Protein and chemical embeddings enable binding prediction without experimental structures\nToxicity prediction: PheWAS associations, expression patterns, and off-target predictions inform safety assessment\nFunctional genomics: Foundation models guide library design and enable perturbation matching for target validation\nBiomarker development: Foundation model features enhance polygenic prediction and enable multi-omic stratification\nIndustry infrastructure: Strategic choices between external models, fine-tuning, and bespoke training depend on data assets and use cases\n\nKey Themes:\n\nFoundation models provide prioritization, not automation—they identify what to test, not replace testing\nGenetic evidence is necessary but not sufficient; experimental validation remains essential\nConfounding pervades drug discovery data; causal inference requires careful epidemiologic design\nValue is measured by advancing programs through gates, not benchmark performance\nThe lab-in-the-loop cycle connects foundation model predictions to experimental refinement\n\nConnections: This chapter bridges variant effect prediction (Chapter 17), network analysis (Chapter 21), and confounding (Chapter 12) to practical drug discovery applications. The concepts connect forward to molecular design (Chapter 30).\n\n\n\n\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. “AlphaGenome: AI for Better Understanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.” Nature Genetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. “[AlphaMissense] Accurate Proteome-Wide Missense Variant Effect Prediction with AlphaMissense.” Science 381 (6664): eadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. “[ESM-2] Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025. “[MIFM] Multiple Instance Fine-Mapping: Predicting Causal Regulatory Variants with a Deep Sequence Model.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray, Peter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping Improves Identification of Causal Variants.” Research Square, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Drug Discovery</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html",
    "href": "part_6/p6-ch30-design.html",
    "title": "30  Sequence Design",
    "section": "",
    "text": "30.1 Design Formalism\nSequence design inverts the standard prediction problem. Where prediction maps from sequence to function (given sequence \\(x\\), estimate property \\(f(x)\\)), design maps from desired function to sequence (given target property \\(y^\\star\\), find sequence \\(x^\\star\\) such that \\(f(x^\\star) \\approx y^\\star\\)). This inversion is computationally challenging because biological sequence spaces are astronomically large. A 200-residue protein admits \\(20^{200}\\) possible sequences, vastly exceeding the number of atoms in the observable universe. Even a modest 500-base-pair regulatory element spans \\(4^{500}\\) possibilities. Exhaustive enumeration is impossible; intelligent search strategies are essential.\nThe design objective can take several mathematical forms depending on the application. Optimization problems seek sequences that maximize (or minimize) a scalar objective, such as finding \\(x^\\star = \\arg\\max_x f_\\theta(x)\\) where \\(f_\\theta\\) might represent predicted binding affinity, expression level, or stability. Conditional generation problems sample sequences from a distribution conditioned on desired properties, drawing \\(x \\sim p_\\theta(x \\mid y)\\) where \\(y\\) specifies structural constraints, functional requirements, or context. Constrained optimization problems combine objective maximization with explicit constraints, seeking \\(x^\\star = \\arg\\max_x f_\\theta(x)\\) subject to \\(c(x) \\leq 0\\), where constraints \\(c\\) might enforce GC content limits, avoid restriction sites, or maintain similarity to natural sequences.\nThe table below summarizes how these different formulations apply to common design scenarios.\nFoundation models contribute to design through multiple mechanisms. As generative priors, they assign higher probability to sequences resembling natural biology, regularizing optimization toward plausible regions of sequence space. As differentiable oracles, they enable gradient-based optimization where sequence modifications are guided by gradients of predicted properties. As embedding functions, they map discrete sequences into continuous spaces where interpolation and optimization become tractable (Section 5.6 for representation fundamentals; Section 8.1.2 for how pretraining shapes these spaces). The challenge lies in searching enormous combinatorial spaces while remaining within regimes where these model-based estimates remain reliable.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-formalism",
    "href": "part_6/p6-ch30-design.html#sec-ch30-formalism",
    "title": "30  Sequence Design",
    "section": "",
    "text": "Stop and Think\n\n\n\nBefore reading about the design formalism, consider: if you wanted to find a protein sequence with a specific binding property, why couldn’t you simply enumerate all possible sequences and pick the best one? What makes this approach impractical, and what alternative strategies might you consider?\n\n\n\n\n\n\n\n\n\n\n\nForward prediction: sequence to function\n\n\n\n\n\n\n\nInverse design: function to sequence through vast fitness landscape\n\n\n\n\n\n\nFigure 30.1: Sequence design problem formulation. (A) Forward versus inverse problems: foundation models excel at forward prediction (sequence → function) but inverse design (function → sequence) requires search through vast combinatorial spaces. (B) Design space landscape: sequence fitness varies across 4^N (DNA) or 20^N (protein) possibilities. Optimization must navigate between local optima traps toward global optimum—a fundamentally harder problem than prediction.\n\n\n\n\n\n\n\n\nTable 30.1: Design problem formulations and their applications.\n\n\n\n\n\n\n\n\n\n\n\nDesign Formulation\nMathematical Form\nTypical Applications\nKey Challenge\n\n\n\n\nOptimization\n\\(x^\\star = \\arg\\max_x f_\\theta(x)\\)\nMaximize binding affinity, expression level\nMay find adversarial sequences\n\n\nConditional generation\n\\(x \\sim p_\\theta(x \\mid y)\\)\nGenerate sequences with specified structure\nRequires well-calibrated conditional models\n\n\nConstrained optimization\n\\(\\max_x f_\\theta(x)\\) s.t. \\(c(x) \\leq 0\\)\nOptimize function while avoiding restriction sites\nConstraint satisfaction adds complexity\n\n\nMulti-objective\nPareto frontier of \\((f_1, f_2, \\ldots)\\)\nBalance affinity, stability, immunogenicity\nNo single optimal solution exists",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-protein-design",
    "href": "part_6/p6-ch30-design.html#sec-ch30-protein-design",
    "title": "30  Sequence Design",
    "section": "30.2 Protein Design with Language Models",
    "text": "30.2 Protein Design with Language Models\nProtein language models trained on evolutionary sequence databases (Chapter 15) have emerged as effective tools for protein design, providing both generative sampling capabilities and fitness estimation for candidate sequences. The masked language modeling objectives that enable fitness estimation are detailed in Section 8.1. The success of these approaches stems from a key insight: evolution has conducted billions of years of experiments on protein sequence space, and models trained on the surviving sequences implicitly encode constraints on what works.\n\n\n\n\n\n\n\n\nDirected evolution in silico: mutate, score with FM, select, iterate\n\n\n\n\n\n\n\nGenerative design: sample from FM-learned priors conditioned on properties\n\n\n\n\n\n\nFigure 30.2: Protein design approaches. (A) Directed evolution in silico: generate variants through mutation, score with foundation models, select top performers, iterate. Achieves in seconds what lab evolution requires weeks for. (B) Generative design: sample from generative models (ProGen, ProteinMPNN, RFdiffusion) trained on evolutionary data, conditioning on desired properties. Foundation models contribute as fitness predictors (A) or as generative priors (B)—both leverage representations learned from millions of natural sequences.\n\n\n\n\n30.2.1 Sequence Generation from Language Model Priors\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore reading about protein language model generation, recall from Chapter 8: What is the difference between autoregressive and masked language modeling objectives? How would each approach support sequence generation differently?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nAutoregressive models (like GPT-style) predict each token given all previous tokens, making them naturally suited for sequential generation. Masked models (like BERT-style) predict masked tokens given surrounding context bidirectionally, supporting iterative refinement by masking and resampling positions. For protein design, autoregressive models generate sequences left-to-right, while masked models enable position-specific refinement of existing sequences.\n\n\n\n\n\nAutoregressive protein language models such as ProGen and ProtGPT2 generate novel protein sequences by sampling tokens sequentially from learned distributions (Madani et al. 2023; Ferruz, Schmidt, and Höcker 2022). Given a partial sequence, the model predicts probability distributions over the next amino acid, enabling iterative extension until a complete protein emerges. This generation process can be unconditional (sampling from the full learned distribution) or conditional on control signals such as protein family annotations, organism of origin, or functional keywords.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider the temperature parameter in sequence generation. If you sample at T=0 (deterministic, always picking highest-probability amino acid), what kind of sequences would you expect to generate? What about at very high temperature? What tradeoff does this create for protein design?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nAt T=0, you generate the single most probable sequence—likely a “consensus” protein similar to highly abundant natural proteins. At very high T, you sample broadly including low-probability amino acids, producing diverse but potentially nonfunctional sequences. The tradeoff: low temperature exploits known-good sequence space (safe but unoriginal), high temperature explores novel space (creative but risky). Practical workflows sample across temperatures and use downstream filters.\n\n\n\n\n\nThe quality of generated sequences depends critically on how closely the sampling distribution matches functional proteins. Sequences sampled at low temperature (more deterministic) tend to resemble common protein families but may lack novelty. Sequences sampled at high temperature (more stochastic) exhibit greater diversity but risk straying into nonfunctional regions. The temperature parameter controls the entropy of the sampling distribution: at temperature T=0, the model deterministically selects the highest-probability token at each position; as T increases, lower-probability tokens become increasingly likely to be sampled. This creates a fundamental exploration-exploitation tradeoff: low temperatures exploit the model’s knowledge of natural sequences but may miss novel functional solutions, while high temperatures explore more broadly but venture into regions where the model’s predictions become unreliable. Practical design workflows often generate large libraries of candidates across temperature ranges, then filter using downstream oracles for structure, stability, or function.\nMasked language models like ESM-2 support design through a different mechanism. Rather than generating sequences de novo, these models estimate the probability of each amino acid at each position given the surrounding context. Design proceeds by iterative refinement: starting from an initial sequence, positions are masked and resampled according to model predictions, gradually shifting the sequence toward higher-likelihood regions. This Gibbs-sampling-like procedure can be biased toward specific objectives by combining model likelihoods with scores from downstream predictors.\nThe key advantage of protein language model-based design lies in data efficiency. Because models are pretrained on millions of natural sequences, they generalize to design tasks with minimal task-specific data. A model fine-tuned on a few hundred functional variants can propose candidates across sequence space, extrapolating far beyond the training examples. This contrasts with traditional directed evolution approaches that require extensive experimental screening to navigate sequence space.\n\n\n30.2.2 Structure-Aware Design with Diffusion Models\n\n\n\n\n\n\nConceptual Difficulty\n\n\n\nStructure-aware design involves understanding how diffusion models operate in three-dimensional coordinate space. If you are unfamiliar with diffusion models (progressive denoising from noise to signal), you may wish to review the diffusion model literature or focus on the conceptual workflow rather than mathematical details.\n\n\nStructure-aware design addresses a fundamental limitation of sequence-only approaches: proteins function through three-dimensional structures, and sequence optimization without structural guidance may produce sequences that fail to fold correctly. The advent of accurate structure prediction (AlphaFold2, ESMFold; Section 15.4; Section 15.4) enables new design paradigms that jointly consider sequence and structure.\nRFdiffusion exemplifies this approach by generating protein backbones through a diffusion process in three-dimensional coordinate space (Watson et al. 2023). Starting from random noise, the model iteratively denoises toward plausible backbone geometries, conditioned on design specifications such as target binding interfaces, desired topology, or symmetric assembly requirements. The resulting backbones represent novel structures not observed in nature but predicted to be physically realizable.\nConverting designed backbones to sequences requires inverse folding models that predict amino acid sequences likely to adopt a given structure. ProteinMPNN and ESM-IF operate on this principle, taking backbone coordinates as input and outputting probability distributions over sequences predicted to fold onto that backbone(Dauparas et al. 2022; Hsu et al. 2022). ESM-IF leverages the representations learned by ESM-2 to condition sequence generation on structural constraints, connecting the inverse folding task directly to the protein language model paradigm. The model can generate thousands of candidate sequences for a single backbone, enabling selection based on additional criteria such as expression likelihood or immunogenicity.\n\n\n\n\n\n\nKey Insight: Structure as Intermediate Representation\n\n\n\nThe power of structure-aware design lies in using 3D structure as an intermediate representation between function and sequence. Rather than searching directly in the astronomically large space of sequences, design first identifies a structure that would achieve the desired function, then finds sequences that fold to that structure. This factorization dramatically constrains the search space.\n\n\nThis two-stage pipeline (structure diffusion followed by inverse folding) has proven effective for creating novel proteins. Designed binders targeting challenging therapeutic targets, de novo enzymes with specified active site geometries, and symmetric protein assemblies with precise nanoscale dimensions have all been realized experimentally. [Citation Needed] The key insight is that structure provides a useful intermediate representation: rather than searching directly in the vast space of sequences, design proceeds through the more constrained space of physically realizable structures.\nThe table below compares sequence-based and structure-aware approaches across key design considerations.\n\n\n\nTable 30.2: Comparison of protein design paradigms.\n\n\n\n\n\n\n\n\n\n\nConsideration\nSequence-Based (PLM)\nStructure-Aware (Diffusion + Inverse Folding)\n\n\n\n\nPrior knowledge required\nProtein family or starting sequence\nTarget structure or binding interface\n\n\nNovel structure capability\nLimited to known folds\nCan generate entirely new topologies\n\n\nComputational cost\nLower (sequence operations only)\nHigher (3D coordinate generation)\n\n\nOutput diversity\nDepends on sampling temperature\nHigh (many sequences per backbone)\n\n\nExperimental success rate\n30-50% express\n30-70% express; 5-30% functional\n\n\nBest applications\nVariant optimization, library design\nDe novo binders, enzymes, assemblies\n\n\n\n\n\n\n\n\n30.2.3 Functional Conditioning and Multi-Objective Optimization\n\n\n\n\n\n\nKnowledge Check\n\n\n\nConsider a therapeutic antibody design project. List at least four properties you would need to optimize simultaneously, and explain why optimizing for just one property (e.g., binding affinity) would be insufficient for a successful therapeutic.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nKey properties include: (1) binding affinity to target, (2) specificity to avoid off-target effects, (3) manufacturability and expression levels, (4) stability during storage, (5) solubility to prevent aggregation, and (6) low immunogenicity to minimize immune responses. Optimizing only affinity could yield an antibody that binds excellently but aggregates during manufacturing, triggers immune responses in patients, or binds unintended targets causing toxicity.\n\n\n\n\n\nReal therapeutic or industrial applications rarely optimize a single objective. A designed enzyme must not only be catalytically active but also stable at process temperatures, expressible in the production host, and resistant to proteolytic degradation. A therapeutic antibody must bind its target with high affinity while avoiding off-target interactions, maintaining solubility, and minimizing immunogenicity. These competing demands create multi-objective optimization problems where no single sequence optimizes all criteria simultaneously.\nMulti-objective design produces Pareto frontiers—the set of solutions where no objective can be improved without worsening another—representing different trade-offs among objectives. A sequence might achieve exceptional binding affinity at the cost of reduced stability, while another balances moderate affinity with excellent developability properties. Practitioners must select among Pareto-optimal solutions based on application-specific priorities, and foundation models increasingly support this selection by providing diverse oracles across multiple property dimensions.\nFoundation models contribute to multi-objective design in three ways. Generative priors propose candidate sequences that satisfy basic plausibility constraints (foldability, expressibility) before optimization begins. Multiple differentiable oracles (for binding, stability, immunogenicity) enable gradient-based optimization toward Pareto frontiers. Embedding spaces support interpolation between sequences with different property profiles, enabling exploration of intermediate trade-offs. The combination of these capabilities makes foundation models central to modern protein design pipelines.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-regulatory-design",
    "href": "part_6/p6-ch30-design.html#sec-ch30-regulatory-design",
    "title": "30  Sequence Design",
    "section": "30.3 Regulatory Sequence Design",
    "text": "30.3 Regulatory Sequence Design\nGenomic foundation models trained on chromatin accessibility, transcription factor binding, and gene expression data enable design of synthetic regulatory elements with specified activity profiles. Unlike protein design where the sequence-to-function mapping operates through three-dimensional structure, regulatory design must account for the genomic and cellular context in which elements function. The functional genomics resources described in Section 2.4 provide training data for these models, while the interpretability methods from Section 24.1 inform design strategies by revealing which sequence features drive predictions.\n\n\n\n\n\n\nRegulatory element design workflow with DNA foundation models\n\n\n\n\nFigure 30.3: Regulatory element design with DNA foundation models. Design targets span synthetic promoters (expression level), tissue-specific enhancers (spatial control), 5’ UTRs (translation efficiency), and CRISPR guides (specificity). Workflow combines sequence generation methods (gradient optimization, evolutionary search, generative sampling) with FM-based scoring. Validation through MPRA enables testing thousands of designs in parallel, creating feedback loops that improve both designs and models.\n\n\n\n\n30.3.1 Promoter and Enhancer Engineering\n\n\n\n\n\n\nStop and Think\n\n\n\nGradient-based design for regulatory elements uses the same saliency computations described in Section 24.1.2 for interpretation, but runs them “in reverse.” Before reading on, consider: if a saliency map tells you which nucleotides most affect the current prediction, how might you use this information to increase predicted expression in a target cell type?\n\n\nMassively parallel reporter assays (MPRAs) have generated training data for models that predict expression levels from promoter and enhancer sequences (Section 2.4.4; Section 2.4.4) (Boer et al. 2020). These models learn sequence determinants of regulatory activity, including transcription factor binding sites, spacing constraints between elements, and context-dependent interactions. Once trained, the same models serve as oracles for design: by evaluating expression predictions across millions of candidate sequences, optimization algorithms can identify synthetic regulatory elements with desired properties.\nGradient-based design treats the sequence-to-expression model as a differentiable function. Starting from an initial sequence, gradients of predicted expression with respect to input positions indicate which mutations would increase (or decrease) activity. Because sequences are discrete while gradients are continuous, optimization requires relaxation strategies that operate on “soft” sequence representations before projecting back to discrete nucleotides. These approaches leverage the same saliency map computations used for model interpretation (Section 24.1.2), running the analysis in reverse to guide design rather than explain predictions.\nDesign objectives for regulatory elements extend beyond maximizing expression in a target context. Cell-type-specific enhancers should drive high expression in desired tissues while remaining inactive elsewhere. Inducible promoters should respond to specific signals while maintaining low basal activity. Compact regulatory elements are preferred for gene therapy applications where vector capacity is limited. These constraints transform simple optimization into multi-objective problems requiring careful balancing of competing requirements.\nGenerative models trained directly on regulatory sequences offer an alternative to optimization-based approaches. Autoregressive or diffusion models learn to sample novel enhancers and promoters that match the statistical properties of natural regulatory elements. Conditioning on cell type labels, chromatin state annotations, or other metadata enables generation of elements with targeted activity profiles. The advantage of generative approaches lies in their ability to produce diverse candidate libraries for experimental screening, rather than converging on a single optimized sequence that may exploit model artifacts rather than genuine biology.\n\n\n30.3.2 Splicing and RNA Processing Elements\nModels trained on splicing outcomes (SpliceAI and related architectures described in Chapter 6; see also Chapter 18 for RNA-specific foundation models) enable design of sequences that modulate RNA processing. Therapeutic applications include correcting pathogenic splice site mutations by strengthening weak splice sites or weakening aberrant ones, designing antisense oligonucleotides that redirect splicing to skip exons containing disease-causing mutations, and engineering alternative splicing outcomes to produce desired protein isoforms.\nThe design space for splicing elements encompasses splice site sequences themselves (the canonical GT-AG dinucleotides and surrounding intronic and exonic enhancers and silencers), branch point sequences, and auxiliary sequences that recruit splicing regulatory proteins. Foundation models that predict splicing patterns from local sequence context serve as oracles for evaluating candidate modifications, while gradient-based optimization identifies changes predicted to shift splicing toward therapeutic outcomes.\nDesign of splicing modulators requires particular attention to off-target effects. The splicing code is highly context-dependent, and sequence modifications intended to affect one splice site may inadvertently alter recognition of others. Genome-wide splicing models that predict effects across all splice sites provide essential off-target assessment, flagging candidate designs that would disrupt normal splicing at unintended locations.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-mrna-design",
    "href": "part_6/p6-ch30-design.html#sec-ch30-mrna-design",
    "title": "30  Sequence Design",
    "section": "30.4 mRNA Design and Optimization",
    "text": "30.4 mRNA Design and Optimization\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider what you learned about protein design in Section 30.2. How does mRNA design differ fundamentally? What additional constraints does an mRNA therapeutic face compared to a designed protein?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nKey differences: (1) mRNA must encode the same protein sequence (constrained by genetic code), limiting design to synonymous codon choices and UTRs; (2) mRNA faces immune recognition as a foreign nucleic acid, requiring evasion strategies; (3) mRNA degrades rapidly, requiring stability optimization; (4) mRNA must be manufactured and delivered, adding constraints proteins don’t face. Protein design optimizes amino acid sequence directly; mRNA design optimizes the nucleotide encoding while keeping the protein constant.\n\n\n\n\n\nThe clinical success of mRNA vaccines has intensified interest in systematic approaches to mRNA sequence design. Unlike protein or regulatory element design where the primary challenge is achieving desired function, mRNA design must simultaneously optimize translation efficiency, molecular stability, immune evasion, and manufacturing tractability. Foundation models increasingly contribute to each of these objectives.\n\n\n\n\n\n\nmRNA therapeutic design optimization\n\n\n\n\nFigure 30.4: mRNA therapeutic design optimization. Each region requires distinct considerations: 5’ UTR affects translation initiation (Kozak sequence, structure minimization); coding sequence requires codon optimization balanced against GC content and immunogenicity; 3’ UTR controls stability and localization; chemical modifications (N1-methylpseudouridine) reduce immunogenicity. Foundation models predict translation efficiency, stability, and immunogenic potential. COVID-19 mRNA vaccines demonstrate successful integration of these design principles.\n\n\n\n\n30.4.1 Codon Optimization Principles\n\n\n\n\n\n\nStop and Think\n\n\n\nBefore diving into codon optimization, recall the genetic code’s degeneracy. For a 100-amino-acid protein, roughly how many different mRNA sequences could encode the same protein? Why does this create both an opportunity and a challenge for mRNA design?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nMost amino acids have 2-6 synonymous codons (Met and Trp have only 1). For 100 amino acids with ~3 average synonymous options per position, there are roughly 3^100 ≈ 10^47 possible encodings—an astronomical number. This creates opportunity because we can search for optimal encodings, but also creates a challenge because the search space is impossibly large, requiring smart optimization strategies rather than exhaustive search.\n\n\n\n\n\nThe genetic code is degenerate: sixty-one sense codons encode twenty amino acids, meaning that any protein sequence can be encoded by many different mRNA sequences. These synonymous sequences differ in translation efficiency, mRNA stability, and immunogenicity despite producing identical proteins. Codon optimization exploits this redundancy to improve therapeutic mRNA performance.\n\n\n\n\n\n\nKey Insight: The Hidden Complexity of Synonymous Mutations\n\n\n\nAlthough synonymous codons produce identical proteins, the mRNA sequences differ in ways that profoundly affect therapeutic outcomes. A single synonymous mutation can alter: (1) translation speed at that position, (2) mRNA secondary structure affecting stability, (3) recognition by innate immune sensors, and (4) ribosome pausing that affects co-translational folding. Codon optimization must navigate all these effects simultaneously.\n\n\nTraditional codon optimization relied on codon adaptation indices derived from highly expressed genes in target organisms. Codons frequently used in abundant proteins were assumed to be efficiently translated, leading to optimization strategies that maximize use of preferred codons. This approach oversimplifies the complex relationship between codon choice and expression. Translation elongation rate varies with codon-anticodon interactions, tRNA abundance, mRNA secondary structure, and ribosome queuing effects. Local codon context matters: rare codons following abundant ones may be translated efficiently, while runs of preferred codons can cause ribosome collisions.\nMachine learning models trained on ribosome profiling data and reporter assays have begun to capture these context-dependent effects. [Citation Needed] These models predict translation efficiency from sequence features including codon frequencies, local secondary structure, and amino acid properties. Using such models as oracles, optimization algorithms can search for mRNA sequences that maximize predicted translation while avoiding problematic sequence features. The resulting designs often differ substantially from simple codon-frequency optimization, incorporating rare codons at specific positions to optimize local translation dynamics.\n\n\n30.4.2 Stability Engineering and UTR Design\n\n\n\n\n\n\nStop and Think\n\n\n\nThink back to regulatory element design (Section 30.3). How is UTR design similar to enhancer/promoter design? How is it different? What properties must a 5’ UTR balance that a promoter does not?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nSimilarities: Both are non-coding regulatory sequences where foundation models predict activity; both use gradient-based or generative design approaches. Key differences: (1) UTRs are transcribed into RNA (must consider RNA structure, not DNA), (2) 5’ UTRs must balance ribosome recruitment (high translation) against secondary structure that blocks scanning (low translation), (3) 3’ UTRs affect mRNA half-life via RNA-binding protein sites, while promoters don’t face degradation. UTRs operate post-transcriptionally; promoters control transcription initiation.\n\n\n\n\n\nmRNA stability in the cytoplasm determines the duration of protein production and thus the dose required for therapeutic effect. Stability is governed by multiple sequence features: the 5’ and 3’ untranslated regions (UTRs) that flank the coding sequence, the presence of destabilizing sequence motifs recognized by RNA-binding proteins, and secondary structures that protect against or expose the molecule to nucleases.\nUTR engineering represents a particularly active area of foundation model application. Natural UTRs contain binding sites for regulatory proteins and microRNAs, sequences that affect ribosome recruitment, and structures that influence mRNA localization and stability. Foundation models trained on expression data across diverse UTR sequences learn which features promote stability and efficient translation. [Citation Needed] Design algorithms then search for synthetic UTRs that maximize these properties while avoiding sequences that trigger immune recognition or rapid degradation.\nChemical modifications of mRNA (pseudouridine, N1-methylpseudouridine, and other nucleoside analogs) dramatically improve stability and reduce immunogenicity. These modifications alter the sequence-function relationship in ways that current foundation models, trained primarily on natural RNA, may not fully capture. Emerging models that incorporate modification information promise to enable joint optimization of sequence and modification patterns.\n\n\n30.4.3 Immunogenicity Considerations\n\n\n\n\n\n\nStop and Think\n\n\n\nRecall multi-objective optimization from protein design (Section 30.2.3). For an mRNA therapeutic, you need to balance: (1) high translation efficiency, (2) long mRNA half-life, (3) low immunogenicity, and (4) manufacturability. Why can’t you simply maximize each property independently? What is the likely tradeoff between translation efficiency and immunogenicity?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThese objectives create competing constraints: (1) High GC content can improve stability but increases immunogenicity via TLR recognition; (2) Rare codons reduce translation but may reduce immune detection; (3) Strong secondary structures protect from nucleases but can block ribosome scanning; (4) Chemical modifications reduce immunogenicity but increase manufacturing cost. Translation efficiency often requires features (abundant codons, specific motifs) that immune sensors recognize. Optimization must find Pareto-optimal solutions balancing these tradeoffs, not single-objective maxima.\n\n\n\n\n\nExogenous mRNA triggers innate immune responses through pattern recognition receptors including Toll-like receptors (TLR3, TLR7, TLR8) and cytosolic sensors (RIG-I, MDA5). While some immune activation may be beneficial for vaccine applications, excessive inflammation limits dosing and causes adverse effects. For protein replacement therapies where repeated dosing is required, minimizing immunogenicity is essential.\nThe immunostimulatory potential of mRNA depends on sequence features including GC content, specific sequence motifs recognized by pattern receptors, and secondary structures that resemble viral replication intermediates. Foundation models that predict immunogenicity from sequence enable design of mRNAs that evade innate immune detection. [Citation Needed] These predictions must be balanced against other objectives: modifications that reduce immunogenicity may also reduce translation efficiency, creating multi-objective trade-offs that characterize mRNA design more broadly.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-antibody-vaccine",
    "href": "part_6/p6-ch30-design.html#sec-ch30-antibody-vaccine",
    "title": "30  Sequence Design",
    "section": "30.5 Antibody and Vaccine Design",
    "text": "30.5 Antibody and Vaccine Design\nAntibody engineering represents one of the most commercially significant applications of computational protein design. The modular architecture of antibodies (framework regions that maintain structural integrity surrounding hypervariable complementarity-determining regions (CDRs) that mediate antigen recognition) creates a well-defined design problem: optimize CDR sequences to achieve desired binding properties while maintaining framework stability and developability.\n\n\n\n\n\n\nAntibody design with foundation models\n\n\n\n\nFigure 30.5: Antibody design with foundation models. The antibody structure comprises framework regions (scaffold) and CDR loops (binding interface). Design challenges include affinity maturation (improve binding), specificity (reduce off-targets), humanization (reduce immunogenicity), and developability (ensure manufacturability). Foundation models contribute binding affinity prediction from sequence, humanness scoring against human antibody corpus, and biophysical property prediction (aggregation, stability). Iterative design cycles alternate between FM-guided mutagenesis and experimental validation.\n\n\n\n\n30.5.1 CDR Optimization and Humanization\nAntibodies discovered through animal immunization or phage display often require optimization before therapeutic use. Non-human framework sequences may trigger immune responses in patients, necessitating humanization that replaces framework residues with human equivalents while preserving antigen binding. CDR sequences may require affinity maturation to achieve therapeutic potency or specificity optimization to reduce off-target binding.\nFoundation models support antibody optimization through multiple mechanisms. Antibody-specific language models trained on paired heavy and light chain sequences learn the structural and functional constraints on CDR sequences. [Citation Needed] These models predict which mutations are compatible with the antibody fold and which are likely to disrupt structure. Given a parental antibody sequence, the models can propose libraries of variants enriched for functional candidates, reducing the experimental screening burden required to identify improved variants.\nStructure-aware approaches enable more targeted design. Given a structure of the antibody-antigen complex (determined experimentally or predicted computationally via methods discussed in Section 15.4), optimization focuses on residues at the binding interface. Computational saturation mutagenesis predicts the effect of every possible amino acid substitution at each interface position, identifying combinations expected to improve affinity. These predictions guide the construction of focused libraries that explore the most promising region of sequence space.\n\n\n30.5.2 Vaccine Antigen Design\nVaccine development increasingly employs computational design to create immunogens that elicit protective immune responses. The challenge differs from therapeutic protein design: rather than optimizing for direct biological activity, vaccine antigens must be recognized by the immune system and induce antibodies or T cells that protect against pathogen challenge.\nFoundation models contribute to vaccine design in several ways. Epitope prediction models identify regions of pathogen proteins most likely to be recognized by antibodies or T cells, guiding selection of vaccine targets. Structural models predict how mutations affect epitope conformation, enabling design of stabilized antigens that maintain native epitope structure during manufacturing and storage. Glycan shielding analysis predicts which epitopes will be accessible on the pathogen surface versus hidden by glycosylation, focusing vaccine design on exposed regions.\nThe rapid development of mRNA vaccines against SARS-CoV-2 demonstrated the potential of computational approaches to accelerate vaccine design. Structure-guided stabilization of the prefusion spike conformation, optimization of mRNA sequences for expression and stability, and prediction of variant effects on vaccine efficacy all benefited from computational modeling. [Citation Needed] Future vaccine development will increasingly integrate foundation model predictions throughout the design process.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-dbtl",
    "href": "part_6/p6-ch30-design.html#sec-ch30-dbtl",
    "title": "30  Sequence Design",
    "section": "30.6 Closed-Loop Design-Build-Test-Learn Cycles",
    "text": "30.6 Closed-Loop Design-Build-Test-Learn Cycles\nFoundation models achieve their full potential when integrated into iterative experimental workflows. The design-build-test-learn (DBTL) paradigm treats computational predictions as hypotheses to be tested experimentally, with results feeding back to improve both the designed molecules and the models that guide design. This closed-loop approach connects to the lab-in-the-loop concepts introduced in ?sec-ch27-lab-in-loop.\n\n\n\n\n\n\nDesign-Build-Test-Learn cycle for sequence engineering\n\n\n\n\nFigure 30.6: Design-Build-Test-Learn cycle for sequence engineering. Design: foundation models score candidates and optimization identifies promising sequences. Build: DNA synthesis and assembly create physical constructs. Test: functional assays measure actual performance. Learn: results update models for next cycle. Each iteration improves both designed sequences and predictive accuracy. Bottleneck reality: Build and Test phases (days to weeks) limit iteration speed despite fast computational Design (~seconds). Closed-loop automation increasingly accelerates the cycle.\n\n\n\n\n30.6.1 Active Learning for Efficient Exploration\n\n\n\n\n\n\nStop and Think\n\n\n\nImagine you have a budget to experimentally test 100 protein variants, but your foundation model proposes 10,000 candidates. Some candidates have high predicted fitness but the model is uncertain; others have moderate predictions but high confidence. How would you decide which 100 to test? What are the tradeoffs between “exploiting” high predictions versus “exploring” uncertain regions?\n\n\nExperimental validation remains the bottleneck in biological design. Even high-throughput assays can test at most thousands to millions of variants, a tiny fraction of possible sequences. Active learning strategies select which experiments to perform by balancing two competing objectives: exploiting current model predictions to test sequences likely to succeed, and exploring regions of uncertainty to gather data that will improve the model.\nBayesian optimization provides a principled framework for this trade-off. A surrogate model (typically a Gaussian process or ensemble neural network) approximates the sequence-to-fitness mapping. Acquisition functions such as expected improvement or upper confidence bound combine predicted function values with uncertainty estimates to select informative test sequences. The expected improvement acquisition function, for example, computes the probability-weighted average improvement over the current best sequence, naturally balancing regions of high predicted fitness (likely to improve) against regions of high uncertainty (potentially hiding superior solutions). Upper confidence bound adds a tunable exploration parameter that explicitly controls how much to favor uncertain regions. After each experimental round, the surrogate model is updated with new data, and the process repeats. This iterative refinement concentrates experimental resources on the most promising and informative regions of sequence space rather than uniformly sampling the combinatorially vast possibilities.\nFoundation models enhance active learning by providing informative priors and features. Rather than learning sequence-to-function mappings from scratch, surrogate models can operate on protein language model embeddings that capture evolutionary relationships and structural constraints. These embeddings provide a meaningful notion of sequence similarity even before any task-specific data is available, accelerating the early rounds of optimization when labeled data is scarce.\n\n\n\n\n\n\nPractical Guidance: Choosing an Active Learning Strategy\n\n\n\nConsider these factors when selecting an active learning approach:\n\nWhen labeled data is scarce: Use foundation model embeddings as features for your surrogate model; they provide useful priors even with few labels\nWhen experimental costs are high: Favor acquisition functions that emphasize exploration (e.g., upper confidence bound with high exploration parameter) to maximize information gain per experiment\nWhen you need quick wins: Favor exploitation-heavy strategies that test sequences with highest predicted fitness, accepting that you may miss better optima\nWhen model reliability is uncertain: Use ensemble disagreement as an additional uncertainty measure; avoid testing sequences where all models confidently agree (may be exploiting shared artifacts)\n\n\n\n\n\n30.6.2 High-Throughput Experimentation Integration\nModern experimental platforms generate data at scales well-matched to foundation model training. Deep mutational scanning (DMS) systematically characterizes thousands of single-mutant variants of a protein, mapping the functional landscape around a parental sequence (see Section 2.4.4 for discussion of DMS data resources). Massively parallel reporter assays test tens of thousands of regulatory element variants in a single experiment. CRISPR screens introduce perturbations across the genome and measure phenotypic consequences.\nThese assays generate dense local maps of sequence-function relationships that complement the global patterns captured by foundation models. The integration is bidirectional: model predictions prioritize which variants to include in experimental libraries, and experimental results fine-tune models for improved accuracy in relevant sequence neighborhoods. After several DBTL cycles, the combined system (fine-tuned model plus accumulated experimental data) can often design sequences that substantially outperform the parental molecule.\nThe design of experiments themselves benefits from computational guidance. Rather than testing all possible single mutants, active learning identifies the most informative subset. Rather than random library construction, computational analysis identifies epistatic interactions that should be explored through combinatorial variants. The cost of DNA synthesis and high-throughput assays makes efficient experimental design increasingly important as design ambitions grow.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-validation",
    "href": "part_6/p6-ch30-design.html#sec-ch30-validation",
    "title": "30  Sequence Design",
    "section": "30.7 Validation Requirements and Failure Modes",
    "text": "30.7 Validation Requirements and Failure Modes\nComputational design generates hypotheses; experimental validation determines whether those hypotheses are correct. The gap between predicted and observed performance represents the ultimate test of design methods, and understanding where predictions fail is essential for improving both models and design strategies. The evaluation principles discussed in Chapter 11 and uncertainty quantification from Chapter 23 apply directly to design validation.\n\n30.7.1 Validation Hierarchy\nDesigned sequences must pass through multiple validation stages before achieving real-world impact. Computational validation confirms that designs satisfy specified constraints and achieve predicted scores, filtering obvious failures before synthesis. In vitro validation tests whether designed proteins express, fold, and exhibit predicted activities in simplified experimental systems. In vivo validation assesses function in cellular or animal contexts where additional complexity may reveal unanticipated problems. Clinical validation, for therapeutic applications, determines whether designs are safe and effective in human patients.\nSuccess rates decline at each stage of this hierarchy. Computationally promising designs often fail to express or fold correctly. Designs that succeed in vitro may lose activity in cellular contexts due to incorrect localization, unexpected degradation, or off-target interactions. Molecules that perform well in model organisms may fail in human clinical trials due to immunogenicity, toxicity, or pharmacokinetic limitations. The attrition from computational design to clinical success remains substantial, motivating continued improvement in predictive accuracy and earlier identification of failure modes.\n\n\n30.7.2 Characteristic Failure Patterns\n\n\n\n\n\n\nCommon Pitfalls\n\n\n\nPractitioners should be aware of these systematic failure modes in model-guided design:\n\nDistribution shift: Optimization pushes sequences into regions where model predictions are unreliable\nMode collapse: Generative models produce variants of training sequences rather than genuinely novel molecules\nReward hacking: Optimization exploits model artifacts rather than genuine sequence-function relationships\nMissing properties: Models cannot predict properties absent from training data (e.g., aggregation under manufacturing conditions)\n\nMitigation strategies include ensemble methods, novelty filters, uncertainty quantification, and experimental validation in application-relevant conditions.\n\n\nFoundation model-guided design exhibits systematic failure modes that practitioners must recognize and mitigate. Distribution shift occurs when optimization pushes sequences into regions where model predictions are unreliable (Section 11.7.1 for detailed discussion of distribution shift in genomic models; Section 23.6 for detection methods). A model trained on natural proteins may produce confident but incorrect predictions for designed sequences that lie far from training data. Regularization toward natural sequence statistics and uncertainty quantification help identify when designs have strayed beyond reliable prediction regimes.\nMode collapse in generative models produces designs that are variants of training sequences rather than genuinely novel molecules. When generated sequences can be matched to close homologs in training data, the design process has failed to create anything new. Novelty filters and diversity requirements during generation help ensure that computational design adds value beyond database retrieval.\nReward hacking occurs when optimization exploits model artifacts rather than genuine sequence-function relationships. A model might predict high expression for sequences containing spurious features that happen to correlate with expression in training data but have no causal effect. Ensemble methods, where designs must score highly across multiple independently trained models, provide some protection against hacking individual model weaknesses.\nThe most insidious failures involve properties that models cannot predict because they were absent from training data. A designed protein might aggregate under manufacturing conditions never encountered during model development. A regulatory element might be silenced by chromatin modifications specific to the therapeutic context. These failures can only be identified through experimental validation in relevant conditions, motivating the closed-loop DBTL approach that continuously tests designs in application-relevant settings.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-practical-constraints",
    "href": "part_6/p6-ch30-design.html#sec-ch30-practical-constraints",
    "title": "30  Sequence Design",
    "section": "30.8 Practical Design Constraints",
    "text": "30.8 Practical Design Constraints\nBeyond achieving desired function, practical design must satisfy numerous constraints arising from manufacturing, safety, and deployment requirements.\n\n30.8.1 Manufacturing and Developability\nDesigned proteins must be producible at scale in expression systems such as bacteria, yeast, or mammalian cells. Expression levels, solubility, and purification behavior determine manufacturing feasibility and cost. Foundation models trained on expression data can predict which sequences are likely to express well, enabling design pipelines that optimize not only for function but for manufacturability. [Citation Needed]\nFor therapeutic proteins, developability encompasses additional properties including stability during storage, compatibility with formulation requirements, and behavior during analytical characterization. Aggregation propensity, chemical degradation sites (oxidation, deamidation), and glycosylation patterns all affect developability. Computational tools increasingly predict these properties from sequence, enabling their incorporation as design constraints.\n\n\n30.8.2 Safety and Biosecurity Considerations\nThe same capabilities that enable beneficial design applications also raise biosecurity concerns. Generative models trained on pathogen sequences might in principle be used to design enhanced pathogens or reconstruct dangerous organisms. The dual-use potential of biological design technology requires ongoing attention to safety practices and governance frameworks.\nCurrent foundation models do not provide straightforward paths to bioweapon development; designing a functional pathogen requires capabilities far beyond predicting sequence properties. As models improve and integrate with automated synthesis and testing platforms, the barrier to misuse may decrease. Responsible development practices, including careful consideration of training data, model access policies, and monitoring for concerning use patterns, are essential components of the foundation model ecosystem. These considerations connect to the broader discussion of safety and ethics in Section 26.1.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-algorithms",
    "href": "part_6/p6-ch30-design.html#sec-ch30-algorithms",
    "title": "30  Sequence Design",
    "section": "30.9 Algorithmic Search and Optimization",
    "text": "30.9 Algorithmic Search and Optimization\nDesign algorithms must navigate vast sequence spaces to identify candidates with desired properties. Several algorithmic paradigms have proven effective, each with characteristic strengths and limitations.\n\n\n\n\n\n\nKnowledge Check\n\n\n\nFor each of the following design scenarios, which algorithmic approach would you choose and why?\n\nOptimizing a single position in an enzyme active site for catalytic activity\nDesigning a library of 10,000 diverse antibody variants for experimental screening\nFinding a sequence that maximizes binding affinity while maintaining stability above a threshold\nExploring the fitness landscape around a well-characterized parental protein\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nGradient-based optimization - single position allows exhaustive search of 20 amino acids, or gradient methods if using soft encodings. (2) Monte Carlo or evolutionary algorithms - generate diverse populations naturally; avoid mode collapse. (3) Constrained optimization or multi-objective evolutionary - explicit constraint handling for stability threshold. (4) Bayesian optimization - sample-efficient exploration when starting from known good sequence; balances exploitation and exploration naturally.\n\n\n\n\n\n\nGradient-based optimization treats foundation models as differentiable functions and computes gradients of objectives with respect to input sequence representations. Because sequences are discrete while gradients are continuous, optimization operates on relaxed representations (probability distributions over nucleotides or amino acids) that are projected back to discrete sequences for evaluation. The relaxation step is necessary because gradient descent requires continuous inputs: a “soft” one-hot encoding represents each position as a probability distribution over amino acids (e.g., 0.7 Ala, 0.2 Gly, 0.1 Ser) rather than a discrete choice, allowing gradients to flow and guide optimization. Projection to discrete sequences occurs either through argmax (selecting the highest-probability amino acid) or through stochastic sampling from the learned distribution. This approach efficiently navigates high-dimensional spaces but can produce adversarial sequences that exploit model weaknesses rather than achieving genuine biological function.\nEvolutionary algorithms maintain populations of candidate sequences that undergo mutation, recombination, and selection based on fitness scores from foundation model oracles or experimental assays. This approach naturally handles discrete sequence spaces and can maintain diversity to avoid local optima. Multi-objective evolutionary algorithms explicitly construct Pareto frontiers of solutions trading off competing objectives.\nBayesian optimization models the sequence-to-fitness mapping with a probabilistic surrogate (typically a Gaussian process or ensemble neural network) and uses acquisition functions to balance exploration of uncertain regions with exploitation of predicted optima. This approach is particularly effective when experimental evaluations are expensive and each design round must be carefully chosen.\nMonte Carlo methods sample sequences from distributions defined by foundation model likelihoods, optionally biased toward high-scoring regions through importance weighting or Markov chain Monte Carlo. These approaches naturally integrate foundation model priors with task-specific objectives and can generate diverse candidate sets for experimental screening.\nThe table below summarizes when to use each algorithmic approach.\n\n\n\nTable 30.3: Algorithm selection guide for sequence design.\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest When\nStrengths\nLimitations\n\n\n\n\nGradient-based\nDifferentiable oracle available; continuous relaxation feasible\nFast; high-dimensional\nAdversarial solutions; local optima\n\n\nEvolutionary\nNeed diversity; multi-objective\nHandles discrete spaces; Pareto fronts\nSlower convergence\n\n\nBayesian optimization\nExpensive experiments; need uncertainty\nSample-efficient; principled exploration\nScales poorly to high dimensions\n\n\nMonte Carlo\nFoundation model provides good prior; want library diversity\nNatural uncertainty; diverse outputs\nMay be slow to find optima\n\n\n\n\n\n\nThe choice among algorithmic approaches depends on the specific design problem, available computational resources, and experimental constraints. Many practical pipelines combine multiple approaches: generative sampling to produce initial candidate pools, gradient-based refinement to optimize specific objectives, and active learning to select informative experimental tests.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-generative-evaluation",
    "href": "part_6/p6-ch30-design.html#sec-ch30-generative-evaluation",
    "title": "30  Sequence Design",
    "section": "30.10 Evaluating Generative Design",
    "text": "30.10 Evaluating Generative Design\nAssessing whether a generative model produces useful designs requires metrics that capture multiple dimensions of quality. Unlike discriminative models evaluated by accuracy on held-out data, generative models must produce outputs that are simultaneously novel (not merely retrieving training examples), valid (satisfying basic biological constraints), diverse (exploring the design space rather than collapsing to narrow modes), and functional (achieving desired biological properties). No single metric captures all these requirements; comprehensive evaluation demands a suite of complementary assessments.\n\n30.10.1 Computational Quality Metrics\nPerplexity and likelihood measure how well generated sequences match the statistical patterns of natural biology. A generative model trained on natural proteins should assign higher likelihood to generated sequences that resemble natural ones. Perplexity (the exponentiated average negative log-likelihood) provides a scalar summary: lower perplexity indicates that generated sequences appear more natural according to the model’s learned distribution. However, low perplexity alone does not guarantee functional designs; a model might generate highly probable but biologically inert sequences that closely mimic common motifs without capturing rare functional features.\nNovelty quantifies how different generated sequences are from training data. Sequence identity to nearest training neighbors provides a simple measure: sequences with less than 30% identity to any training protein clearly represent novel designs. More sophisticated approaches compute distances in embedding space, identifying generated sequences that occupy regions unrepresented in training data. The challenge lies in balancing novelty against validity: sequences too similar to training data offer limited design value, while sequences too different may fail to fold or function.\nDiversity measures the variety within a set of generated sequences. Internal diversity metrics quantify pairwise distances within generated batches; low diversity indicates mode collapse where the model repeatedly generates similar sequences. Coverage metrics assess what fraction of the natural sequence space is represented by generated samples. Diversity matters for practical applications: a design campaign benefits from exploring multiple solutions rather than converging on a single candidate, since experimental validation will reveal unpredicted failures among computationally promising designs.\nValidity assesses whether generated sequences satisfy basic biological constraints. For proteins, validity might require that sequences are predicted to fold (using AlphaFold2 or ESMFold structure prediction), contain no forbidden amino acid patterns, and have appropriate length distributions. For regulatory elements, validity might require balanced GC content, absence of restriction enzyme sites, and compatibility with delivery vectors. Validity filters identify obvious failures before expensive experimental testing, but passing validity checks does not guarantee function.\n\n\n\n\n\n\nEvaluating generative sequence models\n\n\n\n\nFigure 30.7: Evaluating generative sequence models. Four dimensions: Diversity measures variation among outputs (avoiding mode collapse); Novelty assesses distance from training examples (avoiding memorization); Validity checks structural correctness (foldable, proper chemistry); Function measures experimental performance (ultimate test). These metrics can trade off—highly novel sequences may have lower validity. Critical insight: computational metrics provide useful screening, but only experimental validation confirms that generated sequences actually function as intended.\n\n\n\n\n\n30.10.2 Functional Assessment\nComputational metrics provide necessary but insufficient evidence of design success. The ultimate test is whether generated sequences achieve their intended biological function, which requires experimental validation that can only partially be predicted computationally.\nStructure prediction offers an intermediate level of assessment between purely computational metrics and experimental validation. AlphaFold2 and ESMFold predict whether generated protein sequences fold into well-defined structures, with predicted local distance difference test (pLDDT) scores providing residue-level confidence estimates (Section 15.4). Designs with low predicted confidence likely fail to fold correctly. For generated regulatory elements, models like Enformer predict expression levels and chromatin state, providing functional estimates without wet-lab experiments. These predictions inherit the limitations of the underlying models: they may overestimate success for sequences that exploit model artifacts rather than achieving genuine biological function.\nOracle model evaluation uses trained predictors to estimate functional properties of generated sequences. Binding affinity predictors assess designed antibodies; stability predictors evaluate protein designs; expression models score regulatory elements. When oracle models are distinct from the generative model, this evaluation provides independent evidence of quality. However, oracle models themselves have limited accuracy, particularly for sequences far from their training distributions. A design might score highly on an oracle that has never seen similar sequences, yet fail experimentally.\nExperimental success rates provide ground truth that computational metrics can only approximate. Published design studies report widely varying success rates depending on the design target and evaluation criteria. De novo protein design achieves expression rates of 30-70% for well-designed sequences, with functional activity observed in 5-30% of expressed candidates (huang_coming_2016?). Designed antibodies targeting challenging epitopes may yield functional binders from 1-10% of tested sequences. Regulatory element design success rates vary enormously depending on the complexity of the specification and the stringency of activity requirements.\nThese success rates reflect the combined limitations of generative models, oracle predictors, and experimental systems. Improving any component, whether by training better generative priors, developing more accurate oracles, or refining experimental assays, can increase overall design success. The closed-loop DBTL approach (Section 30.6) systematically addresses these limitations by using experimental failures to improve models for subsequent design rounds.\n\n\n30.10.3 Benchmarking Generative Models\nStandardized benchmarks enable comparison across generative approaches, though constructing appropriate benchmarks for design presents unique challenges. Unlike prediction benchmarks where held-out data provides unambiguous ground truth, design benchmarks must assess open-ended generation where many valid solutions exist.\nRetrospective benchmarks evaluate whether models can recover known functional sequences when given appropriate conditioning. Given a protein structure, can an inverse folding model generate a sequence that folds to that structure? Given a desired expression profile, can a regulatory model generate an element that achieves it? These evaluations test necessary capabilities but may not predict performance on genuinely novel design targets where the correct answer is unknown.\nProspective experimental validation provides the strongest benchmark but requires substantial resources. Community efforts like CASP (Critical Assessment of Structure Prediction) have driven progress in prediction; analogous competitions for design could similarly accelerate the field. Current efforts to establish design benchmarks include collections of deep mutational scanning data for evaluating predicted fitness landscapes and standardized assays for comparing designed proteins to natural sequences.\nMeta-evaluation assesses whether computational metrics predict experimental outcomes. If high novelty correlates with experimental failure while low perplexity correlates with success, practitioners can use computational metrics to prioritize candidates. Establishing these correlations requires accumulating paired computational-experimental data across diverse design campaigns. As more groups publish both metrics and validation results, the field develops better understanding of which computational assessments matter for practical success.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch30-design.html#sec-ch30-conclusion",
    "href": "part_6/p6-ch30-design.html#sec-ch30-conclusion",
    "title": "30  Sequence Design",
    "section": "30.11 From Understanding to Creating",
    "text": "30.11 From Understanding to Creating\nSequence design represents the frontier where foundation models transition from tools for understanding biology to engines for creating it. The field has advanced from designing individual stable proteins to engineering complex molecular machines, from optimizing isolated regulatory elements to programming cellular behavior, from incremental improvement of existing sequences to de novo creation of functions not found in nature. The constraints of natural evolution no longer bound the sequences we can consider; the statistical patterns of existing biology provide priors that guide exploration of novel territory.\nThe validation bottleneck persists as perhaps the most fundamental limitation. Computational design can propose candidates faster than experiments can test them, creating pressure to improve both predictive accuracy (reducing false positives that waste experimental resources) and experimental throughput (enabling more designs to be evaluated). Automated laboratories, standardized assay platforms, and improved experimental design methods all contribute to accelerating the design-build-test-learn cycle, but the gap between computational proposal and experimental validation remains substantial.\nThe transition from prediction to design amplifies both the potential benefits and the risks of these technologies. A model that predicts protein function enables analysis; a model that designs protein function enables creation. Ensuring that designed biology serves human flourishing while minimizing potential harms requires not just technical advances but thoughtful governance, inclusive deliberation about applications, and ongoing attention to safety. These broader considerations connect sequence design to regulatory, ethical, and societal dimensions (Section 26.1), where the technical capabilities developed throughout genomic AI meet the human systems that will determine how they are used.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhy is the design problem fundamentally harder than the prediction problem? What changes when you invert from “sequence → function” to “function → sequence”?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nPrediction evaluates one sequence (computable in milliseconds); design must search astronomically large spaces (20^200 for a 200-residue protein). Prediction stays near training data where models are accurate; design optimizes toward extremes where models become unreliable (distribution shift). Prediction has ground truth for validation; design creates novel sequences with unknown true function, requiring expensive experimental validation.\n\n\n\n\nCompare sequence-based protein design (using language models) with structure-aware design (using diffusion and inverse folding). What are the advantages of each approach?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nSequence-based (PLMs): Advantages include computational efficiency, strong priors from evolutionary data, good for variant optimization. Limitations: restricted to known fold families. Structure-aware (RFdiffusion + ProteinMPNN): Advantages include ability to generate entirely novel topologies, target specific binding geometries, design for function-first. Limitations: higher computational cost, requires structural specification. Choose PLMs for optimizing existing proteins, structure-aware for de novo creation.\n\n\n\n\nWhat are the four key dimensions for evaluating generative sequence models (novelty, validity, diversity, functionality)? Why is no single metric sufficient?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nNovelty: distance from training data (avoid memorization); (2) Validity: satisfies basic constraints (foldable, correct chemistry); (3) Diversity: variation among outputs (avoid mode collapse); (4) Functionality: achieves intended purpose (ultimate test). These can trade off—highly novel sequences may have lower validity. High validity doesn’t guarantee function. High diversity is useless if all designs fail. Comprehensive evaluation requires assessing all dimensions.\n\n\n\n\n\nDescribe a design-build-test-learn cycle for protein engineering. How do foundation models contribute at each stage, and where does the bottleneck typically occur?\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nDesign: FM scores candidates, optimization identifies promising sequences (seconds-minutes). Build: DNA synthesis and assembly create physical constructs (days). Test: expression and functional assays measure performance (days-weeks). Learn: results update models for next cycle. FMs contribute in Design (generative priors, fitness prediction) and Learn (incorporating experimental data to improve predictions). Bottleneck: Build-Test phases (days-weeks) vs. fast Design (seconds), limiting iteration speed.\n\n\n\n\nWhat characteristic failure modes should you watch for in model-guided design (distribution shift, mode collapse, reward hacking)? Give an example of each.\n\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nDistribution shift: optimization pushes into regions where FM predictions are unreliable (e.g., designed protein with 15% identity to any training sequence—model never saw such distant sequences). Mode collapse: generative model produces minor variants of training sequences rather than novel designs (all generated antibodies are &gt;95% identical to natural sequences in database). Reward hacking: optimization exploits model artifacts (e.g., promoter design finds sequences with spurious features correlating with high expression in training data but having no causal effect).\n\n\n\n\n\nThis chapter explored how foundation models enable the transition from predicting sequence function to designing sequences with desired properties.\nKey Topics Covered:\n\nDesign formalism: Mathematical frameworks (optimization, conditional generation, constrained optimization) that formalize the inverse prediction problem\nProtein design: Sequence-based approaches using language models and structure-aware methods using diffusion and inverse folding\nRegulatory design: Engineering promoters, enhancers, and splicing elements using gradient-based optimization and generative models\nmRNA optimization: Balancing translation efficiency, stability, and immunogenicity through codon and UTR design\nClosed-loop workflows: Design-Build-Test-Learn cycles with active learning for efficient experimental exploration\nValidation and failure modes: Understanding distribution shift, mode collapse, and reward hacking in model-guided design\nGenerative evaluation: Metrics for novelty, validity, diversity, and functionality in design assessment\n\nKey Takeaways:\n\nDesign inverts prediction: instead of asking “what does this sequence do?”, we ask “what sequence achieves this function?” This inversion exposes model limitations invisible during prediction.\nStructure provides a powerful intermediate representation for protein design, constraining the search from astronomical sequence space to physically realizable geometries.\nClosed-loop DBTL cycles are essential because computational design generates hypotheses, not certainties. Models improve through iterative experimental feedback.\n\nLooking Ahead:\n\nChapter 31 explores emerging directions in genomic AI, including many-body effects, temporal dynamics, and increasingly integrated models\nThe biosecurity considerations raised here connect to the broader governance discussion in Section 26.1\nThe lab-in-the-loop concepts from ?sec-ch27-lab-in-loop provide the experimental infrastructure for scaling design campaigns\n\n\n\n\n\n\n\nBoer, Carl G. de, Eeshit Dhaval Vaishnav, Ronen Sadeh, Esteban Luis Abeyta, Nir Friedman, and Aviv Regev. 2020. “Deciphering Eukaryotic Gene-Regulatory Logic with 100 Million Random Promoters.” Nature Biotechnology 38 (1): 56–65. https://doi.org/10.1038/s41587-019-0315-8.\n\n\nDauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. “Robust Deep Learning–Based Protein Sequence Design Using ProteinMPNN.” Science 378 (6615): 49–56. https://doi.org/10.1126/science.add2187.\n\n\nFerruz, Noelia, Steffen Schmidt, and Birte Höcker. 2022. “ProtGPT2 Is a Deep Unsupervised Language Model for Protein Design.” Nature Communications 13 (1): 4348. https://doi.org/10.1038/s41467-022-32007-7.\n\n\nHsu, Chloe, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. 2022. “Learning Inverse Folding from Millions of Predicted Structures.” In Proceedings of the 39th International Conference on Machine Learning, 8946–70. PMLR. https://proceedings.mlr.press/v162/hsu22a.html.\n\n\nMadani, Ali, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P. Mohr, James M. Holton, Jose Luis Olmos, et al. 2023. “Large Language Models Generate Functional Protein Sequences Across Diverse Families.” Nature Biotechnology 41 (8): 1099–1106. https://doi.org/10.1038/s41587-022-01618-2.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. “De Novo Design of Protein Structure and Function with RFdiffusion.” Nature 620 (7976): 1089–1100. https://doi.org/10.1038/s41586-023-06415-8.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Sequence Design</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch31-frontiers.html",
    "href": "part_6/p6-ch31-frontiers.html",
    "title": "31  Frontiers and Synthesis",
    "section": "",
    "text": "31.1 Open Technical Problems\nThe technical challenges surveyed in preceding chapters remain only partially solved. Foundation models for genomics have demonstrated remarkable capabilities, but they operate far below theoretical limits and fail in ways that better architectures, training strategies, or data could address. Three challenges stand out as particularly important for the field’s trajectory: scaling models to capture biological complexity, integrating information across biological scales, and moving from correlation to causal and mechanistic understanding. Progress on any of these fronts would unlock applications currently beyond reach.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Frontiers and Synthesis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch31-frontiers.html#sec-ch31-technical",
    "href": "part_6/p6-ch31-frontiers.html#sec-ch31-technical",
    "title": "31  Frontiers and Synthesis",
    "section": "",
    "text": "Stop and Think\n\n\n\nBefore reading about specific technical challenges, reflect on your own experience with genomic models (or machine learning models more broadly): What are the most frustrating limitations you have encountered? Are they due to insufficient model capacity, wrong training data, inappropriate evaluation, or something else entirely? Keep your answer in mind as you read this section.\n\n\n\n31.1.1 Scaling and Efficiency\nThe largest foundation models in natural language processing now exceed a trillion parameters and were trained on trillions of tokens (Fedus, Zoph, and Shazeer 2022; Chowdhery et al. 2022). Genomic foundation models remain substantially smaller, with typical models ranging from hundreds of millions to low billions of parameters. Whether genomic applications require comparable scale remains uncertain. The human genome spans 3 billion base pairs and encompasses perhaps 20,000 protein-coding genes, a smaller and more constrained space than natural language. But capturing the full complexity of gene regulation, protein structure, and cellular context may require parameter counts that approach or exceed language model scale.\nScaling genomic foundation models faces several bottlenecks. Training data availability constrains scale when models exhaust unique sequences and must rely on data augmentation or repetition. Compute costs remain prohibitive for most academic groups and limit experimentation with truly large architectures. Long sequence lengths required for genomic context (regulatory elements can span hundreds of kilobases) create quadratic attention costs that limit practical context windows despite architectural innovations (see Chapter 7).\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore examining the table below, predict: What are the three or four major bottlenecks limiting genomic foundation model scaling, and what approaches might address each? Write down your predictions, then compare with Table 31.1.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe four major bottlenecks are: (1) Training data - finite unique genomes, addressed through multi-species pretraining and synthetic data; (2) Compute cost - trillion-parameter models cost $10M+, addressed through sparse attention and distillation; (3) Context length - quadratic attention limits practical windows, addressed through linear-time architectures; (4) Evaluation - benchmarks saturate before biological problems solved, requiring task-specific validation. Each bottleneck requires different solutions trading off different constraints.\n\n\n\n\n\n\n\n\nTable 31.1: Scaling bottlenecks for genomic foundation models and emerging approaches to address them. Each bottleneck involves fundamental trade-offs that must be navigated empirically.\n\n\n\n\n\n\n\n\n\n\nBottleneck\nCurrent State\nPotential Solutions\n\n\n\n\nTraining data\nFinite unique genomes (~100k species with assemblies)\nMulti-species pretraining, synthetic data, data augmentation\n\n\nCompute cost\nTrillion-parameter models cost $10M+ to train\nSparse attention, state space models, knowledge distillation\n\n\nContext length\nQuadratic cost limits practical windows to ~100kb\nLinear-time architectures (Mamba), chunking strategies\n\n\nEvaluation\nBenchmarks saturate before biological problems solved\nTask-specific evaluation, clinical validation\n\n\n\n\n\n\n\n\n\n\n\n\nWorked Example: Scaling Laws for Variant Effect Prediction\n\n\n\nHow does model scale affect variant effect prediction performance? Data from multiple DNA language models illustrates diminishing returns:\n\n\n\n\n\n\n\n\n\nModel\nParameters\nClinVar missense AUC\nTraining compute (GPU-hours)\n\n\n\n\nDNABERT-S\n110M\n0.78\n~1,000\n\n\nNucleotide Transformer\n500M\n0.82\n~10,000\n\n\nNucleotide Transformer\n2.5B\n0.84\n~50,000\n\n\nHyenaDNA (long context)\n1.4B\n0.83\n~40,000\n\n\nEvo (multispecies)\n7B\n0.85\n~200,000\n\n\n\nKey observations:\n\nDiminishing returns: Moving from 110M to 500M parameters improves AUC by 0.04 (5× compute). Moving from 500M to 2.5B improves AUC by only 0.02 (5× compute). The marginal benefit per compute dollar decreases.\nArchitecture matters more than scale: HyenaDNA at 1.4B with long-context architecture achieves comparable performance to NT-2.5B with standard attention, suggesting architectural innovation may be more cost-effective than raw scaling.\nTask ceiling: All models plateau around 0.85 AUC on this benchmark, suggesting performance may be limited by label noise in ClinVar rather than model capacity.\nCross-species transfer helps: Evo’s multispecies pretraining achieves highest performance, suggesting data diversity matters alongside model size.\n\nThis pattern—diminishing returns with a task-dependent ceiling—differs from language model scaling laws where performance continues improving with scale. Genomic tasks may have lower information density or more fundamental data limitations.\n\n\n\n\n\n\n\n\nScaling laws for genomic foundation models\n\n\n\n\nFigure 31.1: Scaling laws for genomic foundation models differ from language model patterns. Left: Log-log plot of ClinVar missense AUC versus training compute (GPU-hours). Models show diminishing returns: 5× compute increase from DNABERT-S to NT-500M yields +0.04 AUC, but 5× more to NT-2.5B yields only +0.02. Right: Comparison with language model scaling (dashed line shows power-law improvement). Genomic tasks plateau around 0.85 AUC regardless of scale, suggesting performance may be limited by label noise in ClinVar rather than model capacity. Key insight: architecture innovation (HyenaDNA’s long context) may be more cost-effective than raw scaling. Evo’s multispecies pretraining achieves highest performance, suggesting data diversity matters alongside model size.\n\n\n\nEfficiency improvements that reduce compute requirements without sacrificing capability are thus particularly valuable for genomic applications. Approaches include sparse attention patterns that avoid full quadratic costs, state space models that process sequences in linear time (Gu and Dao 2024), knowledge distillation that transfers capability from large models to smaller ones, and quantization that reduces precision requirements for inference (see Appendix B). Sparse attention achieves efficiency by computing attention only between nearby tokens or predetermined patterns rather than all pairs, reducing complexity from O(n^2) to O(n) or O(n log n) at the cost of limiting which long-range dependencies can be captured. State space models replace attention entirely with recurrent computations that maintain a fixed-size hidden state, enabling linear-time processing but requiring the model to compress all relevant context into that finite state. Knowledge distillation trains a smaller “student” model to match the outputs of a larger “teacher,” preserving much of the teacher’s capability in a more deployable form. Each approach involves trade-offs between efficiency gains and capability preservation that must be evaluated empirically on genomic tasks.\n\n\n\n\n\n\nRecall and Connect\n\n\n\nPause the scaling discussion for a moment. From Chapter 7, what is the time complexity of standard self-attention for a sequence of length n? Why does this create particular challenges for genomic sequences compared to typical language model inputs? How might this relate to the bottlenecks just discussed?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nStandard self-attention has O(n²) time and memory complexity because it computes attention scores between all pairs of tokens. This creates severe challenges for genomic sequences because regulatory elements can span hundreds of kilobases, far exceeding typical language model context windows (which handle thousands of tokens). For a 100kb sequence, quadratic scaling becomes prohibitive. This directly relates to the “context length” bottleneck: the biological context needed (enhancers acting on distant promoters) exceeds what standard attention can efficiently process, motivating linear-time alternatives like Mamba.\n\n\n\n\n\n\n\n\n\n\n\nKey Insight: Scaling Laws Are Not Universal\n\n\n\nThe scaling laws that govern language models may not directly transfer to genomic applications. Genomic sequences have different statistical properties (lower entropy, stronger long-range dependencies, reverse-complement symmetry), and biological function imposes constraints absent in natural language. A model that memorizes more of the genome is not necessarily better at predicting variant effects or gene regulation. The key question is not “how big?” but “what capabilities emerge at what scale for which tasks?”\n\n\n\n\n31.1.2 Context and Multi-Scale Integration\nBiological phenomena span scales from nucleotides to ecosystems. Foundation models must integrate information across these scales to capture biological reality: local sequence motifs, regulatory element architecture, chromosome-level organization, cellular context, tissue environment, organism-level physiology, and population-level variation all contribute to genotype-phenotype relationships.\nCurrent approaches typically focus on single scales or model multi-scale relationships implicitly through large training datasets rather than explicitly through architectural design. A DNA language model processes sequence tokens without explicit representation of chromatin structure. A single-cell model embeds cells without explicit representation of tissue organization. A regulatory model predicts expression without explicit representation of 3D genome contacts.\n\n\n\n\n\n\nRecall and Connect\n\n\n\nBefore reading further about multi-scale integration, retrieve what you learned earlier about scaling. From the discussion ~20 lines above, what were the four major bottlenecks to scaling genomic foundation models? Can you explain in your own words why “bigger” is not automatically “better” for genomic applications?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe four bottlenecks are: (1) training data (finite unique genomes), (2) compute cost (prohibitive for trillion-parameter models), (3) context length (quadratic attention limits practical windows), and (4) evaluation (benchmarks can saturate). “Bigger” is not automatically better because genomic sequences have fundamentally different properties than language (lower entropy, stronger long-range dependencies, reverse-complement symmetry), and biological function imposes constraints absent in natural language. Simply memorizing more genome sequence does not guarantee better variant effect prediction or regulatory understanding—the key question is what capabilities emerge at what scale for which specific tasks.\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nCan you map each of the following model types to the scale(s) they primarily operate at? (1) ESM-2, (2) Enformer, (3) scGPT, (4) Akita, (5) AlphaMissense\nHint: Review the model taxonomy from Section 13.4 and the specific model chapters if needed.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\n\nESM-2: Protein sequence/structure scale (amino acid level). (2) Enformer: Regulatory element scale (kilobase DNA sequences predicting gene expression). (3) scGPT: Single-cell scale (cellular gene expression states). (4) Akita: Chromosome-scale (3D genome organization and chromatin contacts). (5) AlphaMissense: Protein variant scale (missense mutation effects on protein function). Each excels at its primary scale but does not explicitly integrate across scales.\n\n\n\n\n\n\nArchitectures that explicitly integrate across scales remain a frontier. Hierarchical models that compose representations at different resolutions, graph neural networks that encode biological relationships across scales (Section 21.2.2), and hybrid systems that combine modality-specific encoders with cross-modal attention layers all represent active research directions.\nWhy is multi-scale integration fundamentally harder than single-scale modeling? The challenge is not merely computational but conceptual: the rules governing each scale differ qualitatively. Nucleotide-level models learn sequence motifs through local correlations; these patterns are dense, stationary, and amenable to convolutional architectures. Cell-level models learn regulatory programs through gene co-expression; these relationships are sparse, context-dependent, and require attention or graph structures. Tissue-level models learn spatial organization through cell-cell interactions; these patterns are geometric and require architectures that respect physical locality. No single architecture naturally spans these diverse statistical structures. A model that excels at motif detection may fail at capturing cell-state transitions; a model that captures tissue organization may be blind to the sequence features that drive it. True multi-scale integration requires not just concatenating representations but learning how perturbations propagate across scales—how a single nucleotide change becomes a protein misfolding becomes a cellular stress response becomes a tissue pathology. This causal chain crosses multiple levels of biological organization, each with its own dynamics and timescales.\n\n\n\n\n\n\nCase Study: APOE ε4 Across Biological Scales\n\n\n\nThe APOE ε4 allele (rs429358 C→T, resulting in Cys→Arg at position 112) illustrates how molecular perturbations propagate across biological scales to produce disease:\n\n\n\n\n\n\n\n\nScale\nObservation\nModel Type Needed\n\n\n\n\nSequence\nSingle C→T substitution in APOE exon 4\nDNA-LM / VEP\n\n\nProtein\nArg112 disrupts salt bridge with Glu255, destabilizing lipid-binding domain\nESM-2 / AlphaFold\n\n\nCellular\nReduced lipid clearance by astrocytes; impaired Aβ degradation by microglia\nSingle-cell models\n\n\nTissue\nIncreased amyloid plaque deposition in hippocampus and cortex\nSpatial transcriptomics models\n\n\nOrganism\n3-15× increased Alzheimer’s risk; earlier onset by ~7 years\nClinical risk models\n\n\n\nThe multi-scale integration challenge:\nCurrent models excel at individual scales. AlphaMissense correctly predicts ε4 as pathogenic (score: 0.92). ESMFold captures the structural destabilization. scGPT identifies the affected cell types. But no existing model traces the complete causal chain from the single nucleotide change through protein misfolding → lipid dysregulation → cellular stress → tissue pathology → disease.\nA true multi-scale foundation model would take the sequence variant as input and output predictions at each scale: structural impact (0.85), cellular consequence (lipid metabolism disruption in astrocytes), tissue effect (hippocampal vulnerability), and clinical risk (OR = 3.2 for AD by age 75). This requires not just concatenating predictions but learning the causal propagation rules that connect scales.\n\n\nSuccess will require not just architectural innovation but appropriate training data that captures multi-scale relationships and evaluation protocols that probe multi-scale reasoning.\n\n\n\n\n\n\nMultiscale integration for future foundation models\n\n\n\n\nFigure 31.2: Multiscale integration for future foundation models. Biology spans scales from nucleotides (10^0 bp) to populations (10^6+ individuals). Current foundation models excel at single scales: DNA language models at sequence level, protein models at structure level, single-cell models at cellular level. The frontier challenge: integrating across scales so that molecular perturbations can be traced to organismal phenotypes. Approaches include hierarchical architectures, cross-scale attention mechanisms, and explicit causal structure encoding.\n\n\n\n\n\n31.1.3 Causality and Mechanism\nThe distinction between correlation and causation pervades genomic analysis. A variant associated with disease in genome-wide association study (GWAS) may be causal, in linkage disequilibrium with a causal variant, or confounded by population structure or other factors (Section 3.3). A regulatory element predicted to affect expression may directly drive transcription or may merely co-occur with other causal elements. Foundation models, like other statistical learners, capture patterns in training data without distinguishing causal from correlational relationships.\n\n\n\n\n\n\nKey Insight: Correlation Versus Causation in Foundation Models\n\n\n\nFoundation models learn statistical associations from data. When a DNA language model assigns high likelihood to a sequence, it indicates the sequence is consistent with patterns in the training corpus—not that the sequence functions in any particular way. When a variant effect predictor scores a mutation as deleterious, it reflects features associated with pathogenic variants in training data—not necessarily the causal mechanism of pathogenicity. This distinction, discussed in detail in Chapter 25, remains the central limitation for applications requiring mechanistic understanding.\n\n\nProgress toward causal and mechanistic reasoning in genomic AI likely requires integrating diverse evidence types. Perturbation experiments (CRISPR knockouts, drug treatments, environmental exposures) provide interventional data that can distinguish causal effects from correlations. Mendelian randomization approaches leverage genetic instruments to estimate causal effects from observational data (Davey Smith and Ebrahim 2003). Structural causal models provide formal frameworks for encoding and reasoning about causal relationships.\n\n\n\n\n\n\nRecall and Connect\n\n\n\nChapter 25 covered causal inference in depth. Before examining the table below, retrieve from memory: What are the three main approaches to causal inference in genomics discussed in Chapter 25, and what is the fundamental limitation of each? How do these limitations affect foundation model training?\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe three main approaches are: (1) Mendelian randomization - uses genetic variants as instruments but requires valid instruments and can be confounded by pleiotropy; (2) Perturbation screens (CRISPR, drug treatments) - provide direct interventional data but are expensive, context-specific, and may have off-target effects; (3) Structural causal models - provide formal causal frameworks but require prior knowledge of causal structure and are difficult to scale. For foundation models, these limitations mean that training data is predominantly observational (correlational) rather than interventional (causal), making it difficult to learn mechanistic relationships. Models trained on correlational data can predict well on similar distributions but fail when linkage structure changes or when predicting intervention effects.\n\n\n\n\n\nIncorporating causal structure into foundation models is technically challenging. Causal relationships are often unknown, contested, or context-dependent. Training objectives that encourage causal reasoning must balance causal accuracy against predictive performance on tasks where correlation suffices. The tension arises because exploiting correlations often improves prediction accuracy in the short term: a model that learns “variant X associates with disease Y” can predict well on held-out data from the same distribution, even if X is merely linked to the true causal variant. However, such correlational models fail when the linkage structure changes across populations or when the goal is to predict intervention effects rather than associations. Evaluation of causal reasoning requires benchmarks with known causal ground truth, which are scarce for complex biological systems because establishing true causation requires controlled experiments that are often infeasible in humans.\n\n\n\n\n\n\nPredict Before You Look\n\n\n\nBefore examining Table 31.2, predict: What are four distinct approaches to incorporating causal reasoning into genomic AI? For each, what would be the primary limitation or challenge? Write down your predictions.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nThe four approaches are: (1) Mendelian randomization - limited by requirement for valid instruments and confounding by pleiotropy; (2) Perturbation screens - limited by expense, context-specificity, and off-target effects; (3) Structural causal models - limited by need for prior knowledge and difficulty scaling; (4) Counterfactual prediction - limited by observational training data and extrapolation risk when predicting interventions. No single approach resolves the correlation-causation gap; integration across methods provides strongest evidence.\n\n\n\n\n\n\n\n\nTable 31.2: Approaches to causal reasoning in genomic AI. No single approach resolves the correlation-causation gap; integration across methods provides the strongest evidence.\n\n\n\n\n\n\n\n\n\n\n\nApproach\nMechanism\nLimitations\nChapter Reference\n\n\n\n\nMendelian randomization\nUses genetic variants as instruments for causal inference\nRequires valid instruments; pleiotropy confounds\nSection 25.2.1\n\n\nPerturbation screens\nDirect experimental intervention (CRISPR, drugs)\nExpensive; context-specific; off-target effects\nSection 25.4.1\n\n\nStructural causal models\nExplicit DAG representation of causal relationships\nRequires prior knowledge; difficult to scale\nSection 25.5\n\n\nCounterfactual prediction\nModel what would happen under intervention\nTraining data observational; extrapolation risk\nSection 25.3.3\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation versus causation in genomic AI\n\n\n\n\nFigure 31.3: The correlation-causation gap in genomic foundation models. Left: A foundation model trained on GWAS data learns statistical associations between variants and disease. Variant X associates with disease Y in training data. Middle: Three possible underlying causal structures—X is causal (direct arrow), X is in linkage disequilibrium with true causal variant Z (confounded path), or X and Y share ancestry-related confounding (population structure). Right: The model cannot distinguish these from observational data alone. Clinical consequences: if X is merely linked to Z, a different population with different linkage structure will show poor prediction. Resolving the gap requires interventional data (CRISPR screens, MR) or explicit causal structure (DAGs).",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Frontiers and Synthesis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch31-frontiers.html#sec-ch31-emerging",
    "href": "part_6/p6-ch31-frontiers.html#sec-ch31-emerging",
    "title": "31  Frontiers and Synthesis",
    "section": "31.2 Emerging Directions",
    "text": "31.2 Emerging Directions\nBeyond incremental improvements to existing approaches, several emerging directions may reshape how genomic foundation models develop and deploy. Multimodal architectures that jointly model sequence, structure, expression, and phenotype could capture biological relationships invisible to single-modality models. Agentic systems that autonomously design experiments, interpret results, and iterate toward biological goals could accelerate discovery while raising new governance challenges. Clinical integration through learning health systems could enable models that improve continuously from deployment experience. Each direction carries both promise and risk; realizing benefits while managing harms will require technical innovation alongside thoughtful governance.\n\n\n\n\n\n\nAgentic scientific systems for autonomous discovery\n\n\n\n\nFigure 31.4: Agentic scientific systems for autonomous discovery. An AI agent combines foundation model capabilities (reasoning, hypothesis generation) with lab automation (liquid handlers, sequencers, synthesis). The closed loop: agent proposes experiments, automation executes, results return, agent learns and proposes improved hypotheses. Human oversight provides strategic direction, ethical boundaries, and safety constraints. Example application: autonomous protein engineering cycles through design-build-test-learn faster than human-only teams while maintaining scientific rigor.\n\n\n\n\n31.2.1 Multimodal Integration\nCurrent genomic foundation models largely operate on single modalities: DNA sequence, protein sequence, gene expression counts, chromatin accessibility signals. Biological reality is irreducibly multimodal, with information flowing across modalities through transcription, translation, signaling, and metabolism. The next generation of genomic foundation models will need to integrate across modalities more deeply, building on the multi-omic approaches discussed in Chapter 22.\nEarly multimodal genomic models combine encoders trained separately on different modalities, using cross-attention or shared embedding spaces to enable cross-modal reasoning. More ambitious architectures train end-to-end on multimodal data, learning unified representations that capture relationships between sequence and structure, expression and chromatin state, genotype and phenotype. The data requirements for such training are substantial, requiring aligned measurements across modalities at scale.\n\n\n\n\n\n\nStop and Think\n\n\n\nConsider a clinical scenario where you want to predict which patients will respond to a new cancer immunotherapy. What modalities would be most informative? Sequence (tumor mutations)? Expression (immune infiltrate signatures)? Imaging (tumor microenvironment)? Clinical history (prior treatments)? How would you combine them, and what challenges would arise?\nThis exercise illustrates why multimodal integration is both essential and difficult for clinical applications.\n\n\nClinical applications particularly benefit from multimodal integration. A diagnostic model that combines genomic variants with electronic health record data, imaging findings, and laboratory values can capture patterns invisible to any single modality. A prognostic model that integrates germline genetics with tumor transcriptomics and treatment history can personalize predictions in ways that purely genetic models cannot. Building such systems requires not just technical capability but also data governance frameworks that permit multimodal combination while protecting privacy.\n\n\n\n\n\n\nCase Study: Multimodal Immunotherapy Response Prediction\n\n\n\nPredicting which cancer patients will respond to immune checkpoint inhibitors (anti-PD-1/PD-L1) illustrates the value of multimodal integration:\nSingle-modality performance on held-out validation (n = 2,400 patients):\n\n\n\nModality\nFeatures\nAUC for 6-month response\n\n\n\n\nGenomics (TMB)\nTumor mutation burden (variants/Mb)\n0.62\n\n\nTranscriptomics\nImmune infiltrate signature (18 genes)\n0.65\n\n\nImaging\nCT-derived tumor heterogeneity\n0.58\n\n\nClinical\nPrior lines, ECOG status, PD-L1 IHC\n0.64\n\n\n\nMultimodal integration approaches:\n\n\n\nIntegration Method\nAUC\nImprovement over best single\n\n\n\n\nLate fusion (concatenate predictions)\n0.71\n+9%\n\n\nIntermediate fusion (shared embedding)\n0.74\n+14%\n\n\nCross-attention (modality interaction)\n0.76\n+17%\n\n\n\nWhat multimodal integration captures:\nThe best model learns interaction effects invisible to single modalities: - High TMB + low immune infiltrate → poor response (immune “cold” despite mutations) - Moderate TMB + high infiltrate + responding CT pattern → excellent response - High PD-L1 IHC alone is insufficient; context from other modalities determines whether PD-L1 expression predicts response\nPractical impact: At the high-confidence threshold (predicted probability &gt; 0.7), the multimodal model identifies 35% of patients who will respond with 85% positive predictive value, compared to 20% coverage at the same precision for single-modality approaches.\n\n\n\n\n\n\n\n\nPractical Guidance: Starting with Multimodal Integration\n\n\n\nFor researchers beginning multimodal projects:\n\nStart simple: Late fusion (separate encoders, combined predictions) provides a baseline before attempting end-to-end training\nAlign carefully: Ensure samples are truly matched across modalities; batch effects compound across modalities\nHandle missing data: In clinical settings, not all patients have all modalities; design for graceful degradation\nEvaluate per-modality: Understand what each modality contributes before combining\nConsider causality: Which modalities are upstream (sequence) versus downstream (expression)? This affects how to interpret integration\n\nSee Section 22.2 for detailed integration strategies.\n\n\n\n\n31.2.2 Agentic and Closed-Loop Systems\nFoundation models have traditionally operated as passive tools: given an input, they produce an output, and humans decide what to do with it. Emerging agentic architectures allow models to take actions, observe outcomes, and adapt behavior based on feedback. In genomic contexts, agentic systems might design experiments, interpret results, revise hypotheses, and iterate toward biological goals with minimal human intervention.\nClosed-loop systems couple computational prediction with experimental validation in automated cycles. A design model proposes sequences optimized for a target function. An automated synthesis and screening platform tests proposed sequences. Results feed back to update the model or guide subsequent proposals. Such systems can explore sequence space far more efficiently than sequential human-directed experimentation, as discussed in the design-build-test-learn cycles of Section 30.6.\n\n\n\n\n\n\nGovernance Challenge: Agentic Autonomy\n\n\n\nAgentic systems raise governance questions absent from traditional models:\n\nObjective specification: How do we ensure the optimization objective captures what we actually want?\nMonitoring and oversight: How do we detect when the system pursues unintended goals?\nStopping criteria: When should autonomous operation halt for human review?\nAccountability: When an autonomous system makes an error, who is responsible?\nDual use: How do we prevent agentic systems from being misused for harmful purposes?\n\nThese questions connect to the biosecurity considerations in Section 26.6 and require governance frameworks that evolve with technical capabilities.\n\n\nThe promise of agentic and closed-loop approaches is accelerated discovery: identifying functional sequences, characterizing biological mechanisms, and optimizing therapeutic candidates faster than traditional workflows. The risks include models pursuing objectives that diverge from human intent, experimental systems generating safety hazards, and accountability gaps when autonomous systems make consequential errors. Realizing benefits while managing risks requires careful attention to objective specification, monitoring and oversight mechanisms, and safety boundaries that constrain autonomous action.\n\n\n31.2.3 Clinical Integration and Learning Health Systems\nThe ultimate test of genomic foundation models is whether they improve health outcomes. Moving from research demonstrations to clinical impact requires integration into care workflows, evidence of benefit from prospective studies, regulatory clearance, and sustainable business models that support ongoing development and maintenance.\nLearning health systems provide a framework for continuous improvement: clinical use generates data that feeds back into model refinement, creating virtuous cycles where models improve as they serve more patients. The virtuous cycle works because clinical deployment reveals failure modes invisible in research datasets: patients with rare phenotypes, populations underrepresented in training data, and edge cases that benchmarks miss all surface during real-world use. Each model prediction becomes a natural experiment whose outcome can inform future predictions. However, such systems raise governance questions about who controls the learning process, how improvements are validated before deployment, and how benefits and risks are distributed across patients, providers, and technology developers.\nThe foundation model paradigm offers particular advantages for learning health systems. Pretrained models can be adapted to local populations and practices through fine-tuning on institutional data (?sec-ch09-full-finetuning; ?sec-ch22-domain-adaptation). Improvements demonstrated at one institution can potentially transfer to others through shared model updates. Common architectures enable comparison across sites and accumulation of evidence across diverse populations.\n\n\n\n\n\n\nCase Study: Geisinger DiscovEHR Learning Health System\n\n\n\nThe Geisinger DiscovEHR program illustrates a learning health system integrating genomic foundation models into clinical care:\nInfrastructure: - 250,000+ patients with linked exome sequencing and EHR data - Automated variant classification pipeline incorporating foundation model predictions - Clinical decision support alerts for actionable pharmacogenomic variants - Bidirectional data flow: clinical outcomes feedback to improve models\nThe learning loop in practice:\n\nInitial deployment (2014): Variant classifier using SIFT/PolyPhen/CADD scores; 23% of rare disease patients received genetic diagnosis\nModel update v2 (2018): Added protein language model features (ESM-1); diagnostic yield increased to 31%\nModel update v3 (2022): Integrated AlphaMissense and EVE scores; diagnostic yield increased to 38%\nContinuous learning: VUS classified by the model but later confirmed pathogenic through clinical outcomes inform model recalibration\n\nQuantitative impact:\n\n\n\nMetric\n2014 Baseline\n2024 After Learning\nImprovement\n\n\n\n\nDiagnostic yield (rare disease)\n23%\n38%\n+65%\n\n\nTime to diagnosis (median)\n18 months\n6 months\n-67%\n\n\nActionable pharmacogenomic alerts\n2,100/year\n12,400/year\n+490%\n\n\nVUS reclassified through outcomes\n—\n847 variants\n—\n\n\n\nKey lesson: The 847 VUS reclassifications represent knowledge generated by the health system itself—variants that were uncertain at the time of testing but were clarified by observing patient outcomes. This feedback loop, where clinical deployment generates training data that improves future predictions, is the defining feature of a learning health system.\n\n\n\n\n\n\n\n\nLearning health system integrating genomic foundation models\n\n\n\n\nFigure 31.5: Learning health system integrating genomic foundation models. Clinical care generates data: genomic testing, FM-based interpretation, treatment decisions, outcomes. The learning loop: outcomes tracked against predictions, models updated on aggregated data, improved predictions deployed back to care. Multi-institutional networks amplify learning through federated approaches that preserve privacy. Governance layer ensures equity monitoring, consent compliance, and regulatory adherence. Vision: every patient encounter contributes to better predictions for future patients, creating a continuously improving healthcare system.\n\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\nA learning health system is deployed at three hospitals. After six months, Hospital A (large academic center) shows improved outcomes while Hospital B (community hospital) and Hospital C (rural clinic) show no change. What are three possible explanations, and what would you do to investigate?\nThis scenario illustrates why deployment alone is insufficient without careful monitoring and equity analysis.\n\n\n\n\n\n\nCheck Your Answer\n\n\n\n\n\nPossible explanations: (1) Population differences: Hospital A patients resemble training data better; investigate ancestry and phenotype distributions. (2) Workflow integration: Hospital A integrated the system into clinical workflows effectively while B and C did not; assess actual usage patterns and clinician adoption. (3) Infrastructure and expertise: Hospital A has resources for proper implementation; investigate technical support, training, and data quality. Investigation should include stratified performance analysis, qualitative interviews with clinicians, and equity audits across hospital characteristics.\n\n\n\n\n\nRealizing this vision requires infrastructure for secure data sharing, governance frameworks that enable learning while protecting privacy, regulatory pathways that accommodate evolving systems, and clinical workflows that support appropriate use and oversight. Technical capabilities alone are necessary but not sufficient. Genomic foundation models will achieve their potential only through sustained collaboration among technologists, clinicians, patients, policymakers, and communities working together to build systems that are both capable and trustworthy.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Frontiers and Synthesis</span>"
    ]
  },
  {
    "objectID": "part_6/p6-ch31-frontiers.html#sec-ch31-conclusion",
    "href": "part_6/p6-ch31-frontiers.html#sec-ch31-conclusion",
    "title": "31  Frontiers and Synthesis",
    "section": "31.3 Work Ahead",
    "text": "31.3 Work Ahead\nThe ultimate test of genomic foundation models is whether they improve health outcomes. The technical capabilities surveyed in the preceding chapters, from sequence representations through foundation model architectures to clinical applications, are necessary but not sufficient for that goal. Between a model that predicts well on benchmarks and a patient whose diagnosis comes faster or whose treatment works better lies the full complexity of clinical translation: validation across populations, integration into workflows, regulatory approval, equitable access, and ongoing monitoring for drift and harm.\n\n\n\n\n\n\nKey Insight: The Translation Gap\n\n\n\nBenchmark performance is seductive but insufficient. A variant effect predictor with state-of-the-art AUC may fail to improve clinical outcomes if:\n\nIt performs well on average but poorly for underrepresented populations (Section 12.10)\nIts predictions are uncalibrated and clinicians cannot interpret confidence levels (Section 23.2)\nIt flags the same variants that existing tools flag, adding no new information\nIt cannot integrate into existing clinical workflows without disruptive changes\nRegulatory uncertainty prevents adoption despite technical merit\n\nEach of these failure modes requires different solutions—technical, organizational, regulatory, or social.\n\n\n\n\n\n\n\n\nTranslation gap between benchmarks and clinical impact\n\n\n\n\nFigure 31.6: The translation gap between benchmark performance and clinical impact. A variant effect predictor achieves 0.92 AUC on research benchmarks (left), but faces multiple barriers before improving patient outcomes (right). Barriers organized by type: Technical barriers include poor calibration preventing reliable confidence interpretation and performance disparities across ancestry groups. Organizational barriers include workflow integration challenges and lack of clinician training. Regulatory barriers include uncertain FDA pathways and liability concerns. Economic barriers include reimbursement uncertainty and deployment costs. Each barrier requires different solutions—no amount of benchmark optimization addresses workflow integration or regulatory uncertainty. The gap between model capability and patient benefit is fundamentally sociotechnical, not purely technical.\n\n\n\nLearning health systems provide a framework for bridging this gap: clinical use generates data that feeds back into model refinement, creating virtuous cycles where models improve as they serve more patients. Such systems raise governance questions as important as the technical ones. Who controls the learning process? How are improvements validated before deployment? How are benefits and risks distributed across patients, providers, and technology developers? How do we ensure that populations underrepresented in training data are not further disadvantaged by systems that learn primarily from others?\n\n\n\n\n\n\nStop and Think\n\n\n\nAs you finish this book, consider: What is the most important problem in your domain that genomic foundation models could help solve? What would success look like? What are the barriers—technical, regulatory, social, or economic—between current capabilities and that success? This reflection can guide your next steps, whether in research, clinical application, or policy.\n\n\nGenomic foundation models will achieve their potential only through sustained collaboration among technologists, clinicians, patients, policymakers, and communities working together to build systems that are both capable and trustworthy. Capability without trustworthiness is dangerous: models that predict accurately but fail silently for certain populations cause harm even as they help others. Trustworthiness without capability is insufficient: systems that are transparent and fair but do not improve on existing practice offer nothing worth adopting. Technical achievements in genomic deep learning enable new capabilities; the human systems that govern their development and deployment will determine whether those capabilities translate into genuine benefit for the patients and populations that genomic medicine aims to serve.\n\n\n\n\n\n\nChapter Summary\n\n\n\n\n\n\n\n\n\nTest Yourself\n\n\n\nBefore reviewing the summary, test your recall:\n\nWhat are the three major open technical challenges limiting genomic foundation model impact? For each, explain why it remains unsolved.\nWhy might scaling laws from language models not directly transfer to genomic applications? What biological properties make genomic sequences different?\nDescribe how a learning health system would work with genomic foundation models. What are the key governance challenges this raises?\nWhat is the translation gap between benchmark performance and clinical impact? Give three specific barriers that a high-performing model might face in deployment.\n\n\n\n\n\n\n\nCheck Your Answers\n\n\n\n\n\n\nThree major technical challenges: (1) Scaling and efficiency - genomic models remain smaller than language models due to limited training data (only ~100k species with assemblies), prohibitive compute costs ($10M+ for trillion-parameter models), and quadratic attention costs that limit context windows needed for regulatory elements spanning hundreds of kilobases. (2) Multi-scale integration - biological phenomena span nucleotides to organisms, but current models focus on single scales and lack architectures that explicitly integrate across qualitatively different rules at each level (motif detection requires different structures than cell-state modeling). (3) Causality and mechanism - foundation models learn statistical associations rather than causal relationships, but training data is predominantly observational rather than interventional, making it difficult to distinguish true causal variants from those merely in linkage disequilibrium.\nScaling laws and genomic sequences: Language model scaling laws may not transfer to genomics because genomic sequences have fundamentally different statistical properties including lower entropy, stronger long-range dependencies (regulatory elements acting across kilobases), and reverse-complement symmetry constraints absent in natural language. Additionally, biological function imposes constraints that mean memorizing more genome sequence does not automatically improve variant effect prediction or regulatory understanding—the key question is what specific capabilities emerge at what scale for which particular tasks, not simply “how big.”\nLearning health systems: In a learning health system, clinical use of genomic foundation models generates data (predictions, treatment decisions, outcomes) that feeds back to refine the models, creating virtuous cycles where models improve as they serve more patients. Clinical deployment reveals failure modes invisible in research datasets such as rare phenotypes, underrepresented populations, and edge cases. Key governance challenges include: who controls the learning process, how improvements are validated before deployment, how benefits and risks are distributed across stakeholders, ensuring populations underrepresented in training data are not further disadvantaged, maintaining consent compliance, and establishing regulatory pathways that accommodate evolving systems while protecting patient safety.\nTranslation gap barriers: The gap between benchmark performance and clinical impact encompasses multiple barriers beyond technical capability. Three specific barriers a high-performing model might face: (1) Population-specific failures - model performs well on average but poorly for underrepresented ancestries, violating fairness requirements even with strong AUC metrics. (2) Uncalibrated predictions - model has high discrimination but poor calibration, so clinicians cannot interpret confidence levels appropriately for decision-making. (3) Workflow integration challenges - model requires data formats or computational infrastructure incompatible with existing clinical systems, preventing adoption despite technical merit. Other barriers include regulatory uncertainty preventing adoption, adding no incremental information beyond existing tools, and inability to explain predictions in ways clinicians trust.\n\n\n\n\n\n\nCore Concepts:\n\nOpen technical problems: Scaling (data, compute, context length), multi-scale integration (nucleotide to organism), and causality (distinguishing correlation from mechanism) remain fundamental challenges\nEmerging directions: Multimodal architectures, agentic systems, and learning health systems each offer promise alongside new governance challenges\nTranslation gap: Benchmark performance is necessary but insufficient; clinical impact requires validation, workflow integration, regulatory approval, and equitable access\n\nKey Connections:\n\nScaling challenges connect to architectural choices (Chapter 7) and efficiency techniques (Appendix B)\nMulti-scale integration builds on single-cell (Chapter 19), 3D genome (Chapter 20), and network approaches (Chapter 21)\nCausality challenges extend the discussion from Chapter 25\nGovernance requirements connect to Section 26.1 and responsible development practices\n\nLooking Forward:\nThe field stands at an inflection point. Technical capabilities have advanced dramatically, but realizing clinical impact requires progress on multiple fronts simultaneously—not just better models, but better evaluation, better integration, better governance, and better collaboration across disciplines. The work ahead is not just technical; it is fundamentally human.\nFurther Reading:\n\nFor scaling laws in foundation models: Chowdhery et al. (2022)\nFor state space models as efficient alternatives: Gu and Dao (2024)\nFor causal inference frameworks: Davey Smith and Ebrahim (2003)\nFor learning health systems: Institute of Medicine (2013) Best Care at Lower Cost\n\n\n\n\n\n\n\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2022. “PaLM: Scaling Language Modeling with Pathways.” arXiv. https://doi.org/10.48550/arXiv.2204.02311.\n\n\nDavey Smith, George, and Shah Ebrahim. 2003. “‘Mendelian Randomization’: Can Genetic Epidemiology Contribute to Understanding Environmental Determinants of Disease?*.” International Journal of Epidemiology 32 (1): 1–22. https://doi.org/10.1093/ije/dyg070.\n\n\nFedus, William, Barret Zoph, and Noam Shazeer. 2022. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” Journal of Machine Learning Research 23 (120): 1–39. http://jmlr.org/papers/v23/21-0998.html.\n\n\nGu, Albert, and Tri Dao. 2024. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” In. https://openreview.net/forum?id=tEYskw1VY2.",
    "crumbs": [
      "Part VI: Clinical Translation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Frontiers and Synthesis</span>"
    ]
  },
  {
    "objectID": "bib/references.html",
    "href": "bib/references.html",
    "title": "References",
    "section": "",
    "text": "Abràmoff, Michael D., Philip T. Lavin, Michele Birch, Nilay Shah, and\nJames C. Folk. 2018. “Pivotal Trial of an Autonomous\nAI-Based Diagnostic System for Detection of Diabetic\nRetinopathy in Primary Care Offices.” Npj Digital\nMedicine 1 (1): 39. https://doi.org/10.1038/s41746-018-0040-6.\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green,\nAlexander Pritzel, Olaf Ronneberger, et al. 2024.\n“[AlphaFold3] Accurate Structure\nPrediction of Biomolecular Interactions with AlphaFold\n3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\n“ACCE Model Process for\nEvaluating Genetic Tests\n CDC.” 2004. https://archive.cdc.gov/www_cdc_gov/genomics/gtesting/acce/index.htm.\n\n\nAdministration, U. S. Food and Drug. 2021. “Artificial\nIntelligence/Machine Learning\n(‘AI/ML’)-Based\nSoftware as a Medical Device\n(‘SaMD’) Action\nPlan.” https://www.fda.gov/media/145022/download.\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky,\nAnna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev.\n2010a. “A Method and Server for Predicting Damaging Missense\nMutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\n———. 2010b. “A Method and Server for Predicting Damaging Missense\nMutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nAgarwal, Vikram, and Jay Shendure. 2020. “Predicting mRNA Abundance Directly\nfrom Genomic Sequence Using\nDeep Convolutional Neural\nNetworks.” Cell Reports 31 (7): 107663. https://doi.org/10.1016/j.celrep.2020.107663.\n\n\nAmberger, Joanna S., Carol A. Bocchini, François Schiettecatte, Alan F.\nScott, and Ada Hamosh. 2015. “OMIM.org:\nOnline Mendelian Inheritance in\nMan (OMIM®), an Online Catalog of Human Genes\nand Genetic Disorders.” Nucleic Acids Research 43 (D1):\nD789–98. https://doi.org/10.1093/nar/gku1205.\n\n\nAndré, Fabrice, Eva Ciruelos, Gabor Rubovszky, Mario Campone, Sibylle\nLoibl, Hope S. Rugo, Hiroji Iwata, et al. 2019. “Alpelisib for\nPIK3CA-Mutated, Hormone\nReceptor–Positive Advanced\nBreast Cancer.” New England Journal\nof Medicine 380 (20): 1929–40. https://doi.org/10.1056/NEJMoa1813904.\n\n\nAngelopoulos, Anastasios N., and Stephen Bates. 2023. “Conformal\nPrediction: A Gentle\nIntroduction.” Foundations and Trends® in\nMachine Learning 16 (4): 494–591. https://doi.org/10.1561/2200000101.\n\n\n“Assoc. For Molecular Pathology v.\nMyriad Genetics, Inc., 569\nU.S. 576 (2013).” n.d. Justia\nLaw. Accessed December 26, 2025. https://supreme.justia.com/cases/federal/us/569/576/.\n\n\nAuton, Adam, Gonçalo R. Abecasis, David M. Altshuler, Richard M. Durbin,\nGonçalo R. Abecasis, David R. Bentley, Aravinda Chakravarti, et al.\n2015. “A Global Reference for Human Genetic Variation.”\nNature 526 (7571): 68–74. https://doi.org/10.1038/nature15393.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A.\nGrabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet\nKohli, and David R. Kelley. 2021. “[Enformer]\nEffective Gene Expression Prediction from Sequence by\nIntegrating Long-Range Interactions.” Nature Methods 18\n(October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nAvsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025.\n“AlphaGenome: AI for Better\nUnderstanding the Genome.” Google DeepMind. https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/.\n\n\nBen-David, Shai, John Blitzer, Koby Crammer, Alex Kulesza, Fernando\nPereira, and Jennifer Wortman Vaughan. 2010. “A Theory of Learning\nfrom Different Domains.” Machine Learning 79 (1):\n151–75. https://doi.org/10.1007/s10994-009-5152-4.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S.\nSong. 2024. “GPN-MSA: An Alignment-Based\nDNA Language Model for Genome-Wide Variant Effect\nPrediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023.\n“[GPN] DNA Language Models Are Powerful\nPredictors of Genome-Wide Variant Effects.” Proceedings of\nthe National Academy of Sciences 120 (44): e2311219120. https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025.\n“[TraitGym] Benchmarking\nDNA Sequence Models for\nCausal Regulatory Variant\nPrediction in Human\nGenetics.” bioRxiv. https://doi.org/10.1101/2025.02.11.637758.\n\n\nBenner, Christian, Chris C. A. Spencer, Aki S. Havulinna, Veikko\nSalomaa, Samuli Ripatti, and Matti Pirinen. 2016.\n“FINEMAP: Efficient Variable Selection Using Summary\nData from Genome-Wide Association Studies.”\nBioinformatics 32 (10): 1493–1501. https://doi.org/10.1093/bioinformatics/btw018.\n\n\nBergquist, Timothy, Sarah L. Stenton, Emily A. W. Nadeau, Alicia B.\nByrne, Marc S. Greenblatt, Steven M. Harrison, Sean V. Tavtigian, et al.\n2025. “Calibration of Additional Computational Tools Expands\nClinGen Recommendation Options for Variant Classification\nwith PP3/BP4 Criteria.” Genetics in\nMedicine 27 (6): 101402. https://doi.org/10.1016/j.gim.2025.101402.\n\n\nBoer, Carl G. de, Eeshit Dhaval Vaishnav, Ronen Sadeh, Esteban Luis\nAbeyta, Nir Friedman, and Aviv Regev. 2020. “Deciphering\nEukaryotic Gene-Regulatory Logic with 100 Million Random\nPromoters.” Nature Biotechnology 38 (1): 56–65. https://doi.org/10.1038/s41587-019-0315-8.\n\n\nBowden, Jack, George Davey Smith, and Stephen Burgess. 2015.\n“Mendelian Randomization with Invalid Instruments: Effect\nEstimation and Bias Detection Through Egger\nRegression.” International Journal of Epidemiology 44\n(2): 512–25. https://doi.org/10.1093/ije/dyv080.\n\n\nBrandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and\nVasilis Ntranos. 2023. “Genome-Wide Prediction of Disease Variant\nEffects with a Deep Protein Language Model.” Nature\nGenetics 55 (9): 1512–22. https://doi.org/10.1038/s41588-023-01465-0.\n\n\nBreiman, Leo. 2001. “Statistical Modeling:\nThe Two Cultures.”\nStatistical Science 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg\nBrockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025.\n“[Evo 2] Genome Modeling and Design\nAcross All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nBrnich, Sarah E., Ahmad N. Abou Tayoun, Fergus J. Couch, Garry R.\nCutting, Marc S. Greenblatt, Christopher D. Heinen, Dona M. Kanavy, et\nal. 2019. “Recommendations for Application of the Functional\nEvidence PS3/BS3 Criterion Using the\nACMG/AMP Sequence Variant Interpretation\nFramework.” Genome Medicine 12 (1): 3. https://doi.org/10.1186/s13073-019-0690-2.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language\nModels Are Few-Shot\nLearners.” Advances in Neural Information\nProcessing Systems 33: 1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=transaction&utm_medium=email&utm_campaign=linkedin_newsletter.\n\n\nBrowning, Brian L., Xiaowen Tian, Ying Zhou, and Sharon R. Browning.\n2021. “Fast Two-Stage Phasing of Large-Scale Sequence\nData.” American Journal of Human Genetics 108 (10):\n1880–90. https://doi.org/10.1016/j.ajhg.2021.08.005.\n\n\nBuniello, Annalisa, Daniel Suveges, Carlos Cruz-Castillo, Manuel Bernal\nLlinares, Helena Cornu, Irene Lopez, Kirill Tsukanov, et al. 2025.\n“Open Targets Platform: Facilitating\nTherapeutic Hypotheses Building in Drug Discovery.” Nucleic\nAcids Research 53 (D1): D1467–75. https://doi.org/10.1093/nar/gkae1128.\n\n\nBycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T.\nElliott, Kevin Sharp, Allan Motyer, et al. 2018. “The\nUK Biobank Resource with Deep Phenotyping and\nGenomic Data.” Nature 562 (7726): 203–9. https://doi.org/10.1038/s41586-018-0579-z.\n\n\nCamillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T.\nHiggins-Chen, Steve Horvath, and Bo Wang. 2024.\n“CpGPT: A Foundation Model\nfor DNA Methylation.” bioRxiv. https://doi.org/10.1101/2024.10.24.619766.\n\n\nCandès, Emmanuel, Yingying Fan, Lucas Janson, and Jinchi Lv. 2018.\n“Panning for Gold: ’Model-X’ Knockoffs\nfor High Dimensional Controlled Variable Selection.” Journal\nof the Royal Statistical Society: Series B (Statistical\nMethodology) 80 (3): 551–77. https://doi.org/10.1111/rssb.12265.\n\n\nCao, Zhi-Jie, and Ge Gao. 2022. “[GLUE]\nMulti-Omics Single-Cell Data Integration and Regulatory\nInference with Graph-Linked Embedding.” Nature\nBiotechnology 40 (10): 1458–66. https://doi.org/10.1038/s41587-022-01284-4.\n\n\nChapman, Paul B., Axel Hauschild, Caroline Robert, John B. Haanen, Paolo\nAscierto, James Larkin, Reinhard Dummer, et al. 2011. “Improved\nSurvival with Vemurafenib in\nMelanoma with BRAF V600E\nMutation.” New England Journal of Medicine\n364 (26): 2507–16. https://doi.org/10.1056/NEJMoa1103782.\n\n\nChen, Jiayang, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze\nYu, Licheng Zong, et al. 2022. “[RNA-FM]\nInterpretable RNA Foundation\nModel from Unannotated Data for\nHighly Accurate RNA\nStructure and Function\nPredictions.” arXiv. https://doi.org/10.48550/arXiv.2204.00300.\n\n\nChen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou.\n2022b. “[DeepSEA Sei] A\nSequence-Based Global Map of Regulatory Activity for Deciphering Human\nGenetics.” Nature Genetics 54 (7): 940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\n———. 2022a. “[DeepSEA Sei]\nA Sequence-Based Global Map of Regulatory Activity for\nDeciphering Human Genetics.” Nature Genetics 54 (7):\n940–49. https://doi.org/10.1038/s41588-022-01102-2.\n\n\nCheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė,\nTaylor Applebaum, Alexander Pritzel, et al. 2023.\n“[AlphaMissense] Accurate Proteome-Wide\nMissense Variant Effect Prediction with\nAlphaMissense.” Science 381 (6664):\neadg7492. https://doi.org/10.1126/science.adg7492.\n\n\nCheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu\nYang, Lei Li, and Jian Ma. 2024. “DNALONGBENCH:\nA Benchmark Suite\nFor Long-Range DNA\nPrediction Tasks,” October. https://openreview.net/forum?id=opv67PpqLS.\n\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua\nBengio. 2014. “On the Properties of\nNeural Machine Translation:\nEncoder-Decoder\nApproaches.” arXiv. https://doi.org/10.48550/arXiv.1409.1259.\n\n\nChoi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O’Reilly. 2020.\n“[PRS] Tutorial: A Guide to Performing\nPolygenic Risk Score Analyses.” Nature Protocols 15 (9):\n2759–72. https://doi.org/10.1038/s41596-020-0353-1.\n\n\nChoromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou\nSong, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2022.\n“Rethinking Attention with\nPerformers.” arXiv. https://doi.org/10.48550/arXiv.2009.14794.\n\n\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, et al. 2022.\n“PaLM: Scaling Language\nModeling with Pathways.” arXiv. https://doi.org/10.48550/arXiv.2204.02311.\n\n\nCirulli, Elizabeth T., Simon White, Robert W. Read, Gai Elhanan, William\nJ. Metcalf, Francisco Tanudjaja, Donna M. Fath, et al. 2020.\n“Genome-Wide Rare Variant Analysis for Thousands of Phenotypes in\nover 70,000 Exomes from Two Cohorts.” Nature\nCommunications 11 (1): 542. https://doi.org/10.1038/s41467-020-14288-y.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus\nWahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024.\n“[DeepRVAT] Integration of Variant\nAnnotations Using Deep Set Networks Boosts Rare Variant Association\nTesting.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan\nDuan, and Bo Wang. 2024. “scGPT:\nToward Building a Foundation Model for Single-Cell Multi-Omics Using\nGenerative AI.” Nature Methods 21 (8):\n1470–80. https://doi.org/10.1038/s41592-024-02201-0.\n\n\nDabernig-Heinz, Johanna, Mara Lohde, Martin Hölzer, Adriana Cabal, Rick\nConzemius, Christian Brandt, Matthias Kohl, et al. 2024. “A\nMulticenter Study on Accuracy and Reproducibility of Nanopore\nSequencing-Based Genotyping of Bacterial Pathogens.” Journal\nof Clinical Microbiology 62 (9): e00628–24. https://doi.org/10.1128/jcm.00628-24.\n\n\nDallago, Christian, Jody Mou, Kadina E. Johnston, Bruce J. Wittmann,\nNicholas Bhattacharya, Samuel Goldman, Ali Madani, and Kevin K. Yang.\n2022. “FLIP: Benchmark Tasks in Fitness\nLandscape Inference for Proteins.” bioRxiv. https://doi.org/10.1101/2021.11.09.467890.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez\nCarranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago,\net al. 2023. “Nucleotide Transformer: Building and\nEvaluating Robust Foundation Models for Human Genomics.”\nNature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDao, Tri, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022.\n“FlashAttention: Fast and\nMemory-Efficient Exact\nAttention with\nIO-Awareness.” Advances in Neural\nInformation Processing Systems 35 (December): 16344–59. https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.\n\n\nDauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F.\nMilles, B. I. M. Wicky, et al. 2022. “Robust Deep Learning–Based\nProtein Sequence Design Using ProteinMPNN.”\nScience 378 (6615): 49–56. https://doi.org/10.1126/science.add2187.\n\n\nDavey Smith, George, and Shah Ebrahim. 2003.\n“‘Mendelian Randomization’: Can Genetic\nEpidemiology Contribute to Understanding Environmental Determinants of\nDisease?*.” International Journal of Epidemiology 32\n(1): 1–22. https://doi.org/10.1093/ije/dyg070.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper,\nArend Sidow, and Serafim Batzoglou. 2010b. “Identifying a\nHigh Fraction of the Human\nGenome to Be Under Selective\nConstraint Using GERP++.”\nPLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\n———. 2010a. “Identifying a High Fraction\nof the Human Genome to Be Under\nSelective Constraint Using\nGERP++.” PLOS Computational Biology 6 (12):\ne1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nDePristo, Mark A., Eric Banks, Ryan Poplin, Kiran V. Garimella, Jared R.\nMaguire, Christopher Hartl, Anthony A. Philippakis, et al. 2011.\n“A Framework for Variation Discovery and Genotyping Using\nNext-Generation DNA Sequencing Data.” Nature\nGenetics 43 (5): 491–98. https://doi.org/10.1038/ng.806.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“BERT: Pre-Training of Deep\nBidirectional Transformers for\nLanguage Understanding.” arXiv. https://doi.org/10.48550/arXiv.1810.04805.\n\n\nDixit, Atray, Oren Parnas, Biyu Li, Jenny Chen, Charles P. Fulco, Livnat\nJerby-Arnon, Nemanja D. Marjanovic, et al. 2016.\n“Perturb-Seq: Dissecting\nMolecular Circuits with Scalable\nSingle-Cell RNA\nProfiling of Pooled Genetic\nScreens.” Cell 167 (7): 1853–1866.e17. https://doi.org/10.1016/j.cell.2016.11.038.\n\n\nDuncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R.\nPeterson, and B. Domingue. 2019. “Analysis of Polygenic Risk Score\nUsage and Performance in Diverse Human Populations.” Nature\nCommunications 10 (1): 3328. https://doi.org/10.1038/s41467-019-11112-0.\n\n\nEdgar, Ron, Michael Domrachev, and Alex E. Lash. 2002. “Gene\nExpression Omnibus: NCBI Gene\nExpression and Hybridization Array Data Repository.” Nucleic\nAcids Research 30 (1): 207–10. https://doi.org/10.1093/nar/30.1.207.\n\n\nElks, Cathy E., Marcel Den Hoed, Jing Hua Zhao, Stephen J. Sharp,\nNicholas J. Wareham, Ruth J. F. Loos, and Ken K. Ong. 2012.\n“Variability in the Heritability of Body\nMass Index: A\nSystematic Review and\nMeta-Regression.” Frontiers in\nEndocrinology 3 (February). https://doi.org/10.3389/fendo.2012.00029.\n\n\nElnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi,\nYu Wang, Llion Jones, Tom Gibbs, et al. 2021.\n“ProtTrans: Towards\nCracking the Language of Life’s\nCode Through\nSelf-Supervised Deep\nLearning and High Performance\nComputing.” arXiv. https://doi.org/10.48550/arXiv.2007.06225.\n\n\nErlich, Yaniv, and Arvind Narayanan. 2014. “Routes for Breaching\nand Protecting Genetic Privacy.” Nature Reviews Genetics\n15 (6): 409–21. https://doi.org/10.1038/nrg3723.\n\n\n“Federal Policy for the Protection of\nHuman Subjects (’Common\nRule.” 2009. Page. https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html.\n\n\nFedus, William, Barret Zoph, and Noam Shazeer. 2022. “Switch\nTransformers: Scaling to Trillion\nParameter Models with Simple and\nEfficient Sparsity.” Journal of\nMachine Learning Research 23 (120): 1–39. http://jmlr.org/papers/v23/21-0998.html.\n\n\nFerruz, Noelia, Steffen Schmidt, and Birte Höcker. 2022.\n“ProtGPT2 Is a Deep Unsupervised Language Model for\nProtein Design.” Nature Communications 13 (1): 4348. https://doi.org/10.1038/s41467-022-32007-7.\n\n\nFindlay, Gregory M., Riza M. Daza, Beth Martin, Melissa D. Zhang, Anh P.\nLeith, Molly Gasperini, Joseph D. Janizek, Xingfan Huang, Lea M.\nStarita, and Jay Shendure. 2018. “Accurate Classification of\nBRCA1 Variants with Saturation Genome Editing.”\nNature 562 (7726): 217–22. https://doi.org/10.1038/s41586-018-0461-z.\n\n\nFinn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017.\n“Model-Agnostic\nMeta-Learning for Fast\nAdaptation of Deep\nNetworks.” In Proceedings of the 34th\nInternational Conference on\nMachine Learning, 1126–35. PMLR. https://proceedings.mlr.press/v70/finn17a.html.\n\n\nFokkema, Ivo F. A. C., Peter E. M. Taschner, Gerard C. P. Schaafsma, J.\nCelli, Jeroen F. J. Laros, and Johan T. den Dunnen. 2011.\n“LOVD v.2.0: The Next Generation in Gene Variant\nDatabases.” Human Mutation 32 (5): 557–63. https://doi.org/10.1002/humu.21438.\n\n\nFowler, Douglas M., and Stanley Fields. 2014. “Deep Mutational\nScanning: A New Style of Protein Science.” Nature\nMethods 11 (8): 801–7. https://doi.org/10.1038/nmeth.3027.\n\n\nFrankish, Adam, Mark Diekhans, Anne-Maud Ferreira, Rory Johnson, Irwin\nJungreis, Jane Loveland, Jonathan M Mudge, et al. 2019.\n“GENCODE Reference Annotation for the Human and Mouse\nGenomes.” Nucleic Acids Research 47 (D1): D766–73. https://doi.org/10.1093/nar/gky955.\n\n\nFrazer, Jonathan, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K.\nMin, Kelly Brock, Yarin Gal, and Debora S. Marks. 2021.\n“[EVE] Disease Variant Prediction with\nDeep Generative Models of Evolutionary Data.” Nature 599\n(7883): 91–95. https://doi.org/10.1038/s41586-021-04043-8.\n\n\nFudenberg, Geoff, David R. Kelley, and Katherine S. Pollard. 2020.\n“[Akita] Predicting 3D\nGenome Folding from DNA Sequence with\nAkita.” Nature Methods 17 (11): 1111–17. https://doi.org/10.1038/s41592-020-0958-x.\n\n\nGaedigk, Andrea, Magnus Ingelman-Sundberg, Neil A. Miller, J. Steven\nLeeder, Michelle Whirl-Carrillo, Teri E. Klein, and the PharmVar\nSteering Committee. 2018. “The Pharmacogene\nVariation (PharmVar) Consortium:\nIncorporation of the Human\nCytochrome P450 (CYP)\nAllele Nomenclature\nDatabase.” Clinical Pharmacology &\nTherapeutics 103 (3): 399–401. https://doi.org/10.1002/cpt.910.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a\nBayesian Approximation:\nRepresenting Model Uncertainty in\nDeep Learning.” In Proceedings of\nThe 33rd International Conference\non Machine Learning, 1050–59. PMLR. https://proceedings.mlr.press/v48/gal16.html.\n\n\nGamazon, Eric R., Heather E. Wheeler, Kaanan P. Shah, Sahar V.\nMozaffari, Keston Aquino-Michaels, Robert J. Carroll, Anne E. Eyler, et\nal. 2015. “A Gene-Based Association Method for Mapping Traits\nUsing Reference Transcriptome Data.” Nature Genetics 47\n(9): 1091–98. https://doi.org/10.1038/ng.3367.\n\n\nGanin, Yaroslav, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\nLarochelle, François Laviolette, Mario March, and Victor Lempitsky.\n2016. “Domain-Adversarial Training of\nNeural Networks.” Journal of\nMachine Learning Research 17 (59): 1–35. http://jmlr.org/papers/v17/15-239.html.\n\n\nGarrison, Erik, Jouni Sirén, Adam M. Novak, Glenn Hickey, Jordan M.\nEizenga, Eric T. Dawson, William Jones, et al. 2018. “Variation\nGraph Toolkit Improves Read Mapping by Representing Genetic Variation in\nthe Reference.” Nature Biotechnology 36 (9): 875–79. https://doi.org/10.1038/nbt.4227.\n\n\nGasperini, Molly, Andrew J. Hill, José L. McFaline-Figueroa, Beth\nMartin, Seungsoo Kim, Melissa D. Zhang, Dana Jackson, et al. 2019.\n“A Genome-Wide Framework for\nMapping Gene Regulation via\nCellular Genetic Screens.”\nCell 176 (1): 377–390.e19. https://doi.org/10.1016/j.cell.2018.11.029.\n\n\nGe, Tian, Chia-Yen Chen, Yang Ni, Yen-Chen Anne Feng, and Jordan W.\nSmoller. 2019. “Polygenic Prediction via Bayesian\nRegression and Continuous Shrinkage Priors.” Nature\nCommunications 10 (1): 1776. https://doi.org/10.1038/s41467-019-09718-5.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Commun. ACM 64 (12):\n86–92. https://doi.org/10.1145/3458723.\n\n\n“Genetic Information Nondiscrimination\nAct of 2008.” n.d. U.S. Equal Employment\nOpportunity Commission. Accessed December 26, 2025. https://www.eeoc.gov/statutes/genetic-information-nondiscrimination-act-2008.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024.\n“Delphi: A Deep-Learning\nMethod for Polygenic Risk\nPrediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nGong, Li, Clarissa J Klein, Kelly E Caudle, Ann M Moyer, Stuart A Scott,\nMichelle Whirl-Carrillo, Teri E Klein, ClinGen Pharmacogenomics Working\nGroup (PGxWG), and on behalf of the. 2025. “Integrating\nPharmacogenomics into the Broader\nConstruct of Genomic Medicine:\nEfforts by the ClinGen\nPharmacogenomics Working Group\n(PGxWG).” Clinical Chemistry 71 (1): 36–44.\nhttps://doi.org/10.1093/clinchem/hvae181.\n\n\nGoodwin, Sara, John D. McPherson, and W. Richard McCombie. 2016.\n“Coming of Age: Ten Years of Next-Generation Sequencing\nTechnologies.” Nature Reviews Genetics 17 (6): 333–51.\nhttps://doi.org/10.1038/nrg.2016.49.\n\n\nGrešová, Katarína, Vlastimil Martinek, David Čechák, Petr Šimeček, and\nPanagiotis Alexiou. 2023. “Genomic Benchmarks: A Collection of\nDatasets for Genomic Sequence Classification.” BMC Genomic\nData 24 (1): 25. https://doi.org/10.1186/s12863-023-01123-8.\n\n\nGu, Albert, and Tri Dao. 2024. “Mamba:\nLinear-Time Sequence\nModeling with Selective State\nSpaces.” In. https://openreview.net/forum?id=tEYskw1VY2.\n\n\nGu, Albert, Karan Goel, Ankit Gupta, and Christopher Ré. 2022. “On\nthe Parameterization and Initialization of\nDiagonal State Space\nModels.” Advances in Neural Information\nProcessing Systems 35 (December): 35971–83. https://proceedings.neurips.cc/paper_files/paper/2022/hash/e9a32fade47b906de908431991440f7c-Abstract-Conference.html.\n\n\nGudbjartsson, Daniel F., Patrick Sulem, Hannes Helgason, Arnaldur\nGylfason, Sigurjon A. Gudjonsson, Florian Zink, Asmundur Oddson, et al.\n2015. “Sequence Variants from Whole Genome Sequencing a Large\nGroup of Icelanders.” Scientific Data 2\n(1): 150011. https://doi.org/10.1038/sdata.2015.11.\n\n\nGuo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017.\n“On Calibration of Modern\nNeural Networks.” In Proceedings of\nthe 34th International Conference on\nMachine Learning, 1321–30. PMLR. https://proceedings.mlr.press/v70/guo17a.html.\n\n\nGusev, Alexander, Arthur Ko, Huwenbo Shi, Gaurav Bhatia, Wonil Chung,\nBrenda W. J. H. Penninx, Rick Jansen, et al. 2016. “Integrative\nApproaches for Large-Scale Transcriptome-Wide Association\nStudies.” Nature Genetics 48 (3): 245–52. https://doi.org/10.1038/ng.3506.\n\n\nGymrek, Melissa, Amy L. McGuire, David Golan, Eran Halperin, and Yaniv\nErlich. 2013. “Identifying Personal\nGenomes by Surname\nInference.” Science 339 (6117): 321–24. https://doi.org/10.1126/science.1229566.\n\n\nHao, Minsheng, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi\nCheng, Taifeng Wang, Jianzhu Ma, Xuegong Zhang, and Le Song. 2024.\n“Large-Scale Foundation Model on Single-Cell\nTranscriptomics.” Nature Methods 21 (8): 1481–91. https://doi.org/10.1038/s41592-024-02305-7.\n\n\nHartwig, Fernando Pires, George Davey Smith, and Jack Bowden. 2017.\n“Robust Inference in Summary Data Mendelian\nRandomization via the Zero Modal Pleiotropy Assumption.”\nInternational Journal of Epidemiology 46 (6): 1985–98. https://doi.org/10.1093/ije/dyx102.\n\n\nHealth, Center for Devices and Radiological. 2025. “Artificial\nIntelligence-Enabled Medical\nDevices.” FDA, December. https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-enabled-medical-devices.\n\n\nHenikoff, S, and J G Henikoff. 1992. “Amino Acid Substitution\nMatrices from Protein Blocks.” Proceedings of the National\nAcademy of Sciences 89 (22): 10915–19. https://doi.org/10.1073/pnas.89.22.10915.\n\n\nHilker, Rikke, Dorte Helenius, Birgitte Fagerlund, Axel Skytthe, Kaare\nChristensen, Thomas M. Werge, Merete Nordentoft, and Birte Glenthøj.\n2018. “Heritability of Schizophrenia and\nSchizophrenia Spectrum Based on\nthe Nationwide Danish Twin\nRegister.” Biological Psychiatry, Novel\nMechanisms in Schizophrenia\nPathophysiology, 83 (6): 492–98. https://doi.org/10.1016/j.biopsych.2017.08.017.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long\nShort-Term Memory.”\nNeural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022b.\n“Training Compute-Optimal\nLarge Language Models.”\narXiv. https://doi.org/10.48550/arXiv.2203.15556.\n\n\n———, et al. 2022a. “Training\nCompute-Optimal Large\nLanguage Models.” arXiv. https://doi.org/10.48550/arXiv.2203.15556.\n\n\nHomer, Nils, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav\nTembe, Jill Muehling, John V. Pearson, Dietrich A. Stephan, Stanley F.\nNelson, and David W. Craig. 2008. “Resolving\nIndividuals Contributing Trace\nAmounts of DNA to Highly\nComplex Mixtures Using\nHigh-Density SNP\nGenotyping Microarrays.” PLOS\nGenetics 4 (8): e1000167. https://doi.org/10.1371/journal.pgen.1000167.\n\n\nHormozdiari, Farhad, Emrah Kostem, Eun Yong kang, Bogdan Pasaniuc, and\nEleazar Eskin. 2014. “Identifying Causal Variants at Loci with\nMultiple Signals of Association.” In Proceedings of the 5th\nACM Conference on Bioinformatics,\nComputational Biology, and Health\nInformatics, 610–11. BCB ’14. New York,\nNY, USA: Association for Computing Machinery. https://doi.org/10.1145/2649387.2660800.\n\n\nHoward, Jeremy, and Sebastian Ruder. 2018. “Universal\nLanguage Model Fine-Tuning for\nText Classification.” arXiv. https://doi.org/10.48550/arXiv.1801.06146.\n\n\nHsu, Chloe, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu,\nAdam Lerer, and Alexander Rives. 2022. “Learning Inverse Folding\nfrom Millions of Predicted Structures.” In Proceedings of the\n39th International Conference on\nMachine Learning, 8946–70. PMLR. https://proceedings.mlr.press/v162/hsu22a.html.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, and Weizhu Chen. 2021. “LoRA:\nLow-Rank Adaptation of\nLarge Language Models.”\narXiv. https://doi.org/10.48550/arXiv.2106.09685.\n\n\nIoannidis, Nilah M., Joseph H. Rothstein, Vikas Pejaver, Sumit Middha,\nShannon K. McDonnell, Saurabh Baheti, Anthony Musolf, et al. 2016.\n“REVEL: An Ensemble\nMethod for Predicting the\nPathogenicity of Rare Missense\nVariants.” The American Journal of Human\nGenetics 99 (4): 877–85. https://doi.org/10.1016/j.ajhg.2016.08.016.\n\n\nIonita-Laza, Iuliana, Kenneth McCallum, Bin Xu, and Joseph D. Buxbaum.\n2016. “A Spectral Approach Integrating Functional Genomic\nAnnotations for Coding and Noncoding Variants.” Nature\nGenetics 48 (2): 214–20. https://doi.org/10.1038/ng.3477.\n\n\nJagadeesh, Karthik A., Aaron M. Wenger, Mark J. Berger, Harendra Guturu,\nPeter D. Stenson, David N. Cooper, Jonathan A. Bernstein, and Gill\nBejerano. 2016. “M-CAP Eliminates a Majority of\nVariants of Uncertain Significance in Clinical Exomes at High\nSensitivity.” Nature Genetics 48 (12): 1581–86. https://doi.org/10.1038/ng.3703.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F.\nMcRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A.\nKosmicki, et al. 2019. “[SpliceAI]\nPredicting Splicing from Primary\nSequence with Deep\nLearning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJawahar, Ganesh, Benoît Sagot, and Djamé Seddah. 2019. “What Does\nBERT Learn about the Structure of Language?” In\nACL 2019 - 57th Annual\nMeeting of the Association for\nComputational Linguistics. Florence,\nItaly. https://inria.hal.science/hal-02131630.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021.\n“DNABERT: Pre-Trained Bidirectional\nEncoder Representations from\nTransformers Model for DNA-Language in\nGenome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nJiang, Tao, Yongzhuang Liu, Yue Jiang, Junyi Li, Yan Gao, Zhe Cui,\nYadong Liu, Bo Liu, and Yadong Wang. 2020. “Long-Read-Based Human\nGenomic Structural Variation Detection with cuteSV.” Genome Biology 21 (1):\n189. https://doi.org/10.1186/s13059-020-02107-y.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael\nFigurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.\n“[AlphaFold2] Highly Accurate Protein\nStructure Prediction with AlphaFold.”\nNature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nJurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge,\nand René Jäkel. 2024. “SetQuence &\nSetOmic: Deep Set Transformers for Whole\nGenome and Exome Tumour Analysis.” BioSystems 235\n(January): 105095. https://doi.org/10.1016/j.biosystems.2023.105095.\n\n\nKagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A.\nSloan, Emma Spragins, Forrest Tanaka, et al. 2025. “Data\nNavigation on the ENCODE Portal.” Nature\nCommunications 16 (1): 9592. https://doi.org/10.1038/s41467-025-64343-9.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. “Scaling Laws for Neural\nLanguage Models.” arXiv. https://doi.org/10.48550/arXiv.2001.08361.\n\n\nKarczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B.\nCummings, Jessica Alföldi, Qingbo Wang, Ryan L. Collins, et al. 2020.\n“The Mutational Constraint Spectrum Quantified from Variation in\n141,456 Humans.” Nature 581 (7809): 434–43. https://doi.org/10.1038/s41586-020-2308-7.\n\n\nKatzman, Jared L., Uri Shaham, Alexander Cloninger, Jonathan Bates,\nTingting Jiang, and Yuval Kluger. 2018. “DeepSurv:\nPersonalized Treatment Recommender System Using a Cox\nProportional Hazards Deep Neural Network.” BMC Medical\nResearch Methodology 18 (1): 24. https://doi.org/10.1186/s12874-018-0482-1.\n\n\nKaye, Jane, Edgar A. Whitley, David Lund, Michael Morrison, Harriet\nTeare, and Karen Melham. 2015. “Dynamic Consent: A Patient\nInterface for Twenty-First Century Research Networks.”\nEuropean Journal of Human Genetics 23 (2): 141–46. https://doi.org/10.1038/ejhg.2014.71.\n\n\nKelley, David R. 2020. “[Basenji2]\nCross-Species Regulatory Sequence Activity\nPrediction.” PLOS Computational Biology 16 (7):\ne1008050. https://doi.org/10.1371/journal.pcbi.1008050.\n\n\nKelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger,\nCory Y. McLean, and Jasper Snoek. 2018. “[Basenji2]\nSequential Regulatory Activity Prediction Across\nChromosomes with Convolutional Neural Networks.” Genome\nResearch 28 (5): 739–50. https://doi.org/10.1101/gr.227819.117.\n\n\nKelley, David R., Jasper Snoek, and John L. Rinn. 2016. “Basset:\nLearning the Regulatory Code of the Accessible Genome with Deep\nConvolutional Neural Networks.” Genome Research 26 (7):\n990–99. https://doi.org/10.1101/gr.200535.115.\n\n\nKhera, Amit V., and Sekar Kathiresan. 2017. “Genetics of Coronary\nArtery Disease: Discovery, Biology and Clinical Translation.”\nNature Reviews Genetics 18 (6): 331–44. https://doi.org/10.1038/nrg.2016.160.\n\n\nKichaev, Gleb, Megan Roytman, Ruth Johnson, Eleazar Eskin, Sara\nLindström, Peter Kraft, and Bogdan Pasaniuc. 2017. “Improved\nMethods for Multi-Trait Fine Mapping of Pleiotropic Risk Loci.”\nBioinformatics 33 (2): 248–55. https://doi.org/10.1093/bioinformatics/btw615.\n\n\nKircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O’Roak, Gregory\nM. Cooper, and Jay Shendure. 2014. “A General Framework for\nEstimating the Relative Pathogenicity of Human Genetic Variants.”\nNature Genetics 46 (3): 310–15. https://doi.org/10.1038/ng.2892.\n\n\nKong, Augustine, Michael L. Frigge, Gisli Masson, Soren Besenbacher,\nPatrick Sulem, Gisli Magnusson, Sigurjon A. Gudjonsson, et al. 2012.\n“Rate of de Novo Mutations and the Importance of Father’s Age to\nDisease Risk.” Nature 488 (7412): 471–75. https://doi.org/10.1038/nature11396.\n\n\nKrusche, Peter, Len Trigg, Paul C. Boutros, Christopher E. Mason,\nFrancisco M. De La Vega, Benjamin L. Moore, Mar Gonzalez-Porta, et al.\n2019. “Best Practices for Benchmarking\nGermline Small Variant\nCalls in Human Genomes.”\nNature Biotechnology 37 (5): 555–60. https://doi.org/10.1038/s41587-019-0054-x.\n\n\nKundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela\nYen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015.\n“Integrative Analysis of 111 Reference Human Epigenomes.”\nNature 518 (7539): 317–30. https://doi.org/10.1038/nature14248.\n\n\nKurki, Mitja I., Juha Karjalainen, Priit Palta, Timo P. Sipilä, Kati\nKristiansson, Kati M. Donner, Mary P. Reeve, et al. 2023.\n“FinnGen Provides Genetic Insights from a\nWell-Phenotyped Isolated Population.” Nature 613 (7944):\n508–18. https://doi.org/10.1038/s41586-022-05473-8.\n\n\nLambert, Samuel A, Gad Abraham, and Michael Inouye. 2019. “Towards\nClinical Utility of Polygenic Risk Scores.” Human Molecular\nGenetics 28 (R2): R133–42. https://doi.org/10.1093/hmg/ddz187.\n\n\nLambert, Samuel A., Laurent Gil, Simon Jupp, Scott C. Ritchie, Yu Xu,\nAnnalisa Buniello, Aoife McMahon, et al. 2021. “The\nPolygenic Score Catalog as an\nOpen Database for Reproducibility and Systematic Evaluation.”\nNature Genetics 53 (4): 420–25. https://doi.org/10.1038/s41588-021-00783-5.\n\n\nLandrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen\nChao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018.\n“ClinVar: Improving Access to Variant Interpretations\nand Supporting Evidence.” Nucleic Acids Research 46\n(D1): D1062–67. https://doi.org/10.1093/nar/gkx1153.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam,\nAmit R. Majithia, and Trey Ideker. 2025. “[G2PT]\nA Genotype-Phenotype Transformer to Assess and Explain\nPolygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLi, Heng. 2013. “Aligning Sequence Reads, Clone Sequences and\nAssembly Contigs with BWA-MEM.” arXiv.\nhttps://doi.org/10.48550/arXiv.1303.3997.\n\n\n———. 2014. “Towards Better Understanding\nof Artifacts in Variant Calling\nfrom High-Coverage\nSamples.” Bioinformatics 30 (20): 2843–51.\nhttps://doi.org/10.1093/bioinformatics/btu356.\n\n\n———. 2018. “Minimap2: Pairwise Alignment for Nucleotide\nSequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nLi, Sizhen, Saeed Moayedpour, Ruijiang Li, Michael Bailey, Saleh Riahi,\nMilad Miladi, Jacob Miner, et al. 2023. “CodonBERT:\nLarge Language Models for mRNA Design and Optimization.” bioRxiv. https://doi.org/10.1101/2023.09.09.556981.\n\n\nLi, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and\nYunping Zhu. 2022. “MoGCN: A\nMulti-Omics Integration\nMethod Based on Graph\nConvolutional Network for Cancer\nSubtype Analysis.” Frontiers in\nGenetics 13 (February). https://doi.org/10.3389/fgene.2022.806842.\n\n\nLi, Zehui, Vallijah Subasri, Yifei Shen, Dongsheng Li, Yiren Zhao,\nGuy-Bart Stan, and Caihua Shan. 2025. “Omni-DNA:\nA Unified Genomic\nFoundation Model for\nCross-Modal and\nMulti-Task Learning.”\narXiv. https://doi.org/10.48550/arXiv.2502.03499.\n\n\nLiao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness,\nGlenn Hickey, Shuangjia Lu, et al. 2023. “A Draft Human Pangenome\nReference.” Nature 617 (7960): 312–24. https://doi.org/10.1038/s41586-023-05896-x.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting\nLu, Allan dos Santos Costa, et al. 2022. “[ESM-2]\nLanguage Models of Protein Sequences at the Scale of\nEvolution Enable Accurate Structure Prediction.” bioRxiv. https://doi.org/10.1101/2022.07.20.500902.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and\nDavid R. Kelley. 2025. “[Borzoi]\nPredicting RNA-Seq Coverage from\nDNA Sequence as a Unifying Model of Gene\nRegulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nLipsitch, Marc, Eric Tchetgen Tchetgen, and Ted Cohen. 2010.\n“Negative Controls: A Tool\nfor Detecting Confounding and\nBias in Observational\nStudies.” Epidemiology 21 (3): 383. https://doi.org/10.1097/EDE.0b013e3181d61eeb.\n\n\nLiu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang,\nYucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025.\n“Life-Code: Central Dogma\nModeling with Multi-Omics\nSequence Unification.” arXiv. https://doi.org/10.48550/arXiv.2502.07299.\n\n\nLoh, Po-Ru, Petr Danecek, Pier Francesco Palamara, Christian\nFuchsberger, Yakir A Reshef, Hilary K Finucane, Sebastian Schoenherr, et\nal. 2016. “Reference-Based Phasing Using the\nHaplotype Reference Consortium\nPanel.” Nature Genetics 48 (11): 1443–48. https://doi.org/10.1038/ng.3679.\n\n\nLORD JUSTICE ARNOLD&lt;br&gt;LADY JUSTICE ELISABETH\nLAING&lt;br&gt;and&lt;br&gt;LORD JUSTICE BIRSS. 2021. “Thaler v\nComptroller General of Patents\nTrade Marks And\nDesigns [2021] EWCA Civ\n1374.” https://www.bailii.org/ew/cases/EWCA/Civ/2021/1374.html.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2019. “Decoupled\nWeight Decay\nRegularization.” arXiv. https://doi.org/10.48550/arXiv.1711.05101.\n\n\nLupiáñez, Darío G., Katerina Kraft, Verena Heinrich, Peter Krawitz,\nFrancesco Brancati, Eva Klopocki, Denise Horn, et al. 2015.\n“Disruptions of Topological Chromatin\nDomains Cause Pathogenic\nRewiring of Gene-Enhancer\nInteractions.” Cell 161 (5): 1012–25. https://doi.org/10.1016/j.cell.2015.04.004.\n\n\nLynch, Thomas J., Daphne W. Bell, Raffaella Sordella, Sarada\nGurubhagavatula, Ross A. Okimoto, Brian W. Brannigan, Patricia L.\nHarris, et al. 2004. “Activating Mutations in the\nEpidermal Growth Factor\nReceptor Underlying\nResponsiveness of\nNon–Small-Cell Lung\nCancer to Gefitinib.” New England\nJournal of Medicine 350 (21): 2129–39. https://doi.org/10.1056/NEJMoa040938.\n\n\nMadani, Ali, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P.\nMohr, James M. Holton, Jose Luis Olmos, et al. 2023. “Large\nLanguage Models Generate Functional Protein Sequences Across Diverse\nFamilies.” Nature Biotechnology 41 (8): 1099–1106. https://doi.org/10.1038/s41587-022-01618-2.\n\n\nMallal, Simon, Elizabeth Phillips, Giampiero Carosi, Jean-Michel Molina,\nCassy Workman, Janez Tomažič, Eva Jägel-Guedes, et al. 2008.\n“HLA-B*5701 Screening for\nHypersensitivity to Abacavir.” New\nEngland Journal of Medicine 358 (6): 568–79. https://doi.org/10.1056/NEJMoa0706135.\n\n\nMaller, Julian B., Gilean McVean, Jake Byrnes, Damjan Vukcevic, Kimmo\nPalin, Zhan Su, Joanna M. M. Howson, et al. 2012. “Bayesian\nRefinement of Association Signals for 14 Loci in 3 Common\nDiseases.” Nature Genetics 44 (12): 1294–1301. https://doi.org/10.1038/ng.2435.\n\n\nManolio, Teri A., Francis S. Collins, Nancy J. Cox, David B. Goldstein,\nLucia A. Hindorff, David J. Hunter, Mark I. McCarthy, et al. 2009.\n“Finding the Missing Heritability of Complex Diseases.”\nNature 461 (7265): 747–53. https://doi.org/10.1038/nature08494.\n\n\nManzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025.\n“Comparative Analysis of Deep\nLearning Models for Predicting\nCausative Regulatory\nVariants.” bioRxiv: The Preprint Server for\nBiology, June, 2025.05.19.654920. https://doi.org/10.1101/2025.05.19.654920.\n\n\nMarees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan,\nEmmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018.\n“[GWAS] A Tutorial on Conducting\nGenome-Wide Association Studies: Quality Control and\nStatistical Analysis.” International Journal of Methods in\nPsychiatric Research 27 (2): e1608. https://doi.org/10.1002/mpr.1608.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen,\nDennis Pultz, Ole Winther, and Wouter Boomsma. 2024.\n“BEND: Benchmarking DNA\nLanguage Models on Biologically Meaningful\nTasks.” arXiv. https://doi.org/10.48550/arXiv.2311.12570.\n\n\nMárquez-Luna, Carla, Po-Ru Loh, South Asian Type 2 Diabetes (SAT2D)\nConsortium, The SIGMA Type 2 Diabetes Consortium, and Alkes L. Price.\n2017. “Multiethnic Polygenic Risk Scores Improve Risk Prediction\nin Diverse Populations.” Genetic Epidemiology 41 (8):\n811–23. https://doi.org/10.1002/gepi.22083.\n\n\nMartin, Alicia R., Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada,\nBenjamin M. Neale, and Mark J. Daly. 2019. “Clinical Use of\nCurrent Polygenic Risk Scores May Exacerbate Health Disparities.”\nNature Genetics 51 (4): 584–91. https://doi.org/10.1038/s41588-019-0379-x.\n\n\nMavaddat, Nasim, Kyriaki Michailidou, Joe Dennis, Michael Lush, Laura\nFachal, Andrew Lee, Jonathan P. Tyrer, et al. 2019. “Polygenic\nRisk Scores for Prediction of\nBreast Cancer and Breast\nCancer Subtypes.” The American\nJournal of Human Genetics 104 (1): 21–34. https://doi.org/10.1016/j.ajhg.2018.11.002.\n\n\nMcCloskey, Michael, and Neal Cohen. 1989. “Catastrophic\nInterference in Connectionist\nNetworks: The Sequential\nLearning Problem.” In Psychology of\nLearning and Motivation, 24:109–65.\nAcademic Press. https://doi.org/10.1016/S0079-7421(08)60536-8.\n\n\n“Medical Devices; Laboratory\nDeveloped Tests.” 2024. Federal\nRegister. https://www.federalregister.gov/documents/2024/05/06/2024-08935/medical-devices-laboratory-developed-tests.\n\n\nMedvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill\nVishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel,\nRonnie Rajan, and Shadab Khan. 2025. “BioToken and\nBioFM – Biologically-Informed\nTokenization Enables Accurate and\nEfficient Genomic Foundation\nModels.” bioRxiv. https://doi.org/10.1101/2025.03.27.645711.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and\nAlexander Rives. 2021. “[ESM-1v]\nLanguage Models Enable Zero-Shot Prediction of the Effects\nof Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy\nVasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and\nTimnit Gebru. 2019. “Model Cards for\nModel Reporting.” In Proceedings of\nthe Conference on Fairness,\nAccountability, and Transparency, 220–29.\nFAT* ’19. New York, NY, USA: Association for Computing\nMachinery. https://doi.org/10.1145/3287560.3287596.\n\n\nMorales, Joannella, Shashikant Pujar, Jane E. Loveland, Alex Astashyn,\nRuth Bennett, Andrew Berry, Eric Cox, et al. 2022. “A Joint\nNCBI and EMBL-EBI Transcript Set\nfor Clinical Genomics and Research.” Nature 604 (7905):\n310–15. https://doi.org/10.1038/s41586-022-04558-8.\n\n\nMorcos, Faruck, Andrea Pagnani, Bryan Lunt, Arianna Bertolino, Debora S.\nMarks, Chris Sander, Riccardo Zecchina, José N. Onuchic, Terence Hwa,\nand Martin Weigt. 2011. “Direct-Coupling Analysis of Residue\nCoevolution Captures Native Contacts Across Many Protein\nFamilies.” Proceedings of the National Academy of\nSciences 108 (49): E1293–1301. https://doi.org/10.1073/pnas.1111471108.\n\n\nMountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy\nSchwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021.\n“An Open Approach to Systematically Prioritize Causal Variants and\nGenes at All Published Human GWAS Trait-Associated\nLoci.” Nature Genetics 53 (11): 1527–33. https://doi.org/10.1038/s41588-021-00945-5.\n\n\nMukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom\nSoare, Raghav Tandon, David Amar, et al. 2024.\n“EmbedGEM: A Framework to Evaluate the Utility of\nEmbeddings for Genetic Discovery.” Bioinformatics\nAdvances 4 (1). https://doi.org/10.1093/bioadv/vbae135.\n\n\nNaghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B.\nMacdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow\nCoyote-Maestas, and Hani Goodarzi. 2024. “[cdsFM - EnCodon/DeCodon]\nA Suite of Foundation\nModels Captures the Contextual\nInterplay Between Codons.”\nbioRxiv. https://doi.org/10.1101/2024.10.10.617568.\n\n\nNagpal, Chirag, Xinyu Li, and Artur Dubrawski. 2021. “Deep\nSurvival Machines: Fully\nParametric Survival Regression\nand Representation Learning for\nCensored Data With\nCompeting Risks.” IEEE Journal of\nBiomedical and Health Informatics 25 (8): 3163–75. https://doi.org/10.1109/JBHI.2021.3052441.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT:\nPredicting Amino Acid Changes That Affect Protein\nFunction.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nNguengang Wakap, Stéphanie, Deborah M. Lambert, Annie Olry, Charlotte\nRodwell, Charlotte Gueydan, Valérie Lanneau, Daniel Murphy, Yann Le Cam,\nand Ana Rath. 2020. “Estimating Cumulative Point Prevalence of\nRare Diseases: Analysis of the Orphanet Database.”\nEuropean Journal of Human Genetics 28 (2): 165–73. https://doi.org/10.1038/s41431-019-0508-0.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva\nKatrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence\nModeling and Design from Molecular to Genome Scale with\nEvo.” Science 386 (6723): eado9336. https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum\nBirch-Sykes, Michael Wornow, Aman Patel, et al. 2023.\n“HyenaDNA: Long-Range\nGenomic Sequence Modeling at\nSingle Nucleotide\nResolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nNielsen, Rasmus, Joshua S. Paul, Anders Albrechtsen, and Yun S. Song.\n2011. “Genotype and SNP Calling from Next-Generation\nSequencing Data.” Nature Reviews. Genetics 12 (6):\n443–51. https://doi.org/10.1038/nrg2986.\n\n\nNofziger, Charity, Amy J. Turner, Katrin Sangkuhl, Michelle\nWhirl-Carrillo, José A. G. Agúndez, John L. Black, Henry M.\nDunnenberger, et al. 2019. “PharmVar\nGeneFocus: CYP2D6.” Clinical\nPharmacology & Therapeutics 107 (1): 154–70. https://doi.org/10.1002/cpt.1643.\n\n\nNotin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk,\nSteffanie Paul, Han Spinner, Nathan Rollins, et al. 2023a.\n“ProteinGym: Large-Scale\nBenchmarks for Protein Fitness\nPrediction and Design.” Advances in\nNeural Information Processing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\n———, et al. 2023b. “ProteinGym:\nLarge-Scale Benchmarks for\nProtein Fitness Prediction and\nDesign.” Advances in Neural Information\nProcessing Systems 36 (December): 64331–79. https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html.\n\n\nnull, null. 2019. “The ‘All of\nUs’ Research\nProgram.” New England Journal of Medicine\n381 (7): 668–76. https://doi.org/10.1056/NEJMsr1809937.\n\n\nNurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V.\nBzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. “The\nComplete Sequence of a Human Genome.” Science 376\n(6588): 44–53. https://doi.org/10.1126/science.abj6987.\n\n\nO’Connell, Jared, Deepti Gurdasani, Olivier Delaneau, Nicola Pirastu,\nSheila Ulivi, Massimiliano Cocca, Michela Traglia, et al. 2014. “A\nGeneral Approach for Haplotype\nPhasing Across the Full Spectrum\nof Relatedness.” PLOS Genetics 10 (4):\ne1004234. https://doi.org/10.1371/journal.pgen.1004234.\n\n\nO’Leary, Nuala A., Mathew W. Wright, J. Rodney Brister, Stacy Ciufo,\nDiana Haddad, Rich McVeigh, Bhanu Rajput, et al. 2016. “Reference\nSequence (RefSeq) Database at NCBI: Current\nStatus, Taxonomic Expansion, and Functional Annotation.”\nNucleic Acids Research 44 (D1): D733–45. https://doi.org/10.1093/nar/gkv1189.\n\n\nOord, Aaron van den, Yazhe Li, and Oriol Vinyals. 2019.\n“Representation Learning with\nContrastive Predictive\nCoding.” arXiv. https://doi.org/10.48550/arXiv.1807.03748.\n\n\nOrenbuch, Rose, Courtney A. Shearer, Aaron W. Kollasch, Aviv D. Spinner,\nThomas Hopf, Lood van Niekerk, Dinko Franceschi, Mafalda Dias, Jonathan\nFrazer, and Debora S. Marks. 2025. “[popEVE] Proteome-Wide Model for Human\nDisease Genetics.” Nature Genetics, November, 1–10. https://doi.org/10.1038/s41588-025-02400-1.\n\n\n“PacificBiosciences/Pbsv.” 2025. PacBio. https://github.com/PacificBiosciences/pbsv.\n\n\nParasuraman, Raja, and Dietrich H. Manzey. 2010. “Complacency and\nBias in Human Use of\nAutomation: An Attentional\nIntegration.” Human Factors 52 (3):\n381–410. https://doi.org/10.1177/0018720810376055.\n\n\nPatterson, Nick, Alkes L. Price, and David Reich. 2006.\n“Population Structure and\nEigenanalysis.” PLOS Genetics 2 (12): e190.\nhttps://doi.org/10.1371/journal.pgen.0020190.\n\n\nPe’er, Itsik, Roman Yelensky, David Altshuler, and Mark J. Daly. 2008.\n“Estimation of the Multiple Testing Burden for Genomewide\nAssociation Studies of Nearly All Common Variants.” Genetic\nEpidemiology 32 (4): 381–85. https://doi.org/10.1002/gepi.20303.\n\n\nPearce, James D., Sara E. Simmonds, Gita Mahmoudabadi, Lakshmi Krishnan,\nGiovanni Palla, Ana-Maria Istrate, Alexander Tarashansky, et al. 2025.\n“[TranscriptFormer]\nCross-Species Generative\nCell Atlas Across 1.5\nBillion Years of Evolution:\nThe TranscriptFormer Single-Cell\nModel.” bioRxiv. https://doi.org/10.1101/2025.04.25.650731.\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\nPejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel,\nSean D. Mooney, Rachel Karchin, Anne O’Donnell-Luria, et al. 2022.\n“Calibration of Computational Tools for Missense Variant\nPathogenicity Classification and ClinGen Recommendations\nfor PP3/BP4 Criteria.” American\nJournal of Human Genetics 109 (12): 2163–77. https://doi.org/10.1016/j.ajhg.2022.10.013.\n\n\nPoli, Michael, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao,\nStephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. 2023.\n“Hyena Hierarchy: Towards\nLarger Convolutional Language\nModels.” In Proceedings of the 40th\nInternational Conference on\nMachine Learning, 28043–78. PMLR. https://proceedings.mlr.press/v202/poli23a.html.\n\n\nPoplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas\nColthurst, Alexander Ku, Dan Newburger, et al. 2018.\n“[DeepVariant] A Universal\nSNP and Small-Indel Variant Caller Using Deep Neural\nNetworks.” Nature Biotechnology 36 (10): 983–87. https://doi.org/10.1038/nbt.4235.\n\n\nPress, Ofir, Noah A. Smith, and Mike Lewis. 2022. “Train\nShort, Test Long:\nAttention with Linear Biases\nEnables Input Length\nExtrapolation.” arXiv. https://doi.org/10.48550/arXiv.2108.12409.\n\n\nPrice, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E.\nWeinblatt, Nancy A. Shadick, and David Reich. 2006. “Principal\nComponents Analysis Corrects for Stratification in Genome-Wide\nAssociation Studies.” Nature Genetics 38 (8): 904–9. https://doi.org/10.1038/ng1847.\n\n\nQuang, Daniel, Yifei Chen, and Xiaohui Xie. 2015.\n“DANN: A Deep Learning Approach for Annotating the\nPathogenicity of Genetic Variants.” Bioinformatics 31\n(5): 761–63. https://doi.org/10.1093/bioinformatics/btu703.\n\n\nQuang, Daniel, and Xiaohui Xie. 2016. “DanQ: A Hybrid\nConvolutional and Recurrent Deep Neural Network for Quantifying the\nFunction of DNA Sequences.” Nucleic Acids\nResearch 44 (11): e107. https://doi.org/10.1093/nar/gkw226.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023.\n“Exploring the Limits of Transfer\nLearning with a Unified\nText-to-Text Transformer.”\narXiv. https://doi.org/10.48550/arXiv.1910.10683.\n\n\nRakowski, Alexander, and Christoph Lippert. 2025.\n“[MIFM] Multiple Instance Fine-Mapping:\nPredicting Causal Regulatory Variants with a Deep Sequence\nModel.” medRxiv. https://doi.org/10.1101/2025.06.13.25329551.\n\n\nRao, Roshan, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John\nCanny, Pieter Abbeel, and Yun S. Song. 2019. “Evaluating\nProtein Transfer Learning with\nTAPE.” arXiv. https://doi.org/10.48550/arXiv.1906.08230.\n\n\nRao, Roshan, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander\nRives. 2020. “Transformer Protein Language Models Are Unsupervised\nStructure Learners.” bioRxiv. https://doi.org/10.1101/2020.12.15.422761.\n\n\n“RealTimeGenomics/Rtg-Core.” 2025. Real Time\nGenomics. https://github.com/RealTimeGenomics/rtg-core.\n\n\nRegev, Aviv, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe\nBenoist, Ewan Birney, Bernd Bodenmiller, et al. 2017. “The\nHuman Cell Atlas.” Edited\nby Thomas R Gingeras. eLife 6 (December): e27041. https://doi.org/10.7554/eLife.27041.\n\n\n“Regulation (EU) 2016/679 of the\nEuropean Parliament and of the\nCouncil of 27 April 2016 on the Protection of\nNatural Persons with Regard to the Processing of Personal Data and on\nthe Free Movement of Such Data, and Repealing Directive\n95/46/EC (General Data\nProtection Regulation) (Text with\nEEA Relevance).” 2016. http://data.europa.eu/eli/reg/2016/679/oj.\n\n\n“Regulation (EU) 2017/745 of the\nEuropean Parliament and of the\nCouncil of 5 April 2017 on Medical Devices,\nAmending Directive 2001/83/EC,\nRegulation (EC) No 178/2002 and\nRegulation (EC) No 1223/2009 and\nRepealing Council Directives\n90/385/EEC and 93/42/EEC (Text\nwith EEA Relevance. ).” 2017. http://data.europa.eu/eli/reg/2017/745/oj.\n\n\n“Regulation (EU) 2024/1689 of the\nEuropean Parliament and of the\nCouncil of 13 June 2024 Laying down Harmonised\nRules on Artificial Intelligence and Amending Regulations\n(EC) No 300/2008, (EU)\nNo 167/2013, (EU) No 168/2013,\n(EU) 2018/858, (EU) 2018/1139 and\n(EU) 2019/2144 and Directives\n2014/90/EU, (EU) 2016/797 and\n(EU) 2020/1828 (Artificial\nIntelligence Act) (Text with\nEEA Relevance).” 2024. http://data.europa.eu/eli/reg/2024/1689/oj.\n\n\nRehm, Heidi L., Jonathan S. Berg, Lisa D. Brooks, Carlos D. Bustamante,\nJames P. Evans, Melissa J. Landrum, David H. Ledbetter, et al. 2015.\n“ClinGen — The Clinical\nGenome Resource.” New England\nJournal of Medicine 372 (23): 2235–42. https://doi.org/10.1056/NEJMsr1406261.\n\n\nRelling, Mary V., Teri E. Klein, Roseann S. Gammal, Michelle\nWhirl-Carrillo, James M. Hoffman, and Kelly E. Caudle. 2019. “The\nClinical Pharmacogenetics\nImplementation Consortium: 10\nYears Later.” Clinical Pharmacology\n& Therapeutics 107 (1): 171–75. https://doi.org/10.1002/cpt.1651.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and\nMartin Kircher. 2019. “CADD: Predicting the\nDeleteriousness of Variants Throughout the Human Genome.”\nNucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nRichards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie\nGastier-Foster, Wayne W. Grody, et al. 2015. “Standards and\nGuidelines for the Interpretation of Sequence Variants: A Joint\nConsensus Recommendation of the American\nCollege of Medical Genetics and\nGenomics and the Association for\nMolecular Pathology.” Genetics in\nMedicine 17 (5): 405–24. https://doi.org/10.1038/gim.2015.30.\n\n\nRieke, Nicola, Jonny Hancox, Wenqi Li, Fausto Milletarì, Holger R. Roth,\nShadi Albarqouni, Spyridon Bakas, et al. 2020. “The Future of\nDigital Health with Federated Learning.” Npj Digital\nMedicine 3 (1): 119. https://doi.org/10.1038/s41746-020-00323-1.\n\n\nRisch, Neil, and Kathleen Merikangas. 1996. “The\nFuture of Genetic Studies of\nComplex Human Diseases.”\nScience 273 (5281): 1516–17. https://doi.org/10.1126/science.273.5281.1516.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin,\nJason Liu, Demi Guo, et al. 2021a. “[ESM-1b]\nBiological Structure and Function Emerge from Scaling\nUnsupervised Learning to 250 Million Protein Sequences.”\nProceedings of the National Academy of Sciences of the United States\nof America 118 (15): e2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\n———, et al. 2021b. “[ESM-1b] Biological\nStructure and Function Emerge from Scaling Unsupervised Learning to 250\nMillion Protein Sequences.” Proceedings of the National\nAcademy of Sciences of the United States of America 118 (15):\ne2016239118. https://doi.org/10.1073/pnas.2016239118.\n\n\nRobinson, James, Dominic J Barker, Xenia Georgiou, Michael A Cooper,\nPaul Flicek, and Steven G E Marsh. 2020.\n“IPD-IMGT/HLA\nDatabase.” Nucleic Acids Research 48 (D1):\nD948–55. https://doi.org/10.1093/nar/gkz950.\n\n\nRuan, Yunfeng, Yen-Feng Lin, Yen-Chen Anne Feng, Chia-Yen Chen, Max Lam,\nZhenglin Guo, Lin He, et al. 2022. “Improving Polygenic Prediction\nin Ancestrally Diverse Populations.” Nature Genetics 54\n(5): 573–80. https://doi.org/10.1038/s41588-022-01054-7.\n\n\nRubin, Alan F., Hannah Gelman, Nathan Lucas, Sandra M. Bajjalieh,\nAnthony T. Papenfuss, Terence P. Speed, and Douglas M. Fowler. 2017.\n“A Statistical Framework for Analyzing Deep Mutational Scanning\nData.” Genome Biology 18 (1): 150. https://doi.org/10.1186/s13059-017-1272-5.\n\n\nRubinacci, Simone, Diogo M. Ribeiro, Robin J. Hofmeister, and Olivier\nDelaneau. 2021. “Efficient Phasing and Imputation of Low-Coverage\nSequencing Data Using Large Reference Panels.” Nature\nGenetics 53 (1): 120–26. https://doi.org/10.1038/s41588-020-00756-0.\n\n\nSainz, Oscar, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez\nde Lacalle, and Eneko Agirre. 2023. “NLP\nEvaluation in Trouble: On the\nNeed to Measure LLM\nData Contamination for Each\nBenchmark.” In Findings of the\nAssociation for Computational\nLinguistics: EMNLP 2023, edited by Houda\nBouamor, Juan Pino, and Kalika Bali, 10776–87. Singapore: Association\nfor Computational Linguistics. https://doi.org/10.18653/v1/2023.findings-emnlp.722.\n\n\nSakaue, Saori, Saisriram Gurajala, Michelle Curtis, Yang Luo, Wanson\nChoi, Kazuyoshi Ishigaki, Joyce B. Kang, et al. 2023. “Tutorial: A\nStatistical Genetics Guide to Identifying HLA Alleles\nDriving Complex Disease.” Nature Protocols 18 (9):\n2625–41. https://doi.org/10.1038/s41596-023-00853-4.\n\n\nSample, Paul J., Ban Wang, David W. Reid, Vlad Presnyak, Iain J.\nMcFadyen, David R. Morris, and Georg Seelig. 2019. “Human 5′\nUTR Design and Variant Effect Prediction from a Massively\nParallel Translation Assay.” Nature Biotechnology 37\n(7): 803–9. https://doi.org/10.1038/s41587-019-0164-5.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch.\n2024. “[GROVER] DNA Language Model\nGROVER Learns Sequence Context in the Human Genome.”\nNature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSangkuhl, Katrin, Michelle Whirl-Carrillo, Ryan M. Whaley, Mark Woon,\nAdam Lavertu, Russ B. Altman, Lester Carter, Anurag Verma, Marylyn D.\nRitchie, and Teri E. Klein. 2019. “Pharmacogenomics\nClinical Annotation Tool\n(PharmCAT).” Clinical Pharmacology &\nTherapeutics 107 (1): 203–10. https://doi.org/10.1002/cpt.1568.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and\nVolodymyr Kuleshov. 2024. “Caduceus:\nBi-Directional Equivariant\nLong-Range DNA\nSequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and\nMartin Kircher. 2024. “CADD V1.7: Using Protein\nLanguage Models, Regulatory CNNs and Other Nucleotide-Level\nScores to Improve Genome-Wide Variant Predictions.” Nucleic\nAcids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nShafin, Kishwar, Trevor Pesout, Pi-Chuan Chang, Maria Nattestad, Alexey\nKolesnikov, Sidharth Goel, Gunjan Baid, et al. 2021.\n“Haplotype-Aware Variant Calling with\nPEPPER-Margin-DeepVariant Enables\nHigh Accuracy in Nanopore Long-Reads.” Nature Methods 18\n(11): 1322–32. https://doi.org/10.1038/s41592-021-01299-w.\n\n\nSherry, S. T., M.-H. Ward, M. Kholodov, J. Baker, L. Phan, E. M.\nSmigielski, and K. Sirotkin. 2001. “dbSNP: The NCBI Database of Genetic\nVariation.” Nucleic Acids Research 29 (1): 308–11. https://doi.org/10.1093/nar/29.1.308.\n\n\nShevlane, Toby. 2022. “Structured Access: An Emerging Paradigm for\nSafe AI Deployment.” arXiv. https://doi.org/10.48550/arXiv.2201.05159.\n\n\nShrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. 2017.\n“Learning Important Features\nThrough Propagating Activation\nDifferences.” In Proceedings of the 34th\nInternational Conference on\nMachine Learning, 3145–53. PMLR. https://proceedings.mlr.press/v70/shrikumar17a.html.\n\n\nShrikumar, Avanti, Katherine Tian, Žiga Avsec, Anna Shcherbina,\nAbhimanyu Banerjee, Mahfuza Sharmin, Surag Nair, and Anshul Kundaje.\n2018. “Technical Note on Transcription\nFactor Motif Discovery from\nImportance Scores\n(TF-MoDISco) Version 0.5.6.5.” arXiv.\nhttps://doi.org/10.48550/arXiv.1811.00416.\n\n\nSiepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs,\nMinmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005.\n“[PhastCons] Evolutionarily Conserved\nElements in Vertebrate, Insect, Worm, and Yeast Genomes.”\nGenome Research 15 (8): 1034–50. https://doi.org/10.1101/gr.3715005.\n\n\nSirugo, Giorgio, Scott M. Williams, and Sarah A. Tishkoff. 2019.\n“The Missing Diversity in\nHuman Genetic Studies.”\nCell 177 (1): 26–31. https://doi.org/10.1016/j.cell.2019.02.048.\n\n\nSmolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W.\nHorner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024.\n“Detection of Mosaic and Population-Level Structural Variants with\nSniffles2.” Nature Biotechnology 42 (10):\n1571–80. https://doi.org/10.1038/s41587-023-02024-y.\n\n\nSnell, Jake, Kevin Swersky, and Richard Zemel. 2017. “Prototypical\nNetworks for Few-Shot\nLearning.” In Advances in Neural\nInformation Processing\nSystems. Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html.\n\n\n“Software as a Medical Device\n(SaMD): Clinical Evaluation\n International Medical\nDevice Regulators Forum.”\n2017. https://www.imdrf.org/documents/software-medical-device-samd-clinical-evaluation.\n\n\n“Software as a Medical Device:\nPossible Framework for Risk\nCategorization and Corresponding\nConsiderations  International\nMedical Device Regulators\nForum.” 2014. https://www.imdrf.org/documents/software-medical-device-possible-framework-risk-categorization-and-corresponding-considerations.\n\n\nSoice, Emily H., Rafael Rocha, Kimberlee Cordova, Michael Specter, and\nKevin M. Esvelt. 2023. “Can Large Language Models Democratize\nAccess to Dual-Use Biotechnology?” arXiv. https://doi.org/10.48550/arXiv.2306.03809.\n\n\nSollis, Elliot, Abayomi Mosaku, Ala Abid, Annalisa Buniello, Maria\nCerezo, Laurent Gil, Tudor Groza, et al. 2023. “The\nNHGRI-EBI GWAS\nCatalog: Knowledgebase and Deposition Resource.”\nNucleic Acids Research 51 (D1): D977–85. https://doi.org/10.1093/nar/gkac1010.\n\n\nSong, Li, Gali Bai, X. Shirley Liu, Bo Li, and Heng Li. 2022.\n“T1K: Efficient and Accurate KIR and\nHLA Genotyping with Next-Generation Sequencing\nData.” bioRxiv. https://doi.org/10.1101/2022.10.26.513955.\n\n\nStenson, Peter D., Matthew Mort, Edward V. Ball, Katy Evans, Matthew\nHayden, Sally Heywood, Michelle Hussain, Andrew D. Phillips, and David\nN. Cooper. 2017. “The Human Gene\nMutation Database: Towards a Comprehensive\nRepository of Inherited Mutation Data for Medical Research, Genetic\nDiagnosis and Next-Generation Sequencing Studies.” Human\nGenetics 136 (6): 665–77. https://doi.org/10.1007/s00439-017-1779-6.\n\n\nSu, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng\nLiu. 2024. “RoFormer: Enhanced\nTransformer with Rotary Position\nEmbedding.” Neurocomputing 568 (February):\n127063. https://doi.org/10.1016/j.neucom.2023.127063.\n\n\nSudlow, Cathie, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton,\nJohn Danesh, Paul Downey, et al. 2015. “UK\nBiobank: An Open\nAccess Resource for Identifying\nthe Causes of a Wide Range of\nComplex Diseases of Middle and\nOld Age.” PLOS Medicine 12\n(3): e1001779. https://doi.org/10.1371/journal.pmed.1001779.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N.\nPhan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023.\n“Leveraging Base-Pair Mammalian Constraint to Understand Genetic\nVariation and Human Disease.” Science 380 (6643):\neabn2937. https://doi.org/10.1126/science.abn2937.\n\n\nSundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. “Axiomatic\nAttribution for Deep\nNetworks.” In Proceedings of the 34th\nInternational Conference on\nMachine Learning, 3319–28. PMLR. https://proceedings.mlr.press/v70/sundararajan17a.html.\n\n\nSuzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and\nCathy H. Wu. 2007. “UniRef: Comprehensive and\nNon-Redundant UniProt Reference Clusters.”\nBioinformatics 23 (10): 1282–88. https://doi.org/10.1093/bioinformatics/btm098.\n\n\nTan, Jimin, Nina Shenker-Tauris, Javier Rodriguez-Hernaez, Eric Wang,\nTheodore Sakellaropoulos, Francesco Boccalatte, Palaniraja Thandapani,\net al. 2023. “Cell-Type-Specific Prediction of 3D\nChromatin Organization Enables High-Throughput in Silico Genetic\nScreening.” Nature Biotechnology 41 (8): 1140–50. https://doi.org/10.1038/s41587-022-01612-8.\n\n\nTate, John G, Sally Bamford, Harry C Jubb, Zbyslaw Sondka, David M\nBeare, Nidhi Bindal, Harry Boutselakis, et al. 2019.\n“COSMIC: The Catalogue Of\nSomatic Mutations In\nCancer.” Nucleic Acids Research 47 (D1):\nD941–47. https://doi.org/10.1093/nar/gky1015.\n\n\nTavtigian, Sean V., Marc S. Greenblatt, Steven M. Harrison, Robert L.\nNussbaum, Snehit A. Prabhu, Kenneth M. Boucher, and Leslie G. Biesecker.\n2018. “Modeling the ACMG/AMP Variant\nClassification Guidelines as a Bayesian Classification\nFramework.” Genetics in Medicine 20 (9): 1054–60. https://doi.org/10.1038/gim.2017.210.\n\n\nTHE GTEX CONSORTIUM. 2020. “The GTEx\nConsortium Atlas of Genetic Regulatory Effects Across Human\nTissues.” Science 369 (6509): 1318–30. https://doi.org/10.1126/science.aaz1776.\n\n\nTHE TABULA SAPIENS CONSORTIUM. 2022. “The Tabula\nSapiens: A Multiple-Organ, Single-Cell\nTranscriptomic Atlas of Humans.” Science 376 (6594):\neabl4896. https://doi.org/10.1126/science.abl4896.\n\n\nTheodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina\nR. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023.\n“[Geneformer] Transfer Learning Enables\nPredictions in Network Biology.” Nature 618 (7965):\n616–24. https://doi.org/10.1038/s41586-023-06139-9.\n\n\nTipirneni, Sindhu, and Chandan K. Reddy. 2022.\n“Self-Supervised Transformer for\nSparse and Irregularly Sampled\nMultivariate Clinical\nTime-Series.” ACM Trans. Knowl.\nDiscov. Data 16 (6): 105:1–17. https://doi.org/10.1145/3516367.\n\n\nTorkamani, Ali, Nathan E. Wineinger, and Eric J. Topol. 2018. “The\nPersonal and Clinical Utility of Polygenic Risk Scores.”\nNature Reviews Genetics 19 (9): 581–90. https://doi.org/10.1038/s41576-018-0018-x.\n\n\nVan der Auwera, Geraldine A., Mauricio O. Carneiro, Christopher Hartl,\nRyan Poplin, Guillermo del Angel, Ami Levy-Moonshine, Tadeusz Jordan, et\nal. 2018. “From FastQ Data to\nHigh-Confidence Variant\nCalls: The Genome\nAnalysis Toolkit Best\nPractices Pipeline.” Current\nProtocols in Bioinformatics 43 (1): 11.10.1–33. https://doi.org/10.1002/0471250953.bi1110s43.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.\n“Attention Is All You\nNeed.” arXiv. https://doi.org/10.48550/arXiv.1706.03762.\n\n\nVilhjálmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev,\nSara Lindström, Stephan Ripke, Giulio Genovese, et al. 2015.\n“Modeling Linkage Disequilibrium\nIncreases Accuracy of Polygenic\nRisk Scores.” American Journal of\nHuman Genetics 97 (4): 576–92. https://doi.org/10.1016/j.ajhg.2015.09.001.\n\n\nVisscher, Peter M., William G. Hill, and Naomi R. Wray. 2008.\n“Heritability in the Genomics Era — Concepts and\nMisconceptions.” Nature Reviews Genetics 9 (4): 255–66.\nhttps://doi.org/10.1038/nrg2322.\n\n\nVõsa, Urmo, Annique Claringbould, Harm-Jan Westra, Marc Jan Bonder,\nPatrick Deelen, Biao Zeng, Holger Kirsten, et al. 2021.\n“Large-Scale Cis- and Trans-eQTL\nAnalyses Identify Thousands of Genetic Loci and Polygenic Scores That\nRegulate Blood Gene Expression.” Nature Genetics 53 (9):\n1300–1310. https://doi.org/10.1038/s41588-021-00913-z.\n\n\nWang, Dequan, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor\nDarrell. 2021. “Tent: Fully Test-Time\nAdaptation by Entropy\nMinimization.” arXiv. https://doi.org/10.48550/arXiv.2006.10726.\n\n\nWang, Gao, Abhishek Sarkar, Peter Carbonetto, and Matthew Stephens.\n2020. “A Simple New\nApproach to Variable Selection in\nRegression, with Application to\nGenetic Fine Mapping.”\nJournal of the Royal Statistical Society Series B: Statistical\nMethodology 82 (5): 1273–1300. https://doi.org/10.1111/rssb.12388.\n\n\nWang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020.\n“Linformer: Self-Attention with\nLinear Complexity.” arXiv. https://doi.org/10.48550/arXiv.2006.04768.\n\n\nWang, Zirui, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. 2019.\n“Characterizing and Avoiding Negative\nTransfer.” In, 11293–302. https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Characterizing_and_Avoiding_Negative_Transfer_CVPR_2019_paper.html.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L.\nTrippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023.\n“De Novo Design of Protein Structure and Function with\nRFdiffusion.” Nature 620 (7976): 1089–1100.\nhttps://doi.org/10.1038/s41586-023-06415-8.\n\n\nWeissbrod, Omer, Farhad Hormozdiari, Christian Benber, Roeland Buber,\nSteven Gazal, Ross Dann, Po-Ru Loh, et al. 2020. “Functionally\nInformed Fine-Mapping and Polygenic Localization of Complex Trait\nHeritability.” Nature Genetics 52 (12): 1355–63. https://doi.org/10.1038/s41588-020-00735-5.\n\n\nWenger, Aaron M., Paul Peluso, William J. Rowell, Pi-Chuan Chang,\nRichard J. Hall, Gregory T. Concepcion, Jana Ebler, et al. 2019.\n“Accurate Circular Consensus Long-Read Sequencing Improves Variant\nDetection and Assembly of a Human Genome.” Nature\nBiotechnology 37 (10): 1155–62. https://doi.org/10.1038/s41587-019-0217-9.\n\n\nWhirl-Carrillo, M, E M McDonagh, J M Hebert, L Gong, K Sangkuhl, C F\nThorn, R B Altman, and T E Klein. 2012. “Pharmacogenomics\nKnowledge for Personalized\nMedicine.” Clinical Pharmacology &\nTherapeutics 92 (4): 414–17. https://doi.org/10.1038/clpt.2012.96.\n\n\nWu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray,\nPeter M. Visscher, and Jian Zeng. 2024. “Genome-Wide Fine-Mapping\nImproves Identification of Causal Variants.” Research\nSquare, August, rs.3.rs–4759390. https://doi.org/10.21203/rs.3.rs-4759390/v1.\n\n\nXiong, Ruibin, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,\nHuishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. 2020. “On\nLayer Normalization in the\nTransformer Architecture.” In\nProceedings of the 37th International\nConference on Machine\nLearning, 10524–33. PMLR. https://proceedings.mlr.press/v119/xiong20b.html.\n\n\nXu, Leqi, Wangjie Zheng, Jiaqi Hu, Yingxin Lin, Jia Zhao, Gefei Wang,\nTianyu Liu, and Hongyu Zhao. 2025. “Improving Polygenic Risk\nPrediction Performance by Integrating Electronic Health Records Through\nPhenotype Embedding.” The American Journal of Human\nGenetics 112 (12): 3030–45. https://doi.org/10.1016/j.ajhg.2025.11.006.\n\n\nYang, Jian, Beben Benyamin, Brian P. McEvoy, Scott Gordon, Anjali K.\nHenders, Dale R. Nyholt, Pamela A. Madden, et al. 2010. “Common\nSNPs Explain a Large Proportion of the Heritability for\nHuman Height.” Nature Genetics 42 (7): 565–69. https://doi.org/10.1038/ng.608.\n\n\nYang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan\nSalakhutdinov, and Quoc V. Le. 2020. “XLNet:\nGeneralized Autoregressive\nPretraining for Language\nUnderstanding.” arXiv. https://doi.org/10.48550/arXiv.1906.08237.\n\n\nYengo, Loïc, Sailaja Vedantam, Eirini Marouli, Julia Sidorenko, Eric\nBartell, Saori Sakaue, Marielisa Graff, et al. 2022. “A Saturated\nMap of Common Genetic Variants Associated with Human Height.”\nNature 610 (7933): 704–12. https://doi.org/10.1038/s41586-022-05275-y.\n\n\nYeo, Gene, and Christopher B. Burge. 2004b. “Maximum\nEntropy Modeling of Short\nSequence Motifs with Applications\nto RNA Splicing Signals.”\nJournal of Computational Biology 11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\n———. 2004a. “Maximum Entropy Modeling of\nShort Sequence Motifs with\nApplications to RNA Splicing\nSignals.” Journal of Computational Biology\n11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\nYun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll,\nand Cory Y McLean. 2021. “Accurate, Scalable Cohort Variant Calls\nUsing DeepVariant and GLnexus.”\nBioinformatics 36 (24): 5582–89. https://doi.org/10.1093/bioinformatics/btaa1081.\n\n\nZheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei\nSun, Chen-Hao Chen, et al. 2019. “Cistrome Data\nBrowser: Expanded Datasets and New Tools for Gene\nRegulatory Analysis.” Nucleic Acids Research 47 (D1):\nD729–35. https://doi.org/10.1093/nar/gky1094.\n\n\nZheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam,\nand Ruibang Luo. 2022. “Symphonizing Pileup and Full-Alignment for\nDeep Learning-Based Long-Read Variant Calling.” Nature\nComputational Science 2 (12): 797–803. https://doi.org/10.1038/s43588-022-00387-x.\n\n\nZhou, Jian. 2022. “Sequence-Based Modeling of Three-Dimensional\nGenome Architecture from Kilobase to Chromosome Scale.”\nNature Genetics 54 (5): 725–34. https://doi.org/10.1038/s41588-022-01065-4.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K.\nWong, and Olga G. Troyanskaya. 2018. “[Expecto]\nDeep Learning Sequence-Based Ab Initio Prediction of\nVariant Effects on Expression and Disease Risk.” Nature\nGenetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Jian, and Olga G. Troyanskaya. 2015. “[DeepSEA]\nPredicting Effects of Noncoding Variants with Deep\nLearning–Based Sequence Model.” Nature Methods 12 (10):\n931–34. https://doi.org/10.1038/nmeth.3547.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and\nHan Liu. 2024. “DNABERT-2: Efficient\nFoundation Model and Benchmark\nFor Multi-Species\nGenome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.\n\n\nZhu, Ligeng, Zhijian Liu, and Song Han. 2019. “Deep\nLeakage from Gradients.” arXiv. https://doi.org/10.48550/arXiv.1906.08935.\n\n\nZook, Justin M., Jennifer McDaniel, Nathan D. Olson, Justin Wagner,\nHemang Parikh, Haynes Heaton, Sean A. Irvine, et al. 2019. “An\nOpen Resource for Accurately Benchmarking Small Variant and Reference\nCalls.” Nature Biotechnology 37 (5): 561–66. https://doi.org/10.1038/s41587-019-0074-6.\n\n\nZvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang,\nCindy Orozco Bohorquez, Austin Clyde, et al. 2022.\n“GenSLMs: Genome-Scale Language Models\nReveal SARS-CoV-2 Evolutionary\nDynamics.” bioRxiv. https://doi.org/10.1101/2022.10.10.511571.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html",
    "href": "appendix/app-a-dl.html",
    "title": "Appendix A — Deep Learning Primer",
    "section": "",
    "text": "A.1 Neural Networks as Function Approximators\nA neural network is a parameterized function that maps inputs to outputs through a series of transformations. For genomic applications, inputs might be DNA sequences, protein sequences, or variant annotations; outputs might be pathogenicity scores, expression predictions, or functional class probabilities.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html#sec-apx-a-nn-basics",
    "href": "appendix/app-a-dl.html#sec-apx-a-nn-basics",
    "title": "Appendix A — Deep Learning Primer",
    "section": "",
    "text": "A.1.1 The Perceptron and Linear Layers\nThe simplest neural network component, the perceptron, computes a weighted sum of inputs plus a bias term:\n\\[y = \\sigma\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)\\]\nwhere \\(\\mathbf{x}\\) is the input vector, \\(\\mathbf{w}\\) are learnable weights, \\(b\\) is a learnable bias, and \\(\\sigma\\) is an activation function. A linear layer (also called a fully connected or dense layer) extends this to multiple outputs by using a weight matrix \\(\\mathbf{W}\\) instead of a vector:\n\\[\\mathbf{y} = \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b})\\]\n\n\nA.1.2 Activation Functions\nWithout nonlinear activation functions, stacking linear layers produces only linear transformations (the composition of linear functions is linear). Activation functions introduce nonlinearity, enabling networks to learn complex mappings.\n\n\n\n\n\n\n\n\nFunction\nFormula\nProperties\n\n\n\n\nReLU\n\\(\\max(0, x)\\)\nSimple, fast; standard default\n\n\nGELU\n\\(x \\cdot \\Phi(x)\\)\nSmooth; used in transformers\n\n\nSigmoid\n\\(1/(1 + e^{-x})\\)\nOutput in (0, 1); used for probabilities\n\n\nSoftmax\n\\(e^{x_i}/\\sum_j e^{x_j}\\)\nOutput sums to 1; used for classification\n\n\nTanh\n\\((e^x - e^{-x})/(e^x + e^{-x})\\)\nOutput in (-1, 1); centered\n\n\n\nReLU (Rectified Linear Unit) is the most common choice for hidden layers due to computational efficiency and good gradient properties. GELU (Gaussian Error Linear Unit) has become standard in transformer architectures. Softmax converts a vector of scores into a probability distribution and is typically used in the final layer for classification tasks.\n\n\nA.1.3 Depth and Width\nA network’s depth refers to the number of layers; its width refers to the number of units per layer. Deeper networks can represent more complex hierarchical features but are harder to train. Wider networks have more capacity per layer but may require more data to avoid overfitting.\nModern genomic foundation models are both deep (dozens to hundreds of layers) and wide (thousands of units per layer), requiring specialized training techniques and substantial computational resources.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html#sec-apx-a-training",
    "href": "appendix/app-a-dl.html#sec-apx-a-training",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.2 Training Neural Networks",
    "text": "A.2 Training Neural Networks\nTraining a neural network means finding parameter values that minimize a loss function measuring the discrepancy between predictions and targets.\n\nA.2.1 Loss Functions\nThe loss function quantifies prediction error. Common choices:\nCross-entropy loss for classification measures the divergence between predicted probabilities and true labels:\n\\[\\mathcal{L} = -\\sum_{i} y_i \\log(\\hat{y}_i)\\]\nwhere \\(y_i\\) is the true label (1 for correct class, 0 otherwise) and \\(\\hat{y}_i\\) is the predicted probability.\nMean squared error for regression measures average squared difference:\n\\[\\mathcal{L} = \\frac{1}{n}\\sum_{i}(y_i - \\hat{y}_i)^2\\]\n\n\nA.2.2 Gradient Descent and Backpropagation\nNeural networks are trained using gradient descent: iteratively adjusting parameters in the direction that reduces the loss. The gradient (partial derivatives of the loss with respect to each parameter) indicates the direction of steepest increase; moving opposite to the gradient decreases the loss.\nBackpropagation efficiently computes gradients by applying the chain rule layer by layer, propagating error signals backward from the output to the input. This algorithm makes training deep networks computationally tractable.\nThe learning rate \\(\\eta\\) controls step size:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\mathcal{L}\\]\nToo large a learning rate causes unstable training; too small a rate causes slow convergence.\n\n\nA.2.3 Stochastic Gradient Descent and Minibatches\nComputing gradients over the entire dataset is expensive. Stochastic gradient descent (SGD) approximates the full gradient using random subsets (minibatches) of training examples. This introduces noise but enables efficient training on large datasets and can help escape local minima.\nBatch size affects training dynamics: larger batches provide more stable gradient estimates but may converge to sharper minima that generalize worse; smaller batches introduce more noise but often find flatter minima with better generalization.\n\n\nA.2.4 Optimizers\nModern optimizers improve on basic SGD:\n\n\n\n\n\n\n\nOptimizer\nKey Feature\n\n\n\n\nSGD with momentum\nAccumulates gradient history for smoother updates\n\n\nAdam\nAdapts learning rate per-parameter; default choice\n\n\nAdamW\nAdam with decoupled weight decay; standard for transformers\n\n\nLAMB\nLayer-wise adaptive rates; enables large batch training\n\n\n\nAdam (Adaptive Moment Estimation) maintains running averages of gradients and squared gradients, adapting the learning rate for each parameter. It is the default optimizer for most deep learning applications. AdamW adds proper weight decay regularization and is standard for transformer training.\n\n\nA.2.5 Regularization\nRegularization techniques prevent overfitting by constraining model complexity:\nWeight decay (L2 regularization) penalizes large weights by adding \\(\\lambda \\|\\theta\\|^2\\) to the loss, encouraging simpler solutions.\nDropout randomly sets a fraction of activations to zero during training, preventing co-adaptation of features. At inference, all units are active but scaled appropriately.\nEarly stopping monitors validation loss during training and stops when it begins increasing, preventing the model from memorizing training data.\nData augmentation artificially expands training data by applying label-preserving transformations. For sequences, this might include reverse complementation (for strand-symmetric tasks) or random masking.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html#sec-apx-a-cnn",
    "href": "appendix/app-a-dl.html#sec-apx-a-cnn",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.3 Convolutional Neural Networks",
    "text": "A.3 Convolutional Neural Networks\nConvolutional neural networks (CNNs) are designed for data with spatial or sequential structure. They were the dominant architecture for genomic sequence analysis before transformers and remain important for certain applications.\n\nA.3.1 Convolution Operation\nA convolutional layer applies learnable filters (kernels) that slide across the input, computing dot products at each position. For a 1D sequence (like DNA), a filter of width \\(k\\) detects patterns of length \\(k\\) nucleotides:\n\\[y_i = \\sigma\\left(\\sum_{j=0}^{k-1} w_j \\cdot x_{i+j} + b\\right)\\]\nThe same filter is applied at every position, so the network learns position-invariant patterns. A filter trained to recognize a TATA box will detect it regardless of its location in the sequence.\n\n\nA.3.2 Key CNN Components\nMultiple filters learn different patterns. A layer with 64 filters of width 8 learns 64 different 8-bp motifs.\nPooling reduces spatial dimensions by taking the maximum or average over local regions, providing translation invariance and reducing computational cost.\nDilation inserts gaps in the filter, allowing detection of patterns spanning larger regions without increasing parameters. A dilated convolution with dilation rate 2 and filter width 3 spans 5 positions.\nStride controls how far the filter moves between applications. Stride &gt; 1 downsamples the output.\n\n\nA.3.3 CNNs for Genomics\nEarly genomic deep learning models (DeepSEA, Basset, DeepBind) used CNNs to predict regulatory function from DNA sequence. The architecture naturally captures motifs: first-layer filters learn individual transcription factor binding motifs; deeper layers combine these into higher-order regulatory logic.\nCNNs remain useful for:\n\nShort-range patterns: Splice sites, promoter elements, binding sites\nComputational efficiency: Faster training than transformers for local tasks\nInterpretability: First-layer filters directly correspond to sequence motifs\n\nLimitations include difficulty capturing long-range dependencies (addressed by dilated convolutions in Basenji) and lack of position-specific processing (every position is treated identically).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html#sec-apx-a-rnn",
    "href": "appendix/app-a-dl.html#sec-apx-a-rnn",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.4 Recurrent Neural Networks",
    "text": "A.4 Recurrent Neural Networks\nRecurrent neural networks (RNNs) process sequences by maintaining hidden state that accumulates information across positions. At each position, the network updates its hidden state based on the current input and previous state:\n\\[h_t = f(h_{t-1}, x_t)\\]\nThis allows modeling dependencies across arbitrary distances, in principle.\n\nA.4.1 LSTM and GRU\nBasic RNNs suffer from vanishing gradients: signals from distant positions decay exponentially, preventing learning of long-range dependencies.\nLong Short-Term Memory (LSTM) addresses this with gated units that control information flow, allowing the network to selectively remember or forget information across many steps.\nGated Recurrent Unit (GRU) is a simplified variant with fewer parameters that often performs comparably.\n\n\nA.4.2 Limitations\nRNNs process sequences sequentially, preventing parallelization and making training slow for long sequences. They also struggle with very long-range dependencies despite architectural improvements. These limitations motivated the development of attention mechanisms and transformers, which have largely replaced RNNs in genomic applications.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html#sec-apx-a-attention",
    "href": "appendix/app-a-dl.html#sec-apx-a-attention",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.5 Attention and Transformers",
    "text": "A.5 Attention and Transformers\nThe transformer architecture, introduced by Vaswani et al. (Vaswani et al. 2023), has become the foundation for modern language models and genomic foundation models. Its key innovation is the attention mechanism, which allows direct interaction between any two positions in a sequence.\n\nA.5.1 Self-Attention\nSelf-attention computes, for each position, a weighted combination of all positions based on their relevance. Given input representations \\(\\mathbf{X}\\), the mechanism computes:\n\nQueries \\(\\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q\\): What information is this position looking for?\nKeys \\(\\mathbf{K} = \\mathbf{X}\\mathbf{W}_K\\): What information does this position contain?\nValues \\(\\mathbf{V} = \\mathbf{X}\\mathbf{W}_V\\): What information should be retrieved?\n\nAttention weights are computed as:\n\\[\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\\]\nThe softmax ensures weights sum to 1; the \\(\\sqrt{d_k}\\) scaling prevents dot products from growing too large for high-dimensional representations.\n\n\nA.5.2 Multi-Head Attention\nMulti-head attention runs several attention operations in parallel with different learned projections, allowing the model to attend to different types of relationships simultaneously:\n\\[\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}_O\\]\nEach head might capture different patterns: one head might attend to nearby positions for local context, another to conserved positions across the sequence, another to structurally related positions.\n\n\nA.5.3 Transformer Architecture\nA transformer layer combines multi-head attention with a feed-forward network and residual connections:\nInput → LayerNorm → Multi-Head Attention → + → LayerNorm → Feed-Forward → + → Output\n         ↑_____________________________|         ↑_____________________|\n                (residual connection)                (residual connection)\nLayer normalization stabilizes training by normalizing activations.\nResidual connections add the input directly to the output, allowing gradients to flow unchanged and enabling training of very deep networks.\nFeed-forward networks (typically two linear layers with GELU activation) process each position independently, providing additional transformation capacity.\n\n\nA.5.4 Positional Encoding\nSelf-attention is permutation-invariant: it treats positions as a set, not a sequence. Positional encodings inject position information, either through learned embeddings or fixed sinusoidal patterns. For genomic sequences, positional encoding enables the model to learn position-dependent patterns (like distance from transcription start sites).\n\n\nA.5.5 Encoder vs. Decoder\nEncoder transformers (like BERT, DNABERT) use bidirectional attention: each position attends to all positions. They excel at classification and embedding tasks.\nDecoder transformers (like GPT, HyenaDNA in autoregressive mode) use causal attention: each position attends only to preceding positions. They excel at generation tasks.\nEncoder-decoder transformers use both, with the decoder attending to encoder outputs. Less common in genomics.\n\n\nA.5.6 Computational Complexity\nStandard attention scales quadratically with sequence length (\\(O(n^2)\\)), limiting context length. A 200 kb genomic sequence would require attention over 200,000 positions, demanding enormous memory.\nEfficient attention variants address this:\n\nSparse attention: Attend only to local windows plus global tokens\nLinear attention: Approximate attention with linear complexity\nFlash attention: Exact attention with optimized memory access patterns\n\nModels like HyenaDNA use alternative architectures (state space models) to achieve sub-quadratic scaling while maintaining long-range modeling.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html#sec-apx-a-embeddings",
    "href": "appendix/app-a-dl.html#sec-apx-a-embeddings",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.6 Embeddings and Representations",
    "text": "A.6 Embeddings and Representations\nEmbeddings are dense vector representations of discrete inputs. Rather than representing a nucleotide as a one-hot vector (A = [1,0,0,0]), an embedding maps it to a learned vector in continuous space (A = [0.2, -0.5, 0.8, …]).\n\nA.6.1 Token Embeddings\nThe embedding layer is a lookup table mapping each token (nucleotide, k-mer, amino acid) to a vector. These embeddings are learned during training, with similar tokens (functionally similar amino acids, for instance) often ending up with similar vectors.\n\n\nA.6.2 Contextual Embeddings\nUnlike static embeddings (where each token always has the same representation), transformer outputs are contextual: the same token has different representations depending on its context. An alanine in a buried hydrophobic core has a different representation than an alanine on a solvent-exposed surface, because the surrounding context is different.\nThese contextual embeddings capture rich information about each position’s functional role and can be extracted for downstream tasks.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html#sec-apx-a-pretraining",
    "href": "appendix/app-a-dl.html#sec-apx-a-pretraining",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.7 Pretraining and Transfer Learning",
    "text": "A.7 Pretraining and Transfer Learning\nPretraining trains a model on a large dataset with a self-supervised objective (one that does not require human labels), then fine-tunes or adapts the model for specific downstream tasks. This approach leverages abundant unlabeled data to learn general representations.\n\nA.7.1 Self-Supervised Objectives\nMasked language modeling (MLM): Randomly mask tokens and train the model to predict them from context. Used by BERT, DNABERT, ESM. Captures bidirectional context.\nNext-token prediction: Train the model to predict the next token given all preceding tokens. Used by GPT, HyenaDNA. Enables generation.\nContrastive learning: Train the model to distinguish related from unrelated examples. Useful for learning representations without reconstruction.\n\n\nA.7.2 Transfer Learning\nAfter pretraining, the model can be adapted to downstream tasks:\n\nLinear probing: Freeze pretrained weights, train only a new output layer\nFine-tuning: Update all or some pretrained weights on labeled data\nParameter-efficient fine-tuning: Update only small adapter modules\n\nSee Chapter 9 for detailed treatment of transfer learning strategies.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html#sec-apx-a-practical",
    "href": "appendix/app-a-dl.html#sec-apx-a-practical",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.8 Practical Considerations",
    "text": "A.8 Practical Considerations\n\nA.8.1 Hardware Requirements\nDeep learning requires specialized hardware:\n\nGPUs: Graphics processing units optimized for parallel matrix operations\nTPUs: Tensor processing units designed specifically for neural networks\nMemory: Large models require substantial GPU memory (VRAM)\n\nFoundation models with billions of parameters require multiple high-end GPUs or TPUs for training; smaller models can run on single consumer GPUs for inference.\n\n\nA.8.2 Software Frameworks\n\n\n\nFramework\nDescription\n\n\n\n\nPyTorch\nDominant framework; flexible, research-friendly\n\n\nTensorFlow\nProduction-focused; strong deployment tools\n\n\nJAX\nFunctional approach; used by DeepMind\n\n\nHuggingFace\nModel hub and high-level training utilities\n\n\n\nMost genomic foundation models are implemented in PyTorch and distributed through HuggingFace.\n\n\nA.8.3 Common Pitfalls\nOverfitting: Model memorizes training data instead of learning generalizable patterns. Detect via validation loss diverging from training loss. Address with regularization, more data, or simpler models.\nUnderfitting: Model fails to capture data patterns. Detect via high training loss. Address with larger models, longer training, or better architectures.\nVanishing/exploding gradients: Gradients become too small or large for stable training. Address with proper initialization, normalization, and residual connections.\nData leakage: Test data information inadvertently appears in training, inflating performance estimates. Ensure strict separation of training, validation, and test sets.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-a-dl.html#sec-apx-a-further",
    "href": "appendix/app-a-dl.html#sec-apx-a-further",
    "title": "Appendix A — Deep Learning Primer",
    "section": "A.9 Further Reading",
    "text": "A.9 Further Reading\nThis primer covers only the essentials. For deeper understanding:\n\nFundamentals: Goodfellow et al., Deep Learning (Section E.1)\nTransformers: Vaswani et al. (2017), “Attention Is All You Need”\nGenomic applications: Main text chapters, especially Chapter 5 through Chapter 9\nPractical tutorials: fast.ai course, D2L.ai (Section E.2)\n\n\n\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://doi.org/10.48550/arXiv.1706.03762.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Deep Learning Primer</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html",
    "href": "appendix/app-b-compute.html",
    "title": "Appendix B — Deployment and Compute",
    "section": "",
    "text": "B.1 Hardware Landscape\nGenomic foundation models span a wide range of computational requirements. Understanding hardware options helps practitioners match resources to their specific needs.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html#sec-apx-b-hardware",
    "href": "appendix/app-b-compute.html#sec-apx-b-hardware",
    "title": "Appendix B — Deployment and Compute",
    "section": "",
    "text": "B.1.1 GPU Computing\nGraphics Processing Units (GPUs) are the workhorses of deep learning, providing thousands of parallel cores optimized for matrix operations. Key specifications:\n\n\n\n\n\n\n\n\nMetric\nDescription\nRelevance\n\n\n\n\nVRAM\nGPU memory\nDetermines maximum model/batch size\n\n\nCompute (TFLOPS)\nFloating-point operations per second\nDetermines training/inference speed\n\n\nMemory bandwidth\nData transfer rate\nCritical for transformer attention\n\n\nTensor cores\nSpecialized matrix units\nAccelerate mixed-precision operations\n\n\n\n\n\nB.1.2 Consumer vs. Data Center GPUs\n\n\n\nGPU Class\nExamples\nVRAM\nUse Case\n\n\n\n\nConsumer\nRTX 4090\n24 GB\nSmall model inference, development\n\n\nWorkstation\nRTX A6000\n48 GB\nMedium model training/inference\n\n\nData center\nA100\n40/80 GB\nLarge model training\n\n\nLatest generation\nH100\n80 GB\nFoundation model training\n\n\n\nMemory is typically the bottleneck. A 3-billion parameter model in FP16 requires approximately 6 GB just for weights, plus additional memory for activations, gradients (if training), and KV cache (for transformers). The A100 80GB enables training models that would require multi-GPU setups on smaller cards.\n\n\nB.1.3 TPUs\nTensor Processing Units (TPUs) are Google’s custom accelerators, available through Google Cloud. They offer:\n\nHigh memory bandwidth optimized for matrix operations\nEfficient multi-device scaling through dedicated interconnects\nCost-effective for large-scale training\n\nMany DeepMind models (AlphaFold, Enformer) were trained on TPUs. The JAX framework provides the best TPU support.\n\n\nB.1.4 Multi-GPU and Distributed Training\nLarge models require multiple GPUs:\nData parallelism replicates the model across GPUs, each processing different batches. Gradients are synchronized after each step. Scales batch size but not model size.\nModel parallelism splits the model across GPUs: - Tensor parallelism: Splits individual layers across GPUs - Pipeline parallelism: Assigns different layers to different GPUs\nFully Sharded Data Parallel (FSDP) and DeepSpeed ZeRO combine approaches, sharding model states across GPUs to train models larger than any single GPU’s memory.\n\n\nB.1.5 CPU Inference\nFor smaller models or low-throughput applications, CPU inference may suffice:\n\nAvoids GPU procurement and maintenance\nEnables deployment on standard servers\nSuitable for models with &lt;1B parameters\nCan be accelerated with ONNX Runtime, Intel MKL",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html#sec-apx-b-cloud",
    "href": "appendix/app-b-compute.html#sec-apx-b-cloud",
    "title": "Appendix B — Deployment and Compute",
    "section": "B.2 Cloud Platforms",
    "text": "B.2 Cloud Platforms\nCloud computing provides on-demand access to GPU resources without capital expenditure.\n\nB.2.1 Major Providers\n\n\n\nProvider\nGPU Options\nStrengths\n\n\n\n\nAWS\nA100, H100, Trainium\nBroadest ecosystem, SageMaker\n\n\nGoogle Cloud\nA100, TPU v4/v5\nTPU access, Vertex AI\n\n\nAzure\nA100, H100\nEnterprise integration, Azure ML\n\n\nLambda Labs\nA100, H100\nML-focused, simpler pricing\n\n\nCoreWeave\nA100, H100\nGPU-specialized, Kubernetes-native\n\n\n\n\n\nB.2.2 Cost Considerations\nGPU costs vary significantly:\n\n\n\nResource\nApproximate Cost (2024)\n\n\n\n\nA100 40GB (on-demand)\n$3–4/hour\n\n\nA100 80GB (on-demand)\n$4–5/hour\n\n\nH100 (on-demand)\n$5–8/hour\n\n\nA100 (spot/preemptible)\n$1–2/hour\n\n\n\nSpot instances offer 60–80% discounts but can be interrupted. Suitable for: - Checkpointed training runs - Batch inference jobs - Non-time-critical workloads\nReserved instances provide discounts for committed usage but require upfront planning.\n\n\nB.2.3 Managed ML Platforms\n\n\n\nPlatform\nFeatures\n\n\n\n\nAWS SageMaker\nTraining, hosting, MLOps pipelines\n\n\nGoogle Vertex AI\nTraining, prediction, feature store\n\n\nAzure ML\nTraining, deployment, monitoring\n\n\nHuggingFace Inference Endpoints\nOne-click model deployment\n\n\n\nThese platforms handle infrastructure but add cost overhead. They’re valuable for production deployments requiring reliability, monitoring, and scaling.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html#sec-apx-b-deployment",
    "href": "appendix/app-b-compute.html#sec-apx-b-deployment",
    "title": "Appendix B — Deployment and Compute",
    "section": "B.3 Model Deployment",
    "text": "B.3 Model Deployment\nDeploying a model for real-world use requires careful consideration of latency, throughput, reliability, and cost.\n\nB.3.1 Inference Servers\nSpecialized inference servers optimize model serving:\n\n\n\n\n\n\n\nServer\nFeatures\n\n\n\n\nNVIDIA Triton\nMulti-framework, dynamic batching, model ensemble\n\n\nvLLM\nOptimized for LLM inference, PagedAttention\n\n\nTGI (Text Generation Inference)\nHuggingFace’s optimized inference server\n\n\nTorchServe\nPyTorch-native, simple deployment\n\n\n\nThese servers provide: - Dynamic batching: Combine requests for efficient GPU utilization - Model warmup: Pre-load models to reduce cold start - Health checks: Monitor model availability - Metrics: Track latency, throughput, errors\n\n\nB.3.2 API Design\nA typical genomic model API accepts sequences and returns predictions:\n# Request\n{\n    \"sequences\": [\"ATCGATCG...\", \"GCTAGCTA...\"],\n    \"return_embeddings\": false\n}\n\n# Response\n{\n    \"predictions\": [\n        {\"pathogenicity\": 0.87, \"confidence\": 0.92},\n        {\"pathogenicity\": 0.12, \"confidence\": 0.95}\n    ],\n    \"model_version\": \"v2.1.0\",\n    \"processing_time_ms\": 145\n}\nDesign considerations: - Batch endpoints for throughput-critical applications - Streaming for large outputs (embeddings, long sequences) - Versioning to manage model updates - Input validation to catch malformed sequences early\n\n\nB.3.3 Containerization\nDocker containers package models with dependencies:\nFROM pytorch/pytorch:2.0.0-cuda11.7\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY model/ /app/model/\nCOPY serve.py /app/\n\nEXPOSE 8080\nCMD [\"python\", \"/app/serve.py\"]\nContainers provide: - Reproducible environments - Easy deployment across platforms - Isolation from host system - Simplified scaling with Kubernetes\n\n\nB.3.4 Kubernetes Deployment\nKubernetes orchestrates containerized model deployments:\n\nHorizontal scaling: Add/remove replicas based on load\nGPU scheduling: Allocate GPUs to pods\nRolling updates: Deploy new versions without downtime\nResource limits: Prevent runaway memory/compute usage\n\nExample deployment:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: variant-predictor\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: model\n        image: variant-predictor:v2.1\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n            memory: \"32Gi\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html#sec-apx-b-optimization",
    "href": "appendix/app-b-compute.html#sec-apx-b-optimization",
    "title": "Appendix B — Deployment and Compute",
    "section": "B.4 Inference Optimization",
    "text": "B.4 Inference Optimization\nOptimizing inference reduces latency and cost.\n\nB.4.1 Quantization\nQuantization reduces numerical precision to decrease memory and computation:\n\n\n\nPrecision\nBits\nMemory\nSpeed\nQuality\n\n\n\n\nFP32\n32\n1×\n1×\nBaseline\n\n\nFP16/BF16\n16\n0.5×\n~2×\nMinimal loss\n\n\nINT8\n8\n0.25×\n~4×\nSmall loss\n\n\nINT4\n4\n0.125×\n~8×\nModerate loss\n\n\n\nPost-training quantization converts trained models without retraining. Works well for INT8; INT4 may require calibration data or quality monitoring.\nQuantization-aware training incorporates quantization during training for better INT4/INT8 quality.\nFor genomic models: - FP16/BF16 is standard and nearly lossless - INT8 often acceptable for classification tasks - INT4 requires careful validation, especially for regression outputs\n\n\nB.4.2 Model Pruning\nPruning removes unimportant weights:\n\nMagnitude pruning: Remove weights below threshold\nStructured pruning: Remove entire neurons/attention heads\nMovement pruning: Remove weights based on training dynamics\n\nPruning can achieve 50–90% sparsity with minimal accuracy loss on some tasks, but requires model-specific tuning.\n\n\nB.4.3 Knowledge Distillation\nDistillation trains a smaller “student” model to mimic a larger “teacher”:\n\nRun teacher model on large unlabeled corpus\nTrain student to match teacher outputs (soft labels)\nStudent learns compressed version of teacher’s knowledge\n\nEffective for: - Deploying on resource-constrained devices - Reducing inference cost for high-volume applications - Creating task-specific lightweight models\n\n\nB.4.4 ONNX and TensorRT\nONNX (Open Neural Network Exchange) provides a portable model format:\nimport torch.onnx\n\ntorch.onnx.export(model, sample_input, \"model.onnx\")\nTensorRT optimizes ONNX models for NVIDIA GPUs: - Layer fusion - Kernel auto-tuning - Precision calibration\nTensorRT can provide 2–5× speedup over naive PyTorch inference.\n\n\nB.4.5 Caching and Batching\nKV-cache stores attention key/value pairs for autoregressive generation, avoiding recomputation.\nSpeculative decoding uses a small draft model to propose tokens, verified in parallel by the main model.\nDynamic batching groups incoming requests: - Increases GPU utilization - Trades latency for throughput - Configurable wait time and batch size limits",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html#sec-apx-b-monitoring",
    "href": "appendix/app-b-compute.html#sec-apx-b-monitoring",
    "title": "Appendix B — Deployment and Compute",
    "section": "B.5 Benchmarking and Monitoring",
    "text": "B.5 Benchmarking and Monitoring\n\nB.5.1 Performance Metrics\n\n\n\n\n\n\n\n\nMetric\nDescription\nTarget\n\n\n\n\nLatency (p50)\nMedian response time\nApplication-dependent\n\n\nLatency (p99)\n99th percentile response time\nCritical for SLAs\n\n\nThroughput\nRequests/second\nScale with load\n\n\nGPU utilization\nGPU compute usage\n&gt;80% for efficiency\n\n\nMemory utilization\nGPU memory usage\nMonitor for OOM\n\n\n\n\n\nB.5.2 Monitoring Stack\nMetrics → Prometheus → Grafana (visualization)\n                    → AlertManager (alerts)\n\nLogs → Elasticsearch → Kibana (search/analysis)\n\nTraces → Jaeger/Zipkin (request tracing)\nKey monitoring points: - Request latency distribution - Error rates by error type - Queue depth (if using async processing) - Model prediction distribution (detect drift) - Input sequence characteristics (length, composition)\n\n\nB.5.3 Load Testing\nBefore production deployment:\n# Example with locust\nlocust -f load_test.py --host=http://model-api:8080 \\\n       --users=100 --spawn-rate=10 --run-time=10m\nTest scenarios: - Sustained load at expected traffic - Burst traffic (10× normal) - Long sequences (stress memory) - Concurrent batch requests",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html#sec-apx-b-cost",
    "href": "appendix/app-b-compute.html#sec-apx-b-cost",
    "title": "Appendix B — Deployment and Compute",
    "section": "B.6 Cost Optimization",
    "text": "B.6 Cost Optimization\n\nB.6.1 Right-Sizing\nMatch hardware to workload: - Don’t use A100 for models that fit on RTX 4090 - Use CPU inference for low-throughput applications - Consider spot instances for batch processing\n\n\nB.6.2 Autoscaling\nScale resources with demand:\n# Kubernetes HPA example\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nspec:\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\nScale-to-zero during idle periods for significant savings.\n\n\nB.6.3 Batch Processing\nFor non-real-time workloads: - Accumulate requests and process in batches - Use spot instances with checkpointing - Schedule during off-peak hours for lower costs\n\n\nB.6.4 Model Selection\nChoose appropriately sized models: - 110M parameter DNABERT vs. 2.5B Nucleotide Transformer - Evaluate if larger model accuracy justifies cost - Consider distilled/pruned versions for production",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html#sec-apx-b-security",
    "href": "appendix/app-b-compute.html#sec-apx-b-security",
    "title": "Appendix B — Deployment and Compute",
    "section": "B.7 Security Considerations",
    "text": "B.7 Security Considerations\n\nB.7.1 Data Privacy\nGenomic data is sensitive: - Process in compliant environments (HIPAA, GDPR) - Encrypt data at rest and in transit - Implement access controls and audit logging - Consider on-premises deployment for sensitive data\n\n\nB.7.2 Model Security\n\nInput validation: Reject malformed sequences\nRate limiting: Prevent abuse\nAuthentication: Require API keys/tokens\nModel versioning: Track deployed versions for reproducibility\n\n\n\nB.7.3 Federated Learning\nFor multi-institution collaboration: - Train on distributed data without centralization - Share only model updates, not raw data - Enables learning from diverse populations - See ?sec-ch09-emerging-approaches for details",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html#sec-apx-b-architecture",
    "href": "appendix/app-b-compute.html#sec-apx-b-architecture",
    "title": "Appendix B — Deployment and Compute",
    "section": "B.8 Reference Architecture",
    "text": "B.8 Reference Architecture\nA production genomic model deployment might include:\n                                    ┌─────────────┐\n                                    │  Model      │\n                                    │  Registry   │\n                                    └──────┬──────┘\n                                           │\n┌──────────┐    ┌──────────┐    ┌─────────▼─────────┐    ┌──────────┐\n│  Client  │───►│   API    │───►│  Inference Server │───►│  Cache   │\n│   App    │    │ Gateway  │    │  (Triton/vLLM)    │    │ (Redis)  │\n└──────────┘    └────┬─────┘    └─────────┬─────────┘    └──────────┘\n                     │                    │\n                     ▼                    ▼\n               ┌──────────┐        ┌──────────┐\n               │ Metrics  │        │   GPU    │\n               │(Prometheus)│       │ Cluster  │\n               └──────────┘        └──────────┘\nComponents: - API Gateway: Authentication, rate limiting, routing - Inference Server: Model hosting, batching, optimization - GPU Cluster: Kubernetes-managed GPU nodes - Cache: Store frequent predictions - Model Registry: Version and track deployed models - Metrics: Monitor performance and health",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-b-compute.html#sec-apx-b-checklist",
    "href": "appendix/app-b-compute.html#sec-apx-b-checklist",
    "title": "Appendix B — Deployment and Compute",
    "section": "B.9 Checklist for Production Deployment",
    "text": "B.9 Checklist for Production Deployment\nBefore deploying a genomic model to production:\nModel Validation\n\nValidated on held-out test set matching deployment distribution\nEvaluated across relevant subgroups (populations, variant types)\nCalibration assessed and documented\nFailure modes characterized\n\nInfrastructure\n\nHardware sized for expected load with headroom\nAutoscaling configured for traffic variation\nRollback procedure tested\nDisaster recovery plan in place\n\nMonitoring\n\nLatency and throughput metrics tracked\nError alerting configured\nInput/output distribution monitoring for drift\nResource utilization dashboards\n\nSecurity\n\nData encryption at rest and in transit\nAccess controls and authentication\nAudit logging enabled\nCompliance requirements verified\n\nDocumentation\n\nModel card with capabilities and limitations\nAPI documentation\nRunbook for common issues\nVersion history and changelog",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Deployment and Compute</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html",
    "href": "appendix/app-c-data-curation.html",
    "title": "Appendix C — Data Curation",
    "section": "",
    "text": "C.1 Data Sources\nGenomic foundation models draw from diverse data sources, each with distinct characteristics, biases, and access requirements.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html#sec-apx-c-sources",
    "href": "appendix/app-c-data-curation.html#sec-apx-c-sources",
    "title": "Appendix C — Data Curation",
    "section": "",
    "text": "C.1.1 Reference Genomes and Assemblies\nHuman reference genome (GRCh38/hg38) provides the coordinate system for human genomics:\n\n\n\nAssembly\nRelease\nKey Features\n\n\n\n\nGRCh38\n2013\nCurrent standard, alternate loci\n\n\nT2T-CHM13\n2022\nFirst complete human genome\n\n\nPangenome\n2023\nGraph-based, population diversity\n\n\n\nSpecies genomes from Ensembl, NCBI, and UCSC provide sequences for comparative genomics and cross-species pretraining.\nConsiderations: - Reference genome represents a single haplotype, missing population diversity - Repetitive regions and centromeres are poorly represented in older assemblies - Coordinate systems differ between assemblies; ensure consistency\n\n\nC.1.2 Population-Scale Sequencing\n\n\n\nResource\nSamples\nData Type\nAccess\n\n\n\n\ngnomAD\n730,000+\nExomes/genomes\nOpen\n\n\nUK Biobank\n500,000\nWGS, WES, arrays\nControlled\n\n\nAll of Us\n1,000,000+\nWGS\nControlled\n\n\nTOPMed\n180,000+\nWGS\nControlled\n\n\n1000 Genomes\n3,200\nWGS\nOpen\n\n\n\nPopulation databases provide variant frequency information and diverse genetic backgrounds. gnomAD’s open access makes it valuable for training; controlled-access resources like UK Biobank require applications and data use agreements.\n\n\nC.1.3 Protein Sequence Databases\n\n\n\nDatabase\nSequences\nCoverage\n\n\n\n\nUniRef100\n300M+\nNon-redundant proteins\n\n\nUniRef90\n150M+\n90% identity clusters\n\n\nUniRef50\n55M+\n50% identity clusters\n\n\nUniParc\n500M+\nAll known proteins\n\n\n\nUniRef provides clustered protein sequences at different identity thresholds, enabling control over sequence redundancy during pretraining. Lower redundancy (UniRef50) reduces training time but may miss sequence diversity; higher redundancy captures more variation but increases computational cost.\n\n\nC.1.4 Functional Annotation\n\n\n\nResource\nAssays\nCell Types\n\n\n\n\nENCODE\nChIP-seq, ATAC-seq, RNA-seq\n500+\n\n\nRoadmap\nHistone marks, DNase\n127\n\n\nGTEx\nRNA-seq\n54 tissues\n\n\nFANTOM5\nCAGE\n1,800+\n\n\n\nFunctional genomics data provides supervision signals for regulatory sequence models. ENCODE and Roadmap offer consistent protocols across cell types; GTEx provides tissue-specific expression. Data quality varies by assay and cell type; metadata review is essential.\n\n\nC.1.5 Clinical Variant Databases\n\n\n\nDatabase\nVariants\nCuration\n\n\n\n\nClinVar\n2M+\nSubmitter-dependent\n\n\nHGMD\n350K+\nExpert curated\n\n\nClinGen\nGenes + variants\nExpert panels\n\n\nLOVD\nGene-specific\nVariable\n\n\n\nClinVar provides the largest open-access collection of clinical variant interpretations but includes submitter variability and classification conflicts. See Chapter 2 for detailed discussion of ClinVar biases.\n\n\nC.1.6 Access and Licensing\nData access requirements vary:\n\n\n\nAccess Type\nExamples\nRequirements\n\n\n\n\nOpen\ngnomAD, 1000G, ClinVar\nNone\n\n\nRegistered\nENCODE, GTEx\nAccount creation\n\n\nControlled\nUK Biobank, dbGaP\nApplication, IRB approval\n\n\nCommercial\nHGMD Professional\nSubscription\n\n\n\nControlled-access data requires: - Institutional review board (IRB) approval - Data use agreements (DUA) - Secure computing environments - Compliance with return/destruction policies",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html#sec-apx-c-quality",
    "href": "appendix/app-c-data-curation.html#sec-apx-c-quality",
    "title": "Appendix C — Data Curation",
    "section": "C.2 Quality Filtering",
    "text": "C.2 Quality Filtering\nRaw genomic data contains errors from sequencing, alignment, and annotation. Quality filtering removes problematic entries before training.\n\nC.2.1 Sequence Quality Filters\nFor DNA sequences:\n\n\n\nFilter\nThreshold\nRationale\n\n\n\n\nN content\n&lt;5%\nRemoves poorly sequenced regions\n\n\nLow complexity\nDUST score\nRemoves repetitive sequence\n\n\nLength\nTask-dependent\nEnsures sufficient context\n\n\nGC content\nSpecies-appropriate\nFlags contamination\n\n\n\ndef filter_sequence(seq, max_n_frac=0.05, min_length=100):\n    \"\"\"Basic sequence quality filter.\"\"\"\n    n_frac = seq.count('N') / len(seq)\n    if n_frac &gt; max_n_frac:\n        return False\n    if len(seq) &lt; min_length:\n        return False\n    return True\nFor protein sequences:\n\n\n\nFilter\nThreshold\nRationale\n\n\n\n\nX content\n&lt;1%\nRemoves ambiguous residues\n\n\nLength\n30–10,000 AA\nFilters fragments and artifacts\n\n\nStop codons\n0 internal\nRemoves pseudogenes\n\n\n\n\n\nC.2.2 Variant Quality Filters\nVariant calls include false positives from sequencing errors and alignment artifacts:\n\n\n\nFilter\nTypical Threshold\nNotes\n\n\n\n\nQUAL\n&gt;20\nPhred-scaled quality\n\n\nDP\n&gt;10\nRead depth\n\n\nGQ\n&gt;20\nGenotype quality\n\n\nFILTER\nPASS\nCaller-specific filters\n\n\nAF_gnomAD\n&lt;0.01 for rare\nPopulation frequency\n\n\n\nGATK and other variant callers apply default filters; additional filtering may be needed for training data. Overly stringent filtering biases toward common variants; overly permissive filtering includes false positives.\n\n\nC.2.3 Annotation Quality\nClinical annotations have variable quality:\n\n\n\nQuality Indicator\nInterpretation\n\n\n\n\nReview status (ClinVar)\nStars indicate curation level\n\n\nSubmission count\nMore submissions increase confidence\n\n\nDate\nRecent annotations reflect current knowledge\n\n\nConflicts\nMultiple interpretations reduce reliability\n\n\n\nFiltering strategy for ClinVar: - Require ≥2 stars review status for training labels - Exclude variants with conflicting interpretations - Consider date cutoffs for temporal validation\n\n\nC.2.4 Handling Missing Data\nMissing annotations are common:\n\nExplicit missing: Marked as unknown/uncertain\nImplicit missing: Simply not annotated\n\nStrategies: - Exclude samples with critical missing fields - Impute where appropriate (mean, median, model-based) - Model missingness explicitly (separate category) - Document missing data rates",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html#sec-apx-c-dedup",
    "href": "appendix/app-c-data-curation.html#sec-apx-c-dedup",
    "title": "Appendix C — Data Curation",
    "section": "C.3 Deduplication",
    "text": "C.3 Deduplication\nDuplicate sequences inflate dataset size without providing new information and can cause train-test leakage.\n\nC.3.1 Exact Deduplication\nRemove identical sequences:\nimport hashlib\n\ndef deduplicate_exact(sequences):\n    \"\"\"Remove exact duplicate sequences.\"\"\"\n    seen = set()\n    unique = []\n    for seq in sequences:\n        seq_hash = hashlib.md5(seq.encode()).hexdigest()\n        if seq_hash not in seen:\n            seen.add(seq_hash)\n            unique.append(seq)\n    return unique\n\n\nC.3.2 Near-Duplicate Detection\nSequences differing by few positions may represent the same biological entity:\nFor DNA: - MinHash/LSH for approximate matching - CD-HIT for clustering at identity thresholds - MMseqs2 for scalable clustering\nFor proteins: - CD-HIT at 90%, 70%, 50%, 30% identity - MMseqs2 for large-scale clustering - PSI-BLAST for remote homology\nExample CD-HIT usage:\n# Cluster at 90% sequence identity\ncd-hit -i proteins.fasta -o proteins_nr90.fasta -c 0.9 -n 5\n\n# Cluster at 50% identity (requires different word size)\ncd-hit -i proteins.fasta -o proteins_nr50.fasta -c 0.5 -n 3\n\n\nC.3.3 Redundancy Levels\n\n\n\nRedundancy\nUse Case\n\n\n\n\n100% (no dedup)\nMaximum data, risk of memorization\n\n\n90% identity\nReduce near-duplicates, preserve variants\n\n\n70% identity\nBalance diversity and coverage\n\n\n50% identity\nMaximize diversity, may lose variants\n\n\n30% identity\nRemote homologs only\n\n\n\nThe optimal redundancy level depends on the task and model capacity. Larger models can benefit from more redundancy; smaller models may need aggressive deduplication.\n\n\nC.3.4 Train-Test Deduplication\nCritical for valid evaluation:\n\nDefine test set sequences first\nRemove training sequences similar to test sequences\nUse appropriate similarity threshold (often 30–50% for proteins)\nDocument the deduplication procedure\n\ndef remove_test_similar(train_seqs, test_seqs, threshold=0.5):\n    \"\"\"Remove training sequences similar to test set.\"\"\"\n    # Use MMseqs2 or similar for actual implementation\n    # This is pseudocode for the concept\n    clean_train = []\n    for train_seq in train_seqs:\n        if not any(similarity(train_seq, test_seq) &gt; threshold\n                   for test_seq in test_seqs):\n            clean_train.append(train_seq)\n    return clean_train",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html#sec-apx-c-contamination",
    "href": "appendix/app-c-data-curation.html#sec-apx-c-contamination",
    "title": "Appendix C — Data Curation",
    "section": "C.4 Contamination Detection",
    "text": "C.4 Contamination Detection\nContamination introduces sequences from unintended sources, corrupting training data.\n\nC.4.1 Types of Contamination\n\n\n\n\n\n\n\n\nType\nSource\nDetection\n\n\n\n\nCross-species\nSample mix-up, xenograft\nBLAST to species databases\n\n\nMicrobial\nSample contamination\nScreen against microbial genomes\n\n\nAdapter\nLibrary prep artifacts\nMatch adapter sequences\n\n\nVector\nCloning artifacts\nScreen UniVec database\n\n\nHuman\nNon-human samples\nScreen against human genome\n\n\n\n\n\nC.4.2 Screening Approaches\nBLAST-based screening:\n# Screen against human genome for non-human samples\nblastn -query sequences.fasta -db human_genome \\\n       -outfmt 6 -evalue 1e-10 &gt; human_hits.txt\nSpecialized tools:\n\n\n\nTool\nPurpose\n\n\n\n\nFastQ Screen\nMulti-genome contamination\n\n\nKraken2\nTaxonomic classification\n\n\nBBDuk\nAdapter/contaminant removal\n\n\nVecScreen\nVector contamination\n\n\n\n\n\nC.4.3 Benchmark Contamination\nA subtle but critical issue: test benchmarks contaminated in pretraining data inflate performance estimates.\nDetection: - Search pretraining corpus for benchmark sequences - Check for substring matches, not just exact matches - Verify temporal separation (benchmark created after pretraining data)\nPrevention: - Document pretraining data sources and dates - Use chromosome-based holdouts where possible - Report contamination checks in publications",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html#sec-apx-c-provenance",
    "href": "appendix/app-c-data-curation.html#sec-apx-c-provenance",
    "title": "Appendix C — Data Curation",
    "section": "C.5 Data Provenance",
    "text": "C.5 Data Provenance\nTracking data origins enables reproducibility and debugging.\n\nC.5.1 Metadata Requirements\nEssential metadata for each data source:\n\n\n\nField\nDescription\n\n\n\n\nSource\nDatabase/repository name\n\n\nVersion\nRelease version or date\n\n\nAccess date\nWhen data was downloaded\n\n\nURL\nExact download location\n\n\nProcessing\nFilters and transformations applied\n\n\nChecksum\nMD5/SHA256 for verification\n\n\n\n\n\nC.5.2 Documentation Template\n# data_manifest.yaml\ndataset:\n  name: \"variant_training_v2\"\n  created: \"2024-01-15\"\n\nsources:\n  - name: \"ClinVar\"\n    version: \"2024-01\"\n    url: \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/...\"\n    checksum: \"abc123...\"\n    filters:\n      - \"review_status &gt;= 2 stars\"\n      - \"no conflicting interpretations\"\n    records_raw: 2500000\n    records_filtered: 850000\n\n  - name: \"gnomAD\"\n    version: \"4.0\"\n    url: \"https://gnomad.broadinstitute.org/...\"\n    checksum: \"def456...\"\n    filters:\n      - \"FILTER == PASS\"\n      - \"AF &gt; 0.001\"\n    records_raw: 750000000\n    records_filtered: 45000000\n\nprocessing:\n  deduplication: \"exact + 90% identity clustering\"\n  train_test_split: \"chromosome-based (chr8 test)\"\n\nfinal_counts:\n  train: 800000\n  validation: 50000\n  test: 100000\n\n\nC.5.3 Version Control\nFor reproducibility: - Store data manifests in version control - Use content-addressable storage for large files (DVC, git-lfs) - Tag dataset versions with model training runs - Archive exact preprocessing scripts",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html#sec-apx-c-bias",
    "href": "appendix/app-c-data-curation.html#sec-apx-c-bias",
    "title": "Appendix C — Data Curation",
    "section": "C.6 Bias Assessment",
    "text": "C.6 Bias Assessment\nTraining data biases propagate into model predictions. Proactive assessment enables mitigation.\n\nC.6.1 Population Bias\nGenomic databases are not representative of global populations:\n\n\n\nDatabase\nEuropean\nAfrican\nAsian\nOther\n\n\n\n\nClinVar\n~70%\n~5%\n~15%\n~10%\n\n\ngnomAD\n~50%\n~10%\n~25%\n~15%\n\n\nUK Biobank\n~95%\n~2%\n~2%\n~1%\n\n\n\nConsequences: - Variant frequency estimates biased toward Europeans - Pathogenic variants in non-European populations underrepresented - Models may perform worse on underrepresented populations\nAssessment: - Compute ancestry distribution of training samples - Evaluate model performance stratified by ancestry - Document limitations for underrepresented groups\n\n\nC.6.2 Gene Coverage Bias\nSome genes are more studied than others:\n\nCancer genes (BRCA1, TP53) have extensive annotation\nNovel disease genes have sparse data\nGene function determines ascertainment\n\nAssessment: - Plot variants per gene vs. gene length - Identify genes with suspiciously high/low variant counts - Consider gene-level normalization\n\n\nC.6.3 Ascertainment Bias\nClinical databases reflect clinical practice:\n\nCommon diseases overrepresented\nSevere phenotypes more likely to reach clinical attention\nGeographic patterns in healthcare access\n\nAssessment: - Compare phenotype distribution to population prevalence - Identify systematic gaps in disease coverage - Document clinical ascertainment assumptions\n\n\nC.6.4 Label Bias\nAnnotations reflect annotator knowledge and conventions:\n\nHistorical classifications may be outdated\nDifferent submitters use different standards\nPathogenicity thresholds vary by context\n\nAssessment: - Track annotation dates and sources - Identify conflicting labels - Consider temporal validation (train on old, test on new)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html#sec-apx-c-building",
    "href": "appendix/app-c-data-curation.html#sec-apx-c-building",
    "title": "Appendix C — Data Curation",
    "section": "C.7 Building Training Sets",
    "text": "C.7 Building Training Sets\nPractical workflow for constructing training data.\n\nC.7.1 Step 1: Define Scope\n\nWhat sequences will the model process?\nWhat predictions will it make?\nWhat populations/contexts must it serve?\n\n\n\nC.7.2 Step 2: Identify Sources\n\nList candidate data sources\nAssess access requirements and licenses\nEvaluate quality and coverage\n\n\n\nC.7.3 Step 3: Download and Verify\n# Download with verification\nwget https://example.com/data.vcf.gz\nmd5sum data.vcf.gz  # Compare to published checksum\n\n# Document in manifest\necho \"Downloaded data.vcf.gz on $(date)\" &gt;&gt; data_log.txt\necho \"MD5: $(md5sum data.vcf.gz)\" &gt;&gt; data_log.txt\n\n\nC.7.4 Step 4: Quality Filter\nApply appropriate filters for each data type: - Sequence quality (N content, length, complexity) - Variant quality (QUAL, DP, GQ, FILTER) - Annotation quality (review status, conflicts)\n\n\nC.7.5 Step 5: Deduplicate\n\nRemove exact duplicates\nCluster at appropriate identity threshold\nEnsure train-test separation\n\n\n\nC.7.6 Step 6: Split Data\n\n\n\nSplit\nPurpose\nSize\n\n\n\n\nTrain\nModel training\n80–90%\n\n\nValidation\nHyperparameter tuning\n5–10%\n\n\nTest\nFinal evaluation\n5–10%\n\n\n\nSplitting strategies: - Random (simple but may leak related samples) - Chromosome-based (ensures spatial separation) - Temporal (train on older data, test on newer) - Gene-family-based (tests generalization to new genes)\n\n\nC.7.7 Step 7: Assess Bias\n\nCompute population/gene/phenotype distributions\nCompare to expected distributions\nDocument known biases and limitations\n\n\n\nC.7.8 Step 8: Document\n\nCreate comprehensive data manifest\nArchive preprocessing scripts\nRecord final counts and splits\nPublish data card with limitations",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html#sec-apx-c-datacards",
    "href": "appendix/app-c-data-curation.html#sec-apx-c-datacards",
    "title": "Appendix C — Data Curation",
    "section": "C.8 Data Cards",
    "text": "C.8 Data Cards\nA data card documents dataset characteristics for users:\n# Dataset: VariantBench-v2\n\n## Overview\n- Purpose: Training variant effect predictors\n- Size: 950,000 variants (800K train / 50K val / 100K test)\n- Created: January 2024\n\n## Sources\n- ClinVar 2024-01 (pathogenic/benign labels)\n- gnomAD 4.0 (population frequencies)\n\n## Curation\n- Required 2+ stars review status\n- Excluded conflicting interpretations\n- 90% identity clustering applied\n- Chromosome 8 held out for testing\n\n## Known Biases\n- 70% European ancestry\n- Cancer genes overrepresented (*BRCA1*: 15K variants)\n- Recent submissions may have unstable classifications\n\n## Intended Use\n- Training and evaluating pathogenicity predictors\n- NOT suitable for: clinical diagnosis without validation\n\n## Updates\n- v2.1 (March 2024): Added 50K variants from new ClinVar release",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-c-data-curation.html#sec-apx-c-checklist",
    "href": "appendix/app-c-data-curation.html#sec-apx-c-checklist",
    "title": "Appendix C — Data Curation",
    "section": "C.9 Checklist",
    "text": "C.9 Checklist\nBefore using a dataset for training:\nData Quality\n\nSources documented with versions and dates\nQuality filters applied and documented\nDeduplication performed at appropriate threshold\nTrain-test leakage prevented\n\nBias Assessment\n\nPopulation distribution analyzed\nGene coverage distribution analyzed\nAscertainment biases documented\nLabel quality assessed\n\nDocumentation\n\nData manifest created\nPreprocessing scripts archived\nData card written\nLimitations clearly stated\n\nReproducibility\n\nChecksums recorded\nRandom seeds documented\nScripts version-controlled\nEnvironment recorded",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data Curation</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html",
    "href": "appendix/app-d-models.html",
    "title": "Appendix D — Model Reference",
    "section": "",
    "text": "D.1 DNA Language Models",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-dna-lm",
    "href": "appendix/app-d-models.html#sec-apx-d-dna-lm",
    "title": "Appendix D — Model Reference",
    "section": "",
    "text": "Model\nParameters\nContext\nTokenization\nKey Capability\nCitation\n\n\n\n\nDNABERT\n110M\n512 bp\n6-mer\nChromatin accessibility, TF binding\nJi et al. (2021)\n\n\nDNABERT-2\n117M\n512 bp\nBPE\nImproved efficiency, multi-species\nZ. Zhou et al. (2024)\n\n\nNucleotide Transformer\n50M–2.5B\n6 kb\n6-mer\nEmbeddings, regulatory prediction\nDalla-Torre et al. (2023)\n\n\nHyenaDNA\n1.4M–6.6M\n1 Mb\nSingle nucleotide\nLong-range dependencies\nNguyen et al. (2023)\n\n\nCaduceus\n1.8M–7.4M\n131 kb\nSingle nucleotide\nBidirectional, reverse complement\nSchiff et al. (2024)\n\n\nGROVER\n80M–520M\n2 kb\nSingle nucleotide\nDNA + RNA understanding\nSanabria et al. (2024)\n\n\nEvo\n7B\n131 kb\nSingle nucleotide\nGeneration, whole-genome\nNguyen et al. (2024)\n\n\nEvo 2\n7B–40B\n1 Mb\nSingle nucleotide\nMulti-scale prediction\nBrixi et al. (2025)\n\n\n\n\nD.1.1 Model Access\n\n\n\nModel\nRepository\nWeights\nLicense\n\n\n\n\nDNABERT\ngithub.com/jerryji1993/DNABERT\nHuggingFace\nMIT\n\n\nDNABERT-2\ngithub.com/MAGICS-LAB/DNABERT_2\nHuggingFace\nMIT\n\n\nNucleotide Transformer\ngithub.com/instadeepai/nucleotide-transformer\nHuggingFace\nCC BY-NC-SA 4.0\n\n\nHyenaDNA\ngithub.com/HazyResearch/hyena-dna\nHuggingFace\nApache 2.0\n\n\nCaduceus\ngithub.com/kuleshov-group/caduceus\nHuggingFace\nApache 2.0\n\n\nEvo\ngithub.com/evo-design/evo\nHuggingFace\nApache 2.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-plm",
    "href": "appendix/app-d-models.html#sec-apx-d-plm",
    "title": "Appendix D — Model Reference",
    "section": "D.2 Protein Language Models",
    "text": "D.2 Protein Language Models\n\n\n\n\n\n\n\n\n\n\n\nModel\nParameters\nContext\nArchitecture\nKey Capability\nCitation\n\n\n\n\nESM-2\n8M–15B\n1,024 AA\nTransformer encoder\nStructure, function, variants\n(lin_language_2023?)\n\n\nESM-1v\n650M\n1,024 AA\nTransformer encoder\nZero-shot variant effects\nMeier et al. (2021)\n\n\nESMFold\n15B\n1,024 AA\nEncoder + structure\nSingle-sequence folding\n(lin_language_2023?)\n\n\nProtTrans\n420M–3B\n1,024 AA\nTransformer\nMultilingual protein embeddings\n(elnaggar_prottrans_2022?)\n\n\nProGen2\n151M–6.4B\n1,024 AA\nAutoregressive\nProtein generation\n(nijkamp_progen2_2023?)\n\n\n\n\nD.2.1 Model Access\n\n\n\nModel\nRepository\nWeights\nLicense\n\n\n\n\nESM-2\ngithub.com/facebookresearch/esm\nHuggingFace\nMIT\n\n\nESMFold\ngithub.com/facebookresearch/esm\nHuggingFace\nMIT\n\n\nProtTrans\ngithub.com/agemagician/ProtTrans\nHuggingFace\nAcademic",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-seq2func",
    "href": "appendix/app-d-models.html#sec-apx-d-seq2func",
    "title": "Appendix D — Model Reference",
    "section": "D.3 Sequence-to-Function Models",
    "text": "D.3 Sequence-to-Function Models\n\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nOutput\nArchitecture\nKey Capability\nCitation\n\n\n\n\nDeepSEA\n1 kb\n919 chromatin features\nCNN\nRegulatory variant effects\n(zhou_deep_2015?)\n\n\nBeluga\n2 kb\n2,002 features\nCNN\nExtended DeepSEA\nJ. Zhou et al. (2018)\n\n\nSei\n4 kb\n21,907 targets\nCNN\nSequence classes\n(chen_sequence-based_2022?)\n\n\nBasenji\n131 kb\n4,229 tracks\nDilated CNN\nExpression prediction\nKelley et al. (2018)\n\n\nBasenji2\n131 kb\n5,313 tracks\nDilated CNN\nCross-species, human + mouse\nKelley (2020)\n\n\nEnformer\n196 kb\n5,313 tracks\nTransformer\nLong-range regulation\nAvsec et al. (2021)\n\n\nBorzoi\n524 kb\nRNA-seq\nTransformer\nRNA expression\nLinder et al. (2025)\n\n\n\n\nD.3.1 Model Access\n\n\n\nModel\nRepository\nWeights\nLicense\n\n\n\n\nDeepSEA/Beluga\nkipoi.org\nKipoi\nAcademic\n\n\nSei\ngithub.com/FunctionLab/sei-framework\nZenodo\nMIT\n\n\nBasenji/Basenji2\ngithub.com/calico/basenji\nDirect\nApache 2.0\n\n\nEnformer\ngithub.com/deepmind/deepmind-research/tree/master/enformer\nTF Hub\nApache 2.0\n\n\nBorzoi\ngithub.com/calico/borzoi\nDirect\nApache 2.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-splice",
    "href": "appendix/app-d-models.html#sec-apx-d-splice",
    "title": "Appendix D — Model Reference",
    "section": "D.4 Splice Prediction Models",
    "text": "D.4 Splice Prediction Models\n\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nOutput\nArchitecture\nKey Capability\nCitation\n\n\n\n\nSpliceAI\n10 kb context\nSplice probability\nResNet\nCryptic splice sites\nJaganathan et al. (2019)\n\n\nMaxEntScan\n9+23 nt\nSplice score\nPosition weight matrix\nConsensus scoring\nYeo and Burge (2004)\n\n\nPangolin\n5 kb\nTissue-specific splicing\nTransformer\nTissue context\n(zeng_predicting_2022?)\n\n\n\n\nD.4.1 Model Access\n\n\n\nModel\nRepository\nWeb Interface\nLicense\n\n\n\n\nSpliceAI\ngithub.com/Illumina/SpliceAI\nspliceailookup.broadinstitute.org\nGPLv3\n\n\nPangolin\ngithub.com/tkzeng/Pangolin\n—\nMIT",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-vep",
    "href": "appendix/app-d-models.html#sec-apx-d-vep",
    "title": "Appendix D — Model Reference",
    "section": "D.5 Variant Effect Predictors",
    "text": "D.5 Variant Effect Predictors\n\nD.5.1 Integrative Scores\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nMethod\nKey Features\nCitation\n\n\n\n\nCADD\nAny variant\nEnsemble ML\n100+ annotations, universal\nRentzsch et al. (2019)\n\n\nREVEL\nMissense\nEnsemble\n13 tool integration\nIoannidis et al. (2016)\n\n\nPrimateAI-3D\nMissense\nDeep learning + structure\nPrimate conservation\n(sundaram_primateai_2018?)\n\n\n\n\n\nD.5.2 Protein Language Model–Based\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nMethod\nKey Features\nCitation\n\n\n\n\nAlphaMissense\nMissense\nESM + AlphaFold\nStructure-aware PLM\n(cheng_accurate_2023?)\n\n\nESM-1v\nMissense\nZero-shot PLM\nNo training required\nMeier et al. (2021)\n\n\nEVE\nMissense\nVAE on MSA\nEvolutionary model\n(frazer_disease_2021?)\n\n\nGPN-MSA\nAny variant\nAlignment LM\nConservation + context\nBenegas et al. (2024)\n\n\n\n\n\nD.5.3 Conservation-Based\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nMethod\nKey Features\nCitation\n\n\n\n\nSIFT\nMissense\nSequence conservation\nFast, interpretable\nNg and Henikoff (2003)\n\n\nPolyPhen-2\nMissense\nConservation + structure\nHumDiv/HumVar models\nAdzhubei et al. (2010)\n\n\nGERP++\nAny position\nRejected substitutions\nBase-level conservation\nDavydov et al. (2010)\n\n\nphyloP\nAny position\nPhylogenetic model\nAcceleration/conservation\n(pollard_detection_2010?)\n\n\n\n\n\nD.5.4 Model Access\n\n\n\nModel\nAccess\nWeb Interface\n\n\n\n\nCADD\ncadd.gs.washington.edu\nScore lookup + download\n\n\nAlphaMissense\ngithub.com/google-deepmind/alphamissense\nPrecomputed scores\n\n\nREVEL\nsites.google.com/site/revelgenomics\nPrecomputed scores\n\n\ngnomAD\ngnomad.broadinstitute.org\nIntegrated VEP scores",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-structure",
    "href": "appendix/app-d-models.html#sec-apx-d-structure",
    "title": "Appendix D — Model Reference",
    "section": "D.6 Structure Prediction",
    "text": "D.6 Structure Prediction\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nOutput\nKey Capability\nCitation\n\n\n\n\nAlphaFold2\nProtein sequence + MSA\n3D structure\nHigh-accuracy folding\n(jumper_highly_2021?)\n\n\nAlphaFold3\nProtein/DNA/RNA/ligand\nComplex structure\nMulti-molecule complexes\n(abramson_accurate_2024?)\n\n\nESMFold\nProtein sequence\n3D structure\nSingle-sequence, fast\n(lin_language_2023?)\n\n\nRoseTTAFold\nProtein sequence + MSA\n3D structure\nThree-track architecture\n(baek_accurate_2021?)\n\n\n\n\nD.6.1 Model Access\n\n\n\nModel\nRepository\nServer\nLicense\n\n\n\n\nAlphaFold2\ngithub.com/google-deepmind/alphafold\nalphafold.ebi.ac.uk\nApache 2.0\n\n\nAlphaFold3\ngithub.com/google-deepmind/alphafold3\nalphafoldserver.com\nResearch only\n\n\nESMFold\ngithub.com/facebookresearch/esm\nesmatlas.com\nMIT",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-singlecell",
    "href": "appendix/app-d-models.html#sec-apx-d-singlecell",
    "title": "Appendix D — Model Reference",
    "section": "D.7 Single-Cell and Multi-Omics Models",
    "text": "D.7 Single-Cell and Multi-Omics Models\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nOutput\nKey Capability\nCitation\n\n\n\n\nscGPT\nscRNA-seq\nCell embeddings\nCell type, perturbation\nCui et al. (2024)\n\n\nGeneformer\nscRNA-seq\nGene embeddings\nTransfer learning\nTheodoris et al. (2023)\n\n\nscBERT\nscRNA-seq\nCell embeddings\nCell annotation\n(yang_scbert_2022?)\n\n\nGLUE\nMulti-omics\nIntegrated embeddings\nCross-modality integration\n(cao_multi-omics_2022?)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-clinical",
    "href": "appendix/app-d-models.html#sec-apx-d-clinical",
    "title": "Appendix D — Model Reference",
    "section": "D.8 Polygenic and Clinical Models",
    "text": "D.8 Polygenic and Clinical Models\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nOutput\nKey Capability\nCitation\n\n\n\n\nDelphi\nGenotypes\nDisease risk\nDeep PGS\nGeorgantas, Kutalik, and Richiardi (2024)\n\n\nDeepRVAT\nRare variants\nGene burden\nRare variant aggregation\nClarke et al. (2024)\n\n\nG2PT\nGenotypes + phenotypes\nRisk prediction\nGenotype-to-phenotype\nLee et al. (2025)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-categories",
    "href": "appendix/app-d-models.html#sec-apx-d-categories",
    "title": "Appendix D — Model Reference",
    "section": "D.9 Category Definitions",
    "text": "D.9 Category Definitions\n\nDNA LM\n\nDNA language models using self-supervised pretraining (masked language modeling or autoregressive) on genomic sequences. Produce embeddings useful for diverse downstream tasks.\n\nPLM\n\nProtein language models trained on protein sequences using similar self-supervised objectives. Capture evolutionary and structural information.\n\nSeq→Func\n\nSupervised sequence-to-function models predicting molecular phenotypes (chromatin accessibility, histone modifications, gene expression) directly from DNA sequence.\n\nSplice\n\nSpecialized models for splice site recognition and splicing outcome prediction.\n\nVEP\n\nVariant effect predictors spanning multiple paradigms: conservation-based, integrative ensemble, and foundation model–based approaches.\n\nStructure\n\nProtein (and nucleic acid) structure prediction models.\n\nGFM\n\nGenomic foundation model—a broad term for models with reusable representations applicable across multiple downstream tasks.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-d-models.html#sec-apx-d-practical",
    "href": "appendix/app-d-models.html#sec-apx-d-practical",
    "title": "Appendix D — Model Reference",
    "section": "D.10 Practical Considerations",
    "text": "D.10 Practical Considerations\n\nD.10.1 Selecting a Model\nWhen choosing a model for a specific application:\n\nTask alignment: Does the model’s pretraining objective match your task? MLM-pretrained models excel at classification; autoregressive models enable generation.\nContext requirements: Long-range regulatory effects require models with large context windows (Enformer, HyenaDNA, Evo). Local motif tasks work with shorter contexts.\nComputational resources: Parameter counts range from millions to billions. Smaller models (DNABERT, 110M) run on consumer GPUs; larger models (Evo 2, 40B) require substantial infrastructure.\nLicense restrictions: Some models restrict commercial use (CC BY-NC) or require academic affiliation. Verify license compatibility before deployment.\nBenchmark performance: Consult Chapter 11 for standardized comparisons on tasks relevant to your application.\n\n\n\nD.10.2 Model Versioning\nFoundation models are actively developed, with new versions often substantially outperforming predecessors. When citing or deploying models:\n\nSpecify exact version and checkpoint (e.g., “ESM-2 650M, checkpoint esm2_t33_650M_UR50D”)\nRecord model weights hash for reproducibility\nNote training data version (UniRef versions change over time)\nDocument inference parameters (temperature, sampling strategy for generative models)\n\n\n\n\n\nAdzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky, Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev. 2010. “A Method and Server for Predicting Damaging Missense Mutations.” Nature Methods 7 (4): 248–49. https://doi.org/10.1038/nmeth0410-248.\n\n\nAvsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. “[Enformer] Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.” Nature Methods 18 (October): 1196–1203. https://doi.org/10.1038/s41592-021-01252-x.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2024. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” bioRxiv, April, 2023.10.10.561776. https://doi.org/10.1101/2023.10.10.561776.\n\n\nBrixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. “[Evo 2] Genome Modeling and Design Across All Domains of Life with Evo 2.” bioRxiv. https://doi.org/10.1101/2025.02.18.638918.\n\n\nClarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024. “[DeepRVAT] Integration of Variant Annotations Using Deep Set Networks Boosts Rare Variant Association Testing.” Nature Genetics 56 (10): 2271–80. https://doi.org/10.1038/s41588-024-01919-z.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. 2024. “scGPT: Toward Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI.” Nature Methods 21 (8): 1470–80. https://doi.org/10.1038/s41592-024-02201-0.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDavydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper, Arend Sidow, and Serafim Batzoglou. 2010. “Identifying a High Fraction of the Human Genome to Be Under Selective Constraint Using GERP++.” PLOS Computational Biology 6 (12): e1001025. https://doi.org/10.1371/journal.pcbi.1001025.\n\n\nGeorgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. “Delphi: A Deep-Learning Method for Polygenic Risk Prediction.” medRxiv. https://doi.org/10.1101/2024.04.19.24306079.\n\n\nIoannidis, Nilah M., Joseph H. Rothstein, Vikas Pejaver, Sumit Middha, Shannon K. McDonnell, Saurabh Baheti, Anthony Musolf, et al. 2016. “REVEL: An Ensemble Method for Predicting the Pathogenicity of Rare Missense Variants.” The American Journal of Human Genetics 99 (4): 877–85. https://doi.org/10.1016/j.ajhg.2016.08.016.\n\n\nJaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. “[SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.” Cell 176 (3): 535–548.e24. https://doi.org/10.1016/j.cell.2018.12.015.\n\n\nJi, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-Language in Genome.” Bioinformatics 37 (15): 2112–20. https://doi.org/10.1093/bioinformatics/btab083.\n\n\nKelley, David R. 2020. “[Basenji2] Cross-Species Regulatory Sequence Activity Prediction.” PLOS Computational Biology 16 (7): e1008050. https://doi.org/10.1371/journal.pcbi.1008050.\n\n\nKelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y. McLean, and Jasper Snoek. 2018. “[Basenji2] Sequential Regulatory Activity Prediction Across Chromosomes with Convolutional Neural Networks.” Genome Research 28 (5): 739–50. https://doi.org/10.1101/gr.227819.117.\n\n\nLee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam, Amit R. Majithia, and Trey Ideker. 2025. “[G2PT] A Genotype-Phenotype Transformer to Assess and Explain Polygenic Risk.” bioRxiv. https://doi.org/10.1101/2024.10.23.619940.\n\n\nLinder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and David R. Kelley. 2025. “[Borzoi] Predicting RNA-Seq Coverage from DNA Sequence as a Unifying Model of Gene Regulation.” Nature Genetics 57 (4): 949–61. https://doi.org/10.1038/s41588-024-02053-6.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “[ESM-1v] Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” bioRxiv. https://doi.org/10.1101/2021.07.09.450648.\n\n\nNg, Pauline C., and Steven Henikoff. 2003. “SIFT: Predicting Amino Acid Changes That Affect Protein Function.” Nucleic Acids Research 31 (13): 3812–14. https://doi.org/10.1093/nar/gkg509.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence Modeling and Design from Molecular to Genome Scale with Evo.” Science 386 (6723): eado9336. https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” arXiv. https://doi.org/10.48550/arXiv.2306.15794.\n\n\nRentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and Martin Kircher. 2019. “CADD: Predicting the Deleteriousness of Variants Throughout the Human Genome.” Nucleic Acids Research 47 (D1): D886–94. https://doi.org/10.1093/nar/gky1016.\n\n\nSanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. “[GROVER] DNA Language Model GROVER Learns Sequence Context in the Human Genome.” Nature Machine Intelligence 6 (8): 911–23. https://doi.org/10.1038/s42256-024-00872-0.\n\n\nSchiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. “Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling.” arXiv. https://doi.org/10.48550/arXiv.2403.03234.\n\n\nTheodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023. “[Geneformer] Transfer Learning Enables Predictions in Network Biology.” Nature 618 (7965): 616–24. https://doi.org/10.1038/s41586-023-06139-9.\n\n\nYeo, Gene, and Christopher B. Burge. 2004. “Maximum Entropy Modeling of Short Sequence Motifs with Applications to RNA Splicing Signals.” Journal of Computational Biology 11 (2-3): 377–94. https://doi.org/10.1089/1066527041410418.\n\n\nZhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. “[Expecto] Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk.” Nature Genetics 50 (8): 1171–79. https://doi.org/10.1038/s41588-018-0160-6.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. “DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.” arXiv. https://doi.org/10.48550/arXiv.2306.15006.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Model Reference</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html",
    "href": "appendix/app-e-resources.html",
    "title": "Appendix E — Resources",
    "section": "",
    "text": "E.1 Textbooks",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#sec-apx-e-textbooks",
    "href": "appendix/app-e-resources.html#sec-apx-e-textbooks",
    "title": "Appendix E — Resources",
    "section": "",
    "text": "E.1.1 Genomics and Human Genetics\n\nThompson & Thompson Genetics and Genomics in Medicine (9th ed.)\n\nRonald Cohn, Stephen Scherer, Ada Hamosh. Clinical-focused overview of human genetics and genomics for medicine. Excellent grounding in clinical genomics, variant interpretation, and genetic disease mechanisms.\n\nHuman Molecular Genetics (5th ed.)\n\nTom Strachan, Andrew Read. Higher-level molecular genetics text with strong coverage of mechanisms, technologies, and disease applications. More technical depth than Thompson & Thompson.\n\nMolecular Biology of the Cell (7th ed.)\n\nBruce Alberts et al. Comprehensive cell biology text covering the molecular machinery underlying genomic processes. Essential background for understanding what genomic models are predicting.\n\nGenomes 4\n\nT.A. Brown. Focused specifically on genome organization, evolution, and analysis. Strong coverage of comparative genomics relevant to conservation-based methods.\n\n\n\n\nE.1.2 Immunology\n\nJaneway’s Immunobiology (10th ed.)\n\nKenneth M. Murphy, Casey Weaver, Leslie J. Berg. Standard comprehensive immunology textbook. Relevant for understanding immune-related genomic variation and applications like HLA typing.\n\n\n\n\nE.1.3 Machine Learning and Deep Learning\n\nDeep Learning\n\nIan Goodfellow, Yoshua Bengio, Aaron Courville. The comprehensive deep learning reference. Free online: https://www.deeplearningbook.org/\n\nDive into Deep Learning (D2L)\n\nAston Zhang et al. Interactive deep learning book with executable Jupyter notebooks and multi-framework code (PyTorch, TensorFlow, JAX). Free online: https://d2l.ai/\n\nAn Introduction to Statistical Learning (ISLR, 2nd ed.)\n\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Gentle introduction to statistical learning methods. R and Python editions available free online: https://www.statlearning.com/\n\nThe Elements of Statistical Learning (ESL)\n\nTrevor Hastie, Robert Tibshirani, Jerome Friedman. More advanced, theory-heavy companion to ISLR. Free PDF: https://hastie.su.domains/ElemStatLearn/\n\nPattern Recognition and Machine Learning\n\nChristopher Bishop. Classic ML text with strong probabilistic foundations. Relevant for understanding uncertainty quantification approaches.\n\n\n\n\nE.1.4 Bioinformatics and Computational Biology\n\nBioinformatics: Sequence and Genome Analysis (2nd ed.)\n\nDavid Mount. Foundational algorithms for sequence analysis including alignment, HMMs, and phylogenetics.\n\nBiological Sequence Analysis\n\nRichard Durbin, Sean Eddy, Anders Krogh, Graeme Mitchison. Essential reading for probabilistic approaches to biological sequences. HMM chapter particularly relevant.\n\nComputational Genomics with R\n\nAltuna Akalin. Practical computational genomics using R/Bioconductor. Free online: https://compgenomr.github.io/book/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#sec-apx-e-courses",
    "href": "appendix/app-e-resources.html#sec-apx-e-courses",
    "title": "Appendix E — Resources",
    "section": "E.2 Online Courses",
    "text": "E.2 Online Courses\n\nE.2.1 Machine Learning and Deep Learning\n\nStanford CS229: Machine Learning\n\nAndrew Ng’s foundational ML course. Lecture videos and materials freely available. https://cs229.stanford.edu/\n\nStanford CS231n: CNNs for Visual Recognition\n\nDeep dive into convolutional networks with strong foundations applicable to sequence models. http://cs231n.stanford.edu/\n\nStanford CS224n: NLP with Deep Learning\n\nEssential for understanding transformer architectures, attention mechanisms, and language model pretraining. http://web.stanford.edu/class/cs224n/\n\nfast.ai Practical Deep Learning\n\nTop-down practical approach to deep learning. Free course with notebooks: https://course.fast.ai/\n\nDeepMind x UCL Deep Learning Lecture Series\n\nExcellent coverage of modern deep learning topics including transformers and self-supervised learning. YouTube playlist freely available.\n\n\n\n\nE.2.2 Genomics and Bioinformatics\n\nMIT 7.91J: Foundations of Computational and Systems Biology\n\nComprehensive computational biology course covering sequence analysis, structure, networks. MIT OpenCourseWare: https://ocw.mit.edu/courses/7-91j-foundations-of-computational-and-systems-biology-spring-2014/\n\nCoursera: Genomic Data Science Specialization\n\nJohns Hopkins series covering genomic technologies, Python/R for genomics, and statistical analysis. https://www.coursera.org/specializations/genomic-data-science\n\nEMBL-EBI Training\n\nFree online courses on genomics databases, tools, and analysis methods. https://www.ebi.ac.uk/training/\n\nRosalind\n\nProblem-based bioinformatics learning platform. Excellent for building algorithmic intuition. https://rosalind.info/\n\n\n\n\nE.2.3 Applied Genomic ML\n\nCoursera: AI for Medicine Specialization\n\nDeepLearning.AI course covering ML applications in medical imaging and clinical data. https://www.coursera.org/specializations/ai-for-medicine\n\nML4Bio Summer School\n\nAnnual workshop on machine learning for biology. Materials often available online.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#sec-apx-e-databases",
    "href": "appendix/app-e-resources.html#sec-apx-e-databases",
    "title": "Appendix E — Resources",
    "section": "E.3 Genomic Databases",
    "text": "E.3 Genomic Databases\n\nE.3.1 Variant and Population Databases\n\n\n\nDatabase\nDescription\nURL\n\n\n\n\nClinVar\nClinical variant interpretations\nhttps://www.ncbi.nlm.nih.gov/clinvar/\n\n\ngnomAD\nPopulation allele frequencies (730K+ exomes/genomes)\nhttps://gnomad.broadinstitute.org/\n\n\ndbSNP\nCatalog of genetic variation\nhttps://www.ncbi.nlm.nih.gov/snp/\n\n\nClinGen\nClinical genome resource, gene-disease validity\nhttps://clinicalgenome.org/\n\n\nOMIM\nMendelian inheritance in man\nhttps://omim.org/\n\n\nHGMD\nHuman gene mutation database (subscription)\nhttp://www.hgmd.cf.ac.uk/\n\n\nLOVD\nLocus-specific variant databases\nhttps://www.lovd.nl/\n\n\n\n\n\nE.3.2 Functional Annotation Databases\n\n\n\nDatabase\nDescription\nURL\n\n\n\n\nENCODE\nEncyclopedia of DNA elements\nhttps://www.encodeproject.org/\n\n\nGTEx\nTissue-specific gene expression\nhttps://gtexportal.org/\n\n\nRoadmap Epigenomics\nEpigenome maps across cell types\nhttps://egg2.wustl.edu/roadmap/web_portal/\n\n\nFANTOM5\nFunctional annotation of mammalian genomes\nhttps://fantom.gsc.riken.jp/5/\n\n\n4D Nucleome\n3D genome organization\nhttps://www.4dnucleome.org/\n\n\n\n\n\nE.3.3 Protein Databases\n\n\n\nDatabase\nDescription\nURL\n\n\n\n\nUniProt\nProtein sequences and annotations\nhttps://www.uniprot.org/\n\n\nAlphaFold DB\nPredicted protein structures\nhttps://alphafold.ebi.ac.uk/\n\n\nPDB\nExperimental protein structures\nhttps://www.rcsb.org/\n\n\nInterPro\nProtein families and domains\nhttps://www.ebi.ac.uk/interpro/\n\n\nPfam\nProtein family database\nhttps://www.ebi.ac.uk/interpro/entry/pfam/\n\n\n\n\n\nE.3.4 Gene and Pathway Databases\n\n\n\nDatabase\nDescription\nURL\n\n\n\n\nEnsembl\nGenome browser and annotation\nhttps://www.ensembl.org/\n\n\nUCSC Genome Browser\nGenome visualization and tracks\nhttps://genome.ucsc.edu/\n\n\nKEGG\nPathway and molecular interaction maps\nhttps://www.kegg.jp/\n\n\nReactome\nCurated pathway database\nhttps://reactome.org/\n\n\nGene Ontology\nFunctional annotation ontology\nhttp://geneontology.org/\n\n\nSTRING\nProtein-protein interactions\nhttps://string-db.org/\n\n\n\n\n\nE.3.5 Single-Cell Databases\n\n\n\nDatabase\nDescription\nURL\n\n\n\n\nHuman Cell Atlas\nReference maps of human cells\nhttps://www.humancellatlas.org/\n\n\nCellxGene\nSingle-cell data exploration\nhttps://cellxgene.cziscience.com/\n\n\nSingle Cell Portal\nBroad Institute scRNA-seq repository\nhttps://singlecell.broadinstitute.org/\n\n\nTabula Sapiens\nMulti-organ human cell atlas\nhttps://tabula-sapiens-portal.ds.czbiohub.org/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#sec-apx-e-software",
    "href": "appendix/app-e-resources.html#sec-apx-e-software",
    "title": "Appendix E — Resources",
    "section": "E.4 Software Tools",
    "text": "E.4 Software Tools\n\nE.4.1 Sequence Analysis\n\n\n\nTool\nDescription\nURL\n\n\n\n\nBWA\nBurrows-Wheeler aligner for short reads\nhttps://github.com/lh3/bwa\n\n\nMinimap2\nLong-read alignment\nhttps://github.com/lh3/minimap2\n\n\nSTAR\nRNA-seq aligner\nhttps://github.com/alexdobin/STAR\n\n\nSAMtools\nSAM/BAM manipulation\nhttp://www.htslib.org/\n\n\nBCFtools\nVariant calling and manipulation\nhttp://www.htslib.org/\n\n\nGATK\nGenome analysis toolkit\nhttps://gatk.broadinstitute.org/\n\n\nDeepVariant\nDeep learning variant caller\nhttps://github.com/google/deepvariant\n\n\n\n\n\nE.4.2 Variant Annotation\n\n\n\nTool\nDescription\nURL\n\n\n\n\nVEP\nEnsembl variant effect predictor\nhttps://www.ensembl.org/vep\n\n\nSnpEff\nVariant annotation and effect prediction\nhttps://pcingola.github.io/SnpEff/\n\n\nANNOVAR\nFunctional annotation\nhttps://annovar.openbioinformatics.org/\n\n\nInterVar\nACMG/AMP interpretation\nhttps://wintervar.wglab.org/\n\n\n\n\n\nE.4.3 Deep Learning Frameworks\n\n\n\nFramework\nDescription\nURL\n\n\n\n\nPyTorch\nPrimary framework for genomic DL\nhttps://pytorch.org/\n\n\nHuggingFace Transformers\nPretrained model hub and tools\nhttps://huggingface.co/\n\n\nJax/Flax\nHigh-performance ML (used by DeepMind)\nhttps://github.com/google/jax\n\n\nPyTorch Lightning\nTraining boilerplate reduction\nhttps://lightning.ai/\n\n\n\n\n\nE.4.4 Genomic ML Libraries\n\n\n\nLibrary\nDescription\nURL\n\n\n\n\nKipoi\nModel zoo for genomics\nhttps://kipoi.org/\n\n\nSelene\nDeep learning for sequences\nhttps://github.com/FunctionLab/selene\n\n\nEnformer (TensorFlow)\nOfficial Enformer implementation\nhttps://github.com/deepmind/deepmind-research/tree/master/enformer\n\n\nPysam\nPython interface for SAM/BAM\nhttps://github.com/pysam-developers/pysam\n\n\nBiopython\nBiological computation in Python\nhttps://biopython.org/\n\n\nScanpy\nSingle-cell analysis\nhttps://scanpy.readthedocs.io/\n\n\n\n\n\nE.4.5 Workflow Management\n\n\n\nTool\nDescription\nURL\n\n\n\n\nSnakemake\nPython-based workflow manager\nhttps://snakemake.readthedocs.io/\n\n\nNextflow\nData-driven pipelines\nhttps://www.nextflow.io/\n\n\nWDL/Cromwell\nWorkflow description language\nhttps://cromwell.readthedocs.io/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#sec-apx-e-benchmarks",
    "href": "appendix/app-e-resources.html#sec-apx-e-benchmarks",
    "title": "Appendix E — Resources",
    "section": "E.5 Benchmarks and Datasets",
    "text": "E.5 Benchmarks and Datasets\n\nE.5.1 Genomic Benchmarks\n\n\n\nBenchmark\nDomain\nURL\n\n\n\n\nNucleotide Transformer Benchmarks\nDNA LM evaluation\nhttps://github.com/instadeepai/nucleotide-transformer\n\n\nTAPE\nProtein tasks\nhttps://github.com/songlab-cal/tape\n\n\nProteinGym\nProtein fitness prediction\nhttps://proteingym.org/\n\n\nGenomeBenchmarks\nDNA classification tasks\nhttps://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks\n\n\n\n\n\nE.5.2 Variant Datasets\n\n\n\nDataset\nDescription\nURL\n\n\n\n\nClinVar\nClinical variant annotations\nhttps://www.ncbi.nlm.nih.gov/clinvar/\n\n\nDMS datasets\nDeep mutational scanning\nVarious; see ProteinGym\n\n\nCADD training data\nSimulated and observed variants\nhttps://cadd.gs.washington.edu/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#sec-apx-e-community",
    "href": "appendix/app-e-resources.html#sec-apx-e-community",
    "title": "Appendix E — Resources",
    "section": "E.6 Community and Forums",
    "text": "E.6 Community and Forums\n\nE.6.1 Discussion Forums\n\nBiostars: Bioinformatics Q&A (https://www.biostars.org/)\nSEQanswers: NGS discussion (http://seqanswers.com/)\nCross Validated: Statistics/ML Q&A (https://stats.stackexchange.com/)\nReddit r/bioinformatics: Community discussion (https://www.reddit.com/r/bioinformatics/)\n\n\n\nE.6.2 Preprint Servers\n\nbioRxiv: Life sciences preprints (https://www.biorxiv.org/)\nmedRxiv: Health sciences preprints (https://www.medrxiv.org/)\narXiv q-bio: Quantitative biology (https://arxiv.org/archive/q-bio)\narXiv cs.LG: Machine learning (https://arxiv.org/list/cs.LG/recent)\n\n\n\nE.6.3 Conferences\n\n\n\nConference\nFocus\nTypical Timing\n\n\n\n\nISMB\nComputational biology\nJuly\n\n\nRECOMB\nComputational molecular biology\nApril-May\n\n\nASHG\nHuman genetics\nOctober-November\n\n\nNeurIPS\nMachine learning\nDecember\n\n\nICML\nMachine learning\nJuly\n\n\nMLCB\nML in computational biology\nDecember (NeurIPS workshop)\n\n\n\n\n\nE.6.4 Key Research Groups\nSelected groups active at the intersection of genomics and machine learning:\n\nKundaje Lab (Stanford): Regulatory genomics, interpretability\nKelley Lab (Calico): Sequence-to-function models\nMarks Lab (Harvard): Evolutionary models, protein fitness\nTroyanskaya Lab (Princeton): Functional genomics, Sei\nRegev Lab (Genentech/Broad): Single-cell genomics\nESM Team (Meta AI): Protein language models\nDeepMind: AlphaFold, Enformer, AlphaMissense",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-e-resources.html#sec-apx-e-current",
    "href": "appendix/app-e-resources.html#sec-apx-e-current",
    "title": "Appendix E — Resources",
    "section": "E.7 Keeping Current",
    "text": "E.7 Keeping Current\nThe field moves rapidly. Strategies for staying current:\n\nPreprint alerts: Set bioRxiv/arXiv alerts for keywords like “genomic foundation model,” “variant effect prediction,” “DNA language model”\nTwitter/X: Follow active researchers and labs; the ML4Bio community is particularly active\nConference proceedings: ISMB, RECOMB, and NeurIPS MLCB workshops publish cutting-edge work\nModel hubs: Monitor HuggingFace for new genomic model releases\nDatabase updates: ClinVar and gnomAD release notes track data growth and methodology changes\nReview articles: Annual reviews in Nature Reviews Genetics, Genome Biology, and Nature Methods provide consolidated perspectives",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "appendix/app-f-glossary.html",
    "href": "appendix/app-f-glossary.html",
    "title": "Appendix F — Glossary",
    "section": "",
    "text": "F.1 Terms (A–Z)\nA/B compartments [Genomics]: Broad chromatin domains seen in Hi-C contact maps that separate active (A) from inactive (B) genome regions. They summarize large-scale 3D organization and often correlate with gene density and epigenomic state.\nACMG/AMP guidelines (American College of Medical Genetics and Genomics / Association for Molecular Pathology) [Clinical]: A widely used framework for interpreting genetic variants using standardized evidence categories and rules. In this book, model scores are often mapped to ACMG-style categories such as benign, VUS, or pathogenic to support clinical reporting.\nActionability (clinical) [Clinical]: The degree to which a result can change patient management, such as guiding surveillance, treatment, or family testing. Actionability is a key lens for deciding which model outputs to report.\nAdapter [ML]: A small set of additional layers inserted into a pre-trained model so it can be specialized to a new task without updating all parameters. Useful for adapting large genomic foundation models while keeping compute and storage manageable. See also: fine-tuning, LoRA.\nADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) [Clinical]: A set of properties describing how a drug behaves in the body and whether it is safe. Early ADMET prediction helps reduce late-stage failures in drug discovery.\nAdverse drug reaction (ADR) [Clinical]: An unintended and harmful response to a medication at normal doses. Predicting ADR risk can involve genetics, comedications, and clinical context.\nAnalytical validity [Clinical]: How accurately and reliably a test measures what it claims to measure (e.g., variant detection accuracy). Required before clinical validity and utility can be meaningfully assessed.\nATAC-seq (Assay for Transposase-Accessible Chromatin using sequencing) [Genomics]: A sequencing assay that measures chromatin accessibility by inserting adapters into open DNA with a transposase. Used to map regulatory elements and infer transcription factor activity.\nAUPRC (Area under precision–recall curve) [Statistics]: A metric emphasizing performance on the positive class, especially informative when positives are rare. Common in variant and regulatory benchmarks with strong class imbalance.\nAUROC (Area under ROC curve) [Statistics]: A threshold-free metric for binary classification measuring the probability a random positive is ranked above a random negative. Widely used in benchmarks but can be misleading under severe class imbalance. See also: AUPRC.\nAutoregressive language model (AR LM) [ML]: A model that predicts the next token from previous tokens, learning a probability distribution over sequences. In genomics, AR objectives are used for DNA or protein sequence generation and likelihood-based scoring.\nBarcode (cell barcode) [Genomics]: A short DNA sequence attached during library prep that labels reads from the same cell in droplet-based single-cell assays. Enables multiplexing and cell-level counting.\nBase quality score recalibration (BQSR) [Computation]: A post-processing step (commonly in GATK pipelines) that corrects systematic biases in base quality scores reported by sequencers. Helps improve variant calling accuracy by better modeling sequencing error probabilities.\nBatch correction [Statistics]: Methods that remove technical variation while preserving biological signal, often by aligning latent spaces or matching neighbors across datasets. Essential for integrating studies and avoiding spurious clusters.\nBatch effect [Statistics]: Systematic differences introduced by processing samples in different experimental or computational batches. Batch effects can create spurious associations and must be addressed in QC and modeling.\nBenchmark suite [Computation]: A curated collection of datasets, tasks, and scoring rules used to evaluate models consistently. A good suite documents data provenance, splits, metrics, and known failure modes.\nBinary Alignment/Map (BAM) [Computation]: A compressed binary format for storing aligned sequencing reads and their metadata. Typically produced from FASTQ reads after alignment and used as input for variant calling.\nBrier score [Statistics]: A proper scoring rule that measures the mean squared error between predicted probabilities and binary outcomes. Commonly used to quantify probabilistic calibration. See also: calibration.\nCADD (Combined Annotation Dependent Depletion) [ML]: A widely used variant prioritization score that integrates diverse genomic annotations to estimate how deleterious a variant is likely to be. Trained using an evolutionary proxy task contrasting simulated variants with variants observed in humans.\nCalibration [Statistics]: The agreement between predicted probabilities (or risk scores mapped to probabilities) and observed outcome frequencies. Well-calibrated variant scores help clinicians interpret the meaning of ‘0.9 pathogenic’ as a real-world risk.\nCalibration drift [Statistics]: Degradation of calibration over time or across sites as data distributions change. Monitoring drift is important for long-lived clinical deployments.\nCalibration-in-the-large [Statistics]: A calibration check comparing the average predicted risk to the observed event rate. Useful for detecting systematic over- or underestimation after deployment.\nCanonical correlation analysis (CCA) [Statistics]: A technique that finds linear projections of two datasets that are maximally correlated. Used in multi-omic and batch integration to align shared structure across modalities or batches.\nCDS (Clinical decision support) [Clinical]: Software that provides patient-specific recommendations or alerts in clinical workflows. Genomic AI often reaches clinicians via CDS rather than standalone reports.\nChannel (CNN) [ML]: A feature dimension in convolutional inputs/outputs (e.g., number of filters) analogous to color channels in images. In genomics, channels often represent A/C/G/T in one-hot encoding.\nChinchilla-style compute-optimal training [ML]: A scaling-law insight that, for a fixed compute budget, there is an optimal balance between model size and number of training tokens. Helps decide whether to train a bigger genomic model or train longer on more sequence data. See also: scaling laws.\nChromatin accessibility [Genomics]: How open or closed DNA is in the nucleus, influencing whether transcription factors can bind regulatory sites. Often measured by ATAC-seq or DNase-seq and modeled by sequence-to-function foundation models.\nChromatin loop [Genomics]: A focal 3D contact between two genomic loci, often connecting enhancers to promoters or demarcating domains. Loops can be detected in Hi-C/Micro-C and are important for regulatory interpretation.\nCis-regulatory element (CRE) [Genomics]: A DNA region that regulates nearby genes, such as promoters, enhancers, silencers, and insulators. Regulatory foundation models aim to predict CRE activity from sequence and context.\nClass imbalance [Statistics]: A setting where one label is much rarer than the other. Requires appropriate metrics (AUPRC), sampling, and calibration to avoid misleading conclusions.\nCLIA (Clinical Laboratory Improvement Amendments) [Clinical]: U.S. regulations governing laboratory testing quality. Clinical genomic pipelines often require CLIA-compliant validation and documentation.\nClinical category mapping [Clinical]: A procedure that converts model outputs (continuous scores) into discrete clinical labels such as benign, VUS, likely pathogenic, or pathogenic. Requires calibration and careful threshold selection.\nClinical utility [Clinical]: Evidence that using a test or model improves meaningful outcomes (health, decisions, cost) compared to standard care. Utility is often the hardest and most important bar for translation.\nClinVar [Clinical]: A public archive of variant interpretations and supporting evidence linking genetic variants to clinical phenotypes. Widely used in clinical variant interpretation and to label variants for benchmarking.\nClosed-world assumption [Statistics]: Assuming the deployment environment matches the training/test distribution and all relevant classes are known. Often violated in genomics, motivating OOD detection and robustness tests.\nClumping and thresholding (C+T) [Statistics]: A simple polygenic score construction method that selects variants passing a p-value threshold and prunes correlated variants using linkage disequilibrium. Often used as a baseline against more complex methods like LD-aware shrinkage.\nCo-assay (paired multi-omics) [Genomics]: A protocol that measures multiple modalities from the same cell (e.g., RNA + ATAC). Enables direct linking of regulatory state to gene expression without relying solely on computational alignment.\nCohesin [Genomics]: A protein complex that helps form chromatin loops and organize 3D genome structure, often working with CTCF. Cohesin dynamics influence enhancer–promoter interactions.\nConformal prediction [Statistics]: A method that wraps a predictive model to provide prediction sets or intervals with statistical coverage guarantees under mild assumptions. Useful for communicating uncertainty in variant interpretation.\nConfounder [Statistics]: A variable that influences both inputs and outcomes, creating spurious associations. Confounders are common in biomedical data (batch, ancestry, site) and must be addressed in design and evaluation.\nContamination [Clinical]: Unintended mixing of DNA from different samples, leading to incorrect genotypes and false variants. Detected during sample-level QC and can severely distort downstream analyses.\nContamination (data leakage) [Statistics]: Accidental overlap between training and test information, such as shared individuals, highly similar sequences, or duplicated labels. Leakage can yield unrealistically high benchmark performance.\nContext length [ML]: The maximum sequence length a model can process in one forward pass. In genomics, longer context enables modeling distal regulatory interactions and haplotype-level effects.\nContrastive learning [ML]: A representation-learning approach that brings related examples closer in embedding space while pushing unrelated ones apart. Used to align sequences with functional assays or to learn robust embeddings. See also: embedding.\nConvolutional neural network (CNN) [ML]: A neural network that uses convolutions to learn local patterns with weight sharing and translation equivariance. In genomics, CNNs often learn sequence motifs and local regulatory signatures.\nCopy number variant (CNV) [Genomics]: A structural change where a DNA segment is deleted or duplicated, altering the number of copies. CNVs can have large phenotypic effects but are harder to detect than SNPs in short-read data.\nCoverage (Depth) [Genomics]: The number of sequencing reads overlapping a genomic position. Higher depth generally improves confidence in genotype calls, especially for rare variants or low-quality regions.\nCoverage (single-cell) [Statistics]: The number of reads or unique molecules observed per cell. Low coverage increases dropout and uncertainty, affecting clustering and differential expression.\nCTCF [Genomics]: A DNA-binding protein strongly associated with insulators and TAD boundaries. Frequently used as an anchor for interpreting 3D genome structure and loop formation.\nCuration [Clinical]: Manual or semi-automated review of evidence to produce trusted annotations (variant classification, gene–disease validity). Curation quality strongly affects downstream AI.\nData governance [Clinical]: Policies and processes for data access, privacy, consent, and stewardship. Essential for sharing multi-institutional clinical genomics data responsibly.\nDecision threshold [Clinical]: A cutoff on risk or score that triggers an action, balancing false positives and false negatives under real clinical costs. Thresholds should be validated and monitored, not chosen only on internal benchmarks.\nDegree (node degree) [Statistics]: The number of edges connected to a node in a graph (or the sum of edge weights). Used as a basic measure of connectivity in biological networks.\nDeleteriousness score [ML]: A model-derived score intended to rank variants by likelihood of harmful biological impact. Deleteriousness scores are used for variant prioritization but should not be interpreted as direct clinical risk. See also: CADD.\nDifferential expression (DE) [Statistics]: Testing whether gene expression differs between groups of cells or conditions. Requires models that handle count noise and multiple testing. See also: false discovery rate (FDR).\nDigital biomarker [Clinical]: A biomarker derived from digital sources such as wearables, images, or EHR signals. Can complement genomics for risk prediction and monitoring.\nDilation (dilated convolution) [ML]: A convolution variant that spaces kernel elements apart, expanding receptive field without increasing parameter count. Helps CNNs capture longer-range dependencies; see also: receptive field.\nDimensionality reduction [ML]: Methods that map high-dimensional omics data to fewer dimensions for visualization or modeling. Common choices include PCA (linear) and UMAP/t-SNE (nonlinear).\nDistribution shift [Statistics]: A mismatch between the data distribution seen during training and the distribution at deployment. Can degrade variant scoring when moving across ancestries, assays, tissues, or sequencing technologies. See also: out-of-distribution detection.\nDNA language model (DNA LM) [ML]: A language model trained on DNA sequences to learn general-purpose representations or likelihoods, often using masked or autoregressive objectives. Can be adapted to tasks like regulatory prediction or variant effect scoring. See also: tokenization.\nDoublet [Genomics]: An artifact where two cells are captured together and profiled as one, producing mixed expression. Doublets can form spurious clusters if not detected and filtered.\nDropout (single-cell) [Statistics]: Zero counts caused by limited capture efficiency and sampling, not true absence of expression. Dropout complicates interpretation and motivates models that treat zeros probabilistically.\nDrug–gene interaction [Clinical]: A relationship where genetic variation affects drug response, efficacy, or toxicity. Central to pharmacogenomics and precision prescribing.\nEffect size (beta) [Statistics]: A quantitative estimate of how much a variant changes a trait (for continuous traits) or the log-odds of disease (for binary traits). Effect sizes from GWAS are the weights used in many polygenic scores.\nEHR (Electronic health record) [Clinical]: A system storing patient clinical data, including diagnoses, labs, medications, and notes. EHR integration is key for deploying risk models and monitoring real-world performance.\nEmbedding [ML]: A learned continuous vector representation of discrete inputs such as tokens, k-mers, or amino acids. Embeddings capture similarity and enable neural models to operate in continuous space; see also: token.\nENCODE [Genomics]: The Encyclopedia of DNA Elements project providing large-scale functional genomics datasets (e.g., ChIP-seq, accessibility) used to annotate regulatory elements. Central to building training targets and features for genomic models.\nEnhancer [Genomics]: A regulatory DNA element that can increase transcription of a target gene when bound by transcription factors, often acting at a distance. Enhancers are frequently identified via chromatin accessibility and histone mark data.\nEnhancer–promoter interaction [Genomics]: A functional relationship where an enhancer influences transcription at a promoter, often facilitated by 3D looping. Multi-scale models aim to predict these interactions from sequence and context.\nEpistemic uncertainty [Statistics]: Uncertainty due to limited data or model knowledge, which can often be reduced with more data. Important for identifying when a model is extrapolating beyond training coverage.\neQTL (expression quantitative trait locus) [Genomics]: A genetic variant associated with changes in gene expression levels, often in a tissue-specific manner. eQTL maps (e.g., from GTEx) help link noncoding variants to candidate genes.\nEvidence code [Clinical]: A standardized label used in ACMG/AMP-style variant interpretation (e.g., strong, moderate, supporting evidence categories). Model outputs can provide quantitative evidence that must be integrated with other clinical data.\nEvidence synthesis [Statistics]: Combining multiple evidence sources—studies, assays, clinical reports—into a single conclusion. In variant interpretation, synthesis supports consistent classifications and updates.\nEvolutionary proxy task [ML]: A training strategy that uses evolutionary patterns as labels, such as contrasting simulated variants with variants observed in humans, to learn a deleteriousness signal without direct clinical labels. Used by CADD to scale training.\nExome sequencing (WES) [Genomics]: Sequencing focused on protein-coding regions (exons), typically using capture sequencing. Provides high depth in coding regions at lower cost than whole-genome sequencing, but misses most regulatory DNA.\nExpected calibration error (ECE) [Statistics]: A summary statistic of calibration that averages the gap between predicted confidence and observed accuracy across bins. Useful but sensitive to binning choices.\nExternal validation [Clinical]: Testing a model on data from a different site, cohort, or time period than training. Stronger evidence of real-world generalization than internal splits.\nFalse negative (FN) [Statistics]: A missed positive case (e.g., a pathogenic variant labeled benign). In clinical genetics, false negatives can delay diagnosis and appropriate care.\nFalse positive (FP) [Statistics]: An incorrect positive prediction (e.g., benign variant flagged pathogenic). FPs can trigger unnecessary anxiety, testing, or treatment.\nFamily segregation [Clinical]: Evaluating whether a variant tracks with disease within a family. Strong segregation evidence can support pathogenicity classification in rare disease.\nFASTQ [Computation]: A text file format that stores raw sequencing reads along with per-base quality scores. FASTQ is typically the starting point for alignment and variant calling pipelines.\nFDA SaMD (Software as a Medical Device) [Clinical]: A regulatory category for software intended for medical purposes without being part of a hardware device. Some clinical AI tools may be regulated as SaMD depending on claims and use.\nFew-shot learning [ML]: A regime where a model adapts to a new task using only a small number of labeled examples. Foundation models often improve few-shot performance via transferable representations.\nFine-tuning [ML]: Updating a pre-trained model’s parameters on a downstream task, usually with a smaller learning rate and fewer steps than pretraining. Common for adapting DNA/protein LMs to assays or clinical labels.\nFoundation model (FM) [ML]: A large pre-trained model trained on broad data (often self-supervised) that can be adapted to many tasks. In genomics, FMs can operate on sequences, multi-omic profiles, or sequence-to-function mappings.\nFrameshift variant [Genomics]: An insertion or deletion whose length is not a multiple of three, shifting the reading frame of a coding sequence. Often has large effects on protein sequence and is frequently pathogenic.\nFully connected layer (dense layer) [ML]: A layer that applies a learned linear transformation to all input features, often followed by a nonlinearity. Used in classifier heads or MLP blocks; see also: MLP.\nFunctionally annotated variant [Clinical]: A genetic variant supported by evidence from functional assays or model-based predictions. Functional annotation can help resolve VUS and prioritize variants for follow-up.\nGated Linear Unit (GLU) [ML]: A neural network block that multiplies one linear projection by a learned gate from another projection, improving expressiveness. Variants (e.g., SwiGLU) are common in modern transformers.\nGATK (Genome Analysis Toolkit) [Computation]: A widely used software suite for variant discovery and genotyping from sequencing data. Includes steps like BQSR and VQSR in many classical pipelines.\nGELU [ML]: Gaussian Error Linear Unit, an activation function that smoothly gates inputs based on a Gaussian curve. Common in transformer MLPs; see also: activation function.\nGene regulatory network (GRN) [Genomics]: A model of how transcription factors and other regulators interact to control gene expression. Learned or approximated using multi-omic data and sometimes foundation model embeddings.\nGene set enrichment analysis (GSEA) [Statistics]: A method that tests whether predefined gene sets (pathways) are overrepresented among up- or down-regulated genes. Helps interpret DE results in biological terms.\nGenerative model [ML]: A model that can sample new data points (e.g., sequences) from a learned distribution. DNA/protein LMs are generative and can be used for design or likelihood scoring.\nGenetic counseling [Clinical]: Clinical support that explains genetic results, uncertainty, and implications to patients and families. Even strong models require counseling to communicate limitations and next steps.\nGene–disease validity [Clinical]: The strength of evidence that a gene is causally associated with a disease. High validity supports diagnosis; low validity cautions against overinterpretation of variants in that gene.\nGenome sequencing (WGS) [Genomics]: Sequencing most of the genome, including non-coding regions. Increases diagnostic yield for some cases (SVs, regulatory variants) but adds interpretation complexity.\nGenome-wide association study (GWAS) [Statistics]: A study design that scans the genome for variants associated with a trait by testing each variant for statistical association across many individuals. GWAS identifies correlated signals that typically require fine-mapping to suggest causality.\nGenomic holdout (gene/locus holdout) [Statistics]: A split that withholds entire genes, chromosomes, or loci to test extrapolation to unseen genomic regions. Helps reduce local-sequence leakage that can inflate performance.\nGenomic inflation factor (λGC) [Statistics]: A diagnostic statistic summarizing inflation of GWAS test statistics relative to expectation, often used to detect residual confounding or polygenicity. Interpreted alongside QQ plots and study design details.\nGenotype–phenotype correlation [Clinical]: Relationships between genetic variants and observable traits or disease manifestations. Improves diagnosis, prognosis, and therapeutic targeting.\nGERP [Genomics]: A conservation-based score (Genomic Evolutionary Rate Profiling) that estimates evolutionary constraint by comparing observed substitutions to an expected neutral rate. Often used as an input feature in variant prioritization models.\nGIAB (Genome in a Bottle) [Genomics]: A consortium that provides high-confidence benchmark genomes and variant call sets for evaluating sequencing and variant calling accuracy. Frequently used to quantify performance of new callers like DeepVariant.\nGraph attention network (GAT) [ML]: A GNN that learns attention weights over neighbors during message passing. Useful when different neighbors contribute unequally, such as in heterogeneous biological graphs.\nGraph convolutional network (GCN) [ML]: A graph neural network that updates node features by aggregating information from neighbors. Used for tasks like gene function prediction or disease-gene prioritization.\nGraph neural network (GNN) [ML]: A neural model that operates on graphs by iteratively passing and aggregating messages along edges. Enables learning from biological networks such as PPIs or cell–cell interaction graphs.\ngRNA (guide RNA) [Genomics]: An RNA sequence that directs CRISPR systems to a target DNA locus. Used in functional screens and genome editing experiments that generate training data.\nGTEx (Genotype-Tissue Expression) [Genomics]: A consortium resource linking genetic variation to gene expression across many human tissues. Central for eQTL discovery and interpreting regulatory variants.\nHallucination (model output) [ML]: Producing confident but incorrect outputs or explanations. In interpretability and reporting, hallucinations can mislead users if not constrained by evidence.\nHardy–Weinberg equilibrium (HWE) [Statistics]: A population genetics expectation relating genotype frequencies to allele frequencies under random mating and no selection. Deviations can indicate genotyping artifacts or population structure; often used in sample-level QC.\nHGMD (Human Gene Mutation Database) [Clinical]: A curated database of published gene lesions associated with human disease. Often used historically for variant interpretation, though access and curation practices differ from open resources like ClinVar.\nHi-C [Genomics]: A chromosome conformation capture assay that measures 3D contacts between genomic loci. Provides information about long-range regulatory interactions and domain structure.\nHigh-throughput screening (HTS) [Clinical]: Testing large numbers of compounds or perturbations in parallel to identify hits. ML often prioritizes what to screen and how to learn from results.\nHighly variable genes (HVGs) [Statistics]: Genes with higher-than-expected variance across cells, often used as features for clustering and integration. HVG selection helps focus on biological signal but can miss subtle programs.\nHorizon (prediction horizon) [Clinical]: The time window over which a model predicts an outcome, such as 5-year risk or time-to-progression. Horizon choice affects labels, censoring, and clinical utility.\nHPO (Human Phenotype Ontology) [Clinical]: A standardized vocabulary for phenotypic abnormalities used to represent patient features in rare disease. Enables phenotype matching and computational diagnostics.\nHuman-in-the-loop [Clinical]: A deployment pattern where clinicians or curators review model outputs before action. Common for early clinical adoption to manage risk and build trust.\nImputation (single-cell) [Statistics]: Methods that estimate missing or dropped-out expression values. Can improve visualization but may distort variance and inflate confidence if used uncritically.\nIn silico saturation mutagenesis (ISM) [ML]: A technique that measures a model’s sensitivity by systematically mutating each position and observing changes in predictions. Used to interpret regulatory and variant models. See also: saliency map.\nIn-context learning (ICL) [ML]: A capability where a model adapts its behavior from examples provided in the prompt without updating weights. Often associated with large auto-regressive transformers; see also: prompting.\nIndel [Genomics]: A small insertion or deletion of DNA bases relative to the reference sequence. Indels are generally harder to call than SNPs, especially in repetitive regions.\nInformed consent [Clinical]: A process ensuring patients understand what data are collected, how results are used, and what risks exist. Consent scope can limit data use for model training and sharing.\nInterpretability [ML]: Methods that help explain model behavior in human-understandable terms, such as motifs, salient residues, or feature attributions. Supports debugging, trust calibration, and biological insight.\nIRB (Institutional Review Board) [Clinical]: A committee that reviews research involving human subjects for ethics and participant protections. Clinical ML studies often require IRB review even for retrospective analyses.\nKernel (CNN filter) [ML]: A small set of learned weights applied in a convolution to detect a specific local pattern. In genomics, kernels often learn motif-like detectors.\nLabel adjudication [Clinical]: A process where disagreements in labels (e.g., variant classifications) are resolved by expert review or consensus rules. Reduces noise and improves clinical-grade datasets.\nLabel noise [Statistics]: Errors or uncertainty in the target labels used for training and evaluation. Common in biomedical outcomes and variant labels, and can cap achievable performance.\nLayer normalization (LayerNorm) [ML]: A normalization technique that normalizes activations within each example across features, stabilizing training in transformers. Unlike BatchNorm, it does not rely on batch statistics.\nLead compound [Clinical]: A chemically optimized hit with improved potency, selectivity, and drug-like properties. Leads progress into preclinical development.\nLinkage disequilibrium (LD) [Genomics]: Non-random association of alleles at nearby loci, common in human genomes. LD can create confounding in variant benchmarks if splits do not separate correlated regions.\nLinkage disequilibrium (LD) [Statistics]: Non-random correlation between alleles at nearby loci caused by shared ancestry and limited recombination. LD is central to interpreting GWAS peaks and constructing polygenic scores.\nLongitudinal data [Clinical]: Measurements collected over time, such as repeated labs or outcomes. Enables risk prediction, progression modeling, and monitoring of post-deployment drift.\nLoRA (Low-Rank Adaptation) [ML]: A parameter-efficient fine-tuning method that learns low-rank updates to weight matrices while keeping original weights frozen. Often used to adapt large foundation models on limited compute. See also: adapter.\nMasked language modeling (MLM) [ML]: A self-supervised objective where random tokens are masked and the model predicts them from surrounding context. Widely used for DNA and protein foundation models to learn rich embeddings.\nMechanistic interpretability [ML]: Methods that aim to understand internal model computations (circuits, attention patterns, neurons) rather than only correlational feature importance. In genomics, used to connect learned features to motifs, domains, or pathways.\nMendelian disease [Clinical]: A disorder primarily caused by variants in a single gene with strong effects, often following dominant or recessive inheritance. Many rare diseases are Mendelian, making them targets for genomic diagnosis.\nMinimal clinically important difference (MCID) [Clinical]: The smallest change in an outcome that patients perceive as beneficial and that would justify a change in care. Helps translate statistical improvements into meaningful utility.\nMinor allele frequency (MAF) [Genomics]: The frequency of the less common allele at a biallelic locus. MAF thresholds are used in QC, filtering, and defining rare vs common variants.\nmiRNA (microRNA) [Genomics]: A short non-coding RNA that regulates gene expression post-transcriptionally, typically by binding target mRNAs and reducing translation or stability. Important in development and disease.\nMissense variant [Genomics]: A single-nucleotide change that alters the amino acid at a position in a protein. Effects vary widely, making missense interpretation a central VEP challenge.\nMLOps (machine learning operations) [Computation]: Practices for deploying, monitoring, and maintaining models in production. In clinical settings, MLOps includes audit trails, versioning, drift monitoring, and governance.\nMLP (Multi-Layer Perceptron) [ML]: A feed-forward block composed of one or more dense layers with nonlinearities, often used inside transformer blocks. Provides token-wise feature mixing; see also: GELU, GLU.\nModel monitoring [Clinical]: Ongoing tracking of model inputs, outputs, calibration, and outcomes after deployment. Detects drift, bias, and performance degradation in real-world use.\nModel updating [Clinical]: Changing a deployed model using new data, recalibration, or retraining. Requires governance because updates can change clinical behavior and may trigger regulatory review.\nMOFA (Multi-Omics Factor Analysis) [Statistics]: A factor model for jointly analyzing multiple omics modalities to identify shared and modality-specific sources of variation. Useful for interpretability in multi-omic integration.\nMonte Carlo dropout (MC dropout) [Statistics]: Using dropout at inference time to generate multiple predictions and estimate uncertainty from their variability. A practical approximation to Bayesian uncertainty in deep nets.\nMotif [Genomics]: A short recurring sequence pattern that is biologically meaningful, often representing a transcription factor binding preference. CNN kernels and attention heads can learn motif-like detectors; see also: transcription factor.\nMultiome (scRNA+scATAC) [Genomics]: Single-cell protocols that measure transcriptome and chromatin accessibility in the same cell. Supports direct regulatory-to-expression linking and improved cell-state resolution.\nMutual nearest neighbors (MNN) [ML]: An integration idea that matches cells across datasets by identifying pairs that are nearest neighbors of each other in embedding space. Helps align batches or modalities while preserving local structure.\nNet benefit [Clinical]: A decision-analytic quantity balancing true positives against false positives at a given threshold. Used in decision curve analysis to compare clinical utility.\nNet reclassification improvement (NRI) [Statistics]: A metric assessing whether a new model more appropriately moves people across clinically meaningful risk categories compared to a baseline model. Useful when categories drive actions.\nNext-token prediction (NTP) [ML]: A self-supervised objective where the model predicts the next token in a sequence, typically with a causal mask. Used for many generative language models and adapted for biological sequences.\nNon-coding RNA (ncRNA) [Genomics]: RNA molecules that are not translated into proteins, including lncRNA and miRNA. ncRNAs often regulate gene expression and can be disease-relevant.\nNonsense variant [Genomics]: A nucleotide change that introduces a premature stop codon, truncating the protein. Often strongly disruptive and frequently classified as pathogenic, depending on context.\nNormalization (single-cell) [Statistics]: Transformations that make expression counts comparable across cells, often correcting for library size and technical effects. Choices strongly affect downstream clustering and DE.\nNumber needed to treat (NNT) [Clinical]: How many patients must receive an intervention to prevent one adverse outcome. Helps translate risk prediction and treatment decisions into intuitive clinical impact.\nOdds ratio (OR) [Statistics]: A measure of association for binary outcomes, comparing odds of disease between genotypes or exposure groups. Often reported for case-control GWAS; related to the effect size on the log-odds scale.\nOOD detection (out-of-distribution detection) [Statistics]: Methods that identify inputs unlike the training data where predictions may be unreliable. Important for genomic models deployed across species, assays, ancestries, or sequencing pipelines. See also: distribution shift.\nOpen set recognition [ML]: Recognizing when an input belongs to a class not seen during training. Relevant for clinical settings where rare diseases or novel variant mechanisms appear.\nOut-of-distribution (OOD) [Statistics]: Inputs that differ substantially from training data, where predictions may be unreliable. Evaluations often include OOD tests across sites, assays, species, or ancestries.\nOutcome definition [Clinical]: Operationalizing a clinical endpoint from data (often EHR), such as defining ‘myocardial infarction’ by codes and labs. Poor definitions create label noise and confounding.\nParameter-efficient fine-tuning (PEFT) [ML]: A family of methods that adapt a pre-trained model by training only a small subset of parameters (adapters, LoRA, prompt tuning). Reduces compute and storage while preserving base model weights.\nPathogenic [Clinical]: A clinical classification indicating strong evidence that a variant contributes to disease. Common classification schemes also include likely pathogenic, VUS, likely benign, and benign.\nPathogenicity score [Clinical]: A numeric output estimating how likely a variant is to be disease-causing. For clinical utility, the score must be calibrated and validated in representative cohorts.\nPatient stratification [Clinical]: Grouping patients into clinically meaningful subgroups, such as responders vs non-responders or high vs low risk. Stratification can improve trial design and personalized care.\nPeak (chromatin peak) [Genomics]: A genomic region with elevated signal in an accessibility or binding assay (e.g., ATAC-seq peak). Peaks approximate regulatory elements used as features in scATAC-seq.\nPerformance stratification [Statistics]: Reporting metrics separately by subgroup (ancestry, tissue, site, gene class) rather than only overall averages. Reveals inequities and hidden failure modes.\nPharmacodynamics (PD) [Clinical]: What a drug does to the body, including biological effects and biomarker changes. PD helps connect target engagement to outcomes.\nPharmacogenomics (PGx) [Clinical]: The study of how genetic variation influences drug response and toxicity. Supports dose adjustments and drug selection to improve safety and efficacy.\nPharmacokinetics (PK) [Clinical]: What the body does to a drug, including absorption and clearance. PK informs dosing and is a major cause of trial failure when misestimated.\nPhenotyping (computational) [ML]: Extracting structured phenotypes from EHR data, including diagnoses, labs, and notes. Accurate phenotyping is essential for training and evaluating clinical risk models.\nPlatt scaling [Statistics]: A calibration method that fits a logistic regression mapping from model scores to probabilities. Often used to calibrate classifiers, including variant effect predictors. See also: temperature scaling.\nPointwise convolution (1×1 convolution) [ML]: A convolution with kernel size 1 that mixes information across channels at each position. Often used after depthwise convolutions; see also: depthwise separable convolution.\nPolygenic risk score (PRS) [Clinical]: A weighted sum of many genetic variants estimating genetic predisposition to a complex trait. PRS can inform screening and prevention but must be evaluated for calibration and fairness across ancestries.\nPolygenic risk score (PRS) [Statistics]: A polygenic score for a disease outcome, usually reported as a relative risk indicator rather than a diagnosis. Often used interchangeably with PGS, though PRS emphasizes clinical risk contexts.\nPolygenic score (PGS) [Statistics]: A weighted sum of many genetic variants used to predict a trait or disease risk. Weights typically come from GWAS effect sizes and may be adjusted for LD and ancestry.\nPost-market surveillance [Clinical]: Monitoring safety and effectiveness after approval or deployment, using real-world data. Critical for identifying rare adverse events and model drift.\nPosterior inclusion probability (PIP) [Statistics]: In Bayesian fine-mapping, the probability that a given variant is causal (or included) in the model explaining an association signal. Used to prioritize variants within a credible set.\nPredictive interval [Statistics]: A range expected to contain a future observation with a specified probability. For clinical risk, intervals can communicate uncertainty beyond point estimates.\nPretest probability [Clinical]: The probability of disease before considering a test result, based on prevalence and clinical presentation. Determines how strongly a result shifts belief and impacts PPV/NPV.\nPrincipal components (genetic PCs) [Statistics]: Low-dimensional axes summarizing genetic variation patterns across individuals. Included as covariates in GWAS to reduce confounding from population structure.\nPromoter [Genomics]: A regulatory region near a gene’s transcription start site that helps initiate transcription. Promoter activity is often measured via ChIP-seq and chromatin accessibility assays.\nProspective study [Clinical]: A study where outcomes are measured after model deployment or enrollment, reducing certain biases in retrospective datasets. Strong evidence for clinical utility.\nProspective validation [Clinical]: Evaluating a model in a forward-looking setting where outcomes occur after deployment or enrollment. Stronger evidence than retrospective evaluation for clinical utility.\nProtein language model (protein LM) [ML]: A language model trained on amino acid sequences to learn representations related to structure and function. Used for tasks like mutation effect prediction, annotation transfer, and protein design.\nProtein–protein interaction (PPI) [Genomics]: A physical interaction between proteins, often represented as a network. PPIs provide priors for pathway analysis and graph-based prediction.\nProxy endpoint (surrogate endpoint) [Clinical]: A biomarker or intermediate outcome used in place of a direct clinical outcome. Can speed trials but may fail if not causally linked to patient benefit.\nQuality control (QC) [Computation]: Procedures to detect and remove low-quality samples, reads, or variants (e.g., contamination, batch effects, low call rate). QC is essential to avoid spurious associations in GWAS and errors in variant calling.\nQuality management system (QMS) [Clinical]: A formal system of processes and documentation to ensure consistent quality in clinical products. Many regulated clinical tools require a QMS to manage changes and audits.\nQuery–Key–Value (QKV) [ML]: The three learned projections used in attention: queries decide what to look for, keys index what is available, and values carry information to be aggregated. Core to transformer self-attention.\nReal-world evidence (RWE) [Clinical]: Evidence derived from real-world data such as EHRs, claims, and registries. RWE supports post-market surveillance and can complement randomized trials.\nRecalibration [Statistics]: Updating the mapping from scores to probabilities when deploying in a new setting or after drift. Often required for safe clinical use across sites.\nReliability diagram [Statistics]: A plot that compares predicted probabilities to observed frequencies across bins to assess calibration. In well-calibrated models, points lie near the diagonal; see also: calibration.\nRepetitive region [Genomics]: DNA sequences with many similar copies (e.g., STRs, segmental duplications) that are difficult to map with short reads. Such regions increase uncertainty in alignment and variant calling.\nRepresentation learning [ML]: Learning features (embeddings) from data that capture useful structure for many tasks. Self-supervised DNA/protein models are primarily representation learners.\nResidual connection (skip connection) [ML]: A connection that adds a layer’s input to its output, easing optimization and enabling very deep networks. Standard in transformers and modern CNNs; see also: layer normalization.\nRisk model [Clinical]: A model that estimates probability of a future clinical event, such as disease onset or progression. For use in care, it must be calibrated, validated externally, and tied to actions.\nrRNA (ribosomal RNA) [Genomics]: RNA components of ribosomes that are highly abundant in cells. rRNA is often depleted in RNA-seq library prep to focus sequencing on informative transcripts.\nSafety margin (therapeutics) [Clinical]: The gap between effective and toxic dose levels. A narrow safety margin increases the importance of accurate dosing and careful monitoring.\nSaliency map [ML]: An interpretation method that highlights which input positions most influence a model’s prediction, typically using gradients. In genomics, saliency often aligns with motifs or critical residues. See also: ISM.\nSaMD change control [Clinical]: A governance process managing software updates that might affect clinical performance. Important for maintaining compliance and safety when models are updated.\nScaling laws [ML]: Empirical relationships showing how model performance changes with model size, dataset size, and compute, often following power laws. Used to plan compute-optimal training and forecast returns from scaling.\nscATAC-seq (single-cell ATAC-seq) [Genomics]: Single-cell assay that measures chromatin accessibility per cell. Complements scRNA-seq by reflecting regulatory potential and transcription factor activity.\nscRNA-seq (single-cell RNA sequencing) [Genomics]: Single-cell assay measuring transcript abundance per cell. Enables discovery of cell types and states and characterization of heterogeneity in tissues.\nSequence length (context length) [ML]: The number of tokens processed in a single model input. Longer contexts capture more information but increase compute, especially for quadratic-time attention; see also: FlashAttention.\nSequence-to-function model [ML]: A model that predicts functional outputs (e.g., accessibility, expression, binding) directly from DNA sequence, often conditioned on cell type or assay. Bridges sequence and phenotype-relevant measurements.\nSHAP (SHapley Additive exPlanations) [ML]: A feature-attribution approach based on Shapley values that estimates each feature’s contribution to a prediction. Common for tabular clinical models; expensive at scale.\nSIFT [Genomics]: A missense variant effect predictor that estimates whether an amino acid substitution is likely to affect protein function based on sequence conservation. Often used as a baseline comparator to learned scores.\nSingle nucleotide polymorphism (SNP) [Genomics]: A common single-base variant in a population, often used to refer to biallelic SNVs in GWAS arrays. SNPs are the most common variant type in many association studies.\nSingle nucleotide variant (SNV) [Genomics]: A change of a single nucleotide at a specific position relative to the reference genome. SNV is a general term; SNP typically implies the variant is common in a population.\nSplice variant [Genomics]: A variant that disrupts RNA splicing signals (e.g., donor/acceptor sites), potentially causing exon skipping or intron retention. Splicing effects can be modeled by sequence-based predictors.\nState space model (SSM) [Computation]: A sequence modeling architecture that can scale to long contexts with different compute trade-offs than attention. Sometimes explored as an alternative backbone for long-genome modeling.\nStructural variant (SV) [Genomics]: A larger genomic alteration such as deletions, duplications, inversions, translocations, or mobile element insertions. SVs can have large functional effects but are challenging to detect and represent.\nT2T-CHM13 [Genomics]: A telomere-to-telomere human genome assembly that improves representation of previously unresolved regions. Highlights how reference improvements can change mapping and variant interpretation.\nTAD (Topologically associating domain) [Genomics]: A genomic region with elevated internal contact frequency in 3D genome assays. TAD boundaries constrain enhancer–promoter interactions and can be affected by structural variation.\nTarget engagement [Clinical]: Evidence that a drug binds or modulates its intended biological target in vivo. Target engagement is necessary but not sufficient for clinical benefit.\nTarget validation [Clinical]: Experimental and clinical evidence confirming that modulating a target affects disease-relevant outcomes. Validation reduces attrition in drug programs.\nTF (transcription factor) [Genomics]: A protein that binds DNA motifs to regulate gene transcription. TF activity is often inferred from accessibility and expression rather than measured directly.\nThreshold selection [Clinical]: Choosing score cutoffs that balance false positives and false negatives given clinical costs and resources. Should be justified with decision analysis and validated externally.\nToken [ML]: A discrete unit of input to a model, such as a character, k-mer, or subword. Tokenization choices trade off biological resolution, sequence length, and compute; see also: vocabulary.\nTokenization [ML]: The process of converting raw sequences into tokens (characters, k-mers, or learned units) for model input. In genomics, tokenization controls resolution and computational cost. See also: k-mer, context length.\nTopic model (gene program) [ML]: A model that represents cells as mixtures of latent topics/programs, each involving a set of genes or peaks. Useful for capturing continuous variation and shared programs across cell types.\nTopologically associating domain (TAD) [Genomics]: A 3D genomic region with higher internal contact frequency than contacts across its boundaries. TADs help constrain enhancer–promoter interactions and can influence regulatory variant effects.\nTPM (transcripts per million) [Statistics]: A normalization unit that scales transcript counts by gene length and library size to approximate relative abundance. Used primarily in bulk RNA-seq; in single-cell, UMI counts are often preferred.\nTranscription factor (TF) [Genomics]: A protein that binds specific DNA motifs to regulate gene expression. Many genomics models aim to predict TF binding or learn TF-like motif detectors; see also: motif.\nTransformer [ML]: A neural network architecture built around self-attention and feed-forward blocks, enabling flexible modeling of long-range dependencies. Widely used for language and increasingly for biological sequences.\nTriage model [Clinical]: A model used to prioritize cases for further review or testing rather than making final decisions. Often the safest initial deployment pattern for genomic AI.\nUMAP (Uniform Manifold Approximation and Projection) [ML]: A nonlinear embedding method widely used for single-cell visualization that preserves neighborhood structure and can reflect some global geometry. Typically built on a kNN graph.\nUMI (Unique molecular identifier) [Genomics]: A short random sequence attached to molecules before amplification to identify duplicates. UMIs enable more accurate molecule counting and reduce PCR bias in scRNA-seq.\nUncertainty communication [Clinical]: Presenting uncertainty in a way clinicians and patients can act on, such as confidence categories or calibrated probabilities. Essential when reporting VUS or borderline risk.\nUncertainty quantification (UQ) [Statistics]: Methods for estimating how reliable a prediction is, including epistemic uncertainty (model uncertainty) and aleatoric uncertainty (data noise). Critical for clinical deployment of variant predictors.\nVAE (variational autoencoder) [ML]: A generative latent-variable model that learns a probabilistic latent space for high-dimensional data. VAEs underpin many single-cell tools for denoising, integration, and batch correction.\nVariance (metric variance) [Statistics]: The variability of an evaluation metric due to finite sample size or randomness (initialization, sampling). Reporting variance or CIs prevents overclaiming small differences.\nVariant Call Format (VCF) [Computation]: A standard text format for storing variant sites, genotypes, and associated annotations/quality fields across one or more samples. Common output of variant calling and common input to downstream analyses.\nVariant effect prediction (VEP) [Clinical]: Estimating how a genetic variant changes molecular function or disease risk using computational models and evidence. Foundation models contribute by improving coverage and transfer, but still require validation and calibration.\nVariant interaction (epistasis) [Genomics]: A non-additive effect where the impact of one variant depends on another variant. Long-context and haplotype-aware models aim to capture epistatic patterns.\nVariant of uncertain significance (VUS) [Clinical]: A variant whose relationship to disease cannot be confidently classified as benign or pathogenic from available evidence. A major challenge for translating genomic data into clinical decisions.\nVariant prioritization [Clinical]: The process of ranking candidate variants for follow-up or interpretation based on predicted functional impact, frequency, inheritance, and phenotype match. Tools like CADD provide one layer of evidence, not a final diagnosis.\nVariant quality score recalibration (VQSR) [Computation]: A statistical modeling step (commonly in GATK pipelines) that assigns calibrated quality scores to variant calls based on multiple annotation features. Used to separate likely true variants from artifacts.\nWarmup (learning-rate warmup) [ML]: A training schedule that gradually increases the learning rate during early steps to stabilize optimization, especially for transformers. Commonly followed by decay; see also: optimization.\nWeighted nearest neighbors (WNN) [ML]: An integration approach that constructs a neighbor graph using modality-specific similarities combined with learned weights. Often used to integrate RNA with protein or ATAC signals.\nWhole-exome sequencing (WES) [Genomics]: See Exome sequencing (WES).\nWhole-genome sequencing (WGS) [Genomics]: Sequencing of (nearly) the entire genome, including coding and noncoding regions. Provides the most comprehensive variant discovery but is more expensive and data-intensive than targeted approaches.\nWorst-group performance [Statistics]: A robustness/fairness summary that reports the lowest performance across defined subgroups. Helps prevent good average performance from hiding harmful failures.\nZero-shot prediction [ML]: Making predictions on a new task or dataset without task-specific training, often by using a pre-trained model’s likelihood or embeddings. Common for mutation effect scoring from protein LMs.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "part_1/p1--foundations.html",
    "href": "part_1/p1--foundations.html",
    "title": "Part I: Data Foundations",
    "section": "",
    "text": "Part I at a Glance\n\n\n\nCentral question: What data and pre-deep-learning tools form the backdrop that any genomic foundation model must respect, integrate with, or improve upon?\nPrerequisites: Basic familiarity with molecular biology (DNA, genes, proteins) and statistics (regression, hypothesis testing). No deep learning background required.\n\n\n\n\n\n\n\n\nChapter\nTopic\nKey Concepts\n\n\n\n\n1  From Reads to Variants\nSequencing & Variant Calling\nNGS technologies, alignment, variant calling pipelines, error sources\n\n\n2  Data Landscape\nData Resources\nReference genomes, gnomAD, ClinVar, ENCODE, GTEx, UK Biobank\n\n\n3  GWAS and Polygenic Scores\nGWAS & Polygenic Scores\nAssociation studies, LD, fine-mapping, PGS construction, portability\n\n\n4  Classical Variant Prediction\nClassical VEP\nSIFT, PolyPhen, CADD, feature engineering, circularity problems\n\n\n\nAfter completing Part I, you will understand:\n\nHow sequencing data becomes the variants that models predict\nWhat public resources exist and their systematic biases\nWhat classical methods achieved and where they hit limitations\nWhy data quality and provenance matter for everything that follows\n\n\n\nEvery genomic foundation model inherits the biases of its training data. A model trained on European-dominated biobanks will miscalibrate predictions for other populations. A variant effect predictor learning from ClinVar inherits whatever ascertainment biases clinical laboratories embedded in those classifications. A regulatory model trained on ENCODE cell lines may fail on primary tissues absent from the training compendium. Foundation models do not transcend their data sources; they compress and reflect them. Understanding what data resources contain, what they systematically miss, and what assumptions they encode is prerequisite to understanding what foundation models can and cannot accomplish.\nGenomic foundation models inherit both the power and the limitations of the technologies that generate their training data. Next-generation sequencing and variant calling (1  From Reads to Variants) transform biological samples into the VCF files that serve as inputs to nearly all downstream analysis. Understanding these technologies reveals their remarkable power alongside their systematic blind spots: reference bias, missing structural variants, and error patterns that propagate into every model trained on their outputs.\nPublic resources underpin modern computational genomics, serving simultaneously as training data, evaluation benchmarks, and sources of prior biological knowledge (2  Data Landscape): reference genomes, population variation catalogs like gnomAD, functional genomics consortia such as ENCODE and Roadmap Epigenomics, and biobank-scale cohorts including the UK Biobank and GTEx. Genome-wide association studies and polygenic scores (3  GWAS and Polygenic Scores) provide both baselines against which deep models are measured and conceptual frameworks that inform their design. Pre-deep-learning variant effect prediction through CADD and related methods (4  Classical Variant Prediction) establishes what careful feature engineering achieved and where its limitations motivated the learned representations developed in subsequent parts.\n\n\n\n\n\n\nConnections to Later Parts\n\n\n\nThe data foundations established here recur throughout the book:\n\nPart II builds architectures that learn from the sequence data described in 1  From Reads to Variants\nPart III foundation models are evaluated against the benchmarks derived from 2  Data Landscape resources\nPart V (11  Benchmarks and Evaluation, 12  Confounding and Data Leakage) examines how biases introduced here propagate through evaluation\nPart VI clinical applications must navigate the ancestry and ascertainment biases documented in 3  GWAS and Polygenic Scores",
    "crumbs": [
      "Part I: Data Foundations"
    ]
  },
  {
    "objectID": "part_2/p2--principles.html",
    "href": "part_2/p2--principles.html",
    "title": "Part II: Sequence Architectures",
    "section": "",
    "text": "Part II at a Glance\n\n\n\nCentral question: How do architectural choices made before training begins determine what a model can learn about biological sequences?\nPrerequisites: Part I (genomic data context). For deep learning background, see Appendix A.\n\n\n\n\n\n\n\n\nChapter\nTopic\nKey Concepts\n\n\n\n\n5  Tokens and Embeddings\nSequence Representations\nOne-hot, k-mers, BPE, learned embeddings, position encodings\n\n\n6  Convolutional Networks\nConvolutional Networks\nMotif detection, regulatory prediction, receptive field limitations\n\n\n7  Transformers and Attention\nAttention & Transformers\nSelf-attention, position encodings, long-range dependencies\n\n\n8  Pretraining Strategies\nPretraining Objectives\nMLM, next-token prediction, contrastive learning, multi-task\n\n\n9  Transfer Learning Foundations\nTransfer Learning\nFine-tuning, domain adaptation, few-shot learning\n\n\n10  Adaptation Strategies\nModel Adaptation\nParameter-efficient fine-tuning, LoRA, prompt tuning\n\n\n11  Benchmarks and Evaluation\nBenchmarks & Evaluation\nBenchmark suites, evaluation methodology, metrics\n\n\n12  Confounding and Data Leakage\nConfounding & Leakage\nData leakage, batch effects, population stratification\n\n\n\nAfter completing Part II, you will understand:\n\nHow tokenization and representation choices shape what models can learn\nWhy CNNs revolutionized genomic deep learning and where they hit limits\nHow attention mechanisms enable long-range dependency modeling\nWhat pretraining objectives teach models about biological sequence\nHow to transfer learned representations to new tasks\nHow to evaluate models rigorously and detect confounding\n\n\n\nEvery neural network architecture encodes assumptions about biology. Convolutional networks assume that local patterns matter and that the same motifs are meaningful regardless of genomic position. Attention mechanisms assume that distant positions can interact directly without passing information through intermediate representations. Pretraining objectives assume that certain patterns in unlabeled sequence provide useful supervision in the absence of functional labels. These assumptions, embedded in architectural choices made before any training begins, determine which biological phenomena the model can capture and which remain invisible to it.\nArchitectural choices made before training begins constrain everything a model can learn. Tokenization choices (5  Tokens and Embeddings) propagate through model design, from one-hot encoding through byte-pair encoding to biologically informed vocabularies. Convolutional neural networks (6  Convolutional Networks) first demonstrated that deep learning could outperform handcrafted features for regulatory genomics by learning sequence-to-function mappings directly from data. Self-attention mechanisms and transformer architecture (7  Transformers and Attention) enable both local pattern recognition and long-range dependency modeling across genomic sequences.\nSelf-supervised objectives shape what models learn from unlabeled sequence (8  Pretraining Strategies). Masked language modeling, next-token prediction, and denoising approaches each encourage models to discover different biological patterns and produce representations with distinct properties. Adapting pretrained models to downstream tasks (9  Transfer Learning Foundations) through fine-tuning, few-shot learning, and deployment strategies completes the path from raw sequence to useful prediction. Parameter-efficient adaptation methods (10  Adaptation Strategies) enable practical fine-tuning when computational resources or labeled data are limited.\nRigorous evaluation requires understanding both what benchmarks measure (11  Benchmarks and Evaluation) and how confounding can inflate apparent performance (12  Confounding and Data Leakage). These evaluation principles apply throughout the book; mastering them here enables critical assessment of the foundation models surveyed in Parts III and IV.\n\n\n\n\n\n\nConnections to Other Parts\n\n\n\n\nPart I provides the data context that makes architectural choices meaningful\nPart III applies these principles to specific foundation model families (DNA-LM, protein-LM, regulatory)\nPart IV extends these architectures to cellular context and systems-scale modeling\nPart V deepens the evaluation framework with uncertainty, interpretability, and causal reasoning",
    "crumbs": [
      "Part II: Sequence Architectures"
    ]
  },
  {
    "objectID": "part_3/p3--architectures.html",
    "href": "part_3/p3--architectures.html",
    "title": "Part III: Foundation Model Families",
    "section": "",
    "text": "Part III at a Glance\n\n\n\nCentral question: What are the major foundation model families for genomics, and how do their different assumptions determine where each excels?\nPrerequisites: Part II (sequence architectures, pretraining, transfer learning)\n\n\n\n\n\n\n\n\nChapter\nTopic\nKey Models\n\n\n\n\n13  Foundation Model Paradigm\nFM Principles & Taxonomy\nScaling laws, emergence, foundation model definition\n\n\n14  DNA Language Models\nDNA Language Models\nDNABERT, Nucleotide Transformer, HyenaDNA, Evo\n\n\n15  Protein Language Models\nProtein Language Models\nESM, ProtTrans, ESMFold, AlphaFold2\n\n\n16  Regulatory Models\nRegulatory Models\nEnformer, Borzoi, AlphaGenome\n\n\n17  Variant Effect Prediction\nVariant Effect Prediction\nAlphaMissense, SpliceAI, integrated VEP\n\n\n\nAfter completing Part III, you will understand:\n\nWhat distinguishes foundation models from earlier supervised approaches\nHow DNA language models learn regulatory grammar from sequence\nWhy protein language models achieved such dramatic success\nHow hybrid architectures enable 200kb+ context windows\nHow different approaches combine for comprehensive variant effect prediction\n\n\n\nEach architecture embodies a different set of assumptions about biological sequence. Convolutional models assume that local motifs and their short-range combinations are the primary carriers of regulatory information; they learn to recognize transcription factor binding sites, splice signals, and chromatin accessibility patterns from the sequence grammar immediately surrounding each position. Protein language models treat amino acid sequences as structured compositions whose meaning emerges from evolutionary context; they learn what substitutions are tolerated by observing which sequences survived natural selection. DNA language models extend this paradigm to nucleotides, learning regulatory grammar through self-supervised objectives that predict masked or next tokens. Hybrid architectures attempt to reconcile local and global perspectives, using convolutions to extract features efficiently while deploying attention to model interactions spanning tens or hundreds of kilobases. Understanding these assumptions clarifies what each model family can capture and where each will fail.\nFoundation models in genomic deep learning span distinct architectural families, each with characteristic strengths. Foundational principles and taxonomy (13  Foundation Model Paradigm) establish what defines a foundation model and provide a framework for navigating the rapidly expanding ecosystem. DNA language models (14  DNA Language Models), including DNABERT, Nucleotide Transformer, and HyenaDNA, apply self-supervised pretraining to genomic sequence, learning representations that transfer across diverse downstream tasks. Protein language models (15  Protein Language Models) achieved the earliest and most dramatic foundation model successes; ESM, ProtTrans, and their descendants emerged alongside AlphaFold2 in 2020, collectively demonstrating that deep learning could capture protein structure and function from sequence alone. AlphaFold2 revolutionized structure prediction through its Evoformer architecture, and AlphaMissense subsequently adapted that architecture for proteome-wide variant effect prediction. Hybrid architectures (16  Regulatory Models), including Enformer, Borzoi, and AlphaGenome, combine convolutional processing with transformer blocks to achieve context windows spanning hundreds of kilobases, enabling direct prediction of gene expression from sequence. Variant effect prediction (17  Variant Effect Prediction) synthesizes these approaches, translating foundation model representations into pathogenicity scores across variant types and genomic contexts.\n\n\n\n\n\n\nConnections to Other Parts\n\n\n\n\nPart II provides the architectural foundations (CNNs, attention, pretraining) these models build upon\nPart IV extends foundation model principles to RNA, single-cell, 3D genome, and multi-omics\nPart V provides tools to evaluate what these models actually learn versus what they claim\nPart VI deploys these models in clinical and translational contexts",
    "crumbs": [
      "Part III: Foundation Model Families"
    ]
  },
  {
    "objectID": "part_4/p4--cellular-context.html",
    "href": "part_4/p4--cellular-context.html",
    "title": "Part IV: Cellular Context",
    "section": "",
    "text": "Part IV at a Glance\n\n\n\nCentral question: How do foundation model principles extend beyond one-dimensional sequence to embrace the full complexity of cellular and systems-level biology?\nPrerequisites: Part III (foundation model families). Part II (7  Transformers and Attention, 8  Pretraining Strategies) for architectural background.\n\n\n\n\n\n\n\n\nChapter\nTopic\nKey Concepts\n\n\n\n\n18  RNA Structure and Function\nRNA Models\nSecondary structure, splicing, RNA foundation models\n\n\n19  Single-Cell Models\nSingle-Cell Models\nSparsity, cell type annotation, perturbation prediction\n\n\n20  3D Genome Organization\n3D Genome\nChromatin loops, TADs, Hi-C prediction, spatial transcriptomics\n\n\n21  Graph and Network Models\nNetwork Models\nGNNs, PPI networks, regulatory networks, pathway integration\n\n\n22  Multi-Omics Integration\nMulti-Omics Integration\nCross-modal learning, genotype-to-phenotype paths\n\n\n\nAfter completing Part IV, you will understand:\n\nHow RNA structure adds a second dimension to sequence modeling\nWhy single-cell data requires different architectural adaptations\nHow 3D genome organization connects distal regulatory elements\nWhen graph neural networks complement sequence models\nHow to integrate multiple data modalities toward phenotype prediction\n\n\n\nBiology operates across scales that sequence alone cannot capture. Cells of different types read the same genome differently, activating distinct gene programs that produce neurons, hepatocytes, and immune cells from identical DNA. Genes function not in isolation but within networks of regulation and interaction, where perturbing one node propagates effects throughout the system. The three-dimensional folding of chromatin brings distal elements into contact, creating regulatory logic invisible to models that treat genomes as one-dimensional strings. Sequence foundation models ask what a genome encodes; the models in this part ask what that sequence becomes in particular cellular contexts, interaction networks, and spatial architectures.\nThis transition from sequence-centric to systems-scale modeling demands new data modalities and new computational approaches. Single-cell transcriptomics reveals the cellular heterogeneity that bulk measurements average over. Hi-C and related methods expose the spatial organization that determines which enhancers contact which promoters. Protein interaction networks and gene regulatory graphs capture relational structure that no amount of sequence analysis can infer. Foundation model principles extend naturally to these modalities: learn representations from large-scale data, then transfer to specific prediction tasks.\nRNA structure and function (18  RNA Structure and Function) extend sequence modeling beyond DNA, covering secondary structure prediction, splicing regulation, and the emerging frontier of RNA foundation models. Single-cell transcriptomics and epigenomics (19  Single-Cell Models) present distinct challenges of sparsity, noise, and scale that transformer architectures must adapt to capture. Three-dimensional genome organization (20  3D Genome Organization) adds spatial context, with models predicting chromatin contacts from sequence and connecting spatial structure to gene regulation. Graph neural networks (21  Graph and Network Models) represent biological entities and their interactions as structured graphs, integrating foundation model embeddings with relational reasoning. Multi-omics integration (22  Multi-Omics Integration) broadens the view further, jointly representing genomic, transcriptomic, proteomic, and clinical information to trace the path from genotype to phenotype.\n\n\n\n\n\n\nConnections to Other Parts\n\n\n\n\nPart III provides the sequence-level foundation models that feed into systems-scale approaches\nPart II (7  Transformers and Attention) introduces the attention mechanisms adapted for these new modalities\nPart V evaluation principles apply to multi-modal models with additional complexity\nPart VI clinical applications increasingly depend on systems-level integration",
    "crumbs": [
      "Part IV: Cellular Context"
    ]
  },
  {
    "objectID": "part_5/p5--responsible-deployment.html",
    "href": "part_5/p5--responsible-deployment.html",
    "title": "Part V: Evaluation and Trust",
    "section": "",
    "text": "Part V at a Glance\n\n\n\nCentral question: How do we know whether genomic foundation models work, what they’ve actually learned, and when we can trust their predictions?\nPrerequisites: Parts II-III for model context. This part is essential reading before Part VI (clinical applications).\n\n\n\n\n\n\n\n\nChapter\nTopic\nKey Concepts\n\n\n\n\n23  Uncertainty Quantification\nUncertainty Quantification\nCalibration, epistemic vs. aleatoric, ensembles, conformal prediction\n\n\n24  Interpretability\nInterpretability\nAttribution methods, motif analysis, mechanistic interpretability\n\n\n25  Causal Inference with Foundation Models\nCausal Inference\nCausal graphs, Mendelian randomization, foundation models for causality\n\n\n26.1 Regulatory Frameworks for Genomic AI\nEthics & Regulation\nBias, fairness, regulatory frameworks, responsible deployment\n\n\n\nAfter completing Part V, you will understand:\n\nWhen model confidence can be trusted and when it cannot\nHow to distinguish genuine biological insight from spurious pattern matching\nWhat causal claims foundation models can and cannot support\nWhat ethical and regulatory constraints govern clinical deployment\n\n\n\nEvaluating genomic models presents challenges that distinguish this domain from natural language processing or computer vision. Biological sequences contain evolutionary history: a model tested on homologous sequences may appear to generalize when it has merely memorized. Population structure creates spurious associations: a variant predictor may learn ancestry rather than pathogenicity. Nested functional hierarchies obscure what models actually capture: strong performance on common variants provides no guarantee of accuracy on the rare variants that drive most clinical decisions. Standard machine learning evaluation practices, developed for domains where training and test examples are approximately independent and identically distributed, become actively misleading when applied to genomic data without careful adaptation.\nRigorous evaluation determines whether genomic foundation models deliver on their promises. The benchmark and evaluation methodology chapters from Part II (11  Benchmarks and Evaluation, 12  Confounding and Data Leakage) established foundational principles; this part extends that framework to address deeper questions about model reliability.\nCalibration and uncertainty quantification (23  Uncertainty Quantification) determine whether model outputs can inform decisions or require careful reinterpretation. A model achieving high discrimination (auROC) may still provide dangerously miscalibrated probabilities that mislead clinical decisions. Moving beyond black-box prediction toward mechanistic understanding (24  Interpretability) requires distinguishing faithful explanations that accurately reflect model computation from plausible explanations that merely satisfy human intuition. Causal inference frameworks (25  Causal Inference with Foundation Models) clarify what kinds of causal claims foundation models can support and where correlation remains stubbornly distinct from causation. Ethical and regulatory considerations (26.1 Regulatory Frameworks for Genomic AI) establish the constraints that govern responsible deployment, from algorithmic fairness to regulatory approval pathways.\nThis critical toolkit enables rigorous evaluation of genomic AI claims and responsible deployment in research and clinical settings.\n\n\n\n\n\n\nEssential for Clinical Applications\n\n\n\nIf you plan to deploy genomic foundation models in clinical or high-stakes settings, Part V is not optional. The evaluation frameworks, uncertainty quantification methods, and ethical considerations developed here are prerequisites for responsible deployment. Part VI assumes familiarity with these concepts.\n\n\n\n\n\n\n\n\nConnections to Other Parts\n\n\n\n\nPart II (11  Benchmarks and Evaluation, 12  Confounding and Data Leakage) establishes foundational evaluation principles\nParts III-IV model chapters benefit from the critical lens developed here\nPart VI clinical applications require the uncertainty and interpretability tools from this part",
    "crumbs": [
      "Part V: Evaluation and Trust"
    ]
  },
  {
    "objectID": "part_6/p6--applications.html",
    "href": "part_6/p6--applications.html",
    "title": "Part VI: Clinical Translation",
    "section": "",
    "text": "Part VI at a Glance\n\n\n\nCentral question: How do we move from promising benchmark results to tools that support real decisions about patients, drug development, and biological design?\nPrerequisites: Parts III (foundation models) and V (evaluation and trust). Part I for data context.\n\n\n\n\n\n\n\n\nChapter\nTopic\nKey Applications\n\n\n\n\n27  Clinical Risk Prediction\nClinical Risk Prediction\nDisease risk stratification, EHR integration, fairness\n\n\n28  Rare Disease Diagnosis\nRare Disease & Cancer\nVariant prioritization, diagnostic pipelines, tumor analysis\n\n\n29  Drug Discovery\nDrug Discovery\nTarget identification, genetic validation, biomarker development\n\n\n30  Sequence Design\nBiological Design\nProtein engineering, regulatory design, synthetic biology\n\n\n31  Frontiers and Synthesis\nFrontiers\nEmerging directions, open problems, future outlook\n\n\n\nAfter completing Part VI, you will understand:\n\nWhat additional requirements clinical deployment imposes beyond research benchmarks\nHow to integrate foundation model predictions into diagnostic workflows\nWhere foundation models contribute to the drug discovery pipeline\nHow generative foundation models enable biological design\nWhat major challenges remain for the field\n\n\n\nThe question shifts from how these models work to how they are used, and from what they can predict to what they enable us to do. This transition is not merely practical but conceptual: deploying a model in a clinical or industrial setting exposes assumptions that benchmarks leave implicit and reveals failure modes that curated evaluations obscure. A model achieving impressive metrics on held-out test sets may falter when deployed on populations underrepresented in training data, when integrated into workflows designed around different assumptions, or when its outputs must be communicated to clinicians and patients who lack the technical background to interpret confidence intervals. The gap between benchmark performance and real-world utility represents one of the most consequential challenges in genomic AI.\nDeployment transforms the requirements for genomic foundation models. Clinical risk prediction, rare disease diagnosis, drug discovery, and biological design each impose constraints absent from research settings: calibration requirements become stricter, fairness considerations become urgent, interpretability demands become concrete, and the consequences of failure become measured in patient outcomes rather than leaderboard rankings.\nClinical risk prediction (27  Clinical Risk Prediction) combines foundation model features with electronic health records to stratify patients for disease, progression, and treatment response. Variant interpretation in rare disease (28  Rare Disease Diagnosis) enters diagnostic pipelines alongside clinical geneticists and laboratory scientists. Drug discovery applications (29  Drug Discovery) contribute to target identification, genetic validation, and biomarker development. Reversing the direction of inference from prediction to generation (30  Sequence Design), foundation models guide protein engineering, regulatory element design, and programmable biology. Emerging directions and open problems (31  Frontiers and Synthesis) will shape how genomic AI moves from research to practice.\nThe goal is not definitive protocols for each domain but a framework for reasoning about deployment: what questions to ask, what pitfalls to anticipate, and what principles should guide responsible development.\n\n\n\n\n\n\nThe Benchmark-Deployment Gap\n\n\n\nStrong benchmark performance does not guarantee clinical utility. Every chapter in this part addresses specific ways that models successful in research settings can fail in deployment:\n\nCalibration failures where confident predictions are systematically wrong\nPopulation shift where performance degrades for underrepresented groups\nIntegration challenges where model outputs don’t fit clinical workflows\nCommunication gaps where predictions cannot be explained to stakeholders\n\nPart V provides the tools to anticipate and address these failure modes.\n\n\n\n\n\n\n\n\nConnections to Other Parts\n\n\n\n\nPart I data foundations determine what populations and variants models can serve\nPart III foundation model capabilities determine what predictions are possible\nPart V evaluation and trust frameworks are prerequisites for responsible deployment",
    "crumbs": [
      "Part VI: Clinical Translation"
    ]
  }
]