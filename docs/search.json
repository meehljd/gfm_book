[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Genomic Foundation Models",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Ma et al. (2023) for additional discussion of literate programming.\n\n\n\n\nMa, Jiani, Jiangning Song, Neil D. Young, Bill C. H. Chang, Pasi K. Korhonen, Tulio L. Campos, Hui Liu, and Robin B. Gasser. 2023. “’Bingo’-a Large Language Model- and Graph Neural Network-Based Workflow for the Prediction of Essential Genes from Protein Data.” Briefings in Bioinformatics 25 (1): bbad472. https://doi.org/10.1093/bib/bbad472.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "ch01.html",
    "href": "ch01.html",
    "title": "1  Next-Generation Sequencing (NGS) and High-Fidelity Variant Calling",
    "section": "",
    "text": "1.1 The Challenge of NGS Data",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Next-Generation Sequencing (NGS) and High-Fidelity Variant Calling</span>"
    ]
  },
  {
    "objectID": "ch02.html",
    "href": "ch02.html",
    "title": "2  Establishing Genome-Wide Deleteriousness Scores",
    "section": "",
    "text": "2.1 The Need for Universal Scoring",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Establishing Genome-Wide Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "ch03.html",
    "href": "ch03.html",
    "title": "3  Foundational Functional Genomics Data",
    "section": "",
    "text": "3.1 Data Sources",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Foundational Functional Genomics Data</span>"
    ]
  },
  {
    "objectID": "ch04.html",
    "href": "ch04.html",
    "title": "4  DeepSEA and Ab Initio Regulatory Prediction",
    "section": "",
    "text": "4.1 The DeepSEA Methodology (Zhou & Troyanskaya, 2015)",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DeepSEA and Ab Initio Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "ch05.html",
    "href": "ch05.html",
    "title": "5  Predicting Transcriptional Effects (ExPecto)",
    "section": "",
    "text": "5.1 Extending DeepSEA (ExPecto, Zhou et al., 2018)",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Predicting Transcriptional Effects (ExPecto)</span>"
    ]
  },
  {
    "objectID": "ch06.html",
    "href": "ch06.html",
    "title": "6  Specialized Prediction – Splicing (SpliceAI)",
    "section": "",
    "text": "6.1 The Importance of Splicing",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Specialized Prediction – Splicing (SpliceAI)</span>"
    ]
  },
  {
    "objectID": "ch07.html",
    "href": "ch07.html",
    "title": "7  Architectural Shift to Attention and Foundation Models",
    "section": "",
    "text": "7.1 The Transformer Core",
    "crumbs": [
      "Part III: The Age of Transformers and Long-Range Context (Hybrid Era)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Architectural Shift to Attention and Foundation Models</span>"
    ]
  },
  {
    "objectID": "ch08.html",
    "href": "ch08.html",
    "title": "8  Integrating Long-Range Interactions for Expression Prediction",
    "section": "",
    "text": "8.1 Enformer (Avsec et al., 2021)",
    "crumbs": [
      "Part III: Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Integrating Long-Range Interactions for Expression Prediction</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ma, Jiani, Jiangning Song, Neil D. Young, Bill C. H. Chang, Pasi K.\nKorhonen, Tulio L. Campos, Hui Liu, and Robin B. Gasser. 2023.\n“’Bingo’-a Large Language Model- and Graph Neural\nNetwork-Based Workflow for the Prediction of Essential Genes from\nProtein Data.” Briefings in Bioinformatics 25 (1):\nbbad472. https://doi.org/10.1093/bib/bbad472.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "ch02.html#the-need-for-universal-scoring",
    "href": "ch02.html#the-need-for-universal-scoring",
    "title": "2  Establishing Genome-Wide Deleteriousness Scores",
    "section": "",
    "text": "Genetic variant interpretation, crucial for personalized medicine, requires estimating the functional impact of variations throughout the human genome.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Establishing Genome-Wide Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "ch02.html#combined-annotation-dependent-depletion-cadd",
    "href": "ch02.html#combined-annotation-dependent-depletion-cadd",
    "title": "2  Establishing Genome-Wide Deleteriousness Scores",
    "section": "2.2 Combined Annotation-Dependent Depletion (CADD)",
    "text": "2.2 Combined Annotation-Dependent Depletion (CADD)\n\nPrinciple:\n\nCADD is a machine-learning–based scoring system (logistic regression model).\nIntegrates hundreds of genomic annotations to assign a single deleteriousness score to SNVs and short InDels.\n\nTraining Objective:\n\nLearns to contrast a set of “proxy-neutral” variants (those observed in humans, assumed harmless) against a set of “proxy-deleterious” variants (simulated variants that have not been filtered by selection).\n\nEvolution (CADD v1.7):\n\nRecent iterations integrate features derived from advanced models, including:\n\nProtein language model scores (Meta ESM-1v).\nRegulatory variant effect predictions from sequence-based CNNs.\n\nDemonstrates the flexible and modular nature of the CADD framework.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Establishing Genome-Wide Deleteriousness Scores</span>"
    ]
  },
  {
    "objectID": "ch03.html#data-sources",
    "href": "ch03.html#data-sources",
    "title": "3  Foundational Functional Genomics Data",
    "section": "",
    "text": "The ENCODE and Roadmap Epigenomics projects are fundamental public data resources providing annotations for functional regulatory elements, such as:\n\nHistone marks (HMs)\nTranscription factor binding sites (TFBSs)\nDNA accessibility (DNase I hypersensitive sites)",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Foundational Functional Genomics Data</span>"
    ]
  },
  {
    "objectID": "ch03.html#role-in-model-training",
    "href": "ch03.html#role-in-model-training",
    "title": "3  Foundational Functional Genomics Data",
    "section": "3.2 Role in Model Training",
    "text": "3.2 Role in Model Training\n\nThese data serve as the crucial molecular phenotype targets used to train supervised Sequence-to-Function models (like DeepSEA and its successors) that interpret non-coding sequence variation.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Foundational Functional Genomics Data</span>"
    ]
  },
  {
    "objectID": "ch09.html",
    "href": "ch09.html",
    "title": "9  State-of-the-Art in VEP – Missense, Multi-Modality, and Conservation",
    "section": "",
    "text": "9.1 AlphaMissense (Cheng et al., 2023)",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>State-of-the-Art in VEP – Missense, Multi-Modality, and Conservation</span>"
    ]
  },
  {
    "objectID": "ch10.html",
    "href": "ch10.html",
    "title": "10  Scaling Sequence Context Beyond Attention",
    "section": "",
    "text": "10.1 The Quadratic Bottleneck",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Scaling Sequence Context Beyond Attention</span>"
    ]
  },
  {
    "objectID": "ch11.html",
    "href": "ch11.html",
    "title": "11  Integrating Multi-omics and Systems Context",
    "section": "",
    "text": "11.1 Epigenomic Foundation Models (CpGPT)",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrating Multi-omics and Systems Context</span>"
    ]
  },
  {
    "objectID": "ch11.html#epigenomic-foundation-models-cpgpt",
    "href": "ch11.html#epigenomic-foundation-models-cpgpt",
    "title": "11  Integrating Multi-omics and Systems Context",
    "section": "",
    "text": "CpGPT (Cytosine-phosphate-Guanine Pretrained Transformer):\n\nSpecialized foundation model for DNA methylation.\nLeverages embeddings from nucleotide LMs (like NTv2) and positional context.\nTrained on the massive CpGCorpus dataset (&gt;150,000 samples).\nAchieves SOTA performance in clinically relevant tasks:\n\nMortality risk assessment.\nCancer classification.",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrating Multi-omics and Systems Context</span>"
    ]
  },
  {
    "objectID": "ch11.html#graph-neural-networks-gnns-for-subtyping",
    "href": "ch11.html#graph-neural-networks-gnns-for-subtyping",
    "title": "11  Integrating Multi-omics and Systems Context",
    "section": "11.2 Graph Neural Networks (GNNs) for Subtyping",
    "text": "11.2 Graph Neural Networks (GNNs) for Subtyping\n\nGNNs are powerful for modeling non-linear biological relationships and integrating heterogeneous data.\n\nExamples:\n\nMoGCN (Multi-Omics Graph Convolutional Network):\n\nDesigned for interpretable cancer subtype analysis.\nUses GCN to extract significant features from multi-omics data for clinical diagnosis.\n\nCGMega:\n\nExplainable GNN framework using graph attention.\nIntegrates multi-omics data:\n\nGenomics\nEpigenomics\n3D genome architecture\nProtein–protein interactions (PPIs)\n\nIdentifies influential cancer gene modules.",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrating Multi-omics and Systems Context</span>"
    ]
  },
  {
    "objectID": "ch10.html#the-quadratic-bottleneck",
    "href": "ch10.html#the-quadratic-bottleneck",
    "title": "10  Scaling Sequence Context Beyond Attention",
    "section": "",
    "text": "Standard Transformer attention scales quadratically (O(L²)) with sequence length (L).\nLimits input context to around 4k tokens, which is insufficient for long-range genomic regulation.",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Scaling Sequence Context Beyond Attention</span>"
    ]
  },
  {
    "objectID": "ch10.html#alternatives-ssms-and-hyena",
    "href": "ch10.html#alternatives-ssms-and-hyena",
    "title": "10  Scaling Sequence Context Beyond Attention",
    "section": "10.2 Alternatives (SSMs and Hyena)",
    "text": "10.2 Alternatives (SSMs and Hyena)\n\nNew architectures leverage:\n\nState Space Models (SSMs)\nHierarchical convolutions\n\nAchieve quasi-linear scaling, enabling contexts up to 1 million nucleotides.\n\nExamples:\n\nHyenaDNA:\n\nDecoder-only model optimized for runtime scalability.\nHandles ultra-long genomic sequences.\n\nCaduceus (MambaDNA):\n\nBuilt on the Mamba SSM block.\nDesigned explicitly for genomics, incorporating:\n\nBi-directionality.\nReverse Complement (RC) equivariance.\n\nOutperforms 10× larger Transformer models on long-range VEP tasks.",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Scaling Sequence Context Beyond Attention</span>"
    ]
  },
  {
    "objectID": "ch09.html#genomic-language-models-leveraging-evolution-gpn-msa-evo-2",
    "href": "ch09.html#genomic-language-models-leveraging-evolution-gpn-msa-evo-2",
    "title": "9  State-of-the-Art in VEP – Missense, Multi-Modality, and Conservation",
    "section": "9.2 Genomic Language Models Leveraging Evolution (GPN-MSA, Evo 2)",
    "text": "9.2 Genomic Language Models Leveraging Evolution (GPN-MSA, Evo 2)\n\nGPN-MSA:\n\nIntegrates a Multiple Sequence Alignment (MSA) across diverse vertebrate species into a Transformer architecture.\nEvolutionary information enables it to outperform:\n\nTraditional CADD.\nFunctional models like Enformer / SpliceAI in genome-wide deleteriousness prediction (coding and non-coding variants).\n\n\nEvo 2:\n\nGenome-scale language model.\nStrong zero-shot performance across various mutation types:\n\nCoding, non-coding, non-SNV, splice-associated.\n\nSet a new SOTA for BRCA1 non-coding SNVs.\nPerformed strongly on splice variant prediction using SpliceVarDB.",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>State-of-the-Art in VEP – Missense, Multi-Modality, and Conservation</span>"
    ]
  },
  {
    "objectID": "ch09.html#alphagenome-avsec-et-al.-2025",
    "href": "ch09.html#alphagenome-avsec-et-al.-2025",
    "title": "9  State-of-the-Art in VEP – Missense, Multi-Modality, and Conservation",
    "section": "9.3 AlphaGenome (Avsec et al., 2025)",
    "text": "9.3 AlphaGenome (Avsec et al., 2025)\n\nUnification and Scale:\n\nDeep learning system designed to unify predictions across thousands of tracks:\n\nGene expression\nSplicing\nChromatin accessibility\n3D contact maps\n\nUses a massive 1 Mb input sequence at base-pair resolution.\n\nSOTA Performance:\n\nMatches or exceeds specialized external models on 24 out of 26 VEP evaluations.\nExcels in multi-faceted splicing prediction:\n\nSplice site probability\nUsage",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>State-of-the-Art in VEP – Missense, Multi-Modality, and Conservation</span>"
    ]
  },
  {
    "objectID": "ch09.html#alphamissense-cheng-et-al.-2023",
    "href": "ch09.html#alphamissense-cheng-et-al.-2023",
    "title": "9  State-of-the-Art in VEP – Missense, Multi-Modality, and Conservation",
    "section": "",
    "text": "Method:\n\nRepresents the SOTA for missense VEP.\nLeverages unsupervised protein language modeling.\nIncorporates structural context derived from the AlphaFold system.\nFine-tuned on weak labels from population frequency data (avoiding human-curated annotation bias).\n\nCoverage:\n\nProvides predictions for all possible single amino acid substitutions in the human proteome.\nConfidently classifies:\n\n32% as likely pathogenic.\n57% as likely benign.",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>State-of-the-Art in VEP – Missense, Multi-Modality, and Conservation</span>"
    ]
  },
  {
    "objectID": "ch08.html#borzoi-linder-et-al.-2025",
    "href": "ch08.html#borzoi-linder-et-al.-2025",
    "title": "8  Integrating Long-Range Interactions for Expression Prediction",
    "section": "8.2 Borzoi (Linder et al., 2025)",
    "text": "8.2 Borzoi (Linder et al., 2025)\n\nUnifying Regulatory Layers:\n\nBased on the Enformer architecture.\nTrained to predict cell-type–specific and tissue-specific RNA-seq coverage directly from DNA sequence.\n\nMultilayer Scoring:\n\nBy predicting coverage, Borzoi allows variant effects to be scored across multiple regulatory layers:\n\nTranscription\nSplicing\nPolyadenylation\n\nOften outperforms state-of-the-art specialized models trained on individual functions.\nBorzoi-derived variant scores, combined with CADD scores, show equal power in discriminating between common benign variants and rare singletons in regulatory elements.",
    "crumbs": [
      "Part III: Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Integrating Long-Range Interactions for Expression Prediction</span>"
    ]
  },
  {
    "objectID": "ch08.html#enformer-avsec-et-al.-2021",
    "href": "ch08.html#enformer-avsec-et-al.-2021",
    "title": "8  Integrating Long-Range Interactions for Expression Prediction",
    "section": "",
    "text": "Architecture:\n\nHybrid CNN–Transformer model.\nExtends receptive field up to 100 kb (vs. 20 kb for predecessors like ExPecto).\nUses attention layers to refine predictions by gathering information from distal elements like enhancers.\n\nClinical Relevance:\n\nLarger context size is crucial because distal elements drive tissue-specific gene expression.\nSignificantly improved accuracy of non-coding Variant Effect Prediction (VEP) on eQTL data.\nEnables signed prediction of mutation effects (activating vs. repressive).",
    "crumbs": [
      "Part III: Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Integrating Long-Range Interactions for Expression Prediction</span>"
    ]
  },
  {
    "objectID": "ch07.html#genomic-language-models-glms",
    "href": "ch07.html#genomic-language-models-glms",
    "title": "7  Architectural Shift to Attention and Foundation Models",
    "section": "7.2 Genomic Language Models (gLMs)",
    "text": "7.2 Genomic Language Models (gLMs)\n\nApply the principles of language models (like BERT, using the Masked Language Modeling objective) to DNA sequences.\nPre-trained on massive amounts of unlabeled data to learn complex features and dependencies.\n\nExamples:\n\nDNABERT\n\nOne of the earliest examples, applying BERT with k-mer tokenization to the human genome.\n\nNucleotide Transformer (NT)\n\nA family of encoder-only transformer foundation models (up to 2.5B parameters).\n\nPretrained on diverse human and multispecies genomes.\n\nEmbeddings and derived scores have been used to prioritize functional variants such as eQTLs and meQTLs.",
    "crumbs": [
      "Part III: The Age of Transformers and Long-Range Context (Hybrid Era)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Architectural Shift to Attention and Foundation Models</span>"
    ]
  },
  {
    "objectID": "ch07.html#the-transformer-core",
    "href": "ch07.html#the-transformer-core",
    "title": "7  Architectural Shift to Attention and Foundation Models",
    "section": "",
    "text": "Transformer architecture (from NLP) uses self-attention to capture long-range dependencies efficiently.\nContrasts with the local receptive fields of traditional CNNs.",
    "crumbs": [
      "Part III: The Age of Transformers and Long-Range Context (Hybrid Era)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Architectural Shift to Attention and Foundation Models</span>"
    ]
  },
  {
    "objectID": "ch11.html#interpreting-rare-and-complex-variants",
    "href": "ch11.html#interpreting-rare-and-complex-variants",
    "title": "11  Integrating Multi-omics and Systems Context",
    "section": "11.3 11.3 Interpreting Rare and Complex Variants",
    "text": "11.3 11.3 Interpreting Rare and Complex Variants\n\nDeepRVAT (Deep Rare Variant Association Testing):\n\nUses a deep set network to learn a single, trait-agnostic gene impairment score from dozens of rare variant annotations.\nScore is efficient and generalizable across traits.\nFacilitates refinement of Polygenic Risk Scores (PRS) by accounting for rare variant effects.\n\nNeEDL (Epistasis):\n\nUses network medicine to inform selection of higher-order interactions (Epistatic Interactions, EIs) between multiple SNPs.\nGoes beyond pair-wise limits.\nAddresses the polygenic nature of most heritable diseases.\n\nG2PT (Genotype-Phenotype Transformer):\n\nGraph transformer designed for transparent genotype-to-phenotype translation.\nIncorporates hierarchical biological knowledge.\nEnables analysis of nonlinear gene-by-gene epistatic interactions underlying polygenic risk prediction.",
    "crumbs": [
      "Part IV: Case Studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrating Multi-omics and Systems Context</span>"
    ]
  },
  {
    "objectID": "ch04.html#the-deepsea-methodology-zhou-troyanskaya-2015",
    "href": "ch04.html#the-deepsea-methodology-zhou-troyanskaya-2015",
    "title": "4  DeepSEA and Ab Initio Regulatory Prediction",
    "section": "",
    "text": "Introduced the fundamental application of deep convolutional neural networks (CNNs) to predict the functional effects of non-coding variants directly from sequence input.",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DeepSEA and Ab Initio Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "ch04.html#training-scale",
    "href": "ch04.html#training-scale",
    "title": "4  DeepSEA and Ab Initio Regulatory Prediction",
    "section": "4.2 Training Scale",
    "text": "4.2 Training Scale\n\nTrained to predict a large number of chromatin profiles (e.g., 919 tracks).",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DeepSEA and Ab Initio Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "ch04.html#significance",
    "href": "ch04.html#significance",
    "title": "4  DeepSEA and Ab Initio Regulatory Prediction",
    "section": "4.3 Significance",
    "text": "4.3 Significance\n\nEstablished the viability of using CNNs to implicitly learn the underlying regulatory code or “syntax” of the genome by recognizing local sequence patterns and motifs.",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DeepSEA and Ab Initio Regulatory Prediction</span>"
    ]
  },
  {
    "objectID": "ch05.html#extending-deepsea-expecto-zhou-et-al.-2018",
    "href": "ch05.html#extending-deepsea-expecto-zhou-et-al.-2018",
    "title": "5  Predicting Transcriptional Effects (ExPecto)",
    "section": "",
    "text": "ExPecto is a modular framework built on a refined DeepSEA architecture.\nDesigned to predict the tissue-specific transcriptional effects of mutations ab initio (from sequence only), without having trained on any variant information.",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Predicting Transcriptional Effects (ExPecto)</span>"
    ]
  },
  {
    "objectID": "ch05.html#architecture-components",
    "href": "ch05.html#architecture-components",
    "title": "5  Predicting Transcriptional Effects (ExPecto)",
    "section": "5.2 Architecture Components",
    "text": "5.2 Architecture Components\n\nEpigenomic Effects Model\n\nA deep CNN predicts probabilities for 2,002 histone mark, TF, and DNA accessibility profiles across over 200 tissues/cell types.\n\nSpatial Feature Transformation\n\nSummarizes the predicted epigenomic information across a 40 kb region.\n\nTissue-Specific Prediction\n\nRegularized linear models use the transformed features to predict Pol II–transcribed gene expression in 218 tissues/cell types.",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Predicting Transcriptional Effects (ExPecto)</span>"
    ]
  },
  {
    "objectID": "ch06.html#the-importance-of-splicing",
    "href": "ch06.html#the-importance-of-splicing",
    "title": "6  Specialized Prediction – Splicing (SpliceAI)",
    "section": "",
    "text": "Aberrant splicing is a major cause of genetic disorders.\nSpecialized tools are necessary to interpret variants affecting this process.",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Specialized Prediction – Splicing (SpliceAI)</span>"
    ]
  },
  {
    "objectID": "ch06.html#spliceai-jaganathan-et-al.-2019",
    "href": "ch06.html#spliceai-jaganathan-et-al.-2019",
    "title": "6  Specialized Prediction – Splicing (SpliceAI)",
    "section": "6.2 SpliceAI (Jaganathan et al., 2019)",
    "text": "6.2 SpliceAI (Jaganathan et al., 2019)\n\nUses an ultra-deep residual neural network architecture.\nPredicts splice junctions based on a large flanking context sequence (up to 10,000 nucleotides).",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Specialized Prediction – Splicing (SpliceAI)</span>"
    ]
  },
  {
    "objectID": "ch06.html#clinical-impact",
    "href": "ch06.html#clinical-impact",
    "title": "6  Specialized Prediction – Splicing (SpliceAI)",
    "section": "6.3 Clinical Impact",
    "text": "6.3 Clinical Impact\n\nAccurately predicts noncoding cryptic splice mutations.\nApprox. 9%–11% of pathogenic mutations in patients with rare genetic disorders are caused by this class of variation.\nThe network is trained only on reference sequences; predicting variant effects by comparing reference and alternate sequences (Δ score) is a robust test of its splicing determinant modeling ability.",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Specialized Prediction – Splicing (SpliceAI)</span>"
    ]
  },
  {
    "objectID": "ch05.html#clinical-utility",
    "href": "ch05.html#clinical-utility",
    "title": "5  Predicting Transcriptional Effects (ExPecto)",
    "section": "5.3 Clinical Utility",
    "text": "5.3 Clinical Utility\n\nProvides an end-to-end computational framework for in silico prediction of disease-associated regulatory variation.\nDemonstrates potential for interpreting clinically-relevant mutations, including those missed by traditional quantitative genetics.",
    "crumbs": [
      "Part II: Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Predicting Transcriptional Effects (ExPecto)</span>"
    ]
  },
  {
    "objectID": "ch01.html#the-challenge-of-ngs-data",
    "href": "ch01.html#the-challenge-of-ngs-data",
    "title": "1  Next-Generation Sequencing (NGS) and High-Fidelity Variant Calling",
    "section": "",
    "text": "NGS generates billions of error-prone, short reads, making the accurate identification of genetic variants challenging.\n\nTraditional methods like GATK rely on hand-crafted statistical models (e.g., hidden Markov models, logistic regression) that are difficult to generalize across different sequencing technologies.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Next-Generation Sequencing (NGS) and High-Fidelity Variant Calling</span>"
    ]
  },
  {
    "objectID": "ch01.html#deepvariant-deep-learning-for-variant-calling-poplin-et-al.-2018",
    "href": "ch01.html#deepvariant-deep-learning-for-variant-calling-poplin-et-al.-2018",
    "title": "1  Next-Generation Sequencing (NGS) and High-Fidelity Variant Calling",
    "section": "1.2 DeepVariant: Deep Learning for Variant Calling (Poplin et al., 2018)",
    "text": "1.2 DeepVariant: Deep Learning for Variant Calling (Poplin et al., 2018)\n\nConcept: DeepVariant replaces the assortment of statistical modeling components with a single deep learning model.\nArchitecture:\n\nUses a deep Convolutional Neural Network (CNN), such as the Inception architecture.\nProcesses read pileups encoded as images around candidate variants.\nThe CNN learns complex dependencies among reads, approximating the true but unknown interdependent likelihood function.\n\nClinical Impact:\n\nDeepVariant demonstrated higher accuracy than state-of-the-art tools like GATK.\nIn clinical Whole-Exome Sequencing (WES) trio analysis, DeepVariant yielded a significantly lower Mendelian error rate and shorter execution time compared to GATK HaplotypeCaller, suggesting proportionally more true positive calls.",
    "crumbs": [
      "Part I: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Next-Generation Sequencing (NGS) and High-Fidelity Variant Calling</span>"
    ]
  },
  {
    "objectID": "ch07.html#test-genomic-language-models-glms",
    "href": "ch07.html#test-genomic-language-models-glms",
    "title": "7  Architectural Shift to Attention and Foundation Models",
    "section": "7.2 TEST Genomic Language Models (gLMs)",
    "text": "7.2 TEST Genomic Language Models (gLMs)\n\nApply the principles of language models (like BERT, using the Masked Language Modeling objective) to DNA sequences.\nPre-trained on massive amounts of unlabeled data to learn complex features and dependencies.\n\nExamples:\n\nDNABERT\n\nOne of the earliest examples, applying BERT with k-mer tokenization to the human genome.\n\nNucleotide Transformer (NT)\n\nA family of encoder-only transformer foundation models (up to 2.5B parameters).\n\nPretrained on diverse human and multispecies genomes.\n\nEmbeddings and derived scores have been used to prioritize functional variants such as eQTLs and meQTLs.",
    "crumbs": [
      "Part III: Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Architectural Shift to Attention and Foundation Models</span>"
    ]
  }
]