<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>25&nbsp; Interpretability – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_6/p6-ch26-causal.html" rel="next">
<link href="../part_6/p6-ch24-uncertainty.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_6/p6--responsible-deployment.html">Part VI: Responsible Deployment</a></li><li class="breadcrumb-item"><a href="../part_6/p6-ch25-interpretability.html"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretability</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Learning &amp; Evaluation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmark Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch12-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Evaluation Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--fm-families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch14-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch15-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch16-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch17-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--cellular-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Cellular Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch19-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch20-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch21-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch22-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Responsible Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch24-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch25-interpretability.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch26-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Causality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_7/p7--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VII: Applications &amp; Frontiers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch28-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch29-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch30-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch31-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch32-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch25-attribution" id="toc-sec-ch25-attribution" class="nav-link active" data-scroll-target="#sec-ch25-attribution"><span class="header-section-number">25.1</span> Attribution Methods and Input Importance</a>
  <ul class="collapse">
  <li><a href="#sec-ch25-ism" id="toc-sec-ch25-ism" class="nav-link" data-scroll-target="#sec-ch25-ism"><span class="header-section-number">25.1.1</span> <em>In Silico</em> Mutagenesis</a></li>
  <li><a href="#sec-ch25-gradient" id="toc-sec-ch25-gradient" class="nav-link" data-scroll-target="#sec-ch25-gradient"><span class="header-section-number">25.1.2</span> Gradient-Based Attribution</a></li>
  <li><a href="#sec-ch25-reconciling" id="toc-sec-ch25-reconciling" class="nav-link" data-scroll-target="#sec-ch25-reconciling"><span class="header-section-number">25.1.3</span> Reconciling Attribution Methods</a></li>
  </ul></li>
  <li><a href="#sec-ch25-cnn-filters" id="toc-sec-ch25-cnn-filters" class="nav-link" data-scroll-target="#sec-ch25-cnn-filters"><span class="header-section-number">25.2</span> Interpreting Convolutional Filters</a>
  <ul class="collapse">
  <li><a href="#sec-ch25-filter-pwm" id="toc-sec-ch25-filter-pwm" class="nav-link" data-scroll-target="#sec-ch25-filter-pwm"><span class="header-section-number">25.2.1</span> From Filters to Position Weight Matrices</a></li>
  <li><a href="#sec-ch25-deeper-layers" id="toc-sec-ch25-deeper-layers" class="nav-link" data-scroll-target="#sec-ch25-deeper-layers"><span class="header-section-number">25.2.2</span> Deeper Layers and Combinatorial Patterns</a></li>
  </ul></li>
  <li><a href="#sec-ch25-motif-discovery" id="toc-sec-ch25-motif-discovery" class="nav-link" data-scroll-target="#sec-ch25-motif-discovery"><span class="header-section-number">25.3</span> Motif Discovery from Attributions</a></li>
  <li><a href="#sec-ch25-probing" id="toc-sec-ch25-probing" class="nav-link" data-scroll-target="#sec-ch25-probing"><span class="header-section-number">25.4</span> Probing Learned Representations</a>
  <ul class="collapse">
  <li><a href="#sec-ch25-probing-methods" id="toc-sec-ch25-probing-methods" class="nav-link" data-scroll-target="#sec-ch25-probing-methods"><span class="header-section-number">25.4.1</span> Probing Methodology</a></li>
  <li><a href="#sec-ch25-probing-limits" id="toc-sec-ch25-probing-limits" class="nav-link" data-scroll-target="#sec-ch25-probing-limits"><span class="header-section-number">25.4.2</span> Limitations of Probing</a></li>
  </ul></li>
  <li><a href="#sec-ch25-attention" id="toc-sec-ch25-attention" class="nav-link" data-scroll-target="#sec-ch25-attention"><span class="header-section-number">25.5</span> Attention Patterns in Transformer Models</a>
  <ul class="collapse">
  <li><a href="#sec-ch25-attention-patterns" id="toc-sec-ch25-attention-patterns" class="nav-link" data-scroll-target="#sec-ch25-attention-patterns"><span class="header-section-number">25.5.1</span> What Attention Patterns Reveal</a></li>
  <li><a href="#sec-ch25-attention-caution" id="toc-sec-ch25-attention-caution" class="nav-link" data-scroll-target="#sec-ch25-attention-caution"><span class="header-section-number">25.5.2</span> Why Attention Weights Mislead</a></li>
  </ul></li>
  <li><a href="#sec-ch25-global" id="toc-sec-ch25-global" class="nav-link" data-scroll-target="#sec-ch25-global"><span class="header-section-number">25.6</span> Regulatory Vocabularies and Global Interpretability</a>
  <ul class="collapse">
  <li><a href="#sec-ch25-sei" id="toc-sec-ch25-sei" class="nav-link" data-scroll-target="#sec-ch25-sei"><span class="header-section-number">25.6.1</span> Sequence Classes from <em>Sei</em></a></li>
  <li><a href="#sec-ch25-embedding-geometry" id="toc-sec-ch25-embedding-geometry" class="nav-link" data-scroll-target="#sec-ch25-embedding-geometry"><span class="header-section-number">25.6.2</span> Embedding Geometry and Regulatory Programs</a></li>
  </ul></li>
  <li><a href="#sec-ch25-mechanistic" id="toc-sec-ch25-mechanistic" class="nav-link" data-scroll-target="#sec-ch25-mechanistic"><span class="header-section-number">25.7</span> Mechanistic Interpretability</a>
  <ul class="collapse">
  <li><a href="#sec-ch25-circuits" id="toc-sec-ch25-circuits" class="nav-link" data-scroll-target="#sec-ch25-circuits"><span class="header-section-number">25.7.1</span> Circuits and Features</a></li>
  <li><a href="#sec-ch25-mechanistic-genomics" id="toc-sec-ch25-mechanistic-genomics" class="nav-link" data-scroll-target="#sec-ch25-mechanistic-genomics"><span class="header-section-number">25.7.2</span> Applications to Genomic Models</a></li>
  </ul></li>
  <li><a href="#sec-ch25-validation" id="toc-sec-ch25-validation" class="nav-link" data-scroll-target="#sec-ch25-validation"><span class="header-section-number">25.8</span> Validation: From Explanations to Experiments</a>
  <ul class="collapse">
  <li><a href="#sec-ch25-faithfulness" id="toc-sec-ch25-faithfulness" class="nav-link" data-scroll-target="#sec-ch25-faithfulness"><span class="header-section-number">25.8.1</span> Faithfulness Testing</a></li>
  <li><a href="#sec-ch25-experimental" id="toc-sec-ch25-experimental" class="nav-link" data-scroll-target="#sec-ch25-experimental"><span class="header-section-number">25.8.2</span> Experimental Validation</a></li>
  </ul></li>
  <li><a href="#sec-ch25-clinical" id="toc-sec-ch25-clinical" class="nav-link" data-scroll-target="#sec-ch25-clinical"><span class="header-section-number">25.9</span> Interpretability in Clinical Variant Assessment</a></li>
  <li><a href="#sec-ch25-practical" id="toc-sec-ch25-practical" class="nav-link" data-scroll-target="#sec-ch25-practical"><span class="header-section-number">25.10</span> Practical Approaches for Foundation Model Analysis</a></li>
  <li><a href="#sec-ch25-conclusion" id="toc-sec-ch25-conclusion" class="nav-link" data-scroll-target="#sec-ch25-conclusion"><span class="header-section-number">25.11</span> Plausibility Is Not Faithfulness</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_6/p6--responsible-deployment.html">Part VI: Responsible Deployment</a></li><li class="breadcrumb-item"><a href="../part_6/p6-ch25-interpretability.html"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretability</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch25-interpretability" class="quarto-section-identifier"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretability</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>The model found something. But did it find what we think it found?</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Prerequisites:</strong> This chapter builds on convolutional neural network architectures (<a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>), attention mechanisms (<a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>), DNA language models (<a href="../part_4/p4-ch15-dna-lm.html" class="quarto-xref"><span>Chapter 15</span></a>), and protein language models (<a href="../part_4/p4-ch16-protein-lm.html" class="quarto-xref"><span>Chapter 16</span></a>). Familiarity with gradient-based optimization and basic linear algebra will help with the mathematical sections.</p>
<p><strong>Learning Objectives:</strong> After completing this chapter, you should be able to:</p>
<ol type="1">
<li>Distinguish between <strong>plausible</strong> and <strong>faithful</strong> model explanations, and explain why this distinction is critical for scientific discovery</li>
<li>Apply and compare attribution methods (ISM, gradient-based, integrated gradients) to identify important input positions</li>
<li>Explain how <em>TF-MoDISco</em> discovers motifs from attribution scores and why this approach is superior to traditional motif finding for model interpretation</li>
<li>Critically evaluate attention weight visualizations, recognizing when they accurately reflect model computation versus when they mislead</li>
<li>Design validation experiments that test whether interpretability-derived hypotheses are necessary and sufficient for model predictions</li>
<li>Articulate how interpretability enables (or limits) the use of computational evidence in clinical variant assessment</li>
</ol>
<p><strong>Estimated Time:</strong> 45-60 minutes</p>
</div>
</div>
<p>An attribution method highlights a GATA motif when explaining why a model predicts enhancer activity. The explanation is biologically plausible: GATA transcription factors bind this motif and drive tissue-specific expression. But plausibility is not faithfulness. The model may have learned a completely different pattern (perhaps GC content correlating with enhancer labels in the training data) and the attribution method may be highlighting the GATA motif because human-interpretable explanations tend to find human-interpretable patterns. The explanation matches biological intuition without accurately reflecting model computation. This distinction between plausible and faithful interpretation structures the entire field of model interpretability, and failing to respect it produces explanations that provide false comfort rather than genuine insight.</p>
<p>The stakes extend beyond scientific curiosity. Variant interpretation guidelines from the American College of Medical Genetics require that computational evidence be weighed alongside functional assays, segregation data, and population frequency (see <a href="../part_7/p7-ch29-rare-disease.html" class="quarto-xref"><span>Chapter 29</span></a> for detailed discussion of the ACMG-AMP framework). A pathogenicity score alone satisfies only weak evidence criteria; knowing that a variant disrupts a specific CTCF binding site in a cardiac enhancer provides interpretable mechanistic evidence that can be combined with clinical presentation and family history. When models cannot explain their predictions faithfully, clinicians cannot integrate computational evidence with biological reasoning. The same limitation affects research: a model that predicts enhancer activity cannot generate testable hypotheses about regulatory grammar unless its internal computations can be translated into statements about motifs, spacing constraints, and combinatorial logic that can be experimentally validated.</p>
<p><strong>Attribution methods</strong> identify important input positions. Motif discovery algorithms translate attributions into regulatory vocabularies. <strong>Probing classifiers</strong> diagnose what representations encode. <strong>Mechanistic interpretability</strong> traces computational circuits within transformer architectures. Throughout, the plausible-versus-faithful distinction guides interpretation. We examine how to validate interpretability claims experimentally, distinguishing explanations that accurately reflect model computation from those that merely satisfy human intuition. Understanding when these diverge determines whether model explanations accelerate discovery or mislead researchers pursuing patterns the model never actually learned.</p>
<section id="sec-ch25-attribution" class="level2" data-number="25.1">
<h2 data-number="25.1" class="anchored" data-anchor-id="sec-ch25-attribution"><span class="header-section-number">25.1</span> Attribution Methods and Input Importance</h2>
<p>When a model predicts that a 200-kilobase genomic region will show high chromatin accessibility in hepatocytes, a natural question arises: which bases within that region drive the prediction? Attribution methods answer this question by assigning importance scores to input positions, identifying where the model focuses its computational attention. These scores can reveal candidate regulatory elements, highlight the sequence features underlying variant effects, and provide the raw material for downstream motif discovery.</p>
<div id="fig-attribution-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attribution-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/01-fig-attribution-comparison.svg" class="img-fluid figure-img"></p>
<figcaption>Attribution methods comparison on the same genomic sequence</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attribution-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25.1: Attribution methods comparison on a single genomic sequence. From top: Gradient × Input (fast but noisy), Integrated Gradients (principled with theoretical guarantees), DeepLIFT (reference-based attributions), attention weights (model attention ≠ importance), and in silico mutagenesis (ground truth but 3L forward passes). Regions where methods agree provide high-confidence importance; disagreement flags positions for closer investigation.
</figcaption>
</figure>
</div>
<section id="sec-ch25-ism" class="level3" data-number="25.1.1">
<h3 data-number="25.1.1" class="anchored" data-anchor-id="sec-ch25-ism"><span class="header-section-number">25.1.1</span> <em>In Silico</em> Mutagenesis</h3>
<p>The most direct approach to measuring input importance is simply to change each base and observe what happens to the prediction. <em>In silico</em> mutagenesis (ISM) systematically introduces mutations at every position, computing the difference between mutant and reference predictions. For a sequence of length <em>L</em>, ISM creates three mutant sequences at each position (substituting each non-reference nucleotide), yielding 3<em>L</em> forward passes through the model. The resulting mutation effect matrix captures how sensitive the prediction is to changes at each position and to each alternative base.</p>
<div id="fig-in-silico-mutagenesis" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-in-silico-mutagenesis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/02-A-fig-in-silico-mutagenesis.svg" class="img-fluid figure-img"></p>
<figcaption>Procedure: substitute each position to all alternatives</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/02-B-fig-in-silico-mutagenesis.svg" class="img-fluid figure-img"></p>
<figcaption>Visualization: position × mutation heatmap</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/02-C-fig-in-silico-mutagenesis.svg" class="img-fluid figure-img"></p>
<figcaption>Validation against deep mutational scanning</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/02-D-fig-in-silico-mutagenesis.svg" class="img-fluid figure-img"></p>
<figcaption>Mechanistic insights from ISM profiles</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-in-silico-mutagenesis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25.2: In silico mutagenesis (ISM). (A) Procedure: substitute each position to all alternatives, compute prediction difference. (B) Visualization: position × mutation heatmap with functional regions showing large effects. (C) Validation: ISM predictions correlate with experimental deep mutational scanning (r ≈ 0.6-0.8). (D) Mechanistic insights: ISM reveals binding site boundaries, position-specific tolerance, and allele-specific effects.
</figcaption>
</figure>
</div>
<p>ISM provides true <strong>counterfactual</strong> information rather than approximations. When ISM shows that mutating position 47 from A to G reduces the predicted accessibility by 0.3 log-fold, that is a direct observation about model behavior, not an estimate derived from gradients or attention weights. This directness makes ISM the gold standard for faithfulness: if ISM identifies a position as important, perturbing that position genuinely changes the output.</p>
<p>The limitation is computational cost. Scoring all single-nucleotide substitutions in a 200-kilobase input requires 600,000 forward passes, which becomes prohibitive for large models or genome-wide analysis. Practical applications often restrict ISM to targeted windows around variants of interest, using faster methods to identify candidate regions for detailed analysis. For variant effect prediction specifically, ISM reduces to comparing reference and alternative allele predictions, requiring only two forward passes per variant. This forms the computational basis for zero-shot variant scoring in foundation models (<a href="../part_4/p4-ch18-vep-fm.html#sec-ch18-zeroshot-supervised" class="quarto-xref"><span>Section 18.1.1</span></a>), where the difference between wild-type and mutant log-likelihoods directly measures predicted effect.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think: Attribution Method Tradeoffs
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading the next section on gradient-based methods, consider: if ISM provides the most faithful importance scores, why would we ever use anything else? What properties would an alternative method need to be useful in practice?</p>
<p><em>Hint: Think about computational cost, but also about what types of patterns each method can and cannot detect.</em></p>
</div>
</div>
</section>
<section id="sec-ch25-gradient" class="level3" data-number="25.1.2">
<h3 data-number="25.1.2" class="anchored" data-anchor-id="sec-ch25-gradient"><span class="header-section-number">25.1.2</span> Gradient-Based Attribution</h3>
<p>Gradient-based methods approximate the counterfactual information from ISM using backpropagation. The gradient of the output with respect to each input position measures how much an infinitesimal change at that position would affect the prediction. With one-hot encoded sequence, the gradient at each base indicates the sensitivity to substituting that nucleotide.</p>
<p>The simplest approach, often called <strong>saliency mapping</strong>, computes raw gradients and visualizes their magnitudes across the sequence. A common variant multiplies gradients by inputs (gradient <span class="math inline">\(\times\)</span> input), focusing on positions where the current nucleotide is both important and present. These methods require only a single backward pass, making them orders of magnitude faster than ISM.</p>
<p>Gradient-based methods suffer from saturation in regions where the model is already confident. If a strong motif drives the prediction into a saturated region of the output nonlinearity, small perturbations produce near-zero gradients even though the motif is functionally critical. <em>DeepLIFT</em> addresses this limitation by comparing activations between an input sequence and a reference, propagating differences through the network using custom rules that avoid gradient saturation. The resulting attributions satisfy a completeness property: contributions sum to the difference between input and reference predictions <span class="citation" data-cites="shrikumar_learning_2017">(<a href="../bib/references.html#ref-shrikumar_learning_2017" role="doc-biblioref">Shrikumar, Greenside, and Kundaje 2017</a>)</span>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: The Saturation Problem
</div>
</div>
<div class="callout-body-container callout-body">
<p>A critical limitation of gradient-based methods is saturation: when a model is highly confident in its prediction, gradients become very small even for positions that are functionally essential. This happens because gradients measure sensitivity to <em>infinitesimal</em> changes, but a saturated sigmoid or softmax barely changes regardless of the perturbation size. A GATA motif that drives 95% of the prediction might show near-zero gradients because the model is already “certain.” This is why ISM (which measures finite perturbation effects) often reveals importance that gradients miss.</p>
</div>
</div>
<p><strong>Integrated gradients</strong> provide theoretical grounding through the path integral of gradients along a linear interpolation from reference to input <span class="citation" data-cites="sundararajan_axiomatic_2017">(<a href="../bib/references.html#ref-sundararajan_axiomatic_2017" role="doc-biblioref">Sundararajan, Taly, and Yan 2017</a>)</span>:</p>
<p><span id="eq-25-01"><span class="math display">\[
\text{IG}_i(\mathbf{x}) = (x_i - x'_i) \int_{\alpha=0}^{1} \frac{\partial f(\mathbf{x}' + \alpha(\mathbf{x} - \mathbf{x}'))}{\partial x_i} \, d\alpha
\tag{25.1}\]</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{x}\)</span> is the input sequence</li>
<li><span class="math inline">\(\mathbf{x}'\)</span> is the reference sequence (e.g., shuffled or zero baseline)</li>
<li><span class="math inline">\(\alpha \in [0, 1]\)</span> interpolates between reference and input</li>
<li><span class="math inline">\(f(\cdot)\)</span> is the model’s prediction function</li>
<li>In practice, approximated by: <span class="math inline">\(\text{IG}_i \approx (x_i - x'_i) \cdot \frac{1}{m} \sum_{k=1}^{m} \frac{\partial f(\mathbf{x}' + \frac{k}{m}(\mathbf{x} - \mathbf{x}'))}{\partial x_i}\)</span> with <span class="math inline">\(m = 20\)</span>-<span class="math inline">\(50\)</span> steps</li>
</ul>
<p>This integral, approximated by summing gradients at discrete interpolation steps, satisfies sensitivity (any input that affects the output receives nonzero attribution) and implementation invariance (functionally equivalent networks produce identical attributions). Integrated gradients have become a standard choice for genomic models, balancing computational efficiency with theoretical guarantees.</p>
<p>All gradient-based methods require choosing a reference sequence, which substantially affects the resulting attributions. Common choices include dinucleotide-shuffled versions of the input (preserving local composition while disrupting motifs), average non-functional sequence, or simply zeros. The reference defines what counts as informative: attributions highlight features that differ from the reference and contribute to the prediction difference. A shuffled reference emphasizes motif content; a zero reference treats any sequence information as potentially important.</p>
<p>The following table summarizes the key properties of attribution methods to help guide method selection:</p>
<div id="tbl-attribution-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-attribution-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;25.1: Comparison of attribution methods for genomic sequence models. Faithfulness indicates how accurately the method reflects true model behavior; computational cost scales with sequence length <em>L</em>.
</figcaption>
<div aria-describedby="tbl-attribution-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 23%">
<col style="width: 21%">
<col style="width: 20%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Forward Passes</th>
<th>Faithfulness</th>
<th>Limitations</th>
<th>Best Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISM</td>
<td>3<em>L</em> per sequence</td>
<td>High (direct measurement)</td>
<td>Computationally expensive</td>
<td>Validating importance in targeted regions</td>
</tr>
<tr class="even">
<td>Gradient <span class="math inline">\(\times\)</span> Input</td>
<td>1 backward pass</td>
<td>Low-Medium</td>
<td>Saturation, local approximation</td>
<td>Fast initial screening</td>
</tr>
<tr class="odd">
<td>DeepLIFT</td>
<td>1 pass (custom)</td>
<td>Medium</td>
<td>Reference-dependent</td>
<td>Attribution with completeness guarantees</td>
</tr>
<tr class="even">
<td>Integrated Gradients</td>
<td>10-50 passes</td>
<td>Medium-High</td>
<td>Reference-dependent, slower</td>
<td>Principled attribution with efficiency</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-ch25-reconciling" class="level3" data-number="25.1.3">
<h3 data-number="25.1.3" class="anchored" data-anchor-id="sec-ch25-reconciling"><span class="header-section-number">25.1.3</span> Reconciling Attribution Methods</h3>
<p>Different attribution methods can produce strikingly different importance maps for the same sequence and prediction. A position might show high importance under ISM but near-zero gradients due to saturation, or high gradient magnitude but minimal effect when actually mutated due to redundancy with nearby positions. This disagreement reflects genuine differences in what each method measures: gradients capture local sensitivity, ISM captures counterfactual effects, and <em>DeepLIFT</em> captures contribution relative to a reference.</p>
<p>Practical workflows often combine multiple methods. Gradient-based approaches efficiently scan long sequences to identify candidate regions, ISM validates importance in targeted windows, and agreement across methods increases confidence that identified features genuinely drive predictions. Disagreement flags positions for closer investigation, potentially revealing saturation effects, redundancy, or artifacts in individual methods.</p>
</section>
</section>
<section id="sec-ch25-cnn-filters" class="level2" data-number="25.2">
<h2 data-number="25.2" class="anchored" data-anchor-id="sec-ch25-cnn-filters"><span class="header-section-number">25.2</span> Interpreting Convolutional Filters</h2>
<p>Convolutional neural networks remain central to genomic sequence modeling, as discussed in <a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>, and their first-layer filters offer a particularly tractable interpretability target. Each filter slides along the sequence computing dot products with local windows, and high activation indicates that the local sequence matches the filter’s learned pattern. This architecture creates a natural correspondence between filters and sequence motifs.</p>
<section id="sec-ch25-filter-pwm" class="level3" data-number="25.2.1">
<h3 data-number="25.2.1" class="anchored" data-anchor-id="sec-ch25-filter-pwm"><span class="header-section-number">25.2.1</span> From Filters to Position Weight Matrices</h3>
<p>Converting learned filters to interpretable motifs follows a standard workflow. The trained model processes a large sequence set, typically training data or genome-wide tiles, recording positions where each filter’s activation exceeds a threshold. The fixed-length windows around high-activation positions are extracted and aligned, and nucleotide frequencies at each position are computed to build a <strong>position weight matrix (PWM)</strong>. This PWM can be visualized as a sequence logo and compared to databases like JASPAR or HOCOMOCO.</p>
<p>When this procedure is applied to models trained on chromatin accessibility or transcription factor binding, first-layer filters frequently match known transcription factor motifs. <em>DeepSEA</em> filters include recognizable matches to CTCF, AP-1, and cell-type-specific factors <em>[Citation Needed: Zhou &amp; Troyanskaya, DeepSEA paper]</em>. This correspondence validates that models discover biologically meaningful patterns rather than arbitrary correlations, and it provides a direct link between model weights and decades of experimental characterization of transcription factor binding preferences.</p>
<p>Several complications affect filter interpretation. DNA is double-stranded, and models may learn forward and reverse-complement versions of the same motif as separate filters. Some filters capture general sequence composition (GC-rich regions, homopolymer runs) rather than specific binding sites. These patterns can be biologically meaningful in contexts like nucleosome positioning or purely artifactual depending on the training task. Distinguishing informative filters from compositional shortcuts requires cross-referencing with known biology and testing whether filter-derived motifs predict binding in held-out data.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check: CNN Filter Interpretation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a CNN trained to predict CTCF binding from DNA sequence. You extract the top-activated sequences for one filter and find they all contain the pattern CCGCGNGGNGGCAG.</p>
<ol type="1">
<li>How would you determine if this filter is recognizing the forward or reverse-complement CTCF motif?</li>
<li>If another filter shows high activation for poly-G stretches, what follow-up analysis would distinguish whether this reflects true CTCF biology versus a training data artifact?</li>
<li>Why might the model learn separate filters for the same motif in different orientations, even though CTCF binding is largely orientation-independent at the ChIP-seq level?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Check the reverse complement of the discovered pattern against known CTCF motifs in JASPAR.</p></li>
<li><p>Test whether removing poly-G sequences reduces prediction accuracy for CTCF binding in held-out data, and check whether poly-G enrichment correlates with GC content or mappability artifacts in the training set.</p></li>
<li><p>CNNs without explicit reverse-complement architecture learn separate filters because the convolution operation treats forward and reverse strands as independent patterns, even when biology treats them equivalently.</p></li>
</ol>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-ch25-deeper-layers" class="level3" data-number="25.2.2">
<h3 data-number="25.2.2" class="anchored" data-anchor-id="sec-ch25-deeper-layers"><span class="header-section-number">25.2.2</span> Deeper Layers and Combinatorial Patterns</h3>
<p>Beyond the first layer, convolutional filters combine lower-level patterns into complex representations. Deeper layers can encode motif pairs that co-occur at characteristic spacing, orientation preferences between binding sites, and contextual dependencies where a motif’s importance varies with surrounding sequence. These combinatorial patterns capture aspects of regulatory grammar that individual motifs cannot represent.</p>
<p>Direct interpretation of deeper filters becomes increasingly difficult as receptive fields expand and nonlinearities accumulate. The activation of a layer-5 filter depends on intricate combinations of earlier patterns, resisting simple biological annotation. Indirect approaches prove more tractable: analyzing which input regions drive high activation at deeper layers, clustering high-activation sequences to find common themes, or probing whether deeper representations encode specific biological properties.</p>
</section>
</section>
<section id="sec-ch25-motif-discovery" class="level2" data-number="25.3">
<h2 data-number="25.3" class="anchored" data-anchor-id="sec-ch25-motif-discovery"><span class="header-section-number">25.3</span> Motif Discovery from Attributions</h2>
<p>Attribution maps highlight important positions but do not directly reveal motifs. A <em>DeepLIFT</em> track might show scattered high-importance bases throughout a sequence without indicating that those bases collectively form instances of the same transcription factor binding site. <em>TF-MoDISco</em> (Transcription Factor Motif Discovery from Importance Scores) bridges this gap by discovering motifs from attribution scores rather than raw sequences <span class="citation" data-cites="shrikumar_technical_2018">(<a href="../bib/references.html#ref-shrikumar_technical_2018" role="doc-biblioref">Shrikumar et al. 2018</a>)</span>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Why TF-MoDISco Works Better Than Traditional Motif Finding
</div>
</div>
<div class="callout-body-container callout-body">
<p>Traditional motif discovery algorithms (like MEME) scan raw sequences and must contend with a fundamental problem: most positions in regulatory sequences do not participate in functional motifs. The algorithm wastes effort on irrelevant positions and may find patterns that happen to be overrepresented but have no functional significance. <em>TF-MoDISco</em> solves this by using attribution scores to weight the search: positions the model actually uses for prediction get prioritized, while unimportant positions contribute minimally. This importance-weighted approach discovers the motifs that drive model predictions, not just patterns that occur frequently.</p>
</div>
</div>
<p>The insight underlying <em>TF-MoDISco</em> is that importance-weighted sequences focus motif discovery on positions the model actually uses. Traditional motif finders must contend with the fact that most positions in regulatory sequences do not participate in functional motifs. By extracting seqlets (short windows where total importance exceeds a threshold) and clustering them based on both sequence content and importance profiles, <em>TF-MoDISco</em> identifies patterns that drive model predictions.</p>
<p>The workflow proceeds through several stages. Base-level importance scores are computed for many sequences using <em>DeepLIFT</em>, ISM, or integrated gradients. Windows where total importance exceeds a threshold are extracted as seqlets, each representing a candidate motif instance. These seqlets are compared using metrics that consider both sequence content and importance profiles, then clustered into groups corresponding to putative motifs. Within each cluster, seqlets are aligned and consolidated into PWMs and importance-weighted logos. The resulting motifs can be matched to known transcription factors or flagged as novel patterns.</p>
<p>Beyond individual motifs, <em>TF-MoDISco</em> enables grammar inference by analyzing motif co-occurrence. Mapping discovered motif instances back to genomic coordinates reveals characteristic spacing between motif pairs, orientation preferences, and cell-type-specific usage patterns. These grammatical rules can be validated through <em>in silico</em> experiments: inserting or removing motifs in synthetic sequences and checking whether predictions change as expected.</p>
<p>Applications to models like <em>BPNet</em> trained on ChIP-seq data have recovered known transcription factor motifs, discovered novel sequence variants, and revealed spacing constraints validated through synthetic reporter assays <em>[Citation Needed: BPNet paper]</em>. The same workflow applies to foundation model analysis: use the model to produce base-level attributions for a downstream task, run <em>TF-MoDISco</em> to extract a task-specific motif vocabulary, and analyze how motif usage varies across conditions.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Worked Example: TF-MoDISco Workflow">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Worked Example: TF-MoDISco Workflow
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Scenario:</strong> You have trained a model to predict liver enhancer activity and want to understand what sequence features it uses.</p>
<p><strong>Step 1: Compute attributions</strong> Run integrated gradients on 10,000 predicted enhancer sequences (selecting sequences with high prediction scores). This produces a 4 × L attribution matrix for each sequence, where L is sequence length.</p>
<p><strong>Step 2: Extract seqlets</strong> Scan each attribution profile for windows where summed importance exceeds a threshold (e.g., top 5% of all windows). A typical 500bp enhancer might yield 3-5 seqlets of 15-25bp each.</p>
<p><strong>Step 3: Cluster seqlets</strong> Using both sequence similarity and attribution profile similarity, cluster the ~40,000 extracted seqlets. Suppose this produces 12 distinct clusters.</p>
<p><strong>Step 4: Generate motifs</strong> For each cluster, align seqlets and compute position weight matrices. Results might include:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Cluster</th>
<th># Seqlets</th>
<th>Top JASPAR Match</th>
<th>Match Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>8,200</td>
<td>HNF4A</td>
<td>0.92</td>
</tr>
<tr class="even">
<td>2</td>
<td>5,100</td>
<td>CEBPA</td>
<td>0.89</td>
</tr>
<tr class="odd">
<td>3</td>
<td>3,400</td>
<td>FOXA1</td>
<td>0.85</td>
</tr>
<tr class="even">
<td>4</td>
<td>2,800</td>
<td>Novel (no match &gt; 0.7)</td>
<td>—</td>
</tr>
</tbody>
</table>
<p><strong>Step 5: Validate</strong> The HNF4A and CEBPA motifs are known liver-specific transcription factors, confirming the model learned biologically relevant features. Cluster 4 represents a potential novel regulatory element requiring experimental validation.</p>
<p><strong>Interpretation:</strong> The model relies heavily on canonical liver transcription factor binding sites, consistent with known liver enhancer biology. The novel motif warrants ChIP-seq or MPRA follow-up.</p>
</div>
</div>
<div id="fig-tfmodisco" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tfmodisco-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/03-fig-tfmodisco.svg" class="img-fluid figure-img"></p>
<figcaption>TF-MoDISco motif discovery from attribution scores</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tfmodisco-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25.3: TF-MoDISco motif discovery from attribution scores. Starting from attribution maps for many sequences, the method extracts high-importance windows (seqlets), clusters similar seqlets, consolidates into position weight matrices, and matches to known motif databases. Discovered motifs represent what the model learned—known transcription factors validate biological relevance while novel patterns may indicate previously unknown regulatory features.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch25-probing" class="level2" data-number="25.4">
<h2 data-number="25.4" class="anchored" data-anchor-id="sec-ch25-probing"><span class="header-section-number">25.4</span> Probing Learned Representations</h2>
<p>Attribution methods ask which input positions matter; probing asks what information the model’s internal representations encode. The approach resembles asking a student to “show their work” on an exam: if they can correctly answer follow-up questions about intermediate steps, they likely understood the underlying concepts rather than memorizing answers. A probing classifier is a simple supervised model (typically linear) trained to predict some property of interest from the hidden representations of a pretrained model. If a linear probe can accurately predict a property, that property is encoded in an accessible form within the representation—the model “knows” this information in a way that can be easily extracted.</p>
<section id="sec-ch25-probing-methods" class="level3" data-number="25.4.1">
<h3 data-number="25.4.1" class="anchored" data-anchor-id="sec-ch25-probing-methods"><span class="header-section-number">25.4.1</span> Probing Methodology</h3>
<p>The standard probing workflow extracts hidden states from a pretrained model for a set of inputs where the property of interest is known. These hidden states, without further transformation, serve as features for training a simple classifier to predict the property. The classifier’s accuracy indicates how well the representation encodes the probed property, while its simplicity (linearity, minimal parameters) ensures that the probe identifies information present in the representation rather than information the probe itself computes.</p>
<p>For protein language models like <em>ESM-2</em>, probing has revealed that representations encode secondary structure, solvent accessibility, contact maps, and even 3D coordinates to a surprising degree, as discussed in <a href="../part_4/p4-ch16-protein-lm.html" class="quarto-xref"><span>Chapter 16</span></a>. These properties emerge despite training on sequence alone, demonstrating that masked language modeling on evolutionary sequences induces representations that capture structural information. For DNA language models (see <a href="../part_4/p4-ch15-dna-lm.html" class="quarto-xref"><span>Chapter 15</span></a>), probing can assess whether representations encode chromatin state, gene boundaries, promoter versus enhancer identity, or species-specific regulatory signatures.</p>
<p>Probing provides diagnostic information distinct from downstream task performance. A model might achieve high accuracy on a regulatory prediction task by learning shortcuts (correlations with GC content, distance to annotated genes) rather than encoding genuine regulatory grammar. Probing can detect such shortcuts: if representations strongly encode GC content but weakly encode transcription factor binding site presence, the model may be exploiting composition rather than sequence logic. This diagnostic function complements the confounder analysis discussed in <a href="../part_3/p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think: Probing and Confounding
</div>
</div>
<div class="callout-body-container callout-body">
<p>You train a DNA language model on human genome sequences, then probe its representations to understand what it has learned. You find:</p>
<ul>
<li>Linear probe for GC content: 95% accuracy</li>
<li>Linear probe for promoter vs.&nbsp;enhancer: 78% accuracy</li>
<li>Linear probe for tissue-specific enhancer activity: 52% accuracy</li>
</ul>
<p>What do these results suggest about the model’s representations? Before reading further, consider:</p>
<ol type="1">
<li>Which result is most concerning for downstream variant effect prediction?</li>
<li>How would you distinguish whether the promoter/enhancer probe reflects genuine regulatory learning versus correlation with GC content?</li>
<li>What additional probing experiments would you design?</li>
</ol>
</div>
</div>
</section>
<section id="sec-ch25-probing-limits" class="level3" data-number="25.4.2">
<h3 data-number="25.4.2" class="anchored" data-anchor-id="sec-ch25-probing-limits"><span class="header-section-number">25.4.2</span> Limitations of Probing</h3>
<p>Probing results require careful interpretation. A probe’s failure to predict some property might indicate that the representation does not encode it, or might reflect limitations of the probe architecture, insufficient training data, or mismatch between the probe’s capacity and the complexity of the encoding. Linear probes may miss nonlinearly encoded information; more complex probes risk learning the property themselves rather than reading it from the representation.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Challenge Alert: The Selectivity-Accessibility Tradeoff
</div>
</div>
<div class="callout-body-container callout-body">
<p>This concept is subtle but important. A representation can encode information in two fundamentally different ways:</p>
<ul>
<li><strong>Accessible encoding</strong>: A simple (linear) probe can extract the information. The representation makes the property easy to read.</li>
<li><strong>Selective encoding</strong>: The information is present but requires nonlinear decoding. The property is represented but not prominently exposed.</li>
</ul>
<p>The challenge: if you use a more powerful (nonlinear) probe to detect selective encoding, how do you know the probe is reading information from the representation versus computing it from scratch? This is an active area of methodological research with no perfect solution. Best practice: compare probe performance to a control where the same probe is trained on random representations. If performance drops substantially, the original representation genuinely encoded the property.</p>
</div>
</div>
<p>The selectivity-accessibility tradeoff complicates interpretation. A representation might encode a property accessibly (recoverable by a linear probe) or selectively (encoded but requiring nonlinear decoding). Properties encoded selectively might be present but not easily extracted, while properties encoded accessibly might be incidentally correlated with the training objective rather than causally important. Combining probing with causal interventions (ablating representation components and measuring effects on downstream predictions) provides stronger evidence about which encoded properties actually matter.</p>
</section>
</section>
<section id="sec-ch25-attention" class="level2" data-number="25.5">
<h2 data-number="25.5" class="anchored" data-anchor-id="sec-ch25-attention"><span class="header-section-number">25.5</span> Attention Patterns in Transformer Models</h2>
<p>Transformer-based genomic models use self-attention to aggregate information across long sequence contexts (see <a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a> for architectural details), potentially capturing distal regulatory interactions invisible to models with narrow receptive fields. Attention weights indicate which positions each position attends to, creating natural candidates for interpretability: perhaps high attention weights identify functionally related sequence elements.</p>
<div id="fig-attention-visualization" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/04-A-fig-attention-visualization.svg" class="img-fluid figure-img"></p>
<figcaption>Attention heatmap showing position-position weights</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/04-B-fig-attention-visualization.svg" class="img-fluid figure-img"></p>
<figcaption>Biological overlay: attention sometimes aligns with regulatory elements</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/04-C-fig-attention-visualization.svg" class="img-fluid figure-img"></p>
<figcaption>Multi-head specialization: different heads capture different patterns</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25.4: Attention patterns in genomic transformers. (A) Attention heatmap showing position-position weights with local and long-range patterns. (B) Biological overlay: attention sometimes aligns with regulatory elements—but correlation does not prove the model learned these relationships causally. (C) Multi-head specialization: different heads capture local context, long-range interactions, or specific motifs. <strong>Caution</strong>: attention weights describe information routing, not importance—always validate with perturbation experiments.
</figcaption>
</figure>
</div>
<section id="sec-ch25-attention-patterns" class="level3" data-number="25.5.1">
<h3 data-number="25.5.1" class="anchored" data-anchor-id="sec-ch25-attention-patterns"><span class="header-section-number">25.5.1</span> What Attention Patterns Reveal</h3>
<p>When attention weights are analyzed in genomic language models, certain heads exhibit strikingly structured patterns. Some heads preferentially connect positions within the same predicted gene or operon, suggesting the model has learned gene boundaries from sequence alone. Other heads show long-range connections that align with known enhancer-promoter relationships or chromatin loop anchors. Still others cluster positions by functional annotation, connecting genes with similar Gene Ontology terms despite lacking explicit functional labels during training.</p>
<p>In models like <em>Enformer</em> that predict regulatory outputs from long genomic windows (see <a href="../part_4/p4-ch17-regulatory.html#sec-ch17-enformer" class="quarto-xref"><span>Section 17.2</span></a>), attention can reveal which distal regions influence predictions at a target gene. Contribution scores aggregated across attention heads often peak at known enhancers, insulators, and chromatin domain boundaries. These patterns suggest that the model has learned aspects of regulatory architecture from the correlation between sequence and chromatin output labels.</p>
</section>
<section id="sec-ch25-attention-caution" class="level3" data-number="25.5.2">
<h3 data-number="25.5.2" class="anchored" data-anchor-id="sec-ch25-attention-caution"><span class="header-section-number">25.5.2</span> Why Attention Weights Mislead</h3>
<p>Raw attention weights require skeptical interpretation. High attention between two positions indicates information flow in the model’s computation but does not necessarily indicate causal influence on predictions. Attention serves multiple computational roles beyond identifying important features: routing information for intermediate computations, implementing positional reasoning, and satisfying architectural constraints. A position receiving high attention might be used for bookkeeping rather than contributing to the final output.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Attention Is Not Explanation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The most common interpretability mistake with transformers is treating attention weights as importance scores. This is seductive because attention weights are easy to extract and visualize, and high-attention patterns often look biologically plausible. But attention describes <em>information routing</em>, not <em>causal contribution</em>. Consider this analogy: in a complex recipe, you might frequently consult the measurements section (high “attention”) while the actual flavor comes from the spice section (low “attention” but high importance). To know if an attention pattern matters, you must perturb it and measure the prediction change. Attention without perturbation is correlation without causation.</p>
</div>
</div>
<p>Several specific issues undermine naive attention interpretation. Attention weights describe information movement before value vectors are applied; positions with high attention but small value vector magnitudes contribute little to the output. Multi-head attention averages across heads with different functions; examining average attention obscures specialized head behavior. Cross-layer effects mean that the importance of early-layer attention depends on what later layers do with the routed information.</p>
<p>More robust approaches combine attention analysis with perturbation experiments. If deleting a position that receives high attention changes the prediction substantially, the attention is functionally meaningful. If deletion has minimal effect, the attention may serve computational purposes unrelated to the target output. Attention rollout and attention flow methods propagate attention through layers to better capture information movement across the full network, though these too provide correlational rather than causal evidence.</p>
</section>
</section>
<section id="sec-ch25-global" class="level2" data-number="25.6">
<h2 data-number="25.6" class="anchored" data-anchor-id="sec-ch25-global"><span class="header-section-number">25.6</span> Regulatory Vocabularies and Global Interpretability</h2>
<p>Local interpretability methods explain individual predictions; global interpretability characterizes what a model has learned across its entire training distribution. For genomic models trained to predict thousands of chromatin features, global interpretability asks whether the model has learned a coherent vocabulary of regulatory sequence classes and how those classes map to biological programs.</p>
<section id="sec-ch25-sei" class="level3" data-number="25.6.1">
<h3 data-number="25.6.1" class="anchored" data-anchor-id="sec-ch25-sei"><span class="header-section-number">25.6.1</span> Sequence Classes from <em>Sei</em></h3>
<p><em>Sei</em> exemplifies the global interpretability approach by learning a vocabulary of regulatory sequence classes that summarize chromatin profile diversity across the genome (see <a href="../part_4/p4-ch17-regulatory.html#sec-ch17-sei" class="quarto-xref"><span>Section 17.4</span></a> for architectural details). The model predicts tens of thousands of chromatin outputs (transcription factor binding, histone modifications, accessibility across cell types), then compresses this high-dimensional prediction space into approximately 40 sequence classes through dimensionality reduction and clustering.</p>
<p>Each sequence class corresponds to a characteristic regulatory activity pattern. Some classes show promoter-like signatures (H3K4me3, TSS proximity, broad expression). Others exhibit enhancer patterns (H3K27ac, H3K4me1, cell-type-restricted activity). Repressive classes display H3K27me3 or H3K9me3 enrichment. Cell-type-specific classes capture lineage-restricted regulatory programs (neuronal, immune, hepatic). This vocabulary transforms thousands of raw chromatin predictions into a compact, interpretable representation.</p>
<p>Variants can be characterized by their effects on sequence class scores, yielding functional descriptions more informative than raw pathogenicity predictions. A variant that shifts a region from enhancer-like to promoter-like class, or from active to repressive, provides mechanistic hypotheses about its functional consequences. <strong>Genome-wide association study (GWAS)</strong> enrichment analysis can identify which sequence classes are overrepresented among disease-associated variants, revealing the regulatory programs most relevant to specific phenotypes (see <a href="../part_1/p1-ch03-gwas.html" class="quarto-xref"><span>Chapter 3</span></a> for GWAS foundations).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check: Local vs.&nbsp;Global Interpretability
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a foundation model that predicts tissue-specific enhancer activity across 100 cell types.</p>
<ol type="1">
<li>What would a <em>local</em> interpretability analysis tell you about a specific variant in a cardiac enhancer?</li>
<li>What would a <em>global</em> interpretability analysis (like Sei’s sequence classes) tell you about the same variant?</li>
<li>In what clinical scenario would you prefer local interpretability? In what research scenario would global interpretability be more valuable?</li>
</ol>
<p><em>Think about the difference between explaining one prediction versus characterizing the model’s overall regulatory vocabulary.</em></p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Local analysis (attribution methods) would identify which specific nucleotides the variant disrupts and what motifs are affected—for example, “variant disrupts a GATA4 binding site at position 142.”</p></li>
<li><p>Global analysis would show how the variant shifts regulatory program membership—for example, “variant shifts sequence class from cardiac-specific enhancer to generic promoter-like.”</p></li>
<li><p>Prefer local for clinical variant interpretation where you need mechanistic detail for a specific case; prefer global for GWAS follow-up where you want to understand which regulatory programs are disease-relevant across many variants.</p></li>
</ol>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-ch25-embedding-geometry" class="level3" data-number="25.6.2">
<h3 data-number="25.6.2" class="anchored" data-anchor-id="sec-ch25-embedding-geometry"><span class="header-section-number">25.6.2</span> Embedding Geometry and Regulatory Programs</h3>
<p>Beyond discrete sequence classes, the continuous geometry of learned representations encodes regulatory relationships. Sequences with similar regulatory functions cluster in embedding space; directions in this space correspond to biological axes of variation. Dimensionality reduction techniques (UMAP, t-SNE, principal component analysis) visualize these relationships, revealing how the model organizes regulatory diversity.</p>
<p>For foundation models trained on diverse genomic tasks, embedding geometry can capture cross-task relationships. Sequences that function as enhancers in one cell type might cluster near sequences with enhancer function in related cell types, even if trained independently. Variants that disrupt shared regulatory logic should produce similar embedding perturbations. These geometric properties enable transfer of interpretability insights across tasks and provide compact summaries of model knowledge.</p>
</section>
</section>
<section id="sec-ch25-mechanistic" class="level2" data-number="25.7">
<h2 data-number="25.7" class="anchored" data-anchor-id="sec-ch25-mechanistic"><span class="header-section-number">25.7</span> Mechanistic Interpretability</h2>
<p>Classical interpretability methods treat models as input-output functions, probing what they compute without examining how they compute it. Mechanistic interpretability takes a different approach, attempting to reverse-engineer the algorithms implemented by neural network weights. Think of it like the difference between knowing that a car gets you from A to B versus opening the hood to understand how the engine, transmission, and fuel system work together. Classical interpretability tells you the car runs; mechanistic interpretability identifies which piston fires when and how the carburetor mixes fuel. This emerging field, most developed for language models, offers tools increasingly applicable to genomic foundation models.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Challenge Alert: Frontier Research Area
</div>
</div>
<div class="callout-body-container callout-body">
<p>Mechanistic interpretability represents the frontier of interpretability research. The concepts in this section are powerful but the techniques are still maturing. Current methods require substantial manual analysis, work best for small models, and have been validated primarily in language models rather than genomic models. As you read, focus on understanding the conceptual framework (circuits, features, superposition) rather than expecting turnkey tools. The field is evolving rapidly, and today’s research prototypes may become tomorrow’s standard practices.</p>
</div>
</div>
<section id="sec-ch25-circuits" class="level3" data-number="25.7.1">
<h3 data-number="25.7.1" class="anchored" data-anchor-id="sec-ch25-circuits"><span class="header-section-number">25.7.1</span> Circuits and Features</h3>
<p>The central hypothesis of mechanistic interpretability is that neural networks implement interpretable computations through identifiable circuits: connected subnetworks that perform specific functions. A circuit might detect whether a motif is present, compute the distance between two motifs, or integrate evidence across regulatory elements. Identifying circuits requires tracing information flow through the network and characterizing what each component contributes.</p>
<p>Features are the atomic units of this analysis: directions in activation space that correspond to interpretable concepts. In language models, features have been found that activate for specific topics, syntactic structures, or semantic properties. Analogous features in genomic models might activate for transcription factor binding sites, coding versus non-coding sequence, or regulatory element types. <strong>Sparse autoencoders</strong> trained on model activations can extract interpretable features by encouraging representations where most features are inactive for any given input.</p>
<p><strong>Superposition</strong> complicates feature identification. Neural networks can represent more features than they have dimensions by using overlapping, nearly orthogonal directions. Why would networks do this? The answer lies in the statistics of natural data: most features are sparse (active for only a small fraction of inputs), so they rarely need to be represented simultaneously. By packing many sparse features into a lower-dimensional space using nearly orthogonal directions, networks can represent far more concepts than their dimensionality would naively suggest. Features active for different inputs can share parameters, enabling high-capacity representations but complicating interpretation—when we observe an activation pattern, multiple overlapping features may contribute. Techniques from compressed sensing and dictionary learning help decompose superposed representations into constituent features.</p>
</section>
<section id="sec-ch25-mechanistic-genomics" class="level3" data-number="25.7.2">
<h3 data-number="25.7.2" class="anchored" data-anchor-id="sec-ch25-mechanistic-genomics"><span class="header-section-number">25.7.2</span> Applications to Genomic Models</h3>
<p>Mechanistic interpretability remains nascent for genomic foundation models, but initial applications show promise. Attention head analysis in DNA language models has identified heads specialized for different genomic functions: some attend within genes, others across regulatory regions, still others implement positional computations <em>[Citation Needed]</em>. Probing activations at different layers reveals hierarchical feature construction, from local sequence patterns in early layers to long-range regulatory relationships in later layers.</p>
<p>Circuit analysis can explain specific model behaviors. If a model predicts that a variant disrupts regulation, mechanistic analysis can trace which features activate differently for reference versus variant sequence, which attention heads route information about the variant to the prediction, and which intermediate computations change. This mechanistic trace provides far richer explanation than attribution scores alone, potentially identifying the regulatory logic the model has learned.</p>
<p>The challenge is scalability. Current mechanistic interpretability techniques require substantial manual analysis and work best for small models or specific behaviors. Foundation models with billions of parameters resist exhaustive circuit enumeration. Developing automated tools for circuit discovery and scaling mechanistic analysis to large genomic models represents an active research frontier.</p>
</section>
</section>
<section id="sec-ch25-validation" class="level2" data-number="25.8">
<h2 data-number="25.8" class="anchored" data-anchor-id="sec-ch25-validation"><span class="header-section-number">25.8</span> Validation: From Explanations to Experiments</h2>
<p>Interpretability methods produce explanations, but explanations are only valuable if they accurately reflect model behavior and connect to biological reality. Validation closes the loop by testing whether interpretability-derived hypotheses hold when subjected to experimental scrutiny.</p>
<section id="sec-ch25-faithfulness" class="level3" data-number="25.8.1">
<h3 data-number="25.8.1" class="anchored" data-anchor-id="sec-ch25-faithfulness"><span class="header-section-number">25.8.1</span> Faithfulness Testing</h3>
<p>An interpretation is faithful if it accurately describes what the model does. Testing faithfulness requires interventions: changing the features identified as important and verifying that predictions change accordingly. If an attribution method highlights certain positions as driving a prediction, deleting or scrambling those positions should reduce the prediction. If discovered motifs are claimed to be necessary for regulatory activity, removing them from sequences should impair predicted and measured function.</p>
<p>Sanity checks provide baseline validation. When model weights are randomized, attributions should degrade to uninformative noise. When training labels are scrambled, discovered motifs should disappear or lose predictive power. These checks identify methods that produce plausible-looking outputs regardless of model content, revealing explanations that reflect method biases rather than genuine model features.</p>
<p>Counterfactual experiments go further by testing whether identified features are sufficient as well as necessary. Inserting discovered motifs into neutral sequences should increase predicted regulatory activity if the motifs genuinely encode functional elements. Constructing synthetic sequences that combine motifs according to discovered grammatical rules should produce predictions consistent with those rules. Discrepancies between expected and observed effects indicate gaps in the interpretation.</p>
<p>The following table summarizes the hierarchy of validation tests, from weakest to strongest evidence:</p>
<div id="tbl-validation-hierarchy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-validation-hierarchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;25.2: Validation hierarchy for interpretability claims. Stronger evidence requires more experimental investment but provides greater confidence that model explanations reflect biological reality.
</figcaption>
<div aria-describedby="tbl-validation-hierarchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 10%">
<col style="width: 26%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Validation Level</th>
<th>Test</th>
<th>What It Proves</th>
<th>What It Cannot Prove</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sanity Check</td>
<td>Random weights produce random attributions</td>
<td>Method is not trivially broken</td>
<td>Method accurately reflects model</td>
</tr>
<tr class="even">
<td>Computational Necessity</td>
<td>Ablating feature reduces prediction</td>
<td>Feature is used by model</td>
<td>Feature is the only cause</td>
</tr>
<tr class="odd">
<td>Computational Sufficiency</td>
<td>Inserting feature increases prediction</td>
<td>Feature is sufficient in isolation</td>
<td>Feature is necessary or biologically meaningful</td>
</tr>
<tr class="even">
<td>Biological Necessity</td>
<td>Experimental deletion (CRISPR) abolishes activity</td>
<td>Feature is biologically required</td>
<td>Model learned it correctly</td>
</tr>
<tr class="odd">
<td>Biological Sufficiency</td>
<td>Synthetic construct with feature is active</td>
<td>Feature is biologically sufficient</td>
<td>Model captured all relevant features</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-plausible-vs-faithful" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plausible-vs-faithful-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/05-fig-plausible-vs-faithful.svg" class="img-fluid figure-img"></p>
<figcaption>Plausible versus faithful explanations: validation distinguishes them</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plausible-vs-faithful-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25.5: Plausible versus faithful explanations. Both paths start with attributions highlighting a GATA motif. Left: plausible but unfaithful—the explanation matches intuition, but the model learned GC content; validation tests fail. Right: faithful—the model actually uses the GATA motif; validation tests pass. The distinction determines whether interpretability generates genuine insight or false comfort. Always validate: does ablating the feature change prediction? Does inserting it increase prediction?
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch25-experimental" class="level3" data-number="25.8.2">
<h3 data-number="25.8.2" class="anchored" data-anchor-id="sec-ch25-experimental"><span class="header-section-number">25.8.2</span> Experimental Validation</h3>
<p>The ultimate test of interpretability connects model-derived hypotheses to biological experiments. Motifs discovered through <em>TF-MoDISco</em> can be tested through electrophoretic mobility shift assays, ChIP-qPCR, or reporter constructs. Predicted spacing constraints can be validated by varying distances between motifs in synthetic constructs and measuring activity. Hypothesized enhancer-promoter connections can be tested through CRISPR deletion of predicted enhancers and measurement of target gene expression.</p>
<p>This experimental validation distinguishes genuine mechanistic discovery from pattern matching that happens to produce plausible-looking results. A model might learn that certain k-mers correlate with regulatory activity for confounded reasons (batch effects, mappability artifacts) yet produce motif logos resembling real transcription factors. Only experimental testing can determine whether model-derived hypotheses reflect causal regulatory logic.</p>
<p>High-throughput functional assays enable systematic validation at scale. <strong>Massively parallel reporter assays (MPRAs)</strong> can test thousands of model-predicted regulatory elements simultaneously. Perturb-seq combines CRISPR perturbations with single-cell RNA-seq to measure effects of knocking out predicted regulatory factors (see <a href="../part_5/p5-ch20-single-cell.html#sec-ch20-perturbation" class="quarto-xref"><span>Section 20.3</span></a>). These technologies create opportunities for iterative model improvement: interpretability generates hypotheses, experiments test them, and results refine both model architecture and training.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think: Designing Validation Experiments
</div>
</div>
<div class="callout-body-container callout-body">
<p>You have used integrated gradients and TF-MoDISco to analyze a model that predicts liver-specific enhancer activity. The analysis reveals that the model relies heavily on HNF4A and CEBP motifs, often appearing within 50bp of each other.</p>
<p>Before reading further, design a validation strategy:</p>
<ol type="1">
<li>What computational experiments would test whether these motifs are necessary for the model’s predictions?</li>
<li>What computational experiments would test sufficiency?</li>
<li>What biological experiments would test whether the model’s reliance on these motifs reflects genuine liver regulatory logic?</li>
<li>If the biological experiments fail to validate the model’s predictions, what are the possible explanations?</li>
</ol>
</div>
</div>
<div id="fig-validation-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-validation-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/06-fig-validation-pipeline.svg" class="img-fluid figure-img"></p>
<figcaption>Closed-loop interpretability validation workflow</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-validation-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25.6: Closed-loop interpretability validation. Starting from model predictions, interpretability analysis generates hypotheses about important features. Experimental validation tests necessity (CRISPR deletion) and sufficiency (reporter assays). Validated hypotheses advance biological understanding; failures identify model limitations. This cycle progressively improves both mechanistic understanding and model reliability.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ch25-clinical" class="level2" data-number="25.9">
<h2 data-number="25.9" class="anchored" data-anchor-id="sec-ch25-clinical"><span class="header-section-number">25.9</span> Interpretability in Clinical Variant Assessment</h2>
<p>Variant interpretation guidelines require that computational predictions be weighed alongside experimental and clinical evidence, as discussed further in <a href="../part_7/p7-ch29-rare-disease.html" class="quarto-xref"><span>Chapter 29</span></a>. Interpretability determines whether model predictions can contribute meaningful evidence beyond raw pathogenicity scores.</p>
<p>Current ACMG-AMP criteria allow computational evidence as supporting (<em>PP3</em>) or opposing (<em>BP4</em>) pathogenicity, but the evidence strength depends on understanding what the prediction reflects <span class="citation" data-cites="richards_standards_2015">(<a href="../bib/references.html#ref-richards_standards_2015" role="doc-biblioref">Richards et al. 2015</a>)</span>. The full ACMG-AMP framework and its integration with computational evidence is examined in <a href="../part_7/p7-ch29-rare-disease.html#sec-ch29-acmg-amp" class="quarto-xref"><span>Section 29.2</span></a>. A splice site disruption score from <em>SpliceAI</em> provides interpretable mechanistic evidence: the variant is predicted to alter splicing because it changes the consensus splice site sequence (<a href="../part_2/p2-ch06-cnn.html#sec-ch06-spliceai" class="quarto-xref"><span>Section 6.5</span></a>) <span class="citation" data-cites="jaganathan_spliceai_2019">(<a href="../bib/references.html#ref-jaganathan_spliceai_2019" role="doc-biblioref">Jaganathan et al. 2019</a>)</span>. This prediction can be evaluated against splice site models, tested with minigene assays, and combined with observations of aberrant transcripts in patient samples. The interpretation enables evidence integration.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Communicating Interpretability in Clinical Reports
</div>
</div>
<div class="callout-body-container callout-body">
<p>When preparing computational evidence for clinical variant interpretation:</p>
<ol type="1">
<li><p><strong>Always include the mechanism, not just the score.</strong> “Pathogenicity score: 0.92” is less useful than “Predicted to disrupt CTCF binding site (attribution score -0.8), shifting sequence class from insulator to neutral.”</p></li>
<li><p><strong>Specify what was tested.</strong> Did you run ISM to validate the attribution? Did the motif match a known transcription factor? Is the affected sequence class enriched in relevant GWAS?</p></li>
<li><p><strong>Acknowledge limitations explicitly.</strong> If the model was not trained on the relevant tissue type, or if the variant type (structural, repeat) was underrepresented in training, say so.</p></li>
<li><p><strong>Suggest validation experiments.</strong> “This prediction could be validated by EMSA for CTCF binding or minigene assay for splicing effects.”</p></li>
<li><p><strong>Cross-reference related evidence.</strong> Does the computational mechanism explain the patient’s phenotype? Is there functional data in ClinVar for nearby variants?</p></li>
</ol>
</div>
</div>
<p>Foundation model predictions are less immediately interpretable but potentially more informative. A pathogenicity score from <em>ESM-1v</em> (<a href="../part_4/p4-ch16-protein-lm.html#sec-ch16-esm-family" class="quarto-xref"><span>Section 16.1</span></a>) reflects evolutionary constraint inferred from protein language modeling, but the specific sequence features driving the prediction require attribution analysis to identify. The protein VEP paradigm is examined in <a href="../part_4/p4-ch18-vep-fm.html#sec-ch18-protein-vep" class="quarto-xref"><span>Section 18.2</span></a>. An expression effect predicted by <em>Enformer</em> (<a href="../part_4/p4-ch17-regulatory.html#sec-ch17-enformer" class="quarto-xref"><span>Section 17.2</span></a>) might result from disrupted transcription factor binding, altered chromatin accessibility, or changed 3D regulatory contacts; interpretability analysis distinguishes these mechanisms and guides experimental validation. The DNA-based VEP approaches are detailed in <a href="../part_4/p4-ch18-vep-fm.html#sec-ch18-dna-vep" class="quarto-xref"><span>Section 18.3</span></a>.</p>
<p>For clinical utility, interpretability must be communicated effectively. Genome browsers displaying attribution tracks alongside variant calls help clinicians identify mechanistic hypotheses. Reports that accompany pathogenicity scores with regulatory vocabulary classifications (this variant shifts an enhancer toward a repressive state) provide actionable context. These communication challenges extend interpretability beyond algorithm development to user interface design and clinical workflow integration.</p>
<div id="fig-clinical-interpretability" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clinical-interpretability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_6/ch25/07-fig-clinical-interpretability.svg" class="img-fluid figure-img"></p>
<figcaption>Interpretability for clinical variant assessment</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clinical-interpretability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25.7: Interpretability for clinical variant assessment. ACMG evidence strength depends on interpretability: a pathogenicity score alone (PP3/BP4 supporting) provides weak evidence; a score with mechanistic explanation (splice site disruption) is moderate; a score with experimentally validated mechanism (ChIP-confirmed binding loss, functional assay) is strong. Clinical reports should communicate the mechanism, not just the score, to enable evidence integration.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch25-practical" class="level2" data-number="25.10">
<h2 data-number="25.10" class="anchored" data-anchor-id="sec-ch25-practical"><span class="header-section-number">25.10</span> Practical Approaches for Foundation Model Analysis</h2>
<p>Working with genomic foundation models requires matching interpretability methods to specific questions. Several complementary strategies address different aspects of model behavior.</p>
<p>For understanding variant effects, the primary goal is explaining why a specific variant receives a particular prediction. Attribution methods (ISM for validation, integrated gradients for efficiency) identify which input positions drive the difference between reference and alternative predictions. If the variant falls within a discovered motif, the interpretation is straightforward. If attributions spread across the sequence, the effect may operate through long-range regulatory changes requiring attention analysis or contribution scores from models like <em>Enformer</em>.</p>
<p>For characterizing model representations, probing classifiers diagnose what information is encoded and at which layers. Probing for known regulatory features (promoter versus enhancer, tissue specificity, evolutionary conservation) establishes which biological properties the model captures. Probing for potential confounders (GC content, distance to annotated genes, technical artifacts) identifies shortcuts that might inflate benchmark performance without reflecting genuine regulatory understanding (see <a href="../part_3/p3-ch11-benchmarks.html#sec-ch11-systematic-gaps" class="quarto-xref"><span>Section 11.8</span></a> for benchmark limitations and <a href="../part_3/p3-ch13-confounding.html#sec-ch13-detection" class="quarto-xref"><span>Section 13.8</span></a> for confounder detection methods).</p>
<p>For discovering regulatory logic, <em>TF-MoDISco</em> applied to high-confidence predictions extracts motif vocabularies specific to prediction tasks or cell types. Grammar analysis of motif co-occurrence reveals combinatorial rules. <em>Sei</em>-style sequence class analysis situates local motifs within global regulatory programs. Comparing discovered vocabularies across models or training conditions reveals shared versus idiosyncratic features.</p>
<p>For debugging and auditing, interpretability methods identify what features drive predictions in held-out distributions. If a model fails on a new cell type, attribution analysis can reveal whether it relies on cell-type-specific versus generalizable features. If performance degrades on specific genomic regions, local interpretability can identify confounding patterns or training data gaps.</p>
<p>For generating experimental hypotheses, interpretability produces testable predictions. Discovered motifs can be synthesized and tested. Predicted regulatory elements can be perturbed. Hypothesized transcription factor binding can be validated by ChIP. Model-derived predictions that survive experimental testing represent genuine mechanistic insights; predictions that fail point toward model limitations or confounding.</p>
<p>The following table provides a decision framework for selecting interpretability methods based on your analysis goal:</p>
<div id="tbl-interpretability-decision" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-interpretability-decision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;25.3: Decision framework for interpretability analysis of genomic foundation models. Validation requirements increase with the strength of claims being made.
</figcaption>
<div aria-describedby="tbl-interpretability-decision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 25%">
<col style="width: 32%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Goal</th>
<th>Primary Method</th>
<th>Supporting Methods</th>
<th>Validation Required</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Explain single variant</td>
<td>Integrated gradients</td>
<td>ISM for verification</td>
<td>Motif match, literature</td>
</tr>
<tr class="even">
<td>Find regulatory motifs</td>
<td>TF-MoDISco</td>
<td>Filter visualization</td>
<td>JASPAR match, MPRA</td>
</tr>
<tr class="odd">
<td>Diagnose model shortcuts</td>
<td>Probing classifiers</td>
<td>Attribution for confounders</td>
<td>Held-out distribution</td>
</tr>
<tr class="even">
<td>Understand long-range effects</td>
<td>Attention analysis</td>
<td>Contribution scores</td>
<td>Perturbation experiment</td>
</tr>
<tr class="odd">
<td>Characterize model vocabulary</td>
<td>Sei-style clustering</td>
<td>Embedding geometry</td>
<td>GWAS enrichment</td>
</tr>
<tr class="even">
<td>Generate hypotheses for experiments</td>
<td>TF-MoDISco + grammar</td>
<td>Circuit analysis</td>
<td>EMSA, reporter, CRISPR</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-ch25-conclusion" class="level2" data-number="25.11">
<h2 data-number="25.11" class="anchored" data-anchor-id="sec-ch25-conclusion"><span class="header-section-number">25.11</span> Plausibility Is Not Faithfulness</h2>
<p>The distinction between plausibility and faithfulness remains central to interpretability for genomic foundation models. Models can produce compelling motifs, structured attention patterns, and interpretable probing results while operating through mechanisms that do not correspond to biological reality. A model that correctly predicts splice site strength may do so by recognizing confounded sequence features rather than learning splice site grammar. A model that attributes importance to a transcription factor binding site may be exploiting correlation with GC content rather than modeling regulatory mechanism. Plausible explanations that match biological intuition are not the same as faithful explanations that accurately reflect model computation.</p>
<p>Only interventional experiments can distinguish genuine regulatory insight from sophisticated pattern matching. Computational interventions (deletion tests, counterfactual sequence generation, circuit analysis) probe whether identified features are necessary and sufficient for model predictions. Biological interventions (reporter assays, CRISPR perturbations, massively parallel experiments) test whether model-derived hypotheses hold in living systems. The sequence design applications in <a href="../part_7/p7-ch31-design.html" class="quarto-xref"><span>Chapter 31</span></a> operationalize this validation loop, using interpretability-derived hypotheses to guide experimental libraries. The conjunction of computational and experimental validation transforms interpretability from rationalization into discovery, generating testable hypotheses that advance biological understanding rather than merely explaining model behavior.</p>
<p>As foundation models grow in scale and capability, interpretability becomes simultaneously more important and more challenging. Larger models implement more complex computations, potentially capturing subtler regulatory logic but resisting simple interpretation. Mechanistic interpretability offers a path forward by characterizing model internals directly, though scaling these techniques to billion-parameter genomic models remains an open problem. The evaluation challenges this creates are examined in <a href="../part_3/p3-ch12-evaluation.html#sec-ch12-evaluating-fm" class="quarto-xref"><span>Section 12.9</span></a>, while the confounding risks of scale are addressed in <a href="../part_3/p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>. The integration of interpretability with model development points toward a future where understanding and prediction advance together: motifs discovered through interpretation inform architecture design, experimentally validated hypotheses become supervision signals, and interpretability failures that reveal confounding drive improvements in training data and evaluation. In this vision, interpretability is not merely a tool for explaining existing models but a methodology for building models whose predictions we trust because we understand the mechanisms they have learned.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Test Yourself">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Test Yourself
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reviewing the summary, test your recall:</p>
<ol type="1">
<li>What is the difference between a plausible and a faithful explanation? Why might a model produce attributions that highlight a biologically plausible motif even when that motif doesn’t drive the prediction?</li>
<li>Why is in silico mutagenesis (ISM) considered the “gold standard” for attribution faithfulness, and what is its main practical limitation?</li>
<li>What problem does the saturation issue create for gradient-based attribution methods, and how do integrated gradients address this?</li>
<li>Explain why attention weights are not reliable indicators of input importance. What do they actually measure?</li>
<li>Describe the validation hierarchy from sanity checks to biological sufficiency. What distinguishes computational necessity from biological necessity?</li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Core Concepts:</strong></p>
<ul>
<li><p><strong>Plausibility vs.&nbsp;Faithfulness:</strong> Plausible explanations match human intuition; faithful explanations accurately reflect model computation. Interpretability methods can produce plausible but unfaithful explanations, providing false comfort rather than genuine insight.</p></li>
<li><p><strong>Attribution Methods:</strong> Assign importance scores to input positions. ISM provides faithful counterfactual information but is computationally expensive. Gradient-based methods (saliency, DeepLIFT, integrated gradients) are efficient but can miss important features due to saturation.</p></li>
<li><p><strong>TF-MoDISco:</strong> Discovers motifs from attribution scores rather than raw sequences, focusing on patterns the model actually uses for prediction. Enables grammar inference through co-occurrence analysis.</p></li>
<li><p><strong>Probing Classifiers:</strong> Diagnose what information model representations encode. Simple (linear) probes identify accessible information; probe failure may indicate absence or inaccessible encoding.</p></li>
<li><p><strong>Attention Interpretation:</strong> Attention weights describe information routing, not causal importance. High attention does not imply the attended position drives the prediction. Perturbation experiments are required to establish functional relevance.</p></li>
<li><p><strong>Global Interpretability:</strong> Methods like <em>Sei</em> sequence classes characterize what a model has learned across its training distribution, providing regulatory vocabularies more informative than individual predictions.</p></li>
<li><p><strong>Mechanistic Interpretability:</strong> Reverse-engineers the algorithms implemented by model weights, identifying circuits and features. Promising but nascent for genomic models.</p></li>
<li><p><strong>Validation Hierarchy:</strong> Sanity checks → computational necessity → computational sufficiency → biological necessity → biological sufficiency. Each level provides stronger evidence but requires more experimental investment.</p></li>
</ul>
<p><strong>Key Connections:</strong></p>
<ul>
<li>Interpretability enables clinical utility by providing mechanistic evidence that satisfies ACMG-AMP criteria (<a href="../part_7/p7-ch29-rare-disease.html#sec-ch29-acmg-amp" class="quarto-xref"><span>Section 29.2</span></a>, <a href="../part_7/p7-ch29-rare-disease.html" class="quarto-xref"><span>Chapter 29</span></a>)</li>
<li>Confounder detection (<a href="../part_3/p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>) relies on interpretability to identify shortcuts</li>
<li>Sequence design (<a href="../part_7/p7-ch31-design.html" class="quarto-xref"><span>Chapter 31</span></a>) uses interpretability-derived hypotheses to guide experimental validation</li>
</ul>
<p><strong>Looking Ahead:</strong> <a href="p6-ch26-causal.html" class="quarto-xref"><span>Chapter 26</span></a> extends interpretability to causal inference, examining how to distinguish correlation from causation in model predictions and when interpretable features reflect genuine regulatory mechanisms.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-jaganathan_spliceai_2019" class="csl-entry" role="listitem">
Jaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F. McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A. Kosmicki, et al. 2019. <span>“[<span>SpliceAI</span>] <span>Predicting</span> <span>Splicing</span> from <span>Primary</span> <span>Sequence</span> with <span>Deep</span> <span>Learning</span>.”</span> <em>Cell</em> 176 (3): 535–548.e24. <a href="https://doi.org/10.1016/j.cell.2018.12.015">https://doi.org/10.1016/j.cell.2018.12.015</a>.
</div>
<div id="ref-richards_standards_2015" class="csl-entry" role="listitem">
Richards, Sue, Nazneen Aziz, Sherri Bale, David Bick, Soma Das, Julie Gastier-Foster, Wayne W. Grody, et al. 2015. <span>“Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the <span>American</span> <span>College</span> of <span>Medical</span> <span>Genetics</span> and <span>Genomics</span> and the <span>Association</span> for <span>Molecular</span> <span>Pathology</span>.”</span> <em>Genetics in Medicine</em> 17 (5): 405–24. <a href="https://doi.org/10.1038/gim.2015.30">https://doi.org/10.1038/gim.2015.30</a>.
</div>
<div id="ref-shrikumar_learning_2017" class="csl-entry" role="listitem">
Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. 2017. <span>“Learning <span>Important</span> <span>Features</span> <span>Through</span> <span>Propagating</span> <span>Activation</span> <span>Differences</span>.”</span> In <em>Proceedings of the 34th <span>International</span> <span>Conference</span> on <span>Machine</span> <span>Learning</span></em>, 3145–53. PMLR.
</div>
<div id="ref-shrikumar_technical_2018" class="csl-entry" role="listitem">
Shrikumar, Avanti, Katherine Tian, Žiga Avsec, Anna Shcherbina, Abhimanyu Banerjee, Mahfuza Sharmin, Surag Nair, and Anshul Kundaje. 2018. <span>“Technical <span>Note</span> on <span>Transcription</span> <span>Factor</span> <span>Motif</span> <span>Discovery</span> from <span>Importance</span> <span>Scores</span> (<span>TF</span>-<span>MoDISco</span>) Version 0.5.6.5.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1811.00416">https://doi.org/10.48550/arXiv.1811.00416</a>.
</div>
<div id="ref-sundararajan_axiomatic_2017" class="csl-entry" role="listitem">
Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. <span>“Axiomatic <span>Attribution</span> for <span>Deep</span> <span>Networks</span>.”</span> In <em>Proceedings of the 34th <span>International</span> <span>Conference</span> on <span>Machine</span> <span>Learning</span></em>, 3319–28. PMLR.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_6/p6-ch24-uncertainty.html" class="pagination-link" aria-label="Uncertainty Quantification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_6/p6-ch26-causal.html" class="pagination-link" aria-label="Causality">
        <span class="nav-page-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Causality</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>